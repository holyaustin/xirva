[{"id": "1005.0063", "submitter": "Manas Pathak", "authors": "Manas A. Pathak and Bhiksha Raj", "title": "Large Margin Multiclass Gaussian Classification with Differential\n  Privacy", "comments": "14 pages", "journal-ref": "Proceedings of the ECML/PKDD Workshop on Privacy and Security\n  issues in Data Mining and Machine Learning, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As increasing amounts of sensitive personal information is aggregated into\ndata repositories, it has become important to develop mechanisms for processing\nthe data without revealing information about individual data instances. The\ndifferential privacy model provides a framework for the development and\ntheoretical analysis of such mechanisms. In this paper, we propose an algorithm\nfor learning a discriminatively trained multi-class Gaussian classifier that\nsatisfies differential privacy using a large margin loss function with a\nperturbed regularization term. We present a theoretical upper bound on the\nexcess risk of the classifier introduced by the perturbation.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 11:06:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2010 23:24:04 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Pathak", "Manas A.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1005.0188", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Alexander G. Gray", "title": "Generative and Latent Mean Map Kernels", "comments": "16 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two kernels that extend the mean map, which embeds probability\nmeasures in Hilbert spaces. The generative mean map kernel (GMMK) is a smooth\nsimilarity measure between probabilistic models. The latent mean map kernel\n(LMMK) generalizes the non-iid formulation of Hilbert space embeddings of\nempirical distributions in order to incorporate latent variable models. When\ncomparing certain classes of distributions, the GMMK exhibits beneficial\nregularization and generalization properties not shown for previous generative\nkernels. We present experiments comparing support vector machine performance\nusing the GMMK and LMMK between hidden Markov models to the performance of\nother methods on discrete and continuous observation sequence data. The results\nsuggest that, in many cases, the GMMK has generalization error competitive with\nor better than other methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 05:59:41 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1005.0208", "submitter": "Gerard Biau", "authors": "G\\'erard Biau (LSTA, DMA, LPMA)", "title": "Analysis of a Random Forests Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a scheme proposed by Leo Breiman in the 2000's for\nbuilding a predictor ensemble with a set of decision trees that grow in\nrandomly selected subspaces of data. Despite growing interest and practical\nuse, there has been little exploration of the statistical properties of random\nforests, and little is known about the mathematical forces driving the\nalgorithm. In this paper, we offer an in-depth analysis of a random forests\nmodel suggested by Breiman in \\cite{Bre04}, which is very close to the original\nalgorithm. We show in particular that the procedure is consistent and adapts to\nsparsity, in the sense that its rate of convergence depends only on the number\nof strong features and not on how many noise variables are present.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 06:50:17 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2011 12:06:27 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2012 07:31:02 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, DMA, LPMA"]]}, {"id": "1005.0437", "submitter": "Marius Kloft", "authors": "Marius Kloft, Ulrich R\\\"uckert and Peter L. Bartlett", "title": "A Unifying View of Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on multiple kernel learning has lead to a number of\napproaches for combining kernels in regularized risk minimization. The proposed\napproaches include different formulations of objectives and varying\nregularization strategies. In this paper we present a unifying general\noptimization criterion for multiple kernel learning and show how existing\nformulations are subsumed as special cases. We also derive the criterion's dual\nrepresentation, which is suitable for general smooth optimization algorithms.\nFinally, we evaluate multiple kernel learning in this framework analytically\nusing a Rademacher complexity bound on the generalization error and empirically\nin a set of experiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 06:05:51 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Kloft", "Marius", ""], ["R\u00fcckert", "Ulrich", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1005.0530", "submitter": "Mohak Shah", "authors": "Mohak Shah, Mario Marchand and Jacques Corbeil", "title": "Feature Selection with Conjunctions of Decision Stumps and Learning from\n  Microarray Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the objectives of designing feature selection learning algorithms is\nto obtain classifiers that depend on a small number of attributes and have\nverifiable future performance guarantees. There are few, if any, approaches\nthat successfully address the two goals simultaneously. Performance guarantees\nbecome crucial for tasks such as microarray data analysis due to very small\nsample sizes resulting in limited empirical evaluation. To the best of our\nknowledge, such algorithms that give theoretical bounds on the future\nperformance have not been proposed so far in the context of the classification\nof gene expression data. In this work, we investigate the premise of learning a\nconjunction (or disjunction) of decision stumps in Occam's Razor, Sample\nCompression, and PAC-Bayes learning settings for identifying a small subset of\nattributes that can be used to perform reliable classification tasks. We apply\nthe proposed approaches for gene identification from DNA microarray data and\ncompare our results to those of well known successful approaches proposed for\nthe task. We show that our algorithm not only finds hypotheses with much\nsmaller number of genes while giving competitive classification accuracy but\nalso have tight risk guarantees on future performance unlike other approaches.\nThe proposed approaches are general and extensible in terms of both designing\nnovel algorithms and application to other domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 14:01:10 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Shah", "Mohak", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1005.0766", "submitter": "Vincent Tan", "authors": "Vincent Y. F. Tan, Animashree Anandkumar, Alan S. Willsky", "title": "Learning High-Dimensional Markov Forest Distributions: Analysis of Error\n  Rates", "comments": "Accepted to the Journal of Machine Learning Research (Feb 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning forest-structured discrete graphical models from\ni.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu\ntree through adaptive thresholding is proposed. It is shown that this algorithm\nis both structurally consistent and risk consistent and the error probability\nof structure learning decays faster than any polynomial in the number of\nsamples under fixed model size. For the high-dimensional scenario where the\nsize of the model d and the number of edges k scale with the number of samples\nn, sufficient conditions on (n,d,k) are given for the algorithm to satisfy\nstructural and risk consistencies. In addition, the extremal structures for\nlearning are identified; we prove that the independent (resp. tree) model is\nthe hardest (resp. easiest) to learn using the proposed algorithm in terms of\nerror rates for structure learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 14:41:06 GMT"}, {"version": "v2", "created": "Sun, 13 Feb 2011 02:19:59 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Tan", "Vincent Y. F.", ""], ["Anandkumar", "Animashree", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1005.0794", "submitter": "Xiaoran Yan", "authors": "Xiaoran Yan, Yaojia Zhu, Jean-Baptiste Rouquier, and Cristopher Moore", "title": "Active Learning for Hidden Attributes in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.IT cs.LG math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many networks, vertices have hidden attributes, or types, that are\ncorrelated with the networks topology. If the topology is known but these\nattributes are not, and if learning the attributes is costly, we need a method\nfor choosing which vertex to query in order to learn as much as possible about\nthe attributes of the other vertices. We assume the network is generated by a\nstochastic block model, but we make no assumptions about its assortativity or\ndisassortativity. We choose which vertex to query using two methods: 1)\nmaximizing the mutual information between its attributes and those of the\nothers (a well-known approach in active learning) and 2) maximizing the average\nagreement between two independent samples of the conditional Gibbs\ndistribution. Experimental results show that both these methods do much better\nthan simple heuristics. They also consistently identify certain vertices as\nimportant by querying them early on.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 17:11:26 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Yan", "Xiaoran", ""], ["Zhu", "Yaojia", ""], ["Rouquier", "Jean-Baptiste", ""], ["Moore", "Cristopher", ""]]}, {"id": "1005.0826", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Lille - Nord Europe)", "title": "Clustering processes", "comments": "in proceedings of ICML 2010. arXiv-admin Note: This is a newer\n  version of the article arXiv:1004.5194v1, please see that article for any\n  previous version", "journal-ref": "27th International Conference on Machine Learning (2010) 919-926", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 19:41:55 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 09:32:30 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Lille - Nord Europe"]]}, {"id": "1005.0928", "submitter": "Antti Airola", "authors": "Antti Airola, Tapio Pahikkala, Tapio Salakoski", "title": "Training linear ranking SVMs in linearithmic time using red-black trees", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.patrec.2011.03.014", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient method for training the linear ranking support\nvector machine. The method combines cutting plane optimization with red-black\ntree based approach to subgradient calculations, and has O(m*s+m*log(m)) time\ncomplexity, where m is the number of training examples, and s the average\nnumber of non-zero features per example. Best previously known training\nalgorithms achieve the same efficiency only for restricted special cases,\nwhereas the proposed approach allows any real valued utility scores in the\ntraining data. Experiments demonstrate the superior scalability of the proposed\napproach, when compared to the fastest existing RankSVM implementations.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 08:38:24 GMT"}, {"version": "v2", "created": "Mon, 31 Jan 2011 14:03:39 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Airola", "Antti", ""], ["Pahikkala", "Tapio", ""], ["Salakoski", "Tapio", ""]]}, {"id": "1005.1036", "submitter": "Marco Scutari", "authors": "Marco Scutari and Korbinian Strimmer", "title": "Introduction to Graphical Modelling", "comments": "Handbook of Statistical Systems Biology (D. Balding, M. Stumpf, M.\n  Girolami, eds.), Wiley. 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this chapter is twofold. In the first part we will provide a brief\noverview of the mathematical and statistical foundations of graphical models,\nalong with their fundamental properties, estimation and basic inference\nprocedures. In particular we will develop Markov networks (also known as Markov\nrandom fields) and Bayesian networks, which comprise most past and current\nliterature on graphical models. In the second part we will review some\napplications of graphical models in systems biology.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 16:33:40 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 08:09:02 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2011 09:14:14 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Scutari", "Marco", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1005.1440", "submitter": "Javier Rojo", "authors": "Javier Rojo and Tuan Nguyen", "title": "Improving the Johnson-Lindenstrauss Lemma", "comments": "24 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in\n$p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k\n\\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise\ndistances are preserved within a factor of $1\\pm\\epsilon$. Here, working\ndirectly with the distributions of the random distances rather than resorting\nto the moment generating function technique, an improvement on the lower bound\nfor $k$ is obtained. The additional reduction in dimension when compared to\nbounds found in the literature, is at least $13\\%$, and, in some cases, up to\n$30\\%$ additional reduction is achieved. Using the moment generating function\ntechnique, we further provide a lower bound for $k$ using pairwise $L_2$\ndistances in the space of points to be projected and pairwise $L_1$ distances\nin the space of the projected points. Comparison with the results obtained in\nthe literature shows that the bound presented here provides an additional\n$36-40\\%$ reduction.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 03:05:23 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Rojo", "Javier", ""], ["Nguyen", "Tuan", ""]]}, {"id": "1005.1593", "submitter": "Guido Montufar", "authors": "Guido Montufar and Nihat Ay", "title": "Refinements of Universal Approximation Results for Deep Belief Networks\n  and Restricted Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve recently published results about resources of Restricted Boltzmann\nMachines (RBM) and Deep Belief Networks (DBN) required to make them Universal\nApproximators. We show that any distribution p on the set of binary vectors of\nlength n can be arbitrarily well approximated by an RBM with k-1 hidden units,\nwhere k is the minimal number of pairs of binary vectors differing in only one\nentry such that their union contains the support set of p. In important cases\nthis number is half of the cardinality of the support set of p. We construct a\nDBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of\napproximating any distribution on {0,1}^n arbitrarily well. This confirms a\nconjecture presented by Le Roux and Bengio 2010.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 15:38:54 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2010 14:08:24 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Montufar", "Guido", ""], ["Ay", "Nihat", ""]]}, {"id": "1005.2012", "submitter": "John Duchi", "authors": "John Duchi, Alekh Agarwal, Martin Wainwright", "title": "Dual Averaging for Distributed Optimization: Convergence Analysis and\n  Network Scaling", "comments": "40 pages, 4 figures", "journal-ref": "IEEE Transactions on Automatic Control 57(3), pp. 592 - 606. March\n  2012", "doi": "10.1109/TAC.2011.2161027", "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of decentralized optimization over a network is to optimize a global\nobjective formed by a sum of local (possibly nonsmooth) convex functions using\nonly local computation and communication. It arises in various application\ndomains, including distributed tracking and localization, multi-agent\nco-ordination, estimation in sensor networks, and large-scale optimization in\nmachine learning. We develop and analyze distributed algorithms based on dual\naveraging of subgradients, and we provide sharp bounds on their convergence\nrates as a function of the network size and topology. Our method of analysis\nallows for a clear separation between the convergence of the optimization\nalgorithm itself and the effects of communication constraints arising from the\nnetwork structure. In particular, we show that the number of iterations\nrequired by our algorithm scales inversely in the spectral gap of the network.\nThe sharpness of this prediction is confirmed both by theoretical lower bounds\nand simulations for various networks. Our approach includes both the cases of\ndeterministic optimization and communication, as well as problems with\nstochastic optimization and/or communication.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2010 07:44:49 GMT"}, {"version": "v2", "created": "Fri, 14 May 2010 17:10:01 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2011 22:21:08 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Duchi", "John", ""], ["Agarwal", "Alekh", ""], ["Wainwright", "Martin", ""]]}, {"id": "1005.2238", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Geoff R. Hosack, Keith R. Hayes", "title": "Ecological non-linear state space model selection via adaptive particle\n  Markov chain Monte Carlo (AdPMCMC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel advanced Particle Markov chain Monte Carlo algorithm that\nis capable of sampling from the posterior distribution of non-linear state\nspace models for both the unobserved latent states and the unknown model\nparameters. We apply this novel methodology to five population growth models,\nincluding models with strong and weak Allee effects, and test if it can\nefficiently sample from the complex likelihood surface that is often associated\nwith these models. Utilising real and also synthetically generated data sets we\nexamine the extent to which observation noise and process error may frustrate\nefforts to choose between these models. Our novel algorithm involves an\nAdaptive Metropolis proposal combined with an SIR Particle MCMC algorithm\n(AdPMCMC). We show that the AdPMCMC algorithm samples complex, high-dimensional\nspaces efficiently, and is therefore superior to standard Gibbs or Metropolis\nHastings algorithms that are known to converge very slowly when applied to the\nnon-linear state space ecological models considered in this paper.\nAdditionally, we show how the AdPMCMC algorithm can be used to recursively\nestimate the Bayesian Cram\\'er-Rao Lower Bound of Tichavsk\\'y (1998). We derive\nexpressions for these Cram\\'er-Rao Bounds and estimate them for the models\nconsidered. Our results demonstrate a number of important features of common\npopulation growth models, most notably their multi-modal posterior surfaces and\ndependence between the static and dynamic parameters. We conclude by sampling\nfrom the posterior distribution of each of the models, and use Bayes factors to\nhighlight how observation noise significantly diminishes our ability to select\namong some of the models, particularly those that are designed to reproduce an\nAllee effect.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 01:28:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Peters", "Gareth W.", ""], ["Hosack", "Geoff R.", ""], ["Hayes", "Keith R.", ""]]}, {"id": "1005.2263", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Context models on sequences of covers", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of models that, via a simple construction, enables exact,\nincremental, non-parametric, polynomial-time, Bayesian inference of conditional\nmeasures. The approach relies upon creating a sequence of covers on the\nconditioning variable and maintaining a different model for each set within a\ncover. Inference remains tractable by specifying the probabilistic model in\nterms of a random walk within the sequence of covers. We demonstrate the\napproach on problems of conditional density estimation, which, to our knowledge\nis the first closed-form, non-parametric Bayesian approach to this problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 07:02:54 GMT"}, {"version": "v2", "created": "Mon, 30 May 2011 11:22:44 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1005.2638", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets", "comments": "41 pages, 13 figures, 6 tables. 81 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2010 23:12:03 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1005.3014", "submitter": "Cameron Freer", "authors": "Nathanael L. Ackerman, Cameron E. Freer, Daniel M. Roy", "title": "On the computability of conditional probability", "comments": "44 pages, 3 figures. Final published version", "journal-ref": "Journal of the ACM, 66:3 (2019), pp. 23:1-23:40", "doi": "10.1145/3321699", "report-no": null, "categories": "math.LO cs.LO math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As inductive inference and machine learning methods in computer science see\ncontinued success, researchers are aiming to describe ever more complex\nprobabilistic models and inference algorithms. It is natural to ask whether\nthere is a universal computational procedure for probabilistic inference. We\ninvestigate the computability of conditional probability, a fundamental notion\nin probability theory and a cornerstone of Bayesian statistics. We show that\nthere are computable joint distributions with noncomputable conditional\ndistributions, ruling out the prospect of general inference algorithms, even\ninefficient ones. Specifically, we construct a pair of computable random\nvariables in the unit interval such that the conditional distribution of the\nfirst variable given the second encodes the halting problem. Nevertheless,\nprobabilistic inference is possible in many common modeling settings, and we\nprove several results giving broadly applicable conditions under which\nconditional distributions are computable. In particular, conditional\ndistributions become computable when measurements are corrupted by independent\ncomputable noise with a sufficiently smooth bounded density.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2010 19:58:54 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2011 00:11:50 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 02:03:29 GMT"}, {"version": "v4", "created": "Sat, 16 Nov 2019 06:49:45 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ackerman", "Nathanael L.", ""], ["Freer", "Cameron E.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1005.3579", "submitter": "Xi Chen", "authors": "Xi Chen and Seyoung Kim and Qihang Lin and Jaime G. Carbonell and Eric\n  P. Xing", "title": "Graph-Structured Multi-task Regression and an Efficient Optimization\n  Method for General Fused Lasso", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a structured multi-task regression, where\nthe output consists of multiple responses that are related by a graph and the\ncorrelated response variables are dependent on the common inputs in a sparse\nbut synergistic manner. Previous methods such as l1/l2-regularized multi-task\nregression assume that all of the output variables are equally related to the\ninputs, although in many real-world problems, outputs are related in a complex\nmanner. In this paper, we propose graph-guided fused lasso (GFlasso) for\nstructured multi-task regression that exploits the graph structure over the\noutput variables. We introduce a novel penalty function based on fusion penalty\nto encourage highly correlated outputs to share a common set of relevant\ninputs. In addition, we propose a simple yet efficient proximal-gradient method\nfor optimizing GFlasso that can also be applied to any optimization problems\nwith a convex smooth loss and the general class of fusion penalty defined on\narbitrary graph structures. By exploiting the structure of the non-smooth\n''fusion penalty'', our method achieves a faster convergence rate than the\nstandard first-order method, sub-gradient method, and is significantly more\nscalable than the widely adopted second-order cone-programming and\nquadratic-programming formulations. In addition, we provide an analysis of the\nconsistency property of the GFlasso model. Experimental results not only\ndemonstrate the superiority of GFlasso over the standard lasso but also show\nthe efficiency and scalability of our proximal-gradient method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2010 01:59:42 GMT"}], "update_date": "2010-05-21", "authors_parsed": [["Chen", "Xi", ""], ["Kim", "Seyoung", ""], ["Lin", "Qihang", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1005.4006", "submitter": "Daniel Dunlavy", "authors": "Daniel M. Dunlavy, Tamara G. Kolda and Evrim Acar", "title": "Temporal Link Prediction using Matrix and Tensor Factorizations", "comments": null, "journal-ref": "ACM Transactions on Knowledge Discovery from Data 5(2):10 (27\n  pages), February 2011", "doi": "10.1145/1921632.1921636", "report-no": null, "categories": "math.NA cs.NA physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data in many disciplines such as social networks, web analysis, etc. is\nlink-based, and the link structure can be exploited for many different data\nmining tasks. In this paper, we consider the problem of temporal link\nprediction: Given link data for times 1 through T, can we predict the links at\ntime T+1? If our data has underlying periodic structure, can we predict out\neven further in time, i.e., links at time T+2, T+3, etc.? In this paper, we\nconsider bipartite graphs that evolve over time and consider matrix- and\ntensor-based methods for predicting future links. We present a weight-based\nmethod for collapsing multi-year data into a single matrix. We show how the\nwell-known Katz method for link prediction can be extended to bipartite graphs\nand, moreover, approximated in a scalable way using a truncated singular value\ndecomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we\nillustrate the usefulness of exploiting the natural three-dimensional structure\nof temporal link data. Through several numerical experiments, we demonstrate\nthat both matrix- and tensor-based techniques are effective for temporal link\nprediction despite the inherent difficulty of the problem. Additionally, we\nshow that tensor-based techniques are particularly effective for temporal data\nwith varying periodic patterns.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:07:35 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2010 04:53:56 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Dunlavy", "Daniel M.", ""], ["Kolda", "Tamara G.", ""], ["Acar", "Evrim", ""]]}, {"id": "1005.4214", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Measures of Variability for Bayesian Network Graphical Structures", "comments": "19 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:0909.1685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a Bayesian network includes a great deal of information\nabout the probability distribution of the data, which is uniquely identified\ngiven some general distributional assumptions. Therefore it's important to\nstudy its variability, which can be used to compare the performance of\ndifferent learning algorithms and to measure the strength of any arbitrary\nsubset of arcs.\n  In this paper we will introduce some descriptive statistics and the\ncorresponding parametric and Monte Carlo tests on the undirected graph\nunderlying the structure of a Bayesian network, modeled as a multivariate\nBernoulli random variable. A simple numeric example and the comparison of the\nperformance of some structure learning algorithm on small samples will then\nillustrate their use.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2010 18:14:39 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 10:40:21 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1005.4717", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing", "title": "Smoothing proximal gradient method for general structured sparse\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS514 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 719-752", "doi": "10.1214/11-AOAS514", "report-no": "IMS-AOAS-AOAS514", "categories": "stat.ML cs.LG math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating high-dimensional regression models\nregularized by a structured sparsity-inducing penalty that encodes prior\nstructural information on either the input or output variables. We consider two\nwidely adopted types of penalties of this kind as motivating examples: (1) the\ngeneral overlapping-group-lasso penalty, generalized from the group-lasso\npenalty; and (2) the graph-guided-fused-lasso penalty, generalized from the\nfused-lasso penalty. For both types of penalties, due to their nonseparability\nand nonsmoothness, developing an efficient optimization method remains a\nchallenging problem. In this paper we propose a general optimization approach,\nthe smoothing proximal gradient (SPG) method, which can solve structured sparse\nregression problems with any smooth convex loss under a wide spectrum of\nstructured sparsity-inducing penalties. Our approach combines a smoothing\ntechnique with an effective proximal gradient method. It achieves a convergence\nrate significantly faster than the standard first-order methods, subgradient\nmethods, and is much more scalable than the most widely used interior-point\nmethods. The efficiency and scalability of our method are demonstrated on both\nsimulation experiments and real genetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 00:50:17 GMT"}, {"version": "v2", "created": "Sun, 21 Nov 2010 21:24:00 GMT"}, {"version": "v3", "created": "Sat, 26 Mar 2011 01:17:05 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2012 05:53:50 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Kim", "Seyoung", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1005.5081", "submitter": "Luke Bornn", "authors": "Luke Bornn, Fran\\c{c}ois Caron", "title": "Bayesian clustering in decomposable graphs", "comments": "3 figures, 1 table", "journal-ref": "Bornn, L., Caron, F. (2011) Bayesian Clustering in Decomposable\n  Graphs. Bayesian Analysis Vol. 6, No. 4, 829-846", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a class of prior distributions on decomposable\ngraphs, allowing for improved modeling flexibility. While existing methods\nsolely penalize the number of edges, the proposed work empowers practitioners\nto control clustering, level of separation, and other features of the graph.\nEmphasis is placed on a particular prior distribution which derives its\nmotivation from the class of product partition models; the properties of this\nprior relative to existing priors is examined through theory and simulation. We\nthen demonstrate the use of graphical models in the field of agriculture,\nshowing how the proposed prior distribution alleviates the inflexibility of\nprevious approaches in properly modeling the interactions between the yield of\ndifferent crop varieties.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 14:41:10 GMT"}, {"version": "v2", "created": "Thu, 3 May 2012 06:27:01 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Bornn", "Luke", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "1005.5337", "submitter": "James Demers", "authors": "Marten F. Byl, James T. Demers, and Edward A. Rietman", "title": "Using a Kernel Adatron for Object Classification with RCS Data", "comments": "This material is based upon work supported by US Army Space & Missile\n  Command under Contract Number W9113M-07-C-0204. Any opinions, findings and\n  conclusions or recommendations expressed in this material are those of the\n  authors and do not necessarily re flect the views of US Army Space & Missile\n  Command", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid identification of object from radar cross section (RCS) signals is\nimportant for many space and military applications. This identification is a\nproblem in pattern recognition which either neural networks or support vector\nmachines should prove to be high-speed. Bayesian networks would also provide\nvalue but require significant preprocessing of the signals. In this paper, we\ndescribe the use of a support vector machine for object identification from\nsynthesized RCS data. Our best results are from data fusion of X-band and\nS-band signals, where we obtained 99.4%, 95.3%, 100% and 95.6% correct\nidentification for cylinders, frusta, spheres, and polygons, respectively. We\nalso compare our results with a Bayesian approach and show that the SVM is\nthree orders of magnitude faster, as measured by the number of floating point\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 17:25:05 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Byl", "Marten F.", ""], ["Demers", "James T.", ""], ["Rietman", "Edward A.", ""]]}]