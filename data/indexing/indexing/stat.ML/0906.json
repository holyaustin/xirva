[{"id": "0906.1980", "submitter": "Aram Galstyan", "authors": "Armen Allahverdyan, Aram Galstyan", "title": "On Maximum a Posteriori Estimation of Hidden Markov Processes", "comments": "9 pages, to appear in the Proceedings of the 25th Conference on\n  Uncertainty in Artificial Intelligence (UAI-2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.stat-mech cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis of Maximum a Posteriori (MAP) sequence\nestimation for binary symmetric hidden Markov processes. We reduce the MAP\nestimation to the energy minimization of an appropriately defined Ising spin\nmodel, and focus on the performance of MAP as characterized by its accuracy and\nthe number of solutions corresponding to a typical observed sequence. It is\nshown that for a finite range of sufficiently low noise levels, the solution is\nuniquely related to the observed sequence, while the accuracy degrades linearly\nwith increasing the noise strength. For intermediate noise values, the accuracy\nis nearly noise-independent, but now there are exponentially many solutions to\nthe estimation problem, which is reflected in non-zero ground-state entropy for\nthe Ising model. Finally, for even larger noise intensities, the number of\nsolutions reduces again, but the accuracy is poor. It is shown that these\nregimes are different thermodynamic phases of the Ising model that are related\nto each other via first-order phase transitions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 17:23:12 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Allahverdyan", "Armen", ""], ["Galstyan", "Aram", ""]]}, {"id": "0906.2027", "submitter": "Sewoong Oh", "authors": "Raghunandan H. Keshavan, Andrea Montanari and Sewoong Oh", "title": "Matrix Completion from Noisy Entries", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix M of low-rank, we consider the problem of reconstructing it\nfrom noisy observations of a small, random subset of its entries. The problem\narises in a variety of applications, from collaborative filtering (the `Netflix\nproblem') to structure-from-motion and positioning. We study a low complexity\nalgorithm introduced by Keshavan et al.(2009), based on a combination of\nspectral techniques and manifold optimization, that we call here OptSpace. We\nprove performance guarantees that are order-optimal in a number of\ncircumstances.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 00:22:58 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2012 17:37:45 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "0906.2034", "submitter": "Trevor Hastie", "authors": "Rahul Mazumder, Trevor Hastie and Rob Tibshirani", "title": "Regularization methods for learning incomplete matrices", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use convex relaxation techniques to provide a sequence of solutions to the\nmatrix completion problem. Using the nuclear norm as a regularizer, we provide\nsimple and very efficient algorithms for minimizing the reconstruction error\nsubject to a bound on the nuclear norm. Our algorithm iteratively replaces the\nmissing elements with those obtained from a thresholded SVD. With warm starts\nthis allows us to efficiently compute an entire regularization path of\nsolutions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 18:33:19 GMT"}], "update_date": "2009-06-12", "authors_parsed": [["Mazumder", "Rahul", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Rob", ""]]}, {"id": "0906.3465", "submitter": "Genevera I. Allen", "authors": "Genevera I. Allen, Robert Tibshirani", "title": "Transposable regularized covariance models with an application to\n  missing data imputation", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS314 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 764-790", "doi": "10.1214/09-AOAS314", "report-no": "IMS-AOAS-AOAS314", "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data estimation is an important challenge with high-dimensional data\narranged in the form of a matrix. Typically this data matrix is transposable,\nmeaning that either the rows, columns or both can be treated as features. To\nmodel transposable data, we present a modification of the matrix-variate\nnormal, the mean-restricted matrix-variate normal, in which the rows and\ncolumns each have a separate mean vector and covariance matrix. By placing\nadditive penalties on the inverse covariance matrices of the rows and columns,\nthese so-called transposable regularized covariance models allow for maximum\nlikelihood estimation of the mean and nonsingular covariance matrices. Using\nthese models, we formulate EM-type algorithms for missing data imputation in\nboth the multivariate and transposable frameworks. We present theoretical\nresults exploiting the structure of our transposable models that allow these\nmodels and imputation methods to be applied to high-dimensional data.\nSimulations and results on microarray data and the Netflix data show that these\nimputation techniques often outperform existing methods and offer a greater\ndegree of flexibility.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 17:42:31 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2009 20:21:11 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2009 18:09:36 GMT"}, {"version": "v4", "created": "Tue, 9 Nov 2010 07:31:12 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0906.3590", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen", "title": "Forest Garrote", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for high-dimensional linear models has received a lot of\nattention lately, mostly in the context of l1-regularization. Part of the\nattraction is the variable selection effect: parsimonious models are obtained,\nwhich are very suitable for interpretation. In terms of predictive power,\nhowever, these regularized linear models are often slightly inferior to machine\nlearning procedures like tree ensembles. Tree ensembles, on the other hand,\nlack usually a formal way of variable selection and are difficult to visualize.\nA Garrote-style convex penalty for trees ensembles, in particular Random\nForests, is proposed. The penalty selects functional groups of nodes in the\ntrees. These could be as simple as monotone functions of individual predictor\nvariables. This yields a parsimonious function fit, which lends itself easily\nto visualization and interpretation. The predictive power is maintained at\nleast at the same level as the original tree ensemble. A key feature of the\nmethod is that, once a tree ensemble is fitted, no further tuning parameter\nneeds to be selected. The empirical performance is demonstrated on a wide array\nof datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2009 07:29:34 GMT"}], "update_date": "2009-06-22", "authors_parsed": [["Meinshausen", "Nicolai", ""]]}, {"id": "0906.4258", "submitter": "Nicole Kraemer", "authors": "Alexander Zien, Nicole Kraemer, Soeren Sonnenburg, Gunnar Raetsch", "title": "The Feature Importance Ranking Measure", "comments": "15 pages, 3 figures. to appear in the Proceedings of the European\n  Conference on Machine Learning and Principles and Practice of Knowledge\n  Discovery in Databases (ECML/PKDD), 2009", "journal-ref": "Proceedings of the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),\n  Lecture Notes in Computer Science 5782, 694 - 709, 2009", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most accurate predictions are typically obtained by learning machines with\ncomplex feature spaces (as e.g. induced by kernels). Unfortunately, such\ndecision rules are hardly accessible to humans and cannot easily be used to\ngain insights about the application domain. Therefore, one often resorts to\nlinear models in combination with variable selection, thereby sacrificing some\npredictive power for presumptive interpretability. Here, we introduce the\nFeature Importance Ranking Measure (FIRM), which by retrospective analysis of\narbitrary learning machines allows to achieve both excellent predictive\nperformance and superior interpretation. In contrast to standard raw feature\nweighting, FIRM takes the underlying correlation structure of the features into\naccount. Thereby, it is able to discover the most relevant features, even if\ntheir appearance in the training data is entirely prevented by noise. The\ndesirable properties of FIRM are investigated analytically and illustrated in\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 13:45:10 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Zien", "Alexander", ""], ["Kraemer", "Nicole", ""], ["Sonnenburg", "Soeren", ""], ["Raetsch", "Gunnar", ""]]}, {"id": "0906.4391", "submitter": "Genevera Allen", "authors": "Genevera I. Allen", "title": "KNIFE: Kernel Iterative Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting important features in non-linear or kernel spaces is a difficult\nchallenge in both classification and regression problems. When many of the\nfeatures are irrelevant, kernel methods such as the support vector machine and\nkernel ridge regression can sometimes perform poorly. We propose weighting the\nfeatures within a kernel with a sparse set of weights that are estimated in\nconjunction with the original classification or regression problem. The\niterative algorithm, KNIFE, alternates between finding the coefficients of the\noriginal problem and finding the feature weights through kernel linearization.\nIn addition, a slight modification of KNIFE yields an efficient algorithm for\nfinding feature regularization paths, or the paths of each feature's weight.\nSimulation results demonstrate the utility of KNIFE for both kernel regression\nand support vector machines with a variety of kernels. Feature path\nrealizations also reveal important non-linear correlations among features that\nprove useful in determining a subset of significant variables. Results on vowel\nrecognition data, Parkinson's disease data, and microarray data are also given.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 02:17:36 GMT"}], "update_date": "2009-06-25", "authors_parsed": [["Allen", "Genevera I.", ""]]}, {"id": "0906.4582", "submitter": "Patrick J. Wolfe", "authors": "Mohamed-Ali Belabbas and Patrick J. Wolfe", "title": "On landmark selection and sampling in high-dimensional data analysis", "comments": "18 pages, 6 figures, submitted for publication", "journal-ref": "Philosophical Transactions of the Royal Society, Series A, vol.\n  367, pp. 4295-4312, 2009", "doi": "10.1098/rsta.2009.0161", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the spectral analysis of appropriately defined kernel\nmatrices has emerged as a principled way to extract the low-dimensional\nstructure often prevalent in high-dimensional data. Here we provide an\nintroduction to spectral methods for linear and nonlinear dimension reduction,\nemphasizing ways to overcome the computational limitations currently faced by\npractitioners with massive datasets. In particular, a data subsampling or\nlandmark selection process is often employed to construct a kernel based on\npartial information, followed by an approximate spectral analysis termed the\nNystrom extension. We provide a quantitative framework to analyse this\nprocedure, and use it to demonstrate algorithmic performance bounds on a range\nof practical approaches designed to optimize the landmark selection process. We\ncompare the practical implications of these bounds by way of real-world\nexamples drawn from the field of computer vision, whereby low-dimensional\nmanifold structure is shown to emerge from high-dimensional video data streams.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 23:40:22 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Belabbas", "Mohamed-Ali", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.4779", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Peter Battaglino and Michael R. DeWeese", "title": "Minimum Probability Flow Learning", "comments": "Updated to match ICML conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting probabilistic models to data is often difficult, due to the general\nintractability of the partition function and its derivatives. Here we propose a\nnew parameter estimation technique that does not require computing an\nintractable normalization factor or sampling from the equilibrium distribution\nof the model. This is achieved by establishing dynamics that would transform\nthe observed data distribution into the model distribution, and then setting as\nthe objective the minimization of the KL divergence between the data\ndistribution and the distribution produced by running the dynamics for an\ninfinitesimal time. Score matching, minimum velocity learning, and certain\nforms of contrastive divergence are shown to be special cases of this learning\ntechnique. We demonstrate parameter estimation in Ising models, deep belief\nnetworks and an independent component analysis model of natural scenes. In the\nIsing model case, current state of the art techniques are outperformed by at\nleast an order of magnitude in learning time, with lower error in recovered\ncoupling parameters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 19:15:44 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2009 02:20:21 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2010 07:03:16 GMT"}, {"version": "v4", "created": "Sun, 25 Sep 2011 01:33:51 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Battaglino", "Peter", ""], ["DeWeese", "Michael R.", ""]]}, {"id": "0906.4982", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov, Sergei O. Kuznetsov", "title": "Concept-based Recommendations for Internet Advertisement", "comments": "D.I.Ignatov, S.O. Kuznetsov. Concept-based Recommendations for\n  Internet Advertisement//In proceedings of The Sixth International Conference\n  Concept Lattices and Their Applications (CLA'08), Olomouc, Czech Republic,\n  2008 ISBN 978-80-244-2111-7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting terms that can be interesting to the advertiser is\nconsidered. If a company has already bought some advertising terms which\ndescribe certain services, it is reasonable to find out the terms bought by\ncompeting companies. A part of them can be recommended as future advertising\nterms to the company. The goal of this work is to propose better interpretable\nrecommendations based on FCA and association rules.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 17:26:05 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Kuznetsov", "Sergei O.", ""]]}, {"id": "0906.5190", "submitter": "Tong Zhang", "authors": "Kai Yu, Tong Zhang", "title": "High Dimensional Nonlinear Learning using Local Coordinate Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for semi-supervised learning on high\ndimensional nonlinear manifolds, which includes a phase of unsupervised basis\nlearning and a phase of supervised function learning. The learned bases provide\na set of anchor points to form a local coordinate system, such that each data\npoint $x$ on the manifold can be locally approximated by a linear combination\nof its nearby anchor points, with the linear weights offering a\nlocal-coordinate coding of $x$. We show that a high dimensional nonlinear\nfunction can be approximated by a global linear function with respect to this\ncoding scheme, and the approximation quality is ensured by the locality of such\ncoding. The method turns a difficult nonlinear learning problem into a simple\nglobal linear learning problem, which overcomes some drawbacks of traditional\nlocal learning methods. The work also gives a theoretical justification to the\nempirical success of some biologically-inspired models using sparse coding of\nsensory data, since a local coding scheme must be sufficiently sparse. However,\nsparsity does not always satisfy locality conditions, and can thus possibly\nlead to suboptimal results. The properties and performances of the method are\nempirically verified on synthetic data, handwritten digit classification, and\nobject recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 01:22:09 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Yu", "Kai", ""], ["Zhang", "Tong", ""]]}, {"id": "0906.5263", "submitter": "Kai Puolamaki", "authors": "Sami Hanhij\\\"arvi, Kai Puolam\\\"aki, Gemma C. Garriga", "title": "Multiple Hypothesis Testing in Pattern Discovery", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multiple hypothesis testing arises when there are more than\none hypothesis to be tested simultaneously for statistical significance. This\nis a very common situation in many data mining applications. For instance,\nassessing simultaneously the significance of all frequent itemsets of a single\ndataset entails a host of hypothesis, one for each itemset. A multiple\nhypothesis testing method is needed to control the number of false positives\n(Type I error). Our contribution in this paper is to extend the multiple\nhypothesis framework to be used with a generic data mining algorithm. We\nprovide a method that provably controls the family-wise error rate (FWER, the\nprobability of at least one false positive) in the strong sense. We evaluate\nthe performance of our solution on both real and generated data. The results\nshow that our method controls the FWER while maintaining the power of the test.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 13:31:07 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Hanhij\u00e4rvi", "Sami", ""], ["Puolam\u00e4ki", "Kai", ""], ["Garriga", "Gemma C.", ""]]}]