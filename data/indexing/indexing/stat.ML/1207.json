[{"id": "1207.0057", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio and Guillaume Alain and Salah Rifai", "title": "Implicit Density Estimation by Local Moment Matching to Sample from\n  Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests that some auto-encoder variants do a good job of\ncapturing the local manifold structure of the unknown data generating density.\nThis paper contributes to the mathematical understanding of this phenomenon and\nhelps define better justified sampling algorithms for deep learning based on\nauto-encoder variants. We consider an MCMC where each step samples from a\nGaussian whose mean and covariance matrix depend on the previous state, defines\nthrough its asymptotic distribution a target density. First, we show that good\nchoices (in the sense of consistency) for these mean and covariance functions\nare the local expected value and local covariance under that target density.\nThen we show that an auto-encoder with a contractive penalty captures\nestimators of these local moments in its reconstruction function and its\nJacobian. A contribution of this work is thus a novel alternative to\nmaximum-likelihood density estimation, which we call local moment matching. It\nalso justifies a recently proposed sampling algorithm for the Contractive\nAuto-Encoder and extends it to the Denoising Auto-Encoder.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 07:45:11 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Bengio", "Yoshua", ""], ["Alain", "Guillaume", ""], ["Rifai", "Salah", ""]]}, {"id": "1207.0099", "submitter": "Masashi Sugiyama", "authors": "Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus\n  Christoffel du Plessis, Song Liu, Ichiro Takeuchi", "title": "Density-Difference Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the difference between two probability\ndensities. A naive approach is a two-step procedure of first estimating two\ndensities separately and then computing their difference. However, such a\ntwo-step procedure does not necessarily work well because the first step is\nperformed without regard to the second step and thus a small error incurred in\nthe first stage can cause a big error in the second stage. In this paper, we\npropose a single-shot procedure for directly estimating the density difference\nwithout separately estimating two densities. We derive a non-parametric\nfinite-sample error bound for the proposed single-shot density-difference\nestimator and show that it achieves the optimal convergence rate. The\nusefulness of the proposed method is also demonstrated experimentally.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 14:21:46 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sugiyama", "Masashi", ""], ["Kanamori", "Takafumi", ""], ["Suzuki", "Taiji", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Liu", "Song", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1207.0170", "submitter": "M Taghizadeh-Popp", "authors": "M. Taghizadeh-Popp, S. Heinis and A. S. Szalay", "title": "Single parameter galaxy classification: The Principal Curve through the\n  multi-dimensional space of galaxy properties", "comments": "Full abstract in downloadable version", "journal-ref": null, "doi": "10.1088/0004-637X/755/2/143", "report-no": null, "categories": "astro-ph.CO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to describe the variety of galaxies from SDSS by using only one\naffine parameter. To this aim, we build the Principal Curve (P-curve) passing\nthrough the spine of the data point cloud, considering the eigenspace derived\nfrom Principal Component Analysis of morphological, physical and photometric\ngalaxy properties. Thus, galaxies can be labeled, ranked and classified by a\nsingle arc length value of the curve, measured at the unique closest projection\nof the data points on the P-curve. We find that the P-curve has a \"W\" letter\nshape with 3 turning points, defining 4 branches that represent distinct galaxy\npopulations. This behavior is controlled mainly by 2 properties, namely u-r and\nSFR. We further present the variations of several galaxy properties as a\nfunction of arc length. Luminosity functions variate from steep Schechter fits\nat low arc length, to double power law and ending in Log-normal fits at high\narc length. Galaxy clustering shows increasing autocorrelation power at large\nscales as arc length increases. PCA analysis allowed to find peculiar galaxy\npopulations located apart from the main cloud of data points, such as small red\ngalaxies dominated by a disk, of relatively high stellar mass-to-light ratio\nand surface mass density. The P-curve allows not only dimensionality reduction,\nbut also provides supporting evidence for relevant physical models and\nscenarios in extragalactic astronomy: 1) Evidence for the hierarchical merging\nscenario in the formation of a selected group of red massive galaxies. These\ngalaxies present a log-normal r-band luminosity function, which might arise\nfrom multiplicative processes involved in this scenario. 2) Connection between\nthe onset of AGN activity and star formation quenching, which appears in green\ngalaxies when transitioning from blue to red populations. (Full abstract in\ndownloadable version)\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2012 00:32:15 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Taghizadeh-Popp", "M.", ""], ["Heinis", "S.", ""], ["Szalay", "A. S.", ""]]}, {"id": "1207.0268", "submitter": "Shivani Agarwal", "authors": "Shivani Agarwal", "title": "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses", "comments": "20 pages", "journal-ref": "Journal of Machine Learning Research, 15:1653-1674, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of bipartite ranking, where instances are labeled positive or\nnegative and the goal is to learn a scoring function that minimizes the\nprobability of mis-ranking a pair of positive and negative instances (or\nequivalently, that maximizes the area under the ROC curve), has been widely\nstudied in recent years. A dominant theoretical and algorithmic framework for\nthe problem has been to reduce bipartite ranking to pairwise classification; in\nparticular, it is well known that the bipartite ranking regret can be\nformulated as a pairwise classification regret, which in turn can be upper\nbounded using usual regret bounds for classification problems. Recently,\nKotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of\nthe regret associated with balanced versions of the standard (non-pairwise)\nlogistic and exponential losses. In this paper, we show that such\n(non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in\nterms of a broad class of proper (composite) losses that we term as strongly\nproper. Our proof technique is much simpler than that of Kotlowski et al.\n(2011), and relies on properties of proper (composite) losses as elucidated\nrecently by Reid and Williamson (2010, 2011) and others. Our result yields\nexplicit surrogate bounds (with no hidden balancing terms) in terms of a\nvariety of strongly proper losses, including for example logistic, exponential,\nsquared and squared hinge losses as special cases. We also obtain tighter\nsurrogate bounds under certain low-noise conditions via a recent result of\nClemencon and Robbiano (2011).\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 02:57:30 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Agarwal", "Shivani", ""]]}, {"id": "1207.0577", "submitter": "Ji Liu", "authors": "Ji Liu and Stephen J. Wright", "title": "Robust Dequantized Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the reconstruction problem in compressed sensing in which the\nobservations are recorded in a finite number of bits. They may thus contain\nquantization errors (from being rounded to the nearest representable value) and\nsaturation errors (from being outside the range of representable values). Our\nformulation has an objective of weighted $\\ell_2$-$\\ell_1$ type, along with\nconstraints that account explicitly for quantization and saturation errors, and\nis solved with an augmented Lagrangian method. We prove a consistency result\nfor the recovered solution, stronger than those that have appeared to date in\nthe literature, showing in particular that asymptotic consistency can be\nobtained without oversampling. We present extensive computational comparisons\nwith formulations proposed previously, and variants thereof.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 06:07:13 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 17:19:31 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Liu", "Ji", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1207.0704", "submitter": "Alejandro Frery", "authors": "Leonardo Torres, Tamer Cavalcante and Alejandro C. Frery", "title": "Speckle Reduction using Stochastic Distances", "comments": "Accepted for publication on the proceedings of the 17th Iberoamerican\n  Congress on Patter Recognition (CIARP), to be published in the Lecture Notes\n  in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for filter design based on stochastic\ndistances and tests between distributions. A window is defined around each\npixel, samples are compared and only those which pass a goodness-of-fit test\nare used to compute the filtered value. The technique is applied to intensity\nSynthetic Aperture Radar (SAR) data, using the Gamma model with varying number\nof looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyama\nwindows are used to define the samples. The proposal is compared with the Lee's\nfilter which is considered a standard, using a protocol based on simulation.\nAmong the criteria used to quantify the quality of filters, we employ the\nequivalent number of looks (related to the signal-to-noise ratio), line\ncontrast, and edge preservation. Moreover, we also assessed the filters by the\nUniversal Image Quality Index and the Pearson's correlation between edges.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 14:57:44 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Torres", "Leonardo", ""], ["Cavalcante", "Tamer", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.0757", "submitter": "Alejandro Frery", "authors": "Eliana S. de Almeida, Antonio Carlos de Medeiros, Osvaldo A. Rosso and\n  Alejandro C. Frery", "title": "Generalized Statistical Complexity of SAR Imagery", "comments": "Article accepted for publication in the proceedings of the 17\n  Iberoamerican Conference on Pattern Recognition (CIARP), to be published in\n  the Lecture Notes in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generalized Statistical Complexity Measure (SCM) was proposed by Rosso\net al in 2010. It is a functional that captures the notions of order/disorder\nand of distance to an equilibrium distribution. The former is computed by a\nmeasure of entropy, while the latter depends on the definition of a stochastic\ndivergence. When the scene is illuminated by coherent radiation, image data is\ncorrupted by speckle noise, as is the case of ultrasound-B, sonar, laser and\nSynthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats,\nthis noise is multiplicative and non-Gaussian requiring, thus, specialized\ntechniques for image processing and understanding. One of the most successful\nfamily of models for describing these images is the Multiplicative Model which\nleads, among other probability distributions, to the G0 law. This distribution\nhas been validated in the literature as an expressive and tractable model,\ndeserving the \"universal\" denomination for its ability to describe most types\nof targets. In order to compute the statistical complexity of a site in an\nimage corrupted by speckle noise, we assume that the equilibrium distribution\nis that of fully developed speckle, namely the Gamma law in intensity format,\nwhich appears in areas with little or no texture. We use the Shannon entropy\nalong with the Hellinger distance to measure the statistical complexity of\nintensity SAR images, and we show that it is an expressive feature capable of\nidentifying many types of targets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 17:25:18 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["de Almeida", "Eliana S.", ""], ["de Medeiros", "Antonio Carlos", ""], ["Rosso", "Osvaldo A.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.0771", "submitter": "Alejandro Frery", "authors": "Leonardo Torres, Antonio C. Medeiros and Alejandro C. Frery", "title": "Polarimetric SAR Image Smoothing with Stochastic Distances", "comments": "Accepted for publication in the proceedings of the 17th Iberoamerican\n  Conference on Pattern Recognition, to be published in the Lecture Notes in\n  Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as an\nimportant source of information in remote sensing applications. The most\ncomplete format this type of imaging produces consists of complex-valued\nHermitian matrices in every image coordinate and, as such, their visualization\nis challenging. They also suffer from speckle noise which reduces the\nsignal-to-noise ratio. Smoothing techniques have been proposed in the\nliterature aiming at preserving different features and, analogously,\nprojections from the cone of Hermitian positive matrices to different color\nrepresentation spaces are used for enhancing certain characteristics. In this\nwork we propose the use of stochastic distances between models that describe\nthis type of data in a Nagao-Matsuyama-type of smoothing technique. The\nresulting images are shown to present good visualization properties (noise\nreduction with preservation of fine details) in all the considered\nvisualization spaces.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 18:11:46 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Torres", "Leonardo", ""], ["Medeiros", "Antonio C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.0833", "submitter": "Fr\\'ed\\'eric Blanchard", "authors": "Fr\\'ed\\'eric Blanchard and Michel Herbin", "title": "Relational Data Mining Through Extraction of Representative Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing interest on Network Analysis, Relational Data Mining is\nbecoming an emphasized domain of Data Mining. This paper addresses the problem\nof extracting representative elements from a relational dataset. After defining\nthe notion of degree of representativeness, computed using the Borda\naggregation procedure, we present the extraction of exemplars which are the\nrepresentative elements of the dataset. We use these concepts to build a\nnetwork on the dataset. We expose the main properties of these notions and we\npropose two typical applications of our framework. The first application\nconsists in resuming and structuring a set of binary images and the second in\nmining co-authoring relation in a research team.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 20:48:36 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Blanchard", "Fr\u00e9d\u00e9ric", ""], ["Herbin", "Michel", ""]]}, {"id": "1207.1019", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (LIF), Amaury Habrard (LAHC), St\\'ephane Ayache (LIF)", "title": "PAC-Bayesian Majority Vote for Late Classifier Fusion", "comments": "7 pages, Research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of attention has been devoted to multimedia indexing over the past few\nyears. In the literature, we often consider two kinds of fusion schemes: The\nearly fusion and the late fusion. In this paper we focus on late classifier\nfusion, where one combines the scores of each modality at the decision level.\nTo tackle this problem, we investigate a recent and elegant well-founded\nquadratic program named MinCq coming from the Machine Learning PAC-Bayes\ntheory. MinCq looks for the weighted combination, over a set of real-valued\nfunctions seen as voters, leading to the lowest misclassification rate, while\nmaking use of the voters' diversity. We provide evidence that this method is\nnaturally adapted to late fusion procedure. We propose an extension of MinCq by\nadding an order- preserving pairwise loss for ranking, helping to improve Mean\nAveraged Precision measure. We confirm the good behavior of the MinCq-based\nfusion approaches with experiments on a real image benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 15:09:05 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Morvant", "Emilie", "", "LIF"], ["Habrard", "Amaury", "", "LAHC"], ["Ayache", "St\u00e9phane", "", "LIF"]]}, {"id": "1207.1115", "submitter": "Jameson Toole", "authors": "Jameson L. Toole, Michael Ulm, Dietmar Bauer, Marta C. Gonzalez", "title": "Inferring land use from mobile phone activity", "comments": "To be presented at ACM UrbComp2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spatiotemporal distribution of people within a city is\ncrucial to many planning applications. Obtaining data to create required\nknowledge, currently involves costly survey methods. At the same time\nubiquitous mobile sensors from personal GPS devices to mobile phones are\ncollecting massive amounts of data on urban systems. The locations,\ncommunications, and activities of millions of people are recorded and stored by\nnew information technologies. This work utilizes novel dynamic data, generated\nby mobile phone users, to measure spatiotemporal changes in population. In the\nprocess, we identify the relationship between land use and dynamic population\nover the course of a typical week. A machine learning classification algorithm\nis used to identify clusters of locations with similar zoned uses and mobile\nphone activity patterns. It is shown that the mobile phone data is capable of\ndelivering useful information on actual land use that supplements zoning\nregulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 16:20:08 GMT"}], "update_date": "2012-07-06", "authors_parsed": [["Toole", "Jameson L.", ""], ["Ulm", "Michael", ""], ["Bauer", "Dietmar", ""], ["Gonzalez", "Marta C.", ""]]}, {"id": "1207.1119", "submitter": "Fatma Kilinc Karzan", "authors": "Anatoli Juditsky, Fatma Kilinc Karzan, Arkadi Nemirovski", "title": "On unified view of nullspace-type conditions for recoveries associated\n  with general sparsity structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general notion of \"sparsity structure\" and associated recoveries\nof a sparse signal from its linear image of reduced dimension possibly\ncorrupted with noise. Our approach allows for unified treatment of (a) the\n\"usual sparsity\" and \"usual $\\ell_1$ recovery,\" (b) block-sparsity with\npossibly overlapping blocks and associated block-$\\ell_1$ recovery, and (c)\nlow-rank-oriented recovery by nuclear norm minimization. The proposed recovery\nroutines are natural extensions of the usual $\\ell_1$ minimization used in\nCompressed Sensing. Specifically we present nullspace-type sufficient\nconditions for the recovery to be precise on sparse signals in the noiseless\ncase. Then we derive error bounds for imperfect (nearly sparse signal, presence\nof observation noise, etc.) recovery under these conditions. In all of these\ncases, we present efficiently verifiable sufficient conditions for the validity\nof the associated nullspace properties.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 20:34:12 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Karzan", "Fatma Kilinc", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1207.1358", "submitter": "Susan Shortreed", "authors": "Susan Shortreed, Marina Meila", "title": "Unsupervised spectral learning", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-534-541", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spectral clustering and spectral image segmentation, the data is partioned\nstarting from a given matrix of pairwise similarities S. the matrix S is\nconstructed by hand, or learned on a separate training set. In this paper we\nshow how to achieve spectral clustering in unsupervised mode. Our algorithm\nstarts with a set of observed pairwise features, which are possible components\nof an unknown, parametric similarity function. This function is learned\niteratively, at the same time as the clustering of the data. The algorithm\nshows promosing results on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 12:14:50 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Shortreed", "Susan", ""], ["Meila", "Marina", ""]]}, {"id": "1207.1364", "submitter": "Eric E. Altendorf", "authors": "Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich", "title": "Learning from Sparse Data by Exploiting Monotonicity Constraints", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-18-26", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training data is sparse, more domain knowledge must be incorporated into\nthe learning algorithm in order to reduce the effective size of the hypothesis\nspace. This paper builds on previous work in which knowledge about qualitative\nmonotonicities was formally represented and incorporated into learning\nalgorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm).\nWe show how to interpret knowledge of qualitative influences, and in particular\nof monotonicities, as constraints on probability distributions, and to\nincorporate this knowledge into Bayesian network learning algorithms. We show\nthat this yields improved accuracy, particularly with very small training sets\n(e.g. less than 10 examples).\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:03:10 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Altendorf", "Eric E.", ""], ["Restificar", "Angelo C.", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1207.1366", "submitter": "Pieter Abbeel", "authors": "Pieter Abbeel, Daphne Koller, Andrew Y. Ng", "title": "Learning Factor Graphs in Polynomial Time & Sample Complexity", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-1-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study computational and sample complexity of parameter and structure\nlearning in graphical models. Our main result shows that the class of factor\ngraphs with bounded factor size and bounded connectivity can be learned in\npolynomial time and polynomial number of samples, assuming that the data is\ngenerated by a network in this class. This result covers both parameter\nestimation for a known network structure and structure learning. It implies as\na corollary that we can learn factor graphs for both Bayesian networks and\nMarkov networks of bounded degree, in polynomial time and sample complexity.\nUnlike maximum likelihood estimation, our method does not require inference in\nthe underlying network, and so applies to networks where inference is\nintractable. We also show that the error of our learned model degrades\ngracefully when the generating distribution is not a member of the target class\nof networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:03:31 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Abbeel", "Pieter", ""], ["Koller", "Daphne", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1207.1367", "submitter": "Cassio Polpo de Campos", "authors": "Cassio Polpo de Campos, Fabio Gagliardi Cozman", "title": "Belief Updating and Learning in Semi-Qualitative Probabilistic Networks", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-153-160", "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores semi-qualitative probabilistic networks (SQPNs) that\ncombine numeric and qualitative information. We first show that exact\ninferences with SQPNs are NPPP-Complete. We then show that existing qualitative\nrelations in SQPNs (plus probabilistic logic and imprecise assessments) can be\ndealt effectively through multilinear programming. We then discuss learning: we\nconsider a maximum likelihood method that generates point estimates given a\nSQPN and empirical data, and we describe a Bayesian-minded method that employs\nthe Imprecise Dirichlet Model to generate set-valued estimates.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:05:05 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["de Campos", "Cassio Polpo", ""], ["Cozman", "Fabio Gagliardi", ""]]}, {"id": "1207.1379", "submitter": "Shen-Shyang Ho", "authors": "Shen-Shyang Ho, Harry Wechsler", "title": "On the Detection of Concept Changes in Time-Varying Data Stream by\n  Testing Exchangeability", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-267-274", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A martingale framework for concept change detection based on testing data\nexchangeability was recently proposed (Ho, 2005). In this paper, we describe\nthe proposed change-detection test based on the Doob's Maximal Inequality and\nshow that it is an approximation of the sequential probability ratio test\n(SPRT). The relationship between the threshold value used in the proposed test\nand its size and power is deduced from the approximation. The mean delay time\nbefore a change is detected is estimated using the average sample number of a\nSPRT. The performance of the test using various threshold values is examined on\nfive different data stream scenarios simulated using two synthetic data sets.\nFinally, experimental results show that the test is effective in detecting\nchanges in time-varying data streams simulated using three benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:10:01 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Ho", "Shen-Shyang", ""], ["Wechsler", "Harry", ""]]}, {"id": "1207.1380", "submitter": "Markus Harva", "authors": "Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha\n  Karhunen", "title": "Bayes Blocks: An Implementation of the Variational Bayesian Building\n  Blocks Framework", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-259-266", "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A software library for constructing and learning probabilistic models is\npresented. The library offers a set of building blocks from which a large\nvariety of static and dynamic models can be built. These include hierarchical\nmodels for variances of other variables and many nonlinear models. The\nunderlying variational Bayesian machinery, providing for fast and robust\nestimation but being mathematically rather involved, is almost completely\nhidden from the user thus making it very easy to use the library. The building\nblocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables\nand computational nodes which can be combined rather freely.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:10:18 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Harva", "Markus", ""], ["Raiko", "Tapani", ""], ["Honkela", "Antti", ""], ["Valpola", "Harri", ""], ["Karhunen", "Juha", ""]]}, {"id": "1207.1382", "submitter": "Yuhong Guo", "authors": "Yuhong Guo, Dana Wilkinson, Dale Schuurmans", "title": "Maximum Margin Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-233-242", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning Bayesian network classifiers that\nmaximize the marginover a set of classification variables. We find that this\nproblem is harder for Bayesian networks than for undirected graphical models\nlike maximum margin Markov networks. The main difficulty is that the parameters\nin a Bayesian network must satisfy additional normalization constraints that an\nundirected graphical model need not respect. These additional constraints\ncomplicate the optimization task. Nevertheless, we derive an effective training\nalgorithm that solves the maximum margin training problem for a range of\nBayesian network topologies, and converges to an approximate solution for\narbitrary network topologies. Experimental results show that the method can\ndemonstrate improved generalization performance over Markov networks when the\ndirected graphical structure encodes relevant knowledge. In practice, the\ntraining technique allows one to combine prior knowledge expressed as a\ndirected (causal) model with state of the art discriminative learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:12:02 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Guo", "Yuhong", ""], ["Wilkinson", "Dana", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1207.1387", "submitter": "Ad Feelders", "authors": "Ad Feelders, Linda C. van der Gaag", "title": "Learning Bayesian Network Parameters with Prior Knowledge about\n  Context-Specific Qualitative Influences", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-193-200", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning the parameters of a Bayesian network with\nprior knowledge about the signs of influences between variables. Our method\naccommodates not just the standard signs, but provides for context-specific\nsigns as well. We show how the various signs translate into order constraints\non the network parameters and how isotonic regression can be used to compute\norder-constrained estimates from the available data. Our experimental results\nshow that taking prior knowledge about the signs of influences into account\nleads to an improved fit of the true distribution, especially when only a small\nsample of data is available. Moreover, the computed estimates are guaranteed to\nbe consistent with the specified signs, thereby resulting in a network that is\nmore likely to be accepted by experts in its domain of application.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:13:39 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Feelders", "Ad", ""], ["van der Gaag", "Linda C.", ""]]}, {"id": "1207.1393", "submitter": "Hendrik Kuck", "authors": "Hendrik Kuck, Nando de Freitas", "title": "Learning about individuals from group statistics", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-332-339", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new problem formulation which is similar to, but more\ninformative than, the binary multiple-instance learning problem. In this\nsetting, we are given groups of instances (described by feature vectors) along\nwith estimates of the fraction of positively-labeled instances per group. The\ntask is to learn an instance level classifier from this information. That is,\nwe are trying to estimate the unknown binary labels of individuals from\nknowledge of group statistics. We propose a principled probabilistic model to\nsolve this problem that accounts for uncertainty in the parameters and in the\nunknown individual labels. This model is trained with an efficient MCMC\nalgorithm. Its performance is demonstrated on both synthetic and real-world\ndata arising in general object recognition.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:16:02 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Kuck", "Hendrik", ""], ["de Freitas", "Nando", ""]]}, {"id": "1207.1396", "submitter": "Mike Klaas", "authors": "Mike Klaas, Nando de Freitas, Arnaud Doucet", "title": "Toward Practical N2 Monte Carlo: the Marginal Particle Filter", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-308-315", "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo techniques are useful for state estimation in\nnon-linear, non-Gaussian dynamic models. These methods allow us to approximate\nthe joint posterior distribution using sequential importance sampling. In this\nframework, the dimension of the target distribution grows with each time step,\nthus it is necessary to introduce some resampling steps to ensure that the\nestimates provided by the algorithm have a reasonable variance. In many\napplications, we are only interested in the marginal filtering distribution\nwhich is defined on a space of fixed dimension. We present a Sequential Monte\nCarlo algorithm called the Marginal Particle Filter which operates directly on\nthe marginal distribution, hence avoiding having to perform importance sampling\non a space of growing dimension. Using this idea, we also derive an improved\nversion of the auxiliary particle filter. We show theoretic and empirical\nresults which demonstrate a reduction in variance over conventional particle\nfiltering, and present techniques for reducing the cost of the marginal\nparticle filter with N particles from O(N2) to O(N logN).\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:17:01 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Klaas", "Mike", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1207.1403", "submitter": "Alexandru Niculescu-Mizil", "authors": "Alexandru Niculescu-Mizil, Richard A. Caruana", "title": "Obtaining Calibrated Probabilities from Boosting", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-413-420", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosted decision trees typically yield good accuracy, precision, and ROC\narea. However, because the outputs from boosting are not well calibrated\nposterior probabilities, boosting yields poor squared error and cross-entropy.\nWe empirically demonstrate why AdaBoost predicts distorted probabilities and\nexamine three calibration methods for correcting this distortion: Platt\nScaling, Isotonic Regression, and Logistic Correction. We also experiment with\nboosting using log-loss instead of the usual exponential loss. Experiments show\nthat Logistic Correction and boosting with log-loss work well when boosting\nweak models such as decision stumps, but yield poor performance when boosting\nmore complex models such as full decision trees. Platt Scaling and Isotonic\nRegression, however, significantly improve the probabilities predicted by\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:19:55 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Niculescu-Mizil", "Alexandru", ""], ["Caruana", "Richard A.", ""]]}, {"id": "1207.1404", "submitter": "Mukund Narasimhan", "authors": "Mukund Narasimhan, Jeff A. Bilmes", "title": "A submodular-supermodular procedure with applications to discriminative\n  structure learning", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-404-412", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for minimizing the difference between\ntwo submodular functions using a variational framework which is based on (an\nextension of) the concave-convex procedure [17]. Because several commonly used\nmetrics in machine learning, like mutual information and conditional mutual\ninformation, are submodular, the problem of minimizing the difference of two\nsubmodular problems arises naturally in many machine learning applications. Two\nsuch applications are learning discriminatively structured graphical models and\nfeature selection under computational complexity constraints. A commonly used\nmetric for measuring discriminative capacity is the EAR measure which is the\ndifference between two conditional mutual information terms. Feature selection\ntaking complexity considerations into account also fall into this framework\nbecause both the information that a set of features provide and the cost of\ncomputing and using the features can be modeled as submodular functions. This\nproblem is NP-hard, and we give a polynomial time heuristic for it. We also\npresent results on synthetic data to show that classifiers based on\ndiscriminative graphical models using this algorithm can significantly\noutperform classifiers based on generative graphical models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:20:12 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Narasimhan", "Mukund", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1207.1409", "submitter": "Charles Sutton", "authors": "Charles Sutton, Andrew McCallum", "title": "Piecewise Training for Undirected Models", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-568-575", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many large undirected models that arise in real-world applications, exact\nmaximumlikelihood training is intractable, because it requires computing\nmarginal distributions of the model. Conditional training is even more\ndifficult, because the partition function depends not only on the parameters,\nbut also on the observed input, requiring repeated inference over each training\nexample. An appealing idea for such models is to independently train a local\nundirected classifier over each clique, afterwards combining the learned\nweights into a single global model. In this paper, we show that this piecewise\nmethod can be justified as minimizing a new family of upper bounds on the log\npartition function. On three natural-language data sets, piecewise training is\nmore accurate than pseudolikelihood, and often performs comparably to global\ntraining using belief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:22:14 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Sutton", "Charles", ""], ["McCallum", "Andrew", ""]]}, {"id": "1207.1413", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu, Aapo Hyvarinen, Yutaka Kano, Patrik O. Hoyer", "title": "Discovery of non-gaussian linear causal models using ICA", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-525-533", "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several methods have been proposed for the discovery of\ncausal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).\nSuch methods make various assumptions on the data generating process to\nfacilitate its identification from purely observational data. Continuing this\nline of research, we show how to discover the complete causal structure of\ncontinuous-valued data, under the assumptions that (a) the data generating\nprocess is linear, (b) there are no unobserved confounders, and (c) disturbance\nvariables have non-gaussian distributions of non-zero variances. The solution\nrelies on the use of the statistical method known as independent component\nanalysis (ICA), and does not require any pre-specified time-ordering of the\nvariables. We provide a complete Matlab package for performing this LiNGAM\nanalysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the\neffectiveness of the method using artificially generated data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:23:35 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Kano", "Yutaka", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1207.1414", "submitter": "Eerika Savia", "authors": "Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski", "title": "Two-Way Latent Grouping Model for User Preference Prediction", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-518-524", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent grouping model for predicting the relevance of a\nnew document to a user. The model assumes a latent group structure for both\nusers and documents. We compared the model against a state-of-the-art method,\nthe User Rating Profile model, where only users have a latent group structure.\nWe estimate both models by Gibbs sampling. The new method predicts relevance\nmore accurately for new documents that have few known ratings. The reason is\nthat generalization over documents then becomes necessary and hence the twoway\ngrouping is profitable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:23:52 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Savia", "Eerika", ""], ["Puolamaki", "Kai", ""], ["Sinkkonen", "Janne", ""], ["Kaski", "Samuel", ""]]}, {"id": "1207.1417", "submitter": "Michal Rosen-Zvi", "authors": "Michal Rosen-Zvi, Michael I. Jordan, Alan Yuille", "title": "The DLR Hierarchy of Approximate Inference", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-493-500", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchy for approximate inference based on the Dobrushin,\nLanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms,\nsuch as belief propagation, and also motivates novel algorithms such as\nfactorized neighbors (FN) algorithms and variants of mean field (MF)\nalgorithms. In particular, we show that extrema of the Bethe free energy\ncorrespond to approximate solutions of the DLR equations. In addition, we\ndemonstrate a close connection between these approximate algorithms and Gibbs\nsampling. Finally, we compare and contrast various of the algorithms in the DLR\nhierarchy on spin-glass problems. The experiments show that algorithms higher\nup in the hierarchy give more accurate results when they converge but tend to\nbe less stable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:25:12 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rosen-Zvi", "Michal", ""], ["Jordan", "Michael I.", ""], ["Yuille", "Alan", ""]]}, {"id": "1207.1421", "submitter": "Huizhen Yu", "authors": "Huizhen Yu", "title": "A Function Approximation Approach to Estimation of Policy Gradient for\n  POMDP with Structured Policies", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-642-649", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the policy gradient in partially observable\nMarkov decision processes (POMDP) with a special class of structured policies\nthat are finite-state controllers. We show that the gradient estimation can be\ndone in the Actor-Critic framework, by making the critic compute a \"value\"\nfunction that does not depend on the states of POMDP. This function is the\nconditional mean of the true value function that depends on the states. We show\nthat the critic can be implemented using temporal difference (TD) methods with\nlinear function approximations, and the analytical results on TD and\nActor-Critic can be transfered to this case. Although Actor-Critic algorithms\nhave been used extensively in Markov decision processes (MDP), up to now they\nhave not been proposed for POMDP as an alternative to the earlier proposal\nGPOMDP algorithm, an actor-only method. Furthermore, we show that the same idea\napplies to semi-Markov problems with a subset of finite-state controllers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:28:10 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Yu", "Huizhen", ""]]}, {"id": "1207.1423", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Rong Yan, Alexander G. Hauptmann", "title": "Mining Associated Text and Images with Dual-Wing Harmoniums", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-633-641", "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-wing harmonium model for mining multimedia data that\nextends and improves on earlier models based on two-layer random fields, which\ncapture bidirectional dependencies between hidden topic aspects and observed\ninputs. This model can be viewed as an undirected counterpart of the two-layer\ndirected models such as LDA for similar tasks, but bears significant difference\nin inference/learning cost tradeoffs, latent topic representations, and topic\nmixing mechanisms. In particular, our model facilitates efficient inference and\nrobust topic mixing, and potentially provides high flexibilities in modeling\nthe latent topic spaces. A contrastive divergence and a variational algorithm\nare derived for learning. We specialized our model to a dual-wing harmonium for\ncaptioned images, incorporating a multivariate Poisson for word-counts and a\nmultivariate Gaussian for color histogram. We present empirical results on the\napplications of this model to classification, retrieval and image annotation on\nnews video collections, and we report an extensive comparison with various\nextant models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:28:40 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Yan", "Rong", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1207.1429", "submitter": "Marc Teyssier", "authors": "Marc Teyssier, Daphne Koller", "title": "Ordering-Based Search: A Simple and Effective Algorithm for Learning\n  Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-584-590", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the basic tasks for Bayesian networks (BNs) is that of learning a\nnetwork structure from data. The BN-learning problem is NP-hard, so the\nstandard solution is heuristic search. Many approaches have been proposed for\nthis task, but only a very small number outperform the baseline of greedy\nhill-climbing with tabu lists; moreover, many of the proposed algorithms are\nquite complex and hard to implement. In this paper, we propose a very simple\nand easy-to-implement method for addressing this task. Our approach is based on\nthe well-known fact that the best network (of bounded in-degree) consistent\nwith a given node ordering can be found very efficiently. We therefore propose\na search not over the space of structures, but over the space of orderings,\nselecting for each ordering the best network consistent with it. This search\nspace is much smaller, makes more global search steps, has a lower branching\nfactor, and avoids costly acyclicity checks. We present results for this\nalgorithm on both synthetic and real data sets, evaluating both the score of\nthe network found and in the running time. We show that ordering-based search\noutperforms the standard baseline, and is competitive with recent algorithms\nthat are much harder to implement.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:31:04 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Teyssier", "Marc", ""], ["Koller", "Daphne", ""]]}, {"id": "1207.1687", "submitter": "Arash Ali Amini", "authors": "Arash Ali Amini, XuanLong Nguyen", "title": "Sequential detection of multiple change points in networks: a graphical\n  model approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic formulation that enables sequential detection of\nmultiple change points in a network setting. We present a class of sequential\ndetection rules for certain functionals of change points (minimum among a\nsubset), and prove their asymptotic optimality properties in terms of expected\ndetection delay time. Drawing from graphical model formalism, the sequential\ndetection rules can be implemented by a computationally efficient\nmessage-passing protocol which may scale up linearly in network size and in\nwaiting time. The effectiveness of our inference algorithm is demonstrated by\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 17:07:38 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Amini", "Arash Ali", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1207.1727", "submitter": "Paul McNicholas", "authors": "Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas", "title": "Mixtures of Shifted Asymmetric Laplace Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2013.216", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of shifted asymmetric Laplace distributions is introduced and used\nfor clustering and classification. A variant of the EM algorithm is developed\nfor parameter estimation by exploiting the relationship with the general\ninverse Gaussian distribution. This approach is mathematically elegant and\nrelatively computationally straightforward. Our novel mixture modelling\napproach is demonstrated on both simulated and real data to illustrate\nclustering and classification applications. In these analyses, our mixture of\nshifted asymmetric Laplace distributions performs favourably when compared to\nthe popular Gaussian approach. This work, which marks an important step in the\nnon-Gaussian model-based clustering and classification direction, concludes\nwith discussion as well as suggestions for future work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 20:06:48 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 18:40:24 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2012 21:03:10 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Franczak", "Brian C.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1207.1888", "submitter": "David  Biagioni", "authors": "David J. Biagioni and Ryan Elmore and Wesley Jones", "title": "Keeping greed good: sparse regression under design uncertainty with\n  application to biomass characterization", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the classic measurement error regression scenario\nin which our independent, or design, variables are observed with several\nsources of additive noise. We will show that our motivating example's\nreplicated measurements on both the design and dependent variables may be\nleveraged to enhance a sparse regression algorithm. Specifically, we estimate\nthe variance and use it to scale our design variables. We demonstrate the\nefficacy of scaling from several points of view and validate it empirically\nwith a biomass characterization data set using two of the most widely used\nsparse algorithms: least angle regression (LARS) and the Dantzig selector (DS).\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 17:15:59 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Biagioni", "David J.", ""], ["Elmore", "Ryan", ""], ["Jones", "Wesley", ""]]}, {"id": "1207.1915", "submitter": "Alejandro Frery", "authors": "Edwin Gir\\'on, Alejandro C. Frery and Francisco Cribari-Neto", "title": "Nonparametric Edge Detection in Speckled Imagery", "comments": "Accepted for publication in Mathematics and Computers in Simulation", "journal-ref": "Mathematics and Computers in Simulation, vol. 82, pages 2182-2198,\n  2012", "doi": "10.1016/j.matcom.2012.04.013", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of edge detection in Synthetic Aperture Radar imagery.\nIn particular, we propose nonparametric methods for edge detection, and\nnumerically compare them to an alternative method that has been recently\nproposed in the literature. Our results show that some of the proposed methods\ndisplay superior results and are computationally simpler than the existing\nmethod. An application to real (not simulated) data is presented and discussed.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 21:45:43 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Gir\u00f3n", "Edwin", ""], ["Frery", "Alejandro C.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1207.1965", "submitter": "Gilles Stoltz", "authors": "Marie Devaine (DMA), Pierre Gaillard (DMA, INRIA Paris -\n  Rocquencourt), Yannig Goude, Gilles Stoltz (DMA, INRIA Paris - Rocquencourt,\n  GREGH)", "title": "Forecasting electricity consumption by aggregating specialized experts", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of sequential prediction of arbitrary sequences based\non specialized experts. We first provide a review of the relevant literature\nand present two theoretical contributions: a general analysis of the specialist\naggregation rule of Freund et al. (1997) and an adaptation of fixed-share rules\nof Herbster and Warmuth (1998) in this setting. We then apply these rules to\nthe sequential short-term (one-day-ahead) forecasting of electricity\nconsumption; to do so, we consider two data sets, a Slovakian one and a French\none, respectively concerned with hourly and half-hourly predictions. We follow\na general methodology to perform the stated empirical studies and detail in\nparticular tuning issues of the learning parameters. The introduced aggregation\nrules demonstrate an improved accuracy on the data sets at hand; the\nimprovements lie in a reduced mean squared error but also in a more robust\nbehavior with respect to large occasional errors.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 06:47:39 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Devaine", "Marie", "", "DMA"], ["Gaillard", "Pierre", "", "DMA, INRIA Paris -\n  Rocquencourt"], ["Goude", "Yannig", "", "DMA, INRIA Paris - Rocquencourt,\n  GREGH"], ["Stoltz", "Gilles", "", "DMA, INRIA Paris - Rocquencourt,\n  GREGH"]]}, {"id": "1207.1977", "submitter": "Doris Entner", "authors": "Doris Entner, Patrik O. Hoyer", "title": "Estimating a Causal Order among Groups of Variables in Linear Models", "comments": "To appear at the International Conference on Artificial Neural\n  Networks 2012 (proceedings to be published in LNCS, Springer); To be\n  presented at the UAI Workshop on Causal Structure Learning 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has recently devoted much attention to the\nproblem of inferring causal relationships from statistical data. Most of this\nwork has focused on uncovering connections among scalar random variables. We\ngeneralize existing methods to apply to collections of multi-dimensional random\nvectors, focusing on techniques applicable to linear models. The performance of\nthe resulting algorithms is evaluated and compared in simulations, which show\nthat our methods can, in many cases, provide useful information on causal\nrelationships even for relatively small sample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 08:05:44 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Entner", "Doris", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1207.2328", "submitter": "Pan Zhang", "authors": "Pan Zhang and Florent Krzakala and J\\\"org Reichardt and Lenka\n  Zdeborov\\'a", "title": "Comparative Study for Inference of Hidden Classes in Stochastic Block\n  Models", "comments": "8 pages, 5 figures AIGM12", "journal-ref": "J. Stat. Mech. (2012) P12021", "doi": "10.1088/1742-5468/2012/12/P12021", "report-no": null, "categories": "cs.LG cond-mat.stat-mech physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of hidden classes in stochastic block model is a classical problem\nwith important applications. Most commonly used methods for this problem\ninvolve na\\\"{\\i}ve mean field approaches or heuristic spectral methods.\nRecently, belief propagation was proposed for this problem. In this\ncontribution we perform a comparative study between the three methods on\nsynthetically created networks. We show that belief propagation shows much\nbetter performance when compared to na\\\"{\\i}ve mean field and spectral\napproaches. This applies to accuracy, computational efficiency and the tendency\nto overfit the data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 12:22:21 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2012 09:41:10 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Zhang", "Pan", ""], ["Krzakala", "Florent", ""], ["Reichardt", "J\u00f6rg", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1207.2340", "submitter": "Arash A. Amini", "authors": "Arash A. Amini, Aiyou Chen, Peter J. Bickel, Elizaveta Levina", "title": "Pseudo-likelihood methods for community detection in large sparse\n  networks", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1138 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 2097-2122", "doi": "10.1214/13-AOS1138", "report-no": "IMS-AOS-AOS1138", "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms have been proposed for fitting network models with\ncommunities, but most of them do not scale well to large networks, and often\nfail on sparse networks. Here we propose a new fast pseudo-likelihood method\nfor fitting the stochastic block model for networks, as well as a variant that\nallows for an arbitrary degree distribution by conditioning on degrees. We show\nthat the algorithms perform well under a range of settings, including on very\nsparse networks, and illustrate on the example of a network of political blogs.\nWe also propose spectral clustering with perturbations, a method of independent\ninterest, which works well on sparse networks where regular spectral clustering\nfails, and use it to provide an initial value for pseudo-likelihood. We prove\nthat pseudo-likelihood provides consistent estimates of the communities under a\nmild condition on the starting value, for the case of a block model with two\ncommunities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 13:28:32 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 18:52:23 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2013 15:49:54 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Amini", "Arash A.", ""], ["Chen", "Aiyou", ""], ["Bickel", "Peter J.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1207.2422", "submitter": "David Wipf", "authors": "David Wipf, Yi Wu", "title": "Dual-Space Analysis of the Sparse Linear Model", "comments": "9 pages, 2 figures, submission to NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear (or generalized linear) models combine a standard likelihood\nfunction with a sparse prior on the unknown coefficients. These priors can\nconveniently be expressed as a maximization over zero-mean Gaussians with\ndifferent variance hyperparameters. Standard MAP estimation (Type I) involves\nmaximizing over both the hyperparameters and coefficients, while an empirical\nBayesian alternative (Type II) first marginalizes the coefficients and then\nmaximizes over the hyperparameters, leading to a tractable posterior\napproximation. The underlying cost functions can be related via a dual-space\nframework from Wipf et al. (2011), which allows both the Type I or Type II\nobjectives to be expressed in either coefficient or hyperparmeter space. This\nperspective is useful because some analyses or extensions are more conducive to\ndevelopment in one space or the other. Herein we consider the estimation of a\ntrade-off parameter balancing sparsity and data fit. As this parameter is\neffectively a variance, natural estimators exist by assessing the problem in\nhyperparameter (variance) space, transitioning natural ideas from Type II to\nsolve what is much less intuitive for Type I. In contrast, for analyses of\nupdate rules and sparsity properties of local and global solutions, as well as\nextensions to more general likelihood models, we can leverage coefficient-space\ntechniques developed for Type I and apply them to Type II. For example, this\nallows us to prove that Type II-inspired techniques can be successful\nrecovering sparse coefficients when unfavorable restricted isometry properties\n(RIP) lead to failure of popular L1 reconstructions. It also facilitates the\nanalysis of Type II when non-Gaussian likelihood models lead to intractable\nintegrations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 17:38:18 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Wipf", "David", ""], ["Wu", "Yi", ""]]}, {"id": "1207.2440", "submitter": "David Wipf", "authors": "David Wipf", "title": "Non-Convex Rank Minimization via an Empirical Bayesian Approach", "comments": "10 pages, 6 figures, UAI 2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications that require matrix solutions of minimal rank, the\nunderlying cost function is non-convex leading to an intractable, NP-hard\noptimization problem. Consequently, the convex nuclear norm is frequently used\nas a surrogate penalty term for matrix rank. The problem is that in many\npractical scenarios there is no longer any guarantee that we can correctly\nestimate generative low-rank matrices of interest, theoretical special cases\nnotwithstanding. Consequently, this paper proposes an alternative empirical\nBayesian procedure build upon a variational approximation that, unlike the\nnuclear norm, retains the same globally minimizing point estimate as the rank\nfunction under many useful constraints. However, locally minimizing solutions\nare largely smoothed away via marginalization, allowing the algorithm to\nsucceed when standard convex relaxations completely fail. While the proposed\nmethodology is generally applicable to a wide range of low-rank applications,\nwe focus our attention on the robust principal component analysis problem\n(RPCA), which involves estimating an unknown low-rank matrix with unknown\nsparse corruptions. Theoretical and empirical evidence are presented to show\nthat our method is potentially superior to related MAP-based approaches, for\nwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,\n2011) can be viewed as a special case.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 18:41:04 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Wipf", "David", ""]]}, {"id": "1207.2491", "submitter": "Byron Boots", "authors": "Byron Boots and Geoffrey J. Gordon", "title": "A Spectral Learning Approach to Range-Only SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel spectral learning algorithm for simultaneous localization\nand mapping (SLAM) from range data with known correspondences. This algorithm\nis an instance of a general spectral system identification framework, from\nwhich it inherits several desirable properties, including statistical\nconsistency and no local optima. Compared with popular batch optimization or\nmultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral\napproach offers guaranteed low computational requirements and good tracking\nperformance. Compared with popular extended Kalman filter (EKF) or extended\ninformation filter (EIF) approaches, and many MHT ones, our approach does not\nneed to linearize a transition or measurement model; such linearizations can\ncause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly\nfor the highly non-Gaussian posteriors encountered in range-only SLAM. We\nprovide a theoretical analysis of our method, including finite-sample error\nbounds. Finally, we demonstrate on a real-world robotic SLAM problem that our\nalgorithm is not only theoretically justified, but works well in practice: in a\ncomparison of multiple methods, the lowest errors come from a combination of\nour algorithm with batch optimization, but our method alone produces nearly as\ngood a result at far lower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 21:19:33 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Boots", "Byron", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1207.2812", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha", "title": "Near-Optimal Algorithms for Differentially-Private Principal Components", "comments": "37 pages, 8 figures; final version to appear in the Journal of\n  Machine Learning Research, preliminary version was at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is a standard tool for identifying good\nlow-dimensional approximations to data in high dimension. Many data sets of\ninterest contain private or sensitive information about individuals. Algorithms\nwhich operate on such data should be sensitive to the privacy risks in\npublishing their outputs. Differential privacy is a framework for developing\ntradeoffs between privacy and the utility of these outputs. In this paper we\ninvestigate the theory and empirical performance of differentially private\napproximations to PCA and propose a new method which explicitly optimizes the\nutility of the output. We show that the sample complexity of the proposed\nmethod differs from the existing procedure in the scaling with the data\ndimension, and that our method is nearly optimal in terms of this scaling. We\nfurthermore illustrate our results, showing that on real data there is a large\nperformance gap between the existing method and our method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 00:05:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2012 00:29:51 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2013 21:48:35 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Sarwate", "Anand D.", ""], ["Sinha", "Kaushik", ""]]}, {"id": "1207.2940", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Shakir Mohamed", "title": "Expectation Propagation in Gaussian Process Dynamical Systems: Extended\n  Version", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 25 (NIPS), pp.\n  2609-2617, 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich and complex time-series data, such as those generated from engineering\nsystems, financial markets, videos or neural recordings, are now a common\nfeature of modern data analysis. Explaining the phenomena underlying these\ndiverse data sets requires flexible and accurate models. In this paper, we\npromote Gaussian process dynamical systems (GPDS) as a rich model class that is\nappropriate for such analysis. In particular, we present a message passing\nalgorithm for approximate inference in GPDSs based on expectation propagation.\nBy posing inference as a general message passing problem, we iterate\nforward-backward smoothing. Thus, we obtain more accurate posterior\ndistributions over latent structures, resulting in improved predictive\nperformance compared to state-of-the-art GPDS smoothers, which are special\ncases of our general message passing algorithm. Hence, we provide a unifying\napproach within which to contextualize message passing in GPDSs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 12:37:57 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2012 14:45:28 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2012 06:45:30 GMT"}, {"version": "v4", "created": "Fri, 25 Jul 2014 10:25:19 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 13:23:57 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1207.2959", "submitter": "Alejandro Frery", "authors": "Abra\\~ao D. C. Nascimento, Renato J. Cintra and Alejandro C. Frery", "title": "Hypothesis Testing in Speckled Data with Stochastic Distances", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 48, p.\n  373-385, 2010", "doi": "10.1109/TGRS.2009.2025498", "report-no": null, "categories": "stat.ML cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images obtained with coherent illumination, as is the case of sonar,\nultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected by\nspeckle noise which reduces the ability to extract information from the data.\nSpecialized techniques are required to deal with such imagery, which has been\nmodeled by the G0 distribution and under which regions with different degrees\nof roughness and mean brightness can be characterized by two parameters; a\nthird parameter, the number of looks, is related to the overall signal-to-noise\nratio. Assessing distances between samples is an important step in image\nanalysis; they provide grounds of the separability and, therefore, of the\nperformance of classification procedures. This work derives and compares eight\nstochastic distances and assesses the performance of hypothesis tests that\nemploy them and maximum likelihood estimation. We conclude that tests based on\nthe triangular distance have the closest empirical size to the theoretical one,\nwhile those based on the arithmetic-geometric distances have the best power.\nSince the power of tests based on the triangular distance is close to optimum,\nwe conclude that the safest choice is using this distance for hypothesis\ntesting, even when compared with classical distances as Kullback-Leibler and\nBhattacharyya.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 13:45:41 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Nascimento", "Abra\u00e3o D. C.", ""], ["Cintra", "Renato J.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.3012", "submitter": "Aarti Singh", "authors": "Aaditya Ramdas and Aarti Singh", "title": "Optimal rates for first-order stochastic convex optimization under\n  Tsybakov noise condition", "comments": "Accepted for publication at ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of minimizing a convex function $f$ over a convex set\n$S$ given $T$ queries to a stochastic first order oracle. We argue that the\ncomplexity of convex minimization is only determined by the rate of growth of\nthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like\nnoise condition. Specifically, we prove that if $f$ grows at least as fast as\n$\\|x-x^*_{f,S}\\|^\\kappa$ around its minimum, for some $\\kappa > 1$, then the\noptimal rate of learning $f(x^*_{f,S})$ is\n$\\Theta(T^{-\\frac{\\kappa}{2\\kappa-2}})$. The classic rate $\\Theta(1/\\sqrt T)$\nfor convex functions and $\\Theta(1/T)$ for strongly convex functions are\nspecial cases of our result for $\\kappa \\rightarrow \\infty$ and $\\kappa=2$, and\neven faster rates are attained for $\\kappa <2$. We also derive tight bounds for\nthe complexity of learning $x_{f,S}^*$, where the optimal rate is\n$\\Theta(T^{-\\frac{1}{2\\kappa-2}})$. Interestingly, these precise rates for\nconvex optimization also characterize the complexity of active learning and our\nresults further strengthen the connections between the two fields, both of\nwhich rely on feedback-driven queries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 16:33:49 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2013 00:08:51 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""]]}, {"id": "1207.3031", "submitter": "Konstantinos Tsianos", "authors": "Konstantinos I. Tsianos and Michael G. Rabbat", "title": "Distributed Strongly Convex Optimization", "comments": "18 pages single column draftcls format, 1 figure, Submitted to\n  Allerton 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of effort has been invested into characterizing the convergence rates\nof gradient based algorithms for non-linear convex optimization. Recently,\nmotivated by large datasets and problems in machine learning, the interest has\nshifted towards distributed optimization. In this work we present a distributed\nalgorithm for strongly convex constrained optimization. Each node in a network\nof n computers converges to the optimum of a strongly convex, L-Lipchitz\ncontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the\nnumber of iterations. This rate is achieved in the online setting where the\ndata is revealed one at a time to the nodes, and in the batch setting where\neach node has access to its full local dataset from the start. The same\nconvergence rate is achieved in expectation when the subgradients used at each\nnode are corrupted with additive zero-mean noise.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 17:38:46 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 03:08:51 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Tsianos", "Konstantinos I.", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1207.3127", "submitter": "Quan Wang", "authors": "Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim", "title": "Tracking Tetrahymena Pyriformis Cells using Decision Trees", "comments": "21st International Conference on Pattern Recognition, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.CB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching cells over time has long been the most difficult step in cell\ntracking. In this paper, we approach this problem by recasting it as a\nclassification problem. We construct a feature set for each cell, and compute a\nfeature difference vector between a cell in the current frame and a cell in a\nprevious frame. Then we determine whether the two cells represent the same cell\nover time by training decision trees as our binary classifiers. With the output\nof decision trees, we are able to formulate an assignment problem for our cell\nassociation task and solve it using a modified version of the Hungarian\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 01:22:04 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wang", "Quan", ""], ["Ou", "Yan", ""], ["Julius", "A. Agung", ""], ["Boyer", "Kim L.", ""], ["Kim", "Min Jun", ""]]}, {"id": "1207.3285", "submitter": "Sarvesh Nikumbh", "authors": "Sarvesh Nikumbh, Shameek Ghosh, Valadi Jayaraman", "title": "Biogeography-Based Informative Gene Selection and Cancer Classification\n  Using SVM and Random Forests", "comments": "6 pages; Author's copy; Presented at the IEEE World Congress on\n  Computational Intelligence (at IEEE Congress on Evolutionary Computation),\n  Brisbane, Australia, June 2012", "journal-ref": null, "doi": "10.1109/CEC.2012.6256127", "report-no": "CMS-TR-20120509", "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray cancer gene expression data comprise of very high dimensions.\nReducing the dimensions helps in improving the overall analysis and\nclassification performance. We propose two hybrid techniques, Biogeography -\nbased Optimization - Random Forests (BBO - RF) and BBO - SVM (Support Vector\nMachines) with gene ranking as a heuristic, for microarray gene expression\nanalysis. This heuristic is obtained from information gain filter ranking\nprocedure. The BBO algorithm generates a population of candidate subset of\ngenes, as part of an ecosystem of habitats, and employs the migration and\nmutation processes across multiple generations of the population to improve the\nclassification accuracy. The fitness of each gene subset is assessed by the\nclassifiers - SVM and Random Forests. The performances of these hybrid\ntechniques are evaluated on three cancer gene expression datasets retrieved\nfrom the Kent Ridge Biomedical datasets collection and the libSVM data\nrepository. Our results demonstrate that genes selected by the proposed\ntechniques yield classification accuracies comparable to previously reported\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 19:00:46 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nikumbh", "Sarvesh", ""], ["Ghosh", "Shameek", ""], ["Jayaraman", "Valadi", ""]]}, {"id": "1207.3399", "submitter": "Guido F.  Montufar", "authors": "Guido F. Montufar and Johannes Rauh", "title": "Scaling of Model Approximation Errors and Expected Entropy Distances", "comments": "13 pages, 3 figures, WUPES'12", "journal-ref": "Kybernetika 50 (2014) 2, p. 234-245", "doi": "10.14736/kyb-2014-2-0234", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the expected value of the Kullback-Leibler divergence to various\nfundamental statistical models with respect to canonical priors on the\nprobability simplex. We obtain closed formulas for the expected model\napproximation errors, depending on the dimension of the models and the\ncardinalities of their sample spaces. For the uniform prior, the expected\ndivergence from any model containing the uniform distribution is bounded by a\nconstant $1-\\gamma$, and for the models that we consider, this bound is\napproached if the state space is very large and the models' dimension does not\ngrow too fast. For Dirichlet priors the expected divergence is bounded in a\nsimilar way, if the concentration parameters take reasonable values. These\nresults serve as reference values for more complicated statistical models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 07:10:19 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 19:49:04 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Montufar", "Guido F.", ""], ["Rauh", "Johannes", ""]]}, {"id": "1207.3438", "submitter": "Dacheng Tao", "authors": "Naiyang Guan, Dacheng Tao, Zhigang Luo, John Shawe-Taylor", "title": "MahNMF: Manhattan Non-negative Matrix Factorization", "comments": "43 pages, 20 figures, 2 tables, submission to Journal of Machine\n  Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) approximates a non-negative matrix\n$X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMF\nand its extensions minimize either the Kullback-Leibler divergence or the\nEuclidean distance between $X$ and $W^T H$ to model the Poisson noise or the\nGaussian noise. In practice, when the noise distribution is heavy tailed, they\ncannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizes\nthe Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailed\nLaplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMF\nrobustly estimates the low-rank part and the sparse part of a non-negative\nmatrix and thus performs effectively when data are contaminated by outliers. We\nextend MahNMF for various practical applications by developing box-constrained\nMahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducing\nMahNMF, and symmetric MahNMF. The major contribution of this paper lies in two\nfast optimization algorithms for MahNMF and its extensions: the rank-one\nresidual iteration (RRI) method and Nesterov's smoothing method. In particular,\nby approximating the residual matrix by the outer product of one row of W and\none row of $H$ in MahNMF, we develop an RRI method to iteratively update each\nvariable of $W$ and $H$ in a closed form solution. Although RRI is efficient\nfor small scale MahNMF and some of its extensions, it is neither scalable to\nlarge scale matrices nor flexible enough to optimize all MahNMF extensions.\nSince the objective functions of MahNMF and its extensions are neither convex\nnor smooth, we apply Nesterov's smoothing method to recursively optimize one\nfactor matrix with another matrix fixed. By setting the smoothing parameter\ninversely proportional to the iteration number, we improve the approximation\naccuracy iteratively for both MahNMF and its extensions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 16:19:40 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Guan", "Naiyang", ""], ["Tao", "Dacheng", ""], ["Luo", "Zhigang", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1207.3520", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Paris - Rocquencourt), Alexandre Gramfort\n  (LNAO, INRIA Saclay - Ile de France), Ga\\\"el Varoquaux (LNAO, INRIA Saclay -\n  Ile de France), Bertrand Thirion (INRIA Saclay - Ile de France), Christophe\n  Pallier (NEUROSPIN), Elodie Cauvet (NEUROSPIN)", "title": "Improved brain pattern recovery through ranking approaches", "comments": null, "journal-ref": "Pattern Recognition in NeuroImaging (PRNI 2012) (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the functional specificity of brain regions from functional\nMagnetic Resonance Images (fMRI) data is a challenging statistical problem.\nWhile the General Linear Model (GLM) remains the standard approach for brain\nmapping, supervised learning techniques (a.k.a.} decoding) have proven to be\nuseful to capture multivariate statistical effects distributed across voxels\nand brain regions. Up to now, much effort has been made to improve decoding by\nincorporating prior knowledge in the form of a particular regularization term.\nIn this paper we demonstrate that further improvement can be made by accounting\nfor non-linearities using a ranking approach rather than the commonly used\nleast-square regression. Through simulation, we compare the recovery properties\nof our approach to linear models commonly used in fMRI based decoding. We\ndemonstrate the superiority of ranking with a real fMRI dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2012 15:06:35 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Paris - Rocquencourt"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Varoquaux", "Ga\u00ebl", "", "LNAO, INRIA Saclay -\n  Ile de France"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France"], ["Pallier", "Christophe", "", "NEUROSPIN"], ["Cauvet", "Elodie", "", "NEUROSPIN"]]}, {"id": "1207.3554", "submitter": "Akisato Kimura", "authors": "Akisato Kimura, Masashi Sugiyama, Sakano Hitoshi, Hirokazu Kameoka", "title": "Designing various component analysis at will", "comments": "Accepted to IAPR International Conference on Pattern Recognition,\n  submitted to IPSJ Transactions on Mathematical Modeling and its Applications\n  (TOM). Just only one-page abstract for new due to novelty violation for\n  journal submission. The details will be disclosed in late September", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a generic framework of component analysis (CA) methods\nintroducing a new expression for scatter matrices and Gram matrices, called\nGeneralized Pairwise Expression (GPE). This expression is quite compact but\nhighly powerful: The framework includes not only (1) the standard CA methods\nbut also (2) several regularization techniques, (3) weighted extensions, (4)\nsome clustering methods, and (5) their semi-supervised extensions. This paper\nalso presents quite a simple methodology for designing a desired CA method from\nthe proposed framework: Adopting the known GPEs as templates, and generating a\nnew method by combining these templates appropriately.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 00:07:44 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 23:45:32 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Kimura", "Akisato", ""], ["Sugiyama", "Masashi", ""], ["Hitoshi", "Sakano", ""], ["Kameoka", "Hirokazu", ""]]}, {"id": "1207.3649", "submitter": "Pasi Jyl\\\"anki", "authors": "Jaakko Riihim\\\"aki, Pasi Jyl\\\"anki and Aki Vehtari", "title": "Nested Expectation Propagation for Gaussian Process Classification with\n  a Multinomial Probit Likelihood", "comments": null, "journal-ref": "Journal of Machine Learning Research 14 (2013) 75-109", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider probabilistic multinomial probit classification using Gaussian\nprocess (GP) priors. The challenges with the multiclass GP classification are\nthe integration over the non-Gaussian posterior distribution, and the increase\nof the number of unknown latent variables as the number of target classes\ngrows. Expectation propagation (EP) has proven to be a very accurate method for\napproximate inference but the existing EP approaches for the multinomial probit\nGP classification rely on numerical quadratures or independence assumptions\nbetween the latent values from different classes to facilitate the\ncomputations. In this paper, we propose a novel nested EP approach which does\nnot require numerical quadratures, and approximates accurately all\nbetween-class posterior dependencies of the latent values, but still scales\nlinearly in the number of classes. The predictive accuracy of the nested EP\napproach is compared to Laplace, variational Bayes, and Markov chain Monte\nCarlo (MCMC) approximations with various benchmark data sets. In the\nexperiments nested EP was the most consistent method with respect to MCMC\nsampling, but the differences between the compared methods were small if only\nthe classification accuracy is concerned.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 12:29:22 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Riihim\u00e4ki", "Jaakko", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Vehtari", "Aki", ""]]}, {"id": "1207.3718", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov, Joey Paquet, Mourad Debbabi, Yankui Sun", "title": "MARFCAT: Transitioning to Binary and Larger Data Sets of SATE IV", "comments": "A shorter version submitted for review to 13th IEEE International\n  Working Conference on Source Code Analysis and Manipulation. 39 pages with\n  figures, tables, TOC, and index", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a second iteration of a machine learning approach to static code\nanalysis and fingerprinting for weaknesses related to security, software\nengineering, and others using the open-source MARF framework and the MARFCAT\napplication based on it for the NIST's SATE IV static analysis tool exposition\nworkshop's data sets that include additional test cases, including new large\nsynthetic cases. To aid detection of weak or vulnerable code, including source\nor binary on different platforms the machine learning approach proved to be\nfast and accurate to for such tasks where other tools are either much slower or\nhave much smaller recall of known vulnerabilities. We use signal and NLP\nprocessing techniques in our approach to accomplish the identification and\nclassification tasks. MARFCAT's design from the beginning in 2010 made is\nindependent of the language being analyzed, source code, bytecode, or binary.\nIn this follow up work with explore some preliminary results in this area. We\nevaluated also additional algorithms that were used to process the data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 16:26:59 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 19:58:14 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Mokhov", "Serguei A.", ""], ["Paquet", "Joey", ""], ["Debbabi", "Mourad", ""], ["Sun", "Yankui", ""]]}, {"id": "1207.3772", "submitter": "Steve Hanneke", "authors": "Steve Hanneke and Liu Yang", "title": "Surrogate Losses in Passive and Active Learning", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 13, Number 2 (2019),\n  4646-4708", "doi": "10.1214/19-EJS1635", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a type of sequential design for supervised machine\nlearning, in which the learning algorithm sequentially requests the labels of\nselected instances from a large pool of unlabeled data points. The objective is\nto produce a classifier of relatively low risk, as measured under the 0-1 loss,\nideally using fewer label requests than the number of random labeled data\npoints sufficient to achieve the same. This work investigates the potential\nuses of surrogate loss functions in the context of active learning.\nSpecifically, it presents an active learning algorithm based on an arbitrary\nclassification-calibrated surrogate loss function, along with an analysis of\nthe number of label requests sufficient for the classifier returned by the\nalgorithm to achieve a given risk under the 0-1 loss. Interestingly, these\nresults cannot be obtained by simply optimizing the surrogate risk via active\nlearning to an extent sufficient to provide a guarantee on the 0-1 loss, as is\ncommon practice in the analysis of surrogate losses for passive learning. Some\nof the results have additional implications for the use of surrogate losses in\npassive learning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 19:26:24 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 02:51:59 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2015 15:25:54 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 17:30:55 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1207.3944", "submitter": "Alejandro Frery", "authors": "Alejandro C. Frery and Julio Jacobo-Berlles and Juliana Gambini and\n  Marta Mejail", "title": "Polarimetric SAR Image Segmentation with B-Splines and a New Statistical\n  Model", "comments": null, "journal-ref": "Multidimensional Systems and Signal Processing, vol. 21, 319-342,\n  2010", "doi": "10.1007/s11045-010-0113-4", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for polarimetric Synthetic Aperture Radar (SAR) image\nregion boundary detection based on the use of B-Spline active contours and a\nnew model for polarimetric SAR data: the GHP distribution. In order to detect\nthe boundary of a region, initial B-Spline curves are specified, either\nautomatically or manually, and the proposed algorithm uses a deformable\ncontours technique to find the boundary. In doing this, the parameters of the\npolarimetric GHP model for the data are estimated, in order to find the\ntransition points between the region being segmented and the surrounding area.\nThis is a local algorithm since it works only on the region to be segmented.\nResults of its performance are presented.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 11:09:37 GMT"}], "update_date": "2012-07-18", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Jacobo-Berlles", "Julio", ""], ["Gambini", "Juliana", ""], ["Mejail", "Marta", ""]]}, {"id": "1207.3961", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Ensemble Clustering with Logic Rules", "comments": "Replacing two articles with one", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the logic rule ensembles approach to supervised learning is\napplied to the unsupervised or semi-supervised clustering. Logic rules which\nwere obtained by combining simple conjunctive rules are used to partition the\ninput space and an ensemble of these rules is used to define a similarity\nmatrix. Similarity partitioning is used to partition the data in an\nhierarchical manner. We have used internal and external measures of cluster\nvalidity to evaluate the quality of clusterings or to identify the number of\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 11:54:31 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 15:30:34 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 00:44:30 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1207.3994", "submitter": "Xiaoran Yan", "authors": "Xiaoran Yan, Cosma Rohilla Shalizi, Jacob E. Jensen, Florent Krzakala,\n  Cristopher Moore, Lenka Zdeborova, Pan Zhang, Yaojia Zhu", "title": "Model Selection for Degree-corrected Block Models", "comments": null, "journal-ref": "J. Stat. Mech. (2014) P05007", "doi": "10.1088/1742-5468/2014/05/P05007", "report-no": null, "categories": "cs.SI cond-mat.stat-mech math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of models for networks raises challenging problems of model\nselection: the data are sparse and globally dependent, and models are typically\nhigh-dimensional and have large numbers of latent variables. Together, these\nissues mean that the usual model-selection criteria do not work properly for\nnetworks. We illustrate these challenges, and show one way to resolve them, by\nconsidering the key network-analysis problem of dividing a graph into\ncommunities or blocks of nodes with homogeneous patterns of links to the rest\nof the network. The standard tool for doing this is the stochastic block model,\nunder which the probability of a link between two nodes is a function solely of\nthe blocks to which they belong. This imposes a homogeneous degree distribution\nwithin each block; this can be unrealistic, so degree-corrected block models\nadd a parameter for each node, modulating its over-all degree. The choice\nbetween ordinary and degree-corrected block models matters because they make\nvery different inferences about communities. We present the first principled\nand tractable approach to model selection between standard and degree-corrected\nblock models, based on new large-graph asymptotics for the distribution of\nlog-likelihood ratios under the stochastic block model, finding substantial\ndepartures from classical results for sparse graphs. We also develop\nlinear-time approximations for log-likelihoods under both the stochastic block\nmodel and the degree-corrected model, using belief propagation. Applications to\nsimulated and real networks show excellent agreement with our approximations.\nOur results thus both solve the practical problem of deciding on degree\ncorrection, and point to a general approach to model selection in network\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 13:58:56 GMT"}, {"version": "v2", "created": "Thu, 30 May 2013 18:25:07 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Yan", "Xiaoran", ""], ["Shalizi", "Cosma Rohilla", ""], ["Jensen", "Jacob E.", ""], ["Krzakala", "Florent", ""], ["Moore", "Cristopher", ""], ["Zdeborova", "Lenka", ""], ["Zhang", "Pan", ""], ["Zhu", "Yaojia", ""]]}, {"id": "1207.4110", "submitter": "Amir Globerson", "authors": "Amir Globerson, Naftali Tishby", "title": "The Minimum Information Principle for Discriminative Learning", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-193-200", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential models of distributions are widely used in machine learning for\nclassiffication and modelling. It is well known that they can be interpreted as\nmaximum entropy models under empirical expectation constraints. In this work,\nwe argue that for classiffication tasks, mutual information is a more suitable\ninformation theoretic measure to be optimized. We show how the principle of\nminimum mutual information generalizes that of maximum entropy, and provides a\ncomprehensive framework for building discriminative classiffiers. A game\ntheoretic interpretation of our approach is then given, and several\ngeneralization bounds provided. We present iterative algorithms for solving the\nminimum information problem and its convex dual, and demonstrate their\nperformance on various classiffication tasks. The results show that minimum\ninformation classiffiers outperform the corresponding maximum entropy models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:41:52 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Globerson", "Amir", ""], ["Tishby", "Naftali", ""]]}, {"id": "1207.4112", "submitter": "Luis David Garcia", "authors": "Luis David Garcia", "title": "Algebraic Statistics in Model Selection", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-177-184", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the necessary theory in computational algebraic geometry to place\nBayesian networks into the realm of algebraic statistics. We present an\nalgebra{statistics dictionary focused on statistical modeling. In particular,\nwe link the notion of effiective dimension of a Bayesian network with the\nnotion of algebraic dimension of a variety. We also obtain the independence and\nnon{independence constraints on the distributions over the observable variables\nimplied by a Bayesian network with hidden variables, via a generating set of an\nideal of polynomials associated to the network. These results extend previous\nwork on the subject. Finally, the relevance of these results for model\nselection is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:42:26 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Garcia", "Luis David", ""]]}, {"id": "1207.4113", "submitter": "Alex Gammerman", "authors": "Alex Gammerman, Yuri Kalnishkan, Vladimir Vovk", "title": "On-line Prediction with Kernels and the Complexity Approximation\n  Principle", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-170-176", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes an application of Aggregating Algorithm to the problem of\nregression. It generalizes earlier results concerned with plain linear\nregression to kernel techniques and presents an on-line algorithm which\nperforms nearly as well as any oblivious kernel predictor. The paper contains\nthe derivation of an estimate on the performance of this algorithm. The\nestimate is then used to derive an application of the Complexity Approximation\nPrinciple to kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:42:45 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Gammerman", "Alex", ""], ["Kalnishkan", "Yuri", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1207.4118", "submitter": "Mathias Drton", "authors": "Mathias Drton, Thomas S. Richardson", "title": "Iterative Conditional Fitting for Gaussian Ancestral Graph Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-130-137", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ancestral graph models, introduced by Richardson and Spirtes (2002),\ngeneralize both Markov random fields and Bayesian networks to a class of graphs\nwith a global Markov property that is closed under conditioning and\nmarginalization. By design, ancestral graphs encode precisely the conditional\nindependence structures that can arise from Bayesian networks with selection\nand unobserved (hidden/latent) variables. Thus, ancestral graph models provide\na potentially very useful framework for exploratory model selection when\nunobserved variables might be involved in the data-generating process but no\nparticular hidden structure can be specified. In this paper, we present the\nIterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation\nin Gaussian ancestral graph models. The name reflects that in each step of the\nprocedure a conditional distribution is estimated, subject to constraints,\nwhile a marginal distribution is held fixed. This approach is in duality to the\nwell-known Iterative Proportional Fitting algorithm, in which marginal\ndistributions are fitted while conditional distributions are held fixed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:44:26 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Drton", "Mathias", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1207.4125", "submitter": "Wray L. Buntine", "authors": "Wray L. Buntine, Aleks Jakulin", "title": "Applying Discrete PCA in Data Analysis", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-59-66", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for analysis of principal components in discrete data have existed\nfor some time under various names such as grade of membership modelling,\nprobabilistic latent semantic analysis, and genotype inference with admixture.\nIn this paper we explore a number of extensions to the common theory, and\npresent some application of these methods to some common statistical tasks. We\nshow that these methods can be interpreted as a discrete version of ICA. We\ndevelop a hierarchical version yielding components at different levels of\ndetail, and additional techniques for Gibbs sampling. We compare the algorithms\non a text prediction task using support vector machines, and to information\nretrieval.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:46:50 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Buntine", "Wray L.", ""], ["Jakulin", "Aleks", ""]]}, {"id": "1207.4131", "submitter": "Yasemin Altun", "authors": "Yasemin Altun, Alex Smola, Thomas Hofmann", "title": "Exponential Families for Conditional Random Fields", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-2-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we de ne conditional random elds in reproducing kernel Hilbert\nspaces and show connections to Gaussian Process classi cation. More speci\ncally, we prove decomposition results for undirected graphical models and we\ngive constructions for kernels. Finally we present e cient means of solving the\noptimization problem using reduced rank decompositions and we show how\nstationarity can be exploited e ciently in the optimization process.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:48:54 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Altun", "Yasemin", ""], ["Smola", "Alex", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1207.4132", "submitter": "Rodney Nielsen", "authors": "Rodney Nielsen", "title": "MOB-ESP and other Improvements in Probability Estimation", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-418-425", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key prerequisite to optimal reasoning under uncertainty in intelligent\nsystems is to start with good class probability estimates. This paper improves\non the current best probability estimation trees (Bagged-PETs) and also\npresents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using\nseveral benchmark datasets and multiple metrics. These experiments show that\nMOB-ESP outputs significantly more accurate class probabilities than either the\nbaseline BPETs algorithm or the enhanced version presented here (EB-PETs).\nThese results are based on metrics closely associated with the average accuracy\nof the predictions. MOB-ESP also provides much better probability rankings than\nB-PETs. The paper further suggests how these estimation techniques can be\napplied in concert with a broader category of classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:03 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Nielsen", "Rodney", ""]]}, {"id": "1207.4133", "submitter": "Iftach Nachman", "authors": "Iftach Nachman, Gal Elidan, Nir Friedman", "title": "\"Ideal Parent\" Structure Learning for Continuous Variable Networks", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-400-409", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a growing interest in learning Bayesian networks\nwith continuous variables. Learning the structure of such networks is a\ncomputationally expensive procedure, which limits most applications to\nparameter learning. This problem is even more acute when learning networks with\nhidden variables. We present a general method for significantly speeding the\nstructure search algorithm for continuous variable networks with common\nparametric distributions. Importantly, our method facilitates the addition of\nnew hidden variables into the network structure efficiently. We demonstrate the\nmethod on several data sets, both for learning structure on fully observable\ndata, and for introducing new hidden variables during structure search.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:23 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Nachman", "Iftach", ""], ["Elidan", "Gal", ""], ["Friedman", "Nir", ""]]}, {"id": "1207.4134", "submitter": "Iain Murray", "authors": "Iain Murray, Zoubin Ghahramani", "title": "Bayesian Learning in Undirected Graphical Models: Approximate MCMC\n  algorithms", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-392-399", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning in undirected graphical models|computing posterior\ndistributions over parameters and predictive quantities is exceptionally\ndifficult. We conjecture that for general undirected models, there are no\ntractable MCMC (Markov Chain Monte Carlo) schemes giving the correct\nequilibrium distribution over parameters. While this intractability, due to the\npartition function, is familiar to those performing parameter optimisation,\nBayesian learning of posterior distributions over undirected model parameters\nhas been unexplored and poses novel challenges. we propose several approximate\nMCMC schemes and test on fully observed binary models (Boltzmann machines) for\na small coronary heart disease data set and larger artificial systems. While\napproximations must perform well on the model, their interaction with the\nsampling scheme is also important. Samplers based on variational mean- field\napproximations generally performed poorly, more advanced methods using loopy\npropagation, brief sampling and stochastic dynamics lead to acceptable\nparameter posteriors. Finally, we demonstrate these techniques on a Markov\nrandom field with hidden variables.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:41 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Murray", "Iain", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1207.4138", "submitter": "Omid Madani", "authors": "Omid Madani, Daniel J. Lizotte, Russell Greiner", "title": "Active Model Selection", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-357-365", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical learning assumes the learner is given a labeled data sample, from\nwhich it learns a model. The field of Active Learning deals with the situation\nwhere the learner begins not with a training sample, but instead with resources\nthat it can use to obtain information to help identify the optimal model. To\nbetter understand this task, this paper presents and analyses the simplified\n\"(budgeted) active model selection\" version, which captures the pure\nexploration aspect of many active learning problems in a clean and simple\nproblem formulation. Here the learner can use a fixed budget of \"model probes\"\n(where each probe evaluates the specified model on a random indistinguishable\ninstance) to identify which of a given set of possible models has the highest\nexpected accuracy. Our goal is a policy that sequentially determines which\nmodel to probe next, based on the information observed so far. We present a\nformal description of this task, and show that it is NPhard in general. We then\ninvestigate a number of algorithms for this task, including several existing\nones (eg, \"Round-Robin\", \"Interval Estimation\", \"Gittins\") as well as some\nnovel ones (e.g., \"Biased-Robin\"), describing first their approximation\nproperties and then their empirical performance on various problem instances.\nWe observe empirically that the simple biased-robin algorithm significantly\noutperforms the other algorithms in the case of identical costs and priors.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:52:51 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Madani", "Omid", ""], ["Lizotte", "Daniel J.", ""], ["Greiner", "Russell", ""]]}, {"id": "1207.4139", "submitter": "Guy Lebanon", "authors": "Guy Lebanon", "title": "An Extended Cencov-Campbell Characterization of Conditional Information\n  Geometry", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-341-348", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and prove an axiomatic characterization of conditional\ninformation geometry, for both the normalized and the nonnormalized cases. This\ncharacterization extends the axiomatic derivation of the Fisher geometry by\nCencov and Campbell to the cone of positive conditional models, and as a\nspecial case to the manifold of conditional distributions. Due to the close\nconnection between the conditional I-divergence and the product Fisher\ninformation metric the characterization provides a new axiomatic interpretation\nof the primal problems underlying logistic regression and AdaBoost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:53:33 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Lebanon", "Guy", ""]]}, {"id": "1207.4142", "submitter": "Sergey Kirshner", "authors": "Sergey Kirshner, Padhraic Smyth, Andrew Robertson", "title": "Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector\n  Time Series", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-317-324", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling discrete-valued vector time series data\nusing extensions of Chow-Liu tree models to capture both dependencies across\ntime and dependencies across variables. Conditional Chow-Liu tree models are\nintroduced, as an extension to standard Chow-Liu trees, for modeling\nconditional rather than joint densities. We describe learning algorithms for\nsuch models and show how they can be used to learn parsimonious representations\nfor the output distributions in hidden Markov models. These models are applied\nto the important problem of simulating and forecasting daily precipitation\noccurrence for networks of rain stations. To demonstrate the effectiveness of\nthe models, we compare their performance versus a number of alternatives using\nhistorical precipitation data from Southwestern Australia and the Western\nUnited States. We illustrate how the structure and parameters of the models can\nbe used to provide an improved meteorological interpretation of such data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:54:25 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Kirshner", "Sergey", ""], ["Smyth", "Padhraic", ""], ["Robertson", "Andrew", ""]]}, {"id": "1207.4144", "submitter": "Joseph Kahn", "authors": "Joseph Kahn", "title": "A Generative Bayesian Model for Aggregating Experts' Probabilities", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-301-308", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve forecasts, a decisionmaker often combines probabilities\ngiven by various sources, such as human experts and machine learning\nclassifiers. When few training data are available, aggregation can be improved\nby incorporating prior knowledge about the event being forecasted and about\nsalient properties of the experts. To this end, we develop a generative\nBayesian aggregation model for probabilistic classi cation. The model includes\nan event-specific prior, measures of individual experts' bias, calibration,\naccuracy, and a measure of dependence betweeen experts. Rather than require\nabsolute measures, we show that aggregation may be expressed in terms of\nrelative accuracy between experts. The model results in a weighted logarithmic\nopinion pool (LogOps) that satis es consistency criteria such as the external\nBayesian property. We derive analytic solutions for independent and for\nexchangeable experts. Empirical tests demonstrate the model's use, comparing\nits accuracy with other aggregation methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:54:55 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Kahn", "Joseph", ""]]}, {"id": "1207.4146", "submitter": "Rong Jin", "authors": "Rong Jin, Luo Si", "title": "A Bayesian Approach toward Active Learning for Collaborative Filtering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-278-285", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a useful technique for exploiting the preference\npatterns of a group of users to predict the utility of items for the active\nuser. In general, the performance of collaborative filtering depends on the\nnumber of rated examples given by the active user. The more the number of rated\nexamples given by the active user, the more accurate the predicted ratings will\nbe. Active learning provides an effective way to acquire the most informative\nrated examples from active users. Previous work on active learning for\ncollaborative filtering only considers the expected loss function based on the\nestimated model, which can be misleading when the estimated model is\ninaccurate. This paper takes one step further by taking into account of the\nposterior distribution of the estimated model, which results in more robust\nactive learning algorithm. Empirical studies with datasets of movie ratings\nshow that when the number of ratings from the active user is restricted to be\nsmall, active learning methods only based on the estimated model don't perform\nwell while the active learning method using the model distribution achieves\nsubstantially better performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:55:41 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Jin", "Rong", ""], ["Si", "Luo", ""]]}, {"id": "1207.4148", "submitter": "Andrew Howard", "authors": "Andrew Howard, Tony S. Jebara", "title": "Dynamical Systems Trees", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-260-267", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose dynamical systems trees (DSTs) as a flexible class of models for\ndescribing multiple processes that interact via a hierarchy of aggregating\nparent chains. DSTs extend Kalman filters, hidden Markov models and nonlinear\ndynamical systems to an interactive group scenario. Various individual\nprocesses interact as communities and sub-communities in a tree structure that\nis unrolled in time. To accommodate nonlinear temporal activity, each\nindividual leaf process is modeled as a dynamical system containing discrete\nand/or continuous hidden states with discrete and/or Gaussian emissions.\nSubsequent higher level parent processes act like hidden Markov models and\nmediate the interaction between leaf processes or between other parent\nprocesses in the hierarchy. Aggregator chains are parents of child processes\nthat they combine and mediate, yielding a compact overall parameterization. We\nprovide tractable inference and learning algorithms for arbitrary DST\ntopologies via an efficient structured mean-field algorithm. The diverse\napplicability of DSTs is demonstrated by experiments on gene expression data\nand by modeling group behavior in the setting of an American football game.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:56:09 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Howard", "Andrew", ""], ["Jebara", "Tony S.", ""]]}, {"id": "1207.4151", "submitter": "Mukund Narasimhan", "authors": "Mukund Narasimhan, Jeff A. Bilmes", "title": "PAC-learning bounded tree-width Graphical Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-410-417", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the class of strongly connected graphical models with treewidth\nat most k can be properly efficiently PAC-learnt with respect to the\nKullback-Leibler Divergence. Previous approaches to this problem, such as those\nof Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by\nreducing it to a combinatorial optimization problem. However, for k > 1, this\nproblem is NP-complete ([15]), and so unless P=NP, these approaches will take\nexponential amounts of time. Our approach differs significantly from these, in\nthat it first attempts to find approximate conditional independencies by\nsolving (polynomially many) submodular optimization problems, and then using a\ndynamic programming formulation to combine the approximate conditional\nindependence information to derive a graphical model with underlying graph of\nthe tree-width specified. This gives us an efficient (polynomial time in the\nnumber of random variables) PAC-learning algorithm which requires only\npolynomial number of samples of the true distribution, and only polynomial\nrunning time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:57:38 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Narasimhan", "Mukund", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1207.4155", "submitter": "Xuejian Xiong", "authors": "Xuejian Xiong, Kap Chan, Kian Lee Tan", "title": "Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy\n  Clustering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-611-618", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a similarity-driven cluster merging method is proposed for\nunsuper-vised fuzzy clustering. The cluster merging method is used to resolve\nthe problem of cluster validation. Starting with an overspecified number of\nclusters in the data, pairs of similar clusters are merged based on the\nproposed similarity-driven cluster merging criterion. The similarity between\nclusters is calculated by a fuzzy cluster similarity matrix, while an adaptive\nthreshold is used for merging. In addition, a modified generalized ob- jective\nfunction is used for prototype-based fuzzy clustering. The function includes\nthe p-norm distance measure as well as principal components of the clusters.\nThe number of the principal components is determined automatically from the\ndata being clustered. The properties of this unsupervised fuzzy clustering\nalgorithm are illustrated by several experiments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:59:55 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Xiong", "Xuejian", ""], ["Chan", "Kap", ""], ["Tan", "Kian Lee", ""]]}, {"id": "1207.4156", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Michael I. Jordan, Stuart Russell", "title": "Graph partition strategies for generalized mean field inference", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-602-610", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autonomous variational inference algorithm for arbitrary graphical models\nrequires the ability to optimize variational approximations over the space of\nmodel parameters as well as over the choice of tractable families used for the\nvariational approximation. In this paper, we present a novel combination of\ngraph partitioning algorithms with a generalized mean field (GMF) inference\nalgorithm. This combination optimizes over disjoint clustering of variables and\nperforms inference using those clusters. We provide a formal analysis of the\nrelationship between the graph cut and the GMF approximation, and explore\nseveral graph partition strategies empirically. Our empirical results provide\nrather clear support for a weighted version of MinCut as a useful clustering\nalgorithm for GMF inference, which is consistent with the implications from the\nformal analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:00:11 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Xing", "Eric P.", ""], ["Jordan", "Michael I.", ""], ["Russell", "Stuart", ""]]}, {"id": "1207.4157", "submitter": "Ben Wellner", "authors": "Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay", "title": "An Integrated, Conditional Model of Information Extraction and\n  Coreference with Applications to Citation Matching", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-593-601", "categories": "cs.LG cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although information extraction and coreference resolution appear together in\nmany applications, most current systems perform them as ndependent steps. This\npaper describes an approach to integrated inference for extraction and\ncoreference based on conditionally-trained undirected graphical models. We\ndiscuss the advantages of conditional probability training, and of a\ncoreference model structure based on graph partitioning. On a data set of\nresearch paper citations, we show significant reduction in error by using\nextraction uncertainty to improve coreference citation matching accuracy, and\nusing coreference to improve the accuracy of the extracted fields.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:00:28 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Wellner", "Ben", ""], ["McCallum", "Andrew", ""], ["Peng", "Fuchun", ""], ["Hay", "Michael", ""]]}, {"id": "1207.4164", "submitter": "Chris Stauffer", "authors": "Chris Stauffer", "title": "Factored Latent Analysis for far-field tracking data", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-536-543", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses Factored Latent Analysis (FLA) to learn a factorized,\nsegmental representation for observations of tracked objects over time.\nFactored Latent Analysis is latent class analysis in which the observation\nspace is subdivided and each aspect of the original space is represented by a\nseparate latent class model. One could simply treat these factors as completely\nindependent and ignore their interdependencies or one could concatenate them\ntogether and attempt to learn latent class structure for the complete\nobservation space. Alternatively, FLA allows the interdependencies to be\nexploited in estimating an effective model, which is also capable of\nrepresenting a factored latent state. In this paper, FLA is used to learn a set\nof factored latent classes to represent different modalities of observations of\ntracked objects. Different characteristics of the state of tracked objects are\neach represented by separate latent class models, including normalized size,\nnormalized speed, normalized direction, and position. This model also enables\neffective temporal segmentation of these sequences. This method is data-driven,\nunsupervised using only pairwise observation statistics. This data-driven and\nunsupervised activity classi- fication technique exhibits good performance in\nmultiple challenging environments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:03:34 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Stauffer", "Chris", ""]]}, {"id": "1207.4169", "submitter": "Michal Rosen-Zvi", "authors": "Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth", "title": "The Author-Topic Model for Authors and Documents", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-487-494", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the author-topic model, a generative model for documents that\nextends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include\nauthorship information. Each author is associated with a multinomial\ndistribution over topics and each topic is associated with a multinomial\ndistribution over words. A document with multiple authors is modeled as a\ndistribution over topics that is a mixture of the distributions associated with\nthe authors. We apply the model to a collection of 1,700 NIPS conference papers\nand 160,000 CiteSeer abstracts. Exact inference is intractable for these\ndatasets and we use Gibbs sampling to estimate the topic and author\ndistributions. We compare the performance with two other generative models for\ndocuments, which are special cases of the author-topic model: LDA (a topic\nmodel) and a simple author model in which each author is associated with a\ndistribution over words rather than a distribution over topics. We show topics\nrecovered by the author-topic model, and demonstrate applications to computing\nsimilarity between authors and entropy of author output.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:05:53 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Rosen-Zvi", "Michal", ""], ["Griffiths", "Thomas", ""], ["Steyvers", "Mark", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1207.4172", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, John Lafferty", "title": "Variational Chernoff Bounds for Graphical Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-462-469", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has made significant progress on the problem of bounding log\npartition functions for exponential family graphical models. Such bounds have\nassociated dual parameters that are often used as heuristic estimates of the\nmarginal probabilities required in inference and learning. However these\nvariational estimates do not give rigorous bounds on marginal probabilities,\nnor do they give estimates for probabilities of more general events than simple\nmarginals. In this paper we build on this recent work by deriving rigorous\nupper and lower bounds on event probabilities for graphical models. Our\napproach is based on the use of generalized Chernoff bounds to express bounds\non event probabilities in terms of convex optimization problems; these\noptimization problems, in turn, require estimates of generalized log partition\nfunctions. Simulations indicate that this technique can result in useful,\nrigorous bounds to complement the heuristic variational estimates, with\ncomparable computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:06:59 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Lafferty", "John", ""]]}, {"id": "1207.4180", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, William Cohen", "title": "A Hierarchical Graphical Model for Record Linkage", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-454-461", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of matching co-referent records is known among other names as rocord\nlinkage. For large record-linkage problems, often there is little or no labeled\ndata available, but unlabeled data shows a reasonable clear structure. For such\nproblems, unsupervised or semi-supervised methods are preferable to supervised\nmethods. In this paper, we describe a hierarchical graphical model framework\nfor the linakge-problem in an unsupervised setting. In addition to proposing\nnew methods, we also cast existing unsupervised probabilistic record-linkage\nmethods in this framework. Some of the techniques we propose to minimize\noverfitting in the above model are of interest in the general graphical model\nsetting. We describe a method for incorporating monotinicity constraints in a\ngraphical model. We also outline a bootstrapping approach of using\n\"single-field\" classifiers to noisily label latent variables in a hierarchical\nmodel. Experimental results show that our proposed unsupervised methods perform\nquite competitively even with fully supervised record-linkage methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 19:48:03 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Cohen", "William", ""]]}, {"id": "1207.4255", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi Jaakkola and Dimitris Samaras", "title": "On the Statistical Efficiency of $\\ell_{1,p}$ Multi-Task Learning of\n  Gaussian Graphical Models", "comments": "Submitted on October 21, 2015 to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present $\\ell_{1,p}$ multi-task structure learning for\nGaussian graphical models. We analyze the sufficient number of samples for the\ncorrect recovery of the support union and edge signs. We also analyze the\nnecessary number of samples for any conceivable method by providing\ninformation-theoretic lower bounds. We compare the statistical efficiency of\nmulti-task learning versus that of single-task learning. For experiments, we\nuse a block coordinate descent method that is provably convergent and generates\na sequence of positive definite solutions. We provide experimental validation\non synthetic data as well as on two publicly available real-world data sets,\nincluding functional magnetic resonance imaging and gene expression data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 02:53:02 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2015 08:11:13 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1207.4421", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Sahand Negahban, Martin J. Wainwright", "title": "Stochastic optimization and sparse statistical recovery: An optimal\n  algorithm for high dimensions", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze stochastic optimization algorithms for problems in\nwhich the expected loss is strongly convex, and the optimum is (approximately)\nsparse. Previous approaches are able to exploit only one of these two\nstructures, yielding an $\\order(\\pdim/T)$ convergence rate for strongly convex\nobjectives in $\\pdim$ dimensions, and an $\\order(\\sqrt{(\\spindex \\log\n\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our\nalgorithm is based on successively solving a series of $\\ell_1$-regularized\noptimization problems using Nesterov's dual averaging algorithm. We establish\nthat the error of our solution after $T$ iterations is at most\n$\\order((\\spindex \\log\\pdim)/T)$, with natural extensions to approximate\nsparsity. Our results apply to locally Lipschitz losses including the logistic,\nexponential, hinge and least-squares losses. By recourse to statistical minimax\nresults, we show that our convergence rates are optimal up to multiplicative\nconstant factors. The effectiveness of our approach is also confirmed in\nnumerical simulations, in which we compare to several baselines on a\nleast-squares regression problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 17:40:11 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Agarwal", "Alekh", ""], ["Negahban", "Sahand", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1207.4510", "submitter": "Jelena Bradic", "authors": "Jelena Bradic and Rui Song", "title": "Structured Estimation in Nonparameteric Cox Model", "comments": null, "journal-ref": "Electron. J. Statist. Volume 9, Number 1 (2015), 492-534", "doi": "10.1214/15-EJS1004", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand the interplay of censoring and sparsity we develop\nfinite sample properties of nonparametric Cox proportional hazard's model. Due\nto high impact of sequencing data, carrying genetic information of each\nindividual, we work with over-parametrized problem and propose general class of\ngroup penalties suitable for sparse structured variable selection and\nestimation. Novel non-asymptotic sandwich bounds for the partial likelihood are\ndeveloped. We establish how they extend notion of local asymptotic normality\n(LAN) of Le Cam's. Such non-asymptotic LAN principles are further extended to\nhigh dimensional spaces where $p \\gg n$. Finite sample prediction properties of\npenalized estimator in non-parametric Cox proportional hazards model, under\nsuitable censoring conditions, agree with those of penalized estimator in\nlinear models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 22:14:52 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 18:47:42 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 00:57:38 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""], ["Song", "Rui", ""]]}, {"id": "1207.4597", "submitter": "Victorin Martin", "authors": "Victorin Martin, Jean-Marc Lasgouttes and Cyril Furtlehner", "title": "Local stability of Belief Propagation algorithm with multiple fixed\n  points", "comments": "arXiv admin note: substantial text overlap with arXiv:1101.4170", "journal-ref": null, "doi": "10.3233/978-1-61499-096-3-180", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems in statistical physics and computer science can be\nexpressed as the computation of marginal probabilities over a Markov random\nfield. Belief propagation, an iterative message-passing algorithm, computes\nexactly such marginals when the underlying graph is a tree. But it has gained\nits popularity as an efficient way to approximate them in the more general\ncase, even if it can exhibits multiple fixed points and is not guaranteed to\nconverge. In this paper, we express a new sufficient condition for local\nstability of a belief propagation fixed point in terms of the graph structure\nand the beliefs values at the fixed point. This gives credence to the usual\nunderstanding that Belief Propagation performs better on sparse graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 09:49:54 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Martin", "Victorin", ""], ["Lasgouttes", "Jean-Marc", ""], ["Furtlehner", "Cyril", ""]]}, {"id": "1207.4674", "submitter": "Iead Rezek", "authors": "Iead Rezek and Christian Beckmann", "title": "Models of Disease Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case vs control comparisons have been the classical approach to the study of\nneurological diseases. However, most patients will not fall cleanly into either\ngroup. Instead, clinicians will typically find patients that cannot be\nclassified as having clearly progressed into the disease state. For those\nsubjects, very little can be said about their brain function on the basis of\nanalyses of group differences. To describe the intermediate brain function\nrequires models that interpolate between the disease states. We have chosen\nGaussian Processes (GP) regression to obtain a continuous spectrum of brain\nactivation and to extract the unknown disease progression profile. Our models\nincorporate spatial distribution of measures of activation, e.g. the\ncorrelation of an fMRI trace with an input stimulus, and so constitute\nultra-high multi-variate GP regressors. We applied GPs to model fMRI image\nphenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACE\netc. scores, and obtained predictions at non-observed MMSE/ACE values. The\noverall model confirmed the known reduction in the spatial extent of activity\nin response to reading versus false-font stimulation. The predictive\nuncertainty indicated the worsening confidence intervals at behavioural scores\ndistance from those used for GP training. Thus, the model indicated the type of\npatient (what behavioural score) that would need to included in the training\ndata to improve models predictions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 14:06:36 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Rezek", "Iead", ""], ["Beckmann", "Christian", ""]]}, {"id": "1207.4676", "submitter": "Amir Globerson", "authors": "John Langford and Joelle Pineau (Editors)", "title": "Proceedings of the 29th International Conference on Machine Learning\n  (ICML-12)", "comments": "Proceedings of the 29th International Conference on Machine Learning\n  (ICML-12). Editors: John Langford and Joelle Pineau. Publisher: Omnipress,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 14:08:22 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 11:24:54 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Langford", "John", "", "Editors"], ["Pineau", "Joelle", "", "Editors"]]}, {"id": "1207.4684", "submitter": "Michael Mahoney", "authors": "Kenneth L. Clarkson and Petros Drineas and Malik Magdon-Ismail and\n  Michael W. Mahoney and Xiangrui Meng and David P. Woodruff", "title": "The Fast Cauchy Transform and Faster Robust Linear Regression", "comments": "48 pages; substantially extended and revised; short version in SODA\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide fast algorithms for overconstrained $\\ell_p$ regression and\nrelated problems: for an $n\\times d$ input matrix $A$ and vector\n$b\\in\\mathbb{R}^n$, in $O(nd\\log n)$ time we reduce the problem\n$\\min_{x\\in\\mathbb{R}^d} \\|Ax-b\\|_p$ to the same problem with input matrix\n$\\tilde A$ of dimension $s \\times d$ and corresponding $\\tilde b$ of dimension\n$s\\times 1$. Here, $\\tilde A$ and $\\tilde b$ are a coreset for the problem,\nconsisting of sampled and rescaled rows of $A$ and $b$; and $s$ is independent\nof $n$ and polynomial in $d$. Our results improve on the best previous\nalgorithms when $n\\gg d$, for all $p\\in[1,\\infty)$ except $p=2$. We also\nprovide a suite of improved results for finding well-conditioned bases via\nellipsoidal rounding, illustrating tradeoffs between running time and\nconditioning quality, including a one-pass conditioning algorithm for general\n$\\ell_p$ problems.\n  We also provide an empirical evaluation of implementations of our algorithms\nfor $p=1$, comparing them with related algorithms. Our empirical results show\nthat, in the asymptotic regime, the theory is a very good guide to the\npractical performance of these algorithms. Our algorithms use our faster\nconstructions of well-conditioned bases for $\\ell_p$ spaces and, for $p=1$, a\nfast subspace embedding of independent interest that we call the Fast Cauchy\nTransform: a distribution over matrices $\\Pi:\\mathbb{R}^n\\mapsto\n\\mathbb{R}^{O(d\\log d)}$, found obliviously to $A$, that approximately\npreserves the $\\ell_1$ norms: that is, with large probability, simultaneously\nfor all $x$, $\\|Ax\\|_1 \\approx \\|\\Pi Ax\\|_1$, with distortion $O(d^{2+\\eta})$,\nfor an arbitrarily small constant $\\eta>0$; and, moreover, $\\Pi A$ can be\ncomputed in $O(nd\\log d)$ time. The techniques underlying our Fast Cauchy\nTransform include fast Johnson-Lindenstrauss transforms, low-coherence\nmatrices, and rescaling by Cauchy random variables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 14:26:05 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 00:49:03 GMT"}, {"version": "v3", "created": "Sat, 5 Apr 2014 04:48:55 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""], ["Mahoney", "Michael W.", ""], ["Meng", "Xiangrui", ""], ["Woodruff", "David P.", ""]]}, {"id": "1207.4747", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "comments": "Appears in Proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013). 9 pages main text + 22 pages appendix. Changes\n  from v3 to v4: 1) Re-organized appendix; improved & clarified duality gap\n  proofs; re-drew all plots; 2) Changed convention for Cf definition; 3) Added\n  weighted averaging experiments + convergence results; 4) Clarified main text\n  and relationship with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized block-coordinate variant of the classic Frank-Wolfe\nalgorithm for convex optimization with block-separable constraints. Despite its\nlower iteration cost, we show that it achieves a similar convergence rate in\nduality gap as the full Frank-Wolfe algorithm. We also show that, when applied\nto the dual structural support vector machine (SVM) objective, this yields an\nonline algorithm that has the same low iteration complexity as primal\nstochastic subgradient methods. However, unlike stochastic subgradient methods,\nthe block-coordinate Frank-Wolfe algorithm allows us to compute the optimal\nstep-size and yields a computable duality gap guarantee. Our experiments\nindicate that this simple algorithm outperforms competing structural SVM\nsolvers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 18:02:41 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 18:03:32 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2012 19:25:10 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2013 13:26:51 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Jaggi", "Martin", ""], ["Schmidt", "Mark", ""], ["Pletscher", "Patrick", ""]]}, {"id": "1207.4748", "submitter": "Brian Eriksson", "authors": "Brian Eriksson", "title": "Hierarchical Clustering using Randomly Selected Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of hierarchical clustering items from pairwise similarities is\nfound across various scientific disciplines, from biology to networking. Often,\napplications of clustering techniques are limited by the cost of obtaining\nsimilarities between pairs of items. While prior work has been developed to\nreconstruct clustering using a significantly reduced set of pairwise\nsimilarities via adaptive measurements, these techniques are only applicable\nwhen choice of similarities are available to the user. In this paper, we\nexamine reconstructing hierarchical clustering under similarity observations\nat-random. We derive precise bounds which show that a significant fraction of\nthe hierarchical clustering can be recovered using fewer than all the pairwise\nsimilarities. We find that the correct hierarchical clustering down to a\nconstant fraction of the total number of items (i.e., clusters sized O(N)) can\nbe found using only O(N log N) randomly selected pairwise similarities in\nexpectation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 18:06:37 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Eriksson", "Brian", ""]]}, {"id": "1207.4814", "submitter": "Hung Bui", "authors": "Hung Hai Bui and Tuyen N. Huynh and Sebastian Riedel", "title": "Automorphism Groups of Graphical Models and Lifted Variational Inference", "comments": "Extended version of the paper to appear in Statistical Relational AI\n  (StaRAI-12) workshop at UAI '12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.CO stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the theory of group action, we first introduce the concept of the\nautomorphism group of an exponential family or a graphical model, thus\nformalizing the general notion of symmetry of a probabilistic model. This\nautomorphism group provides a precise mathematical framework for lifted\ninference in the general exponential family. Its group action partitions the\nset of random variables and feature functions into equivalent classes (called\norbits) having identical marginals and expectations. Then the inference problem\nis effectively reduced to that of computing marginals or expectations for each\nclass, thus avoiding the need to deal with each individual variable or feature.\nWe demonstrate the usefulness of this general framework in lifting two classes\nof variational approximation for MAP inference: local LP relaxation and local\nLP relaxation with cycle constraints; the latter yields the first lifted\ninference that operate on a bound tighter than local constraints. Initial\nexperimental results demonstrate that lifted MAP inference with cycle\nconstraints achieved the state of the art performance, obtaining much better\nobjective function values than local approximation while remaining relatively\nefficient.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 21:30:42 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Bui", "Hung Hai", ""], ["Huynh", "Tuyen N.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1207.4992", "submitter": "Pavel Bazovkin", "authors": "Tatjana Lange, Karl Mosler and Pavlo Mozharovskyi", "title": "Fast nonparametric classification based on data depth", "comments": null, "journal-ref": "Statistical Papers 55 (2014), 49-69", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A new procedure, called DDa-procedure, is developed to solve the problem of\nclassifying d-dimensional objects into q >= 2 classes. The procedure is\ncompletely nonparametric; it uses q-dimensional depth plots and a very\nefficient algorithm for discrimination analysis in the depth space [0,1]^q.\nSpecifically, the depth is the zonoid depth, and the algorithm is the\nalpha-procedure. In case of more than two classes several binary\nclassifications are performed and a majority rule is applied. Special\ntreatments are discussed for 'outsiders', that is, data having zero depth\nvector. The DDa-classifier is applied to simulated as well as real data, and\nthe results are compared with those of similar procedures that have been\nrecently proposed. In most cases the new procedure has comparable error rates,\nbut is much faster than other classification approaches, including the SVM.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2012 16:28:57 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 00:10:49 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Lange", "Tatjana", ""], ["Mosler", "Karl", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1207.5058", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Thomas S. Richardson, James M. Robins, and Robin Evans", "title": "Parameter and Structure Learning in Nested Markov Models", "comments": "To be presented at the UAI Workshop on Causal Structure Learning 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraints arising from DAG models with latent variables can be\nnaturally represented by means of acyclic directed mixed graphs (ADMGs). Such\ngraphs contain directed and bidirected arrows, and contain no directed cycles.\nDAGs with latent variables imply independence constraints in the distribution\nresulting from a 'fixing' operation, in which a joint distribution is divided\nby a conditional. This operation generalizes marginalizing and conditioning.\nSome of these constraints correspond to identifiable 'dormant' independence\nconstraints, with the well known 'Verma constraint' as one example. Recently,\nmodels defined by a set of the constraints arising after fixing from a DAG with\nlatents, were characterized via a recursive factorization and a nested Markov\nproperty. In addition, a parameterization was given in the discrete case. In\nthis paper we use this parameterization to describe a parameter fitting\nalgorithm, and a search and score structure learning algorithm for these nested\nMarkov models. We apply our algorithms to a variety of datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2012 20:52:47 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Shpitser", "Ilya", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""], ["Evans", "Robin", ""]]}, {"id": "1207.5136", "submitter": "Jonas Peters", "authors": "Jonas Peters, Dominik Janzing and Bernhard Sch\\\"olkopf", "title": "Causal Inference on Time Series using Structural Equation Models", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 26, 154-162,\n  2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference uses observations to infer the causal structure of the data\ngenerating system. We study a class of functional models that we call Time\nSeries Models with Independent Noise (TiMINo). These models require independent\nresidual time series, whereas traditional methods like Granger causality\nexploit the variance of residuals. There are two main contributions: (1)\nTheoretical: By restricting the model class (e.g. to additive noise) we can\nprovide a more general identifiability result than existing ones. This result\nincorporates lagged and instantaneous effects that can be nonlinear and do not\nneed to be faithful, and non-instantaneous feedbacks between the time series.\n(2) Practical: If there are no feedback loops between time series, we propose\nan algorithm based on non-linear independence tests of time series. When the\ndata are causally insufficient, or the data generating process does not satisfy\nthe model assumptions, this algorithm may still give partial results, but\nmostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks\nis possible, but not discussed. It outperforms existing methods on artificial\nand real data. Code can be provided upon request.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2012 13:31:56 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1207.5208", "submitter": "Francis Maes", "authors": "Francis Maes and Damien Ernst and Louis Wehenkel", "title": "Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed\n  Bandit Case", "comments": "16 pages, Springer Selection of papers of ICAART'12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration/exploitation (E/E) dilemma arises naturally in many subfields\nof Science. Multi-armed bandit problems formalize this dilemma in its canonical\nform. Most current research in this field focuses on generic solutions that can\nbe applied to a wide range of problems. However, in practice, it is often the\ncase that a form of prior information is available about the specific class of\ntarget problems. Prior knowledge is rarely used in current solutions due to the\nlack of a systematic approach to incorporate it into the E/E strategy.\n  To address a specific class of E/E problems, we propose to proceed in three\nsteps: (i) model prior knowledge in the form of a probability distribution over\nthe target class of E/E problems; (ii) choose a large hypothesis space of\ncandidate E/E strategies; and (iii), solve an optimization problem to find a\ncandidate E/E strategy of maximal average performance over a sample of problems\ndrawn from the prior distribution.\n  We illustrate this meta-learning approach with two different hypothesis\nspaces: one where E/E strategies are numerically parameterized and another\nwhere E/E strategies are represented as small symbolic formulas. We propose\nappropriate optimization algorithms for both cases. Our experiments, with\ntwo-armed Bernoulli bandit problems and various playing budgets, show that the\nmeta-learnt E/E strategies outperform generic strategies of the literature\n(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the\nrobustness of the learnt E/E strategies, by tests carried out on arms whose\nrewards follow a truncated Gaussian distribution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 09:34:49 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Maes", "Francis", ""], ["Ernst", "Damien", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1207.5259", "submitter": "Sebastien Bubeck", "authors": "Sebastien Bubeck and Damien Ernst and Aurelien Garivier", "title": "Optimal discovery with probabilistic expert advice: finite time analysis\n  and macroscopic optimality", "comments": "arXiv admin note: substantial text overlap with arXiv:1110.5447", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an original problem that arises from the issue of security\nanalysis of a power system and that we name optimal discovery with\nprobabilistic expert advice. We address it with an algorithm based on the\noptimistic paradigm and on the Good-Turing missing mass estimator. We prove two\ndifferent regret bounds on the performance of this algorithm under weak\nassumptions on the probabilistic experts. Under more restrictive hypotheses, we\nalso prove a macroscopic optimality result, comparing the algorithm both with\nan oracle strategy and with uniform sampling. Finally, we provide numerical\nexperiments illustrating these theoretical findings.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 21:01:09 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 21:45:35 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2013 21:47:06 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Ernst", "Damien", ""], ["Garivier", "Aurelien", ""]]}, {"id": "1207.5437", "submitter": "Yiming Ying", "authors": "Qiong Cao, Zheng-Chu Guo and Yiming Ying", "title": "Generalization Bounds for Metric and Similarity Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, metric learning and similarity learning have attracted a large\namount of interest. Many models and optimisation algorithms have been proposed.\nHowever, there is relatively little work on the generalization analysis of such\nmethods. In this paper, we derive novel generalization bounds of metric and\nsimilarity learning. In particular, we first show that the generalization\nanalysis reduces to the estimation of the Rademacher average over\n\"sums-of-i.i.d.\" sample-blocks related to the specific matrix norm. Then, we\nderive generalization bounds for metric/similarity learning with different\nmatrix-norm regularisers by estimating their specific Rademacher complexities.\nOur analysis indicates that sparse metric/similarity learning with $L^1$-norm\nregularisation could lead to significantly better bounds than those with\nFrobenius-norm regularisation. Our novel generalization analysis develops and\nrefines the techniques of U-statistics and Rademacher complexity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 16:20:05 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2013 10:56:35 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Cao", "Qiong", ""], ["Guo", "Zheng-Chu", ""], ["Ying", "Yiming", ""]]}, {"id": "1207.5451", "submitter": "Nicolas Dobigeon", "authors": "Yoann Altmann and Nicolas Dobigeon and Steve McLaughlin and Jean-Yves\n  Tourneret", "title": "Nonlinear spectral unmixing of hyperspectral images using Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2245127", "report-no": null, "categories": "stat.ML physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised algorithm for nonlinear unmixing of\nhyperspectral images. The proposed model assumes that the pixel reflectances\nresult from a nonlinear function of the abundance vectors associated with the\npure spectral components. We assume that the spectral signatures of the pure\ncomponents and the nonlinear function are unknown. The first step of the\nproposed method consists of the Bayesian estimation of the abundance vectors\nfor all the image pixels and the nonlinear function relating the abundance\nvectors to the observations. The endmembers are subsequently estimated using\nGaussian process regression. The performance of the unmixing strategy is\nevaluated with simulations conducted on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 16:51:10 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Altmann", "Yoann", ""], ["Dobigeon", "Nicolas", ""], ["McLaughlin", "Steve", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1207.5554", "submitter": "Mahdi Milani Fard", "authors": "Mahdi Milani Fard, Yuri Grinberg, Amir-massoud Farahmand, Joelle\n  Pineau, Doina Precup", "title": "Bellman Error Based Feature Generation using Random Projections on\n  Sparse Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatic generation of features for value function\napproximation. Bellman Error Basis Functions (BEBFs) have been shown to improve\nthe error of policy evaluation with function approximation, with a convergence\nrate similar to that of value iteration. We propose a simple, fast and robust\nalgorithm based on random projections to generate BEBFs for sparse feature\nspaces. We provide a finite sample analysis of the proposed method, and prove\nthat projections logarithmic in the dimension of the original space are enough\nto guarantee contraction in the error. Empirical results demonstrate the\nstrength of this method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 22:39:51 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2012 22:53:01 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2012 22:51:40 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Fard", "Mahdi Milani", ""], ["Grinberg", "Yuri", ""], ["Farahmand", "Amir-massoud", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""]]}, {"id": "1207.5871", "submitter": "Haizhang Zhang", "authors": "Rui Wang, Haizhang Zhang", "title": "Optimal Sampling Points in Reproducing Kernel Hilbert Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent developments of basis pursuit and compressed sensing seek to\nextract information from as few samples as possible. In such applications,\nsince the number of samples is restricted, one should deploy the sampling\npoints wisely. We are motivated to study the optimal distribution of finite\nsampling points. Formulation under the framework of optimal reconstruction\nyields a minimization problem. In the discrete case, we estimate the distance\nbetween the optimal subspace resulting from a general Karhunen-Loeve transform\nand the kernel space to obtain another algorithm that is computationally\nfavorable. Numerical experiments are then presented to illustrate the\nperformance of the algorithms for the searching of optimal sampling points.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 02:43:12 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Wang", "Rui", ""], ["Zhang", "Haizhang", ""]]}, {"id": "1207.6005", "submitter": "Coryn Bailer-Jones", "authors": "C. Liu (MPIA, Heidelberg), C. A. L. Bailer-Jones (MPIA, Heidelberg),\n  R. Sordo (INAF, Padova), A. Vallenari (INAF, Padova), R. Borrachero (U.\n  Barcelona), X. Luri (U. Barcelona), P. Sartoretti (Obs Paris)", "title": "The expected performance of stellar parametrization with Gaia\n  spectrophotometry", "comments": "MNRAS, in press. Minor corrections made in v2", "journal-ref": null, "doi": "10.1111/j.1365-2966.2012.21797.x", "report-no": null, "categories": "astro-ph.IM astro-ph.GA physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaia will obtain astrometry and spectrophotometry for essentially all sources\nin the sky down to a broad band magnitude limit of G=20, an expected yield of\n10^9 stars. Its main scientific objective is to reveal the formation and\nevolution of our Galaxy through chemo-dynamical analysis. In addition to\ninferring positions, parallaxes and proper motions from the astrometry, we must\nalso infer the astrophysical parameters of the stars from the\nspectrophotometry, the BP/RP spectrum. Here we investigate the performance of\nthree different algorithms (SVM, ILIUM, Aeneas) for estimating the effective\ntemperature, line-of-sight interstellar extinction, metallicity and surface\ngravity of A-M stars over a wide range of these parameters and over the full\nmagnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas,\ninfers the posterior probability density function over all parameters, and can\noptionally take into account the parallax and the Hertzsprung-Russell diagram\nto improve the estimates. For all algorithms the accuracy of estimation depends\non G and on the value of the parameters themselves, so a broad summary of\nperformance is only approximate. For stars at G=15 with less than two\nmagnitudes extinction, we expect to be able to estimate Teff to within 1%, logg\nto 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RP\nspectrum (mean absolute error statistics are quoted). Performance degrades at\nlarger extinctions, but not always by a large amount. Extinction can be\nestimated to an accuracy of 0.05-0.2mag for stars across the full parameter\nrange with a priori unknown extinction between 0 and 10mag. Performance\ndegrades at fainter magnitudes, but even at G=19 we can estimate logg to better\nthan 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKM\nstars, for extinctions below 1mag.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 14:22:01 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 06:27:45 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Liu", "C.", "", "MPIA, Heidelberg"], ["Bailer-Jones", "C. A. L.", "", "MPIA, Heidelberg"], ["Sordo", "R.", "", "INAF, Padova"], ["Vallenari", "A.", "", "INAF, Padova"], ["Borrachero", "R.", "", "U.\n  Barcelona"], ["Luri", "X.", "", "U. Barcelona"], ["Sartoretti", "P.", "", "Obs Paris"]]}, {"id": "1207.6076", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu", "title": "Equivalence of distance-based and RKHS-based statistics in hypothesis\n  testing", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1140 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2263-2291", "doi": "10.1214/13-AOS1140", "report-no": "IMS-AOS-AOS1140", "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unifying framework linking two classes of statistics used in\ntwo-sample and independence testing: on the one hand, the energy distances and\ndistance covariances from the statistics literature; on the other, maximum mean\ndiscrepancies (MMD), that is, distances between embeddings of distributions to\nreproducing kernel Hilbert spaces (RKHS), as established in machine learning.\nIn the case where the energy distance is computed with a semimetric of negative\ntype, a positive definite kernel, termed distance kernel, may be defined such\nthat the MMD corresponds exactly to the energy distance. Conversely, for any\npositive definite kernel, we can interpret the MMD as energy distance with\nrespect to some negative-type semimetric. This equivalence readily extends to\ndistance covariance using kernels on the product space. We determine the class\nof probability distributions for which the test statistics are consistent\nagainst all alternatives. Finally, we investigate the performance of the family\nof distance kernels in two-sample and independence tests: we show in particular\nthat the energy distance most commonly employed in statistics is just one\nmember of a parametric family of kernels, and that other choices from this\nfamily can yield more powerful tests.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 18:17:20 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 11:29:37 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2013 12:22:53 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Sriperumbudur", "Bharath", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1207.6083", "submitter": "Alex Kulesza", "authors": "Alex Kulesza, Ben Taskar", "title": "Determinantal point processes for machine learning", "comments": "120 pages", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 5: No 2-3, pp\n  123-286", "doi": "10.1561/2200000044", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Determinantal point processes (DPPs) are elegant probabilistic models of\nrepulsion that arise in quantum physics and random matrix theory. In contrast\nto traditional structured models like Markov random fields, which become\nintractable and hard to approximate in the presence of negative correlations,\nDPPs offer efficient and exact algorithms for sampling, marginalization,\nconditioning, and other inference tasks. We provide a gentle introduction to\nDPPs, focusing on the intuitions, algorithms, and extensions that are most\nrelevant to the machine learning community, and show how DPPs can be applied to\nreal-world applications like finding diverse sets of high-quality search\nresults, building informative summaries by selecting diverse sentences from\ndocuments, modeling non-overlapping human poses in images or video, and\nautomatically building timelines of important news stories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 18:45:43 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 16:10:06 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2012 14:05:25 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2013 20:43:53 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Kulesza", "Alex", ""], ["Taskar", "Ben", ""]]}, {"id": "1207.6379", "submitter": "Jose Bento", "authors": "Jos\\'e Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis", "title": "Identifying Users From Their Rating Patterns", "comments": "Winner of the 2011 Challenge on Context-Aware Movie Recommendation\n  (RecSys 2011 - CAMRa2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track\n2) for context-aware movie recommendation systems. The train dataset comprises\n4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the\nhousehold groupings of a subset of the users. The test dataset comprises 5,450\nratings for which the user label is missing, but the household label is\nprovided. The challenge required to identify the user labels for the ratings in\nthe test set. Our main finding is that temporal information (time labels of the\nratings) is significantly more useful for achieving this objective than the\nuser preferences (the actual ratings). Using a model that leverages on this\nfact, we are able to identify users within a known household with an accuracy\nof approximately 96% (i.e. misclassification rate around 4%).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 19:27:03 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Fawaz", "Nadia", ""], ["Montanari", "Andrea", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "1207.6430", "submitter": "Christoph Brune", "authors": "Braxton Osting and Christoph Brune and Stanley J. Osher", "title": "Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs", "comments": "31 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "UCLA CAM report 12-32", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 23:14:34 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 08:31:57 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Osting", "Braxton", ""], ["Brune", "Christoph", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1207.6684", "submitter": "Yiyuan She", "authors": "Yiyuan She, Huanghuang Li, Jiangping Wang, and Dapeng Wu", "title": "Group Iterative Spectrum Thresholding for Super-Resolution Sparse\n  Spectral Selection", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2281303", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sparsity-based algorithms are proposed for super-resolution\nspectrum estimation. However, to achieve adequately high resolution in\nreal-world signal analysis, the dictionary atoms have to be close to each other\nin frequency, thereby resulting in a coherent design. The popular convex\ncompressed sensing methods break down in presence of high coherence and large\nnoise. We propose a new regularization approach to handle model collinearity\nand obtain parsimonious frequency selection simultaneously. It takes advantage\nof the pairing structure of sine and cosine atoms in the frequency dictionary.\nA probabilistic spectrum screening is also developed for fast computation in\nhigh dimensions. A data-resampling version of high-dimensional Bayesian\nInformation Criterion is used to determine the regularization parameters.\nExperiments show the efficacy and efficiency of the proposed algorithms in\nchallenging situations with small sample size, high frequency resolution, and\nlow signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2012 07:01:09 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 20:57:12 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["She", "Yiyuan", ""], ["Li", "Huanghuang", ""], ["Wang", "Jiangping", ""], ["Wu", "Dapeng", ""]]}, {"id": "1207.6745", "submitter": "Daniel Sussman", "authors": "Daniel L. Sussman, Minh Tang, Carey E. Priebe", "title": "Universally Consistent Latent Position Estimation and Vertex\n  Classification for Random Dot Product Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that, using the eigen-decomposition of the adjacency\nmatrix, we can consistently estimate latent positions for random dot product\ngraphs provided the latent positions are i.i.d. from some distribution. If\nclass labels are observed for a number of vertices tending to infinity, then we\nshow that the remaining vertices can be classified with error converging to\nBayes optimal using the $k$-nearest-neighbors classification rule. We evaluate\nthe proposed methods on simulated data and a graph derived from Wikipedia.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2012 04:29:03 GMT"}], "update_date": "2012-07-31", "authors_parsed": [["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1207.7253", "submitter": "S\\'ebastien Gigu\\`ere", "authors": "S\\'ebastien Gigu\\`ere, Mario Marchand, Fran\\c{c}ois Laviolette,\n  Alexandre Drouin and Jacques Corbeil", "title": "Learning a peptide-protein binding affinity predictor with kernel ridge\n  regression", "comments": "22 pages, 4 figures, 5 tables", "journal-ref": "BMC Bioinformatics 2013, 14:82", "doi": "10.1186/1471-2105-14-82", "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a specialized string kernel for small bio-molecules, peptides and\npseudo-sequences of binding interfaces. The kernel incorporates\nphysico-chemical properties of amino acids and elegantly generalize eight\nkernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the\nRadial Basis Function. We provide a low complexity dynamic programming\nalgorithm for the exact computation of the kernel and a linear time algorithm\nfor it's approximation. Combined with kernel ridge regression and SupCK, a\nnovel binding pocket kernel, the proposed kernel yields biologically relevant\nand good prediction accuracy on the PepX database. For the first time, a\nmachine learning predictor is capable of accurately predicting the binding\naffinity of any peptide to any protein. The method was also applied to both\nsingle-target and pan-specific Major Histocompatibility Complex class II\nbenchmark datasets and three Quantitative Structure Affinity Model benchmark\ndatasets.\n  On all benchmarks, our method significantly (p-value < 0.057) outperforms the\ncurrent state-of-the-art methods at predicting peptide-protein binding\naffinities. The proposed approach is flexible and can be applied to predict any\nquantitative biological activity. The method should be of value to a large\nsegment of the research community with the potential to accelerate\npeptide-based drug and vaccine development.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 14:11:31 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Gigu\u00e8re", "S\u00e9bastien", ""], ["Marchand", "Mario", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Drouin", "Alexandre", ""], ["Corbeil", "Jacques", ""]]}]