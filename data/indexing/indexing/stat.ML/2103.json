[{"id": "2103.00025", "submitter": "Peide Li", "authors": "Peide Li and Rejaul Karim and Tapabrata Maiti", "title": "TEC: Tensor Ensemble Classifier for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor (multidimensional array) classification problem has become very\npopular in modern applications such as image recognition and high dimensional\nspatio-temporal data analysis. Support Tensor Machine (STM) classifier, which\nis extended from the support vector machine, takes CANDECOMP / Parafac (CP)\nform of tensor data as input and predicts the data labels. The\ndistribution-free and statistically consistent properties of STM highlight its\npotential in successfully handling wide varieties of data applications.\nTraining a STM can be computationally expensive with high-dimensional tensors.\nHowever, reducing the size of tensor with a random projection technique can\nreduce the computational time and cost, making it feasible to handle large size\ntensors on regular machines. We name an STM estimated with randomly projected\ntensor as Random Projection-based Support Tensor Machine (RPSTM). In this work,\nwe propose a Tensor Ensemble Classifier (TEC), which aggregates multiple RPSTMs\nfor big tensor classification. TEC utilizes the ensemble idea to minimize the\nexcessive classification risk brought by random projection, providing\nstatistically consistent predictions while taking the computational advantage\nof RPSTM. Since each RPSTM can be estimated independently, TEC can further take\nadvantage of parallel computing techniques and be more computationally\nefficient. The theoretical and numerical results demonstrate the decent\nperformance of TEC model in high-dimensional tensor classification problems.\nThe model prediction is statistically consistent as its risk is shown to\nconverge to the optimal Bayes risk. Besides, we highlight the trade-off between\nthe computational cost and the prediction risk for TEC model. The method is\nvalidated by extensive simulation and a real data example. We prepare a python\npackage for applying TEC, which is available at our GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:15:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Peide", ""], ["Karim", "Rejaul", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2103.00034", "submitter": "Hunter Lang", "authors": "Hunter Lang, Aravind Reddy, David Sontag, Aravindan Vijayaraghavan", "title": "Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference\n  on Noisy Stable Instances", "comments": "25 pages, 2 figures, 2 tables. To appear in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several works have shown that perturbation stable instances of the MAP\ninference problem in Potts models can be solved exactly using a natural linear\nprogramming (LP) relaxation. However, most of these works give few (or no)\nguarantees for the LP solutions on instances that do not satisfy the relatively\nstrict perturbation stability definitions. In this work, we go beyond these\nstability results by showing that the LP approximately recovers the MAP\nsolution of a stable instance even after the instance is corrupted by noise.\nThis \"noisy stable\" model realistically fits with practical MAP inference\nproblems: we design an algorithm for finding \"close\" stable instances, and show\nthat several real-world instances from computer vision have nearby instances\nthat are perturbation stable. These results suggest a new theoretical\nexplanation for the excellent performance of this LP relaxation in practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 20:01:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lang", "Hunter", ""], ["Reddy", "Aravind", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2103.00065", "submitter": "Jeremy Cohen", "authors": "Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, Ameet\n  Talwalkar", "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of\n  Stability", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We empirically demonstrate that full-batch gradient descent on neural network\ntraining objectives typically operates in a regime we call the Edge of\nStability. In this regime, the maximum eigenvalue of the training loss Hessian\nhovers just above the numerical value $2 / \\text{(step size)}$, and the\ntraining loss behaves non-monotonically over short timescales, yet consistently\ndecreases over long timescales. Since this behavior is inconsistent with\nseveral widespread presumptions in the field of optimization, our findings\nraise questions as to whether these presumptions are relevant to neural network\ntraining. We hope that our findings will inspire future efforts aimed at\nrigorously understanding optimization at the Edge of Stability. Code is\navailable at https://github.com/locuslab/edge-of-stability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:08:19 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 20:35:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Cohen", "Jeremy M.", ""], ["Kaur", "Simran", ""], ["Li", "Yuanzhi", ""], ["Kolter", "J. Zico", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2103.00083", "submitter": "Taesup Kim", "authors": "Taesup Kim, Rasool Fakoor, Jonas Mueller, Ryan J. Tibshirani,\n  Alexander J. Smola", "title": "Deep Quantile Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conditional quantile estimation is a key statistical learning challenge\nmotivated by the need to quantify uncertainty in predictions or to model a\ndiverse population without being overly reductive. As such, many models have\nbeen developed for this problem. Adopting a meta viewpoint, we propose a\ngeneral framework (inspired by neural network optimization) for aggregating any\nnumber of conditional quantile models in order to boost predictive accuracy. We\nconsider weighted ensembling strategies of increasing flexibility where the\nweights may vary over individual models, quantile levels, and feature values.\nAn appeal of our approach is its portability: we ensure that estimated\nquantiles at adjacent levels do not cross by applying simple transformations\nthrough which gradients can be backpropagated, and this allows us to leverage\nthe modern deep learning toolkit for building quantile ensembles. Our\nexperiments confirm that ensembling can lead to big gains in accuracy, even\nwhen the constituent models are themselves powerful and flexible.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:21:16 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 01:23:59 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 17:11:29 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kim", "Taesup", ""], ["Fakoor", "Rasool", ""], ["Mueller", "Jonas", ""], ["Tibshirani", "Ryan J.", ""], ["Smola", "Alexander J.", ""]]}, {"id": "2103.00107", "submitter": "Tadashi Kozuno", "authors": "Tadashi Kozuno, Yunhao Tang, Mark Rowland, R\\'emi Munos, Steven\n  Kapturowski, Will Dabney, Michal Valko, David Abel", "title": "Revisiting Peng's Q($\\lambda$) for Modern Reinforcement Learning", "comments": "26 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Off-policy multi-step reinforcement learning algorithms consist of\nconservative and non-conservative algorithms: the former actively cut traces,\nwhereas the latter do not. Recently, Munos et al. (2016) proved the convergence\nof conservative algorithms to an optimal Q-function. In contrast,\nnon-conservative algorithms are thought to be unsafe and have a limited or no\ntheoretical guarantee. Nonetheless, recent studies have shown that\nnon-conservative algorithms empirically outperform conservative ones. Motivated\nby the empirical results and the lack of theory, we carry out theoretical\nanalyses of Peng's Q($\\lambda$), a representative example of non-conservative\nalgorithms. We prove that it also converges to an optimal policy provided that\nthe behavior policy slowly tracks a greedy policy in a way similar to\nconservative policy iteration. Such a result has been conjectured to be true\nbut has not been proven. We also experiment with Peng's Q($\\lambda$) in complex\ncontinuous control tasks, confirming that Peng's Q($\\lambda$) often outperforms\nconservative algorithms despite its simplicity. These results indicate that\nPeng's Q($\\lambda$), which was thought to be unsafe, is a theoretically-sound\nand practically effective algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 02:29:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kozuno", "Tadashi", ""], ["Tang", "Yunhao", ""], ["Rowland", "Mark", ""], ["Munos", "R\u00e9mi", ""], ["Kapturowski", "Steven", ""], ["Dabney", "Will", ""], ["Valko", "Michal", ""], ["Abel", "David", ""]]}, {"id": "2103.00136", "submitter": "Takeshi Teshima", "authors": "Takeshi Teshima and Masashi Sugiyama", "title": "Incorporating Causal Graphical Prior Knowledge into Predictive Modeling\n  via Simple Data Augmentation", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Causal graphs (CGs) are compact representations of the knowledge of the data\ngenerating processes behind the data distributions. When a CG is available,\ne.g., from the domain knowledge, we can infer the conditional independence (CI)\nrelations that should hold in the data distribution. However, it is not\nstraightforward how to incorporate this knowledge into predictive modeling. In\nthis work, we propose a model-agnostic data augmentation method that allows us\nto exploit the prior knowledge of the CI encoded in a CG for supervised machine\nlearning. We theoretically justify the proposed method by providing an excess\nrisk bound indicating that the proposed method suppresses overfitting by\nreducing the apparent complexity of the predictor hypothesis class. Using\nreal-world data with CGs provided by domain experts, we experimentally show\nthat the proposed method is effective in improving the prediction accuracy,\nespecially in the small-data regime.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:13:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Teshima", "Takeshi", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.00139", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Om Pandey, Pooyan Jamshidi", "title": "Scalable Causal Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important problems in transfer learning is the task of domain\nadaptation, where the goal is to apply an algorithm trained in one or more\nsource domains to a different (but related) target domain. This paper deals\nwith domain adaptation in the presence of covariate shift while there exist\ninvariances across domains. A main limitation of existing causal inference\nmethods for solving this problem is scalability. To overcome this difficulty,\nwe propose SCTL, an algorithm that avoids an exhaustive search and identifies\ninvariant causal features across the source and target domains based on Markov\nblanket discovery. SCTL does not require to have prior knowledge of the causal\nstructure, the type of interventions, or the intervention targets. There is an\nintrinsic locality associated with SCTL that makes SCTL practically scalable\nand robust because local causal discovery increases the power of computational\nindependence tests and makes the task of domain adaptation computationally\ntractable. We show the scalability and robustness of SCTL for domain adaptation\nusing synthetic and real data sets in low-dimensional and high-dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:25:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Pandey", "Om", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "2103.00222", "submitter": "Laurence Aitchison", "authors": "Ali Unlu, Laurence Aitchison", "title": "Variational Laplace for Bayesian neural networks", "comments": "Accidental resubmission of new version of arXiv:2011.10443", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 14:06:29 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 14:56:18 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 07:52:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Unlu", "Ali", ""], ["Aitchison", "Laurence", ""]]}, {"id": "2103.00299", "submitter": "Daniil Tiapkin", "authors": "Daniil Tiapkin, Alexander Gasnikov", "title": "Parallel Stochastic Mirror Descent for MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the optimal policy for infinite-horizon\nMarkov decision processes (MDPs). For this purpose, some variant of Stochastic\nMirror Descent is proposed for convex programming problems with\nLipschitz-continuous functionals. An important detail is the ability to use\ninexact values of functional constraints. We analyze this algorithm in a\ngeneral case and obtain an estimate of the convergence rate that does not\naccumulate errors during the operation of the method. Using this algorithm, we\nget the first parallel algorithm for average-reward MDPs with a generative\nmodel. One of the main features of the presented method is low communication\ncosts in a distributed centralized setting.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:28:39 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 22:04:08 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 08:17:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tiapkin", "Daniil", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2103.00349", "submitter": "David Eriksson", "authors": "David Eriksson and Martin Jankowiak", "title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned\n  Subspaces", "comments": "To appear in UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a powerful paradigm for efficient optimization\nof black-box objective functions. High-dimensional BO presents a particular\nchallenge, in part because the curse of dimensionality makes it difficult to\ndefine -- as well as do inference over -- a suitable class of surrogate models.\nWe argue that Gaussian process surrogate models defined on sparse axis-aligned\nsubspaces offer an attractive compromise between flexibility and parsimony. We\ndemonstrate that our approach, which relies on Hamiltonian Monte Carlo for\ninference, can rapidly identify sparse subspaces relevant to modeling the\nunknown objective function, enabling sample-efficient high-dimensional BO. In\nan extensive suite of experiments comparing to existing methods for\nhigh-dimensional BO we demonstrate that our algorithm, Sparse Axis-Aligned\nSubspace BO (SAASBO), achieves excellent performance on several synthetic and\nreal-world problems without the need to set problem-specific hyperparameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 23:06:24 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 05:54:46 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Eriksson", "David", ""], ["Jankowiak", "Martin", ""]]}, {"id": "2103.00373", "submitter": "Xingcai Zhou", "authors": "Xingcai Zhou, Le Chang, Pengfei Xu and Shaogao Lv", "title": "Communication-efficient Byzantine-robust distributed learning with\n  statistical guarantee", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication efficiency and robustness are two major issues in modern\ndistributed learning framework. This is due to the practical situations where\nsome computing nodes may have limited communication power or may behave\nadversarial behaviors. To address the two issues simultaneously, this paper\ndevelops two communication-efficient and robust distributed learning algorithms\nfor convex problems. Our motivation is based on surrogate likelihood framework\nand the median and trimmed mean operations. Particularly, the proposed\nalgorithms are provably robust against Byzantine failures, and also achieve\noptimal statistical rates for strong convex losses and convex (non-smooth)\npenalties. For typical statistical models such as generalized linear models,\nour results show that statistical errors dominate optimization errors in finite\niterations. Simulated and real data experiments are conducted to demonstrate\nthe numerical performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:38:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhou", "Xingcai", ""], ["Chang", "Le", ""], ["Xu", "Pengfei", ""], ["Lv", "Shaogao", ""]]}, {"id": "2103.00381", "submitter": "Shihua Zhang", "authors": "Penglong Zhai and Shihua Zhang", "title": "Adversarial Information Bottleneck", "comments": "10 pages,7 figures,2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The information bottleneck (IB) principle has been adopted to explain deep\nlearning in terms of information compression and prediction, which are balanced\nby a trade-off hyperparameter. How to optimize the IB principle for better\nrobustness and figure out the effects of compression through the trade-off\nhyperparameter are two challenging problems. Previous methods attempted to\noptimize the IB principle by introducing random noise into learning the\nrepresentation and achieved state-of-the-art performance in the nuisance\ninformation compression and semantic information extraction. However, their\nperformance on resisting adversarial perturbations is far less impressive. To\nthis end, we propose an adversarial information bottleneck (AIB) method without\nany explicit assumptions about the underlying distribution of the\nrepresentations, which can be optimized effectively by solving a Min-Max\noptimization problem. Numerical experiments on synthetic and real-world\ndatasets demonstrate its effectiveness on learning more invariant\nrepresentations and mitigating adversarial perturbations compared to several\ncompeting IB methods. In addition, we analyse the adversarial robustness of\ndiverse IB methods contrasting with their IB curves, and reveal that IB models\nwith the hyperparameter $\\beta$ corresponding to the knee point in the IB curve\nachieve the best trade-off between compression and prediction, and has best\nrobustness against various attacks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 03:14:56 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 03:42:00 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhai", "Penglong", ""], ["Zhang", "Shihua", ""]]}, {"id": "2103.00393", "submitter": "Luhuan Wu", "authors": "Luhuan Wu, Andrew Miller, Lauren Anderson, Geoff Pleiss, David Blei,\n  John Cunningham", "title": "Hierarchical Inducing Point Gaussian Process for Inter-domain\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the general problem of inter-domain Gaussian Processes (GPs):\nproblems where the GP realization and the noisy observations of that\nrealization lie on different domains. When the mapping between those domains is\nlinear, such as integration or differentiation, inference is still closed form.\nHowever, many of the scaling and approximation techniques that our community\nhas developed do not apply to this setting. In this work, we introduce the\nhierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference\nmethod that enables us to improve the approximation accuracy by increasing the\nnumber of inducing points to the millions. HIP-GP, which relies on inducing\npoints with grid structure and a stationary kernel assumption, is suitable for\nlow-dimensional problems. In developing HIP-GP, we introduce (1) a fast\nwhitening strategy, and (2) a novel preconditioner for conjugate gradients\nwhich can be helpful in general GP settings. Our code is available at https:\n//github.com/cunningham-lab/hipgp.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 04:20:58 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 06:02:27 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Wu", "Luhuan", ""], ["Miller", "Andrew", ""], ["Anderson", "Lauren", ""], ["Pleiss", "Geoff", ""], ["Blei", "David", ""], ["Cunningham", "John", ""]]}, {"id": "2103.00394", "submitter": "Yixing Zhang", "authors": "Yixing Zhang, Xiuyuan Cheng, Galen Reeves", "title": "Convergence of Gaussian-smoothed optimal transport distance with\n  sub-gamma distributions and dependent samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Gaussian-smoothed optimal transport (GOT) framework, recently proposed by\nGoldfeld et al., scales to high dimensions in estimation and provides an\nalternative to entropy regularization. This paper provides convergence\nguarantees for estimating the GOT distance under more general settings. For the\nGaussian-smoothed $p$-Wasserstein distance in $d$ dimensions, our results\nrequire only the existence of a moment greater than $d + 2p$. For the special\ncase of sub-gamma distributions, we quantify the dependence on the dimension\n$d$ and establish a phase transition with respect to the scale parameter. We\nalso prove convergence for dependent samples, only requiring a condition on the\npairwise dependence of the samples measured by the covariance of the feature\nmap of a kernel space.\n  A key step in our analysis is to show that the GOT distance is dominated by a\nfamily of kernel maximum mean discrepancy (MMD) distances with a kernel that\ndepends on the cost function as well as the amount of Gaussian smoothing. This\ninsight provides further interpretability for the GOT framework and also\nintroduces a class of kernel MMD distances with desirable properties. The\ntheoretical results are supported by numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 04:30:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "Yixing", ""], ["Cheng", "Xiuyuan", ""], ["Reeves", "Galen", ""]]}, {"id": "2103.00396", "submitter": "Junru Luo", "authors": "Junru Luo, Hong Qiao and Bo Zhang", "title": "A Minimax Probability Machine for Non-Decomposable Performance Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imbalanced classification tasks are widespread in many real-world\napplications. For such classification tasks, in comparison with the accuracy\nrate, it is usually much more appropriate to use non-decomposable performance\nmeasures such as the Area Under the receiver operating characteristic Curve\n(AUC) and the $F_\\beta$ measure as the classification criterion since the label\nclass is imbalanced. On the other hand, the minimax probability machine is a\npopular method for binary classification problems and aims at learning a linear\nclassifier by maximizing the accuracy rate, which makes it unsuitable to deal\nwith imbalanced classification tasks. The purpose of this paper is to develop a\nnew minimax probability machine for the $F_\\beta$ measure, called MPMF, which\ncan be used to deal with imbalanced classification tasks. A brief discussion is\nalso given on how to extend the MPMF model for several other non-decomposable\nperformance measures listed in the paper. To solve the MPMF model effectively,\nwe derive its equivalent form which can then be solved by an alternating\ndescent method to learn a linear classifier. Further, the kernel trick is\nemployed to derive a nonlinear MPMF model to learn a nonlinear classifier.\nSeveral experiments on real-world benchmark datasets demonstrate the\neffectiveness of our new model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 04:58:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 06:48:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luo", "Junru", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""]]}, {"id": "2103.00445", "submitter": "Oren Peer", "authors": "Oren Peer, Chen Tessler, Nadav Merlis, Ron Meir", "title": "Ensemble Bootstrapping for Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning (QL), a common reinforcement learning algorithm, suffers from\nover-estimation bias due to the maximization term in the optimal Bellman\noperator. This bias may lead to sub-optimal behavior. Double-Q-learning tackles\nthis issue by utilizing two estimators, yet results in an under-estimation\nbias. Similar to over-estimation in Q-learning, in certain scenarios, the\nunder-estimation bias may degrade performance. In this work, we introduce a new\nbias-reduced algorithm called Ensemble Bootstrapped Q-Learning (EBQL), a\nnatural extension of Double-Q-learning to ensembles. We analyze our method both\ntheoretically and empirically. Theoretically, we prove that EBQL-like updates\nyield lower MSE when estimating the maximal mean of a set of independent random\nvariables. Empirically, we show that there exist domains where both over and\nunder-estimation result in sub-optimal performance. Finally, We demonstrate the\nsuperior performance of a deep RL variant of EBQL over other deep QL algorithms\nfor a suite of ATARI games.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 10:19:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 11:01:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Peer", "Oren", ""], ["Tessler", "Chen", ""], ["Merlis", "Nadav", ""], ["Meir", "Ron", ""]]}, {"id": "2103.00476", "submitter": "Shi Gu", "authors": "Shikuang Deng, Shi Gu", "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are biology-inspired artificial neural\nnetworks (ANNs) that comprise of spiking neurons to process asynchronous\ndiscrete signals. While more efficient in power consumption and inference speed\non the neuromorphic hardware, SNNs are usually difficult to train directly from\nscratch with spikes due to the discreteness. As an alternative, many efforts\nhave been devoted to converting conventional ANNs into SNNs by copying the\nweights from ANNs and adjusting the spiking threshold potential of neurons in\nSNNs. Researchers have designed new SNN architectures and conversion algorithms\nto diminish the conversion error. However, an effective conversion should\naddress the difference between the SNN and ANN architectures with an efficient\napproximation \\DSK{of} the loss function, which is missing in the field. In\nthis work, we analyze the conversion error by recursive reduction to layer-wise\nsummation and propose a novel strategic pipeline that transfers the weights to\nthe target SNN by combining threshold balance and soft-reset mechanisms. This\npipeline enables almost no accuracy loss between the converted SNNs and\nconventional ANNs with only $\\sim1/10$ of the typical SNN simulation time. Our\nmethod is promising to get implanted onto embedded platforms with better\nsupport of SNNs with limited energy and memory.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:04:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Deng", "Shikuang", ""], ["Gu", "Shi", ""]]}, {"id": "2103.00486", "submitter": "Mark He", "authors": "Mark He, Dylan Lu, Jason Xu, Rose Mary Xavier", "title": "Community Detection in Weighted Multilayer Networks with Ambient Noise", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of stochastic blockmodel for multilayer weighted\nnetworks that accounts for the presence of a global ambient noise governing\nbetween-block interactions. We induce a hierarchy of classifications in\nweighted multilayer networks by assuming that all but one cluster (block) are\ngoverned by unique local signals, while a single block behaves identically as\ninteractions across differing blocks (ambient noise). Hierarchical variational\ninference is employed to jointly detect and typologize blocks as signal or\nnoise. We call this model for multilayer weighted networks the Stochastic Block\n(with) Ambient Noise Model(SBANM) and develop an associated community detection\nalgorithm. Then we apply this method to subjects in the Philadelphia\nNeurodevelopmental Cohort to discover communities of subjects with similar\npsychopathological symptoms in relation to psychosis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:47:28 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 05:05:35 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 15:08:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["He", "Mark", ""], ["Lu", "Dylan", ""], ["Xu", "Jason", ""], ["Xavier", "Rose Mary", ""]]}, {"id": "2103.00500", "submitter": "Masaaki Imaizumi", "authors": "Ryumei Nakada, Masaaki Imaizumi", "title": "Asymptotic Risk of Overparameterized Likelihood Models: Double Descent\n  Theory for Deep Neural Networks", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the asymptotic risk of a general class of overparameterized\nlikelihood models, including deep models. The recent empirical success of\nlarge-scale models has motivated several theoretical studies to investigate a\nscenario wherein both the number of samples, $n$, and parameters, $p$, diverge\nto infinity and derive an asymptotic risk at the limit. However, these theorems\nare only valid for linear-in-feature models, such as generalized linear\nregression, kernel regression, and shallow neural networks. Hence, it is\ndifficult to investigate a wider class of nonlinear models, including deep\nneural networks with three or more layers. In this study, we consider a\nlikelihood maximization problem without the model constraints and analyze the\nupper bound of an asymptotic risk of an estimator with penalization.\nTechnically, we combine a property of the Fisher information matrix with an\nextended Marchenko-Pastur law and associate the combination with empirical\nprocess techniques. The derived bound is general, as it describes both the\ndouble descent and the regularized risk curves, depending on the penalization.\nOur results are valid without the linear-in-feature constraints on models and\nallow us to derive the general spectral distributions of a Fisher information\nmatrix from the likelihood. We demonstrate that several explicit models, such\nas parallel deep neural networks, ensemble learning, and residual networks, are\nin agreement with our theory. This result indicates that even large and deep\nmodels have a small asymptotic risk if they exhibit a specific structure, such\nas divisibility. To verify this finding, we conduct a real-data experiment with\nparallel deep neural networks. Our results expand the applicability of the\nasymptotic risk analysis, and may also contribute to the understanding and\napplication of deep learning.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:02:08 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:25:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nakada", "Ryumei", ""], ["Imaizumi", "Masaaki", ""]]}, {"id": "2103.00502", "submitter": "Shijun Zhang", "authors": "Zuowei Shen, Haizhao Yang, Shijun Zhang", "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:15:55 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 08:06:23 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 16:33:29 GMT"}, {"version": "v4", "created": "Sat, 17 Jul 2021 04:05:16 GMT"}, {"version": "v5", "created": "Sat, 24 Jul 2021 16:48:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shen", "Zuowei", ""], ["Yang", "Haizhao", ""], ["Zhang", "Shijun", ""]]}, {"id": "2103.00580", "submitter": "Wenkai Xu", "authors": "Wenkai Xu and Gesine Reinert", "title": "A Stein Goodness of fit Test for Exponential Random Graph Models", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyse a novel nonparametric goodness of fit testing\nprocedure for exchangeable exponential random graph models (ERGMs) when a\nsingle network realisation is observed. The test determines how likely it is\nthat the observation is generated from a target unnormalised ERGM density. Our\ntest statistics are derived from a kernel Stein discrepancy, a divergence\nconstructed via Steins method using functions in a reproducing kernel Hilbert\nspace, combined with a discrete Stein operator for ERGMs. The test is a Monte\nCarlo test based on simulated networks from the target ERGM. We show\ntheoretical properties for the testing procedure for a class of ERGMs.\nSimulation studies and real network applications are presented.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:16:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Wenkai", ""], ["Reinert", "Gesine", ""]]}, {"id": "2103.00631", "submitter": "Tao Zou", "authors": "Tao Zou, Xian Li, Xuan Liang, Hansheng Wang", "title": "On the Subbagging Estimation for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces subbagging (subsample aggregating) estimation\napproaches for big data analysis with memory constraints of computers.\nSpecifically, for the whole dataset with size $N$, $m_N$ subsamples are\nrandomly drawn, and each subsample with a subsample size $k_N\\ll N$ to meet the\nmemory constraint is sampled uniformly without replacement. Aggregating the\nestimators of $m_N$ subsamples can lead to subbagging estimation. To analyze\nthe theoretical properties of the subbagging estimator, we adapt the incomplete\n$U$-statistics theory with an infinite order kernel to allow overlapping drawn\nsubsamples in the sampling procedure. Utilizing this novel theoretical\nframework, we demonstrate that via a proper hyperparameter selection of $k_N$\nand $m_N$, the subbagging estimator can achieve $\\sqrt{N}$-consistency and\nasymptotic normality under the condition $(k_Nm_N)/N\\to \\alpha \\in (0,\\infty]$.\nCompared to the full sample estimator, we theoretically show that the\n$\\sqrt{N}$-consistent subbagging estimator has an inflation rate of $1/\\alpha$\nin its asymptotic variance. Simulation experiments are presented to demonstrate\nthe finite sample performances. An American airline dataset is analyzed to\nillustrate that the subbagging estimate is numerically close to the full sample\nestimate, and can be computationally fast under the memory constraint.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 21:38:22 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zou", "Tao", ""], ["Li", "Xian", ""], ["Liang", "Xuan", ""], ["Wang", "Hansheng", ""]]}, {"id": "2103.00654", "submitter": "Gregory Canal", "authors": "Gregory Canal, Matthieu Bloch, Christopher Rozell", "title": "Feedback Coding for Active Learning", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The iterative selection of examples for labeling in active machine learning\nis conceptually similar to feedback channel coding in information theory: in\nboth tasks, the objective is to seek a minimal sequence of actions to encode\ninformation in the presence of noise. While this high-level overlap has been\npreviously noted, there remain open questions on how to best formulate active\nlearning as a communications system to leverage existing analysis and\nalgorithms in feedback coding. In this work, we formally identify and leverage\nthe structural commonalities between the two problems, including the\ncharacterization of encoder and noisy channel components, to design a new\nalgorithm. Specifically, we develop an optimal transport-based feedback coding\nscheme called Approximate Posterior Matching (APM) for the task of active\nexample selection and explore its application to Bayesian logistic regression,\na popular model in active learning. We evaluate APM on a variety of datasets\nand demonstrate learning performance comparable to existing active learning\nmethods, at a reduced computational cost. These results demonstrate the\npotential of directly deploying concepts from feedback channel coding to design\nefficient active learning strategies.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 23:00:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Canal", "Gregory", ""], ["Bloch", "Matthieu", ""], ["Rozell", "Christopher", ""]]}, {"id": "2103.00668", "submitter": "Sam Stites", "authors": "Sam Stites, Heiko Zimmermann, Hao Wu, Eli Sennesh, Jan-Willem van de\n  Meent", "title": "Learning Proposals for Probabilistic Programs with Inference Combinators", "comments": "Accepted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop operators for construction of proposals in probabilistic programs,\nwhich we refer to as inference combinators. Inference combinators define a\ngrammar over importance samplers that compose primitive operations such as\napplication of a transition kernel and importance resampling. Proposals in\nthese samplers can be parameterized using neural networks, which in turn can be\ntrained by optimizing variational objectives. The result is a framework for\nuser-programmable variational methods that are correct by construction and can\nbe tailored to specific models. We demonstrate the flexibility of this\nframework by implementing advanced variational methods based on amortized Gibbs\nsampling and annealing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:17:53 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:47:15 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 01:11:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Stites", "Sam", ""], ["Zimmermann", "Heiko", ""], ["Wu", "Hao", ""], ["Sennesh", "Eli", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "2103.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Zhigen Zhao, Wen Zhou", "title": "BEAUTY Powered BEAST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study inference about the uniform distribution with the proposed binary\nexpansion approximation of uniformity (BEAUTY) approach. Through an extension\nof the celebrated Euler's formula, we approximate the characteristic function\nof any copula distribution with a linear combination of means of binary\ninteractions from marginal binary expansions. This novel characterization\nenables a unification of many important existing tests through an approximation\nfrom some quadratic form of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a uniformly\nhigh power, we study test statistics with data-adaptive weights through an\noracle approach, referred to as the binary expansion adaptive symmetry test\n(BEAST). By utilizing the properties of the binary expansion filtration, we\nshow that the Neyman-Pearson test of uniformity can be approximated by an\noracle weighted sum of symmetry statistics. The BEAST with this oracle leads\nall existing tests we considered in empirical power against all complex forms\nof alternatives. This oracle therefore sheds light on the potential of\nsubstantial improvements in power and on the form of optimal weights under each\nalternative. By approximating this oracle with data-adaptive weights, we\ndevelop the BEAST that improves the empirical power of many existing tests\nagainst a wide spectrum of common alternatives while providing clear\ninterpretation of the form of non-uniformity upon rejection. We illustrate the\nBEAST with a study of the relationship between the location and brightness of\nstars.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:36:15 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:08:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Kai", ""], ["Zhao", "Zhigen", ""], ["Zhou", "Wen", ""]]}, {"id": "2103.00684", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Atsutoshi Kumagai", "title": "Meta-learning One-class Classifiers with Eigenvalue Solvers for\n  Supervised Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based anomaly detection methods have shown to achieve high\nperformance. However, they require a large amount of training data for each\ntask. We propose a neural network-based meta-learning method for supervised\nanomaly detection. The proposed method improves the anomaly detection\nperformance on unseen tasks, which contains a few labeled normal and anomalous\ninstances, by meta-training with various datasets. With a meta-learning\nframework, quick adaptation to each task and its effective backpropagation are\nimportant since the model is trained by the adaptation for each epoch. Our\nmodel enables them by formulating adaptation as a generalized eigenvalue\nproblem with one-class classification; its global optimum solution is obtained,\nand the solver is differentiable. We experimentally demonstrate that the\nproposed method achieves better performance than existing anomaly detection and\nfew-shot learning methods on various datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:43:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Kumagai", "Atsutoshi", ""]]}, {"id": "2103.00694", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata", "title": "Meta-learning representations for clustering with infinite Gaussian\n  mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For better clustering performance, appropriate representations are critical.\nAlthough many neural network-based metric learning methods have been proposed,\nthey do not directly train neural networks to improve clustering performance.\nWe propose a meta-learning method that train neural networks for obtaining\nrepresentations such that clustering performance improves when the\nrepresentations are clustered by the variational Bayesian (VB) inference with\nan infinite Gaussian mixture model. The proposed method can cluster unseen\nunlabeled data using knowledge meta-learned with labeled data that are\ndifferent from the unlabeled data. For the objective function, we propose a\ncontinuous approximation of the adjusted Rand index (ARI), by which we can\nevaluate the clustering performance from soft clustering assignments. Since the\napproximated ARI and the VB inference procedure are differentiable, we can\nbackpropagate the objective function through the VB inference procedure to\ntrain the neural networks. With experiments using text and image data sets, we\ndemonstrate that our proposed method has a higher adjusted Rand index than\nexisting methods do.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:05:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Iwata", "Tomoharu", ""]]}, {"id": "2103.00704", "submitter": "Xiao Guo", "authors": "Xiao Guo and Xiang Li and Xiangyu Chang and Shusen Wang and Zhihua\n  Zhang", "title": "Privacy-Preserving Distributed SVD via Federated Power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singular value decomposition (SVD) is one of the most fundamental tools in\nmachine learning and statistics.The modern machine learning community usually\nassumes that data come from and belong to small-scale device users. The low\ncommunication and computation power of such devices, and the possible privacy\nbreaches of users' sensitive data make the computation of SVD challenging.\nFederated learning (FL) is a paradigm enabling a large number of devices to\njointly learn a model in a communication-efficient way without data sharing. In\nthe FL framework, we develop a class of algorithms called FedPower for the\ncomputation of partial SVD in the modern setting. Based on the well-known power\nmethod, the local devices alternate between multiple local power iterations and\none global aggregation to improve communication efficiency. In the aggregation,\nwe propose to weight each local eigenvector matrix with Orthogonal Procrustes\nTransformation (OPT). Considering the practical stragglers' effect, the\naggregation can be fully participated or partially participated, where for the\nlatter we propose two sampling and aggregation schemes. Further, to ensure\nstrong privacy protection, we add Gaussian noise whenever the communication\nhappens by adopting the notion of differential privacy (DP). We theoretically\nshow the convergence bound for FedPower. The resulting bound is interpretable\nwith each part corresponding to the effect of Gaussian noise, parallelization,\nand random sampling of devices, respectively. We also conduct experiments to\ndemonstrate the merits of FedPower. In particular, the local iterations not\nonly improve communication efficiency but also reduce the chance of privacy\nbreaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:33:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Guo", "Xiao", ""], ["Li", "Xiang", ""], ["Chang", "Xiangyu", ""], ["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2103.00711", "submitter": "Xingcai Zhou", "authors": "Xingcai Zhou and Jiangyan Wang", "title": "Panel semiparametric quantile regression neural network for electricity\n  consumption forecasting", "comments": "30", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  China has made great achievements in electric power industry during the\nlong-term deepening of reform and opening up. However, the complex regional\neconomic, social and natural conditions, electricity resources are not evenly\ndistributed, which accounts for the electricity deficiency in some regions of\nChina. It is desirable to develop a robust electricity forecasting model.\nMotivated by which, we propose a Panel Semiparametric Quantile Regression\nNeural Network (PSQRNN) by utilizing the artificial neural network and\nsemiparametric quantile regression. The PSQRNN can explore a potential linear\nand nonlinear relationships among the variables, interpret the unobserved\nprovincial heterogeneity, and maintain the interpretability of parametric\nmodels simultaneously. And the PSQRNN is trained by combining the penalized\nquantile regression with LASSO, ridge regression and backpropagation algorithm.\nTo evaluate the prediction accuracy, an empirical analysis is conducted to\nanalyze the provincial electricity consumption from 1999 to 2018 in China based\non three scenarios. From which, one finds that the PSQRNN model performs better\nfor electricity consumption forecasting by considering the economic and\nclimatic factors. Finally, the provincial electricity consumptions of the next\n$5$ years (2019-2023) in China are reported by forecasting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:47:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhou", "Xingcai", ""], ["Wang", "Jiangyan", ""]]}, {"id": "2103.00719", "submitter": "Ziqing Lu", "authors": "Ziqing Lu, Chang Xu, Bo Du, Takashi Ishida, Lefei Zhang, and Masashi\n  Sugiyama", "title": "LocalDrop: A Hybrid Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2021.3061463", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural networks, developing regularization algorithms to settle\noverfitting is one of the major study areas. We propose a new approach for the\nregularization of neural networks by the local Rademacher complexity called\nLocalDrop. A new regularization function for both fully-connected networks\n(FCNs) and convolutional neural networks (CNNs), including drop rates and\nweight matrices, has been developed based on the proposed upper bound of the\nlocal Rademacher complexity by the strict mathematical deduction. The analyses\nof dropout in FCNs and DropBlock in CNNs with keep rate matrices in different\nlayers are also included in the complexity analyses. With the new\nregularization function, we establish a two-stage procedure to obtain the\noptimal keep rate matrix and weight matrix to realize the whole training model.\nExtensive experiments have been conducted to demonstrate the effectiveness of\nLocalDrop in different models by comparing it with several algorithms and the\neffects of different hyperparameters on the final performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:10:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lu", "Ziqing", ""], ["Xu", "Chang", ""], ["Du", "Bo", ""], ["Ishida", "Takashi", ""], ["Zhang", "Lefei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.00733", "submitter": "T Shen", "authors": "T Shen", "title": "The Mathematics Behind Spectral Clustering And The Equivalence To PCA", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Spectral clustering is a popular algorithm that clusters points using the\neigenvalues and eigenvectors of Laplacian matrices derived from the data. For\nyears, spectral clustering has been working mysteriously. This paper explains\nspectral clustering by dividing it into two categories based on whether the\ngraph Laplacian is fully connected or not. For a fully connected graph, this\npaper demonstrates the dimension reduction part by offering an objective\nfunction: the covariance between the original data points' similarities and the\nmapped data points' similarities. For a multi-connected graph, this paper\nproves that with a proper $k$, the first $k$ eigenvectors are the indicators of\nthe connected components. This paper also proves there is an equivalence\nbetween spectral embedding and PCA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:42:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shen", "T", ""]]}, {"id": "2103.00903", "submitter": "Vitor Cerqueira", "authors": "Vitor Cerqueira, Heitor Murilo Gomes, Albert Bifet, Luis Torgo", "title": "STUDD: A Student-Teacher Method for Unsupervised Concept Drift Detection", "comments": "23 pages, single column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Concept drift detection is a crucial task in data stream evolving\nenvironments. Most of state of the art approaches designed to tackle this\nproblem monitor the loss of predictive models. However, this approach falls\nshort in many real-world scenarios, where the true labels are not readily\navailable to compute the loss. In this context, there is increasing attention\nto approaches that perform concept drift detection in an unsupervised manner,\ni.e., without access to the true labels. We propose a novel approach to\nunsupervised concept drift detection based on a student-teacher learning\nparadigm. Essentially, we create an auxiliary model (student) to mimic the\nbehaviour of the primary model (teacher). At run-time, our approach is to use\nthe teacher for predicting new instances and monitoring the mimicking loss of\nthe student for concept drift detection. In a set of experiments using 19 data\nstreams, we show that the proposed approach can detect concept drift and\npresent a competitive behaviour relative to the state of the art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:51:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cerqueira", "Vitor", ""], ["Gomes", "Heitor Murilo", ""], ["Bifet", "Albert", ""], ["Torgo", "Luis", ""]]}, {"id": "2103.00959", "submitter": "Yukuo Cen", "authors": "Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng\n  Yao, Aohan Zeng, Shiguang Guo, Yang Yang, Peng Zhang, Guohao Dai, Yu Wang,\n  Chang Zhou, Hongxia Yang, Jie Tang", "title": "CogDL: Toolkit for Deep Learning on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on graphs has attracted tremendous attention from the graph\nlearning community in recent years. It has been widely used in several\nreal-world applications such as social network analysis and recommender\nsystems. In this paper, we introduce CogDL, an extensive toolkit for deep\nlearning on graphs that allows researchers and developers to easily conduct\nexperiments and build applications. It provides standard training and\nevaluation for the most important tasks in the graph domain, including node\nclassification, graph classification, etc. For each task, it provides\nimplementations of state-of-the-art models. The models in our toolkit are\ndivided into two major parts, graph embedding methods and graph neural\nnetworks. Most of the graph embedding methods learn node-level or graph-level\nrepresentations in an unsupervised way and preserves the graph properties such\nas structural information, while graph neural networks capture node features\nand work in semi-supervised or self-supervised settings. All models implemented\nin our toolkit can be easily reproducible for leaderboard results. Most models\nin CogDL are developed on top of PyTorch, and users can leverage the advantages\nof PyTorch to implement their own models. Furthermore, we demonstrate the\neffectiveness of CogDL for real-world applications in AMiner, a large academic\nmining system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:35:16 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 03:29:00 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cen", "Yukuo", ""], ["Hou", "Zhenyu", ""], ["Wang", "Yan", ""], ["Chen", "Qibin", ""], ["Luo", "Yizhen", ""], ["Yao", "Xingcheng", ""], ["Zeng", "Aohan", ""], ["Guo", "Shiguang", ""], ["Yang", "Yang", ""], ["Zhang", "Peng", ""], ["Dai", "Guohao", ""], ["Wang", "Yu", ""], ["Zhou", "Chang", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "2103.00988", "submitter": "Christian Wildner", "authors": "Christian Wildner and Heinz Koeppl", "title": "Moment-Based Variational Inference for Stochastic Differential Equations", "comments": "Appearing in Proceedings of the 24th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2021, San Diego, California,\n  USA. PMLR: Volume 130", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deterministic variational inference approaches for diffusion\nprocesses use simple proposals and target the marginal density of the\nposterior. We construct the variational process as a controlled version of the\nprior process and approximate the posterior by a set of moment functions. In\ncombination with moment closure, the smoothing problem is reduced to a\ndeterministic optimal control problem. Exploiting the path-wise Fisher\ninformation, we propose an optimization procedure that corresponds to a natural\ngradient descent in the variational parameters. Our approach allows for richer\nvariational approximations that extend to state-dependent diffusion terms. The\nclassical Gaussian process approximation is recovered as a special case.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:20:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wildner", "Christian", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2103.01030", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "An Easy to Interpret Diagnostic for Approximate Inference: Symmetric\n  Divergence Over Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to estimate the errors of probabilistic inference algorithms.\nExisting diagnostics for Markov chain Monte Carlo methods assume inference is\nasymptotically exact, and are not appropriate for approximate methods like\nvariational inference or Laplace's method. This paper introduces a diagnostic\nbased on repeatedly simulating datasets from the prior and performing inference\non each. The central observation is that it is possible to estimate a symmetric\nKL-divergence defined over these simulations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:51:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "2103.01043", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, Petar\n  Veli\\v{c}kovi\\'c", "title": "Persistent Message Passing", "comments": "7 pages, 2 figures. Published as a workshop paper at ICLR 2021 SimDL\n  Workshop. Accepted at the ICLR 2021 Workshop on Geometrical and Topological\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:35:36 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 10:39:33 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Strathmann", "Heiko", ""], ["Barekatain", "Mohammadamin", ""], ["Blundell", "Charles", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2103.01085", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael Riis\n  Andersen, Jonathan Huggins, Aki Vehtari", "title": "Challenges and Opportunities in High-dimensional Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current black-box variational inference (BBVI) methods require the user to\nmake numerous design choices -- such as the selection of variational objective\nand approximating family -- yet there is little principled guidance on how to\ndo so. We develop a conceptual framework and set of experimental tools to\nunderstand the effects of these choices, which we leverage to propose best\npractices for maximizing posterior approximation accuracy. Our approach is\nbased on studying the pre-asymptotic tail behavior of the density ratios\nbetween the joint distribution and the variational approximation, then\nexploiting insights and tools from the importance sampling literature. Our\nframework and supporting experiments help to distinguish between the behavior\nof BBVI methods for approximating low-dimensional versus\nmoderate-to-high-dimensional posteriors. In the latter case, we show that\nmass-covering variational objectives are difficult to optimize and do not\nimprove accuracy, but flexible variational families can improve accuracy and\nthe effectiveness of importance sampling -- at the cost of additional\noptimization challenges. Therefore, for moderate-to-high-dimensional posteriors\nwe recommend using the (mode-seeking) exclusive KL divergence since it is the\neasiest to optimize, and improving the variational family or using model\nparameter transformations to make the posterior and optimal variational\napproximation more similar. On the other hand, in low-dimensional settings, we\nshow that heavy-tailed variational families and mass-covering divergences are\neffective and can increase the chances that the approximation can be improved\nby importance sampling.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:53:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 16:48:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Catalina", "Alejandro", ""], ["Welandawe", "Manushi", ""], ["Andersen", "Michael Riis", ""], ["Huggins", "Jonathan", ""], ["Vehtari", "Aki", ""]]}, {"id": "2103.01126", "submitter": "Andr\\'e Bodmer Dr.", "authors": "Michael Freunek and Andr\\'e Bodmer", "title": "BERT based patent novelty search by training claims to their own\n  description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method to concatenate patent claims to their own\ndescription. By applying this method, BERT trains suitable descriptions for\nclaims. Such a trained BERT (claim-to-description- BERT) could be able to\nidentify novelty relevant descriptions for patents. In addition, we introduce a\nnew scoring scheme, relevance scoring or novelty scoring, to process the output\nof BERT in a meaningful way. We tested the method on patent applications by\ntraining BERT on the first claims of patents and corresponding descriptions.\nBERT's output has been processed according to the relevance score and the\nresults compared with the cited X documents in the search reports. The test\nshowed that BERT has scored some of the cited X documents as highly relevant.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:54:50 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:56:25 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 13:39:23 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 07:15:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Freunek", "Michael", ""], ["Bodmer", "Andr\u00e9", ""]]}, {"id": "2103.01148", "submitter": "Alperen G\\\"ormez", "authors": "Alperen Gormez and Erdem Koyuncu", "title": "Class Means as an Early Exit Decision Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural networks with early exit mechanisms often need\nconsiderable amount of training and fine-tuning to achieve good performance\nwith low computational cost. We propose a novel early exit technique based on\nthe class means of samples. Unlike most existing schemes, our method does not\nrequire gradient-based training of internal classifiers. This makes our method\nparticularly useful for neural network training in low-power devices, as in\nwireless edge networks. In particular, given a fixed training time budget, our\nscheme achieves higher accuracy as compared to existing early exit mechanisms.\nMoreover, if there are no limitations on the training time budget, our method\ncan be combined with an existing early exit scheme to boost its performance,\nachieving a better trade-off between computational cost and network accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:31:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gormez", "Alperen", ""], ["Koyuncu", "Erdem", ""]]}, {"id": "2103.01197", "submitter": "Anirudh Goyal", "authors": "Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan\n  Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer,\n  Yoshua Bengio", "title": "Coordination Among Neural Modules Through a Shared Global Workspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has seen a movement away from representing examples with a\nmonolithic hidden state towards a richly structured state. For example,\nTransformers segment by position, and object-centric architectures decompose\nimages into entities. In all these architectures, interactions between\ndifferent elements are modeled via pairwise interactions: Transformers make use\nof self-attention to incorporate information from other positions;\nobject-centric architectures make use of graph neural networks to model\ninteractions among entities. However, pairwise interactions may not achieve\nglobal coordination or a coherent, integrated representation that can be used\nfor downstream tasks. In cognitive science, a global workspace architecture has\nbeen proposed in which functionally specialized components share information\nthrough a common, bandwidth-limited communication channel. We explore the use\nof such a communication channel in the context of deep learning for modeling\nthe structure of complex environments. The proposed method includes a shared\nworkspace through which communication among different specialist modules takes\nplace but due to limits on the communication bandwidth, specialist modules must\ncompete for access. We show that capacity limitations have a rational basis in\nthat (1) they encourage specialization and compositionality and (2) they\nfacilitate the synchronization of otherwise independent specialists.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:43:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Goyal", "Anirudh", ""], ["Didolkar", "Aniket", ""], ["Lamb", "Alex", ""], ["Badola", "Kartikeya", ""], ["Ke", "Nan Rosemary", ""], ["Rahaman", "Nasim", ""], ["Binas", "Jonathan", ""], ["Blundell", "Charles", ""], ["Mozer", "Michael", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2103.01201", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe, Massimiliano Marcellino, Dalibor Stevanovic", "title": "Can Machine Learning Catch the COVID-19 Recession?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on evidence gathered from a newly built large macroeconomic data set\nfor the UK, labeled UK-MD and comparable to similar datasets for the US and\nCanada, it seems the most promising avenue for forecasting during the pandemic\nis to allow for general forms of nonlinearity by using machine learning (ML)\nmethods. But not all nonlinear ML methods are alike. For instance, some do not\nallow to extrapolate (like regular trees and forests) and some do (when\ncomplemented with linear dynamic components). This and other crucial aspects of\nML-based forecasting in unprecedented times are studied in an extensive\npseudo-out-of-sample exercise.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:47:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["Marcellino", "Massimiliano", ""], ["Stevanovic", "Dalibor", ""]]}, {"id": "2103.01210", "submitter": "Pritish Kamath", "authors": "Eran Malach, Pritish Kamath, Emmanuel Abbe, Nathan Srebro", "title": "Quantifying the Benefit of Using Differentiable Learning over Tangent\n  Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the relative power of learning with gradient descent on\ndifferentiable models, such as neural networks, versus using the corresponding\ntangent kernels. We show that under certain conditions, gradient descent\nachieves small error only if a related tangent kernel method achieves a\nnon-trivial advantage over random guessing (a.k.a. weak learning), though this\nadvantage might be very small even when gradient descent can achieve\narbitrarily high accuracy. Complementing this, we show that without these\nconditions, gradient descent can in fact learn with small error even when no\nkernel method, in particular using the tangent kernel, can achieve a\nnon-trivial advantage over random guessing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:54:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Malach", "Eran", ""], ["Kamath", "Pritish", ""], ["Abbe", "Emmanuel", ""], ["Srebro", "Nathan", ""]]}, {"id": "2103.01242", "submitter": "Avia Efrat", "authors": "Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy", "title": "Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in\n  Language", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current NLP datasets targeting ambiguity can be solved by a native speaker\nwith relative ease. We present Cryptonite, a large-scale dataset based on\ncryptic crosswords, which is both linguistically complex and naturally sourced.\nEach example in Cryptonite is a cryptic clue, a short phrase or sentence with a\nmisleading surface reading, whose solving requires disambiguating semantic,\nsyntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues\npose a challenge even for experienced solvers, though top-tier experts can\nsolve them with almost 100% accuracy. Cryptonite is a challenging task for\ncurrent models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%\naccuracy, on par with the accuracy of a rule-based clue solver (8.6%).\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:01:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Efrat", "Avia", ""], ["Shaham", "Uri", ""], ["Kilman", "Dan", ""], ["Levy", "Omer", ""]]}, {"id": "2103.01276", "submitter": "Pranjal Awasthi", "authors": "Jacob Abernethy, Pranjal Awasthi, Satyen Kale", "title": "A Multiclass Boosting Framework for Achieving Fast and Provable\n  Adversarial Robustness", "comments": "Fixed misspelled first author name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alongside the well-publicized accomplishments of deep neural networks there\nhas emerged an apparent bug in their success on tasks such as object\nrecognition: with deep models trained using vanilla methods, input images can\nbe slightly corrupted in order to modify output predictions, even when these\ncorruptions are practically invisible. This apparent lack of robustness has led\nresearchers to propose methods that can help to prevent an adversary from\nhaving such capabilities. The state-of-the-art approaches have incorporated the\nrobustness requirement into the loss function, and the training process\ninvolves taking stochastic gradient descent steps not using original inputs but\non adversarially-corrupted ones. In this paper we propose a multiclass boosting\nframework to ensure adversarial robustness. Boosting algorithms are generally\nwell-suited for adversarial scenarios, as they were classically designed to\nsatisfy a minimax guarantee. We provide a theoretical foundation for this\nmethodology and describe conditions under which robustness can be achieved\ngiven a weak training oracle. We show empirically that adversarially-robust\nmulticlass boosting not only outperforms the state-of-the-art methods, it does\nso at a fraction of the training time.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:42:31 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 17:17:28 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abernethy", "Jacob", ""], ["Awasthi", "Pranjal", ""], ["Kale", "Satyen", ""]]}, {"id": "2103.01278", "submitter": "Anupama Nandi", "authors": "Raef Bassily, Crist\\'obal Guzm\\'an, Anupama Nandi", "title": "Non-Euclidean Differentially Private Stochastic Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentially private (DP) stochastic convex optimization (SCO) is a\nfundamental problem, where the goal is to approximately minimize the population\nrisk with respect to a convex loss function, given a dataset of i.i.d. samples\nfrom a distribution, while satisfying differential privacy with respect to the\ndataset. Most of the existing works in the literature of private convex\noptimization focus on the Euclidean (i.e., $\\ell_2$) setting, where the loss is\nassumed to be Lipschitz (and possibly smooth) w.r.t. the $\\ell_2$ norm over a\nconstraint set with bounded $\\ell_2$ diameter. Algorithms based on noisy\nstochastic gradient descent (SGD) are known to attain the optimal excess risk\nin this setting.\n  In this work, we conduct a systematic study of DP-SCO for $\\ell_p$-setups.\nFor $p=1$, under a standard smoothness assumption, we give a new algorithm with\nnearly optimal excess risk. This result also extends to general polyhedral\nnorms and feasible sets. For $p\\in(1, 2)$, we give two new algorithms, whose\ncentral building block is a novel privacy mechanism, which generalizes the\nGaussian mechanism. Moreover, we establish a lower bound on the excess risk for\nthis range of $p$, showing a necessary dependence on $\\sqrt{d}$, where $d$ is\nthe dimension of the space. Our lower bound implies a sudden transition of the\nexcess risk at $p=1$, where the dependence on $d$ changes from logarithmic to\npolynomial, resolving an open question in prior work [TTZ15] . For $p\\in (2,\n\\infty)$, noisy SGD attains optimal excess risk in the low-dimensional regime;\nin particular, this proves the optimality of noisy SGD for $p=\\infty$. Our work\ndraws upon concepts from the geometry of normed spaces, such as the notions of\nregularity, uniform convexity, and uniform smoothness.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:48:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Bassily", "Raef", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""], ["Nandi", "Anupama", ""]]}, {"id": "2103.01280", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano, Jelena Bradic", "title": "Dynamic covariate balancing: estimating treatment effects over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of estimation and inference on the effects\nof time-varying treatment. We propose a method for inference on the effects\ntreatment histories, introducing a dynamic covariate balancing method combined\nwith penalized regression. Our approach allows for (i) treatments to be\nassigned based on arbitrary past information, with the propensity score being\nunknown; (ii) outcomes and time-varying covariates to depend on treatment\ntrajectories; (iii) high-dimensional covariates; (iv) heterogeneity of\ntreatment effects. We study the asymptotic properties of the estimator, and we\nderive the parametric convergence rate of the proposed procedure. Simulations\nand an empirical application illustrate the advantage of the method over\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:53:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:54:38 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "2103.01291", "submitter": "Neale Ratzlaff", "authors": "Neale Ratzlaff, Qinxun Bai, Li Fuxin, Wei Xu", "title": "Generative Particle Variational Inference via Estimation of Functional\n  Gradients", "comments": "10 pages, 3 figures, 4 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, particle-based variational inference (ParVI) methods have gained\ninterest because they directly minimize the Kullback-Leibler divergence and do\nnot suffer from approximation errors from the evidence-based lower bound.\nHowever, many ParVI approaches do not allow arbitrary sampling from the\nposterior, and the few that do allow such sampling suffer from suboptimality.\nThis work proposes a new method for learning to approximately sample from the\nposterior distribution. We construct a neural sampler that is trained with the\nfunctional gradient of the KL-divergence between the empirical sampling\ndistribution and the target distribution, assuming the gradient resides within\na reproducing kernel Hilbert space. Our generative ParVI (GPVI) approach\nmaintains the asymptotic performance of ParVI methods while offering the\nflexibility of a generative sampler. Through carefully constructed experiments,\nwe show that GPVI outperforms previous generative ParVI methods such as\namortized SVGD, and is competitive with ParVI as well as gold-standard\napproaches like Hamiltonian Monte Carlo for fitting both exactly known and\nintractable target distributions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:29:41 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ratzlaff", "Neale", ""], ["Bai", "Qinxun", ""], ["Fuxin", "Li", ""], ["Xu", "Wei", ""]]}, {"id": "2103.01312", "submitter": "Pierre Menard", "authors": "Pierre Menard, Omar Darwiche Domingues, Xuedong Shang, Michal Valko", "title": "UCB Momentum Q-learning: Correcting the bias without forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm\nfor reinforcement learning in tabular and possibly stage-dependent, episodic\nMarkov decision process. UCBMQ is based on Q-learning where we add a momentum\nterm and rely on the principle of optimism in face of uncertainty to deal with\nexploration. Our new technical ingredient of UCBMQ is the use of momentum to\ncorrect the bias that Q-learning suffers while, at the same time, limiting the\nimpact it has on the second-order term of the regret. For UCBMQ , we are able\nto guarantee a regret of at most $O(\\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the\nlength of an episode, $S$ the number of states, $A$ the number of actions, $T$\nthe number of episodes and ignoring terms in poly$log(SAHT)$. Notably, UCBMQ is\nthe first algorithm that simultaneously matches the lower bound of\n$\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with\nrespect to the horizon $T$) that scales only linearly with the number of states\n$S$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 21:08:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Menard", "Pierre", ""], ["Domingues", "Omar Darwiche", ""], ["Shang", "Xuedong", ""], ["Valko", "Michal", ""]]}, {"id": "2103.01327", "submitter": "Trong-Nghia Nguyen", "authors": "Minh-Ngoc Tran, Trong-Nghia Nguyen, and Viet-Hung Dao", "title": "A practical tutorial on Variational Bayes", "comments": "43 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial gives a quick introduction to Variational Bayes (VB), also\ncalled Variational Inference or Variational Approximation, from a practical\npoint of view. The paper covers a range of commonly used VB methods and an\nattempt is made to keep the materials accessible to the wide community of data\nanalysis practitioners. The aim is that the reader can quickly derive and\nimplement their first VB algorithm for Bayesian inference with their data\nanalysis problem. An end-user software package in Matlab together with the\ndocumentation can be found at https://vbayeslab.github.io/VBLabDocs/\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:11:42 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nguyen", "Trong-Nghia", ""], ["Dao", "Viet-Hung", ""]]}, {"id": "2103.01338", "submitter": "Cyril Zhang", "authors": "Naman Agarwal, Surbhi Goel, Cyril Zhang", "title": "Acceleration via Fractal Learning Rate Schedules", "comments": "v2: revisions for ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications of iterative first-order optimization, the learning\nrate schedule remains notoriously difficult to understand and expensive to\ntune. We demonstrate the presence of these subtleties even in the innocuous\ncase when the objective is a convex quadratic. We reinterpret an iterative\nalgorithm from the numerical analysis literature as what we call the Chebyshev\nlearning rate schedule for accelerating vanilla gradient descent, and show that\nthe problem of mitigating instability leads to a fractal ordering of step\nsizes. We provide some experiments to challenge conventional beliefs about\nstable learning rates in deep learning: the fractal schedule enables training\nto converge with locally unstable updates which make negative progress on the\nobjective.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:52:13 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 21:57:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Agarwal", "Naman", ""], ["Goel", "Surbhi", ""], ["Zhang", "Cyril", ""]]}, {"id": "2103.01356", "submitter": "Ottmar Cronie", "authors": "Ottmar Cronie, Mehdi Moradi, Christophe A.N. Biscio", "title": "Statistical learning and cross-validation for point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first general (supervised) statistical learning\nframework for point processes in general spaces. Our approach is based on the\ncombination of two new concepts, which we define in the paper: i) bivariate\ninnovations, which are measures of discrepancy/prediction-accuracy between two\npoint processes, and ii) point process cross-validation (CV), which we here\ndefine through point process thinning. The general idea is to carry out the\nfitting by predicting CV-generated validation sets using the corresponding\ntraining sets; the prediction error, which we minimise, is measured by means of\nbivariate innovations. Having established various theoretical properties of our\nbivariate innovations, we study in detail the case where the CV procedure is\nobtained through independent thinning and we apply our statistical learning\nmethodology to three typical spatial statistical settings, namely parametric\nintensity estimation, non-parametric intensity estimation and Papangelou\nconditional intensity fitting. Aside from deriving theoretical properties\nrelated to these cases, in each of them we numerically show that our\nstatistical learning approach outperforms the state of the art in terms of mean\n(integrated) squared error.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:47:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cronie", "Ottmar", ""], ["Moradi", "Mehdi", ""], ["Biscio", "Christophe A. N.", ""]]}, {"id": "2103.01379", "submitter": "Mathias Lecuyer", "authors": "Mathias L\\'ecuyer", "title": "Practical Privacy Filters and Odometers with R\\'enyi Differential\n  Privacy and Applications to Differentially Private Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential Privacy (DP) is the leading approach to privacy preserving deep\nlearning. As such, there are multiple efforts to provide drop-in integration of\nDP into popular frameworks. These efforts, which add noise to each gradient\ncomputation to make it DP, rely on composition theorems to bound the total\nprivacy loss incurred over this sequence of DP computations.\n  However, existing composition theorems present a tension between efficiency\nand flexibility. Most theorems require all computations in the sequence to have\na predefined DP parameter, called the privacy budget. This prevents the design\nof training algorithms that adapt the privacy budget on the fly, or that\nterminate early to reduce the total privacy loss. Alternatively, the few\nexisting composition results for adaptive privacy budgets provide complex\nbounds on the privacy loss, with constants too large to be practical.\n  In this paper, we study DP composition under adaptive privacy budgets through\nthe lens of R\\'enyi Differential Privacy, proving a simpler composition theorem\nwith smaller constants, making it practical enough to use in algorithm design.\nWe demonstrate two applications of this theorem for DP deep learning: adapting\nthe noise or batch size online to improve a model's accuracy within a fixed\ntotal privacy loss, and stopping early when fine-tuning a model to reduce total\nprivacy loss.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:37:11 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 20:29:30 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["L\u00e9cuyer", "Mathias", ""]]}, {"id": "2103.01391", "submitter": "Semih Cayci", "authors": "Semih Cayci, Siddhartha Satpathi, Niao He, R. Srikant", "title": "Sample Complexity and Overparameterization Bounds for Temporal\n  Difference Learning with Neural Network Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the dynamics of temporal difference learning with\nneural network-based value function approximation over a general state space,\nnamely, \\emph{Neural TD learning}. We consider two practically used algorithms,\nprojection-free and max-norm regularized Neural TD learning, and establish the\nfirst convergence bounds for these algorithms. An interesting observation from\nour results is that max-norm regularization can dramatically improve the\nperformance of TD learning algorithms, both in terms of sample complexity and\noverparameterization. In particular, we prove that max-norm regularization\nappears to be more effective than $\\ell_2$-regularization, again both in terms\nof sample complexity and overparameterization. The results in this work rely on\na novel Lyapunov drift analysis of the network parameters as a stopped and\ncontrolled random process.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:05:19 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 04:51:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cayci", "Semih", ""], ["Satpathi", "Siddhartha", ""], ["He", "Niao", ""], ["Srikant", "R.", ""]]}, {"id": "2103.01400", "submitter": "Sekitoshi Kanai", "authors": "Sekitoshi Kanai, Masanori Yamada, Hiroshi Takahashi, Yuki Yamanaka,\n  Yasutoshi Ida", "title": "Smoothness Analysis of Adversarial Training", "comments": "22 pages, 7 figures. In V3, we add the results of EntropySGD for\n  adversarial training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks. Recent studies\nabout adversarial robustness focus on the loss landscape in the parameter space\nsince it is related to optimization and generalization performance. These\nstudies conclude that the difficulty of adversarial training is caused by the\nnon-smoothness of the loss function: i.e., its gradient is not Lipschitz\ncontinuous. However, this analysis ignores the dependence of adversarial\nattacks on model parameters. Since adversarial attacks are optimized for\nmodels, they should depend on the parameters. Considering this dependence, we\nanalyze the smoothness of the loss function of adversarial training using the\noptimal attacks for the model parameter in more detail. We reveal that the\nconstraint of adversarial attacks is one cause of the non-smoothness and that\nthe smoothness depends on the types of the constraints. Specifically, the\n$L_\\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.\nMoreover, our analysis implies that if we flatten the loss function with\nrespect to input data, the Lipschitz constant of the gradient of adversarial\nloss tends to increase. To address the non-smoothness, we show that EntropySGD\nsmoothens the non-smooth loss and improves the performance of adversarial\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:27:16 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 06:20:23 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 06:30:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kanai", "Sekitoshi", ""], ["Yamada", "Masanori", ""], ["Takahashi", "Hiroshi", ""], ["Yamanaka", "Yuki", ""], ["Ida", "Yasutoshi", ""]]}, {"id": "2103.01413", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi, Alex Leong, Iman Shames, Mohammad Zamani", "title": "Safe Learning of Uncertain Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning based control methodologies, learning the unknown dynamic\nmodel precedes the control phase, while the aim is to control the system such\nthat it remains in some safe region of the state space. In this work, our aim\nis to guarantee safety while learning and control proceed simultaneously.\nSpecifically, we consider the problem of safe learning in nonlinear\ncontrol-affine systems subject to unknown additive uncertainty. We first model\nthe uncertainty as a Gaussian noise and use state measurements to learn its\nmean and covariance. We provide rigorous time-varying bounds on the mean and\ncovariance of the uncertainty and employ them to modify the control input via\nan optimization program with potentially time-varying safety constraints. We\nshow that with an arbitrarily large probability we can guarantee that the state\nwill remain in the safe set, while learning and control are carried out\nsimultaneously, provided that a feasible solution exists for the optimization\nproblem. We provide a secondary formulation of this optimization that is\ncomputationally more efficient. This is based on tightening the safety\nconstraints to counter the uncertainty about the learned mean and covariance.\nThe magnitude of the tightening can be decreased as our confidence in the\nlearned mean and covariance increases (i.e., as we gather more measurements\nabout the environment). Extensions of the method are provided for non-Gaussian\nprocess noise with unknown mean and covariance as well as Gaussian\nuncertainties with state-dependent mean and covariance to accommodate more\ngeneral environments.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:58:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 06:27:19 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Farokhi", "Farhad", ""], ["Leong", "Alex", ""], ["Shames", "Iman", ""], ["Zamani", "Mohammad", ""]]}, {"id": "2103.01439", "submitter": "Andrew Wilson", "authors": "Wesley J. Maddox, Shuai Tang, Pablo Garcia Moreno, Andrew Gordon\n  Wilson, Andreas Damianou", "title": "Fast Adaptation with Linearized Neural Networks", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inductive biases of trained neural networks are difficult to understand\nand, consequently, to adapt to new settings. We study the inductive biases of\nlinearizations of neural networks, which we show to be surprisingly good\nsummaries of the full network functions. Inspired by this finding, we propose a\ntechnique for embedding these inductive biases into Gaussian processes through\na kernel designed from the Jacobian of the network. In this setting, domain\nadaptation takes the form of interpretable posterior inference, with\naccompanying uncertainty estimation. This inference is analytic and free of\nlocal optima issues found in standard techniques such as fine-tuning neural\nnetwork weights to a new task. We develop significant computational speed-ups\nbased on matrix multiplies, including a novel implementation for scalable\nFisher vector products. Our experiments on both image classification and\nregression demonstrate the promise and convenience of this framework for\ntransfer learning, compared to neural network fine-tuning. Code is available at\nhttps://github.com/amzn/xfer/tree/master/finite_ntk.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:23:03 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:16:02 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Maddox", "Wesley J.", ""], ["Tang", "Shuai", ""], ["Moreno", "Pablo Garcia", ""], ["Wilson", "Andrew Gordon", ""], ["Damianou", "Andreas", ""]]}, {"id": "2103.01454", "submitter": "Andrew Wilson", "authors": "Samuel Stanton, Wesley J. Maddox, Ian Delbridge, Andrew Gordon Wilson", "title": "Kernel Interpolation for Scalable Online Gaussian Processes", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a gold standard for performance in online\nsettings, such as sample-efficient control and black box optimization, where we\nneed to update a posterior distribution as we acquire data in a sequential\nfashion. However, updating a GP posterior to accommodate even a single new\nobservation after having observed $n$ points incurs at least $O(n)$\ncomputations in the exact setting. We show how to use structured kernel\ninterpolation to efficiently recycle computations for constant-time $O(1)$\nonline updates with respect to the number of points $n$, while retaining exact\ninference. We demonstrate the promise of our approach in a range of online\nregression and classification settings, Bayesian optimization, and active\nsampling to reduce error in malaria incidence forecasting. Code is available at\nhttps://github.com/wjmaddox/online_gp.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:41:30 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Stanton", "Samuel", ""], ["Maddox", "Wesley J.", ""], ["Delbridge", "Ian", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "2103.01474", "submitter": "Dong Li", "authors": "Ruoming Jin and Dong Li and Benjamin Mudrak and Jing Gao and Zhi Liu", "title": "On Estimating Recommendation Evaluation Metrics under Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the recent study (Krichene and Rendle 2020) done by Krichene and Rendle\non the sampling-based top-k evaluation metric for recommendation, there has\nbeen a lot of debates on the validity of using sampling to evaluate\nrecommendation algorithms. Though their work and the recent work (Li et\nal.2020) have proposed some basic approaches for mapping the sampling-based\nmetrics to their global counterparts which rank the entire set of items, there\nis still a lack of understanding and consensus on how sampling should be used\nfor recommendation evaluation. The proposed approaches either are rather\nuninformative (linking sampling to metric evaluation) or can only work on\nsimple metrics, such as Recall/Precision (Krichene and Rendle 2020; Li et al.\n2020). In this paper, we introduce a new research problem on learning the\nempirical rank distribution, and a new approach based on the estimated rank\ndistribution, to estimate the top-k metrics. Since this question is closely\nrelated to the underlying mechanism of sampling for recommendation, tackling it\ncan help better understand the power of sampling and can help resolve the\nquestions of if and how should we use sampling for evaluating recommendation.\nWe introduce two approaches based on MLE (MaximalLikelihood Estimation) and its\nweighted variants, and ME(Maximal Entropy) principals to recover the empirical\nrank distribution, and then utilize them for metrics estimation. The\nexperimental results show the advantages of using the new approaches for\nevaluating recommendation algorithms based on top-k metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:08:21 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:04:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Jin", "Ruoming", ""], ["Li", "Dong", ""], ["Mudrak", "Benjamin", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""]]}, {"id": "2103.01496", "submitter": "Wenxiao Wang", "authors": "Wenxiao Wang (1), Tianhao Wang (2), Lun Wang (3), Nanqing Luo (4), Pan\n  Zhou (4), Dawn Song (3), Ruoxi Jia (5) ((1) Tsinghua University, (2) Harvard\n  University, (3) University of California, Berkeley, (4) Huazhong University\n  of Science and Technology, (5) Virginia Tech)", "title": "DPlis: Boosting Utility of Differentially Private Deep Learning via\n  Randomized Smoothing", "comments": "The 21st Privacy Enhancing Technologies Symposium (PETS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have achieved remarkable performance in wide-ranging\ntasks. However, when trained on privacy-sensitive datasets, the model\nparameters may expose private information in training data. Prior attempts for\ndifferentially private training, although offering rigorous privacy guarantees,\nlead to much lower model performance than the non-private ones. Besides,\ndifferent runs of the same training algorithm produce models with large\nperformance variance. To address these issues, we propose DPlis--Differentially\nPrivate Learning wIth Smoothing. The core idea of DPlis is to construct a\nsmooth loss function that favors noise-resilient models lying in large flat\nregions of the loss landscape. We provide theoretical justification for the\nutility improvements of DPlis. Extensive experiments also demonstrate that\nDPlis can effectively boost model quality and training stability under a given\nprivacy budget.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:33:14 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 16:08:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Wenxiao", ""], ["Wang", "Tianhao", ""], ["Wang", "Lun", ""], ["Luo", "Nanqing", ""], ["Zhou", "Pan", ""], ["Song", "Dawn", ""], ["Jia", "Ruoxi", ""]]}, {"id": "2103.01499", "submitter": "Arda Sahiner", "authors": "Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza\n  Mardani, Mert Pilanci", "title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex\n  Optimization Models and Implicit Regularization", "comments": "First two authors contributed equally to this work; 33 pages, 13\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Batch Normalization (BN) is a commonly used technique to accelerate and\nstabilize training of deep neural networks. Despite its empirical success, a\nfull theoretical understanding of BN is yet to be developed. In this work, we\nanalyze BN through the lens of convex optimization. We introduce an analytic\nframework based on convex duality to obtain exact convex representations of\nweight-decay regularized ReLU networks with BN, which can be trained in\npolynomial-time. Our analyses also show that optimal layer weights can be\nobtained as simple closed-form formulas in the high-dimensional and/or\noverparameterized regimes. Furthermore, we find that Gradient Descent provides\nan algorithmic bias effect on the standard non-convex BN network, and we design\nan approach to explicitly encode this implicit regularization into the convex\nobjective. Experiments with CIFAR image classification highlight the\neffectiveness of this explicit regularization for mimicking and substantially\nimproving the performance of standard BN networks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:36:31 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ergen", "Tolga", ""], ["Sahiner", "Arda", ""], ["Ozturkler", "Batu", ""], ["Pauly", "John", ""], ["Mardani", "Morteza", ""], ["Pilanci", "Mert", ""]]}, {"id": "2103.01516", "submitter": "Hilal Asi", "authors": "Hilal Asi, Vitaly Feldman, Tomer Koren, Kunal Talwar", "title": "Private Stochastic Convex Optimization: Optimal Rates in $\\ell_1$\n  Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic convex optimization over an $\\ell_1$-bounded domain is ubiquitous\nin machine learning applications such as LASSO but remains poorly understood\nwhen learning with differential privacy. We show that, up to logarithmic\nfactors the optimal excess population loss of any\n$(\\varepsilon,\\delta)$-differentially private optimizer is $\\sqrt{\\log(d)/n} +\n\\sqrt{d}/\\varepsilon n.$ The upper bound is based on a new algorithm that\ncombines the iterative localization approach of~\\citet{FeldmanKoTa20} with a\nnew analysis of private regularized mirror descent. It applies to $\\ell_p$\nbounded domains for $p\\in [1,2]$ and queries at most $n^{3/2}$ gradients\nimproving over the best previously known algorithm for the $\\ell_2$ case which\nneeds $n^2$ gradients. Further, we show that when the loss functions satisfy\nadditional smoothness assumptions, the excess loss is upper bounded (up to\nlogarithmic factors) by $\\sqrt{\\log(d)/n} + (\\log(d)/\\varepsilon n)^{2/3}.$\nThis bound is achieved by a new variance-reduced version of the Frank-Wolfe\nalgorithm that requires just a single pass over the data. We also show that the\nlower bound in this case is the minimum of the two rates mentioned above.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:53:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Asi", "Hilal", ""], ["Feldman", "Vitaly", ""], ["Koren", "Tomer", ""], ["Talwar", "Kunal", ""]]}, {"id": "2103.01519", "submitter": "Zhenyu Liao", "authors": "Zhenyu Liao and Michael W. Mahoney", "title": "Hessian Eigenspectra of More Realistic Nonlinear Models", "comments": "Identical to v1, except for the inclusion of some additional\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an optimization problem, the Hessian matrix and its eigenspectrum can\nbe used in many ways, ranging from designing more efficient second-order\nalgorithms to performing model analysis and regression diagnostics. When\nnonlinear models and non-convex problems are considered, strong simplifying\nassumptions are often made to make Hessian spectral analysis more tractable.\nThis leads to the question of how relevant the conclusions of such analyses are\nfor more realistic nonlinear models. In this paper, we exploit deterministic\nequivalent techniques from random matrix theory to make a \\emph{precise}\ncharacterization of the Hessian eigenspectra for a broad family of nonlinear\nmodels, including models that generalize the classical generalized linear\nmodels, without relying on strong simplifying assumptions used previously. We\nshow that, depending on the data properties, the nonlinear response model, and\nthe loss function, the Hessian can have \\emph{qualitatively} different spectral\nbehaviors: of bounded or unbounded support, with single- or multi-bulk, and\nwith isolated eigenvalues on the left- or right-hand side of the bulk. By\nfocusing on such a simple but nontrivial nonlinear model, our analysis takes a\nstep forward to unveil the theoretical origin of many visually striking\nfeatures observed in more complex machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:59:52 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 00:29:08 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liao", "Zhenyu", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2103.01550", "submitter": "Christos Thrampoulidis", "authors": "Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak and Christos\n  Thrampoulidis", "title": "Label-Imbalanced and Group-Sensitive Classification under\n  Overparameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Label-imbalanced and group-sensitive classification seeks to appropriately\nmodify standard training algorithms to optimize relevant metrics such as\nbalanced error and/or equal opportunity. For label imbalances, recent works\nhave proposed a logit-adjusted loss modification to standard empirical risk\nminimization. We show that this might be ineffective in general and, in\nparticular so, in the overparameterized regime where training continues in the\nzero training-error regime. Specifically for binary linear classification of a\nseparable dataset, we show that the modified loss converges to the max-margin\nSVM classifier despite the logit adjustment. Instead, we propose a more general\nvector-scaling loss that directly relates to the cost-sensitive SVM (CS-SVM),\nthus favoring larger margin to the minority class. Through an insightful sharp\nasymptotic analysis for a Gaussian-mixtures data model, we demonstrate the\nefficacy of CS-SVM in balancing the errors of the minority/majority classes.\nOur analysis also leads to a simple strategy for optimally tuning the involved\nmargin-ratio parameter. Then, we show how our results extend naturally to\nbinary classification with sensitive groups, thus treating the two common types\nof imbalances (label/group) in a unifying way. We corroborate our theoretical\nfindings with numerical experiments on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:09:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kini", "Ganesh Ramachandra", ""], ["Paraskevas", "Orestis", ""], ["Oymak", "Samet", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2103.01596", "submitter": "Marco Rossi", "authors": "Marco Rossi, Sofia Vallecorsa", "title": "Deep Learning strategies for ProtoDUNE raw data denoising", "comments": "9 pages, 6 figures. Submitted to 25th International Conference on\n  Computing in High-Energy and Nuclear Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-ex physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate different machine learning based strategies for\ndenoising raw simulation data from ProtoDUNE experiment. ProtoDUNE detector is\nhosted by CERN and it aims to test and calibrate the technologies for DUNE, a\nforthcoming experiment in neutrino physics. Our models leverage deep learning\nalgorithms to make the first step in the reconstruction workchain, which\nconsists in converting digital detector signals into physical high level\nquantities. We benchmark this approach against traditional algorithms\nimplemented by the DUNE collaboration. We test the capabilities of graph neural\nnetworks, while exploiting multi-GPU setups to accelerate training and\ninference processes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:42:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Rossi", "Marco", ""], ["Vallecorsa", "Sofia", ""]]}, {"id": "2103.01648", "submitter": "Mario Gonz\\'alez", "authors": "Mario Gonz\\'alez, Andr\\'es Almansa, Pauline Tan", "title": "Solving Inverse Problems by Joint Posterior Maximization with\n  Autoencoding Prior", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.06379", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of solving ill-posed inverse problems in\nimaging where the prior is a variational autoencoder (VAE). Specifically we\nconsider the decoupled case where the prior is trained once and can be reused\nfor many different log-concave degradation models without retraining. Whereas\nprevious MAP-based approaches to this problem lead to highly non-convex\noptimization algorithms, our approach computes the joint (space-latent) MAP\nthat naturally leads to alternate optimization algorithms and to the use of a\nstochastic encoder to accelerate computations. The resulting technique (JPMAP)\nperforms Joint Posterior Maximization using an Autoencoding Prior. We show\ntheoretical and experimental evidence that the proposed objective function is\nquite close to bi-convex. Indeed it satisfies a weak bi-convexity property\nwhich is sufficient to guarantee that our optimization scheme converges to a\nstationary point. We also highlight the importance of correctly training the\nVAE using a denoising criterion, in order to ensure that the encoder\ngeneralizes well to out-of-distribution images, without affecting the quality\nof the generative model. This simple modification is key to providing\nrobustness to the whole procedure. Finally we show how our joint MAP\nmethodology relates to more common MAP approaches, and we propose a\ncontinuation scheme that makes use of our JPMAP algorithm to provide more\nrobust MAP estimates. Experimental results also show the higher quality of the\nsolutions obtained by our JPMAP approach with respect to other non-convex MAP\napproaches which more often get stuck in spurious local optima.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:18:34 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 13:45:36 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Gonz\u00e1lez", "Mario", ""], ["Almansa", "Andr\u00e9s", ""], ["Tan", "Pauline", ""]]}, {"id": "2103.01676", "submitter": "Lawrence Bull", "authors": "Lawrence A. Bull, Paul Gardner, Timothy J. Rogers, Elizabeth J. Cross,\n  Nikolaos Dervilis, Keith Worden", "title": "Probabilistic Inference for Structural Health Monitoring: New Modes of\n  Learning from Data", "comments": "This material may be downloaded for personal use only. Any other use\n  requires prior permission of the American Society of Civil Engineers. This\n  material may be found at https://doi.org/10.1061/AJRUA6.0001106", "journal-ref": "ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems,\n  Part A: Civil Engineering 7.1 (2021): 03120003", "doi": "10.1061/AJRUA6.0001106", "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In data-driven SHM, the signals recorded from systems in operation can be\nnoisy and incomplete. Data corresponding to each of the operational,\nenvironmental, and damage states are rarely available a priori; furthermore,\nlabelling to describe the measurements is often unavailable. In consequence,\nthe algorithms used to implement SHM should be robust and adaptive, while\naccommodating for missing information in the training-data -- such that new\ninformation can be included if it becomes available. By reviewing novel\ntechniques for statistical learning (introduced in previous work), it is argued\nthat probabilistic algorithms offer a natural solution to the modelling of SHM\ndata in practice. In three case-studies, probabilistic methods are adapted for\napplications to SHM signals -- including semi-supervised learning, active\nlearning, and multi-task learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:18:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Bull", "Lawrence A.", ""], ["Gardner", "Paul", ""], ["Rogers", "Timothy J.", ""], ["Cross", "Elizabeth J.", ""], ["Dervilis", "Nikolaos", ""], ["Worden", "Keith", ""]]}, {"id": "2103.01678", "submitter": "Jan Stanczuk", "authors": "Jan Stanczuk, Christian Etmann, Lisa Maria Kreusser, Carola-Bibiane\n  Sch\\\"onlieb", "title": "Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein\n  Distance)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein GANs are based on the idea of minimising the Wasserstein distance\nbetween a real and a generated distribution. We provide an in-depth\nmathematical analysis of differences between the theoretical setup and the\nreality of training Wasserstein GANs. In this work, we gather both theoretical\nand empirical evidence that the WGAN loss is not a meaningful approximation of\nthe Wasserstein distance. Moreover, we argue that the Wasserstein distance is\nnot even a desirable loss function for deep generative models, and conclude\nthat the success of Wasserstein GANs can in truth be attributed to a failure to\napproximate the Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:30:25 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 16:50:36 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 17:41:34 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Stanczuk", "Jan", ""], ["Etmann", "Christian", ""], ["Kreusser", "Lisa Maria", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2103.01804", "submitter": "Irina Deeva", "authors": "Irina Deeva, Anna Bubnova, Petr Andriushchenko, Anton Voskresenskiy,\n  Nikita Bukhanov, Nikolay O. Nikitin, Anna V. Kalyuzhnaya", "title": "Oil and Gas Reservoirs Parameters Analysis Using Mixed Learning of\n  Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multipurpose Bayesian-based method for data analysis, causal\ninference and prediction in the sphere of oil and gas reservoir development is\nconsidered. This allows analysing parameters of a reservoir, discovery\ndependencies among parameters (including cause and effects relations), checking\nfor anomalies, prediction of expected values of missing parameters, looking for\nthe closest analogues, and much more. The method is based on extended algorithm\nMixLearn@BN for structural learning of Bayesian networks. Key ideas of\nMixLearn@BN are following: (1) learning the network structure on homogeneous\ndata subsets, (2) assigning a part of the structure by an expert, and (3)\nlearning the distribution parameters on mixed data (discrete and continuous).\nHomogeneous data subsets are identified as various groups of reservoirs with\nsimilar features (analogues), where similarity measure may be based on several\ntypes of distances. The aim of the described technique of Bayesian network\nlearning is to improve the quality of predictions and causal inference on such\nnetworks. Experimental studies prove that the suggested method gives a\nsignificant advantage in missing values prediction and anomalies detection\naccuracy. Moreover, the method was applied to the database of more than a\nthousand petroleum reservoirs across the globe and allowed to discover novel\ninsights in geological parameters relationships.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 15:27:49 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Deeva", "Irina", ""], ["Bubnova", "Anna", ""], ["Andriushchenko", "Petr", ""], ["Voskresenskiy", "Anton", ""], ["Bukhanov", "Nikita", ""], ["Nikitin", "Nikolay O.", ""], ["Kalyuzhnaya", "Anna V.", ""]]}, {"id": "2103.01828", "submitter": "Edith Heiter", "authors": "Edith Heiter, Jonas Fischer, Jilles Vreeken", "title": "Factoring out prior knowledge from low-dimensional embeddings", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional embedding techniques such as tSNE and UMAP allow visualizing\nhigh-dimensional data and therewith facilitate the discovery of interesting\nstructure. Although they are widely used, they visualize data as is, rather\nthan in light of the background knowledge we have about the data. What we\nalready know, however, strongly determines what is novel and hence interesting.\nIn this paper we propose two methods for factoring out prior knowledge in the\nform of distance matrices from low-dimensional embeddings. To factor out prior\nknowledge from tSNE embeddings, we propose JEDI that adapts the tSNE objective\nin a principled way using Jensen-Shannon divergence. To factor out prior\nknowledge from any downstream embedding approach, we propose CONFETTI, in which\nwe directly operate on the input distance matrices. Extensive experiments on\nboth synthetic and real world data show that both methods work well, providing\nembeddings that exhibit meaningful structure that would otherwise remain\nhidden.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:10:36 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Heiter", "Edith", ""], ["Fischer", "Jonas", ""], ["Vreeken", "Jilles", ""]]}, {"id": "2103.01887", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, and Ilias Zadik", "title": "Self-Regularity of Non-Negative Output Weights for Overparameterized\n  Two-Layer Neural Networks", "comments": "34 pages. Some of the results in the present paper are significantly\n  strengthened versions of certain results appearing in arXiv:2003.10523", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a two-layer neural network with sigmoid,\nrectified linear unit (ReLU), or binary step activation functions that \"fits\" a\ntraining data set as accurately as possible as quantified by the training\nerror; and study the following question: \\emph{does a low training error\nguarantee that the norm of the output layer (outer norm) itself is small?} We\nanswer affirmatively this question for the case of non-negative output weights.\nUsing a simple covering number argument, we establish that under quite mild\ndistributional assumptions on the input/label pairs; any such network achieving\na small training error on polynomially many data necessarily has a\nwell-controlled outer norm. Notably, our results (a) have a polynomial (in $d$)\nsample complexity, (b) are independent of the number of hidden units (which can\npotentially be very high), (c) are oblivious to the training algorithm; and (d)\nrequire quite mild assumptions on the data (in particular the input vector\n$X\\in\\mathbb{R}^d$ need not have independent coordinates). We then leverage our\nbounds to establish generalization guarantees for such networks through\n\\emph{fat-shattering dimension}, a scale-sensitive measure of the complexity\nclass that the network architectures we investigate belong to. Notably, our\ngeneralization bounds also have good sample complexity (polynomials in $d$ with\na low degree), and are in fact near-linear for some important cases of\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:36:03 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "2103.01890", "submitter": "Neil Jethani", "authors": "Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, Rajesh\n  Ranganath", "title": "Have We Learned to Explain?: How Interpretability Methods Can Learn to\n  Encode Predictions in their Interpretations", "comments": "15 pages, 3 figures, Proceedings of the 24th International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the need for interpretable machine learning has been established, many\ncommon approaches are slow, lack fidelity, or hard to evaluate. Amortized\nexplanation methods reduce the cost of providing interpretations by learning a\nglobal selector model that returns feature importances for a single instance of\ndata. The selector model is trained to optimize the fidelity of the\ninterpretations, as evaluated by a predictor model for the target. Popular\nmethods learn the selector and predictor model in concert, which we show allows\npredictions to be encoded within interpretations. We introduce EVAL-X as a\nmethod to quantitatively evaluate interpretations and REAL-X as an amortized\nexplanation method, which learn a predictor model that approximates the true\ndata generating distribution given any subset of the input. We show EVAL-X can\ndetect when predictions are encoded in interpretations and show the advantages\nof REAL-X through quantitative and radiologist evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:42:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jethani", "Neil", ""], ["Sudarshan", "Mukund", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2103.01901", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Qinqing Zheng, Qi Long, Weijie J. Su", "title": "A Theorem of the Alternative for Personalized Federated Learning", "comments": "50 pages (main manuscript: 25 pages, appendices: 25 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely recognized difficulty in federated learning arises from the\nstatistical heterogeneity among clients: local datasets often come from\ndifferent but not entirely unrelated distributions, and personalization is,\ntherefore, necessary to achieve optimal results from each individual's\nperspective. In this paper, we show how the excess risks of personalized\nfederated learning with a smooth, strongly convex loss depend on data\nheterogeneity from a minimax point of view. Our analysis reveals a surprising\ntheorem of the alternative for personalized federated learning: there exists a\nthreshold such that (a) if a certain measure of data heterogeneity is below\nthis threshold, the FedAvg algorithm [McMahan et al., 2017] is minimax optimal;\n(b) when the measure of heterogeneity is above this threshold, then doing pure\nlocal training (i.e., clients solve empirical risk minimization problems on\ntheir local datasets without any communication) is minimax optimal. As an\nimplication, our results show that the presumably difficult\n(infinite-dimensional) problem of adapting to client-wise heterogeneity can be\nreduced to a simple binary decision problem of choosing between the two\nbaseline algorithms. Our analysis relies on a new notion of algorithmic\nstability that takes into account the nature of federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:58:20 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chen", "Shuxiao", ""], ["Zheng", "Qinqing", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2103.01904", "submitter": "Kaleb Smith", "authors": "Kaleb E. Smith and Anthony O. Smith", "title": "A Spectral Enabled GAN for Time Series Data Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time dependent data is a main source of information in today's data driven\nworld. Generating this type of data though has shown its challenges and made it\nan interesting research area in the field of generative machine learning. One\nsuch approach was that by Smith et al. who developed Time Series Generative\nAdversarial Network (TSGAN) which showed promising performance in generating\ntime dependent data and the ability of few shot generation though being flawed\nin certain aspects of training and learning. This paper looks to improve on the\nresults from TSGAN and address those flaws by unifying the training of the\nindependent networks in TSGAN and creating a dependency both in training and\nlearning. This improvement, called unified TSGAN (uTSGAN) was tested and\ncomapred both quantitatively and qualitatively to its predecessor on 70\nbenchmark time series data sets used in the community. uTSGAN showed to\noutperform TSGAN in 80\\% of the data sets by the same number of training epochs\nand 60\\% of the data sets in 3/4th the amount of training time or less while\nmaintaining the few shot generation ability with better FID scores across those\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:05:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Smith", "Kaleb E.", ""], ["Smith", "Anthony O.", ""]]}, {"id": "2103.01907", "submitter": "Nikita Kozodoi", "authors": "Nikita Kozodoi, Johannes Jacob, Stefan Lessmann", "title": "Fairness in Credit Scoring: Assessment, Implementation and Profit\n  Implications", "comments": "Accepted to European Journal of Operational Research", "journal-ref": null, "doi": "10.1016/j.ejor.2021.06.023", "report-no": null, "categories": "stat.ML cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of algorithmic decision-making has spawned much research on fair\nmachine learning (ML). Financial institutions use ML for building risk\nscorecards that support a range of credit-related decisions. Yet, the\nliterature on fair ML in credit scoring is scarce. The paper makes three\ncontributions. First, we revisit statistical fairness criteria and examine\ntheir adequacy for credit scoring. Second, we catalog algorithmic options for\nincorporating fairness goals in the ML model development pipeline. Last, we\nempirically compare different fairness processors in a profit-oriented credit\nscoring context using real-world data. The empirical results substantiate the\nevaluation of fairness measures, identify suitable options to implement fair\ncredit scoring, and clarify the profit-fairness trade-off in lending decisions.\nWe find that multiple fairness criteria can be approximately satisfied at once\nand recommend separation as a proper criterion for measuring the fairness of a\nscorecard. We also find fair in-processors to deliver a good balance between\nprofit and fairness and show that algorithmic discrimination can be reduced to\na reasonable level at a relatively low cost. The codes corresponding to the\npaper are available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:06:44 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 20:45:21 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 09:39:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kozodoi", "Nikita", ""], ["Jacob", "Johannes", ""], ["Lessmann", "Stefan", ""]]}, {"id": "2103.01926", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe", "title": "Slow-Growing Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest's performance can be matched by a single slow-growing tree\n(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits\nthe view that CART is an extreme case of an iterative weighted least square\nprocedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests\n(RF) is presented. Greedy ML algorithms' outcomes can be improved using either\n\"slow learning\" or diversification. SGT applies the former to estimate a single\ndeep tree, and Booging (bagging stochastic BT with a high learning rate) uses\nthe latter with additive shallow trees. The performance of this tree ensemble\nquaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:37:13 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 16:37:56 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Coulombe", "Philippe Goulet", ""]]}, {"id": "2103.01933", "submitter": "Tianmin Shu", "authors": "Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, Joshua B.\n  Tenenbaum", "title": "PHASE: PHysically-grounded Abstract Social Events for Machine Social\n  Perception", "comments": "The first two authors contributed equally; AAAI 2021; 13 pages, 7\n  figures; Project page: https://www.tshu.io/PHASE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perceive and reason about social interactions in the context\nof physical environments is core to human social intelligence and human-machine\ncooperation. However, no prior dataset or benchmark has systematically\nevaluated physically grounded perception of complex social interactions that go\nbeyond short actions, such as high-fiving, or simple group activities, such as\ngathering. In this work, we create a dataset of physically-grounded abstract\nsocial events, PHASE, that resemble a wide range of real-life social\ninteractions by including social concepts such as helping another agent. PHASE\nconsists of 2D animations of pairs of agents moving in a continuous space\ngenerated procedurally using a physics engine and a hierarchical planner.\nAgents have a limited field of view, and can interact with multiple objects, in\nan environment that has multiple landmarks and obstacles. Using PHASE, we\ndesign a social recognition task and a social prediction task. PHASE is\nvalidated with human experiments demonstrating that humans perceive rich\ninteractions in the social events, and that the simulated agents behave\nsimilarly to humans. As a baseline model, we introduce a Bayesian inverse\nplanning approach, SIMPLE (SIMulation, Planning and Local Estimation), which\noutperforms state-of-the-art feed-forward neural networks. We hope that PHASE\ncan serve as a difficult new challenge for developing new models that can\nrecognize complex social interactions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:44:57 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:13:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Netanyahu", "Aviv", ""], ["Shu", "Tianmin", ""], ["Katz", "Boris", ""], ["Barbu", "Andrei", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2103.01937", "submitter": "Aniket Didolkar", "authors": "Anirudh Goyal, Aniket Didolkar, Nan Rosemary Ke, Charles Blundell,\n  Philippe Beaudoin, Nicolas Heess, Michael Mozer, Yoshua Bengio", "title": "Neural Production Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual environments are structured, consisting of distinct objects or\nentities. These entities have properties -- both visible and latent -- that\ndetermine the manner in which they interact with one another. To partition\nimages into entities, deep-learning researchers have proposed structural\ninductive biases such as slot-based architectures. To model interactions among\nentities, equivariant graph neural nets (GNNs) are used, but these are not\nparticularly well suited to the task for two reasons. First, GNNs do not\npredispose interactions to be sparse, as relationships among independent\nentities are likely to be. Second, GNNs do not factorize knowledge about\ninteractions in an entity-conditional manner. As an alternative, we take\ninspiration from cognitive science and resurrect a classic approach, production\nsystems, which consist of a set of rule templates that are applied by binding\nplaceholder variables in the rules to specific entities. Rules are scored on\ntheir match to entities, and the best fitting rules are applied to update\nentity properties. In a series of experiments, we demonstrate that this\narchitecture achieves a flexible, dynamic flow of control and serves to\nfactorize entity-specific and rule-based information. This disentangling of\nknowledge achieves robust future-state prediction in rich visual environments,\noutperforming state-of-the-art methods using GNNs, and allows for the\nextrapolation from simple (few object) environments to more complex\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:53:20 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 18:00:29 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Goyal", "Anirudh", ""], ["Didolkar", "Aniket", ""], ["Ke", "Nan Rosemary", ""], ["Blundell", "Charles", ""], ["Beaudoin", "Philippe", ""], ["Heess", "Nicolas", ""], ["Mozer", "Michael", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2103.02008", "submitter": "Pierre Baudot", "authors": "Pierre Baudot", "title": "On Information (pseudo) Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This short note revisit information metric, underlining that it is a pseudo\nmetric on manifolds of observables (random variables), rather than as usual on\nprobability laws. Geodesics are characterized in terms of their boundaries and\nconditional independence condition. Pythagorean theorem is given, providing in\nspecial case potentially interesting natural integer triplets. This metric is\ncomputed for illustration on Diabetes dataset using infotopo package.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:07:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Baudot", "Pierre", ""]]}, {"id": "2103.02062", "submitter": "Yucheng Lu", "authors": "Yucheng Lu, Youngsuk Park, Lifan Chen, Yuyang Wang, Christopher De Sa,\n  Dean Foster", "title": "Variance Reduced Training with Stratified Sampling for Forecasting\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In large-scale time series forecasting, one often encounters the situation\nwhere the temporal patterns of time series, while drifting over time, differ\nfrom one another in the same dataset. In this paper, we provably show under\nsuch heterogeneity, training a forecasting model with commonly used stochastic\noptimizers (e.g. SGD) potentially suffers large variance on gradient\nestimation, and thus incurs long-time training. We show that this issue can be\nefficiently alleviated via stratification, which allows the optimizer to sample\nfrom pre-grouped time series strata. For better trading-off gradient variance\nand computation complexity, we further propose SCott (Stochastic Stratified\nControl Variate Gradient Descent), a variance reduced SGD-style optimizer that\nutilizes stratified sampling via control variate. In theory, we provide the\nconvergence guarantee of SCott on smooth non-convex objectives. Empirically, we\nevaluate SCott and other baseline optimizers on both synthetic and real-world\ntime series forecasting problems, and demonstrate SCott converges faster with\nrespect to both iterations and wall clock time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:23:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 04:06:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lu", "Yucheng", ""], ["Park", "Youngsuk", ""], ["Chen", "Lifan", ""], ["Wang", "Yuyang", ""], ["De Sa", "Christopher", ""], ["Foster", "Dean", ""]]}, {"id": "2103.02138", "submitter": "Tanya Marwah", "authors": "Tanya Marwah, Zachary C. Lipton, Andrej Risteski", "title": "Parametric Complexity Bounds for Approximating PDEs with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experiments have shown that deep networks can approximate solutions to\nhigh-dimensional PDEs, seemingly escaping the curse of dimensionality. However,\nquestions regarding the theoretical basis for such approximations, including\nthe required network size, remain open. In this paper, we investigate the\nrepresentational power of neural networks for approximating solutions to linear\nelliptic PDEs with Dirichlet boundary conditions. We prove that when a PDE's\ncoefficients are representable by small neural networks, the parameters\nrequired to approximate its solution scale polynomially with the input\ndimension $d$ and proportionally to the parameter counts of the coefficient\nnetworks. To this we end, we develop a proof technique that simulates gradient\ndescent (in an appropriate Hilbert space) by growing a neural network\narchitecture whose iterates each participate as sub-networks in their (slightly\nlarger) successors, and converge to the solution of the PDE. We bound the size\nof the solution, showing a polynomial dependence on $d$ and no dependence on\nthe volume of the domain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:42:57 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 16:25:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Marwah", "Tanya", ""], ["Lipton", "Zachary C.", ""], ["Risteski", "Andrej", ""]]}, {"id": "2103.02164", "submitter": "Yinjun Wu", "authors": "Yinjun Wu, Jingchao Ni, Wei Cheng, Bo Zong, Dongjin Song, Zhengzhang\n  Chen, Yanchi Liu, Xuchao Zhang, Haifeng Chen, Susan Davidson", "title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust\n  Forecasting on Sparse Multivariate Time Series", "comments": "This paper is accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting on sparse multivariate time series (MTS) aims to model the\npredictors of future values of time series given their incomplete past, which\nis important for many emerging applications. However, most existing methods\nprocess MTS's individually, and do not leverage the dynamic distributions\nunderlying the MTS's, leading to sub-optimal results when the sparsity is high.\nTo address this challenge, we propose a novel generative model, which tracks\nthe transition of latent clusters, instead of isolated feature representations,\nto achieve robust modeling. It is characterized by a newly designed dynamic\nGaussian mixture distribution, which captures the dynamics of clustering\nstructures, and is used for emitting timeseries. The generative model is\nparameterized by neural networks. A structured inference network is also\ndesigned for enabling inductive analysis. A gating mechanism is further\nintroduced to dynamically tune the Gaussian mixture distributions. Extensive\nexperimental results on a variety of real-life datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:10:07 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Yinjun", ""], ["Ni", "Jingchao", ""], ["Cheng", "Wei", ""], ["Zong", "Bo", ""], ["Song", "Dongjin", ""], ["Chen", "Zhengzhang", ""], ["Liu", "Yanchi", ""], ["Zhang", "Xuchao", ""], ["Chen", "Haifeng", ""], ["Davidson", "Susan", ""]]}, {"id": "2103.02165", "submitter": "Jed Duersch", "authors": "Jed A. Duersch and Thomas A. Catanach", "title": "Parsimonious Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference provides a uniquely rigorous approach to obtain principled\njustification for uncertainty in predictions, yet it is difficult to articulate\nsuitably general prior belief in the machine learning context, where\ncomputational architectures are pure abstractions subject to frequent\nmodifications by practitioners attempting to improve results. Parsimonious\ninference is an information-theoretic formulation of inference over arbitrary\narchitectures that formalizes Occam's Razor; we prefer simple and sufficient\nexplanations. Our universal hyperprior assigns plausibility to prior\ndescriptions, encoded as sequences of symbols, by expanding on the core\nrelationships between program length, Kolmogorov complexity, and Solomonoff's\nalgorithmic probability. We then cast learning as information minimization over\nour composite change in belief when an architecture is specified, training data\nare observed, and model parameters are inferred. By distinguishing model\ncomplexity from prediction information, our framework also quantifies the\nphenomenon of memorization.\n  Although our theory is general, it is most critical when datasets are\nlimited, e.g. small or skewed. We develop novel algorithms for polynomial\nregression and random forests that are suitable for such data, as demonstrated\nby our experiments. Our approaches combine efficient encodings with prudent\nsampling strategies to construct predictive ensembles without cross-validation,\nthus addressing a fundamental challenge in how to efficiently obtain\npredictions from data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:13:14 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Duersch", "Jed A.", ""], ["Catanach", "Thomas A.", ""]]}, {"id": "2103.02265", "submitter": "Lucas Lingle", "authors": "Lucas D. Lingle", "title": "Meta-Learning with Variational Bayes", "comments": "38 pages, 6 figures. v2 updates: link to code and light edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of meta-learning seeks to improve the ability of today's machine\nlearning systems to adapt efficiently to small amounts of data. Typically this\nis accomplished by training a system with a parametrized update rule to improve\na task-relevant objective based on supervision or a reward function. However,\nin many domains of practical interest, task data is unlabeled, or reward\nfunctions are unavailable. In this paper we introduce a new approach to address\nthe more general problem of generative meta-learning, which we argue is an\nimportant prerequisite for obtaining human-level cognitive flexibility in\nartificial agents, and can benefit many practical applications along the way.\nOur contribution leverages the AEVB framework and mean-field variational Bayes,\nand creates fast-adapting latent-space generative models. At the heart of our\ncontribution is a new result, showing that for a broad class of deep generative\nlatent variable models, the relevant VB updates do not depend on any generative\nneural network. The theoretical merits of our approach are reflected in\nempirical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:02:01 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:05:36 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lingle", "Lucas D.", ""]]}, {"id": "2103.02325", "submitter": "Maksym Andriushchenko", "authors": "Klim Kireev, Maksym Andriushchenko, Nicolas Flammarion", "title": "On the effectiveness of adversarial training against common corruptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions. Then we explain why adversarial training performs better\nthan data augmentation with simple Gaussian noise which has been observed to be\na meaningful baseline on common corruptions. Related to this, we identify the\n$\\sigma$-overfitting phenomenon when Gaussian augmentation overfits to a\nparticular standard deviation used for training which has a significant\ndetrimental effect on common corruption accuracy. We discuss how to alleviate\nthis problem and then how to further enhance $\\ell_p$ adversarial training by\nintroducing an efficient relaxation of adversarial training with learned\nperceptual image patch similarity as the distance metric. Through experiments\non CIFAR-10 and ImageNet-100, we show that our approach does not only improve\nthe $\\ell_p$ adversarial training baseline but also has cumulative gains with\ndata augmentation methods such as AugMix, ANT, and SIN leading to\nstate-of-the-art performance on common corruptions. The code of our experiments\nis publicly available at https://github.com/tml-epfl/adv-training-corruptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:04:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kireev", "Klim", ""], ["Andriushchenko", "Maksym", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2103.02349", "submitter": "Narges Pourshahrokhi", "authors": "Narges Pourshahrokhi, Samaneh Kouchaki, Kord M. Kober, Christine\n  Miaskowski, Payam Barnaghi", "title": "A Hamiltonian Monte Carlo Model for Imputation and Augmentation of\n  Healthcare Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Missing values exist in nearly all clinical studies because data for a\nvariable or question are not collected or not available. Inadequate handling of\nmissing values can lead to biased results and loss of statistical power in\nanalysis. Existing models usually do not consider privacy concerns or do not\nutilise the inherent correlations across multiple features to impute the\nmissing values. In healthcare applications, we are usually confronted with high\ndimensional and sometimes small sample size datasets that need more effective\naugmentation or imputation techniques. Besides, imputation and augmentation\nprocesses are traditionally conducted individually. However, imputing missing\nvalues and augmenting data can significantly improve generalisation and avoid\nbias in machine learning models. A Bayesian approach to impute missing values\nand creating augmented samples in high dimensional healthcare data is proposed\nin this work. We propose folded Hamiltonian Monte Carlo (F-HMC) with Bayesian\ninference as a more practical approach to process the cross-dimensional\nrelations by applying a random walk and Hamiltonian dynamics to adapt posterior\ndistribution and generate large-scale samples. The proposed method is applied\nto a cancer symptom assessment dataset and confirmed to enrich the quality of\ndata in precision, accuracy, recall, F1 score, and propensity metric.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:57:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Pourshahrokhi", "Narges", ""], ["Kouchaki", "Samaneh", ""], ["Kober", "Kord M.", ""], ["Miaskowski", "Christine", ""], ["Barnaghi", "Payam", ""]]}, {"id": "2103.02351", "submitter": "Amirkeivan Mohtashami", "authors": "Sebastian U. Stich, Amirkeivan Mohtashami, Martin Jaggi", "title": "Critical Parameters for Scalable Distributed Learning with Large Batches\n  and Asynchronous Updates", "comments": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been experimentally observed that the efficiency of distributed\ntraining with stochastic gradient (SGD) depends decisively on the batch size\nand -- in asynchronous implementations -- on the gradient staleness.\nEspecially, it has been observed that the speedup saturates beyond a certain\nbatch size and/or when the delays grow too large. We identify a data-dependent\nparameter that explains the speedup saturation in both these settings. Our\ncomprehensive theoretical analysis, for strongly convex, convex and non-convex\nsettings, unifies and generalized prior work directions that often focused on\nonly one of these two aspects. In particular, our approach allows us to derive\nimproved speedup results under frequently considered sparsity assumptions. Our\ninsights give rise to theoretically based guidelines on how the learning rates\ncan be adjusted in practice. We show that our results are tight and illustrate\nkey findings in numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:08:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Mohtashami", "Amirkeivan", ""], ["Jaggi", "Martin", ""]]}, {"id": "2103.02405", "submitter": "Arshdeep Sekhon", "authors": "Arshdeep Sekhon, Zhe Wang, Yanjun Qi", "title": "Relate and Predict: Structure-Aware Prediction with Jointly Optimized\n  Neural DAG", "comments": "8 pages, 6 figures, version appeared in ICML Workshop 2020 Graph\n  Representation Learning and Beyond (GRL+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding relationships between feature variables is one important way\nhumans use to make decisions. However, state-of-the-art deep learning studies\neither focus on task-agnostic statistical dependency learning or do not model\nexplicit feature dependencies during prediction. We propose a deep neural\nnetwork framework, dGAP, to learn neural dependency Graph and optimize\nstructure-Aware target Prediction simultaneously. dGAP trains towards a\nstructure self-supervision loss and a target prediction loss jointly. Our\nmethod leads to an interpretable model that can disentangle sparse feature\nrelationships, informing the user how relevant dependencies impact the target\ntask. We empirically evaluate dGAP on multiple simulated and real datasets.\ndGAP is not only more accurate, but can also recover correct dependency\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:55:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sekhon", "Arshdeep", ""], ["Wang", "Zhe", ""], ["Qi", "Yanjun", ""]]}, {"id": "2103.02438", "submitter": "Adam Foster", "authors": "Adam Foster, Desi R. Ivanova, Ilyas Malik, Tom Rainforth", "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of\nadaptive Bayesian experimental design that allows experiments to be run in\nreal-time. Traditional sequential Bayesian optimal experimental design\napproaches require substantial computation at each stage of the experiment.\nThis makes them unsuitable for most real-world applications, where decisions\nmust typically be made quickly. DAD addresses this restriction by learning an\namortized design network upfront and then using this to rapidly run (multiple)\nadaptive experiments at deployment time. This network represents a design\npolicy which takes as input the data from previous steps, and outputs the next\ndesign using a single forward pass; these design decisions can be made in\nmilliseconds during the live experiment. To train the network, we introduce\ncontrastive information bounds that are suitable objectives for the sequential\nsetting, and propose a customized network architecture that exploits key\nsymmetries. We demonstrate that DAD successfully amortizes the process of\nexperimental design, outperforming alternative strategies on a number of\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:43:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:18:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Foster", "Adam", ""], ["Ivanova", "Desi R.", ""], ["Malik", "Ilyas", ""], ["Rainforth", "Tom", ""]]}, {"id": "2103.02477", "submitter": "Michael Oberst", "authors": "Michael Oberst, Nikolaj Thams, Jonas Peters, David Sontag", "title": "Regularizing towards Causal Invariance: Linear Models with Proxies", "comments": "ICML 2021 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning linear models whose predictive performance\nis robust to causal interventions on unobserved variables, when noisy proxies\nof those variables are available. Our approach takes the form of a\nregularization term that trades off between in-distribution performance and\nrobustness to interventions. Under the assumption of a linear structural causal\nmodel, we show that a single proxy can be used to create estimators that are\nprediction optimal under interventions of bounded strength. This strength\ndepends on the magnitude of the measurement noise in the proxy, which is, in\ngeneral, not identifiable. In the case of two proxy variables, we propose a\nmodified estimator that is prediction optimal under interventions up to a known\nstrength. We further show how to extend these estimators to scenarios where\nadditional information about the \"test time\" intervention is available during\ntraining. We evaluate our theoretical findings in synthetic experiments and\nusing real data of hourly pollution levels across several cities in China.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:39:35 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 20:47:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Oberst", "Michael", ""], ["Thams", "Nikolaj", ""], ["Peters", "Jonas", ""], ["Sontag", "David", ""]]}, {"id": "2103.02506", "submitter": "Michael Lingzhi Li", "authors": "Dimitris Bertsimas, Michael Lingzhi Li", "title": "Stochastic Cutting Planes for Data-Driven Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a stochastic version of the cutting-plane method for a large\nclass of data-driven Mixed-Integer Nonlinear Optimization (MINLO) problems. We\nshow that under very weak assumptions the stochastic algorithm is able to\nconverge to an $\\epsilon$-optimal solution with high probability. Numerical\nexperiments on several problems show that stochastic cutting planes is able to\ndeliver a multiple order-of-magnitude speedup compared to the standard\ncutting-plane method. We further experimentally explore the lower limits of\nsampling for stochastic cutting planes and show that for many problems, a\nsampling size of $O(\\sqrt[3]{n})$ appears to be sufficient for high quality\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:21:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "2103.02512", "submitter": "Ali Vakilian", "authors": "Yury Makarychev and Ali Vakilian", "title": "Approximation Algorithms for Socially Fair Clustering", "comments": "COLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation\nalgorithm for socially fair clustering with the $\\ell_p$-objective. In this\nproblem, we are given a set of points in a metric space. Each point belongs to\none (or several) of $\\ell$ groups. The goal is to find a $k$-medians,\n$k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good\nfor all of the groups. More precisely, we need to find a set of $k$ centers $C$\nso as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group\n}j} d(u,C)^p$. The socially fair clustering problem was independently proposed\nby Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and\nVenkatasubramanian [2021]. Our algorithm improves and generalizes their\n$O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation\nfor the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain\nour result, we introduce a strengthened LP relaxation and show that it has an\nintegrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$.\nAdditionally, we present a bicriteria approximation algorithm, which\ngeneralizes the bicriteria approximation of Abbasi et al. [2021].\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:36:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 04:06:21 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Makarychev", "Yury", ""], ["Vakilian", "Ali", ""]]}, {"id": "2103.02559", "submitter": "Akshay Agrawal", "authors": "Akshay Agrawal, Alnur Ali, Stephen Boyd", "title": "Minimum-Distortion Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the vector embedding problem. We are given a finite set of items,\nwith the goal of assigning a representative vector to each one, possibly under\nsome constraints (such as the collection of vectors being standardized, i.e.,\nhave zero mean and unit covariance). We are given data indicating that some\npairs of items are similar, and optionally, some other pairs are dissimilar.\nFor pairs of similar items, we want the corresponding vectors to be near each\nother, and for dissimilar pairs, we want the corresponding vectors to not be\nnear each other, measured in Euclidean distance. We formalize this by\nintroducing distortion functions, defined for some pairs of the items. Our goal\nis to choose an embedding that minimizes the total distortion, subject to the\nconstraints. We call this the minimum-distortion embedding (MDE) problem.\n  The MDE framework is simple but general. It includes a wide variety of\nembedding methods, such as spectral embedding, principal component analysis,\nmultidimensional scaling, dimensionality reduction methods (like Isomap and\nUMAP), force-directed layout, and others. It also includes new embeddings, and\nprovides principled ways of validating historical and new embeddings alike.\n  We develop a projected quasi-Newton method that approximately solves MDE\nproblems and scales to large data sets. We implement this method in PyMDE, an\nopen-source Python package. In PyMDE, users can select from a library of\ndistortion functions and constraints or specify custom ones, making it easy to\nrapidly experiment with different embeddings. Our software scales to data sets\nwith millions of items and tens of millions of distortion functions. To\ndemonstrate our method, we compute embeddings for several real-world data sets,\nincluding images, an academic co-author network, US county demographic data,\nand single-cell mRNA transcriptomes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:52:13 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 18:15:56 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 20:23:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Agrawal", "Akshay", ""], ["Ali", "Alnur", ""], ["Boyd", "Stephen", ""]]}, {"id": "2103.02565", "submitter": "Somesh Mohapatra", "authors": "Somesh Mohapatra, Joyce An, Rafael G\\'omez-Bombarelli", "title": "Chemistry-informed Macromolecule Graph Representation for Similarity\n  Computation and Supervised Learning", "comments": "Main text: 6 pages, 3 figures, 1 table; Appendix: 21 pages, 26\n  figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY q-bio.BM q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Macromolecules are large, complex molecules composed of covalently bonded\nmonomer units, existing in different stereochemical configurations and\ntopologies. As a result of such chemical diversity, representing, comparing,\nand learning over macromolecules emerge as critical challenges. To address\nthis, we developed a macromolecule graph representation, with monomers and\nbonds as nodes and edges, respectively. We captured the inherent chemistry of\nthe macromolecule by using molecular fingerprints for node and edge attributes.\nFor the first time, we demonstrated computation of chemical similarity between\n2 macromolecules of varying chemistry and topology, using exact graph edit\ndistances and graph kernels. We trained interpretable graph neural networks for\na variety of glycan classification tasks, achieving state-of-the-art results.\nOur work has two-fold implications - it provides a general framework for\nrepresentation, comparison, and learning of macromolecules, and it enables\nquantitative chemistry-informed decision-making and iterative design in the\nmacromolecular chemical space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:05:57 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:51:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Mohapatra", "Somesh", ""], ["An", "Joyce", ""], ["G\u00f3mez-Bombarelli", "Rafael", ""]]}, {"id": "2103.02582", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden", "title": "D'ya like DAGs? A Survey on Structure Learning and Causal Discovery", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal reasoning is a crucial part of science and human intelligence. In\norder to discover causal relationships from data, we need structure discovery\nmethods. We provide a review of background theory and a survey of methods for\nstructure discovery. We primarily focus on modern, continuous optimization\nmethods, and provide reference to further resources such as benchmark datasets\nand software packages. Finally, we discuss the assumptive leap required to take\nus from structure to causality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:24:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 17:37:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2103.02588", "submitter": "Wei Chen", "authors": "Jun Wang, Wei Chen, Mark Fuge, Rahul Rai", "title": "IH-GAN: A Conditional Generative Model for Implicit Surface-Based\n  Inverse Design of Cellular Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variable-density cellular structures can overcome connectivity and\nmanufacturability issues of topologically-optimized, functionally graded\nstructures, particularly when those structures are represented as discrete\ndensity maps. One na\\\"ive approach to creating variable-density cellular\nstructures is simply replacing the discrete density map with an unselective\ntype of unit cells having corresponding densities. However, doing so breaks the\ndesired mechanical behavior, as equivalent density alone does not guarantee\nequivalent mechanical properties. Another approach uses homogenization methods\nto estimate each pre-defined unit cell's effective properties and remaps the\nunit cells following a scaling law. However, a scaling law merely mitigates the\nproblem by performing an indirect and inaccurate mapping from the material\nproperty space to single-type unit cells. In contrast, we propose a deep\ngenerative model that resolves this problem by automatically learning an\naccurate mapping and generating diverse cellular unit cells conditioned on\ndesired properties (i.e., Young's modulus and Poisson's ratio). We demonstrate\nour method via the use of implicit function-based unit cells and conditional\ngenerative adversarial networks. Results show that our method can 1) generate\nvarious unit cells that satisfy given material properties with high accuracy\n(relative error <5%), 2) create functionally graded cellular structures with\nhigh-quality interface connectivity (98.7% average overlap area at interfaces),\nand 3) improve the structural performance over the conventional\ntopology-optimized variable-density structure (84.4% reduction in concentrated\nstress and extra 7% reduction in displacement).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:39:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wang", "Jun", ""], ["Chen", "Wei", ""], ["Fuge", "Mark", ""], ["Rai", "Rahul", ""]]}, {"id": "2103.02667", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky", "title": "Out of Distribution Generalization in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning has achieved tremendous success in a variety of domains in\nrecent years. However, a lot of these success stories have been in places where\nthe training and the testing distributions are extremely similar to each other.\nIn everyday situations when models are tested in slightly different data than\nthey were trained on, ML algorithms can fail spectacularly. This research\nattempts to formally define this problem, what sets of assumptions are\nreasonable to make in our data and what kind of guarantees we hope to obtain\nfrom them. Then, we focus on a certain class of out of distribution problems,\ntheir assumptions, and introduce simple algorithms that follow from these\nassumptions that are able to provide more reliable generalization. A central\ntopic in the thesis is the strong link between discovering the causal structure\nof the data, finding features that are reliable (when using them to predict)\nregardless of their context, and out of distribution generalization.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 20:35:19 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Arjovsky", "Martin", ""]]}, {"id": "2103.02740", "submitter": "Bingbin Liu", "authors": "Bingbin Liu, Pradeep Ravikumar, Andrej Risteski", "title": "Contrastive learning of strong-mixing continuous-time stochastic\n  processes", "comments": "Appearing in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning is a family of self-supervised methods where a model is\ntrained to solve a classification task constructed from unlabeled data. It has\nrecently emerged as one of the leading learning paradigms in the absence of\nlabels across many different domains (e.g. brain imaging, text, images).\nHowever, theoretical understanding of many aspects of training, both\nstatistical and algorithmic, remain fairly elusive.\n  In this work, we study the setting of time series -- more precisely, when we\nget data from a strong-mixing continuous-time stochastic process. We show that\na properly constructed contrastive learning task can be used to estimate the\ntransition kernel for small-to-mid-range intervals in the diffusion case.\nMoreover, we give sample complexity bounds for solving this task and\nquantitatively characterize what the value of the contrastive loss implies for\ndistributional closeness of the learned kernel. As a byproduct, we illuminate\nthe appropriate settings for the contrastive distribution, as well as other\nhyperparameters in this setup.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:06:47 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Bingbin", ""], ["Ravikumar", "Pradeep", ""], ["Risteski", "Andrej", ""]]}, {"id": "2103.02753", "submitter": "Mark Stamp", "authors": "Jing Zhao and Samanvitha Basole and Mark Stamp", "title": "Malware Classification with GMM-HMM Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discrete hidden Markov models (HMM) are often applied to malware detection\nand classification problems. However, the continuous analog of discrete HMMs,\nthat is, Gaussian mixture model-HMMs (GMM-HMM), are rarely considered in the\nfield of cybersecurity. In this paper, we use GMM-HMMs for malware\nclassification and we compare our results to those obtained using discrete\nHMMs. As features, we consider opcode sequences and entropy-based sequences.\nFor our opcode features, GMM-HMMs produce results that are comparable to those\nobtained using discrete HMMs, whereas for our entropy-based features, GMM-HMMs\ngenerally improve significantly on the classification results that we have\nachieved with discrete HMMs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:23:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhao", "Jing", ""], ["Basole", "Samanvitha", ""], ["Stamp", "Mark", ""]]}, {"id": "2103.02761", "submitter": "Mayee F. Chen", "authors": "Mayee F. Chen, Benjamin Cohen-Wang, Stephen Mussmann, Frederic Sala,\n  Christopher R\\'e", "title": "Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments\n  Latent Variable Estimation", "comments": "To appear in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling data for modern machine learning is expensive and time-consuming.\nLatent variable models can be used to infer labels from weaker,\neasier-to-acquire sources operating on unlabeled data. Such models can also be\ntrained using labeled data, presenting a key question: should a user invest in\nfew labeled or many unlabeled points? We answer this via a framework centered\non model misspecification in method-of-moments latent variable estimation. Our\ncore result is a bias-variance decomposition of the generalization error, which\nshows that the unlabeled-only approach incurs additional bias under\nmisspecification. We then introduce a correction that provably removes this\nbias in certain cases. We apply our decomposition framework to three scenarios\n-- well-specified, misspecified, and corrected models -- to 1) choose between\nlabeled and unlabeled data and 2) learn from their combination. We observe\ntheoretically and with synthetic experiments that for well-specified models,\nlabeled points are worth a constant factor more than unlabeled points. With\nmisspecification, however, their relative value is higher due to the additional\nbias but can be reduced with correction. We also apply our approach to study\nreal-world weak supervision techniques for dataset construction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:52:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Chen", "Mayee F.", ""], ["Cohen-Wang", "Benjamin", ""], ["Mussmann", "Stephen", ""], ["Sala", "Frederic", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2103.02827", "submitter": "Audrey Huang", "authors": "Audrey Huang, Liu Leqi, Zachary C. Lipton, Kamyar Azizzadenesheli", "title": "On the Convergence and Optimality of Policy Gradient for Markov Coherent\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to model risk aversion in reinforcement learning, an emerging line\nof research adapts familiar algorithms to optimize coherent risk functionals, a\nclass that includes conditional value-at-risk (CVaR). Because optimizing the\ncoherent risk is difficult in Markov decision processes, recent work tends to\nfocus on the Markov coherent risk (MCR), a time-consistent surrogate. While,\npolicy gradient (PG) updates have been derived for this objective, it remains\nunclear (i) whether PG finds a global optimum for MCR; (ii) how to estimate the\ngradient in a tractable manner. In this paper, we demonstrate that, in general,\nMCR objectives (unlike the expected return) are not gradient dominated and that\nstationary points are not, in general, guaranteed to be globally optimal.\nMoreover, we present a tight upper bound on the suboptimality of the learned\npolicy, characterizing its dependence on the nonlinearity of the objective and\nthe degree of risk aversion. Addressing (ii), we propose a practical\nimplementation of PG that uses state distribution reweighting to overcome\nprevious limitations. Through experiments, we demonstrate that when the\noptimality gap is small, PG can learn risk-sensitive policies. However, we find\nthat instances with large suboptimality gaps are abundant and easy to\nconstruct, outlining an important challenge for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:11:09 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:49:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Huang", "Audrey", ""], ["Leqi", "Liu", ""], ["Lipton", "Zachary C.", ""], ["Azizzadenesheli", "Kamyar", ""]]}, {"id": "2103.02860", "submitter": "Jiyuan Tu", "authors": "Jiyuan Tu, Weidong Liu, Xiaojun Mao, and Xi Chen", "title": "Variance Reduced Median-of-Means Estimator for Byzantine-Robust\n  Distributed Inference", "comments": "64 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an efficient distributed inference algorithm, which is\nrobust against a moderate fraction of Byzantine nodes, namely arbitrary and\npossibly adversarial machines in a distributed learning system. In robust\nstatistics, the median-of-means (MOM) has been a popular approach to hedge\nagainst Byzantine failures due to its ease of implementation and computational\nefficiency. However, the MOM estimator has the shortcoming in terms of\nstatistical efficiency. The first main contribution of the paper is to propose\na variance reduced median-of-means (VRMOM) estimator, which improves the\nstatistical efficiency over the vanilla MOM estimator and is computationally as\nefficient as the MOM. Based on the proposed VRMOM estimator, we develop a\ngeneral distributed inference algorithm that is robust against Byzantine\nfailures. Theoretically, our distributed algorithm achieves a fast convergence\nrate with only a constant number of rounds of communications. We also provide\nthe asymptotic normality result for the purpose of statistical inference. To\nthe best of our knowledge, this is the first normality result in the setting of\nByzantine-robust distributed learning. The simulation results are also\npresented to illustrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:50:52 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Tu", "Jiyuan", ""], ["Liu", "Weidong", ""], ["Mao", "Xiaojun", ""], ["Chen", "Xi", ""]]}, {"id": "2103.02863", "submitter": "Navyata Sanghvi", "authors": "Navyata Sanghvi, Shinnosuke Usami, Mohit Sharma, Joachim Groeger, Kris\n  Kitani", "title": "Inverse Reinforcement Learning with Explicit Policy Estimates", "comments": "To be published in: Proceedings of the 35th AAAI Conference on\n  Artificial Intelligence, February 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods for solving the inverse reinforcement learning (IRL) problem\nhave been developed independently in machine learning and economics. In\nparticular, the method of Maximum Causal Entropy IRL is based on the\nperspective of entropy maximization, while related advances in the field of\neconomics instead assume the existence of unobserved action shocks to explain\nexpert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability\nmethod, Nested Pseudo-Likelihood Algorithm). In this work, we make previously\nunknown connections between these related methods from both fields. We achieve\nthis by showing that they all belong to a class of optimization problems,\ncharacterized by a common form of the objective, the associated policy and the\nobjective gradient. We demonstrate key computational and algorithmic\ndifferences which arise between the methods due to an approximation of the\noptimal soft value function, and describe how this leads to more efficient\nalgorithms. Using insights which emerge from our study of this class of\noptimization problems, we identify various problem scenarios and investigate\neach method's suitability for these problems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:00:58 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sanghvi", "Navyata", ""], ["Usami", "Shinnosuke", ""], ["Sharma", "Mohit", ""], ["Groeger", "Joachim", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.02893", "submitter": "Shuhei M. Yoshida", "authors": "Shuhei M. Yoshida, Takashi Takenouchi, Masashi Sugiyama", "title": "Lower-Bounded Proper Losses for Weakly Supervised Classification", "comments": "ICML2021 camera ready, code available at\n  https://github.com/yoshum/lower-bounded-proper-losses", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of weakly supervised classification, in\nwhich instances are given weak labels that are produced by some\nlabel-corruption process. The goal is to derive conditions under which loss\nfunctions for weak-label learning are proper and lower-bounded -- two essential\nrequirements for the losses used in class-probability estimation. To this end,\nwe derive a representation theorem for proper losses in supervised learning,\nwhich dualizes the Savage representation. We use this theorem to characterize\nproper weak-label losses and find a condition for them to be lower-bounded.\nFrom these theoretical findings, we derive a novel regularization scheme called\ngeneralized logit squeezing, which makes any proper weak-label loss bounded\nfrom below, without losing properness. Furthermore, we experimentally\ndemonstrate the effectiveness of our proposed approach, as compared to improper\nor unbounded losses. The results highlight the importance of properness and\nlower-boundedness.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:47:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:04:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yoshida", "Shuhei M.", ""], ["Takenouchi", "Takashi", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.02898", "submitter": "Kazu Ghalamkari", "authors": "Kazu Ghalamkari, Mahito Sugiyama", "title": "Fast Tucker Rank Reduction for Non-Negative Tensors Using Mean-Field\n  Approximation", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient low-rank approximation algorithm for non-negative\ntensors. The algorithm is derived from our two findings: First, we show that\nrank-1 approximation for tensors can be viewed as a mean-field approximation by\ntreating each tensor as a probability distribution. Second, we theoretically\nprovide a sufficient condition for distribution parameters to reduce Tucker\nranks of tensors and, interestingly, this sufficient condition can be achieved\nby iterative application of the mean-field approximation. Since the mean-field\napproximation is always given as a closed formula, our findings lead to a fast\nlow-rank approximation algorithm without using a gradient method. We\nempirically demonstrate that our algorithm is faster than the existing\nnon-negative Tucker rank reduction methods with achieving competitive or better\napproximation of given tensors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:57:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:11:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ghalamkari", "Kazu", ""], ["Sugiyama", "Mahito", ""]]}, {"id": "2103.02926", "submitter": "Raoul Heese", "authors": "Raoul Heese, Micha{\\l} Walczak, Michael Bortz, Jochen Schmid", "title": "Calibrated Simplex Mapping Classification", "comments": "24 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel supervised multi-class/single-label classifier that maps\ntraining data onto a linearly separable latent space with a simplex-like\ngeometry. This approach allows us to transform the classification problem into\na well-defined regression problem. For its solution we can choose suitable\ndistance metrics in feature space and regression models predicting latent space\ncoordinates. A benchmark on various artificial and real-world data sets is used\nto demonstrate the calibration qualities and prediction performance of our\nclassifier.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:18:22 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Heese", "Raoul", ""], ["Walczak", "Micha\u0142", ""], ["Bortz", "Michael", ""], ["Schmid", "Jochen", ""]]}, {"id": "2103.02961", "submitter": "Juan E Arco", "authors": "Juan E. Arco, Andr\\'es Ortiz, Javier Ram\\'irez, Francisco J.\n  Mart\\'inez-Murcia, Yu-Dong Zhang, Jordi Broncano, M. \\'Alvaro Berb\\'is,\n  Javier Royuela-del-Val, Antonio Luna, Juan M. G\\'orriz", "title": "Probabilistic combination of eigenlungs-based classifiers for COVID-19\n  diagnosis in chest CT images", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The outbreak of the COVID-19 (Coronavirus disease 2019) pandemic has changed\nthe world. According to the World Health Organization (WHO), there have been\nmore than 100 million confirmed cases of COVID-19, including more than 2.4\nmillion deaths. It is extremely important the early detection of the disease,\nand the use of medical imaging such as chest X-ray (CXR) and chest Computed\nTomography (CCT) have proved to be an excellent solution. However, this process\nrequires clinicians to do it within a manual and time-consuming task, which is\nnot ideal when trying to speed up the diagnosis. In this work, we propose an\nensemble classifier based on probabilistic Support Vector Machine (SVM) in\norder to identify pneumonia patterns while providing information about the\nreliability of the classification. Specifically, each CCT scan is divided into\ncubic patches and features contained in each one of them are extracted by\napplying kernel PCA. The use of base classifiers within an ensemble allows our\nsystem to identify the pneumonia patterns regardless of their size or location.\nDecisions of each individual patch are then combined into a global one\naccording to the reliability of each individual classification: the lower the\nuncertainty, the higher the contribution. Performance is evaluated in a real\nscenario, yielding an accuracy of 97.86%. The large performance obtained and\nthe simplicity of the system (use of deep learning in CCT images would result\nin a huge computational cost) evidence the applicability of our proposal in a\nreal-world environment.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:30:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Arco", "Juan E.", ""], ["Ortiz", "Andr\u00e9s", ""], ["Ram\u00edrez", "Javier", ""], ["Mart\u00ednez-Murcia", "Francisco J.", ""], ["Zhang", "Yu-Dong", ""], ["Broncano", "Jordi", ""], ["Berb\u00eds", "M. \u00c1lvaro", ""], ["Royuela-del-Val", "Javier", ""], ["Luna", "Antonio", ""], ["G\u00f3rriz", "Juan M.", ""]]}, {"id": "2103.03048", "submitter": "Christopher Kanan", "authors": "Usman Mahmood, Robik Shrestha, David D.B. Bates, Lorenzo Mannelli,\n  Giuseppe Corrias, Yusuf Erdi, Christopher Kanan", "title": "Detecting Spurious Correlations with Sanity Tests for Artificial\n  Intelligence Guided Radiology Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has been successful at solving numerous problems\nin machine perception. In radiology, AI systems are rapidly evolving and show\nprogress in guiding treatment decisions, diagnosing, localizing disease on\nmedical images, and improving radiologists' efficiency. A critical component to\ndeploying AI in radiology is to gain confidence in a developed system's\nefficacy and safety. The current gold standard approach is to conduct an\nanalytical validation of performance on a generalization dataset from one or\nmore institutions, followed by a clinical validation study of the system's\nefficacy during deployment. Clinical validation studies are time-consuming, and\nbest practices dictate limited re-use of analytical validation data, so it is\nideal to know ahead of time if a system is likely to fail analytical or\nclinical validation. In this paper, we describe a series of sanity tests to\nidentify when a system performs well on development data for the wrong reasons.\nWe illustrate the sanity tests' value by designing a deep learning system to\nclassify pancreatic cancer seen in computed tomography scans.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:14:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mahmood", "Usman", ""], ["Shrestha", "Robik", ""], ["Bates", "David D. B.", ""], ["Mannelli", "Lorenzo", ""], ["Corrias", "Giuseppe", ""], ["Erdi", "Yusuf", ""], ["Kanan", "Christopher", ""]]}, {"id": "2103.03098", "submitter": "Xavier Bouthillier", "authors": "Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov,\n  Brennan Nichyporuk, Justin Szeto, Naz Sepah, Edward Raff, Kanika Madan,\n  Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Dmitriy Serdyuk, Tal\n  Arbel, Chris Pal, Ga\\\"el Varoquaux and Pascal Vincent", "title": "Accounting for Variance in Machine Learning Benchmarks", "comments": "Submitted to MLSys2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strong empirical evidence that one machine-learning algorithm A outperforms\nanother one B ideally calls for multiple trials optimizing the learning\npipeline over sources of variation such as data sampling, data augmentation,\nparameter initialization, and hyperparameters choices. This is prohibitively\nexpensive, and corners are cut to reach conclusions. We model the whole\nbenchmarking process, revealing that variance due to data sampling, parameter\ninitialization and hyperparameter choice impact markedly the results. We\nanalyze the predominant comparison methods used today in the light of this\nvariance. We show a counter-intuitive result that adding more sources of\nvariation to an imperfect estimator approaches better the ideal estimator at a\n51 times reduction in compute cost. Building on these results, we study the\nerror rate of detecting improvements, on five different deep-learning\ntasks/architectures. This study leads us to propose recommendations for\nperformance comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:39:49 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bouthillier", "Xavier", ""], ["Delaunay", "Pierre", ""], ["Bronzi", "Mirko", ""], ["Trofimov", "Assya", ""], ["Nichyporuk", "Brennan", ""], ["Szeto", "Justin", ""], ["Sepah", "Naz", ""], ["Raff", "Edward", ""], ["Madan", "Kanika", ""], ["Voleti", "Vikram", ""], ["Kahou", "Samira Ebrahimi", ""], ["Michalski", "Vincent", ""], ["Serdyuk", "Dmitriy", ""], ["Arbel", "Tal", ""], ["Pal", "Chris", ""], ["Varoquaux", "Ga\u00ebl", ""], ["Vincent", "Pascal", ""]]}, {"id": "2103.03169", "submitter": "Toni Karvonen", "authors": "Toni Karvonen", "title": "Small Sample Spaces for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the membership in a given reproducing kernel Hilbert space\n(RKHS) of the samples of a Gaussian process $X$ is controlled by a certain\nnuclear dominance condition. However, it is less clear how to identify a\n\"small\" set of functions (not necessarily a vector space) that contains the\nsamples. This article presents a general approach for identifying such sets. We\nuse scaled RKHSs, which can be viewed as a generalisation of Hilbert scales, to\ndefine the sample support set as the largest set which is contained in every\nelement of full measure under the law of $X$ in the $\\sigma$-algebra induced by\nthe collection of scaled RKHS. This potentially non-measurable set is then\nshown to consist of those functions that can be expanded in terms of an\northonormal basis of the RKHS of the covariance kernel of $X$ and have their\nsquared basis coefficients bounded away from zero and infinity, a result\nsuggested by the Karhunen-Lo\\`{e}ve theorem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:23:28 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:49:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Karvonen", "Toni", ""]]}, {"id": "2103.03191", "submitter": "Hayden Schaeffer", "authors": "Abolfazl Hashemi, Hayden Schaeffer, Robert Shi, Ufuk Topcu, Giang\n  Tran, Rachel Ward", "title": "Generalization Bounds for Sparse Random Feature Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random feature methods have been successful in various machine learning\ntasks, are easy to compute, and come with theoretical accuracy bounds. They\nserve as an alternative approach to standard neural networks since they can\nrepresent similar function spaces without a costly training phase. However, for\naccuracy, random feature methods require more measurements than trainable\nparameters, limiting their use for data-scarce applications or problems in\nscientific machine learning. This paper introduces the sparse random feature\nexpansion to obtain parsimonious random feature models. Specifically, we\nleverage ideas from compressive sensing to generate random feature expansions\nwith theoretical guarantees even in the data-scarce setting. In particular, we\nprovide uniform bounds on the approximation error and generalization bounds for\nfunctions in a certain class (that is dense in a reproducing kernel Hilbert\nspace) depending on the number of samples and the distribution of features. The\nerror bounds improve with additional structural conditions, such as coordinate\nsparsity, compact clusters of the spectrum, or rapid spectral decay. In\nparticular, by introducing sparse features, i.e. features with random sparse\nweights, we provide improved bounds for low order functions. We show that the\nsparse random feature expansions outperforms shallow networks in several\nscientific machine learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:53:54 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 20:44:43 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Schaeffer", "Hayden", ""], ["Shi", "Robert", ""], ["Topcu", "Ufuk", ""], ["Tran", "Giang", ""], ["Ward", "Rachel", ""]]}, {"id": "2103.03235", "submitter": "Guillaume Braun", "authors": "Guillaume Braun, Hemant Tyagi, Christophe Biernacki", "title": "Clustering multilayer graphs with missing nodes", "comments": "27 pages, 7 figures, accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationship between agents can be conveniently represented by graphs. When\nthese relationships have different modalities, they are better modelled by\nmultilayer graphs where each layer is associated with one modality. Such graphs\narise naturally in many contexts including biological and social networks.\nClustering is a fundamental problem in network analysis where the goal is to\nregroup nodes with similar connectivity profiles. In the past decade, various\nclustering methods have been extended from the unilayer setting to multilayer\ngraphs in order to incorporate the information provided by each layer. While\nmost existing works assume - rather restrictively - that all layers share the\nsame set of nodes, we propose a new framework that allows for layers to be\ndefined on different sets of nodes. In particular, the nodes not recorded in a\nlayer are treated as missing. Within this paradigm, we investigate several\ngeneralizations of well-known clustering methods in the complete setting to the\nincomplete one and prove some consistency results under the Multi-Layer\nStochastic Block Model assumption. Our theoretical results are complemented by\nthorough numerical comparisons between our proposed algorithms on synthetic\ndata, and also on real datasets, thus highlighting the promising behaviour of\nour methods in various settings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:56:59 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Braun", "Guillaume", ""], ["Tyagi", "Hemant", ""], ["Biernacki", "Christophe", ""]]}, {"id": "2103.03236", "submitter": "Gokul Swamy", "authors": "Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, Zhiwei Steven Wu", "title": "Of Moments and Matching: A Game-Theoretic Framework for Closing the\n  Imitation Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unifying view of a large family of previous imitation learning\nalgorithms through the lens of moment matching. At its core, our classification\nscheme is based on whether the learner attempts to match (1) reward or (2)\naction-value moments of the expert's behavior, with each option leading to\ndiffering algorithmic approaches. By considering adversarially chosen\ndivergences between learner and expert behavior, we are able to derive bounds\non policy performance that apply for all algorithms in each of these classes,\nthe first to our knowledge. We also introduce the notion of moment\nrecoverability, implicit in many previous analyses of imitation learning, which\nallows us to cleanly delineate how well each algorithmic family is able to\nmitigate compounding errors. We derive three novel algorithm templates (AdVIL,\nAdRIL, and DAeQuIL) with strong guarantees, simple implementation, and\ncompetitive empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:57:11 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:07:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Swamy", "Gokul", ""], ["Choudhury", "Sanjiban", ""], ["Bagnell", "J. Andrew", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2103.03265", "submitter": "Hoang Tran", "authors": "Hoang Tran, Ashok Cutkosky", "title": "Better SGD using Second-order Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithm for non-convex stochastic optimization that finds\nan $\\epsilon$-critical point in the optimal $O(\\epsilon^{-3})$ stochastic\ngradient and Hessian-vector product computations. Our algorithm uses\nHessian-vector products to \"correct\" a bias term in the momentum of SGD with\nmomentum. This leads to better gradient estimates in a manner analogous to\nvariance reduction methods. In contrast to prior work, we do not require\nexcessively large batch sizes, and are able to provide an adaptive algorithm\nwhose convergence rate automatically improves with decreasing variance in the\ngradient estimates. We validate our results on a variety of large-scale deep\nlearning architectures and benchmarks tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:01:20 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 00:15:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tran", "Hoang", ""], ["Cutkosky", "Ashok", ""]]}, {"id": "2103.03302", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Andrei V. Konstantinov", "title": "Ensembles of Random SHAPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble-based modifications of the well-known SHapley Additive exPlanations\n(SHAP) method for the local explanation of a black-box model are proposed. The\nmodifications aim to simplify SHAP which is computationally expensive when\nthere is a large number of features. The main idea behind the proposed\nmodifications is to approximate SHAP by an ensemble of SHAPs with a smaller\nnumber of features. According to the first modification, called ER-SHAP,\nseveral features are randomly selected many times from the feature set, and\nShapley values for the features are computed by means of \"small\" SHAPs. The\nexplanation results are averaged to get the final Shapley values. According to\nthe second modification, called ERW-SHAP, several points are generated around\nthe explained instance for diversity purposes, and results of their explanation\nare combined with weights depending on distances between points and the\nexplained instance. The third modification, called ER-SHAP-RF, uses the random\nforest for preliminary explanation of instances and determining a feature\nprobability distribution which is applied to selection of features in the\nensemble-based procedure of ER-SHAP. Many numerical experiments illustrating\nthe proposed modifications demonstrate their efficiency and properties for\nlocal explanation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:18:07 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Utkin", "Lev V.", ""], ["Konstantinov", "Andrei V.", ""]]}, {"id": "2103.03307", "submitter": "Achraf Azize", "authors": "Achraf Azize and Othman Gaizi", "title": "Conservative Optimistic Policy Optimization via Multiple Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement Learning (RL) has been able to solve hard problems such as\nplaying Atari games or solving the game of Go, with a unified approach. Yet\nmodern deep RL approaches are still not widely used in real-world applications.\nOne reason could be the lack of guarantees on the performance of the\nintermediate executed policies, compared to an existing (already working)\nbaseline policy. In this paper, we propose an online model-free algorithm that\nsolves conservative exploration in the policy optimization problem. We show\nthat the regret of the proposed approach is bounded by\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ for both discrete and continuous parameter\nspaces.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:23:38 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Azize", "Achraf", ""], ["Gaizi", "Othman", ""]]}, {"id": "2103.03321", "submitter": "Karla Monterrubio G\\'omez", "authors": "Karla Monterrubio-G\\'omez and Sara Wade", "title": "On MCMC for variationally sparse Gaussian processes: A pseudo-marginal\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are frequently used in machine learning and\nstatistics to construct powerful models. However, when employing GPs in\npractice, important considerations must be made, regarding the high\ncomputational burden, approximation of the posterior, choice of the covariance\nfunction and inference of its hyperparmeters. To address these issues, Hensman\net al. (2015) combine variationally sparse GPs with Markov chain Monte Carlo\n(MCMC) to derive a scalable, flexible and general framework for GP models.\nNevertheless, the resulting approach requires intractable likelihood\nevaluations for many observation models. To bypass this problem, we propose a\npseudo-marginal (PM) scheme that offers asymptotically exact inference as well\nas computational gains through doubly stochastic estimators for the intractable\nlikelihood and large datasets. In complex models, the advantages of the PM\nscheme are particularly evident, and we demonstrate this on a two-level GP\nregression model with a nonparametric covariance function to capture\nnon-stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:48:29 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Monterrubio-G\u00f3mez", "Karla", ""], ["Wade", "Sara", ""]]}, {"id": "2103.03323", "submitter": "Aleksandr Podkopaev", "authors": "Aleksandr Podkopaev, Aaditya Ramdas", "title": "Distribution-free uncertainty quantification for classification under\n  label shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trustworthy deployment of ML models requires a proper measure of uncertainty,\nespecially in safety-critical applications. We focus on uncertainty\nquantification (UQ) for classification problems via two avenues -- prediction\nsets using conformal prediction and calibration of probabilistic predictors by\npost-hoc binning -- since these possess distribution-free guarantees for i.i.d.\ndata. Two common ways of generalizing beyond the i.i.d. setting include\nhandling covariate and label shift. Within the context of distribution-free UQ,\nthe former has already received attention, but not the latter. It is known that\nlabel shift hurts prediction, and we first argue that it also hurts UQ, by\nshowing degradation in coverage and calibration. Piggybacking on recent\nprogress in addressing label shift (for better prediction), we examine the\nright way to achieve UQ by reweighting the aforementioned conformal and\ncalibration procedures whenever some unlabeled data from the target\ndistribution is available. We examine these techniques theoretically in a\ndistribution-free framework and demonstrate their excellent practical\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:51:03 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 16:14:11 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 02:45:15 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 16:59:24 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Podkopaev", "Aleksandr", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.03385", "submitter": "Mohamed Aziz Bhouri", "authors": "Mohamed Aziz Bhouri and Paris Perdikaris", "title": "Gaussian processes meet NeuralODEs: A Bayesian framework for learning\n  the dynamics of partially observed systems from scarce and noisy data", "comments": "27 pages, 16 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:2004.06843", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a machine learning framework (GP-NODE) for Bayesian\nsystems identification from partial, noisy and irregular observations of\nnonlinear dynamical systems. The proposed method takes advantage of recent\ndevelopments in differentiable programming to propagate gradient information\nthrough ordinary differential equation solvers and perform Bayesian inference\nwith respect to unknown model parameters using Hamiltonian Monte Carlo sampling\nand Gaussian Process priors over the observed system states. This allows us to\nexploit temporal correlations in the observed data, and efficiently infer\nposterior distributions over plausible models with quantified uncertainty.\nMoreover, the use of sparsity-promoting priors such as the Finnish Horseshoe\nfor free model parameters enables the discovery of interpretable and\nparsimonious representations for the underlying latent dynamics. A series of\nnumerical studies is presented to demonstrate the effectiveness of the proposed\nGP-NODE method including predator-prey systems, systems biology, and a\n50-dimensional human motion dynamical system. Taken together, our findings put\nforth a novel, flexible and robust workflow for data-driven model discovery\nunder uncertainty. All code and data accompanying this manuscript are available\nonline at \\url{https://github.com/PredictiveIntelligenceLab/GP-NODEs}.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 23:42:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bhouri", "Mohamed Aziz", ""], ["Perdikaris", "Paris", ""]]}, {"id": "2103.03391", "submitter": "Riley Hickman", "authors": "Riley J. Hickman, Florian H\\\"ase, Lo\\\"ic M. Roch, Al\\'an Aspuru-Guzik", "title": "Gemini: Dynamic Bias Correction for Autonomous Experimentation and\n  Molecular Simulation", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has emerged as a powerful strategy to accelerate\nscientific discovery by means of autonomous experimentation. However, expensive\nmeasurements are required to accurately estimate materials properties, and can\nquickly become a hindrance to exhaustive materials discovery campaigns. Here,\nwe introduce Gemini: a data-driven model capable of using inexpensive\nmeasurements as proxies for expensive measurements by correcting systematic\nbiases between property evaluation methods. We recommend using Gemini for\nregression tasks with sparse data and in an autonomous workflow setting where\nits predictions of expensive to evaluate objectives can be used to construct a\nmore informative acquisition function, thus reducing the number of expensive\nevaluations an optimizer needs to achieve desired target values. In a\nregression setting, we showcase the ability of our method to make accurate\npredictions of DFT calculated bandgaps of hybrid organic-inorganic perovskite\nmaterials. We further demonstrate the benefits that Gemini provides to\nautonomous workflows by augmenting the Bayesian optimizer Phoenics to yeild a\nscalable optimization framework leveraging multiple sources of measurement.\nFinally, we simulate an autonomous materials discovery platform for optimizing\nthe activity of electrocatalysts for the oxygen evolution reaction. Realizing\nautonomous workflows with Gemini, we show that the number of measurements of a\ncomposition space comprising expensive and rare metals needed to achieve a\ntarget overpotential is significantly reduced when measurements from a proxy\ncomposition system with less expensive metals are available.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:11:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hickman", "Riley J.", ""], ["H\u00e4se", "Florian", ""], ["Roch", "Lo\u00efc M.", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "2103.03399", "submitter": "Esther Rolf", "authors": "Esther Rolf, Theodora Worledge, Benjamin Recht, and Michael I. Jordan", "title": "Representation Matters: Assessing the Importance of Subgroup Allocations\n  in Training Data", "comments": "Accepted to ICML 2021; 31 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting more diverse and representative training data is often touted as a\nremedy for the disparate performance of machine learning predictors across\nsubpopulations. However, a precise framework for understanding how dataset\nproperties like diversity affect learning outcomes is largely lacking. By\ncasting data collection as part of the learning process, we demonstrate that\ndiverse representation in training data is key not only to increasing subgroup\nperformances, but also to achieving population level objectives. Our analysis\nand experiments describe how dataset compositions influence performance and\nprovide constructive results for using trends in existing data, alongside\ndomain knowledge, to help guide intentional, objective-aware dataset design.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:27:08 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 20:57:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rolf", "Esther", ""], ["Worledge", "Theodora", ""], ["Recht", "Benjamin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2103.03452", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, and Lam M. Nguyen", "title": "FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex\n  Federated Composite Optimization", "comments": "38 pages, and 12 figures", "journal-ref": null, "doi": null, "report-no": "UNC-STOR-June 2021", "categories": "stat.ML cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop two new algorithms, called, FedDR and asyncFedDR, for solving a\nfundamental nonconvex composite optimization problem in federated learning. Our\nalgorithms rely on a novel combination between a nonconvex Douglas-Rachford\nsplitting method, randomized block-coordinate strategies, and asynchronous\nimplementation. They can also handle convex regularizers. Unlike recent methods\nin the literature, e.g., FedSplit and FedPD, our algorithms update only a\nsubset of users at each communication round, and possibly in an asynchronous\nmanner, making them more practical. These new algorithms also achieve\ncommunication efficiency and more importantly can handle statistical and system\nheterogeneity, which are the two main challenges in federated learning. Our\nconvergence analysis shows that the new algorithms match the communication\ncomplexity lower bound up to a constant factor under standard assumptions. Our\nnumerical experiments illustrate the advantages of our methods compared to\nexisting ones on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:24:04 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 20:44:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Pham", "Nhan H.", ""], ["Phan", "Dzung T.", ""], ["Nguyen", "Lam M.", ""]]}, {"id": "2103.03460", "submitter": "Hui Tang", "authors": "Hui Tang and Kui Jia", "title": "Vicinal and categorical domain adaptation", "comments": "Accepted by Pattern Recognition", "journal-ref": "Pattern Recognition, Volume 115, July 2021, 107907", "doi": "10.1016/j.patcog.2021.107907", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to learn a task classifier that performs\nwell on the unlabeled target domain, by utilizing the labeled source domain.\nInspiring results have been acquired by learning domain-invariant deep features\nvia domain-adversarial training. However, its parallel design of task and\ndomain classifiers limits the ability to achieve a finer category-level domain\nalignment. To promote categorical domain adaptation (CatDA), based on a joint\ncategory-domain classifier, we propose novel losses of adversarial training at\nboth domain and category levels. Since the joint classifier can be regarded as\na concatenation of individual task classifiers respectively for the two\ndomains, our design principle is to enforce consistency of category predictions\nbetween the two task classifiers. Moreover, we propose a concept of vicinal\ndomains whose instances are produced by a convex combination of pairs of\ninstances respectively from the two domains. Intuitively, alignment of the\npossibly infinite number of vicinal domains enhances that of original domains.\nWe propose novel adversarial losses for vicinal domain adaptation (VicDA) based\non CatDA, leading to Vicinal and Categorical Domain Adaptation (ViCatDA). We\nalso propose Target Discriminative Structure Recovery (TDSR) to recover the\nintrinsic target discrimination damaged by adversarial feature alignment. We\nalso analyze the principles underlying the ability of our key designs to align\nthe joint distributions. Extensive experiments on several benchmark datasets\ndemonstrate that we achieve the new state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:47:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tang", "Hui", ""], ["Jia", "Kui", ""]]}, {"id": "2103.03466", "submitter": "Ryuichi Kanoh", "authors": "Ryuichi Kanoh, Mahito Sugiyama", "title": "Unintended Effects on Adaptive Learning Rate for Training Neural Network\n  with Output Scale Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiplicative constant scaling factor is often applied to the model output\nto adjust the dynamics of neural network parameters. This has been used as one\nof the key interventions in an empirical study of lazy and active behavior.\nHowever, we show that the combination of such scaling and a commonly used\nadaptive learning rate optimizer strongly affects the training behavior of the\nneural network. This is problematic as it can cause \\emph{unintended behavior}\nof neural networks, resulting in the misinterpretation of experimental results.\nSpecifically, for some scaling settings, the effect of the adaptive learning\nrate disappears or is strongly influenced by the scaling factor. To avoid the\nunintended effect, we present a modification of an optimization algorithm and\ndemonstrate remarkable differences between adaptive learning rate optimization\nand simple gradient descent, especially with a small ($<1.0$) scaling factor.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:19:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 08:33:56 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kanoh", "Ryuichi", ""], ["Sugiyama", "Mahito", ""]]}, {"id": "2103.03471", "submitter": "Yanli Yuan", "authors": "Yanli Yuan, De Wen Soh, Xiao Yang, Kun Guo, Tony Q. S. Quek", "title": "Joint Network Topology Inference via Structured Fusion Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint network topology inference represents a canonical problem of jointly\nlearning multiple graph Laplacian matrices from heterogeneous graph signals. In\nsuch a problem, a widely employed assumption is that of a simple common\ncomponent shared among multiple networks. However, in practice, a more\nintricate topological pattern, comprising simultaneously of sparse, homogeneity\nand heterogeneity components, would exhibit in multiple networks. In this\npaper, we propose a general graph estimator based on a novel structured fusion\nregularization that enables us to jointly learn multiple graph Laplacian\nmatrices with such complex topological patterns, and enjoys both high\ncomputational efficiency and rigorous theoretical guarantee. Moreover, in the\nproposed regularization term, the topological pattern among networks is\ncharacterized by a Gram matrix, endowing our graph estimator with the ability\nof flexible modelling different types of topological patterns by different\nchoices of the Gram matrix. Computationally, the regularization term, coupling\nthe parameters together, makes the formulated optimization problem intractable\nand thus, we develop a computationally-scalable algorithm based on the\nalternating direction method of multipliers (ADMM) to solve it efficiently.\nTheoretically, we provide a theoretical analysis of the proposed graph\nestimator, which establishes a non-asymptotic bound of the estimation error\nunder the high-dimensional setting and reflects the effect of several key\nfactors on the convergence rate of our algorithm. Finally, the superior\nperformance of the proposed method is illustrated through simulated and real\ndata examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:42:32 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 06:21:00 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 05:38:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yuan", "Yanli", ""], ["Soh", "De Wen", ""], ["Yang", "Xiao", ""], ["Guo", "Kun", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2103.03532", "submitter": "Sahra Ghalebikesabi", "authors": "Sahra Ghalebikesabi, Rob Cornish, Luke J. Kelly and Chris Holmes", "title": "Deep Generative Pattern-Set Mixture Models for Nonignorable Missingness", "comments": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS)", "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variational autoencoder architecture to model both ignorable and\nnonignorable missing data using pattern-set mixtures as proposed by Little\n(1993). Our model explicitly learns to cluster the missing data into\nmissingness pattern sets based on the observed data and missingness masks.\nUnderpinning our approach is the assumption that the data distribution under\nmissingness is probabilistically semi-supervised by samples from the observed\ndata distribution. Our setup trades off the characteristics of ignorable and\nnonignorable missingness and can thus be applied to data of both types. We\nevaluate our method on a wide range of data sets with different types of\nmissingness and achieve state-of-the-art imputation performance. Our model\noutperforms many common imputation algorithms, especially when the amount of\nmissing data is high and the missingness mechanism is nonignorable.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:21:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ghalebikesabi", "Sahra", ""], ["Cornish", "Rob", ""], ["Kelly", "Luke J.", ""], ["Holmes", "Chris", ""]]}, {"id": "2103.03561", "submitter": "Lorenzo Dall'Amico", "authors": "Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay", "title": "Nishimori meets Bethe: a spectral method for node classification in\n  sparse weighted graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article unveils a new relation between the Nishimori temperature\nparametrizing a distribution P and the Bethe free energy on random Erdos-Renyi\ngraphs with edge weights distributed according to P. Estimating the Nishimori\ntemperature being a task of major importance in Bayesian inference problems, as\na practical corollary of this new relation, a numerical method is proposed to\naccurately estimate the Nishimori temperature from the eigenvalues of the Bethe\nHessian matrix of the weighted graph. The algorithm, in turn, is used to\npropose a new spectral method for node classification in weighted (possibly\nsparse) graphs. The superiority of the method over competing state-of-the-art\napproaches is demonstrated both through theoretical arguments and real-world\ndata experiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:45:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Dall'Amico", "Lorenzo", ""], ["Couillet", "Romain", ""], ["Tremblay", "Nicolas", ""]]}, {"id": "2103.03565", "submitter": "Didier Lucor", "authors": "Didier Lucor (LISN), Atul Agrawal (TUM, LISN), Anne Sergent (LISN, UFR\n  919)", "title": "Physics-aware deep neural networks for surrogate modeling of turbulent\n  natural convection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have explored the potential of machine learning as data-driven\nturbulence closures for RANS and LES techniques. Beyond these advances, the\nhigh expressivity and agility of physics-informed neural networks (PINNs) make\nthem promising candidates for full fluid flow PDE modeling. An important\nquestion is whether this new paradigm, exempt from the traditional notion of\ndiscretization of the underlying operators very much connected to the flow\nscales resolution, is capable of sustaining high levels of turbulence\ncharacterized by multi-scale features? We investigate the use of PINNs\nsurrogate modeling for turbulent Rayleigh-B{\\'e}nard (RB) convection flows in\nrough and smooth rectangular cavities, mainly relying on DNS temperature data\nfrom the fluid bulk. We carefully quantify the computational requirements under\nwhich the formulation is capable of accurately recovering the flow hidden\nquantities. We then propose a new padding technique to distribute some of the\nscattered coordinates-at which PDE residuals are minimized-around the region of\nlabeled data acquisition. We show how it comes to play as a regularization\nclose to the training boundaries which are zones of poor accuracy for standard\nPINNs and results in a noticeable global accuracy improvement at iso-budget.\nFinally, we propose for the first time to relax the incompressibility condition\nin such a way that it drastically benefits the optimization search and results\nin a much improved convergence of the composite loss function. The RB results\nobtained at high Rayleigh number Ra = 2 $\\bullet$ 10 9 are particularly\nimpressive: the predictive accuracy of the surrogate over the entire half a\nbillion DNS coordinates yields errors for all flow variables ranging between\n[0.3% -- 4%] in the relative L 2 norm, with a training relying only on 1.6% of\nthe DNS data points.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:48:57 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Lucor", "Didier", "", "LISN"], ["Agrawal", "Atul", "", "TUM, LISN"], ["Sergent", "Anne", "", "LISN, UFR\n  919"]]}, {"id": "2103.03568", "submitter": "Jiaye Teng", "authors": "Jiaye Teng, Weiran Huang, Haowei He", "title": "Can Pretext-Based Self-Supervised Learning Be Boosted by Downstream\n  Data? A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretext-based self-supervised learning aims to learn the semantic\nrepresentation via a handcrafted pretext task over unlabeled data and then use\nthe learned representation for downstream prediction tasks. It is proved that\npretext-based self-supervised learning can effectively reduce the sample\ncomplexity of downstream tasks under Conditional Independence (CI) between the\ncomponents of the pretext task conditional on the downstream label. However,\nthe downstream sample complexity will get much worse if the CI condition does\nnot hold. One interesting question is whether we can make the CI condition hold\nby using downstream data to refine the unlabeled data to boost self-supervised\nlearning. At first glance, one might think that seeing downstream data in\nadvance would always boost the downstream performance. However, we show that it\nis not intuitively true and point out that in some cases, it will hurt the\nfinal performance instead. In particular, we prove both model-free and\nmodel-dependent lower bounds of the number of downstream samples used for data\nrefinement. Moreover, we conduct several experiments on both synthetic and\nreal-world datasets to verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:53:10 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:36:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Teng", "Jiaye", ""], ["Huang", "Weiran", ""], ["He", "Haowei", ""]]}, {"id": "2103.03606", "submitter": "Kilian Fatras", "authors": "Kilian Fatras, Thibault S\\'ejourn\\'e, Nicolas Courty, R\\'emi Flamary", "title": "Unbalanced minibatch Optimal Transport; applications to Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport distances have found many applications in machine learning\nfor their capacity to compare non-parametric probability distributions. Yet\ntheir algorithmic complexity generally prevents their direct use on large scale\ndatasets. Among the possible strategies to alleviate this issue, practitioners\ncan rely on computing estimates of these distances over subsets of data, {\\em\ni.e.} minibatches. While computationally appealing, we highlight in this paper\nsome limits of this strategy, arguing it can lead to undesirable smoothing\neffects. As an alternative, we suggest that the same minibatch strategy coupled\nwith unbalanced optimal transport can yield more robust behavior. We discuss\nthe associated theoretical properties, such as unbiased estimators, existence\nof gradients and concentration bounds. Our experimental study shows that in\nchallenging problems associated to domain adaptation, the use of unbalanced\noptimal transport leads to significantly better results, competing with or\nsurpassing recent baselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:15:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Fatras", "Kilian", ""], ["S\u00e9journ\u00e9", "Thibault", ""], ["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""]]}, {"id": "2103.03635", "submitter": "Arthur Charpentier", "authors": "Michel Denuit and Arthur Charpentier and Julien Trufin", "title": "Autocalibration and Tweedie-dominance for Insurance Pricing with Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Boosting techniques and neural networks are particularly effective machine\nlearning methods for insurance pricing. Often in practice, there are\nnevertheless endless debates about the choice of the right loss function to be\nused to train the machine learning model, as well as about the appropriate\nmetric to assess the performances of competing models. Also, the sum of fitted\nvalues can depart from the observed totals to a large extent and this often\nconfuses actuarial analysts. The lack of balance inherent to training models by\nminimizing deviance outside the familiar GLM with canonical link setting has\nbeen empirically documented in W\\\"uthrich (2019, 2020) who attributes it to the\nearly stopping rule in gradient descent methods for model fitting. The present\npaper aims to further study this phenomenon when learning proceeds by\nminimizing Tweedie deviance. It is shown that minimizing deviance involves a\ntrade-off between the integral of weighted differences of lower partial moments\nand the bias measured on a specific scale. Autocalibration is then proposed as\na remedy. This new method to correct for bias adds an extra local GLM step to\nthe analysis. Theoretically, it is shown that it implements the autocalibration\nconcept in pure premium calculation and ensures that balance also holds on a\nlocal scale, not only at portfolio level as with existing bias-correction\ntechniques. The convex order appears to be the natural tool to compare\ncompeting models, putting a new light on the diagnostic graphs and associated\nmetrics proposed by Denuit et al. (2019).\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:40:30 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 13:48:50 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Denuit", "Michel", ""], ["Charpentier", "Arthur", ""], ["Trufin", "Julien", ""]]}, {"id": "2103.03655", "submitter": "Georgios Tsialiamanis", "authors": "George Tsialiamanis, Charilaos Mylonas, Eleni Chatzi, Nikolaos\n  Dervilis, David J. Wagg, Keith Worden", "title": "Foundations of Population-Based SHM, Part IV: The Geometry of Spaces of\n  Structures and their Feature Spaces", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing 157 (2021): 107692", "doi": "10.1016/j.ymssp.2021.107692", "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the requirements of the population-based approach to Structural Health\nMonitoring (SHM) proposed in the earlier papers in this sequence, is that\nstructures be represented by points in an abstract space. Furthermore, these\nspaces should be metric spaces in a loose sense; i.e. there should be some\nmeasure of distance applicable to pairs of points; similar structures should\nthen be close in the metric. However, this geometrical construction is not\nenough for the framing of problems in data-based SHM, as it leaves undefined\nthe notion of feature spaces. Interpreting the feature values on a\nstructure-by-structure basis as a type of field over the space of structures,\nit seems sensible to borrow an idea from modern theoretical physics, and define\nfeature assignments as sections in a vector bundle over the structure space.\nWith this idea in place, one can interpret the effect of environmental and\noperational variations as gauge degrees of freedom, as in modern gauge field\ntheories. This paper will discuss the various geometrical structures required\nfor an abstract theory of feature spaces in SHM, and will draw analogies with\nhow these structures have shown their power in modern physics. In the second\npart of the paper, the problem of determining the normal condition cross\nsection of a feature bundle is addressed. The solution is provided by the\napplication of Graph Neural Networks (GNN), a versatile non-Euclidean machine\nlearning algorithm which is not restricted to inputs and outputs from vector\nspaces. In particular, the algorithm is well suited to operating directly on\nthe sort of graph structures which are an important part of the proposed\nframework for PBSHM. The solution of the normal section problem is demonstrated\nfor a heterogeneous population of truss structures for which the feature of\ninterest is the first natural frequency.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:28:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tsialiamanis", "George", ""], ["Mylonas", "Charilaos", ""], ["Chatzi", "Eleni", ""], ["Dervilis", "Nikolaos", ""], ["Wagg", "David J.", ""], ["Worden", "Keith", ""]]}, {"id": "2103.03656", "submitter": "Yoshihiro Okawa", "authors": "Yoshihiro Okawa, Tomotake Sasaki and Hidenao Iwane", "title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning\n  with Joint Chance Constraint Satisfaction", "comments": "Accepted to the 21st IFAC World Congress (IFAC-V 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In reinforcement learning (RL) algorithms, exploratory control inputs are\nused during learning to acquire knowledge for decision making and control,\nwhile the true dynamics of a controlled object is unknown. However, this\nexploring property sometimes causes undesired situations by violating\nconstraints regarding the state of the controlled object. In this paper, we\npropose an automatic exploration process adjustment method for safe RL in\ncontinuous state and action spaces utilizing a linear nominal model of the\ncontrolled object. Specifically, our proposed method automatically selects\nwhether the exploratory input is used or not at each time depending on the\nstate and its predicted value as well as adjusts the variance-covariance matrix\nused in the Gaussian policy for exploration. We also show that our exploration\nprocess adjustment method theoretically guarantees the satisfaction of the\nconstraints with the pre-specified probability, that is, the satisfaction of a\njoint chance constraint at every time. Finally, we illustrate the validity and\nthe effectiveness of our method through numerical simulation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:30:53 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Okawa", "Yoshihiro", ""], ["Sasaki", "Tomotake", ""], ["Iwane", "Hidenao", ""]]}, {"id": "2103.03788", "submitter": "Jayaraman J. Thiagarajan", "authors": "Vivek Narayanaswamy, Jayaraman J. Thiagarajan, Deepta Rajan, Andreas\n  Spanias", "title": "Loss Estimators Improve Model Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increased interest in adopting AI methods for clinical diagnosis, a\nvital step towards safe deployment of such tools is to ensure that the models\nnot only produce accurate predictions but also do not generalize to data\nregimes where the training data provide no meaningful evidence. Existing\napproaches for ensuring the distribution of model predictions to be similar to\nthat of the true distribution rely on explicit uncertainty estimators that are\ninherently hard to calibrate. In this paper, we propose to train a loss\nestimator alongside the predictive model, using a contrastive training\nobjective, to directly estimate the prediction uncertainties. Interestingly, we\nfind that, in addition to producing well-calibrated uncertainties, this\napproach improves the generalization behavior of the predictor. Using a\ndermatology use-case, we show the impact of loss estimators on model\ngeneralization, in terms of both its fidelity on in-distribution data and its\nability to detect out of distribution samples or new classes unseen during\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:35:10 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Narayanaswamy", "Vivek", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Rajan", "Deepta", ""], ["Spanias", "Andreas", ""]]}, {"id": "2103.03841", "submitter": "Charlie Nash", "authors": "Charlie Nash, Jacob Menick, Sander Dieleman, Peter W. Battaglia", "title": "Generating Images with Sparse Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality of images presents architecture and\nsampling-efficiency challenges for likelihood-based generative models. Previous\napproaches such as VQ-VAE use deep autoencoders to obtain compact\nrepresentations, which are more practical as inputs for likelihood-based\nmodels. We present an alternative approach, inspired by common image\ncompression methods like JPEG, and convert images to quantized discrete cosine\ntransform (DCT) blocks, which are represented sparsely as a sequence of DCT\nchannel, spatial location, and DCT coefficient triples. We propose a\nTransformer-based autoregressive architecture, which is trained to sequentially\npredict the conditional distribution of the next element in such sequences, and\nwhich scales effectively to high resolution images. On a range of image\ndatasets, we demonstrate that our approach can generate high quality, diverse\nimages, with sample metric scores competitive with state of the art methods. We\nadditionally show that simple modifications to our method yield effective image\ncolorization and super-resolution models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:56:03 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nash", "Charlie", ""], ["Menick", "Jacob", ""], ["Dieleman", "Sander", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "2103.03872", "submitter": "Ethan Perez", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "title": "Rissanen Data Analysis: Examining Dataset Characteristics via\n  Description Length", "comments": "Code at https://github.com/ethanjperez/rda along with a script to run\n  RDA on your own dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to determine if a certain capability helps to achieve\nan accurate model of given data. We view labels as being generated from the\ninputs by a program composed of subroutines with different capabilities, and we\nposit that a subroutine is useful if and only if the minimal program that\ninvokes it is shorter than the one that does not. Since minimum program length\nis uncomputable, we instead estimate the labels' minimum description length\n(MDL) as a proxy, giving us a theoretically-grounded method for analyzing\ndataset characteristics. We call the method Rissanen Data Analysis (RDA) after\nthe father of MDL, and we showcase its applicability on a wide variety of\nsettings in NLP, ranging from evaluating the utility of generating subquestions\nbefore answering a question, to analyzing the value of rationales and\nexplanations, to investigating the importance of different parts of speech, and\nuncovering dataset gender bias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:58:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Perez", "Ethan", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2103.03905", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Yan Wu, Alexandros Kalousis", "title": "Kanerva++: extending The Kanerva Machine with differentiable, locally\n  block allocated latent memory", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic and semantic memory are critical components of the human memory\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\nsuggests that the compressed representation produced by a serial event\n(episodic memory) is later restructured to build a more generalized form of\nreusable knowledge (semantic memory). In this work we develop a new principled\nBayesian memory allocation scheme that bridges the gap between episodic and\nsemantic memory via a hierarchical latent variable model. We take inspiration\nfrom traditional heap allocation and extend the idea of locally contiguous\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\nmemory writing by treating it as a fully feed forward deterministic process,\nrelying on the stochasticity of the read key distribution to disperse\ninformation within the memory. We demonstrate that this allocation scheme\nimproves performance in memory conditional image generation, resulting in new\nstate-of-the-art conditional likelihood values on binarized MNIST (<=41.58\nnats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:40:40 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:38:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ramapuram", "Jason", ""], ["Wu", "Yan", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "2103.03945", "submitter": "Zhen Lin", "authors": "Zhen Lin, Cao Xiao, Lucas Glass, M. Brandon Westover, Jimeng Sun", "title": "SCRIB: Set-classifier with Class-specific Risk Bounds for Blackbox\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite deep learning (DL) success in classification problems, DL classifiers\ndo not provide a sound mechanism to decide when to refrain from predicting.\nRecent works tried to control the overall prediction risk with classification\nwith rejection options. However, existing works overlook the different\nsignificance of different classes. We introduce Set-classifier with\nClass-specific RIsk Bounds (SCRIB) to tackle this problem, assigning multiple\nlabels to each example. Given the output of a black-box model on the validation\nset, SCRIB constructs a set-classifier that controls the class-specific\nprediction risks with a theoretical guarantee. The key idea is to reject when\nthe set classifier returns more than one label. We validated SCRIB on several\nmedical applications, including sleep staging on electroencephalogram (EEG)\ndata, X-ray COVID image classification, and atrial fibrillation detection based\non electrocardiogram (ECG) data. SCRIB obtained desirable class-specific risks,\nwhich are 35\\%-88\\% closer to the target risks than baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 21:06:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lin", "Zhen", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas", ""], ["Westover", "M. Brandon", ""], ["Sun", "Jimeng", ""]]}, {"id": "2103.04014", "submitter": "Chuan-Zheng Lee", "authors": "Chuan-Zheng Lee, Leighton Pate Barnes and Ayfer Ozgur", "title": "Over-the-Air Statistical Estimation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study schemes and lower bounds for distributed minimax statistical\nestimation over a Gaussian multiple-access channel (MAC) under squared error\nloss, in a framework combining statistical estimation and wireless\ncommunication. First, we develop \"analog\" joint estimation-communication\nschemes that exploit the superposition property of the Gaussian MAC and we\ncharacterize their risk in terms of the number of nodes and dimension of the\nparameter space. Then, we derive information-theoretic lower bounds on the\nminimax risk of any estimation scheme restricted to communicate the samples\nover a given number of uses of the channel and show that the risk achieved by\nour proposed schemes is within a logarithmic factor of these lower bounds. We\ncompare both achievability and lower bound results to previous \"digital\" lower\nbounds, where nodes transmit errorless bits at the Shannon capacity of the MAC,\nshowing that estimation schemes that leverage the physical layer offer a\ndrastic reduction in estimation error over digital schemes relying on a\nphysical-layer abstraction.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:07:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lee", "Chuan-Zheng", ""], ["Barnes", "Leighton Pate", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2103.04021", "submitter": "Xiaowei Zhang", "authors": "Jin Li and Ye Luo and Xiaowei Zhang", "title": "Causal Reinforcement Learning: An Instrumental Variable Approach", "comments": "main body: 38 pages; supplemental material: 58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the standard data analysis framework, data is first collected (once for\nall), and then data analysis is carried out. With the advancement of digital\ntechnology, decisionmakers constantly analyze past data and generate new data\nthrough the decisions they make. In this paper, we model this as a Markov\ndecision process and show that the dynamic interaction between data generation\nand data analysis leads to a new type of bias -- reinforcement bias -- that\nexacerbates the endogeneity problem in standard data analysis.\n  We propose a class of instrument variable (IV)-based reinforcement learning\n(RL) algorithms to correct for the bias and establish their asymptotic\nproperties by incorporating them into a two-timescale stochastic approximation\nframework. A key contribution of the paper is the development of new techniques\nthat allow for the analysis of the algorithms in general settings where noises\nfeature time-dependency.\n  We use the techniques to derive sharper results on finite-time trajectory\nstability bounds: with a polynomial rate, the entire future trajectory of the\niterates from the algorithm fall within a ball that is centered at the true\nparameter and is shrinking at a (different) polynomial rate. We also use the\ntechnique to provide formulas for inferences that are rarely done for RL\nalgorithms. These formulas highlight how the strength of the IV and the degree\nof the noise's time dependency affect the inference.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:57:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Jin", ""], ["Luo", "Ye", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "2103.04031", "submitter": "Yifan Chen", "authors": "Yifan Chen, Yun Yang", "title": "Accumulations of Projections--A Unified Framework for Random Sketches in\n  Kernel Ridge Regression", "comments": "To appear in the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a sketch of an n-by-n empirical kernel matrix is a common approach\nto accelerate the computation of many kernel methods. In this paper, we propose\na unified framework of constructing sketching methods in kernel ridge\nregression (KRR), which views the sketching matrix S as an accumulation of m\nrescaled sub-sampling matrices with independent columns. Our framework\nincorporates two commonly used sketching methods, sub-sampling sketches (known\nas the Nystr\\\"om method) and sub-Gaussian sketches, as special cases with m=1\nand m=infinity respectively. Under the new framework, we provide a unified\nerror analysis of sketching approximation and show that our accumulation scheme\nimproves the low accuracy of sub-sampling sketches when certain incoherence\ncharacteristic is high, and accelerates the more accurate but computationally\nheavier sub-Gaussian sketches. By optimally choosing the number m of\naccumulations, we show that a best trade-off between computational efficiency\nand statistical accuracy can be achieved. In practice, the sketching method can\nbe as efficiently implemented as the sub-sampling sketches, as only minor extra\nmatrix additions are needed. Our empirical evaluations also demonstrate that\nthe proposed method may attain the accuracy close to sub-Gaussian sketches,\nwhile is as efficient as sub-sampling-based sketches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:02:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Yifan", ""], ["Yang", "Yun", ""]]}, {"id": "2103.04032", "submitter": "Vinay Verma Kumar", "authors": "Sakshi Varshney, Vinay Kumar Verma, Lawrence Carin, Piyush Rai", "title": "Efficient Continual Adaptation for Generative Adversarial Networks", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task specific parameters act as local adapters for each task, and help in\nefficiently transforming the previous task's feature map to the new task's\nfeature map. Moreover, we propose an element-wise residual bias in the\ntransformed feature space which highly stabilizes GAN training. In contrast to\nthe recent approaches for continual GANs, we do not rely on memory replay,\nregularization towards previous tasks' parameters, or expensive weight\ntransformations. Through extensive experiments on challenging and diverse\ndatasets, we show that the feature-map transformation based approach\noutperforms state-of-the-art continual GANs methods, with substantially fewer\nparameters, and also generates high-quality samples that can be used in\ngenerative replay based continual learning of discriminative tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:09:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Varshney", "Sakshi", ""], ["Verma", "Vinay Kumar", ""], ["Carin", "Lawrence", ""], ["Rai", "Piyush", ""]]}, {"id": "2103.04046", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Xuanting Cai", "title": "Simplicial Complex Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes form an important class of topological spaces that are\nfrequently used to in many applications areas such as computer-aided design,\ncomputer graphics, and simulation. The representation learning on graphs, which\nare just 1-d simplicial complexes, has witnessed a great attention and success\nin the past few years. Due to the additional complexity higher dimensional\nsimplicial hold, there has not been enough effort to extend representation\nlearning to these objects especially when it comes to learn entire-simplicial\ncomplex representation. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method utilizes a simplex-level embedding induced by a\npre-trained simplicial autoencoder to learn an entire simplicial complex\nrepresentation. To the best of our knowledge, this work presents the first\nmethod for learning simplicial complex-level representation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:33:04 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 07:18:25 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 22:01:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Cai", "Xuanting", ""]]}, {"id": "2103.04060", "submitter": "Eysan Mehrbani", "authors": "Eysan Mehrbani, Mohammad Hossein Kahaei", "title": "Low-Rank Isomap Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Isomap is a well-known nonlinear dimensionality reduction method that\nhighly suffers from computational complexity. Its computational complexity\nmainly arises from two stages; a) embedding a full graph on the data in the\nambient space, and b) a complete eigenvalue decomposition. Although the\nreduction of the computational complexity of the graphing stage has been\ninvestigated, yet the eigenvalue decomposition stage remains a bottleneck in\nthe problem. In this paper, we propose the Low-Rank Isomap algorithm by\nintroducing a projection operator on the embedded graph from the ambient space\nto a low-rank latent space to facilitate applying the partial eigenvalue\ndecomposition. This approach leads to reducing the complexity of Isomap to a\nlinear order while preserving the structural information during the\ndimensionality reduction process. The superiority of the Low-Rank Isomap\nalgorithm compared to some state-of-art algorithms is experimentally verified\non facial image clustering in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:08:16 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mehrbani", "Eysan", ""], ["Kahaei", "Mohammad Hossein", ""]]}, {"id": "2103.04064", "submitter": "Eysan Mehrbani", "authors": "Eysan Mehrbani, Mohammad Hossein Kahaei, Seyed Aliasghar Beheshti", "title": "Tensor Laplacian Regularized Low-Rank Representation for Non-uniformly\n  Distributed Data Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low-Rank Representation (LRR) highly suffers from discarding the locality\ninformation of data points in subspace clustering, as it may not incorporate\nthe data structure nonlinearity and the non-uniform distribution of\nobservations over the ambient space. Thus, the information of the observational\ndensity is lost by the state-of-art LRR models, as they take a constant number\nof adjacent neighbors into account. This, as a result, degrades the subspace\nclustering accuracy in such situations. To cope with deficiency, in this paper,\nwe propose to consider a hypergraph model to facilitate having a variable\nnumber of adjacent nodes and incorporating the locality information of the\ndata. The sparsity of the number of subspaces is also taken into account. To do\nso, an optimization problem is defined based on a set of regularization terms\nand is solved by developing a tensor Laplacian-based algorithm. Extensive\nexperiments on artificial and real datasets demonstrate the higher accuracy and\nprecision of the proposed method in subspace clustering compared to the\nstate-of-the-art methods. The outperformance of this method is more revealed in\npresence of inherent structure of the data such as nonlinearity, geometrical\noverlapping, and outliers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:22:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mehrbani", "Eysan", ""], ["Kahaei", "Mohammad Hossein", ""], ["Beheshti", "Seyed Aliasghar", ""]]}, {"id": "2103.04140", "submitter": "Konstantinos Gatsis", "authors": "Konstantinos Gatsis", "title": "Linear Regression over Networks with Communication Guarantees", "comments": "Accepted at 3rd Annual Learning for Dynamics & Control Conference\n  (L4DC) 2021. arXiv admin note: substantial text overlap with arXiv:2101.10007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key functionality of emerging connected autonomous systems such as smart\ncities, smart transportation systems, and the industrial Internet-of-Things, is\nthe ability to process and learn from data collected at different physical\nlocations. This is increasingly attracting attention under the terms of\ndistributed learning and federated learning. However, in connected autonomous\nsystems, data transfer takes place over communication networks with often\nlimited resources. This paper examines algorithms for communication-efficient\nlearning for linear regression tasks by exploiting the informativeness of the\ndata. The developed algorithms enable a tradeoff between communication and\nlearning with theoretical performance guarantees and efficient practical\nimplementations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:28:21 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gatsis", "Konstantinos", ""]]}, {"id": "2103.04150", "submitter": "David Shuman", "authors": "Yilin Chen, Jennifer DeJong, Tom Halverson, David I Shuman", "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 16:32:32 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 22:57:38 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chen", "Yilin", ""], ["DeJong", "Jennifer", ""], ["Halverson", "Tom", ""], ["Shuman", "David I", ""]]}, {"id": "2103.04215", "submitter": "Ruiyang Song", "authors": "Ruiyang Song, Stefano Rini, Kuang Xu", "title": "Hierarchical Causal Bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal bandit is a nascent learning model where an agent sequentially\nexperiments in a causal network of variables, in order to identify the\nreward-maximizing intervention. Despite the model's wide applicability,\nexisting analytical results are largely restricted to a parallel bandit version\nwhere all variables are mutually independent. We introduce in this work the\nhierarchical causal bandit model as a viable path towards understanding general\ncausal bandits with dependent variables. The core idea is to incorporate a\ncontextual variable that captures the interaction among all variables with\ndirect effects. Using this hierarchical framework, we derive sharp insights on\nalgorithmic design in causal bandits with dependent arms and obtain nearly\nmatching regret bounds in the case of a binary context.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:04:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Song", "Ruiyang", ""], ["Rini", "Stefano", ""], ["Xu", "Kuang", ""]]}, {"id": "2103.04217", "submitter": "Anton Obukhov", "authors": "Anton Obukhov, Maxim Rakhuba, Alexander Liniger, Zhiwu Huang,\n  Stamatios Georgoulis, Dengxin Dai, Luc Van Gool", "title": "Spectral Tensor Train Parameterization of Deep Learning Layers", "comments": "Accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:15:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:43:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Obukhov", "Anton", ""], ["Rakhuba", "Maxim", ""], ["Liniger", "Alexander", ""], ["Huang", "Zhiwu", ""], ["Georgoulis", "Stamatios", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2103.04250", "submitter": "Kyra Gan", "authors": "Kyra Gan, Su Jia, Andrew Li", "title": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of active sequential hypotheses testing (ASHT), a learner\nseeks to identify the true hypothesis from among a known set of hypotheses. The\nlearner is given a set of actions and knows the random distribution of the\noutcome of any action under any true hypothesis. Given a target error\n$\\delta>0$, the goal is to sequentially select the fewest number of actions so\nas to identify the true hypothesis with probability at least $1 - \\delta$.\nMotivated by applications in which the number of hypotheses or actions is\nmassive (e.g. genomics-based cancer detection), we propose efficient (greedy,\nin fact) algorithms and provide the first approximation guarantees for ASHT,\nunder two types of adaptivity. Both of our guarantees are independent of the\nnumber of actions and logarithmic in the number of hypotheses. We numerically\nevaluate the performance of our algorithms using both synthetic and real DNA\nmutation data, demonstrating that our algorithms outperform previous heuristic\npolicies by large margins.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:49:19 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 03:21:34 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gan", "Kyra", ""], ["Jia", "Su", ""], ["Li", "Andrew", ""]]}, {"id": "2103.04387", "submitter": "Nan Wang", "authors": "Nan Wang, Branislav Kveton, Maryam Karimzadehgan", "title": "CORe: Capitalizing On Rewards in Bandit Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bandit algorithm that explores purely by randomizing its past\nobservations. In particular, the sufficient optimism in the mean reward\nestimates is achieved by exploiting the variance in the past observed rewards.\nWe name the algorithm Capitalizing On Rewards (CORe). The algorithm is general\nand can be easily applied to different bandit settings. The main benefit of\nCORe is that its exploration is fully data-dependent. It does not rely on any\nexternal noise and adapts to different problems without parameter tuning. We\nderive a $\\tilde O(d\\sqrt{n\\log K})$ gap-free bound on the $n$-round regret of\nCORe in a stochastic linear bandit, where $d$ is the number of features and $K$\nis the number of arms. Extensive empirical evaluation on multiple synthetic and\nreal-world problems demonstrates the effectiveness of CORe.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:09:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Nan", ""], ["Kveton", "Branislav", ""], ["Karimzadehgan", "Maryam", ""]]}, {"id": "2103.04392", "submitter": "David Newton", "authors": "David Newton, Raghu Bollapragada, Raghu Pasupathy, Nung Kwan Yip", "title": "Retrospective Approximation for Smooth Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider stochastic optimization problems where a smooth (and potentially\nnonconvex) objective is to be minimized using a stochastic first-order oracle.\nThese type of problems arise in many settings from simulation optimization to\ndeep learning. We present Retrospective Approximation (RA) as a universal\nsequential sample-average approximation (SAA) paradigm where during each\niteration $k$, a sample-path approximation problem is implicitly generated\nusing an adapted sample size $M_k$, and solved (with prior solutions as \"warm\nstart\") to an adapted error tolerance $\\epsilon_k$, using a \"deterministic\nmethod\" such as the line search quasi-Newton method. The principal advantage of\nRA is that decouples optimization from stochastic approximation, allowing the\ndirect adoption of existing deterministic algorithms without modification, thus\nmitigating the need to redesign algorithms for the stochastic context. A second\nadvantage is the obvious manner in which RA lends itself to parallelization. We\nidentify conditions on $\\{M_k, k \\geq 1\\}$ and $\\{\\epsilon_k, k\\geq 1\\}$ that\nensure almost sure convergence and convergence in $L_1$-norm, along with\noptimal iteration and work complexity rates. We illustrate the performance of\nRA with line-search quasi-Newton on an ill-conditioned least squares problem,\nas well as an image classification problem using a deep convolutional neural\nnet.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:29:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Newton", "David", ""], ["Bollapragada", "Raghu", ""], ["Pasupathy", "Raghu", ""], ["Yip", "Nung Kwan", ""]]}, {"id": "2103.04413", "submitter": "Guannan Liang", "authors": "Guannan Liang, Qianqian Tong, Chunjiang Zhu, Jinbo Bi", "title": "Escaping Saddle Points with Stochastically Controlled Stochastic\n  Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastically controlled stochastic gradient (SCSG) methods have been proved\nto converge efficiently to first-order stationary points which, however, can be\nsaddle points in nonconvex optimization. It has been observed that a stochastic\ngradient descent (SGD) step introduces anistropic noise around saddle points\nfor deep learning and non-convex half space learning problems, which indicates\nthat SGD satisfies the correlated negative curvature (CNC) condition for these\nproblems. Therefore, we propose to use a separate SGD step to help the SCSG\nmethod escape from strict saddle points, resulting in the CNC-SCSG method. The\nSGD step plays a role similar to noise injection but is more stable. We prove\nthat the resultant algorithm converges to a second-order stationary point with\na convergence rate of $\\tilde{O}( \\epsilon^{-2} log( 1/\\epsilon))$ where\n$\\epsilon$ is the pre-specified error tolerance. This convergence rate is\nindependent of the problem dimension, and is faster than that of CNC-SGD. A\nmore general framework is further designed to incorporate the proposed CNC-SCSG\ninto any first-order method for the method to escape saddle points. Simulation\nstudies illustrate that the proposed algorithm can escape saddle points in much\nfewer epochs than the gradient descent methods perturbed by either noise\ninjection or a SGD step.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:09:43 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 04:19:58 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 17:00:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liang", "Guannan", ""], ["Tong", "Qianqian", ""], ["Zhu", "Chunjiang", ""], ["Bi", "Jinbo", ""]]}, {"id": "2103.04446", "submitter": "Abi Komanduru", "authors": "Abi Komanduru, Jean Honorio", "title": "A Lower Bound for the Sample Complexity of Inverse Reinforcement\n  Learning", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML), 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning (IRL) is the task of finding a reward function\nthat generates a desired optimal policy for a given Markov Decision Process\n(MDP). This paper develops an information-theoretic lower bound for the sample\ncomplexity of the finite state, finite action IRL problem. A geometric\nconstruction of $\\beta$-strict separable IRL problems using spherical codes is\nconsidered. Properties of the ensemble size as well as the Kullback-Leibler\ndivergence between the generated trajectories are derived. The resulting\nensemble is then used along with Fano's inequality to derive a sample\ncomplexity lower bound of $O(n \\log n)$, where $n$ is the number of states in\nthe MDP.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:29:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Komanduru", "Abi", ""], ["Honorio", "Jean", ""]]}, {"id": "2103.04554", "submitter": "Zitong Yang", "authors": "Zitong Yang, Yu Bai, Song Mei", "title": "Exact Gap between Generalization Error and Uniform Convergence in Random\n  Feature Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work showed that there could be a large gap between the classical\nuniform convergence bound and the actual test error of zero-training-error\npredictors (interpolators) such as deep neural networks. To better understand\nthis gap, we study the uniform convergence in the nonlinear random feature\nmodel and perform a precise theoretical analysis on how uniform convergence\ndepends on the sample size and the number of parameters. We derive and prove\nanalytical expressions for three quantities in this model: 1) classical uniform\nconvergence over norm balls, 2) uniform convergence over interpolators in the\nnorm ball (recently proposed by Zhou et al. (2020)), and 3) the risk of minimum\nnorm interpolator. We show that, in the setting where the classical uniform\nconvergence bound is vacuous (diverges to $\\infty$), uniform convergence over\nthe interpolators still gives a non-trivial bound of the test error of\ninterpolating solutions. We also showcase a different setting where classical\nuniform convergence bound is non-vacuous, but uniform convergence over\ninterpolators can give an improved sample complexity guarantee. Our result\nprovides a first exact comparison between the test errors and uniform\nconvergence bounds for interpolators beyond simple linear models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:20:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yang", "Zitong", ""], ["Bai", "Yu", ""], ["Mei", "Song", ""]]}, {"id": "2103.04556", "submitter": "Jiaye Teng", "authors": "Jiaye Teng, Zeren Tan, Yang Yuan", "title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed\n  Coverage for Cox-MLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to deal with censored data, where we only have access to\nthe incomplete information of survival time instead of its exact value.\nFortunately, under linear predictor assumption, people can obtain guaranteed\ncoverage for the confidence band of survival time using methods like Cox\nRegression. However, when relaxing the linear assumption with neural networks\n(e.g., Cox-MLP (Katzman et al., 2018; Kvamme et al., 2019)), we lose the\nguaranteed coverage. To recover the guaranteed coverage without linear\nassumption, we propose two algorithms based on conformal inference. In the\nfirst algorithm WCCI, we revisit weighted conformal inference and introduce a\nnew non-conformity score based on partial likelihood. We then propose a\ntwo-stage algorithm T-SCI, where we run WCCI in the first stage and apply\nquantile conformal inference to calibrate the results in the second stage.\nTheoretical analysis shows that T-SCI returns guaranteed coverage under milder\nassumptions than WCCI. We conduct extensive experiments on synthetic data and\nreal data using different methods, which validate our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:42:05 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 07:20:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Teng", "Jiaye", ""], ["Tan", "Zeren", ""], ["Yuan", "Yang", ""]]}, {"id": "2103.04557", "submitter": "Mojtaba Sahraee-Ardakan", "authors": "Mojtaba Sahraee-Ardakan, Tung Mai, Anup Rao, Ryan Rossi, Sundeep\n  Rangan, Alyson K. Fletcher", "title": "Asymptotics of Ridge Regression in Convolutional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding generalization and estimation error of estimators for simple\nmodels such as linear and generalized linear models has attracted a lot of\nattention recently. This is in part due to an interesting observation made in\nmachine learning community that highly over-parameterized neural networks\nachieve zero training error, and yet they are able to generalize well over the\ntest samples. This phenomenon is captured by the so called double descent\ncurve, where the generalization error starts decreasing again after the\ninterpolation threshold. A series of recent works tried to explain such\nphenomenon for simple models. In this work, we analyze the asymptotics of\nestimation error in ridge estimators for convolutional linear models. These\nconvolutional inverse problems, also known as deconvolution, naturally arise in\ndifferent fields such as seismology, imaging, and acoustics among others. Our\nresults hold for a large class of input distributions that include i.i.d.\nfeatures as a special case. We derive exact formulae for estimation error of\nridge estimators that hold in a certain high-dimensional regime. We show the\ndouble descent phenomenon in our experiments for convolutional models and show\nthat our theoretical results match the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:56:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sahraee-Ardakan", "Mojtaba", ""], ["Mai", "Tung", ""], ["Rao", "Anup", ""], ["Rossi", "Ryan", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "2103.04715", "submitter": "Remi Laumont", "authors": "R\\'emi Laumont, Valentin de Bortoli, Andr\\'es Almansa, Julie Delon,\n  Alain Durmus and Marcelo Pereyra", "title": "Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Venkatakrishnan et al. (2013), Plug & Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:46:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:01:21 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 16:24:03 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 12:11:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Laumont", "R\u00e9mi", ""], ["de Bortoli", "Valentin", ""], ["Almansa", "Andr\u00e9s", ""], ["Delon", "Julie", ""], ["Durmus", "Alain", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2103.04737", "submitter": "Marco Cuturi", "authors": "Meyer Scetbon, Marco Cuturi, Gabriel Peyr\\'e", "title": "Low-Rank Sinkhorn Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several recent applications of optimal transport (OT) theory to machine\nlearning have relied on regularization, notably entropy and the Sinkhorn\nalgorithm. Because matrix-vector products are pervasive in the Sinkhorn\nalgorithm, several works have proposed to \\textit{approximate} kernel matrices\nappearing in its iterations using low-rank factors. Another route lies instead\nin imposing low-rank constraints on the feasible set of couplings considered in\nOT problems, with no approximations on cost nor kernel matrices. This route was\nfirst explored by Forrow et al., 2018, who proposed an algorithm tailored for\nthe squared Euclidean ground cost, using a proxy objective that can be solved\nthrough the machinery of regularized 2-Wasserstein barycenters. Building on\nthis, we introduce in this work a generic approach that aims at solving, in\nfull generality, the OT problem under low-rank constraints with arbitrary\ncosts. Our algorithm relies on an explicit factorization of low rank couplings\nas a product of \\textit{sub-coupling} factors linked by a common marginal;\nsimilar to an NMF approach, we alternatively updates these factors. We prove\nthe non-asymptotic stationary convergence of this algorithm and illustrate its\nefficiency on benchmark experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:18:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Scetbon", "Meyer", ""], ["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "2103.04786", "submitter": "Maximilian Ilse", "authors": "Maximilian Ilse, Patrick Forr\\'e, Max Welling, Joris M. Mooij", "title": "Efficient Causal Inference from Combined Observational and\n  Interventional Data through Causal Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unobserved confounding is one of the main challenges when estimating causal\neffects. We propose a novel causal reduction method that replaces an arbitrary\nnumber of possibly high-dimensional latent confounders with a single latent\nconfounder that lives in the same space as the treatment variable without\nchanging the observational and interventional distributions entailed by the\ncausal model. After the reduction, we parameterize the reduced causal model\nusing a flexible class of transformations, so-called normalizing flows. We\npropose a learning algorithm to estimate the parameterized reduced model\njointly from observational and interventional data. This allows us to estimate\nthe causal effect in a principled way from combined data. We perform a series\nof experiments on data simulated using nonlinear causal mechanisms and find\nthat we can often substantially reduce the number of interventional samples\nwhen adding observational training samples without sacrificing accuracy. Thus,\nadding observational data may help to more accurately estimate causal effects\neven in the presence of unobserved confounders.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:29:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ilse", "Maximilian", ""], ["Forr\u00e9", "Patrick", ""], ["Welling", "Max", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2103.04850", "submitter": "Andrew Jesson D", "authors": "Andrew Jesson, S\\\"oren Mindermann, Yarin Gal, Uri Shalit", "title": "Quantifying Ignorance in Individual-Level Causal-Effect Estimates under\n  Hidden Confounding", "comments": "19 pages, 5 figures, ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning conditional average treatment effects (CATE)\nfrom high-dimensional, observational data with unobserved confounders.\nUnobserved confounders introduce ignorance -- a level of unidentifiability --\nabout an individual's response to treatment by inducing bias in CATE estimates.\nWe present a new parametric interval estimator suited for high-dimensional\ndata, that estimates a range of possible CATE values when given a predefined\nbound on the level of hidden confounding. Further, previous interval estimators\ndo not account for ignorance about the CATE associated with samples that may be\nunderrepresented in the original study, or samples that violate the overlap\nassumption. Our interval estimator also incorporates model uncertainty so that\npractitioners can be made aware of out-of-distribution data. We prove that our\nestimator converges to tight bounds on CATE when there may be unobserved\nconfounding, and assess it using semi-synthetic, high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:58:06 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 15:37:31 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 12:38:36 GMT"}, {"version": "v4", "created": "Mon, 14 Jun 2021 11:36:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Jesson", "Andrew", ""], ["Mindermann", "S\u00f6ren", ""], ["Gal", "Yarin", ""], ["Shalit", "Uri", ""]]}, {"id": "2103.04886", "submitter": "George Dasoulas", "authors": "George Dasoulas, Kevin Scaman, Aladin Virmaux", "title": "Lipschitz Normalization for Self-Attention Layers with Application to\n  Graph Neural Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention based neural networks are state of the art in a large range of\napplications. However, their performance tends to degrade when the number of\nlayers increases. In this work, we show that enforcing Lipschitz continuity by\nnormalizing the attention scores can significantly improve the performance of\ndeep attention models. First, we show that, for deep graph attention networks\n(GAT), gradient explosion appears during training, leading to poor performance\nof gradient-based training algorithms. To address this issue, we derive a\ntheoretical analysis of the Lipschitz continuity of attention modules and\nintroduce LipschitzNorm, a simple and parameter-free normalization for\nself-attention mechanisms that enforces the model to be Lipschitz continuous.\nWe then apply LipschitzNorm to GAT and Graph Transformers and show that their\nperformance is substantially improved in the deep setting (10 to 30 layers).\nMore specifically, we show that a deep GAT model with LipschitzNorm achieves\nstate of the art results for node label prediction tasks that exhibit\nlong-range dependencies, while showing consistent improvements over their\nunnormalized counterparts in benchmark node classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:47:16 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 23:46:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dasoulas", "George", ""], ["Scaman", "Kevin", ""], ["Virmaux", "Aladin", ""]]}, {"id": "2103.04902", "submitter": "Francesca Mignacco", "authors": "Francesca Mignacco, Pierfrancesco Urbani, Lenka Zdeborov\\'a", "title": "Stochasticity helps to navigate rough landscapes: comparing\n  gradient-descent-based algorithms in the phase retrieval problem", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate how gradient-based algorithms such as gradient\ndescent, (multi-pass) stochastic gradient descent, its persistent variant, and\nthe Langevin algorithm navigate non-convex loss-landscapes and which of them is\nable to reach the best generalization error at limited sample complexity. We\nconsider the loss landscape of the high-dimensional phase retrieval problem as\na prototypical highly non-convex example. We observe that for phase retrieval\nthe stochastic variants of gradient descent are able to reach perfect\ngeneralization for regions of control parameters where the gradient descent\nalgorithm is not. We apply dynamical mean-field theory from statistical physics\nto characterize analytically the full trajectories of these algorithms in their\ncontinuous-time limit, with a warm start, and for large system sizes. We\nfurther unveil several intriguing properties of the landscape and the\nalgorithms such as that the gradient descent can obtain better generalization\nproperties from less informed initializations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:06:18 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:39:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mignacco", "Francesca", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2103.04922", "submitter": "Sam Bond-Taylor", "authors": "Sam Bond-Taylor, Adam Leach, Yang Long, Chris G. Willcocks", "title": "Deep Generative Modelling: A Comparative Review of VAEs, GANs,\n  Normalizing Flows, Energy-Based and Autoregressive Models", "comments": "20 pages, 10 figures, updated version submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative modelling is a class of techniques that train deep neural\nnetworks to model the distribution of training samples. Research has fragmented\ninto various interconnected approaches, each of which making trade-offs\nincluding run-time, diversity, and architectural restrictions. In particular,\nthis compendium covers energy-based models, variational autoencoders,\ngenerative adversarial networks, autoregressive models, normalizing flows, in\naddition to numerous hybrid approaches. These techniques are drawn under a\nsingle cohesive framework, comparing and contrasting to explain the premises\nbehind each, while reviewing current state-of-the-art advances and\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:34:03 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:50:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bond-Taylor", "Sam", ""], ["Leach", "Adam", ""], ["Long", "Yang", ""], ["Willcocks", "Chris G.", ""]]}, {"id": "2103.04944", "submitter": "Michael Pfarrhofer", "authors": "Martin Feldkircher, Florian Huber, Gary Koop, Michael Pfarrhofer", "title": "Approximate Bayesian inference and forecasting in huge-dimensional\n  multi-country VARs", "comments": "JEL: C11, C30, E3, D31; Keywords: Multi-country models, macroeconomic\n  forecasting, vector autoregression", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Panel Vector Autoregressive (PVAR) model is a popular tool for\nmacroeconomic forecasting and structural analysis in multi-country applications\nsince it allows for spillovers between countries in a very flexible fashion.\nHowever, this flexibility means that the number of parameters to be estimated\ncan be enormous leading to over-parameterization concerns. Bayesian\nglobal-local shrinkage priors, such as the Horseshoe prior used in this paper,\ncan overcome these concerns, but they require the use of Markov Chain Monte\nCarlo (MCMC) methods rendering them computationally infeasible in high\ndimensions. In this paper, we develop computationally efficient Bayesian\nmethods for estimating PVARs using an integrated rotated Gaussian approximation\n(IRGA). This exploits the fact that whereas own country information is often\nimportant in PVARs, information on other countries is often unimportant. Using\nan IRGA, we split the the posterior into two parts: one involving own country\ncoefficients, the other involving other country coefficients. Fast methods such\nas approximate message passing or variational Bayes can be used on the latter\nand, conditional on these, the former are estimated with precision using MCMC\nmethods. In a forecasting exercise involving PVARs with up to $18$ variables\nfor each of $38$ countries, we demonstrate that our methods produce good\nforecasts quickly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:02:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Koop", "Gary", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2103.04947", "submitter": "Ruosong Wang", "authors": "Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, Sham M. Kakade", "title": "Instabilities of Offline RL with Pre-Trained Neural Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In offline reinforcement learning (RL), we seek to utilize offline data to\nevaluate (or learn) policies in scenarios where the data are collected from a\ndistribution that substantially differs from that of the target policy to be\nevaluated. Recent theoretical advances have shown that such sample-efficient\noffline RL is indeed possible provided certain strong representational\nconditions hold, else there are lower bounds exhibiting exponential error\namplification (in the problem horizon) unless the data collection distribution\nhas only a mild distribution shift relative to the target policy. This work\nstudies these issues from an empirical perspective to gauge how stable offline\nRL methods are. In particular, our methodology explores these ideas when using\nfeatures from pre-trained neural networks, in the hope that these\nrepresentations are powerful enough to permit sample efficient offline RL.\nThrough extensive experiments on a range of tasks, we see that substantial\nerror amplification does occur even when using such pre-trained representations\n(trained on the same task itself); we find offline RL is stable only under\nextremely mild distribution shift. The implications of these results, both from\na theoretical and an empirical perspective, are that successful offline RL\n(where we seek to go beyond the low distribution shift regime) requires\nsubstantially stronger conditions beyond those which suffice for successful\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:06:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Ruosong", ""], ["Wu", "Yifan", ""], ["Salakhutdinov", "Ruslan", ""], ["Kakade", "Sham M.", ""]]}, {"id": "2103.04957", "submitter": "Yan Zhang", "authors": "Yan Zhang", "title": "Learning to Represent and Predict Sets with Deep Neural Networks", "comments": "PhD thesis submitted December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis, we develop various techniques for working with sets in\nmachine learning. Each input or output is not an image or a sequence, but a\nset: an unordered collection of multiple objects, each object described by a\nfeature vector. Their unordered nature makes them suitable for modeling a wide\nvariety of data, ranging from objects in images to point clouds to graphs. Deep\nlearning has recently shown great success on other types of structured data, so\nwe aim to build the necessary structures for sets into deep neural networks.\n  The first focus of this thesis is the learning of better set representations\n(sets as input). Existing approaches have bottlenecks that prevent them from\nproperly modeling relations between objects within the set. To address this\nissue, we develop a variety of techniques for different scenarios and show that\nalleviating the bottleneck leads to consistent improvements across many\nexperiments.\n  The second focus of this thesis is the prediction of sets (sets as output).\nCurrent approaches do not take the unordered nature of sets into account\nproperly. We determine that this results in a problem that causes discontinuity\nissues with many set prediction tasks and prevents them from learning some\nextremely simple datasets. To avoid this problem, we develop two models that\nproperly take the structure of sets into account. Various experiments show that\nour set prediction techniques can significantly benefit over existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:27:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Yan", ""]]}, {"id": "2103.04972", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey and Alex Pentland", "title": "Provably Efficient Cooperative Multi-Agent Reinforcement Learning with\n  Function Approximation", "comments": "53 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning in cooperative multi-agent settings has recently\nadvanced significantly in its scope, with applications in cooperative\nestimation for advertising, dynamic treatment regimes, distributed control, and\nfederated learning. In this paper, we discuss the problem of cooperative\nmulti-agent RL with function approximation, where a group of agents\ncommunicates with each other to jointly solve an episodic MDP. We demonstrate\nthat via careful message-passing and cooperative value iteration, it is\npossible to achieve near-optimal no-regret learning even with a fixed constant\ncommunication budget. Next, we demonstrate that even in heterogeneous\ncooperative settings, it is possible to achieve Pareto-optimal no-regret\nlearning with limited communication. Our work generalizes several ideas from\nthe multi-agent contextual and multi-armed bandit literature to MDPs and\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:51:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Pentland", "Alex", ""]]}, {"id": "2103.04985", "submitter": "Ben Dai", "authors": "Ben Dai, Xiaotong Shen, Wei Pan", "title": "Significance tests of feature relevance for a blackbox learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An exciting recent development is the uptake of deep learning in many\nscientific fields, where the objective is seeking novel scientific insights and\ndiscoveries. To interpret a learning outcome, researchers perform hypothesis\ntesting for explainable features to advance scientific domain knowledge. In\nsuch a situation, testing for a blackbox learner poses a severe challenge\nbecause of intractable models, unknown limiting distributions of parameter\nestimates, and high computational constraints. In this article, we derive two\nconsistent tests for the feature relevance of a blackbox learner. The first one\nevaluates a loss difference with perturbation on an inference sample, which is\nindependent of an estimation sample used for parameter estimation in model\nfitting. The second further splits the inference sample into two but does not\nrequire data perturbation. Also, we develop their combined versions by\naggregating the order statistics of the $p$-values based on repeated sample\nsplitting. To estimate the splitting ratio and the perturbation size, we\ndevelop adaptive splitting schemes for suitably controlling the Type \\rom{1}\nerror subject to computational constraints. By deflating the\n\\textit{bias-sd-ratio}, we establish asymptotic null distributions of the test\nstatistics and their consistency in terms of statistical power. Our theoretical\npower analysis and simulations indicate that the one-split test is more\npowerful than the two-split test, though the latter is easier to apply for\nlarge datasets. Moreover, the combined tests are more stable while compensating\nfor a power loss by repeated sample splitting. Numerically, we demonstrate the\nutility of the proposed tests on two benchmark examples. Accompanying this\npaper is our Python library {\\tt dnn-inference}\nhttps://dnn-inference.readthedocs.io/en/latest/ that implements the proposed\ntests.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:59:19 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 03:28:58 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dai", "Ben", ""], ["Shen", "Xiaotong", ""], ["Pan", "Wei", ""]]}, {"id": "2103.05032", "submitter": "Zachary Charles", "authors": "Zachary Charles, Jakub Kone\\v{c}n\\'y", "title": "Convergence and Accuracy Trade-Offs in Federated Learning and\n  Meta-Learning", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021. PMLR: Volume 130", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of algorithms, which we refer to as local update methods,\ngeneralizing many federated and meta-learning algorithms. We prove that for\nquadratic models, local update methods are equivalent to first-order\noptimization on a surrogate loss we exactly characterize. Moreover, fundamental\nalgorithmic choices (such as learning rates) explicitly govern a trade-off\nbetween the condition number of the surrogate loss and its alignment with the\ntrue loss. We derive novel convergence rates showcasing these trade-offs and\nhighlight their importance in communication-limited settings. Using these\ninsights, we are able to compare local update methods based on their\nconvergence/accuracy trade-off, not just their convergence to critical points\nof the empirical loss. Our results shed new light on a broad range of\nphenomena, including the efficacy of server momentum in federated learning and\nthe impact of proximal client updates.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:40:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Charles", "Zachary", ""], ["Kone\u010dn\u00fd", "Jakub", ""]]}, {"id": "2103.05057", "submitter": "Blake Mason", "authors": "Blake Mason, Ardhendu Tripathy, Robert Nowak", "title": "Nearest Neighbor Search Under Uncertainty", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearest Neighbor Search (NNS) is a central task in knowledge representation,\nlearning, and reasoning. There is vast literature on efficient algorithms for\nconstructing data structures and performing exact and approximate NNS. This\npaper studies NNS under Uncertainty (NNSU). Specifically, consider the setting\nin which an NNS algorithm has access only to a stochastic distance oracle that\nprovides a noisy, unbiased estimate of the distance between any pair of points,\nrather than the exact distance. This models many situations of practical\nimportance, including NNS based on human similarity judgements, physical\nmeasurements, or fast, randomized approximations to exact distances. A naive\napproach to NNSU could employ any standard NNS algorithm and repeatedly query\nand average results from the stochastic oracle (to reduce noise) whenever it\nneeds a pairwise distance. The problem is that a sufficient number of repeated\nqueries is unknown in advance; e.g., a point maybe distant from all but one\nother point (crude distance estimates suffice) or it may be close to a large\nnumber of other points (accurate estimates are necessary). This paper shows how\nideas from cover trees and multi-armed bandits can be leveraged to develop an\nNNSU algorithm that has optimal dependence on the dataset size and the\n(unknown)geometry of the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:20:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Mason", "Blake", ""], ["Tripathy", "Ardhendu", ""], ["Nowak", "Robert", ""]]}, {"id": "2103.05059", "submitter": "Dylan Troop", "authors": "Dylan Troop, Fr\\'ed\\'eric Godin, Jia Yuan Yu", "title": "Bias-Corrected Peaks-Over-Threshold Estimation of the CVaR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional value-at-risk (CVaR) is a useful risk measure in fields such\nas machine learning, finance, insurance, energy, etc. When measuring very\nextreme risk, the commonly used CVaR estimation method of sample averaging does\nnot work well due to limited data above the value-at-risk (VaR), the quantile\ncorresponding to the CVaR level. To mitigate this problem, the CVaR can be\nestimated by extrapolating above a lower threshold than the VaR using a\ngeneralized Pareto distribution (GPD), which is often referred to as the\npeaks-over-threshold (POT) approach. This method often requires a very high\nthreshold to fit well, leading to high variance in estimation, and can induce\nsignificant bias if the threshold is chosen too low. In this paper, we derive a\nnew expression for the GPD approximation error of the CVaR, a bias term induced\nby the choice of threshold, as well as a bias correction method for the\nestimated GPD parameters. This leads to the derivation of a new estimator for\nthe CVaR that we prove to be asymptotically unbiased. In a practical setting,\nwe show through experiments that our estimator provides a significant\nperformance improvement compared with competing CVaR estimators in finite\nsamples. As a consequence of our bias correction method, it is also shown that\na much lower threshold can be selected without introducing significant bias.\nThis allows a larger portion of data to be be used in CVaR estimation compared\nwith the typical POT approach, leading to more stable estimates. As secondary\nresults, a new estimator for a second-order parameter of heavy-tailed\ndistributions is derived, as well as a confidence interval for the CVaR which\nenables quantifying the level of variability in our estimator.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:29:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Troop", "Dylan", ""], ["Godin", "Fr\u00e9d\u00e9ric", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "2103.05092", "submitter": "Larry Wasserman", "authors": "Isabella Verdinelli and Larry Wasserman", "title": "Forest Guided Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use the output of a random forest to define a family of local smoothers\nwith spatially adaptive bandwidth matrices. The smoother inherits the\nflexibility of the original forest but, since it is a simple, linear smoother,\nit is very interpretable and it can be used for tasks that would be intractable\nfor the original forest. This includes bias correction, confidence intervals,\nassessing variable importance and methods for exploring the structure of the\nforest. We illustrate the method on some synthetic examples and on data related\nto Covid-19.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:49:52 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "2103.05104", "submitter": "Ali Al-Sharadqah", "authors": "Ali A. Al-Sharadqah and Lorenzo Rull", "title": "New Methods for Detecting Concentric Objects With High Accuracy", "comments": "31 pages, 18 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting concentric geometric objects to digitized data is an important\nproblem in many areas such as iris detection, autonomous navigation, and\nindustrial robotics operations. There are two common approaches to fitting\ngeometric shapes to data: the geometric (iterative) approach and algebraic\n(non-iterative) approach. The geometric approach is a nonlinear iterative\nmethod that minimizes the sum of the squares of Euclidean distances of the\nobserved points to the ellipses and regarded as the most accurate method, but\nit needs a good initial guess to improve the convergence rate. The algebraic\napproach is based on minimizing the algebraic distances with some constraints\nimposed on parametric space. Each algebraic method depends on the imposed\nconstraint, and it can be solved with the aid of the generalized eigenvalue\nproblem. Only a few methods in literature were developed to solve the problem\nof concentric ellipses. Here we study the statistical properties of existing\nmethods by firstly establishing a general mathematical and statistical\nframework for this problem. Using rigorous perturbation analysis, we derive the\nvariances and biasedness of each method under the small-sigma model. We also\ndevelop new estimators, which can be used as reliable initial guesses for other\niterative methods. Then we compare the performance of each method according to\ntheir theoretical accuracy. Not only do our methods described here outperform\nother existing non-iterative methods, they are also quite robust against large\nnoise. These methods and their practical performances are assessed by a series\nof numerical experiments on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:19:18 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Al-Sharadqah", "Ali A.", ""], ["Rull", "Lorenzo", ""]]}, {"id": "2103.05126", "submitter": "Bal\\'azs Csan\\'ad Cs\\'aji", "authors": "Ambrus Tam\\'as, Bal\\'azs Csan\\'ad Cs\\'aji", "title": "Exact Distribution-Free Hypothesis Tests for the Regression Function of\n  Binary Classification via Conditional Kernel Mean Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we suggest two statistical hypothesis tests for the regression\nfunction of binary classification based on conditional kernel mean embeddings.\nThe regression function is a fundamental object in classification as it\ndetermines both the Bayes optimal classifier and the misclassification\nprobabilities. A resampling based framework is presented and combined with\nconsistent point estimators of the conditional kernel mean map, in order to\nconstruct distribution-free hypothesis tests. These tests are introduced in a\nflexible manner allowing us to control the exact probability of type I error\nfor any sample size. We also prove that both proposed techniques are consistent\nunder weak statistical assumptions, i.e., the type II error probabilities\npointwise converge to zero.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:31:23 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:09:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tam\u00e1s", "Ambrus", ""], ["Cs\u00e1ji", "Bal\u00e1zs Csan\u00e1d", ""]]}, {"id": "2103.05134", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Santiago Paternain and Miguel Calvo-Fullana and\n  Alejandro Ribeiro", "title": "Constrained Learning with Non-Convex Losses", "comments": "arXiv admin note: text overlap with arXiv:2006.05487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though learning has become a core technology of modern information\nprocessing, there is now ample evidence that it can lead to biased, unsafe, and\nprejudiced solutions. The need to impose requirements on learning is therefore\nparamount, especially as it reaches critical applications in social,\nindustrial, and medical domains. However, the non-convexity of most modern\nlearning problems is only exacerbated by the introduction of constraints.\nWhereas good unconstrained solutions can often be learned using empirical risk\nminimization (ERM), even obtaining a model that satisfies statistical\nconstraints can be challenging, all the more so a good one. In this paper, we\novercome this issue by learning in the empirical dual domain, where constrained\nstatistical learning problems become unconstrained, finite dimensional, and\ndeterministic. We analyze the generalization properties of this approach by\nbounding the empirical duality gap, i.e., the difference between our\napproximate, tractable solution and the solution of the original\n(non-convex)~statistical problem, and provide a practical constrained learning\nalgorithm. These results establish a constrained counterpart of classical\nlearning theory and enable the explicit use of constraints in learning. We\nillustrate this algorithm and theory in rate-constrained learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:10:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Paternain", "Santiago", ""], ["Calvo-Fullana", "Miguel", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "2103.05138", "submitter": "Nicolas Emmenegger", "authors": "Nicolas Emmenegger, Rasmus Kyng and Ahad N. Zehmakan", "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum\n  Optimization", "comments": "Added missing upper bound assumption on n in Theorems 4.7 and 4.10", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove lower bounds for higher-order methods in smooth non-convex\nfinite-sum optimization. Our contribution is threefold: We first show that a\ndeterministic algorithm cannot profit from the finite-sum structure of the\nobjective, and that simulating a pth-order regularized method on the whole\nfunction by constructing exact gradient information is optimal up to constant\nfactors. We further show lower bounds for randomized algorithms and compare\nthem with the best known upper bounds. To address some gaps between the bounds,\nwe propose a new second-order smoothness assumption that can be seen as an\nanalogue of the first-order mean-squared smoothness assumption. We prove that\nit is sufficient to ensure state-of-the-art convergence guarantees, while\nallowing for a sharper lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:33:58 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:23:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Emmenegger", "Nicolas", ""], ["Kyng", "Rasmus", ""], ["Zehmakan", "Ahad N.", ""]]}, {"id": "2103.05161", "submitter": "Robert L Obenchain PhD", "authors": "Robert L. Obenchain", "title": "The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE Risk", "comments": "21 pages, 9 figures. arXiv admin note: substantial text overlap with\n  withdrawn arXiv:2005.14291", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generalized ridge regression shrinkage path is proposed that is as\nshort as possible under the restriction that it must pass through the vector of\nregression coefficient estimators that make the overall Optimal Variance-Bias\nTrade-Off under Normal distribution-theory. Five distinct types of ridge TRACE\ndisplays plus other graphics for this efficient path are motivated and\nillustrated here. These visualizations provide invaluable data-analytic\ninsights and improved self-confidence to researchers and data scientists\nfitting linear models to ill-conditioned (confounded) data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:04:55 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 15:56:19 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 19:16:58 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 21:19:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Obenchain", "Robert L.", ""]]}, {"id": "2103.05238", "submitter": "Yifan Chen", "authors": "Yifan Chen, Yun Yang", "title": "Fast Statistical Leverage Score Approximation in Kernel Ridge Regression", "comments": "To appear in the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nystr\\\"om approximation is a fast randomized method that rapidly solves\nkernel ridge regression (KRR) problems through sub-sampling the n-by-n\nempirical kernel matrix appearing in the objective function. However, the\nperformance of such a sub-sampling method heavily relies on correctly\nestimating the statistical leverage scores for forming the sampling\ndistribution, which can be as costly as solving the original KRR. In this work,\nwe propose a linear time (modulo poly-log terms) algorithm to accurately\napproximate the statistical leverage scores in the stationary-kernel-based KRR\nwith theoretical guarantees. Particularly, by analyzing the first-order\ncondition of the KRR objective, we derive an analytic formula, which depends on\nboth the input distribution and the spectral density of stationary kernels, for\ncapturing the non-uniformity of the statistical leverage scores. Numerical\nexperiments demonstrate that with the same prediction accuracy our method is\norders of magnitude more efficient than existing methods in selecting the\nrepresentative sub-samples in the Nystr\\\"om approximation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:57:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chen", "Yifan", ""], ["Yang", "Yun", ""]]}, {"id": "2103.05243", "submitter": "Peizhong Ju", "authors": "Peizhong Ju, Xiaojun Lin, Ness B. Shroff", "title": "On the Generalization Power of Overfitted Two-Layer Neural Tangent\n  Kernel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization performance of min $\\ell_2$-norm\noverfitting solutions for the neural tangent kernel (NTK) model of a two-layer\nneural network. We show that, depending on the ground-truth function, the test\nerror of overfitted NTK models exhibits characteristics that are different from\nthe \"double-descent\" of other overparameterized linear models with simple\nFourier or Gaussian features. Specifically, for a class of learnable functions,\nwe provide a new upper bound of the generalization error that approaches a\nsmall limiting value, even when the number of neurons $p$ approaches infinity.\nThis limiting value further decreases with the number of training samples $n$.\nFor functions outside of this class, we provide a lower bound on the\ngeneralization error that does not diminish to zero even when $n$ and $p$ are\nboth large.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:24:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ju", "Peizhong", ""], ["Lin", "Xiaojun", ""], ["Shroff", "Ness B.", ""]]}, {"id": "2103.05276", "submitter": "Yu Chen", "authors": "Yu Chen, Song Liu, Tom Diethe, Peter Flach", "title": "Continual Density Ratio Estimation in an Online Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In online applications with streaming data, awareness of how far the training\nor test set has shifted away from the original dataset can be crucial to the\nperformance of the model. However, we may not have access to historical samples\nin the data stream. To cope with such situations, we propose a novel method,\nContinual Density Ratio Estimation (CDRE), for estimating density ratios\nbetween the initial and current distributions ($p/q_t$) of a data stream in an\niterative fashion without the need of storing past samples, where $q_t$ is\nshifting away from $p$ over time $t$. We demonstrate that CDRE can be more\naccurate than standard DRE in terms of estimating divergences between\ndistributions, despite not requiring samples from the original distribution.\nCDRE can be applied in scenarios of online learning, such as importance\nweighted covariate shift, tracing dataset changes for better decision making.\nIn addition, (CDRE) enables the evaluation of generative models under the\nsetting of continual learning. To the best of our knowledge, there is no\nexisting method that can evaluate generative models in continual learning\nwithout storing samples from the original distribution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:56:36 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chen", "Yu", ""], ["Liu", "Song", ""], ["Diethe", "Tom", ""], ["Flach", "Peter", ""]]}, {"id": "2103.05277", "submitter": "Kinjal Basu", "authors": "Rohan Ramanath, Sathiya Keerthi, Yao Pan, Konstantin Salomatin, Kinjal\n  Basu", "title": "Efficient Algorithms for Global Inference in Internet Marketplaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching demand to supply in internet marketplaces (e-commerce, ride-sharing,\nfood delivery, professional services, advertising) is a global inference\nproblem that can be formulated as a Linear Program (LP) with (millions of)\ncoupling constraints and (up to a billion) non-coupling polytope constraints.\nUntil recently, solving such problems on web-scale data with an LP formulation\nwas intractable. Recent work (Basu et al., 2020) developed a dual\ndecomposition-based approach to solve such problems when the polytope\nconstraints are simple. In this work, we motivate the need to go beyond these\nsimple polytopes and show real-world internet marketplaces that require more\ncomplex structured polytope constraints. We expand on the recent literature\nwith novel algorithms that are more broadly applicable to global inference\nproblems. We derive an efficient incremental algorithm using a theoretical\ninsight on the nature of solutions on the polytopes to project onto any\narbitrary polytope, that shows massive improvements in performance. Using\nbetter optimization routines along with an adaptive algorithm to control the\nsmoothness of the objective, improves the speed of the solution even further.\nWe showcase the efficacy of our approach via experimental results on web-scale\nmarketplace data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:00:58 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 16:48:54 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ramanath", "Rohan", ""], ["Keerthi", "Sathiya", ""], ["Pan", "Yao", ""], ["Salomatin", "Konstantin", ""], ["Basu", "Kinjal", ""]]}, {"id": "2103.05290", "submitter": "Georgios Arvanitidis", "authors": "Georgios Arvanitidis, Bogdan Georgiev, Bernhard Sch\\\"olkopf", "title": "A prior-based approximate latent Riemannian metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic generative models enable us to capture the geometric structure of\na data manifold lying in a high dimensional space through a Riemannian metric\nin the latent space. However, its practical use is rather limited mainly due to\ninevitable complexity. In this work we propose a surrogate conformal Riemannian\nmetric in the latent space of a generative model that is simple, efficient and\nrobust. This metric is based on a learnable prior that we propose to learn\nusing a basic energy-based model. We theoretically analyze the behavior of the\nproposed metric and show that it is sensible to use in practice. We demonstrate\nexperimentally the efficiency and robustness, as well as the behavior of the\nnew approximate metric. Also, we show the applicability of the proposed\nmethodology for data analysis in the life sciences.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:31:52 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Arvanitidis", "Georgios", ""], ["Georgiev", "Bogdan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2103.05299", "submitter": "Maxime Sangnier", "authors": "Miguel Martinez Herrera (LPSM (UMR\\_8001)), Anna Bonnet (LPSM\n  (UMR\\_8001)), Miguel Herrera, Maxime Sangnier (LPSM (UMR\\_8001))", "title": "Maximum Likelihood Estimation for Hawkes Processes with self-excitation\n  or inhibition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a maximum likelihood method for estimating the\nparameters of a univariate Hawkes process with self-excitation or inhibition.\nOur work generalizes techniques and results that were restricted to the\nself-exciting scenario. The proposed estimator is implemented for the classical\nexponential kernel and we show that, in the inhibition context, our procedure\nprovides more accurate estimations than current alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:56:58 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:29:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Herrera", "Miguel Martinez", "", "LPSM"], ["Bonnet", "Anna", "", "LPSM"], ["Herrera", "Miguel", "", "LPSM"], ["Sangnier", "Maxime", "", "LPSM"]]}, {"id": "2103.05331", "submitter": "Jannik Kossen", "authors": "Jannik Kossen, Sebastian Farquhar, Yarin Gal, Tom Rainforth", "title": "Active Testing: Sample-Efficient Model Evaluation", "comments": "Published at the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for sample-efficient model evaluation that we\ncall active testing. While approaches like active learning reduce the number of\nlabels needed for model training, existing literature largely ignores the cost\nof labeling test data, typically unrealistically assuming large test sets for\nmodel evaluation. This creates a disconnect to real applications, where test\nlabels are important and just as expensive, e.g. for optimizing\nhyperparameters. Active testing addresses this by carefully selecting the test\npoints to label, ensuring model evaluation is sample-efficient. To this end, we\nderive theoretically-grounded and intuitive acquisition strategies that are\nspecifically tailored to the goals of active testing, noting these are distinct\nto those of active learning. As actively selecting labels introduces a bias; we\nfurther show how to remove this bias while reducing the variance of the\nestimator at the same time. Active testing is easy to implement and can be\napplied to any supervised machine learning method. We demonstrate its\neffectiveness on models including WideResNets and Gaussian processes on\ndatasets including Fashion-MNIST and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:20:49 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 07:08:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kossen", "Jannik", ""], ["Farquhar", "Sebastian", ""], ["Gal", "Yarin", ""], ["Rainforth", "Tom", ""]]}, {"id": "2103.05371", "submitter": "Shabbir Bawaji", "authors": "Shabbir Bawaji, Ujjaini Alam, Surajit Mondal and Divya Oberoi", "title": "Exploring Coronal Heating Using Unsupervised Machine-Learning", "comments": "4 pages, 2 figures. This paper has been accepted in the ADASS 2020\n  proceedings. A poster on the same was presented at the ADASS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perplexing mystery of what maintains the solar coronal temperature at\nabout a million K, while the visible disc of the Sun is only at 5800 K, has\nbeen a long standing problem in solar physics. A recent study by Mondal(2020)\nhas provided the first evidence for the presence of numerous ubiquitous\nimpulsive emissions at low radio frequencies from the quiet sun regions, which\ncould hold the key to solving this mystery. These features occur at rates of\nabout five hundred events per minute, and their strength is only a few percent\nof the background steady emission. One of the next steps for exploring the\nfeasibility of this resolution to the coronal heating problem is to understand\nthe morphology of these emissions. To meet this objective we have developed a\ntechnique based on an unsupervised machine learning approach for characterising\nthe morphology of these impulsive emissions. Here we present the results of\napplication of this technique to over 8000 images spanning 70 minutes of data\nin which about 34,500 features could robustly be characterised as 2D elliptical\nGaussians.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:39:00 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bawaji", "Shabbir", ""], ["Alam", "Ujjaini", ""], ["Mondal", "Surajit", ""], ["Oberoi", "Divya", ""]]}, {"id": "2103.05441", "submitter": "Eric Bradford", "authors": "E. Bradford and L. Imsland", "title": "Combining Gaussian processes and polynomial chaos expansions for\n  stochastic nonlinear model predictive control", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model predictive control is an advanced control approach for multivariable\nsystems with constraints, which is reliant on an accurate dynamic model. Most\nreal dynamic models are however affected by uncertainties, which can lead to\nclosed-loop performance deterioration and constraint violations. In this paper\nwe introduce a new algorithm to explicitly consider time-invariant stochastic\nuncertainties in optimal control problems. The difficulty of propagating\nstochastic variables through nonlinear functions is dealt with by combining\nGaussian processes with polynomial chaos expansions. The main novelty in this\npaper is to use this combination in an efficient fashion to obtain mean and\nvariance estimates of nonlinear transformations. Using this algorithm, it is\nshown how to formulate both chance-constraints and a probabilistic objective\nfor the optimal control problem. On a batch reactor case study we firstly\nverify the ability of the new approach to accurately approximate the\nprobability distributions required. Secondly, a tractable stochastic nonlinear\nmodel predictive control approach is formulated with an economic objective to\ndemonstrate the closed-loop performance of the method via Monte Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:25:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bradford", "E.", ""], ["Imsland", "L.", ""]]}, {"id": "2103.05487", "submitter": "T. Konstantin Rusch", "authors": "T. Konstantin Rusch, Siddhartha Mishra", "title": "UnICORNN: A recurrent model for learning very long time dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of recurrent neural networks (RNNs) to accurately process\nsequential inputs with long-time dependencies is very challenging on account of\nthe exploding and vanishing gradient problem. To overcome this, we propose a\nnovel RNN architecture which is based on a structure preserving discretization\nof a Hamiltonian system of second-order ordinary differential equations that\nmodels networks of oscillators. The resulting RNN is fast, invertible (in\ntime), memory efficient and we derive rigorous bounds on the hidden state\ngradients to prove the mitigation of the exploding and vanishing gradient\nproblem. A suite of experiments are presented to demonstrate that the proposed\nRNN provides state of the art performance on a variety of learning tasks with\n(very) long-time dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 15:19:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:31:32 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Rusch", "T. Konstantin", ""], ["Mishra", "Siddhartha", ""]]}, {"id": "2103.05524", "submitter": "St\\'ephane d'Ascoli", "authors": "St\\'ephane d'Ascoli, Marylou Gabri\\'e, Levent Sagun, Giulio Biroli", "title": "More data or more parameters? Investigating the effect of data structure\n  on generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central features of deep learning is the generalization abilities\nof neural networks, which seem to improve relentlessly with\nover-parametrization. In this work, we investigate how properties of data\nimpact the test error as a function of the number of training examples and\nnumber of training parameters; in other words, how the structure of data shapes\nthe \"generalization phase space\". We first focus on the random features model\ntrained in the teacher-student scenario. The synthetic input data is composed\nof independent blocks, which allow us to tune the saliency of low-dimensional\nstructures and their relevance with respect to the target function. Using\nmethods from statistical physics, we obtain an analytical expression for the\ntrain and test errors for both regression and classification tasks in the\nhigh-dimensional limit. The derivation allows us to show that noise in the\nlabels and strong anisotropy of the input data play similar roles on the test\nerror. Both promote an asymmetry of the phase space where increasing the number\nof training examples improves generalization further than increasing the number\nof training parameters. Our analytical insights are confirmed by numerical\nexperiments involving fully-connected networks trained on MNIST and CIFAR10.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:08:41 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["d'Ascoli", "St\u00e9phane", ""], ["Gabri\u00e9", "Marylou", ""], ["Sagun", "Levent", ""], ["Biroli", "Giulio", ""]]}, {"id": "2103.05577", "submitter": "Sofiene Jerbi", "authors": "Sofiene Jerbi, Casper Gyurik, Simon Marshall, Hans J. Briegel, Vedran\n  Dunjko", "title": "Variational quantum policies for reinforcement learning", "comments": "27 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational quantum circuits have recently gained popularity as quantum\nmachine learning models. While considerable effort has been invested to train\nthem in supervised and unsupervised learning settings, relatively little\nattention has been given to their potential use in reinforcement learning. In\nthis work, we leverage the understanding of quantum policy gradient algorithms\nin a number of ways. First, we investigate how to construct and train\nreinforcement learning policies based on variational quantum circuits. We\npropose several designs for quantum policies, provide their learning\nalgorithms, and test their performance on classical benchmarking environments.\nSecond, we show the existence of task environments with a provable separation\nin performance between quantum learning agents and any polynomial-time\nclassical learner, conditioned on the widely-believed classical hardness of the\ndiscrete logarithm problem. We also consider more natural settings, in which we\nshow an empirical quantum advantage of our quantum policies over standard\nneural-network policies. Our results constitute a first step towards\nestablishing a practical near-term quantum advantage in a reinforcement\nlearning setting. Additionally, we believe that some of our design choices for\nvariational quantum policies may also be beneficial to other models based on\nvariational quantum circuits, such as quantum classifiers and quantum\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:33:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jerbi", "Sofiene", ""], ["Gyurik", "Casper", ""], ["Marshall", "Simon", ""], ["Briegel", "Hans J.", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2103.05633", "submitter": "Mohammad Yaghini", "authors": "Hengrui Jia, Mohammad Yaghini, Christopher A. Choquette-Choo, Natalie\n  Dullerud, Anvith Thudi, Varun Chandrasekaran, Nicolas Papernot", "title": "Proof-of-Learning: Definitions and Practice", "comments": "To appear in the 42nd IEEE Symposium on Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning (ML) models typically involves expensive iterative\noptimization. Once the model's final parameters are released, there is\ncurrently no mechanism for the entity which trained the model to prove that\nthese parameters were indeed the result of this optimization procedure. Such a\nmechanism would support security of ML applications in several ways. For\ninstance, it would simplify ownership resolution when multiple parties contest\nownership of a specific model. It would also facilitate the distributed\ntraining across untrusted workers where Byzantine workers might otherwise mount\na denial-of-service by returning incorrect model updates.\n  In this paper, we remediate this problem by introducing the concept of\nproof-of-learning in ML. Inspired by research on both proof-of-work and\nverified computations, we observe how a seminal training algorithm, stochastic\ngradient descent, accumulates secret information due to its stochasticity. This\nproduces a natural construction for a proof-of-learning which demonstrates that\na party has expended the compute require to obtain a set of model parameters\ncorrectly. In particular, our analyses and experiments show that an adversary\nseeking to illegitimately manufacture a proof-of-learning needs to perform *at\nleast* as much work than is needed for gradient descent itself.\n  We also instantiate a concrete proof-of-learning mechanism in both of the\nscenarios described above. In model ownership resolution, it protects the\nintellectual property of models released publicly. In distributed training, it\npreserves availability of the training procedure. Our empirical evaluation\nvalidates that our proof-of-learning mechanism is robust to variance induced by\nthe hardware (ML accelerators) and software stacks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:59:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jia", "Hengrui", ""], ["Yaghini", "Mohammad", ""], ["Choquette-Choo", "Christopher A.", ""], ["Dullerud", "Natalie", ""], ["Thudi", "Anvith", ""], ["Chandrasekaran", "Varun", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2103.05706", "submitter": "Rodolphe Le Riche", "authors": "Reda El Amri, Rodolphe Le Riche, C\\'eline Helbert, Christophette\n  Blanchet-Scalliet, S\\'ebastien Da Veiga", "title": "A sampling criterion for constrained Bayesian optimization with\n  uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of chance constrained optimization where it is sought\nto optimize a function and satisfy constraints, both of which are affected by\nuncertainties. The real world declinations of this problem are particularly\nchallenging because of their inherent computational cost.\n  To tackle such problems, we propose a new Bayesian optimization method. It\napplies to the situation where the uncertainty comes from some of the inputs,\nso that it becomes possible to define an acquisition criterion in the joint\ncontrolled-uncontrolled input space. The main contribution of this work is an\nacquisition criterion that accounts for both the average improvement in\nobjective function and the constraint reliability. The criterion is derived\nfollowing the Stepwise Uncertainty Reduction logic and its maximization\nprovides both optimal controlled and uncontrolled parameters. Analytical\nexpressions are given to efficiently calculate the criterion. Numerical studies\non test functions are presented. It is found through experimental comparisons\nwith alternative sampling criteria that the adequation between the sampling\ncriterion and the problem contributes to the efficiency of the overall\noptimization. As a side result, an expression for the variance of the\nimprovement is given.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:35:56 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 12:28:29 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 13:28:51 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Amri", "Reda El", ""], ["Riche", "Rodolphe Le", ""], ["Helbert", "C\u00e9line", ""], ["Blanchet-Scalliet", "Christophette", ""], ["Da Veiga", "S\u00e9bastien", ""]]}, {"id": "2103.05741", "submitter": "Yihao Feng", "authors": "Yihao Feng, Ziyang Tang, Na Zhang, Qiang Liu", "title": "Non-asymptotic Confidence Intervals of Off-policy Evaluation: Primal and\n  Dual Bounds", "comments": "33 Pages, 5 figures, extended version of a paper with the same title\n  accepted by ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy evaluation (OPE) is the task of estimating the expected reward of\na given policy based on offline data previously collected under different\npolicies. Therefore, OPE is a key step in applying reinforcement learning to\nreal-world domains such as medical treatment, where interactive data collection\nis expensive or even unsafe. As the observed data tends to be noisy and\nlimited, it is essential to provide rigorous uncertainty quantification, not\njust a point estimation, when applying OPE to make high stakes decisions. This\nwork considers the problem of constructing non-asymptotic confidence intervals\nin infinite-horizon off-policy evaluation, which remains a challenging open\nquestion. We develop a practical algorithm through a primal-dual\noptimization-based approach, which leverages the kernel Bellman loss (KBL) of\nFeng et al.(2019) and a new martingale concentration inequality of KBL\napplicable to time-dependent data with unknown mixing conditions. Our algorithm\nmakes minimum assumptions on the data and the function class of the Q-function,\nand works for the behavior-agnostic settings where the data is collected under\na mix of arbitrary unknown behavior policies. We present empirical results that\nclearly demonstrate the advantages of our approach over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:31:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Feng", "Yihao", ""], ["Tang", "Ziyang", ""], ["Zhang", "Na", ""], ["Liu", "Qiang", ""]]}, {"id": "2103.05744", "submitter": "Lukas Herrmann", "authors": "Philipp Grohs and Lukas Herrmann", "title": "Deep neural network approximation for high-dimensional parabolic\n  Hamilton-Jacobi-Bellman equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of solutions to second order Hamilton--Jacobi--Bellman\n(HJB) equations by deep neural networks is investigated. It is shown that for\nHJB equations that arise in the context of the optimal control of certain\nMarkov processes the solution can be approximated by deep neural networks\nwithout incurring the curse of dimension. The dynamics is assumed to depend\naffinely on the controls and the cost depends quadratically on the controls.\nThe admissible controls take values in a bounded set.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:34:13 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Grohs", "Philipp", ""], ["Herrmann", "Lukas", ""]]}, {"id": "2103.05750", "submitter": "Louis Faury", "authors": "Louis Faury and Yoan Russac and Marc Abeille and Cl\\'ement\n  Calauz\\`enes", "title": "Regret Bounds for Generalized Linear Bandits under Parameter Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Bandits (GLBs) are powerful extensions to the Linear\nBandit (LB) setting, broadening the benefits of reward parametrization beyond\nlinearity. In this paper we study GLBs in non-stationary environments,\ncharacterized by a general metric of non-stationarity known as the\nvariation-budget or \\emph{parameter-drift}, denoted $B_T$. While previous\nattempts have been made to extend LB algorithms to this setting, they overlook\na salient feature of GLBs which flaws their results. In this work, we introduce\na new algorithm that addresses this difficulty. We prove that under a geometric\nassumption on the action set, our approach enjoys a\n$\\tilde{\\mathcal{O}}(B_T^{1/3}T^{2/3})$ regret bound. In the general case, we\nshow that it suffers at most a $\\tilde{\\mathcal{O}}(B_T^{1/5}T^{4/5})$ regret.\nAt the core of our contribution is a generalization of the projection step\nintroduced in Filippi et al. (2010), adapted to the non-stationary nature of\nthe problem. Our analysis sheds light on central mechanisms inherited from the\nsetting by explicitly splitting the treatment of the learning and tracking\naspects of the problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:51:50 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Faury", "Louis", ""], ["Russac", "Yoan", ""], ["Abeille", "Marc", ""], ["Calauz\u00e8nes", "Cl\u00e9ment", ""]]}, {"id": "2103.05766", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj", "title": "Interpretable Machines: Constructing Valid Prediction Intervals with\n  Random Forests", "comments": "20 pages including four figures in the main article. Supplementary\n  material available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important issue when using Machine Learning algorithms in recent research\nis the lack of interpretability. Although these algorithms provide accurate\npoint predictions for various learning problems, uncertainty estimates\nconnected with point predictions are rather sparse. A contribution to this gap\nfor the Random Forest Regression Learner is presented here. Based on its\nOut-of-Bag procedure, several parametric and non-parametric prediction\nintervals are provided for Random Forest point predictions and theoretical\nguarantees for its correct coverage probability is delivered. In a second part,\na thorough investigation through Monte-Carlo simulation is conducted evaluating\nthe performance of the proposed methods from three aspects: (i) Analyzing the\ncorrect coverage rate of the proposed prediction intervals, (ii) Inspecting\ninterval width and (iii) Verifying the competitiveness of the proposed\nintervals with existing methods. The simulation yields that the proposed\nprediction intervals are robust towards non-normal residual distributions and\nare competitive by providing correct coverage rates and comparably narrow\ninterval lengths, even for comparably small samples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 23:05:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ramosaj", "Burim", ""]]}, {"id": "2103.05793", "submitter": "Zhifeng Kong", "authors": "Zhifeng Kong, Kamalika Chaudhuri", "title": "Universal Approximation of Residual Flows in Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows are a class of flexible deep generative models that offer\neasy likelihood computation. Despite their empirical success, there is little\ntheoretical understanding of their expressiveness. In this work, we study\nresidual flows, a class of normalizing flows composed of Lipschitz residual\nblocks. We prove residual flows are universal approximators in maximum mean\ndiscrepancy. We provide upper bounds on the number of residual blocks to\nachieve approximation under different assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 00:16:33 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 03:38:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kong", "Zhifeng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "2103.05841", "submitter": "Joshua Minot", "authors": "Joshua R. Minot, Nicholas Cheney, Marc Maier, Danne C. Elbers,\n  Christopher M. Danforth, and Peter Sheridan Dodds", "title": "Interpretable bias mitigation for textual data: Reducing gender bias in\n  patient notes while maintaining classification performance", "comments": "31 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical systems in general, and patient treatment decisions and outcomes in\nparticular, are affected by bias based on gender and other demographic\nelements. As language models are increasingly applied to medicine, there is a\ngrowing interest in building algorithmic fairness into processes impacting\npatient care. Much of the work addressing this question has focused on biases\nencoded in language models -- statistical estimates of the relationships\nbetween concepts derived from distant reading of corpora. Building on this\nwork, we investigate how word choices made by healthcare practitioners and\nlanguage models interact with regards to bias. We identify and remove gendered\nlanguage from two clinical-note datasets and describe a new debiasing procedure\nusing BERT-based gender classifiers. We show minimal degradation in health\ncondition classification tasks for low- to medium-levels of bias removal via\ndata augmentation. Finally, we compare the bias semantically encoded in the\nlanguage models with the bias empirically observed in health records. This work\noutlines an interpretable approach for using data augmentation to identify and\nreduce the potential for bias in natural language processing pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:09:30 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Minot", "Joshua R.", ""], ["Cheney", "Nicholas", ""], ["Maier", "Marc", ""], ["Elbers", "Danne C.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "2103.05844", "submitter": "Lyle Regenwetter", "authors": "Lyle Regenwetter, Brent Curry, Faez Ahmed", "title": "BIKED: A Dataset and Machine Learning Benchmarks for Data-Driven Bicycle\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present \"BIKED,\" a dataset comprised of 4500 individually\ndesigned bicycle models sourced from hundreds of designers. We expect BIKED to\nenable a variety of data-driven design applications for bicycles and support\nthe development of data-driven design methods. The dataset is comprised of a\nvariety of design information including assembly images, component images,\nnumerical design parameters, and class labels. In this paper, we first discuss\nthe processing of the dataset, then highlight some prominent research questions\nthat BIKED can help address. Of these questions, we further explore the\nfollowing in detail: 1) Are there prominent gaps in the current bicycle market\nand design space? We explore the design space using unsupervised dimensionality\nreduction methods. 2) How does one identify the class of a bicycle and what\nfactors play a key role in defining it? We address the bicycle classification\ntask by training a multitude of classifiers using different forms of design\ndata and identifying parameters of particular significance through\npermutation-based interpretability analysis. 3) How does one synthesize new\nbicycles using different representation methods? We consider numerous machine\nlearning methods to generate new bicycle models as well as interpolate between\nand extrapolate from existing models using Variational Autoencoders. The\ndataset and code are available at http://decode.mit.edu/projects/biked/.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:12:32 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:22:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Regenwetter", "Lyle", ""], ["Curry", "Brent", ""], ["Ahmed", "Faez", ""]]}, {"id": "2103.05853", "submitter": "Vatsal Sharan", "authors": "Parikshit Gopalan, Omer Reingold, Vatsal Sharan, Udi Wieder", "title": "Multicalibrated Partitions for Importance Weights", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ratio between the probability that two distributions $R$ and $P$ give to\npoints $x$ are known as importance weights or propensity scores and play a\nfundamental role in many different fields, most notably, statistics and machine\nlearning. Among its applications, importance weights are central to domain\nadaptation, anomaly detection, and estimations of various divergences such as\nthe KL divergence. We consider the common setting where $R$ and $P$ are only\ngiven through samples from each distribution. The vast literature on estimating\nimportance weights is either heuristic, or makes strong assumptions about $R$\nand $P$ or on the importance weights themselves.\n  In this paper, we explore a computational perspective to the estimation of\nimportance weights, which factors in the limitations and possibilities\nobtainable with bounded computational resources. We significantly strengthen\nprevious work that use the MaxEntropy approach, that define the importance\nweights based on a distribution $Q$ closest to $P$, that looks the same as $R$\non every set $C \\in \\mathcal{C}$, where $\\mathcal{C}$ may be a huge collection\nof sets. We show that the MaxEntropy approach may fail to assign high average\nscores to sets $C \\in \\mathcal{C}$, even when the average of ground truth\nweights for the set is evidently large. We similarly show that it may\noverestimate the average scores to sets $C \\in \\mathcal{C}$. We therefore\nformulate Sandwiching bounds as a notion of set-wise accuracy for importance\nweights. We study these bounds to show that they capture natural completeness\nand soundness requirements from the weights. We present an efficient algorithm\nthat under standard learnability assumptions computes weights which satisfy\nthese bounds. Our techniques rely on a new notion of multicalibrated partitions\nof the domain of the distributions, which appear to be useful objects in their\nown right.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:32:36 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Reingold", "Omer", ""], ["Sharan", "Vatsal", ""], ["Wieder", "Udi", ""]]}, {"id": "2103.05896", "submitter": "Suhas S Kowshik", "authors": "Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, Praneeth Netrapalli", "title": "Streaming Linear System Identification with Reverse Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a linear time-invariant (LTI) dynamical\nsystem from a single trajectory via streaming algorithms, which is encountered\nin several applications including reinforcement learning (RL) and time-series\nanalysis. While the LTI system estimation problem is well-studied in the {\\em\noffline} setting, the practically important streaming/online setting has\nreceived little attention. Standard streaming methods like stochastic gradient\ndescent (SGD) are unlikely to work since streaming points can be highly\ncorrelated. In this work, we propose a novel streaming algorithm, SGD with\nReverse Experience Replay ($\\mathsf{SGD}-\\mathsf{RER}$), that is inspired by\nthe experience replay (ER) technique popular in the RL literature.\n$\\mathsf{SGD}-\\mathsf{RER}$ divides data into small buffers and runs SGD\nbackwards on the data stored in the individual buffers. We show that this\nalgorithm exactly deconstructs the dependency structure and obtains information\ntheoretically optimal guarantees for both parameter error and prediction error.\nThus, we provide the first -- to the best of our knowledge -- optimal SGD-style\nalgorithm for the classical problem of linear system identification with a\nfirst order oracle. Furthermore, $\\mathsf{SGD}-\\mathsf{RER}$ can be applied to\nmore general settings like sparse LTI identification with known sparsity\npattern, and non-linear dynamical systems. Our work demonstrates that the\nknowledge of data dependency structure can aid us in designing statistically\nand computationally efficient algorithms which can \"decorrelate\" streaming\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:51:55 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 15:22:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jain", "Prateek", ""], ["Kowshik", "Suhas S", ""], ["Nagaraj", "Dheeraj", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "2103.05909", "submitter": "Luca Maestrini", "authors": "Luca Maestrini, Robert G. Aykroyd and Matt P. Wand", "title": "A Variational Inference Framework for Inverse Problems", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for fitting inverse problem models via variational\nBayes approximations. This methodology guarantees flexibility to statistical\nmodel specification for a broad range of applications, good accuracy\nperformances and reduced model fitting times, when compared with standard\nMarkov chain Monte Carlo methods. The message passing and factor graph fragment\napproach to variational Bayes we describe facilitates streamlined\nimplementation of approximate inference algorithms and forms the basis to\nsoftware development. Such approach allows for supple inclusion of numerous\nresponse distributions and penalizations into the inverse problem model. Albeit\nour analysis is circumscribed to one- and two-dimensional response variables,\nwe lay down an infrastructure where streamlining algorithmic steps based on\nnullifying weak interactions between variables are extendible to inverse\nproblems in higher dimensions. Image processing applications motivated by\nbiomedical and archaeological problems are included as illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:37:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Maestrini", "Luca", ""], ["Aykroyd", "Robert G.", ""], ["Wand", "Matt P.", ""]]}, {"id": "2103.05916", "submitter": "Dominique Vaufreydaz", "authors": "Louis Airale (M-PSI, PERCEPTION), Dominique Vaufreydaz (M-PSI), Xavier\n  Alameda-Pineda (PERCEPTION)", "title": "SocialInteractionGAN: Multi-person Interaction Sequence Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of human actions in social interactions has important applications\nin the design of social robots or artificial avatars. In this paper, we model\nhuman interaction generation as a discrete multi-sequence generation problem\nand present SocialInteractionGAN, a novel adversarial architecture for\nconditional interaction generation. Our model builds on a recurrent\nencoder-decoder generator network and a dual-stream discriminator. This\narchitecture allows the discriminator to jointly assess the realism of\ninteractions and that of individual action sequences. Within each stream a\nrecurrent network operating on short subsequences endows the output signal with\nlocal assessments, better guiding the forthcoming generation. Crucially,\ncontextual information on interacting participants is shared among agents and\nreinjected in both the generation and the discriminator evaluation processes.\nWe show that the proposed SocialInteractionGAN succeeds in producing high\nrealism action sequences of interacting people, comparing favorably to a\ndiversity of recurrent and convolutional discriminator baselines. Evaluations\nare conducted using modified Inception Score and Fr{\\'e}chet Inception Distance\nmetrics, that we specifically design for discrete sequential generated data.\nThe distribution of generated sequences is shown to approach closely that of\nreal data. In particular our model properly learns the dynamics of interaction\nsequences, while exploiting the full range of actions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:11:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Airale", "Louis", "", "M-PSI, PERCEPTION"], ["Vaufreydaz", "Dominique", "", "M-PSI"], ["Alameda-Pineda", "Xavier", "", "PERCEPTION"]]}, {"id": "2103.06002", "submitter": "Lorenz Kuhn", "authors": "Lorenz Kuhn, Clare Lyle, Aidan N. Gomez, Jonas Rothfuss, Yarin Gal", "title": "Robustness to Pruning Predicts Generalization in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing generalization measures that aim to capture a model's simplicity\nbased on parameter counts or norms fail to explain generalization in\noverparameterized deep neural networks. In this paper, we introduce a new,\ntheoretically motivated measure of a network's simplicity which we call\nprunability: the smallest \\emph{fraction} of the network's parameters that can\nbe kept while pruning without adversely affecting its training loss. We show\nthat this measure is highly predictive of a model's generalization performance\nacross a large set of convolutional networks trained on CIFAR-10, does not grow\nwith network size unlike existing pruning-based measures, and exhibits high\ncorrelation with test set loss even in a particularly challenging double\ndescent setting. Lastly, we show that the success of prunability cannot be\nexplained by its relation to known complexity measures based on models' margin,\nflatness of minima and optimization speed, finding that our new measure is\nsimilar to -- but more predictive than -- existing flatness-based measures, and\nthat its predictions exhibit low mutual information with those of other\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 11:39:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kuhn", "Lorenz", ""], ["Lyle", "Clare", ""], ["Gomez", "Aidan N.", ""], ["Rothfuss", "Jonas", ""], ["Gal", "Yarin", ""]]}, {"id": "2103.06114", "submitter": "Timothy Verstynen", "authors": "Timothy Verstynen, Konrad Kording", "title": "A critical reappraisal of predicting suicidal ideation using fMRI", "comments": "6 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many psychiatric disorders, neuroimaging offers a potential for\nrevolutionizing diagnosis and treatment by providing access to preverbal mental\nprocesses. In their study \"Machine learning of neural representations of\nsuicide and emotion concepts identifies suicidal youth.\"1, Just and colleagues\nreport that a Naive Bayes classifier, trained on voxelwise fMRI responses in\nhuman participants during the presentation of words and concepts related to\nmortality, can predict whether an individual had reported having suicidal\nideations with a classification accuracy of 91%. Here we report a reappraisal\nof the methods employed by the authors, including re-analysis of the same data\nset, that calls into question the accuracy of the authors findings.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:08:57 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Verstynen", "Timothy", ""], ["Kording", "Konrad", ""]]}, {"id": "2103.06189", "submitter": "Alberto Bemporad Prof.", "authors": "Alberto Bemporad", "title": "Piecewise linear regression and classification", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a method for solving multivariate regression and\nclassification problems using piecewise linear predictors over a polyhedral\npartition of the feature space. The resulting algorithm that we call PARC\n(Piecewise Affine Regression and Classification) alternates between (i) solving\nridge regression problems for numeric targets, softmax regression problems for\ncategorical targets, and either softmax regression or cluster centroid\ncomputation for piecewise linear separation, and (ii) assigning the training\npoints to different clusters on the basis of a criterion that balances\nprediction accuracy and piecewise-linear separability. We prove that PARC is a\nblock-coordinate descent algorithm that optimizes a suitably constructed\nobjective function, and that it converges in a finite number of steps to a\nlocal minimum of that function. The accuracy of the algorithm is extensively\ntested numerically on synthetic and real-world datasets, showing that the\napproach provides an extension of linear regression/classification that is\nparticularly useful when the obtained predictor is used as part of an\noptimization model. A Python implementation of the algorithm described in this\npaper is available at http://cse.lab.imtlucca.it/~bemporad/parc .\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:07:57 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bemporad", "Alberto", ""]]}, {"id": "2103.06206", "submitter": "Balu Nadiga", "authors": "B. T. Nadiga", "title": "Reservoir Computing as a Tool for Climate Predictability Studies", "comments": "31 pages with 12 figures", "journal-ref": null, "doi": "10.1029/2020MS002290", "report-no": null, "categories": "physics.geo-ph cs.LG nlin.CD physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced-order dynamical models play a central role in developing our\nunderstanding of predictability of climate irrespective of whether we are\ndealing with the actual climate system or surrogate climate-models. In this\ncontext, the Linear-Inverse-Modeling (LIM) approach, by capturing a few\nessential interactions between dynamical components of the full system, has\nproven valuable in providing insights into predictability of the full system.\nWe demonstrate that Reservoir Computing (RC), a form of learning suitable for\nsystems with chaotic dynamics, provides an alternative nonlinear approach that\nimproves on the predictive skill of the LIM approach. We do this in the example\nsetting of predicting sea-surface-temperature in the North Atlantic in the\npre-industrial control simulation of a popular earth system model, the\nCommunity-Earth-System-Model so that we can compare the performance of the new\nRC based approach with the traditional LIM approach both when learning data is\nplentiful and when such data is more limited. The improved predictive skill of\nthe RC approach over a wide range of conditions -- larger number of retained\nEOF coefficients, extending well into the limited data regime, etc. -- suggests\nthat this machine-learning technique may have a use in climate predictability\nstudies. While the possibility of developing a climate emulator -- the ability\nto continue the evolution of the system on the attractor long after failing to\nbe able to track the reference trajectory -- is demonstrated in the Lorenz-63\nsystem, it is suggested that further development of the RC approach may permit\nsuch uses of the new approach in more realistic predictability studies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:22:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Nadiga", "B. T.", ""]]}, {"id": "2103.06219", "submitter": "Shuofeng Zhang", "authors": "Shuofeng Zhang, Isaac Reid, Guillermo Valle P\\'erez, Ard Louis", "title": "Why flatness does and does not correlate with generalization for deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The intuition that local flatness of the loss landscape is correlated with\nbetter generalization for deep neural networks (DNNs) has been explored for\ndecades, spawning many different flatness measures. Recently, this link with\ngeneralization has been called into question by a demonstration that many\nmeasures of flatness are vulnerable to parameter re-scaling which arbitrarily\nchanges their value without changing neural network outputs.\n  Here we show that, in addition, some popular variants of SGD such as Adam and\nEntropy-SGD, can also break the flatness-generalization correlation. As an\nalternative to flatness measures, we use a function based picture and propose\nusing the log of Bayesian prior upon initialization, $\\log P(f)$, as a\npredictor of the generalization when a DNN converges on function $f$ after\ntraining to zero error. The prior is directly proportional to the Bayesian\nposterior for functions that give zero error on a test set. For the case of\nimage classification, we show that $\\log P(f)$ is a significantly more robust\npredictor of generalization than flatness measures are.\n  Whilst local flatness measures fail under parameter re-scaling, the\nprior/posterior, which is global quantity, remains invariant under re-scaling.\nMoreover, the correlation with generalization as a function of data complexity\nremains good for different variants of SGD.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:44:52 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:25:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Shuofeng", ""], ["Reid", "Isaac", ""], ["P\u00e9rez", "Guillermo Valle", ""], ["Louis", "Ard", ""]]}, {"id": "2103.06244", "submitter": "Marc-Henri Bleu-Laine", "authors": "Marc-Henri Bleu-Laine, Tejas G. Puranik, Dimitri N. Mavris, Bryan\n  Matthews", "title": "Multi-Class Multiple Instance Learning for Predicting Precursors to\n  Aviation Safety Events", "comments": "29 pages, 15 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, there has been a rapid growth in the application of machine\nlearning techniques that leverage aviation data collected from commercial\nairline operations to improve safety. Anomaly detection and predictive\nmaintenance have been the main targets for machine learning applications.\nHowever, this paper focuses on the identification of precursors, which is a\nrelatively newer application. Precursors are events correlated with adverse\nevents that happen prior to the adverse event itself. Therefore, precursor\nmining provides many benefits including understanding the reasons behind a\nsafety incident and the ability to identify signatures, which can be tracked\nthroughout a flight to alert the operators of the potential for an adverse\nevent in the future. This work proposes using the multiple-instance learning\n(MIL) framework, a weakly supervised learning task, combined with carefully\ndesigned binary classifier leveraging a Multi-Head Convolutional Neural\nNetwork-Recurrent Neural Network (MHCNN-RNN) architecture. Multi-class\nclassifiers are then created and compared, enabling the prediction of different\nadverse events for any given flight by combining binary classifiers, and by\nmodifying the MHCNN-RNN to handle multiple outputs. Results obtained showed\nthat the multiple binary classifiers perform better and are able to accurately\nforecast high speed and high path angle events during the approach phase.\nMultiple binary classifiers are also capable of determining the aircraft's\nparameters that are correlated to these events. The identified parameters can\nbe considered precursors to the events and may be studied/tracked further to\nprevent these events in the future.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:25:57 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bleu-Laine", "Marc-Henri", ""], ["Puranik", "Tejas G.", ""], ["Mavris", "Dimitri N.", ""], ["Matthews", "Bryan", ""]]}, {"id": "2103.06261", "submitter": "Xiaoqing Tan", "authors": "Xiaoqing Tan, Chung-Chou H. Chang, Lu Tang", "title": "A Tree-based Federated Learning Approach for Personalized Treatment\n  Effect Estimation from Heterogeneous Data Sources", "comments": "An earlier version won JSM 2021 Student Paper Competition (SLDS\n  section) Honorable Mention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an appealing framework for analyzing sensitive data\nfrom distributed health data networks. Under this framework, data partners at\nlocal sites collaboratively build an analytical model under the orchestration\nof a coordinating site, while keeping the data decentralized. While integrating\ninformation from multiple sources may boost statistical efficiency, existing\nfederated learning methods mainly assume data across sites are homogeneous\nsamples of the global population, failing to properly account for the extra\nvariability across sites in estimation and inference. Drawing on a\nmulti-hospital electronic health records network, we develop an efficient and\ninterpretable tree-based ensemble of personalized treatment effect estimators\nto join results across hospital sites, while actively modeling for the\nheterogeneity in data sources through site partitioning. The efficiency of this\napproach is demonstrated by a study of causal effects of oxygen saturation on\nhospital mortality and backed up by comprehensive numerical results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:51:30 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 00:35:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tan", "Xiaoqing", ""], ["Chang", "Chung-Chou H.", ""], ["Tang", "Lu", ""]]}, {"id": "2103.06263", "submitter": "Bahar Taskesen", "authors": "Bahar Taskesen, Soroosh Shafieezadeh-Abadeh, Daniel Kuhn", "title": "Semi-Discrete Optimal Transport: Hardness, Regularization and Numerical\n  Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-discrete optimal transport problems, which evaluate the Wasserstein\ndistance between a discrete and a generic (possibly non-discrete) probability\nmeasure, are believed to be computationally hard. Even though such problems are\nubiquitous in statistics, machine learning and computer vision, however, this\nperception has not yet received a theoretical justification. To fill this gap,\nwe prove that computing the Wasserstein distance between a discrete probability\nmeasure supported on two points and the Lebesgue measure on the standard\nhypercube is already #P-hard. This insight prompts us to seek approximate\nsolutions for semi-discrete optimal transport problems. We thus perturb the\nunderlying transportation cost with an additive disturbance governed by an\nambiguous probability distribution, and we introduce a distributionally robust\ndual optimal transport problem whose objective function is smoothed with the\nmost adverse disturbance distributions from within a given ambiguity set. We\nfurther show that smoothing the dual objective function is equivalent to\nregularizing the primal objective function, and we identify several ambiguity\nsets that give rise to several known and new regularization schemes. As a\nbyproduct, we discover an intimate relation between semi-discrete optimal\ntransport problems and discrete choice models traditionally studied in\npsychology and economics. To solve the regularized optimal transport problems\nefficiently, we use a stochastic gradient descent algorithm with imprecise\nstochastic gradient oracles. A new convergence analysis reveals that this\nalgorithm improves the best known convergence guarantee for semi-discrete\noptimal transport problems with entropic regularizers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:53:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Taskesen", "Bahar", ""], ["Shafieezadeh-Abadeh", "Soroosh", ""], ["Kuhn", "Daniel", ""]]}, {"id": "2103.06397", "submitter": "Wai Tong Chung", "authors": "Wai Tong Chung, Aashwin Ananda Mishra, Matthias Ihme", "title": "Interpretable Data-driven Methods for Subgrid-scale Closure in LES for\n  Transcritical LOX/GCH4 Combustion", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many practical combustion systems such as those in rockets, gas turbines, and\ninternal combustion engines operate under high pressures that surpass the\nthermodynamic critical limit of fuel-oxidizer mixtures. These conditions\nrequire the consideration of complex fluid behaviors that pose challenges for\nnumerical simulations, casting doubts on the validity of existing subgrid-scale\n(SGS) models in large-eddy simulations of these systems. While data-driven\nmethods have shown high accuracy as closure models in simulations of turbulent\nflames, these models are often criticized for lack of physical\ninterpretability, wherein they provide answers but no insight into their\nunderlying rationale. The objective of this study is to assess SGS stress\nmodels from conventional physics-driven approaches and an interpretable machine\nlearning algorithm, i.e., the random forest regressor, in a turbulent\ntranscritical non-premixed flame. To this end, direct numerical simulations\n(DNS) of transcritical liquid-oxygen/gaseous-methane (LOX/GCH4) inert and\nreacting flows are performed. Using this data, a priori analysis is performed\non the Favre-filtered DNS data to examine the accuracy of physics-based and\nrandom forest SGS-models under these conditions. SGS stresses calculated with\nthe gradient model show good agreement with the exact terms extracted from\nfiltered DNS. The accuracy of the random-forest regressor decreased when\nphysics-based constraints are applied to the feature set. Results demonstrate\nthat random forests can perform as effectively as algebraic models when\nmodeling subgrid stresses, only when trained on a sufficiently representative\ndatabase. The employment of random forest feature importance score is shown to\nprovide insight into discovering subgrid-scale stresses through sparse\nregression.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 00:54:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Chung", "Wai Tong", ""], ["Mishra", "Aashwin Ananda", ""], ["Ihme", "Matthias", ""]]}, {"id": "2103.06428", "submitter": "Will Wei Sun", "authors": "Hilda S Ibriga and Will Wei Sun", "title": "Covariate-assisted Sparse Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to provably complete a sparse and highly-missing tensor in the\npresence of covariate information along tensor modes. Our motivation comes from\nonline advertising where users click-through-rates (CTR) on ads over various\ndevices form a CTR tensor that has about 96% missing entries and has many zeros\non non-missing entries, which makes the standalone tensor completion method\nunsatisfactory. Beside the CTR tensor, additional ad features or user\ncharacteristics are often available. In this paper, we propose\nCovariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate\ninformation for the recovery of the sparse tensor. The key idea is to jointly\nextract latent components from both the tensor and the covariate matrix to\nlearn a synthetic representation. Theoretically, we derive the error bound for\nthe recovered tensor components and explicitly quantify the improvements on\nboth the reveal probability condition and the tensor recovery accuracy due to\ncovariates. Finally, we apply COSTCO to an advertisement dataset consisting of\na CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over\nthe baseline. An important by-product is that ad latent components from COSTCO\nreveal interesting ad clusters, which are useful for better ad targeting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:13:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ibriga", "Hilda S", ""], ["Sun", "Will Wei", ""]]}, {"id": "2103.06476", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith, David Arbour, Ritwik Sinha, Edward H. Kennedy, and\n  Aaditya Ramdas", "title": "Doubly robust confidence sequences for sequential causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives time-uniform confidence sequences (CS) for causal effects\nin experimental and observational settings. A confidence sequence for a target\nparameter $\\psi$ is a sequence of confidence intervals $(C_t)_{t=1}^\\infty$\nsuch that every one of these intervals simultaneously captures $\\psi$ with high\nprobability. Such CSs provide valid statistical inference for $\\psi$ at\narbitrary stopping times, unlike classical fixed-time confidence intervals\nwhich require the sample size to be fixed in advance. Existing methods for\nconstructing CSs focus on the nonasymptotic regime where certain assumptions\n(such as known bounds on the random variables) are imposed, while doubly robust\nestimators of causal effects rely on (asymptotic) semiparametric theory. We use\nsequential versions of central limit theorem arguments to construct\nlarge-sample CSs for causal estimands, with a particular focus on the average\ntreatment effect (ATE) under nonparametric conditions. These CSs allow analysts\nto update inferences about the ATE in lieu of new data, and experiments can be\ncontinuously monitored, stopped, or continued for any data-dependent reason,\nall while controlling the type-I error. Finally, we describe how these CSs\nreadily extend to other causal estimands and estimators, providing a new\nframework for sequential causal inference in a wide array of problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:45:35 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:50:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Arbour", "David", ""], ["Sinha", "Ritwik", ""], ["Kennedy", "Edward H.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.06503", "submitter": "Ching-Yao Chuang", "authors": "Ching-Yao Chuang, Youssef Mroueh", "title": "Fair Mixup: Fairness via Interpolation", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training classifiers under fairness constraints such as group fairness,\nregularizes the disparities of predictions between the groups. Nevertheless,\neven though the constraints are satisfied during training, they might not\ngeneralize at evaluation time. To improve the generalizability of fair\nclassifiers, we propose fair mixup, a new data augmentation strategy for\nimposing the fairness constraint. In particular, we show that fairness can be\nachieved by regularizing the models on paths of interpolated samples between\nthe groups. We use mixup, a powerful data augmentation strategy to generate\nthese interpolates. We analyze fair mixup and empirically show that it ensures\na better generalization for both accuracy and fairness measurement in tabular,\nvision, and language benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:57:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Chuang", "Ching-Yao", ""], ["Mroueh", "Youssef", ""]]}, {"id": "2103.06624", "submitter": "Huan Zhang", "authors": "Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh,\n  J. Zico Kolter", "title": "Beta-CROWN: Efficient Bound Propagation with Per-neuron Split\n  Constraints for Complete and Incomplete Neural Network Verification", "comments": "Shiqi Wang, Huan Zhang and Kaidi Xu contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in neural network verification show that cheap incomplete\nverifiers such as CROWN, based upon bound propagations, can effectively be used\nin Branch-and-Bound (BaB) methods to accelerate complete verification,\nachieving significant speedups compared to expensive linear programming (LP)\nbased techniques. However, they cannot fully handle the per-neuron split\nconstraints introduced by BaB like LP verifiers do, leading to looser bounds\nand hurting their verification efficiency. In this work, we develop\n$\\beta$-CROWN, a new bound propagation based method that can fully encode\nper-neuron splits via optimizable parameters $\\beta$. When the optimizable\nparameters are jointly optimized in intermediate layers, $\\beta$-CROWN has the\npotential of producing better bounds than typical LP verifiers with neuron\nsplit constraints, while being efficiently parallelizable on GPUs. Applied to\nthe complete verification setting, $\\beta$-CROWN is close to three orders of\nmagnitude faster than LP-based BaB methods for robustness verification, and\nalso over twice faster than state-of-the-art GPU-based complete verifiers with\nsimilar timeout rates. By terminating BaB early, our method can also be used\nfor incomplete verification. Compared to the state-of-the-art\nsemidefinite-programming (SDP) based verifier, we show a substantial leap\nforward by greatly reducing the gap between verified accuracy and empirical\nadversarial attack accuracy, from 35% (SDP) to 12% on an adversarially trained\nMNIST network ($\\epsilon=0.3$), while being 47 times faster. Our code is\navailable at https://github.com/KaidiXu/Beta-CROWN\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:56:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Shiqi", ""], ["Zhang", "Huan", ""], ["Xu", "Kaidi", ""], ["Lin", "Xue", ""], ["Jana", "Suman", ""], ["Hsieh", "Cho-Jui", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2103.06671", "submitter": "Thanh Nguyen-Tang", "authors": "Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, Svetha Venkatesh", "title": "Sample Complexity of Offline Reinforcement Learning with Deep ReLU\n  Networks", "comments": "A short version published in the ICML Workshop on Reinforcement\n  Learning Theory, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the statistical theory of offline reinforcement learning (RL) with\ndeep ReLU network function approximation. We analyze a variant of fitted-Q\niteration (FQI) algorithm under a new dynamic condition that we call Besov\ndynamic closure, which encompasses the conditions from prior analyses for deep\nneural network function approximation. Under Besov dynamic closure, we prove\nthat the FQI-type algorithm enjoys the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 -\n2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the\ndimensionality of the state-action space, $\\alpha$ is the (possibly fractional)\nsmoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified\nprecision. This is an improvement over the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 -\nd/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an\nalgorithmic iteration number which is arbitrarily large in practice.\nImportantly, our sample complexity is obtained under the new general dynamic\ncondition and a data-dependent structure where the latter is either ignored in\nprior algorithms or improperly handled by prior analyses. This is the first\ncomprehensive analysis for offline RL with deep ReLU network function\napproximation under a general setting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:01:14 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:16:30 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 16:04:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nguyen-Tang", "Thanh", ""], ["Gupta", "Sunil", ""], ["Tran-The", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2103.06701", "submitter": "Anna Kuzina", "authors": "Anna Kuzina, Max Welling, Jakub M. Tomczak", "title": "Diagnosing Vulnerability of Variational Auto-Encoders to Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we explore adversarial attacks on the Variational Autoencoders\n(VAE). We show how to modify data point to obtain a prescribed latent code\n(supervised attack) or just get a drastically different code (unsupervised\nattack). We examine the influence of model modifications ($\\beta$-VAE, NVAE) on\nthe robustness of VAEs and suggest metrics to quantify it.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:23:20 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 07:02:22 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 08:41:15 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kuzina", "Anna", ""], ["Welling", "Max", ""], ["Tomczak", "Jakub M.", ""]]}, {"id": "2103.06712", "submitter": "Matias Bilkis", "authors": "M. Bilkis, M. Cerezo, Guillaume Verdon, Patrick J. Coles, Lukasz\n  Cincio", "title": "A semi-agnostic ansatz with variable structure for quantum machine\n  learning", "comments": "15 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-22040", "categories": "quant-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum machine learning (QML) offers a powerful, flexible paradigm for\nprogramming near-term quantum computers, with applications in chemistry,\nmetrology, materials science, data science, and mathematics. Here, one trains\nan ansatz, in the form of a parameterized quantum circuit, to accomplish a task\nof interest. However, challenges have recently emerged suggesting that deep\nansatzes are difficult to train, due to flat training landscapes caused by\nrandomness or by hardware noise. This motivates our work, where we present a\nvariable structure approach to build ansatzes for QML. Our approach, called\nVAns (Variable Ansatz), applies a set of rules to both grow and (crucially)\nremove quantum gates in an informed manner during the optimization.\nConsequently, VAns is ideally suited to mitigate trainability and noise-related\nissues by keeping the ansatz shallow. We employ VAns in the variational quantum\neigensolver for condensed matter and quantum chemistry applications and also in\nthe quantum autoencoder for data compression, showing successful results in all\ncases.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:58:40 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bilkis", "M.", ""], ["Cerezo", "M.", ""], ["Verdon", "Guillaume", ""], ["Coles", "Patrick J.", ""], ["Cincio", "Lukasz", ""]]}, {"id": "2103.06828", "submitter": "Yijie Wang", "authors": "Yijie Wang, Viet Anh Nguyen, Grani A. Hanasusanto", "title": "Wasserstein Robust Classification with Fairness Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a distributionally robust classification model with a fairness\nconstraint that encourages the classifier to be fair in view of the equality of\nopportunity criterion. We use a type-$\\infty$ Wasserstein ambiguity set\ncentered at the empirical distribution to model distributional uncertainty and\nderive a conservative reformulation for the worst-case equal opportunity\nunfairness measure. We establish that the model is equivalent to a mixed binary\noptimization problem, which can be solved by standard off-the-shelf solvers. To\nimprove scalability, we further propose a convex, hinge-loss-based model for\nlarge problem instances whose reformulation does not incur any binary\nvariables. Moreover, we also consider the distributionally robust learning\nproblem with a generic ground transportation cost to hedge against the\nuncertainties in the label and sensitive attribute. Finally, we numerically\ndemonstrate that our proposed approaches improve fairness with negligible loss\nof predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:53:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 03:11:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Yijie", ""], ["Nguyen", "Viet Anh", ""], ["Hanasusanto", "Grani A.", ""]]}, {"id": "2103.06845", "submitter": "Fabio Ferreira", "authors": "Fabio S. Ferreira, Agoston Mihalik, Rick A. Adams, John Ashburner,\n  Janaina Mourao-Miranda", "title": "A hierarchical Bayesian model to find brain-behaviour associations in\n  incomplete data sets", "comments": "52 pages, 18 figures (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Canonical Correlation Analysis (CCA) and its regularised versions have been\nwidely used in the neuroimaging community to uncover multivariate associations\nbetween two data modalities (e.g., brain imaging and behaviour). However, these\nmethods have inherent limitations: (1) statistical inferences about the\nassociations are often not robust; (2) the associations within each data\nmodality are not modelled; (3) missing values need to be imputed or removed.\nGroup Factor Analysis (GFA) is a hierarchical model that addresses the first\ntwo limitations by providing Bayesian inference and modelling modality-specific\nassociations. Here, we propose an extension of GFA that handles missing data,\nand highlight that GFA can be used as a predictive model. We applied GFA to\nsynthetic and real data consisting of brain connectivity and non-imaging\nmeasures from the Human Connectome Project (HCP). In synthetic data, GFA\nuncovered the underlying shared and specific factors and predicted correctly\nthe non-observed data modalities in complete and incomplete data sets. In the\nHCP data, we identified four relevant shared factors, capturing associations\nbetween mood, alcohol and drug use, cognition, demographics and\npsychopathological measures and the default mode, frontoparietal control,\ndorsal and ventral networks and insula, as well as two factors describing\nassociations within brain connectivity. In addition, GFA predicted a set of\nnon-imaging measures from brain connectivity. These findings were consistent in\ncomplete and incomplete data sets, and replicated previous findings in the\nliterature. GFA is a promising tool that can be used to uncover associations\nbetween and within multiple data modalities in benchmark datasets (such as,\nHCP), and easily extended to more complex models to solve more challenging\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:14:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ferreira", "Fabio S.", ""], ["Mihalik", "Agoston", ""], ["Adams", "Rick A.", ""], ["Ashburner", "John", ""], ["Mourao-Miranda", "Janaina", ""]]}, {"id": "2103.06872", "submitter": "Sirui Lu", "authors": "Sirui Lu, M\\'arton Kan\\'asz-Nagy, Ivan Kukuljan, J. Ignacio Cirac", "title": "Tensor networks and efficient descriptions of classical data", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.str-el cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the potential of tensor network based machine learning methods\nto scale to large image and text data sets. For that, we study how the mutual\ninformation between a subregion and its complement scales with the subsystem\nsize $L$, similarly to how it is done in quantum many-body physics. We find\nthat for text, the mutual information scales as a power law $L^\\nu$ with a\nclose to volume law exponent, indicating that text cannot be efficiently\ndescribed by 1D tensor networks. For images, the scaling is close to an area\nlaw, hinting at 2D tensor networks such as PEPS could have an adequate\nexpressibility. For the numerical analysis, we introduce a mutual information\nestimator based on autoregressive networks, and we also use convolutional\nneural networks in a neural estimator method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:57:16 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lu", "Sirui", ""], ["Kan\u00e1sz-Nagy", "M\u00e1rton", ""], ["Kukuljan", "Ivan", ""], ["Cirac", "J. Ignacio", ""]]}, {"id": "2103.06885", "submitter": "Philip Waggoner", "authors": "Philip D. Waggoner", "title": "Modern Dimension Reduction", "comments": "83 pages, 36 figures, to appear in the Cambridge University Press\n  Elements in Quantitative and Computational Methods for the Social Sciences\n  series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data are not only ubiquitous in society, but are increasingly complex both in\nsize and dimensionality. Dimension reduction offers researchers and scholars\nthe ability to make such complex, high dimensional data spaces simpler and more\nmanageable. This Element offers readers a suite of modern unsupervised\ndimension reduction techniques along with hundreds of lines of R code, to\nefficiently represent the original high dimensional data space in a simplified,\nlower dimensional subspace. Launching from the earliest dimension reduction\ntechnique principal components analysis and using real social science data, I\nintroduce and walk readers through application of the following techniques:\nlocally linear embedding, t-distributed stochastic neighbor embedding (t-SNE),\nuniform manifold approximation and projection, self-organizing maps, and deep\nautoencoders. The result is a well-stocked toolbox of unsupervised algorithms\nfor tackling the complexities of high dimensional data so common in modern\nsociety. All code is publicly accessible on Github.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:54:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Waggoner", "Philip D.", ""]]}, {"id": "2103.06923", "submitter": "Sreejith Sreekumar Dr", "authors": "Sreejith Sreekumar, Zhengxin Zhang, Ziv Goldfeld", "title": "Non-Asymptotic Performance Guarantees for Neural Estimation of\n  $\\mathsf{f}$-Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical distances (SDs), which quantify the dissimilarity between\nprobability distributions, are central to machine learning and statistics. A\nmodern method for estimating such distances from data relies on parametrizing a\nvariational form by a neural network (NN) and optimizing it. These estimators\nare abundantly used in practice, but corresponding performance guarantees are\npartial and call for further exploration. In particular, there seems to be a\nfundamental tradeoff between the two sources of error involved: approximation\nand estimation. While the former needs the NN class to be rich and expressive,\nthe latter relies on controlling complexity. This paper explores this tradeoff\nby means of non-asymptotic error bounds, focusing on three popular choices of\nSDs -- Kullback-Leibler divergence, chi-squared divergence, and squared\nHellinger distance. Our analysis relies on non-asymptotic function\napproximation theorems and tools from empirical process theory. Numerical\nresults validating the theory are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:47:30 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Sreekumar", "Sreejith", ""], ["Zhang", "Zhengxin", ""], ["Goldfeld", "Ziv", ""]]}, {"id": "2103.06939", "submitter": "Preston Biro", "authors": "Preston Biro and Stephen G. Walker", "title": "A Reinforcement Learning Based Approach to Play Calling in Football", "comments": "62 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the vast amount of data collected on football and the growth of\ncomputing abilities, many games involving decision choices can be optimized.\nThe underlying rule is the maximization of an expected utility of outcomes and\nthe law of large numbers. The data available allows us to compute with high\naccuracy the probabilities of outcomes of decisions and the well defined points\nsystem in the game allows us to have the necessary terminal utilities. With\nsome well established theory we can then optimize choices at a single play\nlevel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:23:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Biro", "Preston", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2103.06950", "submitter": "Fergus Simpson", "authors": "Fergus Simpson, Alexis Boukouvalas, Vaclav Cadek, Elvijs Sarkans,\n  Nicolas Durrande", "title": "The Minecraft Kernel: Modelling correlated Gaussian Processes in the\n  Fourier domain", "comments": null, "journal-ref": "Artificial Intelligence and Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the univariate setting, using the kernel spectral representation is an\nappealing approach for generating stationary covariance functions. However,\nperforming the same task for multiple-output Gaussian processes is\nsubstantially more challenging. We demonstrate that current approaches to\nmodelling cross-covariances with a spectral mixture kernel possess a critical\nblind spot. For a given pair of processes, the cross-covariance is not\nreproducible across the full range of permitted correlations, aside from the\nspecial case where their spectral densities are of identical shape. We present\na solution to this issue by replacing the conventional Gaussian components of a\nspectral mixture with block components of finite bandwidth (i.e. rectangular\nstep functions). The proposed family of kernel represents the first\nmulti-output generalisation of the spectral mixture kernel that can approximate\nany stationary multi-output kernel to arbitrary precision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:54:51 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Simpson", "Fergus", ""], ["Boukouvalas", "Alexis", ""], ["Cadek", "Vaclav", ""], ["Sarkans", "Elvijs", ""], ["Durrande", "Nicolas", ""]]}, {"id": "2103.06967", "submitter": "Vijay Gupta", "authors": "Martin Figura, Krishna Chaitanya Kosaraju, and Vijay Gupta", "title": "Adversarial attacks in consensus-based multi-agent reinforcement\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, many cooperative distributed multi-agent reinforcement learning\n(MARL) algorithms have been proposed in the literature. In this work, we study\nthe effect of adversarial attacks on a network that employs a consensus-based\nMARL algorithm. We show that an adversarial agent can persuade all the other\nagents in the network to implement policies that optimize an objective that it\ndesires. In this sense, the standard consensus-based MARL algorithms are\nfragile to attacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:44:18 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Figura", "Martin", ""], ["Kosaraju", "Krishna Chaitanya", ""], ["Gupta", "Vijay", ""]]}, {"id": "2103.07020", "submitter": "Seonho Kim", "authors": "Seonho Kim, Sohail Bahmani, and Kiryung Lee", "title": "Max-Linear Regression by Scalable and Guaranteed Convex Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multivariate max-linear regression problem where the model\nparameters\n$\\boldsymbol{\\beta}_{1},\\dotsc,\\boldsymbol{\\beta}_{k}\\in\\mathbb{R}^{p}$ need to\nbe estimated from $n$ independent samples of the (noisy) observations $y =\n\\max_{1\\leq j \\leq k} \\boldsymbol{\\beta}_{j}^{\\mathsf{T}} \\boldsymbol{x} +\n\\mathrm{noise}$. The max-linear model vastly generalizes the conventional\nlinear model, and it can approximate any convex function to an arbitrary\naccuracy when the number of linear models $k$ is large enough. However, the\ninherent nonlinearity of the max-linear model renders the estimation of the\nregression parameters computationally challenging. Particularly, no estimator\nbased on convex programming is known in the literature. We formulate and\nanalyze a scalable convex program as the estimator for the max-linear\nregression problem. Under the standard Gaussian observation setting, we present\na non-asymptotic performance guarantee showing that the convex program recovers\nthe parameters with high probability. When the $k$ linear components are\nequally likely to achieve the maximum, our result shows that a sufficient\nnumber of observations scales as $k^{2}p$ up to a logarithmic factor. This\nsignificantly improves on the analogous prior result based on alternating\nminimization (Ghosh et al., 2019). Finally, through a set of Monte Carlo\nsimulations, we illustrate that our theoretical result is consistent with\nempirical behavior, and the convex estimator for max-linear regression is as\ncompetitive as the alternating minimization algorithm in practice.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:55:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kim", "Seonho", ""], ["Bahmani", "Sohail", ""], ["Lee", "Kiryung", ""]]}, {"id": "2103.07045", "submitter": "Namjoon Suh", "authors": "Yuchen He, Namjoon Suh, Xiaoming Huo, Sungha Kang, Yajun Mei", "title": "Asymptotic Theory of $\\ell_1$-Regularized PDE Identification from a\n  Single Noisy Trajectory", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove the support recovery for a general class of linear and nonlinear\nevolutionary partial differential equation (PDE) identification from a single\nnoisy trajectory using $\\ell_1$ regularized Pseudo-Least Squares\nmodel~($\\ell_1$-PsLS). In any associative $\\mathbb{R}$-algebra generated by\nfinitely many differentiation operators that contain the unknown PDE operator,\napplying $\\ell_1$-PsLS to a given data set yields a family of candidate models\nwith coefficients $\\mathbf{c}(\\lambda)$ parameterized by the regularization\nweight $\\lambda\\geq 0$. The trace of $\\{\\mathbf{c}(\\lambda)\\}_{\\lambda\\geq 0}$\nsuffers from high variance due to data noises and finite difference\napproximation errors. We provide a set of sufficient conditions which guarantee\nthat, from a single trajectory data denoised by a Local-Polynomial filter, the\nsupport of $\\mathbf{c}(\\lambda)$ asymptotically converges to the true\nsigned-support associated with the underlying PDE for sufficiently many data\nand a certain range of $\\lambda$. We also show various numerical experiments to\nvalidate our theory.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:23:04 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["He", "Yuchen", ""], ["Suh", "Namjoon", ""], ["Huo", "Xiaoming", ""], ["Kang", "Sungha", ""], ["Mei", "Yajun", ""]]}, {"id": "2103.07066", "submitter": "Jann Spiess", "authors": "Jann Spiess and Vasilis Syrgkanis", "title": "Evidence-Based Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past years have seen seen the development and deployment of\nmachine-learning algorithms to estimate personalized treatment-assignment\npolicies from randomized controlled trials. Yet such algorithms for the\nassignment of treatment typically optimize expected outcomes without taking\ninto account that treatment assignments are frequently subject to hypothesis\ntesting. In this article, we explicitly take significance testing of the effect\nof treatment-assignment policies into account, and consider assignments that\noptimize the probability of finding a subset of individuals with a\nstatistically significant positive treatment effect. We provide an efficient\nimplementation using decision trees, and demonstrate its gain over selecting\nsubsets based on positive (estimated) treatment effects. Compared to standard\ntree-based regression and classification tools, this approach tends to yield\nsubstantially higher power in detecting subgroups with positive treatment\neffects.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:36:03 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Spiess", "Jann", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2103.07084", "submitter": "Takayuki Osa", "authors": "Takayuki Osa, Voot Tangkaratt and Masashi Sugiyama", "title": "Discovering Diverse Solutions in Deep Reinforcement Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reinforcement learning (RL) algorithms are typically limited to learning a\nsingle solution of a specified task, even though there often exists diverse\nsolutions to a given task. Compared with learning a single solution, learning a\nset of diverse solutions is beneficial because diverse solutions enable robust\nfew-shot adaptation and allow the user to select a preferred solution. Although\nprevious studies have showed that diverse behaviors can be modeled with a\npolicy conditioned on latent variables, an approach for modeling an infinite\nset of diverse solutions with continuous latent variables has not been\ninvestigated. In this study, we propose an RL method that can learn infinitely\nmany solutions by training a policy conditioned on a continuous or discrete\nlow-dimensional latent variable. Through continuous control tasks, we\ndemonstrate that our method can learn diverse solutions in a data-efficient\nmanner and that the solutions can be used for few-shot adaptation to solve\nunseen tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:54:31 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Osa", "Takayuki", ""], ["Tangkaratt", "Voot", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.07088", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Lexin Li", "title": "Orthogonal Statistical Inference for Multimodal Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal imaging has transformed neuroscience research. While it presents\nunprecedented opportunities, it also imposes serious challenges. Particularly,\nit is difficult to combine the merits of interpretability attributed to a\nsimple association model and flexibility achieved by a highly adaptive\nnonlinear model. In this article, we propose an orthogonal statistical\ninferential framework, built upon the Neyman orthogonality and a form of\ndecomposition orthogonality, for multimodal data analysis. We target the\nsetting that naturally arises in almost all multimodal studies, where there is\na primary modality of interest, plus additional auxiliary modalities. We\nsuccessfully establish the root-$N$-consistency and asymptotic normality of the\nestimated primary parameter, the semi-parametric estimation efficiency, and the\nasymptotic honesty of the confidence interval of the predicted primary modality\neffect. Our proposal enjoys, to a good extent, both model interpretability and\nmodel flexibility. It is also considerably different from the existing\nstatistical methods for multimodal data integration, as well as the\northogonality-based methods for high-dimensional inferences. We demonstrate the\nefficacy of our method through both simulations and an application to a\nmultimodal neuroimaging study of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:04:31 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dai", "Xiaowu", ""], ["Li", "Lexin", ""]]}, {"id": "2103.07155", "submitter": "Manuela Geiss", "authors": "Florian Sobieczky, Salma Mahmoud, Simon Neugebauer, Lukas Rippitsch,\n  Manuela Gei{\\ss}", "title": "Explainable AI by BAPC -- Before and After correction Parameter\n  Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By means of a local surrogate approach, an analytical method to yield\nexplanations of AI-predictions in the framework of regression models is\ndefined. In the case of the AI-model producing additive corrections to the\npredictions of a base model, the explanations are delivered in the form of a\nshift of its interpretable parameters as long as the AI- predictions are small\nin a rigorously defined sense. Criteria are formulated giving a precise\nrelation between lost accuracy and lacking model fidelity. Two applications\nshow how physical or econometric parameters may be used to interpret the action\nof neural network and random forest models in the sense of the underlying base\nmodel. This is an extended version of our paper presented at the ISM 2020\nconference, where we first introduced our new approach BAPC.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:03:51 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Sobieczky", "Florian", ""], ["Mahmoud", "Salma", ""], ["Neugebauer", "Simon", ""], ["Rippitsch", "Lukas", ""], ["Gei\u00df", "Manuela", ""]]}, {"id": "2103.07206", "submitter": "Daniel Barrejon", "authors": "Daniel Barrej\\'on, Pablo M. Olmos, Antonio Art\\'es-Rodr\\'iguez", "title": "Medical data wrangling with sequential variational autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical data sets are usually corrupted by noise and missing data. These\nmissing patterns are commonly assumed to be completely random, but in medical\nscenarios, the reality is that these patterns occur in bursts due to sensors\nthat are off for some time or data collected in a misaligned uneven fashion,\namong other causes. This paper proposes to model medical data records with\nheterogeneous data types and bursty missing data using sequential variational\nautoencoders (VAEs). In particular, we propose a new methodology, the Shi-VAE,\nwhich extends the capabilities of VAEs to sequential streams of data with\nmissing observations. We compare our model against state-of-the-art solutions\nin an intensive care unit database (ICU) and a dataset of passive human\nmonitoring. Furthermore, we find that standard error metrics such as RMSE are\nnot conclusive enough to assess temporal models and include in our analysis the\ncross-correlation between the ground truth and the imputed signal. We show that\nShi-VAE achieves the best performance in terms of using both metrics, with\nlower computational complexity than the GP-VAE model, which is the\nstate-of-the-art method for medical records.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:59:26 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Barrej\u00f3n", "Daniel", ""], ["Olmos", "Pablo M.", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "2103.07281", "submitter": "Joseph Park", "authors": "Joseph Park, Gerald M Pao, Erik Stabenau, George Sugihara, Thomas\n  Lorimer", "title": "Empirical Mode Modeling: A data-driven approach to recover and forecast\n  nonlinear dynamics from noisy data", "comments": "Submitted to Nonlinear Dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven, model-free analytics are natural choices for discovery and\nforecasting of complex, nonlinear systems. Methods that operate in the system\nstate-space require either an explicit multidimensional state-space, or, one\napproximated from available observations. Since observational data are\nfrequently sampled with noise, it is possible that noise can corrupt the\nstate-space representation degrading analytical performance. Here, we evaluate\nthe synthesis of empirical mode decomposition with empirical dynamic modeling,\nwhich we term empirical mode modeling, to increase the information content of\nstate-space representations in the presence of noise. Evaluation of a\nmathematical, and, an ecologically important geophysical application across\nthree different state-space representations suggests that empirical mode\nmodeling may be a useful technique for data-driven, model-free, state-space\nanalysis in the presence of noise.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:21:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Park", "Joseph", ""], ["Pao", "Gerald M", ""], ["Stabenau", "Erik", ""], ["Sugihara", "George", ""], ["Lorimer", "Thomas", ""]]}, {"id": "2103.07287", "submitter": "Xingtu Liu", "authors": "Xingtu Liu", "title": "Neural Networks with Complex-Valued Weights Have No Spurious Local\n  Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the benefits of complex-valued weights for neural networks. We prove\nthat shallow complex neural networks with quadratic activations have no\nspurious local minima. In contrast, shallow real neural networks with quadratic\nactivations have infinitely many spurious local minima under the same\nconditions. In addition, we provide specific examples to demonstrate that\ncomplex-valued weights turn poor local minima into saddle points. The\nactivation function CReLU is also discussed to illustrate the superiority of\nanalytic activations in complex-valued neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:44:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Liu", "Xingtu", ""]]}, {"id": "2103.07405", "submitter": "Christian Agrell", "authors": "Christian Agrell, Kristina Rognlien Dahl, Andreas Hafver", "title": "Optimal sequential decision making with probabilistic digital twins", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twins are emerging in many industries, typically consisting of\nsimulation models and data associated with a specific physical system. One of\nthe main reasons for developing a digital twin, is to enable the simulation of\npossible consequences of a given action, without the need to interfere with the\nphysical system itself. Physical systems of interest, and the environments they\noperate in, do not always behave deterministically. Moreover, information about\nthe system and its environment is typically incomplete or imperfect.\nProbabilistic representations of systems and environments may therefore be\ncalled for, especially to support decisions in application areas where actions\nmay have severe consequences.\n  In this paper we introduce the probabilistic digital twin (PDT). We will\nstart by discussing how epistemic uncertainty can be treated using measure\ntheory, by modelling epistemic information via $\\sigma$-algebras. Based on\nthis, we give a formal definition of how epistemic uncertainty can be updated\nin a PDT. We then study the problem of optimal sequential decision making. That\nis, we consider the case where the outcome of each decision may inform the\nnext. Within the PDT framework, we formulate this optimization problem. We\ndiscuss how this problem may be solved (at least in theory) via the maximum\nprinciple method or the dynamic programming principle. However, due to the\ncurse of dimensionality, these methods are often not tractable in practice. To\nmend this, we propose a generic approximate solution using deep reinforcement\nlearning together with neural networks defined on sets. We illustrate the\nmethod on a practical problem, considering optimal information gathering for\nthe estimation of a failure probability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:06:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Agrell", "Christian", ""], ["Dahl", "Kristina Rognlien", ""], ["Hafver", "Andreas", ""]]}, {"id": "2103.07453", "submitter": "Hiba Nassar", "authors": "Rani Basna, Hiba Nassar and Krzysztof Podg\\'orski", "title": "Machine Learning Assisted Orthonormal Basis Selection for Functional\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In implementations of the functional data methods, the effect of the initial\nchoice of an orthonormal basis has not gained much attention in the past.\nTypically, several standard bases such as Fourier, wavelets, splines, etc. are\nconsidered to transform observed functional data and a choice is made without\nany formal criteria indicating which of the bases is preferable for the initial\ntransformation of the data into functions. In an attempt to address this issue,\nwe propose a strictly data-driven method of orthogonal basis selection. The\nmethod uses recently introduced orthogonal spline bases called the splinets\nobtained by efficient orthogonalization of the B-splines. The algorithm learns\nfrom the data in the machine learning style to efficiently place knots. The\noptimality criterion is based on the average (per functional data point) mean\nsquare error and is utilized both in the learning algorithms and in comparison\nstudies. The latter indicates efficiency that is particularly evident for the\nsparse functional data and to a lesser degree in analyses of responses to\ncomplex physical systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:27:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Basna", "Rani", ""], ["Nassar", "Hiba", ""], ["Podg\u00f3rski", "Krzysztof", ""]]}, {"id": "2103.07501", "submitter": "Soumya Basu", "authors": "Soumya Basu, Karthik Abinav Sankararaman, Abishek Sankararaman", "title": "Beyond $\\log^2(T)$ Regret for Decentralized Bandits in Matching Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design decentralized algorithms for regret minimization in the two-sided\nmatching market with one-sided bandit feedback that significantly improves upon\nthe prior works (Liu et al. 2020a, 2020b, Sankararaman et al. 2020). First, for\ngeneral markets, for any $\\varepsilon > 0$, we design an algorithm that\nachieves a $O(\\log^{1+\\varepsilon}(T))$ regret to the agent-optimal stable\nmatching, with unknown time horizon $T$, improving upon the $O(\\log^{2}(T))$\nregret achieved in (Liu et al. 2020b). Second, we provide the optimal\n$\\Theta(\\log(T))$ agent-optimal regret for markets satisfying uniqueness\nconsistency -- markets where leaving participants don't alter the original\nstable matching. Previously, $\\Theta(\\log(T))$ regret was achievable\n(Sankararaman et al. 2020, Liu et al. 2020b) in the much restricted serial\ndictatorship setting, when all arms have the same preference over the agents.\nWe propose a phase-based algorithm, wherein each phase, besides deleting the\nglobally communicated dominated arms the agents locally delete arms with which\nthey collide often. This local deletion is pivotal in breaking deadlocks\narising from rank heterogeneity of agents across arms. We further demonstrate\nthe superiority of our algorithm over existing works through simulations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 19:46:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Basu", "Soumya", ""], ["Sankararaman", "Karthik Abinav", ""], ["Sankararaman", "Abishek", ""]]}, {"id": "2103.07560", "submitter": "Sofia Triantafillou", "authors": "Sofia Triantafillou and Fattaneh Jabbari and Greg Cooper", "title": "Causal Markov Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection is an important problem in machine learning, which aims to\nselect variables that lead to an optimal predictive model. In this paper, we\nfocus on feature selection for post-intervention outcome prediction from\npre-intervention variables. We are motivated by healthcare settings, where the\ngoal is often to select the treatment that will maximize a specific patient's\noutcome; however, we often do not have sufficient randomized control trial data\nto identify well the conditional treatment effect. We show how we can use\nobservational data to improve feature selection and effect estimation in two\ncases: (a) using observational data when we know the causal graph, and (b) when\nwe do not know the causal graph but have observational and limited experimental\ndata. Our paper extends the notion of Markov boundary to treatment-outcome\npairs. We provide theoretical guarantees for the methods we introduce. In\nsimulated data, we show that combining observational and experimental data\nimproves feature selection and effect estimation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:49:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Triantafillou", "Sofia", ""], ["Jabbari", "Fattaneh", ""], ["Cooper", "Greg", ""]]}, {"id": "2103.07600", "submitter": "Guanzhe Hong", "authors": "Guanzhe Hong, Zhiyuan Mao, Xiaojun Lin, Stanley H. Chan", "title": "Student-Teacher Learning from Clean Inputs to Noisy Inputs", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based student-teacher learning, a training method that encourages the\nstudent's hidden features to mimic those of the teacher network, is empirically\nsuccessful in transferring the knowledge from a pre-trained teacher network to\nthe student network. Furthermore, recent empirical results demonstrate that,\nthe teacher's features can boost the student network's generalization even when\nthe student's input sample is corrupted by noise. However, there is a lack of\ntheoretical insights into why and when this method of transferring knowledge\ncan be successful between such heterogeneous tasks. We analyze this method\ntheoretically using deep linear networks, and experimentally using nonlinear\nnetworks. We identify three vital factors to the success of the method: (1)\nwhether the student is trained to zero training loss; (2) how knowledgeable the\nteacher is on the clean-input problem; (3) how the teacher decomposes its\nknowledge in its hidden features. Lack of proper control in any of the three\nfactors leads to failure of the student-teacher learning method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:29:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hong", "Guanzhe", ""], ["Mao", "Zhiyuan", ""], ["Lin", "Xiaojun", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2103.07614", "submitter": "Lech Szymanski", "authors": "Lech Szymanski, Brendan McCane, Craig Atkinson", "title": "Conceptual capacity and effective complexity of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a complexity measure of a neural network mapping function based on\nthe diversity of the set of tangent spaces from different inputs. Treating each\ntangent space as a linear PAC concept we use an entropy-based measure of the\nbundle of concepts in order to estimate the conceptual capacity of the network.\nThe theoretical maximal capacity of a ReLU network is equivalent to the number\nof its neurons. In practice however, due to correlations between neuron\nactivities within the network, the actual capacity can be remarkably small,\neven for very big networks. Empirical evaluations show that this new measure is\ncorrelated with the complexity of the mapping function and thus the\ngeneralisation capabilities of the corresponding network. It captures the\neffective, as oppose to the theoretical, complexity of the network function. We\nalso showcase some uses of the proposed measure for analysis and comparison of\ntrained neural network models.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 04:32:59 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Szymanski", "Lech", ""], ["McCane", "Brendan", ""], ["Atkinson", "Craig", ""]]}, {"id": "2103.07626", "submitter": "Yu-Chia Chen", "authors": "Yu-Chia Chen, Marina Meil\\u{a}, Ioannis G. Kevrekidis", "title": "Helmholtzian Eigenmap: Topological feature discovery & edge flow\n  learning from point cloud data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manifold Helmholtzian (1-Laplacian) operator $\\Delta_1$ elegantly\ngeneralizes the Laplace-Beltrami operator to vector fields on a manifold\n$\\mathcal M$. In this work, we propose the estimation of the manifold\nHelmholtzian from point cloud data by a weighted 1-Laplacian $\\mathbf{\\mathcal\nL}_1$. While higher order Laplacians ave been introduced and studied, this work\nis the first to present a graph Helmholtzian constructed from a simplicial\ncomplex as an estimator for the continuous operator in a non-parametric\nsetting. Equipped with the geometric and topological information about\n$\\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and\nvector fields on $\\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the\n$\\mathbf{\\mathcal L}_1$ allows the smoothing, prediction, and feature\nextraction of the flows. We demonstrate these possibilities on substantial sets\nof synthetic and real point cloud datasets with non-trivial topological\nstructures; and provide theoretical results on the limit of $\\mathbf{\\mathcal\nL}_1$ to $\\Delta_1$.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 05:56:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Yu-Chia", ""], ["Meil\u0103", "Marina", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "2103.07756", "submitter": "Pengxiang Wu", "authors": "Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, Chao Chen", "title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "comments": "ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise is frequently observed in real-world large-scale datasets. The\nnoise is introduced due to a variety of reasons; it is heterogeneous and\nfeature-dependent. Most existing approaches to handling noisy labels fall into\ntwo categories: they either assume an ideal feature-independent noise, or\nremain heuristic without theoretical guarantees. In this paper, we propose to\ntarget a new family of feature-dependent label noise, which is much more\ngeneral than commonly used i.i.d. label noise and encompasses a broad spectrum\nof noise patterns. Focusing on this general noise family, we propose a\nprogressive label correction algorithm that iteratively corrects labels and\nrefines the model. We provide theoretical guarantees showing that for a wide\nvariety of (unknown) noise patterns, a classifier trained with this strategy\nconverges to be consistent with the Bayes classifier. In experiments, our\nmethod outperforms SOTA baselines and is robust to various noise types and\nlevels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:34:22 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 07:28:12 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 05:34:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Yikai", ""], ["Zheng", "Songzhu", ""], ["Wu", "Pengxiang", ""], ["Goswami", "Mayank", ""], ["Chen", "Chao", ""]]}, {"id": "2103.07776", "submitter": "Kristofer Reyes", "authors": "Soojung Baek, Kristofer G. Reyes", "title": "Problem-fluent models for complex decision-making in autonomous\n  materials research", "comments": "To be published in Computational Materials Science", "journal-ref": null, "doi": "10.1016/j.commatsci.2021.110385", "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We review our recent work in the area of autonomous materials research,\nhighlighting the coupling of machine learning methods and models and more\nproblem-aware modeling. We review the general Bayesian framework for\nclosed-loop design employed by many autonomous materials platforms. We then\nprovide examples of our work on such platforms. We finally review our\napproaches to extend current statistical and ML models to better reflect\nproblem-specific structure including the use of physics-based models and\nincorporation of operational considerations into the decision-making procedure.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:23:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baek", "Soojung", ""], ["Reyes", "Kristofer G.", ""]]}, {"id": "2103.07788", "submitter": "Abhin Shah", "authors": "Abhin Shah, Kartik Ahuja, Karthikeyan Shanmugam, Dennis Wei, Kush\n  Varshney, Amit Dhurandhar", "title": "Treatment Effect Estimation using Invariant Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal individual treatment effect (ITE) from observational data is\na challenging problem whose difficulty is exacerbated by the presence of\ntreatment assignment bias. In this work, we propose a new way to estimate the\nITE using the domain generalization framework of invariant risk minimization\n(IRM). IRM uses data from multiple domains, learns predictors that do not\nexploit spurious domain-dependent factors, and generalizes better to unseen\ndomains. We propose an IRM-based ITE estimator aimed at tackling treatment\nassignment bias when there is little support overlap between the control group\nand the treatment group. We accomplish this by creating diversity: given a\nsingle dataset, we split the data into multiple domains artificially. These\ndiverse domains are then exploited by IRM to more effectively generalize\nregression-based models to data regions that lack support overlap. We show\ngains over classical regression approaches to ITE estimation in settings when\nsupport mismatch is more pronounced.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 20:42:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shah", "Abhin", ""], ["Ahuja", "Kartik", ""], ["Shanmugam", "Karthikeyan", ""], ["Wei", "Dennis", ""], ["Varshney", "Kush", ""], ["Dhurandhar", "Amit", ""]]}, {"id": "2103.07861", "submitter": "Lizhen Nie", "authors": "Lizhen Nie, Mao Ye, Qiang Liu, Dan Nicolae", "title": "VCNet and Functional Targeted Regularization For Learning Causal Effects\n  of Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the rising abundance of observational data with continuous\ntreatments, we investigate the problem of estimating the average dose-response\ncurve (ADRF). Available parametric methods are limited in their model space,\nand previous attempts in leveraging neural network to enhance model\nexpressiveness relied on partitioning continuous treatment into blocks and\nusing separate heads for each block; this however produces in practice\ndiscontinuous ADRFs. Therefore, the question of how to adapt the structure and\ntraining of neural network to estimate ADRFs remains open. This paper makes two\nimportant contributions. First, we propose a novel varying coefficient neural\nnetwork (VCNet) that improves model expressiveness while preserving continuity\nof the estimated ADRF. Second, to improve finite sample performance, we\ngeneralize targeted regularization to obtain a doubly robust estimator of the\nwhole ADRF curve.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 07:37:28 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nie", "Lizhen", ""], ["Ye", "Mao", ""], ["Liu", "Qiang", ""], ["Nicolae", "Dan", ""]]}, {"id": "2103.07948", "submitter": "Shengxi Li", "authors": "Shengxi Li, Danilo Mandic", "title": "Von Mises-Fisher Elliptical Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of modern probabilistic learning systems assumes symmetric\ndistributions, however, real-world data tend to obey skewed distributions and\nare thus not always adequately modelled through symmetric distributions. To\naddress this issue, elliptical distributions are increasingly used to\ngeneralise symmetric distributions, and further improvements to skewed\nelliptical distributions have recently attracted much attention. However,\nexisting approaches are either hard to estimate or have complicated and\nabstract representations. To this end, we propose to employ the\nvon-Mises-Fisher (vMF) distribution to obtain an explicit and simple\nprobability representation of the skewed elliptical distribution. This is shown\nnot only to allow us to deal with non-symmetric learning systems, but also to\nprovide a physically meaningful way of generalising skewed distributions. For\nrigour, our extension is proved to share important and desirable properties\nwith its symmetric counterpart. We also demonstrate that the proposed vMF\ndistribution is both easy to generate and stable to estimate, both\ntheoretically and through examples.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:14:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Li", "Shengxi", ""], ["Mandic", "Danilo", ""]]}, {"id": "2103.08026", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Sirui Bi, Guannan Zhang", "title": "A Scalable Gradient-Free Method for Bayesian Experimental Design with\n  Implicit Models", "comments": "This paper has been accepted by the 24th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian experimental design (BED) is to answer the question that how to\nchoose designs that maximize the information gathering. For implicit models,\nwhere the likelihood is intractable but sampling is possible, conventional BED\nmethods have difficulties in efficiently estimating the posterior distribution\nand maximizing the mutual information (MI) between data and parameters. Recent\nwork proposed the use of gradient ascent to maximize a lower bound on MI to\ndeal with these issues. However, the approach requires a sampling path to\ncompute the pathwise gradient of the MI lower bound with respect to the design\nvariables, and such a pathwise gradient is usually inaccessible for implicit\nmodels. In this paper, we propose a novel approach that leverages recent\nadvances in stochastic approximate gradient ascent incorporated with a smoothed\nvariational MI estimator for efficient and robust BED. Without the necessity of\npathwise gradients, our approach allows the design process to be achieved\nthrough a unified procedure with an approximate gradient for implicit models.\nSeveral experiments show that our approach outperforms baseline methods, and\nsignificantly improves the scalability of BED in high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 20:28:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Bi", "Sirui", ""], ["Zhang", "Guannan", ""]]}, {"id": "2103.08178", "submitter": "Babak Nouri-Moghaddam", "authors": "Jafar Abdollahi, Amir Jalili Irani, Babak Nouri-Moghaddam", "title": "Modeling and forecasting Spread of COVID-19 epidemic in Iran until Sep\n  22, 2021, based on deep learning", "comments": "9 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent global outbreak of covid-19 is affecting many countries around the\nworld. Due to the growing number of newly infected individuals and the\nhealth-care system bottlenecks, it will be useful to predict the upcoming\nnumber of patients. This study aims to efficiently forecast the is used to\nestimate new cases, number of deaths, and number of recovered patients in Iran\nfor 180 days, using the official dataset of the Iranian Ministry of Health and\nMedical Education and the impact of control measures on the spread of COVID-19.\nFour different types of forecasting techniques, time series, and machine\nlearning algorithms, are developed and the best performing method for the given\ncase study is determined. Under the time series, we consider the four\nalgorithms including Prophet, Long short-term memory, Autoregressive,\nAutoregressive Integrated Moving Average models. On comparing the different\ntechniques, we found that deep learning methods yield better results than time\nseries forecasting algorithms. More specifically, the least value of the error\nmeasures is observed in seasonal ANN and LSTM models. Our findings showed that\nif precautionary measures are taken seriously, the number of new cases and\ndeaths will decrease, and the number of deaths in September 2021 will reach\nzero.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:36:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abdollahi", "Jafar", ""], ["Irani", "Amir Jalili", ""], ["Nouri-Moghaddam", "Babak", ""]]}, {"id": "2103.08195", "submitter": "Shunsuke Horii", "authors": "Shunsuke Horii", "title": "Bayesian Model Averaging for Causality Estimation and its Approximation\n  based on Gaussian Scale Mixture Distributions", "comments": "Accepted to International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the estimation of the causal effect under linear Structural Causal Models\n(SCMs), it is common practice to first identify the causal structure, estimate\nthe probability distributions, and then calculate the causal effect. However,\nif the goal is to estimate the causal effect, it is not necessary to fix a\nsingle causal structure or probability distributions. In this paper, we first\nshow from a Bayesian perspective that it is Bayes optimal to weight (average)\nthe causal effects estimated under each model rather than estimating the causal\neffect under a fixed single model. This idea is also known as Bayesian model\naveraging. Although the Bayesian model averaging is optimal, as the number of\ncandidate models increases, the weighting calculations become computationally\nhard. We develop an approximation to the Bayes optimal estimator by using\nGaussian scale mixture distributions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:07:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Horii", "Shunsuke", ""]]}, {"id": "2103.08250", "submitter": "Feng Li", "authors": "Matthias Anderer and Feng Li", "title": "Forecasting reconciliation with a top-down alignment of independent\n  level forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. The overall forecasting performance is heavily\naffected by the forecasting accuracy of intermittent time series at bottom\nlevels. In this paper, we present a forecasting reconciliation approach that\ntreats the bottom level forecast as latent to ensure higher forecasting\naccuracy on the upper levels of the hierarchy. We employ a pure deep learning\nforecasting approach N-BEATS for continuous time series on top levels and a\nwidely used tree-based algorithm LightGBM for the bottom level intermittent\ntime series. The hierarchical forecasting with alignment approach is simple and\nstraightforward to implement in practice. It sheds light on an orthogonal\ndirection for forecasting reconciliation. When there is difficulty finding an\noptimal reconciliation, allowing suboptimal forecasts at a lower level could\nretain a high overall performance. The approach in this empirical study was\ndeveloped by the first author during the M5 Forecasting Accuracy competition\nranking second place. The approach is business orientated and could be\nbeneficial for business strategic planning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:00:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Anderer", "Matthias", ""], ["Li", "Feng", ""]]}, {"id": "2103.08277", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "Representation Theorem for Matrix Product States", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the universal representation capacity of the\nMatrix Product States (MPS) from the perspective of boolean functions and\ncontinuous functions. We show that MPS can accurately realize arbitrary boolean\nfunctions by providing a construction method of the corresponding MPS structure\nfor an arbitrarily given boolean gate. Moreover, we prove that the function\nspace of MPS with the scale-invariant sigmoidal activation is dense in the\nspace of continuous functions defined on a compact subspace of the\n$n$-dimensional real coordinate space $\\mathbb{R^{n}}$. We study the relation\nbetween MPS and neural networks and show that the MPS with a scale-invariant\nsigmoidal function is equivalent to a one-hidden-layer neural network equipped\nwith a kernel function. We construct the equivalent neural networks for several\nspecific MPS models and show that non-linear kernels such as the polynomial\nkernel which introduces the couplings between different components of the input\ninto the model appear naturally in the equivalent neural networks. At last, we\ndiscuss the realization of the Gaussian Process (GP) with infinitely wide MPS\nby studying their equivalent neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:06:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2103.08280", "submitter": "Yuze Han", "authors": "Yuze Han, Guangzeng Xie, Zhihua Zhang", "title": "Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results\n  and Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this paper includes two aspects. First, we study the\nlower bound complexity for the minimax optimization problem whose objective\nfunction is the average of $n$ individual smooth component functions. We\nconsider Proximal Incremental First-order (PIFO) algorithms which have access\nto gradient and proximal oracle for each individual component. We develop a\nnovel approach for constructing adversarial problems, which partitions the\ntridiagonal matrix of classical examples into $n$ groups. This construction is\nfriendly to the analysis of incremental gradient and proximal oracle. With this\napproach, we demonstrate the lower bounds of first-order algorithms for finding\nan $\\varepsilon$-suboptimal point and an $\\varepsilon$-stationary point in\ndifferent settings. Second, we also derive the lower bounds of minimization\noptimization with PIFO algorithms from our approach, which can cover the\nresults in \\citep{woodworth2016tight} and improve the results in\n\\citep{zhou2019lower}.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:20:31 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:35:24 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 16:12:08 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Han", "Yuze", ""], ["Xie", "Guangzeng", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2103.08377", "submitter": "Mark Kelly", "authors": "Mark Kelly, Gilles Bourque, Stephen Dooley", "title": "Toward Machine Learned Highly Reduce Kinetic Models For Methane/Air\n  Combustion", "comments": "Conference Paper: ASME Turbo Expo 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate low dimension chemical kinetic models for methane are an essential\ncomponent in the design of efficient gas turbine combustors. Kinetic models\ncoupled to computational fluid dynamics (CFD) provide quick and efficient ways\nto test the effect of operating conditions, fuel composition and combustor\ndesign compared to physical experiments. However, detailed chemical kinetic\nmodels are too computationally expensive for use in CFD. We propose a novel\ndata orientated three-step methodology to produce compact models that replicate\na target set of detailed model properties to a high fidelity. In the first\nstep, a reduced kinetic model is obtained by removing all non-essential species\nfrom the detailed model containing 118 species using path flux analysis (PFA).\nIt is then numerically optimised to replicate the detailed model's prediction\nin two rounds; First, to selected species (OH,H,CO and CH4) profiles in\nperfectly stirred reactor (PSR) simulations and then re-optimised to the\ndetailed model's prediction of the laminar flame speed. This is implemented by\na purposely developed Machine Learned Optimisation of Chemical Kinetics (MLOCK)\nalgorithm. The MLOCK algorithm systematically perturbs all three Arrhenius\nparameters for selected reactions and assesses the suitability of the new\nparameters through an objective error function which quantifies the error in\nthe compact model's calculation of the optimisation target. This strategy is\ndemonstrated through the production of a 19 species and a 15 species compact\nmodel for methane/air combustion. Both compact models are validated across a\nrange of 0D and 1D calculations across both lean and rich conditions and shows\ngood agreement to the parent detailed mechanism. The 15 species model is shown\nto outperform the current state-of-art models in both accuracy and range of\nconditions the model is valid over.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:29:08 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 15:37:45 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 15:32:09 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kelly", "Mark", ""], ["Bourque", "Gilles", ""], ["Dooley", "Stephen", ""]]}, {"id": "2103.08390", "submitter": "Vasilis Syrgkanis", "authors": "Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Miruna\n  Oprescu, Vasilis Syrgkanis", "title": "Estimating the Long-Term Effects of Novel Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy makers typically face the problem of wanting to estimate the long-term\neffects of novel treatments, while only having historical data of older\ntreatment options. We assume access to a long-term dataset where only past\ntreatments were administered and a short-term dataset where novel treatments\nhave been administered. We propose a surrogate based approach where we assume\nthat the long-term effect is channeled through a multitude of available\nshort-term proxies. Our work combines three major recent techniques in the\ncausal machine learning literature: surrogate indices, dynamic treatment effect\nestimation and double machine learning, in a unified pipeline. We show that our\nmethod is consistent and provides root-n asymptotically normal estimates under\na Markovian assumption on the data and the observational policy. We use a\ndata-set from a major corporation that includes customer investments over a\nthree year period to create a semi-synthetic data distribution where the major\nqualitative properties of the real dataset are preserved. We evaluate the\nperformance of our method and discuss practical challenges of deploying our\nformal methodology and how to address them.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:56:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Battocchi", "Keith", ""], ["Dillon", "Eleanor", ""], ["Hei", "Maggie", ""], ["Lewis", "Greg", ""], ["Oprescu", "Miruna", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2103.08450", "submitter": "Maochao Xu", "authors": "Mingyue Zhang Wu, Jinzhu Luo, Xing Fang, Maochao Xu, Peng Zhao", "title": "Modeling Multivariate Cyber Risks: Deep Learning Dating Extreme Value\n  Theory", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling cyber risks has been an important but challenging task in the domain\nof cyber security. It is mainly because of the high dimensionality and heavy\ntails of risk patterns. Those obstacles have hindered the development of\nstatistical modeling of the multivariate cyber risks. In this work, we propose\na novel approach for modeling the multivariate cyber risks which relies on the\ndeep learning and extreme value theory. The proposed model not only enjoys the\nhigh accurate point predictions via deep learning but also can provide the\nsatisfactory high quantile prediction via extreme value theory. The simulation\nstudy shows that the proposed model can model the multivariate cyber risks very\nwell and provide satisfactory prediction performances. The empirical evidence\nbased on real honeypot attack data also shows that the proposed model has very\nsatisfactory prediction performances.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:18:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wu", "Mingyue Zhang", ""], ["Luo", "Jinzhu", ""], ["Fang", "Xing", ""], ["Xu", "Maochao", ""], ["Zhao", "Peng", ""]]}, {"id": "2103.08509", "submitter": "Songting Shi", "authors": "Songting Shi", "title": "Visualizing Data Velocity using DSNE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new technique called \"DSNE\" which learns the velocity embeddings\nof low dimensional map points when given the high-dimensional data points with\nits velocities. The technique is a variation of Stochastic Neighbor Embedding,\nwhich uses the Euclidean distance on the unit sphere between the unit-length\nvelocity of the point and the unit-length direction from the point to its near\nneighbors to define similarities, and try to match the two kinds of\nsimilarities in the high dimension space and low dimension space to find the\nvelocity embeddings on the low dimension space. DSNE can help to visualize how\nthe data points move in the high dimension space by presenting the movements in\ntwo or three dimensions space. It is helpful for understanding the mechanism of\ncell differentiation and embryo development.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:29:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shi", "Songting", ""]]}, {"id": "2103.08533", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Andreas Themelis, Panagiotis Patrinos", "title": "Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach", "comments": "29th Eur. Signal Process. Conf. (EUSIPCO 2021), accepted. 5 pages, 2\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale optimization, the presence of nonsmooth and nonconvex terms in\na given problem typically makes it hard to solve. A popular approach to address\nnonsmooth terms in convex optimization is to approximate them with their\nrespective Moreau envelopes. In this work, we study the use of Lasry-Lions\ndouble envelopes to approximate nonsmooth terms that are also not convex. These\nenvelopes are an extension of the Moreau ones but exhibit an additional\nsmoothness property that makes them amenable to fast optimization algorithms.\nLasry-Lions envelopes can also be seen as an \"intermediate\" between a given\nfunction and its convex envelope, and we make use of this property to develop a\nmethod that builds a sequence of approximate subproblems that are easier to\nsolve than the original problem. We discuss convergence properties of this\nmethod when used to address composite minimization problems; additionally,\nbased on a number of experiments, we discuss settings where it may be more\nuseful than classical alternatives in two domains: signal decoding and spectral\nunmixing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:55:11 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 09:21:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Themelis", "Andreas", ""], ["Patrinos", "Panagiotis", ""]]}, {"id": "2103.08594", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Sirui Bi, Guannan Zhang", "title": "A Hybrid Gradient Method to Designing Bayesian Experiments for Implicit\n  Models", "comments": "This short paper has been accepted by NeurIPS 2020 Workshop on\n  Machine Learning and the Physical Sciences. arXiv admin note: substantial\n  text overlap with arXiv:2103.08026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian experimental design (BED) aims at designing an experiment to\nmaximize the information gathering from the collected data. The optimal design\nis usually achieved by maximizing the mutual information (MI) between the data\nand the model parameters. When the analytical expression of the MI is\nunavailable, e.g., having implicit models with intractable data distributions,\na neural network-based lower bound of the MI was recently proposed and a\ngradient ascent method was used to maximize the lower bound. However, the\napproach in Kleinegesse et al., 2020 requires a pathwise sampling path to\ncompute the gradient of the MI lower bound with respect to the design\nvariables, and such a pathwise sampling path is usually inaccessible for\nimplicit models. In this work, we propose a hybrid gradient approach that\nleverages recent advances in variational MI estimator and evolution strategies\n(ES) combined with black-box stochastic gradient ascent (SGA) to maximize the\nMI lower bound. This allows the design process to be achieved through a unified\nscalable procedure for implicit models without sampling path gradients. Several\nexperiments demonstrate that our approach significantly improves the\nscalability of BED for implicit models in high-dimensional design space.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:10:03 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Bi", "Sirui", ""], ["Zhang", "Guannan", ""]]}, {"id": "2103.08659", "submitter": "Aleksandr Beknazaryan", "authors": "Aleksandr Beknazaryan", "title": "Function approximation by deep neural networks with parameters $\\{0,\\pm\n  \\frac{1}{2}, \\pm 1, 2\\}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper it is shown that $C_\\beta$-smooth functions can be approximated\nby deep neural networks with ReLU activation function and with parameters\n$\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. The $l_0$ and $l_1$ parameter norms of\nconsidered networks are thus equivalent. The depth, width and the number of\nactive parameters of the constructed networks have, up to a logarithmic factor,\nthe same dependence on the approximation error as the networks with parameters\nin $[-1,1]$. In particular, this means that the nonparametric regression\nestimation with the constructed networks attains the same convergence rate as\nwith sparse networks with parameters in $[-1,1]$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:10:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:36:33 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 02:08:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Beknazaryan", "Aleksandr", ""]]}, {"id": "2103.08721", "submitter": "Jinshuo Dong", "authors": "Jinshuo Dong, Weijie J. Su, Linjun Zhang", "title": "A Central Limit Theorem for Differentially Private Query Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perhaps the single most important use case for differential privacy is to\nprivately answer numerical queries, which is usually achieved by adding noise\nto the answer vector. The central question, therefore, is to understand which\nnoise distribution optimizes the privacy-accuracy trade-off, especially when\nthe dimension of the answer vector is high. Accordingly, extensive literature\nhas been dedicated to the question and the upper and lower bounds have been\nmatched up to constant factors [BUV18, SU17]. In this paper, we take a novel\napproach to address this important optimality question. We first demonstrate an\nintriguing central limit theorem phenomenon in the high-dimensional regime.\nMore precisely, we prove that a mechanism is approximately Gaussian\nDifferentially Private [DRS21] if the added noise satisfies certain conditions.\nIn particular, densities proportional to $\\mathrm{e}^{-\\|x\\|_p^\\alpha}$, where\n$\\|x\\|_p$ is the standard $\\ell_p$-norm, satisfies the conditions. Taking this\nperspective, we make use of the Cramer--Rao inequality and show an \"uncertainty\nprinciple\"-style result: the product of the privacy parameter and the\n$\\ell_2$-loss of the mechanism is lower bounded by the dimension. Furthermore,\nthe Gaussian mechanism achieves the constant-sharp optimal privacy-accuracy\ntrade-off among all such noises. Our findings are corroborated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:06:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dong", "Jinshuo", ""], ["Su", "Weijie J.", ""], ["Zhang", "Linjun", ""]]}, {"id": "2103.08761", "submitter": "Asim Dey", "authors": "Asim K. Dey, Vyacheslav Lyubchich, and Yulia R. Gel", "title": "Modeling Weather-induced Home Insurance Risks with Support Vector\n  Machine Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Insurance industry is one of the most vulnerable sectors to climate change.\nAssessment of future number of claims and incurred losses is critical for\ndisaster preparedness and risk management. In this project, we study the effect\nof precipitation on a joint dynamics of weather-induced home insurance claims\nand losses. We discuss utility and limitations of such machine learning\nprocedures as Support Vector Machines and Artificial Neural Networks, in\nforecasting future claim dynamics and evaluating associated uncertainties. We\nillustrate our approach by application to attribution analysis and forecasting\nof weather-induced home insurance claims in a middle-sized city in the Canadian\nPrairies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:13:32 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dey", "Asim K.", ""], ["Lyubchich", "Vyacheslav", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2103.08801", "submitter": "Kota Dohi", "authors": "Kota Dohi, Takashi Endo, Harsh Purohit, Ryo Tanabe, Yohei Kawaguchi", "title": "Flow-based Self-supervised Density Estimation for Anomalous Sound\n  Detection", "comments": "5 pages, 1 figure, accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. Exact likelihood estimation using Normalizing\nFlows is a promising technique for unsupervised anomaly detection, but it can\nfail at out-of-distribution detection since the likelihood is affected by the\nsmoothness of the data. To improve the detection performance, we train the\nmodel to assign higher likelihood to target machine sounds and lower likelihood\nto sounds from other machines of the same machine type. We demonstrate that\nthis enables the model to incorporate a self-supervised classification-based\napproach. Experiments conducted using the DCASE 2020 Challenge Task2 dataset\nshowed that the proposed method improves the AUC by 4.6% on average when using\nMasked Autoregressive Flow (MAF) and by 5.8% when using Glow, which is a\nsignificant improvement over the previous method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 01:52:03 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dohi", "Kota", ""], ["Endo", "Takashi", ""], ["Purohit", "Harsh", ""], ["Tanabe", "Ryo", ""], ["Kawaguchi", "Yohei", ""]]}, {"id": "2103.08811", "submitter": "Shoaib Bin Masud", "authors": "Shoaib Bin Masud, Boyang Lyu, Shuchin Aeron", "title": "Soft and subspace robust multivariate rank tests based on entropy\n  regularized optimal transport", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we extend the recently proposed multivariate rank energy\ndistance, based on the theory of optimal transport, for statistical testing of\ndistributional similarity, to soft rank energy distance. Being differentiable,\nthis in turn allows us to extend the rank energy to a subspace robust rank\nenergy distance, dubbed Projected soft-Rank Energy distance, which can be\ncomputed via optimization over the Stiefel manifold. We show via experiments\nthat using projected soft rank energy one can trade-off the detection power vs\nthe false alarm via projections onto an appropriately selected low dimensional\nsubspace. We also show the utility of the proposed tests on unsupervised change\npoint detection in multivariate time series data. All codes are publicly\navailable at the link provided in the experiment section.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 02:48:19 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 22:47:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Masud", "Shoaib Bin", ""], ["Lyu", "Boyang", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2103.08889", "submitter": "Arun Sharma Mr.", "authors": "Arun K. Sharma, Nishchal K. Verma", "title": "Quick Learning Mechanism with Cross-Domain Adaptation for Intelligent\n  Fault Diagnosis", "comments": "9 pages, 6 figures, transaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a quick learning mechanism for intelligent fault\ndiagnosis of rotating machines operating under changeable working conditions.\nSince real case machines in industries run under different operating\nconditions, the deep learning model trained for a laboratory case machine fails\nto perform well for the fault diagnosis using recorded data from real case\nmachines. It poses the need of training a new diagnostic model for the fault\ndiagnosis of the real case machine under every new working condition.\nTherefore, there is a need for a mechanism that can quickly transform the\nexisting diagnostic model for machines operating under different conditions. we\npropose a quick learning method with Net2Net transformation followed by a\nfine-tuning method to cancel/minimize the maximum mean discrepancy of the new\ndata to the previous one. This transformation enables us to create a new\nnetwork with any architecture almost ready to be used for the new dataset. The\neffectiveness of the proposed fault diagnosis method has been demonstrated on\nthe CWRU dataset, IMS bearing dataset, and Paderborn university dataset. We\nhave shown that the diagnostic model trained for CWRU data at zero load can be\nused to quickly train another diagnostic model for the CWRU data at different\nloads and also for the IMS dataset. Using the dataset provided by Paderborn\nuniversity, it has been validated that the diagnostic model trained on\nartificially damaged fault dataset can be used for quickly training another\nmodel for real damage dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:24:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sharma", "Arun K.", ""], ["Verma", "Nishchal K.", ""]]}, {"id": "2103.08902", "submitter": "Nastaran Okati", "authors": "Nastaran Okati, Abir De, Manuel Gomez-Rodriguez", "title": "Differentiable Learning Under Triage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple lines of evidence suggest that predictive models may benefit from\nalgorithmic triage. Under algorithmic triage, a predictive model does not\npredict all instances but instead defers some of them to human experts.\nHowever, the interplay between the prediction accuracy of the model and the\nhuman experts under algorithmic triage is not well understood. In this work, we\nstart by formally characterizing under which circumstances a predictive model\nmay benefit from algorithmic triage. In doing so, we also demonstrate that\nmodels trained for full automation may be suboptimal under triage. Then, given\nany model and desired level of triage, we show that the optimal triage policy\nis a deterministic threshold rule in which triage decisions are derived\ndeterministically by thresholding the difference between the model and human\nerrors on a per-instance level. Building upon these results, we introduce a\npractical gradient-based algorithm that is guaranteed to find a sequence of\ntriage policies and predictive models of increasing performance. Experiments on\na wide variety of supervised learning tasks using synthetic and real data from\ntwo important applications -- content moderation and scientific discovery --\nillustrate our theoretical results and show that the models and triage policies\nprovided by our gradient-based algorithm outperform those provided by several\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:07:31 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 11:21:58 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Okati", "Nastaran", ""], ["De", "Abir", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "2103.09177", "submitter": "Alexander Rakhlin", "authors": "Peter L. Bartlett and Andrea Montanari and Alexander Rakhlin", "title": "Deep learning: a statistical viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The remarkable practical success of deep learning has revealed some major\nsurprises from a theoretical perspective. In particular, simple gradient\nmethods easily find near-optimal solutions to non-convex optimization problems,\nand despite giving a near-perfect fit to training data without any explicit\neffort to control model complexity, these methods exhibit excellent predictive\naccuracy. We conjecture that specific principles underlie these phenomena: that\noverparametrization allows gradient methods to find interpolating solutions,\nthat these methods implicitly impose regularization, and that\noverparametrization leads to benign overfitting. We survey recent theoretical\nprogress that provides examples illustrating these principles in simpler\nsettings. We first review classical uniform convergence results and why they\nfall short of explaining aspects of the behavior of deep learning methods. We\ngive examples of implicit regularization in simple settings, where gradient\nmethods lead to minimal norm functions that perfectly fit the training data.\nThen we review prediction methods that exhibit benign overfitting, focusing on\nregression problems with quadratic loss. For these methods, we can decompose\nthe prediction rule into a simple component that is useful for prediction and a\nspiky component that is useful for overfitting but, in a favorable setting,\ndoes not harm prediction accuracy. We focus specifically on the linear regime\nfor neural networks, where the network can be approximated by a linear model.\nIn this regime, we demonstrate the success of gradient flow, and we consider\nbenign overfitting with two-layer networks, giving an exact asymptotic analysis\nthat precisely demonstrates the impact of overparametrization. We conclude by\nhighlighting the key challenges that arise in extending these insights to\nrealistic deep learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:26:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Montanari", "Andrea", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "2103.09267", "submitter": "Tudor Manole", "authors": "Tudor Manole, Aaditya Ramdas", "title": "Sequential Estimation of Convex Divergences using Reverse Submartingales\n  and Exchangeable Filtrations", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified technique for sequential estimation of convex\ndivergences between distributions, including integral probability metrics like\nthe kernel maximum mean discrepancy, $\\varphi$-divergences like the\nKullback-Leibler divergence, and optimal transport costs, such as powers of\nWasserstein distances. The technical underpinnings of our approach lie in the\nobservation that empirical convex divergences are (partially ordered) reverse\nsubmartingales with respect to the exchangeable filtration, coupled with\nmaximal inequalities for such processes. These techniques appear to be powerful\nadditions to the existing literature on both confidence sequences and convex\ndivergences. We construct an offline-to-sequential device that converts a wide\narray of existing offline concentration inequalities into time-uniform\nconfidence sequences that can be continuously monitored, providing valid\ninference at arbitrary stopping times. The resulting sequential bounds pay only\nan iterated logarithmic price over the corresponding fixed-time bounds,\nretaining the same dependence on problem parameters (like dimension or alphabet\nsize if applicable).\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:22:14 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Manole", "Tudor", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.09310", "submitter": "Jake Clarkson", "authors": "Jake Clarkson, Kyle Y. Lin, Kevin D. Glazebrook", "title": "A Classical Search Game in Discrete Locations", "comments": "55 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Consider a two-person zero-sum search game between a hider and a searcher.\nThe hider hides among $n$ discrete locations, and the searcher successively\nvisits individual locations until finding the hider. Known to both players, a\nsearch at location $i$ takes $t_i$ time units and detects the hider -- if\nhidden there -- independently with probability $q_i$, for $i=1,\\ldots,n$. The\nhider aims to maximize the expected time until detection, while the searcher\naims to minimize it. We prove the existence of an optimal strategy for each\nplayer. In particular, the hider's optimal mixed strategy hides in each\nlocation with a nonzero probability, and the searcher's optimal mixed strategy\ncan be constructed with up to $n$ simple search sequences. We develop an\nalgorithm to compute an optimal strategy for each player, and compare the\noptimal hiding strategy with the simple hiding strategy which gives the\nsearcher no location preference at the beginning of the search.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:16:24 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Clarkson", "Jake", ""], ["Lin", "Kyle Y.", ""], ["Glazebrook", "Kevin D.", ""]]}, {"id": "2103.09316", "submitter": "Olanrewaju Akande", "authors": "Zhenhua Wang, Olanrewaju Akande, Jason Poulos and Fan Li", "title": "Are deep learning models superior for missing data imputation in large\n  surveys? Evidence from an empirical comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is the state-of-the-art approach for dealing with\nmissing data arising from non-response in sample surveys. Multiple imputation\nby chained equations (MICE) is the most widely used MI method, but it lacks\ntheoretical foundation and is computationally intensive. Recently, MI methods\nbased on deep learning models have been developed with encouraging results in\nsmall studies. However, there has been limited research on systematically\nevaluating their performance in realistic settings comparing to MICE,\nparticularly in large-scale surveys. This paper provides a general framework\nfor using simulations based on real survey data and several performance metrics\nto compare MI methods. We conduct extensive simulation studies based on the\nAmerican Community Survey data to compare repeated sampling properties of four\nmachine learning based MI methods: MICE with classification trees, MICE with\nrandom forests, generative adversarial imputation network, and multiple\nimputation using denoising autoencoders. We find the deep learning based MI\nmethods dominate MICE in terms of computational time; however, MICE with\nclassification trees consistently outperforms the deep learning MI methods in\nterms of bias, mean squared error, and coverage under a range of realistic\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:24:04 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Zhenhua", ""], ["Akande", "Olanrewaju", ""], ["Poulos", "Jason", ""], ["Li", "Fan", ""]]}, {"id": "2103.09329", "submitter": "Bingling Wang", "authors": "Bingling Wang, Yinxing Li, Wolfgang Karl H\\\"ardle", "title": "K-expectiles clustering", "comments": "All calculation can be redone via https://github.com/QuantLet/KEC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  $K$-means clustering is one of the most widely-used partitioning algorithm in\ncluster analysis due to its simplicity and computational efficiency. However,\n$K$-means does not provide an appropriate clustering result when applying to\ndata with non-spherically shaped clusters. We propose a novel partitioning\nclustering algorithm based on expectiles. The cluster centers are defined as\nmultivariate expectiles and clusters are searched via a greedy algorithm by\nminimizing the within cluster '$\\tau$ -variance'. We suggest two schemes: fixed\n$\\tau$ clustering, and adaptive $\\tau$ clustering. Validated by simulation\nresults, this method beats both $K$-means and spectral clustering on data with\nasymmetric shaped clusters, or clusters with a complicated structure, including\nasymmetric normal, beta, skewed $t$ and $F$ distributed clusters. Applications\nof adaptive $\\tau$ clustering on crypto-currency (CC) market data are provided.\nOne finds that the expectiles clusters of CC markets show the phenomena of an\ninstitutional investors dominated market. The second application is on image\nsegmentation. compared to other center based clustering methods, the adaptive\n$\\tau$ cluster centers of pixel data can better capture and describe the\nfeatures of an image. The fixed $\\tau$ clustering brings more flexibility on\nsegmentation with a decent accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:14:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Bingling", ""], ["Li", "Yinxing", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2103.09383", "submitter": "Dana Yang", "authors": "Jian Ding, Yihong Wu, Jiaming Xu, Dana Yang", "title": "The planted matching problem: Sharp threshold and infinite-order phase\n  transition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.CO math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reconstructing a perfect matching $M^*$ hidden in a\nrandomly weighted $n\\times n$ bipartite graph. The edge set includes every node\npair in $M^*$ and each of the $n(n-1)$ node pairs not in $M^*$ independently\nwith probability $d/n$. The weight of each edge $e$ is independently drawn from\nthe distribution $\\mathcal{P}$ if $e \\in M^*$ and from $\\mathcal{Q}$ if $e\n\\notin M^*$. We show that if $\\sqrt{d} B(\\mathcal{P},\\mathcal{Q}) \\le 1$, where\n$B(\\mathcal{P},\\mathcal{Q})$ stands for the Bhattacharyya coefficient, the\nreconstruction error (average fraction of misclassified edges) of the maximum\nlikelihood estimator of $M^*$ converges to $0$ as $n\\to \\infty$. Conversely, if\n$\\sqrt{d} B(\\mathcal{P},\\mathcal{Q}) \\ge 1+\\epsilon$ for an arbitrarily small\nconstant $\\epsilon>0$, the reconstruction error for any estimator is shown to\nbe bounded away from $0$ under both the sparse and dense model, resolving the\nconjecture in [Moharrami et al. 2019, Semerjian et al. 2020]. Furthermore, in\nthe special case of complete exponentially weighted graph with $d=n$,\n$\\mathcal{P}=\\exp(\\lambda)$, and $\\mathcal{Q}=\\exp(1/n)$, for which the sharp\nthreshold simplifies to $\\lambda=4$, we prove that when $\\lambda \\le\n4-\\epsilon$, the optimal reconstruction error is $\\exp\\left( -\n\\Theta(1/\\sqrt{\\epsilon}) \\right)$, confirming the conjectured infinite-order\nphase transition in [Semerjian et al. 2020].\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:59:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ding", "Jian", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""], ["Yang", "Dana", ""]]}, {"id": "2103.09424", "submitter": "Raj Kumar Maity", "authors": "Avishek Ghosh, Raj Kumar Maity, Arya Mazumdar, Kannan Ramchandran", "title": "Escaping Saddle Points in Distributed Newton's Method with Communication\n  efficiency and Byzantine Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of optimizing a non-convex loss function (with saddle\npoints) in a distributed framework in the presence of Byzantine machines. We\nconsider a standard distributed setting with one central machine (parameter\nserver) communicating with many worker machines. Our proposed algorithm is a\nvariant of the celebrated cubic-regularized Newton method of Nesterov and\nPolyak \\cite{nest}, which avoids saddle points efficiently and converges to\nlocal minima. Furthermore, our algorithm resists the presence of Byzantine\nmachines, which may create \\emph{fake local minima} near the saddle points of\nthe loss function, also known as saddle-point attack. We robustify the\ncubic-regularized Newton algorithm such that it avoids the saddle points and\nthe fake local minimas efficiently. Furthermore, being a second order\nalgorithm, the iteration complexity is much lower than its first order\ncounterparts, and thus our algorithm communicates little with the parameter\nserver. We obtain theoretical guarantees for our proposed scheme under several\nsettings including approximate (sub-sampled) gradients and Hessians. Moreover,\nwe validate our theoretical findings with experiments using standard datasets\nand several types of Byzantine attacks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:53:58 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ghosh", "Avishek", ""], ["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2103.09527", "submitter": "Cheng Lu", "authors": "Cheng Lu, Jianfei Chen, Chongxuan Li, Qiuhao Wang, Jun Zhu", "title": "Implicit Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows define a probability distribution by an explicit invertible\ntransformation $\\boldsymbol{\\mathbf{z}}=f(\\boldsymbol{\\mathbf{x}})$. In this\nwork, we present implicit normalizing flows (ImpFlows), which generalize\nnormalizing flows by allowing the mapping to be implicitly defined by the roots\nof an equation $F(\\boldsymbol{\\mathbf{z}}, \\boldsymbol{\\mathbf{x}})=\n\\boldsymbol{\\mathbf{0}}$. ImpFlows build on residual flows (ResFlows) with a\nproper balance between expressiveness and tractability. Through theoretical\nanalysis, we show that the function space of ImpFlow is strictly richer than\nthat of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks,\nthere exists some function that ResFlow has a non-negligible approximation\nerror. However, the function is exactly representable by a single-block\nImpFlow. We propose a scalable algorithm to train and draw samples from\nImpFlows. Empirically, we evaluate ImpFlow on several classification and\ndensity modeling tasks, and ImpFlow outperforms ResFlow with a comparable\namount of parameters on all the benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:24:04 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Lu", "Cheng", ""], ["Chen", "Jianfei", ""], ["Li", "Chongxuan", ""], ["Wang", "Qiuhao", ""], ["Zhu", "Jun", ""]]}, {"id": "2103.09577", "submitter": "Justyna P. Zwolak", "authors": "Brian J. Weber, Sandesh S. Kalantre, Thomas McJunkin, Jacob M. Taylor,\n  Justyna P. Zwolak", "title": "Theoretical bounds on data requirements for the ray-based classification", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of classifying high-dimensional shapes in real-world data grows\nin complexity as the dimension of the space increases. For the case of\nidentifying convex shapes of different geometries, a new classification\nframework has recently been proposed in which the intersections of a set of\none-dimensional representations, called rays, with the boundaries of the shape\nare used to identify the specific geometry. This ray-based classification (RBC)\nhas been empirically verified using a synthetic dataset of two- and\nthree-dimensional shapes [1] and, more recently, has also been validated\nexperimentally [2]. Here, we establish a bound on the number of rays necessary\nfor shape classification, defined by key angular metrics, for arbitrary convex\nshapes. For two dimensions, we derive a lower bound on the number of rays in\nterms of the shape's length, diameter, and exterior angles. For convex\npolytopes in R^N, we generalize this result to a similar bound given as a\nfunction of the dihedral angle and the geometrical parameters of polygonal\nfaces. This result enables a different approach for estimating high-dimensional\nshapes using substantially fewer data elements than volumetric or surface-based\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:38:45 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Weber", "Brian J.", ""], ["Kalantre", "Sandesh S.", ""], ["McJunkin", "Thomas", ""], ["Taylor", "Jacob M.", ""], ["Zwolak", "Justyna P.", ""]]}, {"id": "2103.09603", "submitter": "Philipp Bach", "authors": "Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler", "title": "DoubleML -- An Object-Oriented Implementation of Double Machine Learning\n  in R", "comments": "40 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package DoubleML implements the double/debiased machine learning\nframework of Chernozhukov et al. (2018). It provides functionalities to\nestimate parameters in causal models based on machine learning methods. The\ndouble machine learning framework consist of three key ingredients: Neyman\northogonality, high-quality machine learning estimation and sample splitting.\nEstimation of nuisance components can be performed by various state-of-the-art\nmachine learning methods that are available in the mlr3 ecosystem. DoubleML\nmakes it possible to perform inference in a variety of causal models, including\npartially linear and interactive regression models and their extensions to\ninstrumental variable estimation. The object-oriented implementation of\nDoubleML enables a high flexibility for the model specification and makes it\neasily extendable. This paper serves as an introduction to the double machine\nlearning framework and the R package DoubleML. In reproducible code examples\nwith simulated and real data sets, we demonstrate how DoubleML users can\nperform valid inference based on machine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:42:41 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 14:11:26 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bach", "Philipp", ""], ["Chernozhukov", "Victor", ""], ["Kurz", "Malte S.", ""], ["Spindler", "Martin", ""]]}, {"id": "2103.09763", "submitter": "Zhimei Ren", "authors": "Emmanuel J. Cand\\`es, Lihua Lei and Zhimei Ren", "title": "Conformalized Survival Analysis", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing survival analysis techniques heavily rely on strong modelling\nassumptions and are, therefore, prone to model misspecification errors. In this\npaper, we develop an inferential method based on ideas from conformal\nprediction, which can wrap around any survival prediction algorithm to produce\ncalibrated, covariate-dependent lower predictive bounds on survival times. In\nthe Type I right-censoring setting, when the censoring times are completely\nexogenous, the lower predictive bounds have guaranteed coverage in finite\nsamples without any assumptions other than that of operating on independent and\nidentically distributed data points. Under a more general conditionally\nindependent censoring assumption, the bounds satisfy a doubly robust property\nwhich states the following: marginal coverage is approximately guaranteed if\neither the censoring mechanism or the conditional survival function is\nestimated well. Further, we demonstrate that the lower predictive bounds remain\nvalid and informative for other types of censoring. The validity and efficiency\nof our procedure are demonstrated on synthetic data and real COVID-19 data from\nthe UK Biobank.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:32:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cand\u00e8s", "Emmanuel J.", ""], ["Lei", "Lihua", ""], ["Ren", "Zhimei", ""]]}, {"id": "2103.09847", "submitter": "Lin Chen", "authors": "Lin Chen, Bruno Scherrer, Peter L. Bartlett", "title": "Infinite-Horizon Offline Reinforcement Learning with Linear Function\n  Approximation: Curse of Dimensionality and Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the sample complexity of policy evaluation in\ninfinite-horizon offline reinforcement learning (also known as the off-policy\nevaluation problem) with linear function approximation. We identify a hard\nregime $d\\gamma^{2}>1$, where $d$ is the dimension of the feature vector and\n$\\gamma$ is the discount rate. In this regime, for any $q\\in[\\gamma^{2},1]$, we\ncan construct a hard instance such that the smallest eigenvalue of its feature\ncovariance matrix is $q/d$ and it requires\n$\\Omega\\left(\\frac{d}{\\gamma^{2}\\left(q-\\gamma^{2}\\right)\\varepsilon^{2}}\\exp\\left(\\Theta\\left(d\\gamma^{2}\\right)\\right)\\right)$\nsamples to approximate the value function up to an additive error\n$\\varepsilon$. Note that the lower bound of the sample complexity is\nexponential in $d$. If $q=\\gamma^{2}$, even infinite data cannot suffice. Under\nthe low distribution shift assumption, we show that there is an algorithm that\nneeds at most $O\\left(\\max\\left\\{ \\frac{\\left\\Vert \\theta^{\\pi}\\right\\Vert\n_{2}^{4}}{\\varepsilon^{4}}\\log\\frac{d}{\\delta},\\frac{1}{\\varepsilon^{2}}\\left(d+\\log\\frac{1}{\\delta}\\right)\\right\\}\n\\right)$ samples ($\\theta^{\\pi}$ is the parameter of the policy in linear\nfunction approximation) and guarantees approximation to the value function up\nto an additive error of $\\varepsilon$ with probability at least $1-\\delta$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:18:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chen", "Lin", ""], ["Scherrer", "Bruno", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "2103.09947", "submitter": "Yaodong Yu", "authors": "Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, Yi Ma", "title": "Understanding Generalization in Adversarial Training via the\n  Bias-Variance Decomposition", "comments": "V2 adds new results and improves organization and presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarially trained models exhibit a large generalization gap: they can\ninterpolate the training set even for large perturbation radii, but at the cost\nof large test error on clean samples. To investigate this gap, we decompose the\ntest risk into its bias and variance components and study their behavior as a\nfunction of adversarial training perturbation radii ($\\varepsilon$). We find\nthat the bias increases monotonically with $\\varepsilon$ and is the dominant\nterm in the risk. Meanwhile, the variance is unimodal as a function of\n$\\varepsilon$, peaking near the interpolation threshold for the training set.\nThis characteristic behavior occurs robustly across different datasets and also\nfor other robust training procedures such as randomized smoothing. It thus\nprovides a test for proposed explanations of the generalization gap. We find\nthat some existing explanations fail this test--for instance, by predicting a\nmonotonically increasing variance curve. This underscores the power of\nbias-variance decompositions in modern settings-by providing two measurements\ninstead of one, they can rule out more explanations than test accuracy alone.\nWe also show that bias and variance can provide useful guidance for scalably\nreducing the generalization gap, highlighting pre-training and unlabeled data\nas promising routes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:30:00 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 02:58:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yu", "Yaodong", ""], ["Yang", "Zitong", ""], ["Dobriban", "Edgar", ""], ["Steinhardt", "Jacob", ""], ["Ma", "Yi", ""]]}, {"id": "2103.09982", "submitter": "Peyman Tavallali", "authors": "Peyman Tavallali, Hamed Hamze Bajgiran, Danial J. Esaid, Houman Owhadi", "title": "Decision Theoretic Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design and testing of supervised machine learning models combine two\nfundamental distributions: (1) the training data distribution (2) the testing\ndata distribution. Although these two distributions are identical and\nidentifiable when the data set is infinite; they are imperfectly known (and\npossibly distinct) when the data is finite (and possibly corrupted) and this\nuncertainty must be taken into account for robust Uncertainty Quantification\n(UQ). We present a general decision-theoretic bootstrapping solution to this\nproblem: (1) partition the available data into a training subset and a UQ\nsubset (2) take $m$ subsampled subsets of the training set and train $m$ models\n(3) partition the UQ set into $n$ sorted subsets and take a random fraction of\nthem to define $n$ corresponding empirical distributions $\\mu_{j}$ (4) consider\nthe adversarial game where Player I selects a model $i\\in\\left\\{\n1,\\ldots,m\\right\\} $, Player II selects the UQ distribution $\\mu_{j}$ and\nPlayer I receives a loss defined by evaluating the model $i$ against data\npoints sampled from $\\mu_{j}$ (5) identify optimal mixed strategies\n(probability distributions over models and UQ distributions) for both players.\nThese randomized optimal mixed strategies provide optimal model mixtures and UQ\nestimates given the adversarial uncertainty of the training and testing\ndistributions represented by the game. The proposed approach provides (1) some\ndegree of robustness to distributional shift in both the distribution of\ntraining data and that of the testing data (2) conditional probability\ndistributions on the output space forming aleatory representations of the\nuncertainty on the output as a function of the input variable.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:00:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tavallali", "Peyman", ""], ["Bajgiran", "Hamed Hamze", ""], ["Esaid", "Danial J.", ""], ["Owhadi", "Houman", ""]]}, {"id": "2103.09983", "submitter": "Jinwen Qiu", "authors": "Agus Sudjianto, Jinwen Qiu, Miaoqi Li and Jie Chen", "title": "Linear Iterative Feature Embedding: An Ensemble Framework for\n  Interpretable Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new ensemble framework for interpretable model called Linear Iterative\nFeature Embedding (LIFE) has been developed to achieve high prediction\naccuracy, easy interpretation and efficient computation simultaneously. The\nLIFE algorithm is able to fit a wide single-hidden-layer neural network (NN)\naccurately with three steps: defining the subsets of a dataset by the linear\nprojections of neural nodes, creating the features from multiple narrow\nsingle-hidden-layer NNs trained on the different subsets of the data, combining\nthe features with a linear model. The theoretical rationale behind LIFE is also\nprovided by the connection to the loss ambiguity decomposition of stack\nensemble methods. Both simulation and empirical experiments confirm that LIFE\nconsistently outperforms directly trained single-hidden-layer NNs and also\noutperforms many other benchmark models, including multi-layers Feed Forward\nNeural Network (FFNN), Xgboost, and Random Forest (RF) in many experiments. As\na wide single-hidden-layer NN, LIFE is intrinsically interpretable. Meanwhile,\nboth variable importance and global main and interaction effects can be easily\ncreated and visualized. In addition, the parallel nature of the base learner\nbuilding makes LIFE computationally efficient by leveraging parallel computing.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:01:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sudjianto", "Agus", ""], ["Qiu", "Jinwen", ""], ["Li", "Miaoqi", ""], ["Chen", "Jie", ""]]}, {"id": "2103.10026", "submitter": "Yuan Yang", "authors": "Yuan Yang and Jie Ding", "title": "Learning Time Series from Scale Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequentially obtained dataset usually exhibits different behavior at\ndifferent data resolutions/scales. Instead of inferring from data at each scale\nindividually, it is often more informative to interpret the data as an ensemble\nof time series from different scales. This naturally motivated us to propose a\nnew concept referred to as the scale-based inference. The basic idea is that\nmore accurate prediction can be made by exploiting scale information of a time\nseries. We first propose a nonparametric predictor based on $k$-nearest\nneighbors with an optimally chosen $k$ for a single time series. Based on that,\nwe focus on a specific but important type of scale information, the\nresolution/sampling rate of time series data. We then propose an algorithm to\nsequentially predict time series using past data at various resolutions. We\nprove that asymptotically the algorithm produces the mean prediction error that\nis no larger than the best possible algorithm at any single resolution, under\nsome optimally chosen parameters. Finally, we establish the general\nformulations for scale inference, and provide further motivating examples.\nExperiments on both synthetic and real data illustrate the potential\napplicability of our approaches to a wide range of time series models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:33:18 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yang", "Yuan", ""], ["Ding", "Jie", ""]]}, {"id": "2103.10027", "submitter": "Wing-Kin Ma", "authors": "Ruiyuan Wu, Wing-Kin Ma, Yuening Li, Anthony Man-Cho So, and Nicholas\n  D. Sidiropoulos", "title": "Probabilistic Simplex Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents PRISM, a probabilistic simplex component analysis\napproach to identifying the vertices of a data-circumscribing simplex from\ndata. The problem has a rich variety of applications, the most notable being\nhyperspectral unmixing in remote sensing and non-negative matrix factorization\nin machine learning. PRISM uses a simple probabilistic model, namely, uniform\nsimplex data distribution and additive Gaussian noise, and it carries out\ninference by maximum likelihood. The inference model is sound in the sense that\nthe vertices are provably identifiable under some assumptions, and it suggests\nthat PRISM can be effective in combating noise when the number of data points\nis large. PRISM has strong, but hidden, relationships with simplex volume\nminimization, a powerful geometric approach for the same problem. We study\nthese fundamental aspects, and we also consider algorithmic schemes based on\nimportance sampling and variational inference. In particular, the variational\ninference scheme is shown to resemble a matrix factorization problem with a\nspecial regularizer, which draws an interesting connection to the matrix\nfactorization approach. Numerical results are provided to demonstrate the\npotential of PRISM.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:39:00 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wu", "Ruiyuan", ""], ["Ma", "Wing-Kin", ""], ["Li", "Yuening", ""], ["So", "Anthony Man-Cho", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "2103.10060", "submitter": "Yihang Gao", "authors": "Yihang Gao, Mingjie Zhou, Michael K. Ng", "title": "Approximation Capabilities of Wasserstein Generative Adversarial\n  Networks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Wasserstein Generative Adversarial Networks (WGANs)\nusing GroupSort neural networks as discriminators. We show that the error bound\nfor the approximation of target distribution depends on both the width/depth\n(capacity) of generators and discriminators, as well as the number of samples\nin training. A quantified generalization bound is established for Wasserstein\ndistance between the generated distribution and the target distribution.\nAccording to our theoretical results, WGANs have higher requirement for the\ncapacity of discriminators than that of generators, which is consistent with\nsome existing theories. More importantly, overly deep and wide (high capacity)\ngenerators may cause worse results (after training) than low capacity\ngenerators if discriminators are not strong enough. Numerical results on the\nsynthetic data (swiss roll) and MNIST data confirm our theoretical results, and\ndemonstrate that the performance by using GroupSort neural networks as\ndiscriminators is better than that of the original WGAN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:40:13 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 07:22:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gao", "Yihang", ""], ["Zhou", "Mingjie", ""], ["Ng", "Michael K.", ""]]}, {"id": "2103.10150", "submitter": "James Townsend", "authors": "James Townsend, Iain Murray", "title": "Lossless compression with state space models using bits back coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We generalize the 'bits back with ANS' method to time-series models with a\nlatent Markov structure. This family of models includes hidden Markov models\n(HMMs), linear Gaussian state space models (LGSSMs) and many more. We provide\nexperimental evidence that our method is effective for small scale models, and\ndiscuss its applicability to larger scale settings such as video compression.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:34:57 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:53:45 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 09:43:19 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Townsend", "James", ""], ["Murray", "Iain", ""]]}, {"id": "2103.10153", "submitter": "Jonathan Schmidt", "authors": "Jonathan Schmidt, Nicholas Kr\\\"amer, Philipp Hennig", "title": "A Probabilistic State Space Model for Joint Inference from Differential\n  Equations and Data", "comments": "12 pages (+ 5 pages appendix), 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic models with differential equations are a key component of\nscientific applications of machine learning. Inference in such models is\nusually computationally demanding, because it involves repeatedly solving the\ndifferential equation. The main problem here is that the numerical solver is\nhard to combine with standard inference techniques. Recent work in\nprobabilistic numerics has developed a new class of solvers for ordinary\ndifferential equations (ODEs) that phrase the solution process directly in\nterms of Bayesian filtering. We here show that this allows such methods to be\ncombined very directly, with conceptual and numerical ease, with latent force\nmodels in the ODE itself. It then becomes possible to perform approximate\nBayesian inference on the latent force as well as the ODE solution in a single,\nlinear complexity pass of an extended Kalman filter / smoother - that is, at\nthe cost of computing a single ODE solution. We demonstrate the expressiveness\nand performance of the algorithm by training, among others, a non-parametric\nSIRD model on data from the COVID-19 outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:36:09 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:08:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schmidt", "Jonathan", ""], ["Kr\u00e4mer", "Nicholas", ""], ["Hennig", "Philipp", ""]]}, {"id": "2103.10159", "submitter": "Pratik Jawanpuria", "authors": "Karthik S. Gurumoorthy and Pratik Jawanpuria and Bamdev Mishra", "title": "SPOT: A framework for selection of prototypes using optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop an optimal transport (OT) based framework to select\ninformative prototypical examples that best represent a given target dataset.\nSummarizing a given target dataset via representative examples is an important\nproblem in several machine learning applications where human understanding of\nthe learning models and underlying data distribution is essential for decision\nmaking. We model the prototype selection problem as learning a sparse\n(empirical) probability distribution having the minimum OT distance from the\ntarget distribution. The learned probability measure supported on the chosen\nprototypes directly corresponds to their importance in representing the target\ndata. We show that our objective function enjoys a key property of\nsubmodularity and propose an efficient greedy method that is both\ncomputationally fast and possess deterministic approximation guarantees.\nEmpirical results on several real world benchmarks illustrate the efficacy of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:50:14 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 13:58:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Jawanpuria", "Pratik", ""], ["Mishra", "Bamdev", ""]]}, {"id": "2103.10182", "submitter": "Matthew Holden", "authors": "Matthew Holden, Marcelo Pereyra, Konstantinos C. Zygalakis", "title": "Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks:\n  Theory, Methods, and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new methodology for performing Bayesian inference in\nimaging inverse problems where the prior knowledge is available in the form of\ntraining data. Following the manifold hypothesis and adopting a generative\nmodelling approach, we construct a data-driven prior that is supported on a\nsub-manifold of the ambient space, which we can learn from the training data by\nusing a variational autoencoder or a generative adversarial network. We\nestablish the existence and well-posedness of the associated posterior\ndistribution and posterior moments under easily verifiable conditions,\nproviding a rigorous underpinning for Bayesian estimators and uncertainty\nquantification analyses. Bayesian computation is performed by using a parallel\ntempered version of the preconditioned Crank-Nicolson algorithm on the\nmanifold, which is shown to be ergodic and robust to the non-convex nature of\nthese data-driven models. In addition to point estimators and uncertainty\nquantification analyses, we derive a model misspecification test to\nautomatically detect situations where the data-driven prior is unreliable, and\nexplain how to identify the dimension of the latent space directly from the\ntraining data. The proposed approach is illustrated with a range of experiments\nwith the MNIST dataset, where it outperforms alternative image reconstruction\napproaches from the state of the art. A model accuracy analysis suggests that\nthe Bayesian probabilities reported by the data-driven models are also\nremarkably accurate under a frequentist definition of probability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:34:08 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Holden", "Matthew", ""], ["Pereyra", "Marcelo", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "2103.10251", "submitter": "Anthony Strittmatter", "authors": "Tobias Cagala, Ulrich Glogowsky, Johannes Rincke, Anthony Strittmatter", "title": "Optimal Targeting in Fundraising: A Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ineffective fundraising lowers the resources charities can use for goods\nprovision. We combine a field experiment and a causal machine-learning approach\nto increase a charity's fundraising effectiveness. The approach optimally\ntargets fundraising to individuals whose expected donations exceed solicitation\ncosts. Among past donors, optimal targeting substantially increases donations\n(net of fundraising costs) relative to benchmarks that target everybody or no\none. Instead, individuals who were previously asked but never donated should\nnot be targeted. Further, the charity requires only publicly available\ngeospatial information to realize the gains from targeting. We conclude that\ncharities not engaging in optimal targeting waste resources.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:06:35 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:54:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cagala", "Tobias", ""], ["Glogowsky", "Ulrich", ""], ["Rincke", "Johannes", ""], ["Strittmatter", "Anthony", ""]]}, {"id": "2103.10292", "submitter": "Veronika Cheplygina", "authors": "Ga\\\"el Varoquaux and Veronika Cheplygina", "title": "How I failed machine learning in medical imaging -- shortcomings and\n  recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Medical imaging is an important research field with many opportunities for\nimproving patients' health. However, there are a number of challenges that are\nslowing down the progress of the field as a whole, such optimizing for\npublication. In this paper we reviewed several problems related to choosing\ndatasets, methods, evaluation metrics, and publication strategies. With a\nreview of literature and our own analysis, we show that at every step,\npotential biases can creep in. On a positive note, we also see that initiatives\nto counteract these problems are already being started. Finally we provide a\nbroad range of recommendations on how to further these address problems in the\nfuture. For reproducibility, data and code for our analyses are available on\n\\url{https://github.com/GaelVaroquaux/ml_med_imaging_failures}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:46:35 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Varoquaux", "Ga\u00ebl", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2103.10369", "submitter": "Sebastian Curi", "authors": "Sebastian Curi, Ilija Bogunovic, Andreas Krause", "title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In real-world tasks, reinforcement learning (RL) agents frequently encounter\nsituations that are not present during training time. To ensure reliable\nperformance, the RL agents need to exhibit robustness against worst-case\nsituations. The robust RL framework addresses this challenge via a worst-case\noptimization between an agent and an adversary. Previous robust RL algorithms\nare either sample inefficient, lack robustness guarantees, or do not scale to\nlarge problems. We propose the Robust Hallucinated Upper-Confidence RL\n(RH-UCRL) algorithm to provably solve this problem while attaining near-optimal\nsample complexity guarantees. RH-UCRL is a model-based reinforcement learning\n(MBRL) algorithm that effectively distinguishes between epistemic and aleatoric\nuncertainty and efficiently explores both the agent and adversary decision\nspaces during policy learning. We scale RH-UCRL to complex tasks via neural\nnetworks ensemble models as well as neural network policies. Experimentally, we\ndemonstrate that RH-UCRL outperforms other robust deep RL algorithms in a\nvariety of adversarial environments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:50:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Curi", "Sebastian", ""], ["Bogunovic", "Ilija", ""], ["Krause", "Andreas", ""]]}, {"id": "2103.10481", "submitter": "Frank Lin", "authors": "Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam,\n  Christopher G. Brinton, Nicolo Michelusi", "title": "Two Timescale Hybrid Federated Learning with Cooperative D2D Local Model\n  Aggregations", "comments": "This paper is currently under review for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a popular technique for distributing\nmachine learning (ML) model training across the wireless edge. In this paper,\nwe propose two timescale hybrid federated learning (TT-HF), which is a hybrid\nbetween the device-to-server communication paradigm in federated learning and\ndevice-to-device (D2D) communications for model training. In TT-HF, during each\nglobal aggregation interval, devices (i) perform multiple stochastic gradient\ndescent iterations on their individual datasets, and (ii) aperiodically engage\nin consensus formation of their model parameters through cooperative,\ndistributed D2D communications within local clusters. With a new general\ndefinition of gradient diversity, we formally study the convergence behavior of\nTT-HF, resulting in new convergence bounds for distributed ML. We leverage our\nconvergence bounds to develop an adaptive control algorithm that tunes the step\nsize, D2D communication rounds, and global aggregation period of TT-HF over\ntime to target a sublinear convergence rate of O(1/t) while minimizing network\nresource utilization. Our subsequent experiments demonstrate that TT-HF\nsignificantly outperforms the current art in federated learning in terms of\nmodel accuracy and/or network energy consumption in different scenarios where\nlocal device datasets exhibit statistical heterogeneity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:58:45 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lin", "Frank Po-Chen", ""], ["Hosseinalipour", "Seyyedali", ""], ["Azam", "Sheikh Shams", ""], ["Brinton", "Christopher G.", ""], ["Michelusi", "Nicolo", ""]]}, {"id": "2103.10510", "submitter": "Chong Huang", "authors": "Chong Huang, Arash Nourian, Kevin Griest", "title": "Hidden Technical Debts for Fair Machine Learning in Financial Services", "comments": "Presented at NeurIPS 2020 Fair AI in Finance Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent advancements in machine learning (ML) have demonstrated the\npotential for providing a powerful solution to build complex prediction systems\nin a short time. However, in highly regulated industries, such as the financial\ntechnology (Fintech), people have raised concerns about the risk of ML systems\ndiscriminating against specific protected groups or individuals. To address\nthese concerns, researchers have introduced various mathematical fairness\nmetrics and bias mitigation algorithms. This paper discusses hidden technical\ndebts and challenges of building fair ML systems in a production environment\nfor Fintech. We explore various stages that require attention for fairness in\nthe ML system development and deployment life cycle. To identify hidden\ntechnical debts that exist in building fair ML system for Fintech, we focus on\nkey pipeline stages including data preparation, model development, system\nmonitoring and integration in production. Our analysis shows that enforcing\nfairness for production-ready ML systems in Fintech requires specific\nengineering commitments at different stages of ML system life cycle. We also\npropose several initial starting points to mitigate these technical debts for\ndeploying fair ML systems in production.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:27:34 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 01:11:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Chong", ""], ["Nourian", "Arash", ""], ["Griest", "Kevin", ""]]}, {"id": "2103.10549", "submitter": "Ali Amiryousefi", "authors": "Ali Amiryousefi", "title": "Inductive Inference in Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inductive inference in supervised classification context constitutes to\nmethods and approaches to assign some objects or items into different\npredefined classes using a formal rule that is derived from training data and\npossibly some additional auxiliary information. The optimality of such an\nassignment varies under different conditions due to intrinsic attributes of the\nobjects being considered for such a task. One of these cases is when all the\nobjects' features are discrete variables with a priori known categories. As\nanother example, one can consider a modification of this case with a priori\nunknown categories. These two cases are the main focus of this thesis and based\non Bayesian inductive theories, de Finetti type exchangeability is a suitable\nassumption that facilitates the derivation of classifiers in the former\nscenario. On the contrary, this type of exchangeability is not applicable in\nthe latter case, instead, it is possible to utilise the partition\nexchangeability due to John Kingman. These two types of exchangeabilities are\ndiscussed and furthermore here I investigate inductive supervised classifiers\nbased on both types of exchangeabilities. I further demonstrate that the\nclassifiers based on de Finetti type exchangeability can optimally handle test\nitems independently of each other in the presence of infinite amounts of\ntraining data while on the other hand, classifiers based on partition\nexchangeability still continue to benefit from joint labelling of all the test\nitems. Additionally, it is shown that the inductive learning process for the\nsimultaneous classifier saturates when the amount of test data tends to\ninfinity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:25:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Amiryousefi", "Ali", ""]]}, {"id": "2103.10620", "submitter": "Juan Carlos Perdomo", "authors": "Juan C. Perdomo, Max Simchowitz, Alekh Agarwal, Peter Bartlett", "title": "Towards a Dimension-Free Understanding of Adaptive Linear Control", "comments": "presented at COLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of adaptive control of the linear quadratic regulator\nfor systems in very high, or even infinite dimension. We demonstrate that while\nsublinear regret requires finite dimensional inputs, the ambient state\ndimension of the system need not be bounded in order to perform online control.\nWe provide the first regret bounds for LQR which hold for infinite dimensional\nsystems, replacing dependence on ambient dimension with more natural notions of\nproblem complexity. Our guarantees arise from a novel perturbation bound for\ncertainty equivalence which scales with the prediction error in estimating the\nsystem parameters, without requiring consistent parameter recovery in more\nstringent measures like the operator norm. When specialized to finite\ndimensional settings, our bounds recover near optimal dimension and time\nhorizon dependence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:59:15 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:40:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Perdomo", "Juan C.", ""], ["Simchowitz", "Max", ""], ["Agarwal", "Alekh", ""], ["Bartlett", "Peter", ""]]}, {"id": "2103.10697", "submitter": "St\\'ephane d'Ascoli", "authors": "St\\'ephane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio\n  Biroli, Levent Sagun", "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive\n  Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional architectures have proven extremely successful for vision\ntasks. Their hard inductive biases enable sample-efficient learning, but come\nat the cost of a potentially lower performance ceiling. Vision Transformers\n(ViTs) rely on more flexible self-attention layers, and have recently\noutperformed CNNs for image classification. However, they require costly\npre-training on large external datasets or distillation from pre-trained\nconvolutional networks. In this paper, we ask the following question: is it\npossible to combine the strengths of these two architectures while avoiding\ntheir respective limitations? To this end, we introduce gated positional\nself-attention (GPSA), a form of positional self-attention which can be\nequipped with a ``soft\" convolutional inductive bias. We initialise the GPSA\nlayers to mimic the locality of convolutional layers, then give each attention\nhead the freedom to escape locality by adjusting a gating parameter regulating\nthe attention paid to position versus content information. The resulting\nconvolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,\nwhile offering a much improved sample efficiency. We further investigate the\nrole of locality in learning by first quantifying how it is encouraged in\nvanilla self-attention layers, then analysing how it is escaped in GPSA layers.\nWe conclude by presenting various ablations to better understand the success of\nthe ConViT. Our code and models are released publicly at\nhttps://github.com/facebookresearch/convit.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:11:20 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:44:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["d'Ascoli", "St\u00e9phane", ""], ["Touvron", "Hugo", ""], ["Leavitt", "Matthew", ""], ["Morcos", "Ari", ""], ["Biroli", "Giulio", ""], ["Sagun", "Levent", ""]]}, {"id": "2103.10710", "submitter": "William Wilkinson", "authors": "William J. Wilkinson, Arno Solin, Vincent Adam", "title": "Sparse Algorithms for Markovian Gaussian Processes", "comments": "Appearing in the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian inference methods that scale to very large datasets are\ncrucial in leveraging probabilistic models for real-world time series. Sparse\nMarkovian Gaussian processes combine the use of inducing variables with\nefficient Kalman filter-like recursions, resulting in algorithms whose\ncomputational and memory requirements scale linearly in the number of inducing\npoints, whilst also enabling parallel parameter updates and stochastic\noptimisation. Under this paradigm, we derive a general site-based approach to\napproximate inference, whereby we approximate the non-Gaussian likelihood with\nlocal Gaussian terms, called sites. Our approach results in a suite of novel\nsparse extensions to algorithms from both the machine learning and signal\nprocessing literature, including variational inference, expectation\npropagation, and the classical nonlinear Kalman smoothers. The derived methods\nare suited to large time series, and we also demonstrate their applicability to\nspatio-temporal data, where the model has separate inducing points in both time\nand space.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:50:53 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:09:19 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 10:44:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wilkinson", "William J.", ""], ["Solin", "Arno", ""], ["Adam", "Vincent", ""]]}, {"id": "2103.10787", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili, Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah,\n  Ajmal Mian", "title": "LSDAT: Low-Rank and Sparse Decomposition for Decision-based Adversarial\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose LSDAT, an image-agnostic decision-based black-box attack that\nexploits low-rank and sparse decomposition (LSD) to dramatically reduce the\nnumber of queries and achieve superior fooling rates compared to the\nstate-of-the-art decision-based methods under given imperceptibility\nconstraints. LSDAT crafts perturbations in the low-dimensional subspace formed\nby the sparse component of the input sample and that of an adversarial sample\nto obtain query-efficiency. The specific perturbation of interest is obtained\nby traversing the path between the input and adversarial sparse components. It\nis set forth that the proposed sparse perturbation is the most aligned sparse\nperturbation with the shortest path from the input sample to the decision\nboundary for some initial adversarial sample (the best sparse approximation of\nshortest path, likely to fool the model). Theoretical analyses are provided to\njustify the functionality of LSDAT. Unlike other dimensionality reduction based\ntechniques aimed at improving query efficiency (e.g, ones based on FFT), LSD\nworks directly in the image pixel domain to guarantee that non-$\\ell_2$\nconstraints, such as sparsity, are satisfied. LSD offers better control over\nthe number of queries and provides computational efficiency as it performs\nsparse decomposition of the input and adversarial images only once to generate\nall queries. We demonstrate $\\ell_0$, $\\ell_2$ and $\\ell_\\infty$ bounded\nattacks with LSDAT to evince its efficiency compared to baseline decision-based\nattacks in diverse low-query budget scenarios as outlined in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:10:47 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:07:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Edraki", "Marzieh", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""], ["Mian", "Ajmal", ""]]}, {"id": "2103.10842", "submitter": "Siegfried Wahl", "authors": "Alexander Leube, Lukas Lang, Gerhard Kelch and Siegfried Wahl", "title": "Prediction of progressive lens performance from neural network\n  simulations", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: The purpose of this study is to present a framework to predict\nvisual acuity (VA) based on a convolutional neural network (CNN) and to further\nto compare PAL designs.\n  Method: A simple two hidden layer CNN was trained to classify the gap\norientations of Landolt Cs by combining the feature extraction abilities of a\nCNN with psychophysical staircase methods. The simulation was validated\nregarding its predictability of clinical VA from induced spherical defocus\n(between +/-1.5 D, step size: 0.5 D) from 39 subjectively measured eyes.\nAfterwards, a simulation for a presbyopic eye corrected by either a generic\nhard or a soft PAL design (addition power: 2.5 D) was performed including lower\nand higher order aberrations.\n  Result: The validation revealed consistent offset of +0.20 logMAR +/-0.035\nlogMAR from simulated VA. Bland-Altman analysis from offset-corrected results\nshowed limits of agreement (+/-1.96 SD) of -0.08 logMAR and +0.07 logMAR, which\nis comparable to clinical repeatability of VA assessment. The application of\nthe simulation for PALs confirmed a bigger far zone for generic hard design but\ndid not reveal zone width differences for the intermediate or near zone.\nFurthermore, a horizontal area of better VA at the mid of the PAL was found,\nwhich confirms the importance for realistic performance simulations using\nobject-based aberration and physiological performance measures as VA.\n  Conclusion: The proposed holistic simulation tool was shown to act as an\naccurate model for subjective visual performance. Further, the simulations\napplication for PALs indicated its potential as an effective method to compare\nvisual performance of different optical designs. Moreover, the simulation\nprovides the basis to incorporate neural aspects of visual perception and thus\nsimulate the VA including neural processing in future.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:51:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Leube", "Alexander", ""], ["Lang", "Lukas", ""], ["Kelch", "Gerhard", ""], ["Wahl", "Siegfried", ""]]}, {"id": "2103.10897", "submitter": "Gaurav Mahajan", "authors": "Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav\n  Mahajan, Wen Sun and Ruosong Wang", "title": "Bilinear Classes: A Structural Framework for Provable Generalization in\n  RL", "comments": "Expanded extension section to include generalized linear bellman\n  complete and changed related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces Bilinear Classes, a new structural framework, which\npermit generalization in reinforcement learning in a wide variety of settings\nthrough the use of function approximation. The framework incorporates nearly\nall existing models in which a polynomial sample complexity is achievable, and,\nnotably, also includes new models, such as the Linear $Q^*/V^*$ model in which\nboth the optimal $Q$-function and the optimal $V$-function are linear in some\nknown feature space. Our main result provides an RL algorithm which has\npolynomial sample complexity for Bilinear Classes; notably, this sample\ncomplexity is stated in terms of a reduction to the generalization error of an\nunderlying supervised learning sub-problem. These bounds nearly match the best\nknown sample complexity bounds for existing models. Furthermore, this framework\nalso extends to the infinite dimensional (RKHS) setting: for the the Linear\n$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample\ncomplexities that have no explicit dependence on the explicit feature dimension\n(which could be infinite), but instead depends only on information theoretic\nquantities.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:34:20 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 06:29:35 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 22:29:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Du", "Simon S.", ""], ["Kakade", "Sham M.", ""], ["Lee", "Jason D.", ""], ["Lovett", "Shachar", ""], ["Mahajan", "Gaurav", ""], ["Sun", "Wen", ""], ["Wang", "Ruosong", ""]]}, {"id": "2103.10935", "submitter": "Romit Maulik", "authors": "Boumediene Hamzi, Romit Maulik, Houman Owhadi", "title": "Data-driven geophysical forecasting: Simple, low-cost, and accurate\n  baselines with kernel methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph math.DS physics.flu-dyn physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling geophysical systems as dynamical systems and regressing their vector\nfield from data is a simple way to learn emulators for such systems. We show\nthat when the kernel of these emulators is also learned from data (using kernel\nflows, a variant of cross-validation), then the resulting data-driven models\nare not only faster than equation-based models but are easier to train than\nneural networks such as the long short-term memory neural network. In addition,\nthey are also more accurate and predictive than the latter. When trained on\nobservational data for the global sea-surface temperature, considerable gains\nare observed by the proposed technique in comparison to classical partial\ndifferential equation-based models in terms of forecast computational cost and\naccuracy. When trained on publicly available re-analysis data for temperatures\nin the North-American continent, we see significant improvements over\nclimatology and persistence based forecast techniques.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 19:57:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hamzi", "Boumediene", ""], ["Maulik", "Romit", ""], ["Owhadi", "Houman", ""]]}, {"id": "2103.10943", "submitter": "Sylvain Le Corff", "authors": "Achille Thin (CMAP), Yazid Janati (IP Paris, TIPIC-SAMOVAR, CITI),\n  Sylvain Le Corff (IP Paris, TIPIC-SAMOVAR, CITI), Charles Ollion (CMAP),\n  Arnaud Doucet, Alain Durmus (CMLA), Eric Moulines (CMAP), Christian Robert\n  (CEREMADE)", "title": "Invertible Flow Non Equilibrium sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously sampling from a complex distribution with intractable\nnormalizing constant and approximating expectations under this distribution is\na notoriously challenging problem. We introduce a novel scheme, Invertible Flow\nNon Equilibrium Sampling (InFine), which departs from classical Sequential\nMonte Carlo (SMC) and Markov chain Monte Carlo (MCMC) approaches. InFine\nconstructs unbiased estimators of expectations and in particular of normalizing\nconstants by combining the orbits of a deterministic transform started from\nrandom initializations.When this transform is chosen as an appropriate\nintegrator of a conformal Hamiltonian system, these orbits are optimization\npaths. InFine is also naturally suited to design new MCMC sampling schemes by\nselecting samples on the optimization paths.Additionally, InFine can be used to\nconstruct an Evidence Lower Bound (ELBO) leading to a new class of Variational\nAutoEncoders (VAE).\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:09:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Thin", "Achille", "", "CMAP"], ["Janati", "Yazid", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Corff", "Sylvain Le", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Ollion", "Charles", "", "CMAP"], ["Doucet", "Arnaud", "", "CMLA"], ["Durmus", "Alain", "", "CMLA"], ["Moulines", "Eric", "", "CMAP"], ["Robert", "Christian", "", "CEREMADE"]]}, {"id": "2103.10974", "submitter": "Sifan Wang", "authors": "Sifan Wang, Hanwen Wang, Paris Perdikaris", "title": "Learning the solution operator of parametric partial differential\n  equations with physics-informed DeepOnets", "comments": "33 pages, 28 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep operator networks (DeepONets) are receiving increased attention thanks\nto their demonstrated capability to approximate nonlinear operators between\ninfinite-dimensional Banach spaces. However, despite their remarkable early\npromise, they typically require large training data-sets consisting of paired\ninput-output observations which may be expensive to obtain, while their\npredictions may not be consistent with the underlying physical principles that\ngenerated the observed data. In this work, we propose a novel model class\ncoined as physics-informed DeepONets, which introduces an effective\nregularization mechanism for biasing the outputs of DeepOnet models towards\nensuring physical consistency. This is accomplished by leveraging automatic\ndifferentiation to impose the underlying physical laws via soft penalty\nconstraints during model training. We demonstrate that this simple, yet\nremarkably effective extension can not only yield a significant improvement in\nthe predictive accuracy of DeepOnets, but also greatly reduce the need for\nlarge training data-sets. To this end, a remarkable observation is that\nphysics-informed DeepONets are capable of solving parametric partial\ndifferential equations (PDEs) without any paired input-output observations,\nexcept for a set of given initial or boundary conditions. We illustrate the\neffectiveness of the proposed framework through a series of comprehensive\nnumerical studies across various types of PDEs. Strikingly, a trained physics\ninformed DeepOnet model can predict the solution of $\\mathcal{O}(10^3)$\ntime-dependent PDEs in a fraction of a second -- up to three orders of\nmagnitude faster compared a conventional PDE solver. The data and code\naccompanying this manuscript are publicly available at\n\\url{https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets}.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:15:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Sifan", ""], ["Wang", "Hanwen", ""], ["Perdikaris", "Paris", ""]]}, {"id": "2103.11003", "submitter": "Po-Ling Loh", "authors": "Marco Avella-Medina, Casey Bradshaw, Po-Ling Loh", "title": "Differentially private inference via noisy optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general optimization-based framework for computing\ndifferentially private M-estimators and a new method for constructing\ndifferentially private confidence regions. Firstly, we show that robust\nstatistics can be used in conjunction with noisy gradient descent or noisy\nNewton methods in order to obtain optimal private estimators with global linear\nor quadratic convergence, respectively. We establish local and global\nconvergence guarantees, under both local strong convexity and self-concordance,\nshowing that our private estimators converge with high probability to a nearly\noptimal neighborhood of the non-private M-estimators. Secondly, we tackle the\nproblem of parametric inference by constructing differentially private\nestimators of the asymptotic variance of our private M-estimators. This\nnaturally leads to approximate pivotal statistics for constructing confidence\nregions and conducting hypothesis testing. We demonstrate the effectiveness of\na bias correction that leads to enhanced small-sample empirical performance in\nsimulations. We illustrate the benefits of our methods in several numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:55:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Avella-Medina", "Marco", ""], ["Bradshaw", "Casey", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2103.11023", "submitter": "Amanda Bower", "authors": "Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, Yuekai Sun", "title": "Individually Fair Ranking", "comments": "ICLR Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm to train individually fair learning-to-rank (LTR)\nmodels. The proposed approach ensures items from minority groups appear\nalongside similar items from majority groups. This notion of fair ranking is\nbased on the definition of individual fairness from supervised learning and is\nmore nuanced than prior fair LTR approaches that simply ensure the ranking\nmodel provides underrepresented items with a basic level of exposure. The crux\nof our method is an optimal transport-based regularizer that enforces\nindividual fairness and an efficient algorithm for optimizing the regularizer.\nWe show that our approach leads to certifiably individually fair LTR models and\ndemonstrate the efficacy of our method on ranking tasks subject to demographic\nbiases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:17:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bower", "Amanda", ""], ["Eftekhari", "Hamid", ""], ["Yurochkin", "Mikhail", ""], ["Sun", "Yuekai", ""]]}, {"id": "2103.11102", "submitter": "Yuanyu Wan", "authors": "Yuanyu Wan, Guanghui Wang, Lijun Zhang", "title": "Projection-free Distributed Online Learning with Strongly Convex Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently solve distributed online learning problems with complicated\nconstraints, previous studies have proposed several distributed projection-free\nalgorithms. The state-of-the-art one achieves the $O({T}^{3/4})$ regret bound\nwith $O(\\sqrt{T})$ communication complexity. In this paper, we further exploit\nthe strong convexity of loss functions to improve the regret bound and\ncommunication complexity. Specifically, we first propose a distributed\nprojection-free algorithm for strongly convex loss functions, which enjoys a\nbetter regret bound of $O(T^{2/3}\\log T)$ with smaller communication complexity\nof $O(T^{1/3})$. Furthermore, we demonstrate that the regret of distributed\nonline algorithms with $C$ communication rounds has a lower bound of\n$\\Omega(T/C)$, even when the loss functions are strongly convex. This lower\nbound implies that the $O(T^{1/3})$ communication complexity of our algorithm\nis nearly optimal for obtaining the $O(T^{2/3}\\log T)$ regret bound up to\npolylogarithmic factors. Finally, we extend our algorithm into the bandit\nsetting and obtain similar theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 05:38:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wan", "Yuanyu", ""], ["Wang", "Guanghui", ""], ["Zhang", "Lijun", ""]]}, {"id": "2103.11107", "submitter": "Rameshwar Pratap", "authors": "Amit Deshpande and Rameshwar Pratap", "title": "On Subspace Approximation and Subset Selection in Fewer Passes by MCMC\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of subset selection for $\\ell_{p}$ subspace\napproximation, i.e., given $n$ points in $d$ dimensions, we need to pick a\nsmall, representative subset of the given points such that its span gives\n$(1+\\epsilon)$ approximation to the best $k$-dimensional subspace that\nminimizes the sum of $p$-th powers of distances of all the points to this\nsubspace. Sampling-based subset selection techniques require adaptive sampling\niterations with multiple passes over the data. Matrix sketching techniques give\na single-pass $(1+\\epsilon)$ approximation for $\\ell_{p}$ subspace\napproximation but require additional passes for subset selection.\n  In this work, we propose an MCMC algorithm to reduce the number of passes\nrequired by previous subset selection algorithms based on adaptive sampling.\nFor $p=2$, our algorithm gives subset selection of nearly optimal size in only\n$2$ passes, whereas the number of passes required in previous work depend on\n$k$. Our algorithm picks a subset of size $\\mathrm{poly}(k/\\epsilon)$ that\ngives $(1+\\epsilon)$ approximation to the optimal subspace. The running time of\nthe algorithm is $nd + d~\\mathrm{poly}(k/\\epsilon)$. We extend our results to\nthe case when outliers are present in the datasets, and suggest a two pass\nalgorithm for the same. Our ideas also extend to give a reduction in the number\nof passes required by adaptive sampling algorithms for $\\ell_{p}$ subspace\napproximation and subset selection, for $p \\geq 2$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 06:07:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Deshpande", "Amit", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "2103.11166", "submitter": "Xin Ding", "authors": "Xin Ding, Yongwei Wang, Z. Jane Wang, William J. Welch", "title": "Efficient Subsampling for Generating High-Quality Images from\n  Conditional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Subsampling unconditional generative adversarial networks (GANs) to improve\nthe overall image quality has been studied recently. However, these methods\noften require high training costs (e.g., storage space, parameter tuning) and\nmay be inefficient or even inapplicable for subsampling conditional GANs, such\nas class-conditional GANs and continuous conditional GANs (CcGANs), when the\ncondition has many distinct values. In this paper, we propose an efficient\nmethod called conditional density ratio estimation in feature space with\nconditional Softplus loss (cDRE-F-cSP). With cDRE-F-cSP, we estimate an image's\nconditional density ratio based on a novel conditional Softplus (cSP) loss in\nthe feature space learned by a specially designed ResNet-34 or sparse\nautoencoder. We then derive the error bound of a conditional density ratio\nmodel trained with the proposed cSP loss. Finally, we propose a rejection\nsampling scheme, termed cDRE-F-cSP+RS, which can subsample both\nclass-conditional GANs and CcGANs efficiently. An extra filtering scheme is\nalso developed for CcGANs to increase the label consistency. Experiments on\nCIFAR-10 and Tiny-ImageNet datasets show that cDRE-F-cSP+RS can substantially\nimprove the Intra-FID and FID scores of BigGAN. Experiments on RC-49 and\nUTKFace datasets demonstrate that cDRE-F-cSP+RS also improves Intra-FID,\nDiversity, and Label Score of CcGANs. Moreover, to show the high efficiency of\ncDRE-F-cSP+RS, we compare it with the state-of-the-art unconditional\nsubsampling method (i.e., DRE-F-SP+RS). With comparable or even better\nperformance, cDRE-F-cSP+RS only requires about \\textbf{10}\\% and \\textbf{1.7}\\%\nof the training costs spent respectively on CIFAR-10 and UTKFace by\nDRE-F-SP+RS.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:19:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ding", "Xin", ""], ["Wang", "Yongwei", ""], ["Wang", "Z. Jane", ""], ["Welch", "William J.", ""]]}, {"id": "2103.11175", "submitter": "Patrick Schwab", "authors": "Sonali Parbhoo, Stefan Bauer, Patrick Schwab", "title": "NCoRE: Neural Counterfactual Representation Learning for Combinations of\n  Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating an individual's potential response to interventions from\nobservational data is of high practical relevance for many domains, such as\nhealthcare, public policy or economics. In this setting, it is often the case\nthat combinations of interventions may be applied simultaneously, for example,\nmultiple prescriptions in healthcare or different fiscal and monetary measures\nin economics. However, existing methods for counterfactual inference are\nlimited to settings in which actions are not used simultaneously. Here, we\npresent Neural Counterfactual Relation Estimation (NCoRE), a new method for\nlearning counterfactual representations in the combination treatment setting\nthat explicitly models cross-treatment interactions. NCoRE is based on a novel\nbranched conditional neural representation that includes learnt treatment\ninteraction modulators to infer the potential causal generative process\nunderlying the combination of multiple treatments. Our experiments show that\nNCoRE significantly outperforms existing state-of-the-art methods for\ncounterfactual treatment effect estimation that do not account for the effects\nof combining multiple treatments across several synthetic, semi-synthetic and\nreal-world benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:25:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Parbhoo", "Sonali", ""], ["Bauer", "Stefan", ""], ["Schwab", "Patrick", ""]]}, {"id": "2103.11181", "submitter": "Kejun Tang", "authors": "Kejun Tang, Xiaoliang Wan, Qifeng Liao", "title": "Adaptive deep density approximation for Fokker-Planck equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel adaptive deep density approximation strategy\nbased on KRnet (ADDA-KR) for solving the steady-state Fokker-Planck equation.\nIt is known that this equation typically has high-dimensional spatial variables\nposed on unbounded domains, which limit the application of traditional grid\nbased numerical methods. With the Knothe-Rosenblatt rearrangement, our newly\nproposed flow-based generative model, called KRnet, provides a family of\nprobability density functions to serve as effective solution candidates of the\nFokker-Planck equation, which have weaker dependence on dimensionality than\ntraditional computational approaches. To result in effective stochastic\ncollocation points for training KRnet, we develop an adaptive sampling\nprocedure, where samples are generated iteratively using KRnet at each\niteration. In addition, we give a detailed discussion of KRnet and show that it\ncan efficiently estimate general high-dimensional density functions. We present\na general mathematical framework of ADDA-KR, validate its accuracy and\ndemonstrate its efficiency with numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:49:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tang", "Kejun", ""], ["Wan", "Xiaoliang", ""], ["Liao", "Qifeng", ""]]}, {"id": "2103.11238", "submitter": "Devesh Jha", "authors": "Devesh K. Jha", "title": "Markov Modeling of Time-Series Data using Symbolic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov models are often used to capture the temporal patterns of sequential\ndata for statistical learning applications. While the Hidden Markov\nmodeling-based learning mechanisms are well studied in literature, we analyze a\nsymbolic-dynamics inspired approach. Under this umbrella, Markov modeling of\ntime-series data consists of two major steps -- discretization of continuous\nattributes followed by estimating the size of temporal memory of the\ndiscretized sequence. These two steps are critical for the accurate and concise\nrepresentation of time-series data in the discrete space. Discretization\ngoverns the information content of the resultant discretized sequence. On the\nother hand, memory estimation of the symbolic sequence helps to extract the\npredictive patterns in the discretized data. Clearly, the effectiveness of\nsignal representation as a discrete Markov process depends on both these steps.\nIn this paper, we will review the different techniques for discretization and\nmemory estimation for discrete stochastic processes. In particular, we will\nfocus on the individual problems of discretization and order estimation for\ndiscrete stochastic process. We will present some results from literature on\npartitioning from dynamical systems theory and order estimation using concepts\nof information theory and statistical learning. The paper also presents some\nrelated problem formulations which will be useful for machine learning and\nstatistical learning application using the symbolic framework of data analysis.\nWe present some results of statistical analysis of a complex thermoacoustic\ninstability phenomenon during lean-premixed combustion in jet-turbine engines\nusing the proposed Markov modeling method.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 20:31:21 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 19:38:10 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Jha", "Devesh K.", ""]]}, {"id": "2103.11251", "submitter": "Cynthia Rudin", "authors": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova,\n  and Chudi Zhong", "title": "Interpretable Machine Learning: Fundamental Principles and 10 Grand\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the \"Rashomon set\" of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 21:58:27 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 01:20:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Rudin", "Cynthia", ""], ["Chen", "Chaofan", ""], ["Chen", "Zhi", ""], ["Huang", "Haiyang", ""], ["Semenova", "Lesia", ""], ["Zhong", "Chudi", ""]]}, {"id": "2103.11269", "submitter": "Xiang Li", "authors": "Varun Buch, Aoxiao Zhong, Xiang Li, Marcio Aloisio Bezerra Cavalcanti\n  Rockenbach, Dufan Wu, Hui Ren, Jiahui Guan, Andrew Liteplo, Sayon Dutta,\n  Ittai Dayan, Quanzheng Li", "title": "Development and Validation of a Deep Learning Model for Prediction of\n  Severe Outcomes in Suspected COVID-19 Infection", "comments": "Varun Buch, Aoxiao Zhong and Xiang Li contribute equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 patient triaging with predictive outcome of the patients upon first\npresent to emergency department (ED) is crucial for improving patient\nprognosis, as well as better hospital resources management and cross-infection\ncontrol. We trained a deep feature fusion model to predict patient outcomes,\nwhere the model inputs were EHR data including demographic information,\nco-morbidities, vital signs and laboratory measurements, plus patient's CXR\nimages. The model output was patient outcomes defined as the most insensitive\noxygen therapy required. For patients without CXR images, we employed Random\nForest method for the prediction. Predictive risk scores for COVID-19 severe\noutcomes (\"CO-RISK\" score) were derived from model output and evaluated on the\ntesting dataset, as well as compared to human performance. The study's dataset\n(the \"MGB COVID Cohort\") was constructed from all patients presenting to the\nMass General Brigham (MGB) healthcare system from March 1st to June 1st, 2020.\nED visits with incomplete or erroneous data were excluded. Patients with no\ntest order for COVID or confirmed negative test results were excluded. Patients\nunder the age of 15 were also excluded. Finally, electronic health record (EHR)\ndata from a total of 11060 COVID-19 confirmed or suspected patients were used\nin this study. Chest X-ray (CXR) images were also collected from each patient\nif available. Results show that CO-RISK score achieved area under the Curve\n(AUC) of predicting MV/death (i.e. severe outcomes) in 24 hours of 0.95, and\n0.92 in 72 hours on the testing dataset. The model shows superior performance\nto the commonly used risk scores in ED (CURB-65 and MEWS). Comparing with\nphysician's decisions, CO-RISK score has demonstrated superior performance to\nhuman in making ICU/floor decisions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 00:03:27 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:46:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Buch", "Varun", ""], ["Zhong", "Aoxiao", ""], ["Li", "Xiang", ""], ["Rockenbach", "Marcio Aloisio Bezerra Cavalcanti", ""], ["Wu", "Dufan", ""], ["Ren", "Hui", ""], ["Guan", "Jiahui", ""], ["Liteplo", "Andrew", ""], ["Dutta", "Sayon", ""], ["Dayan", "Ittai", ""], ["Li", "Quanzheng", ""]]}, {"id": "2103.11327", "submitter": "Jelena Bradic", "authors": "Jelena Bradic and Yinchu Zhu", "title": "Comments on Leo Breiman's paper 'Statistical Modeling: The Two Cultures'\n  (Statistical Science, 2001, 16(3), 199-231)", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman challenged statisticians to think more broadly, to step into the\nunknown, model-free learning world, with him paving the way forward. Statistics\ncommunity responded with slight optimism, some skepticism, and plenty of\ndisbelief. Today, we are at the same crossroad anew. Faced with the enormous\npractical success of model-free, deep, and machine learning, we are naturally\ninclined to think that everything is resolved. A new frontier has emerged; the\none where the role, impact, or stability of the {\\it learning} algorithms is no\nlonger measured by prediction quality, but an inferential one -- asking the\nquestions of {\\it why} and {\\it if} can no longer be safely ignored.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 07:40:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bradic", "Jelena", ""], ["Zhu", "Yinchu", ""]]}, {"id": "2103.11338", "submitter": "Aparna Varde", "authors": "Anita Pampoore-Thampi, Aparna S. Varde, Danlin Yu", "title": "Mining GIS Data to Predict Urban Sprawl", "comments": "8 Pages, 13 figures, KDD 2014 conference Bloomberg track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses the interesting problem of processing and analyzing data\nin geographic information systems (GIS) to achieve a clear perspective on urban\nsprawl. The term urban sprawl refers to overgrowth and expansion of low-density\nareas with issues such as car dependency and segregation between residential\nversus commercial use. Sprawl has impacts on the environment and public health.\nIn our work, spatiotemporal features related to real GIS data on urban sprawl\nsuch as population growth and demographics are mined to discover knowledge for\ndecision support. We adapt data mining algorithms, Apriori for association rule\nmining and J4.8 for decision tree classification to geospatial analysis,\ndeploying the ArcGIS tool for mapping. Knowledge discovered by mining this\nspatiotemporal data is used to implement a prototype spatial decision support\nsystem (SDSS). This SDSS predicts whether urban sprawl is likely to occur.\nFurther, it estimates the values of pertinent variables to understand how the\nvariables impact each other. The SDSS can help decision-makers identify\nproblems and create solutions for avoiding future sprawl occurrence and\nconducting urban planning where sprawl already occurs, thus aiding sustainable\ndevelopment. This work falls in the broad realm of geospatial intelligence and\nsets the stage for designing a large scale SDSS to process big data in complex\nenvironments, which constitutes part of our future work.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:41:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pampoore-Thampi", "Anita", ""], ["Varde", "Aparna S.", ""], ["Yu", "Danlin", ""]]}, {"id": "2103.11349", "submitter": "Renfei Tu", "authors": "Renfei Tu, Yang Liu, Yongzeng Xue, Cheng Wang and Maozu Guo", "title": "Neighbor Embedding Variational Autoencoder", "comments": "Paper under review for ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being one of the most popular generative framework, variational\nautoencoders(VAE) are known to suffer from a phenomenon termed posterior\ncollapse, i.e. the latent variational distributions collapse to the prior,\nespecially when a strong decoder network is used. In this work, we analyze the\nlatent representation of collapsed VAEs, and proposed a novel model, neighbor\nembedding VAE(NE-VAE), which explicitly constraints the encoder to encode\ninputs close in the input space to be close in the latent space. We observed\nthat for VAE variants that report similar ELBO, KL divergence or even mutual\ninformation scores may still behave quite differently in the latent\norganization. In our experiments, NE-VAE can produce qualitatively different\nlatent representations with majority of the latent dimensions remained active,\nwhich may benefit downstream latent space optimization tasks. NE-VAE can\nprevent posterior collapse to a much greater extent than it's predecessors, and\ncan be easily plugged into any autoencoder framework, without introducing\naddition model components and complex training routines.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 09:49:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tu", "Renfei", ""], ["Liu", "Yang", ""], ["Xue", "Yongzeng", ""], ["Wang", "Cheng", ""], ["Guo", "Maozu", ""]]}, {"id": "2103.11352", "submitter": "Yu-Hang Tang", "authors": "Yu-Hang Tang, Yuanran Zhu, Wibe A. de Jong", "title": "Detecting Label Noise via Leave-One-Out Cross-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple algorithm for identifying and correcting real-valued\nnoisy labels from a mixture of clean and corrupted sample points using Gaussian\nprocess regression. A heteroscedastic noise model is employed, in which\nadditive Gaussian noise terms with independent variances are associated with\neach and all of the observed labels. Optimizing the noise model using maximum\nlikelihood estimation leads to the containment of the GPR model's predictive\nerror by the posterior standard deviation in leave-one-out cross-validation. A\nmultiplicative update scheme is proposed for solving the maximum likelihood\nestimation problem under non-negative constraints. While we provide proof of\nconvergence for certain special cases, the multiplicative scheme has\nempirically demonstrated monotonic convergence behavior in virtually all our\nnumerical experiments. We show that the presented method can pinpoint corrupted\nsample points and lead to better regression models when trained on synthetic\nand real-world scientific data sets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:02:50 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 19:00:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tang", "Yu-Hang", ""], ["Zhu", "Yuanran", ""], ["de Jong", "Wibe A.", ""]]}, {"id": "2103.11357", "submitter": "Andreas Holzinger", "authors": "Andr\\'e M. Carrington, Douglas G. Manuel, Paul W. Fieguth, Tim Ramsay,\n  Venet Osmani, Bernhard Wernly, Carol Bennett, Steven Hawken, Matthew McInnes,\n  Olivia Magwood, Yusuf Sheikh, Andreas Holzinger", "title": "Deep ROC Analysis and AUC as Balanced Average Accuracy to Improve Model\n  Selection, Understanding and Interpretation", "comments": "14 pages, 6 Figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (TPAMI), currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimal performance is critical for decision-making tasks from medicine to\nautonomous driving, however common performance measures may be too general or\ntoo specific. For binary classifiers, diagnostic tests or prognosis at a\ntimepoint, measures such as the area under the receiver operating\ncharacteristic curve, or the area under the precision recall curve, are too\ngeneral because they include unrealistic decision thresholds. On the other\nhand, measures such as accuracy, sensitivity or the F1 score are measures at a\nsingle threshold that reflect an individual single probability or predicted\nrisk, rather than a range of individuals or risk. We propose a method in\nbetween, deep ROC analysis, that examines groups of probabilities or predicted\nrisks for more insightful analysis. We translate esoteric measures into\nfamiliar terms: AUC and the normalized concordant partial AUC are balanced\naverage accuracy (a new finding); the normalized partial AUC is average\nsensitivity; and the normalized horizontal partial AUC is average specificity.\nAlong with post-test measures, we provide a method that can improve model\nselection in some cases and provide interpretation and assurance for patients\nin each risk group. We demonstrate deep ROC analysis in two case studies and\nprovide a toolkit in Python.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:27:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carrington", "Andr\u00e9 M.", ""], ["Manuel", "Douglas G.", ""], ["Fieguth", "Paul W.", ""], ["Ramsay", "Tim", ""], ["Osmani", "Venet", ""], ["Wernly", "Bernhard", ""], ["Bennett", "Carol", ""], ["Hawken", "Steven", ""], ["McInnes", "Matthew", ""], ["Magwood", "Olivia", ""], ["Sheikh", "Yusuf", ""], ["Holzinger", "Andreas", ""]]}, {"id": "2103.11435", "submitter": "Ariel Neufeld", "authors": "Ariel Neufeld, Julian Sester", "title": "A deep learning approach to data-driven model-free pricing and to\n  martingale optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel and highly tractable supervised learning approach based\non neural networks that can be applied for the computation of model-free price\nbounds of, potentially high-dimensional, financial derivatives and for the\ndetermination of optimal hedging strategies attaining these bounds. In\nparticular, our methodology allows to train a single neural network offline and\nthen to use it online for the fast determination of model-free price bounds of\na whole class of financial derivatives with current market data. We show the\napplicability of this approach and highlight its accuracy in several examples\ninvolving real market data. Further, we show how a neural network can be\ntrained to solve martingale optimal transport problems involving fixed marginal\ndistributions instead of financial market data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 16:39:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Neufeld", "Ariel", ""], ["Sester", "Julian", ""]]}, {"id": "2103.11489", "submitter": "Sanae Amani", "authors": "Sanae Amani, Christos Thrampoulidis", "title": "UCB-based Algorithms for Multinomial Logistic Regression Bandits", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of the rich family of generalized linear bandits, perhaps the most well\nstudied ones are logisitc bandits that are used in problems with binary\nrewards: for instance, when the learner/agent tries to maximize the profit over\na user that can select one of two possible outcomes (e.g., `click' vs\n`no-click'). Despite remarkable recent progress and improved algorithms for\nlogistic bandits, existing works do not address practical situations where the\nnumber of outcomes that can be selected by the user is larger than two (e.g.,\n`click', `show me later', `never show again', `no click'). In this paper, we\nstudy such an extension. We use multinomial logit (MNL) to model the\nprobability of each one of $K+1\\geq 2$ possible outcomes (+1 stands for the\n`not click' outcome): we assume that for a learner's action $\\mathbf{x}_t$, the\nuser selects one of $K+1\\geq 2$ outcomes, say outcome $i$, with a multinomial\nlogit (MNL) probabilistic model with corresponding unknown parameter\n$\\bar{\\boldsymbol\\theta}_{\\ast i}$. Each outcome $i$ is also associated with a\nrevenue parameter $\\rho_i$ and the goal is to maximize the expected revenue.\nFor this problem, we present MNL-UCB, an upper confidence bound (UCB)-based\nalgorithm, that achieves regret $\\tilde{\\mathcal{O}}(dK\\sqrt{T})$ with small\ndependency on problem-dependent constants that can otherwise be arbitrarily\nlarge and lead to loose regret bounds. We present numerical simulations that\ncorroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 21:09:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Amani", "Sanae", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2103.11516", "submitter": "Guansong Pang", "authors": "Guansong Pang, Longbing Cao, Ling Chen", "title": "Homophily Outlier Detection in Non-IID Categorical Data", "comments": "To appear in Data Ming and Knowledge Discovery Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing outlier detection methods assume that the outlier factors\n(i.e., outlierness scoring measures) of data entities (e.g., feature values and\ndata objects) are Independent and Identically Distributed (IID). This\nassumption does not hold in real-world applications where the outlierness of\ndifferent entities is dependent on each other and/or taken from different\nprobability distributions (non-IID). This may lead to the failure of detecting\nimportant outliers that are too subtle to be identified without considering the\nnon-IID nature. The issue is even intensified in more challenging contexts,\ne.g., high-dimensional data with many noisy features. This work introduces a\nnovel outlier detection framework and its two instances to identify outliers in\ncategorical data by capturing non-IID outlier factors. Our approach first\ndefines and incorporates distribution-sensitive outlier factors and their\ninterdependence into a value-value graph-based representation. It then models\nan outlierness propagation process in the value graph to learn the outlierness\nof feature values. The learned value outlierness allows for either direct\noutlier detection or outlying feature selection. The graph representation and\nmining approach is employed here to well capture the rich non-IID\ncharacteristics. Our empirical results on 15 real-world data sets with\ndifferent levels of data complexities show that (i) the proposed outlier\ndetection methods significantly outperform five state-of-the-art methods at the\n95%/99% confidence level, achieving 10%-28% AUC improvement on the 10 most\ncomplex data sets; and (ii) the proposed feature selection methods\nsignificantly outperform three competing methods in enabling subsequent outlier\ndetection of two different existing detectors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:29:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pang", "Guansong", ""], ["Cao", "Longbing", ""], ["Chen", "Ling", ""]]}, {"id": "2103.11559", "submitter": "Fei Feng Ms.", "authors": "Fei Feng, Wotao Yin, Alekh Agarwal, Lin F. Yang", "title": "Provably Correct Optimization and Exploration with Non-linear Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy optimization methods remain a powerful workhorse in empirical\nReinforcement Learning (RL), with a focus on neural policies that can easily\nreason over complex and continuous state and/or action spaces. Theoretical\nunderstanding of strategic exploration in policy-based methods with non-linear\nfunction approximation, however, is largely missing. In this paper, we address\nthis question by designing ENIAC, an actor-critic method that allows non-linear\nfunction approximation in the critic. We show that under certain assumptions,\ne.g., a bounded eluder dimension $d$ for the critic class, the learner finds a\nnear-optimal policy in $O(\\poly(d))$ exploration rounds. The method is robust\nto model misspecification and strictly extends existing works on linear\nfunction approximation. We also develop some computational optimizations of our\napproach with slightly worse statistical guarantees and an empirical adaptation\nbuilding on existing deep RL tools. We empirically evaluate this adaptation and\nshow that it outperforms prior heuristics inspired by linear methods,\nestablishing the value via correctly reasoning about the agent's uncertainty\nunder non-linear function approximation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:16:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Feng", "Fei", ""], ["Yin", "Wotao", ""], ["Agarwal", "Alekh", ""], ["Yang", "Lin F.", ""]]}, {"id": "2103.11588", "submitter": "Noopur Dilip Jamnikar", "authors": "Noopur Jamnikar, Sen Liu, Craig Brice, and Xiaoli Zhang", "title": "Comprehensive process-molten pool relations modeling using CNN for\n  wire-feed laser additive manufacturing", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wire-feed laser additive manufacturing (WLAM) is gaining wide interest due to\nits high level of automation, high deposition rates, and good quality of\nprinted parts. In-process monitoring and feedback controls that would reduce\nthe uncertainty in the quality of the material are in the early stages of\ndevelopment. Machine learning promises the ability to accelerate the adoption\nof new processes and property design in additive manufacturing by making\nprocess-structure-property connections between process setting inputs and\nmaterial quality outcomes. The molten pool dimensional information and\ntemperature are the indicators for achieving the high quality of the build,\nwhich can be directly controlled by processing parameters. For the purpose of\nin situ quality control, the process parameters should be controlled in\nreal-time based on sensed information from the process, in particular the\nmolten pool. Thus, the molten pool-process relations are of preliminary\nimportance. This paper analyzes experimentally collected in situ sensing data\nfrom the molten pool under a set of controlled process parameters in a WLAM\nsystem. The variations in the steady-state and transient state of the molten\npool are presented with respect to the change of independent process\nparameters. A multi-modality convolutional neural network (CNN) architecture is\nproposed for predicting the control parameter directly from the measurable\nmolten pool sensor data for achieving desired geometric and microstructural\nproperties. Dropout and regularization are applied to the CNN architecture to\navoid the problem of overfitting. The results highlighted that the multi-modal\nCNN, which receives temperature profile as an external feature to the features\nextracted from the image data, has improved prediction performance compared to\nthe image-based uni-modality CNN approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:27:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jamnikar", "Noopur", ""], ["Liu", "Sen", ""], ["Brice", "Craig", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "2103.11598", "submitter": "Li Yang", "authors": "Li Yang", "title": "Adaptive Degradation Process with Deep Learning-Driven Trajectory", "comments": "This work will be submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remaining useful life (RUL) estimation is a crucial component in the\nimplementation of intelligent predictive maintenance and health management.\nDeep neural network (DNN) approaches have been proven effective in RUL\nestimation due to their capacity in handling high-dimensional non-linear\ndegradation features. However, the applications of DNN in practice face two\nchallenges: (a) online update of lifetime information is often unavailable, and\n(b) uncertainties in predicted values may not be analytically quantified. This\npaper addresses these issues by developing a hybrid DNN-based prognostic\napproach, where a Wiener-based-degradation model is enhanced with adaptive\ndrift to characterize the system degradation. An LSTM-CNN encoder-decoder is\ndeveloped to predict future degradation trajectories by jointly learning noise\ncoefficients as well as drift coefficients, and adaptive drift is updated via\nBayesian inference. A computationally efficient algorithm is proposed for the\ncalculation of RUL distributions. Numerical experiments are presented using\nturbofan engines degradation data to demonstrate the superior accuracy of RUL\nprediction of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:00:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yang", "Li", ""]]}, {"id": "2103.11648", "submitter": "Lukas Prediger", "authors": "Lukas Prediger, Niki Loppi, Samuel Kaski, Antti Honkela", "title": "d3p -- A Python Package for Differentially-Private Probabilistic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present d3p, a software package designed to help fielding runtime\nefficient widely-applicable Bayesian inference under differential privacy\nguarantees. d3p achieves general applicability to a wide range of probabilistic\nmodelling problems by implementing the differentially private variational\ninference algorithm, allowing users to fit any parametric probabilistic model\nwith a differentiable density function. d3p adopts the probabilistic\nprogramming paradigm as a powerful way for the user to flexibly define such\nmodels. We demonstrate the use of our software on a hierarchical logistic\nregression example, showing the expressiveness of the modelling approach as\nwell as the ease of running the parameter inference. We also perform an\nempirical evaluation of the runtime of the private inference on a complex model\nand find an $\\sim$10 fold speed-up compared to an implementation using\nTensorFlow Privacy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:15:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Prediger", "Lukas", ""], ["Loppi", "Niki", ""], ["Kaski", "Samuel", ""], ["Honkela", "Antti", ""]]}, {"id": "2103.11678", "submitter": "Michela Carlotta Massi", "authors": "Michela C. Massi, Francesca Ieva, Francesca Gasperoni and Anna Maria\n  Paganoni", "title": "Feature Selection for Imbalanced Data with Deep Sparse Autoencoders\n  Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a common issue in many domain applications of learning\nalgorithms. Oftentimes, in the same domains it is much more relevant to\ncorrectly classify and profile minority class observations. This need can be\naddressed by Feature Selection (FS), that offers several further advantages,\ns.a. decreasing computational costs, aiding inference and interpretability.\nHowever, traditional FS techniques may become sub-optimal in the presence of\nstrongly imbalanced data. To achieve FS advantages in this setting, we propose\na filtering FS algorithm ranking feature importance on the basis of the\nReconstruction Error of a Deep Sparse AutoEncoders Ensemble (DSAEE). We use\neach DSAE trained only on majority class to reconstruct both classes. From the\nanalysis of the aggregated Reconstruction Error, we determine the features\nwhere the minority class presents a different distribution of values w.r.t. the\noverrepresented one, thus identifying the most relevant features to\ndiscriminate between the two. We empirically demonstrate the efficacy of our\nalgorithm in several experiments on high-dimensional datasets of varying sample\nsize, showcasing its capability to select relevant and generalizable features\nto profile and classify minority class, outperforming other benchmark FS\nmethods. We also briefly present a real application in radiogenomics, where the\nmethodology was applied successfully.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:17:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Massi", "Michela C.", ""], ["Ieva", "Francesca", ""], ["Gasperoni", "Francesca", ""], ["Paganoni", "Anna Maria", ""]]}, {"id": "2103.11706", "submitter": "Mario W\\\"uthrich V.", "authors": "M. Merz, R. Richman, T. Tsanakas, M.V. W\\\"uthrich", "title": "Interpreting Deep Learning Models with Marginal Attribution by\n  Conditioning on Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vastly growing literature on explaining deep learning models has emerged.\nThis paper contributes to that literature by introducing a global\ngradient-based model-agnostic method, which we call Marginal Attribution by\nConditioning on Quantiles (MACQ). Our approach is based on analyzing the\nmarginal attribution of predictions (outputs) to individual features (inputs).\nSpecificalllly, we consider variable importance by mixing (global) output\nlevels and, thus, explain how features marginally contribute across different\nregions of the prediction space. Hence, MACQ can be seen as a marginal\nattribution counterpart to approaches such as accumulated local effects (ALE),\nwhich study the sensitivities of outputs by perturbing inputs. Furthermore,\nMACQ allows us to separate marginal attribution of individual features from\ninteraction effect, and visually illustrate the 3-way relationship between\nmarginal attribution, output level, and feature value.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:20:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Merz", "M.", ""], ["Richman", "R.", ""], ["Tsanakas", "T.", ""], ["W\u00fcthrich", "M. V.", ""]]}, {"id": "2103.11749", "submitter": "The Tien Mai", "authors": "The Tien Mai", "title": "Numerical comparisons between Bayesian and frequentist low-rank matrix\n  completion: estimation accuracy and uncertainty quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we perform a numerious numerical studies for the problem of\nlow-rank matrix completion. We compare the Bayesain approaches and a recently\nintroduced de-biased estimator which provides a useful way to build confidence\nintervals of interest. From a theoretical viewpoint, the de-biased estimator\ncomes with a sharp minimax-optinmal rate of estimation error whereas the\nBayesian approach reaches this rate with an additional logarithmic factor. Our\nsimulation studies show originally interesting results that the de-biased\nestimator is just as good as the Bayesain estimators. Moreover, Bayesian\napproaches are much more stable and can outperform the de-biased estimator in\nthe case of small samples. However, we also find that the length of the\nconfidence intervals revealed by the de-biased estimator for an entry is\nabsolutely shorter than the length of the considered credible interval. These\nsuggest further theoretical studies on the estimation error and the\nconcentration for Bayesian methods as they are being quite limited up to\npresent.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:02:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mai", "The Tien", ""]]}, {"id": "2103.11774", "submitter": "Noah Daniels", "authors": "Najib Ishaq, Thomas J. Howard III, Noah M. Daniels", "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms", "comments": "As submitted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly and outlier detection in datasets is a long-standing problem in\nmachine learning. In some cases, anomaly detection is easy, such as when data\nare drawn from well-characterized distributions such as the Gaussian. However,\nwhen data occupy high-dimensional spaces, anomaly detection becomes more\ndifficult. We present CLAM (Clustered Learning of Approximate Manifolds), a\nfast hierarchical clustering technique that learns a manifold in a Banach space\ndefined by a distance metric. CLAM induces a graph from the cluster tree, based\non overlapping clusters determined by several geometric and topological\nfeatures. On these graphs, we implement CHAODA (Clustered Hierarchical Anomaly\nand Outlier Detection Algorithms), exploring various properties of the graphs\nand their constituent clusters to compute scores of anomalousness. On 24\npublicly available datasets, we compare the performance of CHAODA (by measure\nof ROC AUC) to a variety of state-of-the-art unsupervised anomaly-detection\nalgorithms. Six of the datasets are used for training. CHAODA outperforms other\napproaches on 14 of the remaining 18 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:27:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ishaq", "Najib", ""], ["Howard", "Thomas J.", "III"], ["Daniels", "Noah M.", ""]]}, {"id": "2103.11785", "submitter": "Miranda C. N. Cheng", "authors": "Vassilis Anagiannis and Miranda C. N. Cheng", "title": "Entangled q-Convolutional Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a machine learning model, the q-CNN model, sharing key features\nwith convolutional neural networks and admitting a tensor network description.\nAs examples, we apply q-CNN to the MNIST and Fashion MNIST classification\ntasks. We explain how the network associates a quantum state to each\nclassification label, and study the entanglement structure of these network\nstates. In both our experiments on the MNIST and Fashion-MNIST datasets, we\nobserve a distinct increase in both the left/right as well as the up/down\nbipartition entanglement entropy during training as the network learns the fine\nfeatures of the data. More generally, we observe a universal negative\ncorrelation between the value of the entanglement entropy and the value of the\ncost function, suggesting that the network needs to learn the entanglement\nstructure in order the perform the task accurately. This supports the\npossibility of exploiting the entanglement structure as a guide to design the\nmachine learning algorithm suitable for given tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:35:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Anagiannis", "Vassilis", ""], ["Cheng", "Miranda C. N.", ""]]}, {"id": "2103.11802", "submitter": "Zhanlin Chen", "authors": "Zhanlin Chen, Jeremy Goldwasser, Philip Tuckman, Jing Zhang, Mark\n  Gerstein", "title": "Forest Fire Clustering: Iterative Label Propagation Clustering and Monte\n  Carlo Validation For Single-cell Sequencing Analysis", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rise of single-cell sequencing technologies, there is a growing need\nfor robust clustering algorithms to extract deeper insights from data. Here, we\nintroduce an intuitive and efficient clustering method, Forest Fire Clustering,\nfor discovering and validating cell types in single-cell sequencing analysis.\nCompared to existing methods, our clustering algorithm makes minimum prior\nassumptions about the data distribution and can provide a point-wise\nsignificance value via Monte Carlo simulations for internal validation.\nAdditionally, point-wise label entropies can highlight novel transition cell\ntypes \\emph{de novo} along developmental pseudo-time manifolds. Lastly, our\ninductive algorithm has the ability to make robust inferences in an\nonline-learning context. In this paper, we describe the method, provide a\nsummary of its performance against common clustering benchmarks, and\ndemonstrate that Forest Fire Clustering is uniquely suitable for single-cell\nsequencing analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:02:37 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:21:31 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 21:01:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chen", "Zhanlin", ""], ["Goldwasser", "Jeremy", ""], ["Tuckman", "Philip", ""], ["Zhang", "Jing", ""], ["Gerstein", "Mark", ""]]}, {"id": "2103.11821", "submitter": "Joonas Sova", "authors": "J\\\"uri Lember, Joonas Sova", "title": "Regenerativity of Viterbi process for pairwise Markov models", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.03799", "journal-ref": "Journal of Theoretical Probability volume 34 (2021)", "doi": "10.1007/s10959-020-01022-z", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hidden Markov models one of the most popular estimates of the hidden\nchain is the Viterbi path -- the path maximising the posterior probability. We\nconsider a more general setting, called the pairwise Markov model (PMM), where\nthe joint process consisting of finite-state hidden process and observation\nprocess is assumed to be a Markov chain. It has been recently proven that under\nsome conditions the Viterbi path of the PMM can almost surely be extended to\ninfinity, thereby defining the infinite Viterbi decoding of the observation\nsequence, called the Viterbi process. This was done by constructing a block of\nobservations, called a barrier, which ensures that the Viterbi path goes trough\na given state whenever this block occurs in the observation sequence. In this\npaper we prove that the joint process consisting of Viterbi process and PMM is\nregenerative. The proof involves a delicate construction of regeneration times\nwhich coincide with the occurrences of barriers. As one possible application of\nour theory, some results on the asymptotics of the Viterbi training algorithm\nare derived.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:01:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Sova", "Joonas", ""]]}, {"id": "2103.11860", "submitter": "Yi-Shuai Niu", "authors": "Yi-Shuai Niu, Wentao Ding, Junpeng Hu, Wenxu Xu and Stephane Canu", "title": "Spatio-Temporal Neural Network for Fitting and Forecasting COVID-19", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We established a Spatio-Temporal Neural Network, namely STNN, to forecast the\nspread of the coronavirus COVID-19 outbreak worldwide in 2020. The basic\nstructure of STNN is similar to the Recurrent Neural Network (RNN)\nincorporating with not only temporal data but also spatial features. Two\nimproved STNN architectures, namely the STNN with Augmented Spatial States\n(STNN-A) and the STNN with Input Gate (STNN-I), are proposed, which ensure more\npredictability and flexibility. STNN and its variants can be trained using\nStochastic Gradient Descent (SGD) algorithm and its improved variants (e.g.,\nAdam, AdaGrad and RMSProp). Our STNN models are compared with several classical\nepidemic prediction models, including the fully-connected neural network\n(BPNN), and the recurrent neural network (RNN), the classical curve fitting\nmodels, as well as the SEIR dynamical system model. Numerical simulations\ndemonstrate that STNN models outperform many others by providing more accurate\nfitting and prediction, and by handling both spatial and temporal data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:59:14 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Niu", "Yi-Shuai", ""], ["Ding", "Wentao", ""], ["Hu", "Junpeng", ""], ["Xu", "Wenxu", ""], ["Canu", "Stephane", ""]]}, {"id": "2103.11864", "submitter": "Jian Vora", "authors": "Jian Vora, Karthik S. Gurumoorthy, Ajit Rajwade", "title": "Recovery of Joint Probability Distribution from one-way marginals: Low\n  rank Tensors and Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint probability mass function (PMF) estimation is a fundamental machine\nlearning problem. The number of free parameters scales exponentially with\nrespect to the number of random variables. Hence, most work on nonparametric\nPMF estimation is based on some structural assumptions such as clique\nfactorization adopted by probabilistic graphical models, imposition of low rank\non the joint probability tensor and reconstruction from 3-way or 2-way\nmarginals, etc. In the present work, we link random projections of data to the\nproblem of PMF estimation using ideas from tomography. We integrate this idea\nwith the idea of low-rank tensor decomposition to show that we can estimate the\njoint density from just one-way marginals in a transformed space. We provide a\nnovel algorithm for recovering factors of the tensor from one-way marginals,\ntest it across a variety of synthetic and real-world datasets, and also perform\nMAP inference on the estimated model for classification.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:00:57 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 11:40:42 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Vora", "Jian", ""], ["Gurumoorthy", "Karthik S.", ""], ["Rajwade", "Ajit", ""]]}, {"id": "2103.11869", "submitter": "Yiyan Huang", "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu", "title": "Higher-Order Orthogonal Causal Learning for Treatment Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing studies on the double/debiased machine learning method\nconcentrate on the causal parameter estimation recovering from the first-order\northogonal score function. In this paper, we will construct the\n$k^{\\mathrm{th}}$-order orthogonal score function for estimating the average\ntreatment effect (ATE) and present an algorithm that enables us to obtain the\ndebiased estimator recovered from the score function. Such a higher-order\northogonal estimator is more robust to the misspecification of the propensity\nscore than the first-order one does. Besides, it has the merit of being\napplicable with many machine learning methodologies such as Lasso, Random\nForests, Neural Nets, etc. We also undergo comprehensive experiments to test\nthe power of the estimator we construct from the score function using both the\nsimulated datasets and the real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:04:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Yiyan", ""], ["Leung", "Cheuk Hang", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""]]}, {"id": "2103.11937", "submitter": "Mourad El Hamri", "authors": "Mourad El Hamri, Youn\\`es Bennani", "title": "Regularized Optimal Transport for Dynamic Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised learning provides an effective paradigm for leveraging\nunlabeled data to improve a model's performance. Among the many strategies\nproposed, graph-based methods have shown excellent properties, in particular\nsince they allow to solve directly the transductive tasks according to Vapnik's\nprinciple and they can be extended efficiently for inductive tasks. In this\npaper, we propose a novel approach for the transductive semi-supervised\nlearning, using a complete bipartite edge-weighted graph. The proposed approach\nuses the regularized optimal transport between empirical measures defined on\nlabelled and unlabelled data points in order to obtain an affinity matrix from\nthe optimal transport plan. This matrix is further used to propagate labels\nthrough the vertices of the graph in an incremental process ensuring the\ncertainty of the predictions by incorporating a certainty score based on\nShannon's entropy. We also analyze the convergence of our approach and we\nderive an efficient way to extend it for out-of-sample data. Experimental\nanalysis was used to compare the proposed approach with other label propagation\nalgorithms on 12 benchmark datasets, for which we surpass state-of-the-art\nresults. We release our code.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:31:53 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 20:38:13 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hamri", "Mourad El", ""], ["Bennani", "Youn\u00e8s", ""]]}, {"id": "2103.11948", "submitter": "Phillip Murray", "authors": "Hans Buehler, Phillip Murray, Mikko S. Pakkanen, Ben Wood", "title": "Deep Hedging: Learning Risk-Neutral Implied Volatility Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.OC q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a numerically efficient approach for learning a risk-neutral\nmeasure for paths of simulated spot and option prices up to a finite horizon\nunder convex transaction costs and convex trading constraints. This approach\ncan then be used to implement a stochastic implied volatility model in the\nfollowing two steps: 1. Train a market simulator for option prices, as\ndiscussed for example in our recent; 2. Find a risk-neutral density,\nspecifically the minimal entropy martingale measure. The resulting model can be\nused for risk-neutral pricing, or for Deep Hedging in the case of transaction\ncosts or trading constraints. To motivate the proposed approach, we also show\nthat market dynamics are free from \"statistical arbitrage\" in the absence of\ntransaction costs if and only if they follow a risk-neutral measure. We\nadditionally provide a more general characterization in the presence of convex\ntransaction costs and trading constraints. These results can be seen as an\nanalogue of the fundamental theorem of asset pricing for statistical arbitrage\nunder trading frictions and are of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:38:25 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:43:52 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 13:33:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Buehler", "Hans", ""], ["Murray", "Phillip", ""], ["Pakkanen", "Mikko S.", ""], ["Wood", "Ben", ""]]}, {"id": "2103.11988", "submitter": "Radu Tudor Ionescu", "authors": "Nicolae-Catalin Ristea, Radu Tudor Ionescu", "title": "Self-paced ensemble learning for speech and audio classification", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining multiple machine learning models into an ensemble is known to\nprovide superior performance levels compared to the individual components\nforming the ensemble. This is because models can complement each other in\ntaking better decisions. Instead of just combining the models, we propose a\nself-paced ensemble learning scheme in which models learn from each other over\nseveral iterations. During the self-paced learning process based on\npseudo-labeling, in addition to improving the individual models, our ensemble\nalso gains knowledge about the target domain. To demonstrate the generality of\nour self-paced ensemble learning (SPEL) scheme, we conduct experiments on three\naudio tasks. Our empirical results indicate that SPEL significantly outperforms\nthe baseline ensemble models. We also show that applying self-paced learning on\nindividual models is less effective, illustrating the idea that models in the\nensemble actually learn from each other.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 16:34:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:22:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ristea", "Nicolae-Catalin", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2103.12019", "submitter": "Yiqun Xie", "authors": "Yiqun Xie, Shashi Shekhar, Yan Li", "title": "Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots:\n  A Survey", "comments": "36 pages, 5 figures, submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping of spatial hotspots, i.e., regions with significantly higher rates or\nprobability density of generating certain events (e.g., disease or crime\ncases), is a important task in diverse societal domains, including public\nhealth, public safety, transportation, agriculture, environmental science, etc.\nClustering techniques required by these domains differ from traditional\nclustering methods due to the high economic and social costs of spurious\nresults (e.g., false alarms of crime clusters). As a result, statistical rigor\nis needed explicitly to control the rate of spurious detections. To address\nthis challenge, techniques for statistically-robust clustering have been\nextensively studied by the data mining and statistics communities. In this\nsurvey we present an up-to-date and detailed review of the models and\nalgorithms developed by this field. We first present a general taxonomy of the\nclustering process with statistical rigor, covering key steps of data and\nstatistical modeling, region enumeration and maximization, significance\ntesting, and data update. We further discuss different paradigms and methods\nwithin each of key steps. Finally, we highlight research gaps and potential\nfuture directions, which may serve as a stepping stone in generating new ideas\nand thoughts in this growing field and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:22:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xie", "Yiqun", ""], ["Shekhar", "Shashi", ""], ["Li", "Yan", ""]]}, {"id": "2103.12020", "submitter": "Duo Xu", "authors": "Duo Xu, Faramarz Fekri", "title": "Improving Actor-Critic Reinforcement Learning via Hamiltonian Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximating optimal policies in reinforcement learning (RL) is often\nnecessary in many real-world scenarios, which is termed as policy optimization.\nBy viewing the reinforcement learning from the perspective of variational\ninference (VI), the policy network is trained to obtain the approximate\nposterior of actions given the optimality criteria. However, in practice, the\npolicy optimization may lead to suboptimal policy estimates due to the\namortization gap and insufficient exploration. In this work, inspired by the\nprevious use of Hamiltonian Monte Carlo (HMC) in VI, we propose to integrate\npolicy optimization with HMC. As such we choose evolving actions from the base\npolicy according to HMC, which has two benefits: i) HMC can improve the policy\ndistribution to better approximate the posterior and hence reduces the\namortization gap; ii) HMC can also guide the exploration more to the regions\nwith higher action values, enhancing the exploration efficiency. Instead of\ndirectly applying HMC into RL, we propose a new leapfrog operator to simulate\nthe Hamiltonian dynamics. With comprehensive empirical experiments on\ncontinuous control baselines, including MuJoCo and PyBullet Roboschool, we show\nthat the proposed approach is a data-efficient, and an easy-to-implement\nimprovement over previous policy optimization methods. Besides, the proposed\napproach can also outperform previous methods on DeepMind Control Suite which\nhas image-based high-dimensional observation space.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:26:43 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 19:16:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xu", "Duo", ""], ["Fekri", "Faramarz", ""]]}, {"id": "2103.12021", "submitter": "Paria Rashidinejad", "authors": "Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell", "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale\n  of Pessimism", "comments": "84 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline (or batch) reinforcement learning (RL) algorithms seek to learn an\noptimal policy from a fixed dataset without active data collection. Based on\nthe composition of the offline dataset, two main categories of methods are\nused: imitation learning which is suitable for expert datasets and vanilla\noffline RL which often requires uniform coverage datasets. From a practical\nstandpoint, datasets often deviate from these two extremes and the exact data\ncomposition is usually unknown a priori. To bridge this gap, we present a new\noffline RL framework that smoothly interpolates between the two extremes of\ndata composition, hence unifying imitation learning and vanilla offline RL. The\nnew framework is centered around a weak version of the concentrability\ncoefficient that measures the deviation from the behavior policy to the expert\npolicy alone.\n  Under this new framework, we further investigate the question on algorithm\ndesign: can one develop an algorithm that achieves a minimax optimal rate and\nalso adapts to unknown data composition? To address this question, we consider\na lower confidence bound (LCB) algorithm developed based on pessimism in the\nface of uncertainty in offline RL. We study finite-sample properties of LCB as\nwell as information-theoretic limits in multi-armed bandits, contextual\nbandits, and Markov decision processes (MDPs). Our analysis reveals surprising\nfacts about optimality rates. In particular, in all three settings, LCB\nachieves a faster rate of $1/N$ for nearly-expert datasets compared to the\nusual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the number of samples in\nthe batch dataset. In the case of contextual bandits with at least two\ncontexts, we prove that LCB is adaptively optimal for the entire data\ncomposition range, achieving a smooth transition from imitation learning to\noffline RL. We further show that LCB is almost adaptively optimal in MDPs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:27:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rashidinejad", "Paria", ""], ["Zhu", "Banghua", ""], ["Ma", "Cong", ""], ["Jiao", "Jiantao", ""], ["Russell", "Stuart", ""]]}, {"id": "2103.12188", "submitter": "Qing Zhou", "authors": "Jireh Huang and Qing Zhou", "title": "Partitioned hybrid learning of Bayesian network structures", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel hybrid method for Bayesian network structure learning\ncalled partitioned hybrid greedy search (pHGS), composed of three distinct yet\ncompatible new algorithms: Partitioned PC (pPC) accelerates skeleton learning\nvia a divide-and-conquer strategy, $p$-value adjacency thresholding (PATH)\neffectively accomplishes parameter tuning with a single execution, and hybrid\ngreedy initialization (HGI) maximally utilizes constraint-based information to\nobtain a high-scoring and well-performing initial graph for greedy search. We\nestablish structure learning consistency of our algorithms in the large-sample\nlimit, and empirically validate our methods individually and collectively\nthrough extensive numerical comparisons. The combined merits of pPC and PATH\nachieve significant computational reductions compared to the PC algorithm\nwithout sacrificing the accuracy of estimated structures, and our generally\napplicable HGI strategy reliably improves the estimation structural accuracy of\npopular hybrid algorithms with negligible additional computational expense. Our\nempirical results demonstrate the superior empirical performance of pHGS\nagainst many state-of-the-art structure learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 21:34:52 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Huang", "Jireh", ""], ["Zhou", "Qing", ""]]}, {"id": "2103.12197", "submitter": "Vittorio Giammarino", "authors": "Vittorio Giammarino and Ioannis Ch. Paschalidis", "title": "Online Baum-Welch algorithm for Hierarchical Imitation Learning", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The options framework for hierarchical reinforcement learning has increased\nits popularity in recent years and has made improvements in tackling the\nscalability problem in reinforcement learning. Yet, most of these recent\nsuccesses are linked with a proper options initialization or discovery. When an\nexpert is available, the options discovery problem can be addressed by learning\nan options-type hierarchical policy directly from expert demonstrations. This\nproblem is referred to as hierarchical imitation learning and can be handled as\nan inference problem in a Hidden Markov Model, which is done via an\nExpectation-Maximization type algorithm. In this work, we propose a novel\nonline algorithm to perform hierarchical imitation learning in the options\nframework. Further, we discuss the benefits of such an algorithm and compare it\nwith its batch version in classical reinforcement learning benchmarks. We show\nthat this approach works well in both discrete and continuous environments and,\nunder certain conditions, it outperforms the batch version.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:03:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Giammarino", "Vittorio", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "2103.12243", "submitter": "Ayoub El Hanchi", "authors": "Ayoub El Hanchi, David A. Stephens", "title": "Adaptive Importance Sampling for Finite-Sum Optimization and Sampling\n  with Decreasing Step-Sizes", "comments": "Advances in Neural Information Processing Systems, Dec 2020,\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the variance of the gradient estimator is known to improve the\nconvergence rate of stochastic gradient-based optimization and sampling\nalgorithms. One way of achieving variance reduction is to design importance\nsampling strategies. Recently, the problem of designing such schemes was\nformulated as an online learning problem with bandit feedback, and algorithms\nwith sub-linear static regret were designed. In this work, we build on this\nframework and propose Avare, a simple and efficient algorithm for adaptive\nimportance sampling for finite-sum optimization and sampling with decreasing\nstep-sizes. Under standard technical conditions, we show that Avare achieves\n$\\mathcal{O}(T^{2/3})$ and $\\mathcal{O}(T^{5/6})$ dynamic regret for SGD and\nSGLD respectively when run with $\\mathcal{O}(1/t)$ step sizes. We achieve this\ndynamic regret bound by leveraging our knowledge of the dynamics defined by the\nalgorithm, and combining ideas from online learning and variance-reduced\nstochastic optimization. We validate empirically the performance of our\nalgorithm and identify settings in which it leads to significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 00:28:15 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hanchi", "Ayoub El", ""], ["Stephens", "David A.", ""]]}, {"id": "2103.12293", "submitter": "Ayoub El Hanchi", "authors": "Ayoub El Hanchi, David A. Stephens", "title": "Stochastic Reweighted Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the strong theoretical guarantees that variance-reduced finite-sum\noptimization algorithms enjoy, their applicability remains limited to cases\nwhere the memory overhead they introduce (SAG/SAGA), or the periodic full\ngradient computation they require (SVRG/SARAH) are manageable. A promising\napproach to achieving variance reduction while avoiding these drawbacks is the\nuse of importance sampling instead of control variates. While many such methods\nhave been proposed in the literature, directly proving that they improve the\nconvergence of the resulting optimization algorithm has remained elusive. In\nthis work, we propose an importance-sampling-based algorithm we call SRG\n(stochastic reweighted gradient). We analyze the convergence of SRG in the\nstrongly-convex case and show that, while it does not recover the linear rate\nof control variates methods, it provably outperforms SGD. We pay particular\nattention to the time and memory overhead of our proposed method, and design a\nspecialized red-black tree allowing its efficient implementation. Finally, we\npresent empirical results to support our findings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:09:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hanchi", "Ayoub El", ""], ["Stephens", "David A.", ""]]}, {"id": "2103.12323", "submitter": "Nassir Mohammad", "authors": "Nassir Mohammad", "title": "Anomaly detection using principles of human perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the fields of statistics and unsupervised machine learning a fundamental\nand well-studied problem is anomaly detection. Although anomalies are difficult\nto define, many algorithms have been proposed. Underlying the approaches is the\nnebulous understanding that anomalies are rare, unusual or inconsistent with\nthe majority of data. The present work gives a philosophical approach to\nclearly define anomalies and to develop an algorithm for their efficient\ndetection with minimal user intervention. Inspired by the Gestalt School of\nPsychology and the Helmholtz principle of human perception, the idea is to\nassume anomalies are observations that are unexpected to occur with respect to\ncertain groupings made by the majority of the data. Thus, under appropriate\nrandom variable modelling anomalies are directly found in a set of data under a\nuniform and independent random assumption of the distribution of constituent\nelements of the observations; anomalies correspond to those observations where\nthe expectation of occurrence of the elements in a given view is $<1$. Starting\nfrom fundamental principles of human perception an unsupervised anomaly\ndetection algorithm is developed that is simple, real-time and parameter-free.\nExperiments suggest it as the prime choice for univariate data and it shows\npromising performance on the detection of global anomalies in multivariate\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:46:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mohammad", "Nassir", ""]]}, {"id": "2103.12345", "submitter": "Yijian Chuan", "authors": "Yijian Chuan, Chaoyi Zhao, Zhenrui He, and Lan Wu", "title": "The Success of AdaBoost and Its Application in Portfolio Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approach to explain why AdaBoost is a successful\nclassifier. By introducing a measure of the influence of the noise points (ION)\nin the training data for the binary classification problem, we prove that there\nis a strong connection between the ION and the test error. We further identify\nthat the ION of AdaBoost decreases as the iteration number or the complexity of\nthe base learners increases. We confirm that it is impossible to obtain a\nconsistent classifier without deep trees as the base learners of AdaBoost in\nsome complicated situations. We apply AdaBoost in portfolio management via\nempirical studies in the Chinese market, which corroborates our theoretical\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:41:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chuan", "Yijian", ""], ["Zhao", "Chaoyi", ""], ["He", "Zhenrui", ""], ["Wu", "Lan", ""]]}, {"id": "2103.12368", "submitter": "Thomas Konstantinovsky", "authors": "Thomas Konstantinovsky", "title": "A New Approach To Text Rating Classification Using Sentiment Analysis", "comments": "9 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typical use cases of sentiment analysis usually revolve around assessing the\nprobability of a text belonging to a certain sentiment and deriving insight\nconcerning it; little work has been done to explore further use cases derived\nusing those probabilities in the context of rating. In this paper, we redefine\nthe sentiment proportion values as building blocks for a triangle structure,\nallowing us to derive variables for a new formula for classifying text given in\nthe form of product reviews into a group of higher and a group of lower ratings\nand prove a dependence exists between the sentiments and the ratings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:10:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:13:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Konstantinovsky", "Thomas", ""]]}, {"id": "2103.12452", "submitter": "Rianne de Heide", "authors": "Rianne de Heide and James Cheshire and Pierre M\\'enard and Alexandra\n  Carpentier", "title": "Bandits with many optimal arms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a stochastic bandit problem with a possibly infinite number of\narms. We write $p^*$ for the proportion of optimal arms and $\\Delta$ for the\nminimal mean-gap between optimal and sub-optimal arms. We characterize the\noptimal learning rates both in the cumulative regret setting, and in the\nbest-arm identification setting in terms of the problem parameters $T$ (the\nbudget), $p^*$ and $\\Delta$. For the objective of minimizing the cumulative\nregret, we provide a lower bound of order $\\Omega(\\log(T)/(p^*\\Delta))$ and a\nUCB-style algorithm with matching upper bound up to a factor of\n$\\log(1/\\Delta)$. Our algorithm needs $p^*$ to calibrate its parameters, and we\nprove that this knowledge is necessary, since adapting to $p^*$ in this setting\nis impossible. For best-arm identification we also provide a lower bound of\norder $\\Omega(\\exp(-cT\\Delta^2p^*))$ on the probability of outputting a\nsub-optimal arm where $c>0$ is an absolute constant. We also provide an\nelimination algorithm with an upper bound matching the lower bound up to a\nfactor of order $\\log(1/\\Delta)$ in the exponential, and that does not need\n$p^*$ or $\\Delta$ as parameter.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:02:31 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["de Heide", "Rianne", ""], ["Cheshire", "James", ""], ["M\u00e9nard", "Pierre", ""], ["Carpentier", "Alexandra", ""]]}, {"id": "2103.12487", "submitter": "Saeed Masoudian", "authors": "Saeed Masoudian, Yevgeny Seldin", "title": "Improved Analysis of Robustness of the Tsallis-INF Algorithm to\n  Adversarial Corruptions in Stochastic Multiarmed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive improved regret bounds for the Tsallis-INF algorithm of Zimmert and\nSeldin (2021). In the adversarial regime with a self-bounding constraint and\nthe stochastic regime with adversarial corruptions as its special case we\nimprove the dependence on corruption magnitude $C$. In particular, for $C =\n\\Theta\\left(\\frac{T}{\\log T}\\right)$, where $T$ is the time horizon, we achieve\nan improvement by a multiplicative factor of $\\sqrt{\\frac{\\log T}{\\log\\log T}}$\nrelative to the bound of Zimmert and Seldin (2021). We also improve the\ndependence of the regret bound on time horizon from $\\log T$ to $\\log\n\\frac{(K-1)T}{(\\sum_{i\\neq i^*}\\frac{1}{\\Delta_i})^2}$, where $K$ is the number\nof arms, $\\Delta_i$ are suboptimality gaps for suboptimal arms $i$, and $i^*$\nis the optimal arm. Additionally, we provide a general analysis, which allows\nto achieve the same kind of improvement for generalizations of Tsallis-INF to\nother settings beyond multiarmed bandits.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:26:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Masoudian", "Saeed", ""], ["Seldin", "Yevgeny", ""]]}, {"id": "2103.12528", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal,\n  Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, Fabio\n  Petroni", "title": "Multilingual Autoregressive Entity Linking", "comments": "20 pages, 8 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present mGENRE, a sequence-to-sequence system for the Multilingual Entity\nLinking (MEL) problem -- the task of resolving language-specific mentions to a\nmultilingual Knowledge Base (KB). For a mention in a given language, mGENRE\npredicts the name of the target entity left-to-right, token-by-token in an\nautoregressive fashion. The autoregressive formulation allows us to effectively\ncross-encode mention string and entity names to capture more interactions than\nthe standard dot product between mention and entity vectors. It also enables\nfast search within a large KB even for mentions that do not appear in mention\ntables and with no need for large-scale vector indices. While prior MEL works\nuse a single representation for each entity, we match against entity names of\nas many languages as possible, which allows exploiting language connections\nbetween source input and target name. Moreover, in a zero-shot setting on\nlanguages with no training data at all, mGENRE treats the target language as a\nlatent variable that is marginalized at prediction time. This leads to over 50%\nimprovements in average accuracy. We show the efficacy of our approach through\nextensive evaluation including experiments on three popular MEL benchmarks\nwhere mGENRE establishes new state-of-the-art results. Code and pre-trained\nmodels at https://github.com/facebookresearch/GENRE.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:25:55 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["De Cao", "Nicola", ""], ["Wu", "Ledell", ""], ["Popat", "Kashyap", ""], ["Artetxe", "Mikel", ""], ["Goyal", "Naman", ""], ["Plekhanov", "Mikhail", ""], ["Zettlemoyer", "Luke", ""], ["Cancedda", "Nicola", ""], ["Riedel", "Sebastian", ""], ["Petroni", "Fabio", ""]]}, {"id": "2103.12531", "submitter": "Leon Bungert", "authors": "Leon Bungert, Ren\\'e Raab, Tim Roith, Leo Schwinn, Daniel Tenbrinck", "title": "CLIP: Cheap Lipschitz Training of Neural Networks", "comments": "12 pages, 2 figures, accepted at SSVM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the large success of deep neural networks (DNN) in recent years, most\nneural networks still lack mathematical guarantees in terms of stability. For\ninstance, DNNs are vulnerable to small or even imperceptible input\nperturbations, so called adversarial examples, that can cause false\npredictions. This instability can have severe consequences in applications\nwhich influence the health and safety of humans, e.g., biomedical imaging or\nautonomous driving. While bounding the Lipschitz constant of a neural network\nimproves stability, most methods rely on restricting the Lipschitz constants of\neach layer which gives a poor bound for the actual Lipschitz constant.\n  In this paper we investigate a variational regularization method named CLIP\nfor controlling the Lipschitz constant of a neural network, which can easily be\nintegrated into the training procedure. We mathematically analyze the proposed\nmodel, in particular discussing the impact of the chosen regularization\nparameter on the output of the network. Finally, we numerically evaluate our\nmethod on both a nonlinear regression problem and the MNIST and Fashion-MNIST\nclassification databases, and compare our results with a weight regularization\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:29:24 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bungert", "Leon", ""], ["Raab", "Ren\u00e9", ""], ["Roith", "Tim", ""], ["Schwinn", "Leo", ""], ["Tenbrinck", "Daniel", ""]]}, {"id": "2103.12591", "submitter": "Donald Lee", "authors": "Arash Pakbin, Xiaochen Wang, Bobak J. Mortazavi, Donald K.K. Lee", "title": "BoXHED 2.0: Scalable boosting of functional data in survival analysis", "comments": "9 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern applications of survival analysis increasingly involve time-dependent\ncovariates, which constitute a form of functional data. Learning from\nfunctional data generally involves repeated evaluations of time integrals which\nis numerically expensive. In this work we propose a lightweight data\npreprocessing step that transforms functional data into nonfunctional data.\nBoosting implementations for nonfunctional data can then be used, whereby the\nrequired numerical integration comes for free as part of the training phase. We\nuse this to develop BoXHED 2.0, a quantum leap over the tree-boosted hazard\npackage BoXHED 1.0. BoXHED 2.0 extends BoXHED 1.0 to Aalen's multiplicative\nintensity model, which covers censoring schemes far beyond right-censoring and\nalso supports recurrent events data. It is also massively scalable because of\npreprocessing and also because it borrows from the core components of XGBoost.\nBoXHED 2.0 supports the use of GPUs and multicore CPUs, and is available from\nGitHub: www.github.com/BoXHED.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:46:09 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pakbin", "Arash", ""], ["Wang", "Xiaochen", ""], ["Mortazavi", "Bobak J.", ""], ["Lee", "Donald K. K.", ""]]}, {"id": "2103.12648", "submitter": "Fabian Stephany", "authors": "Otto K\\\"assi, Vili Lehdonvirta, Fabian Stephany", "title": "How Many Online Workers are there in the World? A Data-Driven Assessment", "comments": "16 pages, four figures, two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  An unknown number of people around the world are earning income by working\nthrough online labour platforms such as Upwork and Amazon Mechanical Turk. We\ncombine data collected from various sources to build a data-driven assessment\nof the number of such online workers (also known as online freelancers)\nglobally. Our headline estimate is that there are 163 million freelancer\nprofiles registered on online labour platforms globally. Approximately 19\nmillion of them have obtained work through the platform at least once, and 5\nmillion have completed at least 10 projects or earned at least $1000. These\nnumbers suggest a substantial growth from 2015 in registered worker accounts,\nbut much less growth in amount of work completed by workers. Our results\nindicate that online freelancing represents a non-trivial segment of labour\ntoday, but one that is spread thinly across countries and sectors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:00:30 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 13:39:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["K\u00e4ssi", "Otto", ""], ["Lehdonvirta", "Vili", ""], ["Stephany", "Fabian", ""]]}, {"id": "2103.12676", "submitter": "Nils Strodthoff", "authors": "Temesgen Mehari, Nils Strodthoff", "title": "Self-supervised representation learning from 12-lead ECG data", "comments": "11 pages, 6 figures, code available under\n  https://github.com/hhi-aml/ecg-selfsupervised", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forward a comprehensive assessment of self-supervised representation\nlearning from short segments of clinical 12-lead electrocardiography (ECG)\ndata. To this end, we explore adaptations of state-of-the-art self-supervised\nlearning algorithms from computer vision (SimCLR, BYOL, SwAV) and speech (CPC).\nIn a first step, we learn contrastive representations and evaluate their\nquality based on linear evaluation performance on a downstream classification\ntask. For the best-performing method, CPC, we find linear evaluation\nperformances only 0.8% below supervised performance. In a second step, we\nanalyze the impact of self-supervised pretraining on finetuned ECG classifiers\nas compared to purely supervised performance and find improvements in\ndownstream performance of more than 1%, label efficiency, as well as an\nincreased robustness against physiological noise. All experiments are carried\nout exclusively on publicly available datasets, the to-date largest collection\nused for self-supervised representation learning from ECG data, to foster\nreproducible research in the field of ECG representation learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:50:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mehari", "Temesgen", ""], ["Strodthoff", "Nils", ""]]}, {"id": "2103.12690", "submitter": "Yuanhao Wang", "authors": "Yuanhao Wang, Ruosong Wang, Sham M. Kakade", "title": "An Exponential Lower Bound for Linearly-Realizable MDPs with Constant\n  Suboptimality Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A fundamental question in the theory of reinforcement learning is: suppose\nthe optimal $Q$-function lies in the linear span of a given $d$ dimensional\nfeature mapping, is sample-efficient reinforcement learning (RL) possible? The\nrecent and remarkable result of Weisz et al. (2020) resolved this question in\nthe negative, providing an exponential (in $d$) sample size lower bound, which\nholds even if the agent has access to a generative model of the environment.\nOne may hope that this information theoretic barrier for RL can be circumvented\nby further supposing an even more favorable assumption: there exists a\n\\emph{constant suboptimality gap} between the optimal $Q$-value of the best\naction and that of the second-best action (for all states). The hope is that\nhaving a large suboptimality gap would permit easier identification of optimal\nactions themselves, thus making the problem tractable; indeed, provided the\nagent has access to a generative model, sample-efficient RL is in fact possible\nwith the addition of this more favorable assumption.\n  This work focuses on this question in the standard online reinforcement\nlearning setting, where our main result resolves this question in the negative:\nour hardness result shows that an exponential sample complexity lower bound\nstill holds even if a constant suboptimality gap is assumed in addition to\nhaving a linearly realizable optimal $Q$-function. Perhaps surprisingly, this\nimplies an exponential separation between the online RL setting and the\ngenerative model setting. Complementing our negative hardness result, we give\ntwo positive results showing that provably sample-efficient RL is possible\neither under an additional low-variance assumption or under a novel\nhypercontractivity assumption (both implicitly place stronger conditions on the\nunderlying dynamics model).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:05:54 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Yuanhao", ""], ["Wang", "Ruosong", ""], ["Kakade", "Sham M.", ""]]}, {"id": "2103.12692", "submitter": "Quanquan Gu", "authors": "Difan Zou and Jingfeng Wu and Vladimir Braverman and Quanquan Gu and\n  Sham M. Kakade", "title": "Benign Overfitting of Constant-Stepsize SGD for Linear Regression", "comments": "53 pages. This version provides improved upper bound results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing realization that algorithmic inductive biases are\ncentral in preventing overfitting; empirically, we often see a benign\noverfitting phenomenon in overparameterized settings for natural learning\nalgorithms, such as stochastic gradient descent (SGD), where little to no\nexplicit regularization has been employed. This work considers this issue in\narguably the most basic setting: constant-stepsize SGD (with iterate averaging)\nfor linear regression in the overparameterized regime. Our main result provides\na sharp excess risk bound, stated in terms of the full eigenspectrum of the\ndata covariance matrix, that reveals a bias-variance decomposition\ncharacterizing when generalization is possible: (i) the variance bound is\ncharacterized in terms of an effective dimension (specific for SGD) and (ii)\nthe bias bound provides a sharp geometric characterization in terms of the\nlocation of the initial iterate (and how it aligns with the data covariance\nmatrix). We reflect on a number of notable differences between the algorithmic\nregularization afforded by (unregularized) SGD in comparison to ordinary least\nsquares (minimum-norm interpolation) and ridge regression.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:15:53 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 00:07:39 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zou", "Difan", ""], ["Wu", "Jingfeng", ""], ["Braverman", "Vladimir", ""], ["Gu", "Quanquan", ""], ["Kakade", "Sham M.", ""]]}, {"id": "2103.12711", "submitter": "Guillaume Staerman", "authors": "Guillaume Staerman, Pavlo Mozharovskyi, St\\'ephan Cl\\'emen\\c{c}on and\n  Florence d'Alch\\'e-Buc", "title": "A Pseudo-Metric between Probability Distributions based on Depth-Trimmed\n  Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data depth is a non parametric statistical tool that measures centrality of\nany element $x\\in\\mathbb{R}^d$ with respect to (w.r.t.) a probability\ndistribution or a data set. It is a natural median-oriented extension of the\ncumulative distribution function (cdf) to the multivariate case. Consequently,\nits upper level sets -- the depth-trimmed regions -- give rise to a definition\nof multivariate quantiles. In this work, we propose two new pseudo-metrics\nbetween continuous probability measures based on data depth and its associated\ncentral regions. The first one is constructed as the Lp-distance between data\ndepth w.r.t. each distribution while the second one relies on the Hausdorff\ndistance between their quantile regions. It can further be seen as an original\nway to extend the one-dimensional formulae of the Wasserstein distance, which\ninvolves quantiles and cdfs, to the multivariate space. After discussing the\nproperties of these pseudo-metrics and providing conditions under which they\ndefine a distance, we highlight similarities with the Wasserstein distance.\nInterestingly, the derived non-asymptotic bounds show that in contrast to the\nWasserstein distance, the proposed pseudo-metrics do not suffer from the curse\nof dimensionality. Moreover, based on the support function of a convex body, we\npropose an efficient approximation possessing linear time complexity w.r.t. the\nsize of the data set and its dimension. The quality of this approximation as\nwell as the performance of the proposed approach are illustrated in\nexperiments. Furthermore, by construction the regions-based pseudo-metric\nappears to be robust w.r.t. both outliers and heavy tails, a behavior witnessed\nin the numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:33:18 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 09:47:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Staerman", "Guillaume", ""], ["Mozharovskyi", "Pavlo", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "2103.12725", "submitter": "Steve Yadlowsky", "authors": "Steve Yadlowsky, Taedong Yun, Cory McLean, Alexander D'Amour", "title": "SLOE: A Faster Method for Statistical Inference in High-Dimensional\n  Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression remains one of the most widely used tools in applied\nstatistics, machine learning and data science. However, in moderately\nhigh-dimensional problems, where the number of features $d$ is a non-negligible\nfraction of the sample size $n$, the logistic regression maximum likelihood\nestimator (MLE), and statistical procedures based the large-sample\napproximation of its distribution, behave poorly. Recently, Sur and Cand\\`es\n(2019) showed that these issues can be corrected by applying a new\napproximation of the MLE's sampling distribution in this high-dimensional\nregime. Unfortunately, these corrections are difficult to implement in\npractice, because they require an estimate of the \\emph{signal strength}, which\nis a function of the underlying parameters $\\beta$ of the logistic regression.\nTo address this issue, we propose SLOE, a fast and straightforward approach to\nestimate the signal strength in logistic regression. The key insight of SLOE is\nthat the Sur and Cand\\`es (2019) correction can be reparameterized in terms of\nthe \\emph{corrupted signal strength}, which is only a function of the estimated\nparameters $\\widehat \\beta$. We propose an estimator for this quantity, prove\nthat it is consistent in the relevant high-dimensional regime, and show that\ndimensionality correction using SLOE is accurate in finite samples. Compared to\nthe existing ProbeFrontier heuristic, SLOE is conceptually simpler and orders\nof magnitude faster, making it suitable for routine use. We demonstrate the\nimportance of routine dimensionality correction in the Heart Disease dataset\nfrom the UCI repository, and a genomics application using data from the UK\nBiobank. We provide an open source package for this method, available at\n\\url{https://github.com/google-research/sloe-logistic}.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:48:56 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:50:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yadlowsky", "Steve", ""], ["Yun", "Taedong", ""], ["McLean", "Cory", ""], ["D'Amour", "Alexander", ""]]}, {"id": "2103.12726", "submitter": "Hiroki Furuta", "authors": "Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo,\n  Sergey Levine, Ofir Nachum, Shixiang Shane Gu", "title": "Policy Information Capacity: Information-Theoretic Measure for Task\n  Complexity in Deep Reinforcement Learning", "comments": "Accepted to ICML2021. The code is available at:\n  https://github.com/frt03/pic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Progress in deep reinforcement learning (RL) research is largely enabled by\nbenchmark task environments. However, analyzing the nature of those\nenvironments is often overlooked. In particular, we still do not have agreeable\nways to measure the difficulty or solvability of a task, given that each has\nfundamentally different actions, observations, dynamics, rewards, and can be\ntackled with diverse RL algorithms. In this work, we propose policy information\ncapacity (PIC) -- the mutual information between policy parameters and episodic\nreturn -- and policy-optimal information capacity (POIC) -- between policy\nparameters and episodic optimality -- as two environment-agnostic,\nalgorithm-agnostic quantitative metrics for task difficulty. Evaluating our\nmetrics across toy environments as well as continuous control benchmark tasks\nfrom OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that\nthese information-theoretic metrics have higher correlations with normalized\ntask solvability scores than a variety of alternatives. Lastly, we show that\nthese metrics can also be used for fast and compute-efficient optimizations of\nkey design parameters such as reward shaping, policy architectures, and MDP\nproperties for better solvability by RL algorithms without ever running full RL\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:49:50 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:12:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Furuta", "Hiroki", ""], ["Matsushima", "Tatsuya", ""], ["Kozuno", "Tadashi", ""], ["Matsuo", "Yutaka", ""], ["Levine", "Sergey", ""], ["Nachum", "Ofir", ""], ["Gu", "Shixiang Shane", ""]]}, {"id": "2103.12827", "submitter": "Cat Le", "authors": "Cat P. Le, Mohammadreza Soltani, Robert Ravier, Trevor Standley,\n  Silvio Savarese, Vahid Tarokh", "title": "Neural Architecture Search From Fr\\'echet Task Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We formulate a Fr\\'echet-type asymmetric distance between tasks based on\nFisher Information Matrices. We show how the distance between a target task and\neach task in a given set of baseline tasks can be used to reduce the neural\narchitecture search space for the target task. The complexity reduction in\nsearch space for task-specific architectures is achieved by building on the\noptimized architectures for similar tasks instead of doing a full search\nwithout using this side information. Experimental results demonstrate the\nefficacy of the proposed approach and its improvements over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:43:31 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 14:13:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Le", "Cat P.", ""], ["Soltani", "Mohammadreza", ""], ["Ravier", "Robert", ""], ["Standley", "Trevor", ""], ["Savarese", "Silvio", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2103.12828", "submitter": "Xiaohan Chen", "authors": "Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu,\n  Zhangyang Wang, Wotao Yin", "title": "Learning to Optimize: A Primer and A Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to optimize (L2O) is an emerging approach that leverages machine\nlearning to develop optimization methods, aiming at reducing the laborious\niterations of hand engineering. It automates the design of an optimization\nmethod based on its performance on a set of training problems. This data-driven\nprocedure generates methods that can efficiently solve problems similar to\nthose in the training. In sharp contrast, the typical and traditional designs\nof optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems specified by the theory. The difference\nmakes L2O suitable for repeatedly solving a certain type of optimization\nproblems over a specific distribution of data, while it typically fails on\nout-of-distribution problems. The practicality of L2O depends on the type of\ntarget optimization, the chosen architecture of the method to learn, and the\ntraining procedure. This new paradigm has motivated a community of researchers\nto explore L2O and report their findings.\n  This article is poised to be the first comprehensive survey and benchmark of\nL2O for continuous optimization. We set up taxonomies, categorize existing\nworks and research directions, present insights, and identify open challenges.\nWe also benchmarked many existing L2O approaches on a few but representative\noptimization problems. For reproducible research and fair benchmarking\npurposes, we released our software implementation and data in the package\nOpen-L2O at https://github.com/VITA-Group/Open-L2O.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:46:20 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 01:46:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Tianlong", ""], ["Chen", "Xiaohan", ""], ["Chen", "Wuyang", ""], ["Heaton", "Howard", ""], ["Liu", "Jialin", ""], ["Wang", "Zhangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "2103.12866", "submitter": "Deividas Eringis", "authors": "Deividas Eringis and John Leth and Zheng-Hua Tan and Rafal Wisniewski\n  and Alireza Fakhrizadeh Esfahani and Mihaly Petreczky", "title": "PAC-Bayesian theory for stochastic LTI systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we derive a PAC-Bayesian error bound for autonomous stochastic\nLTI state-space models. The motivation for deriving such error bounds is that\nthey will allow deriving similar error bounds for more general dynamical\nsystems, including recurrent neural networks. In turn, PACBayesian error bounds\nare known to be useful for analyzing machine learning algorithms and for\nderiving new ones.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:59:21 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 19:08:41 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Eringis", "Deividas", ""], ["Leth", "John", ""], ["Tan", "Zheng-Hua", ""], ["Wisniewski", "Rafal", ""], ["Esfahani", "Alireza Fakhrizadeh", ""], ["Petreczky", "Mihaly", ""]]}, {"id": "2103.12913", "submitter": "Jack Prescott", "authors": "Jack Prescott, Xiao Zhang, David Evans", "title": "Improved Estimation of Concentration Under $\\ell_p$-Norm Distance\n  Metrics Using Half Spaces", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concentration of measure has been argued to be the fundamental cause of\nadversarial vulnerability. Mahloujifar et al. presented an empirical way to\nmeasure the concentration of a data distribution using samples, and employed it\nto find lower bounds on intrinsic robustness for several benchmark datasets.\nHowever, it remains unclear whether these lower bounds are tight enough to\nprovide a useful approximation for the intrinsic robustness of a dataset. To\ngain a deeper understanding of the concentration of measure phenomenon, we\nfirst extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian\nmeasures and arbitrary $\\ell_p$-norms ($p \\geq 2$). We leverage these\ntheoretical insights to design a method that uses half-spaces to estimate the\nconcentration of any empirical dataset under $\\ell_p$-norm distance metrics.\nOur proposed algorithm is more efficient than Mahloujifar et al.'s, and our\nexperiments on synthetic datasets and image benchmarks demonstrate that it is\nable to find much tighter intrinsic robustness bounds. These tighter estimates\nprovide further evidence that rules out intrinsic dataset concentration as a\npossible explanation for the adversarial vulnerability of state-of-the-art\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:16:28 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Prescott", "Jack", ""], ["Zhang", "Xiao", ""], ["Evans", "David", ""]]}, {"id": "2103.12959", "submitter": "Houman Owhadi", "authors": "Yifan Chen and Bamdad Hosseini and Houman Owhadi and Andrew M Stuart", "title": "Solving and Learning Nonlinear PDEs with Gaussian Processes", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple, rigorous, and unified framework for solving nonlinear\npartial differential equations (PDEs), and for solving inverse problems (IPs)\ninvolving the identification of parameters in PDEs, using the framework of\nGaussian processes. The proposed approach (1) provides a natural generalization\nof collocation kernel methods to nonlinear PDEs and IPs, (2) has guaranteed\nconvergence with a path to compute error bounds in the PDE setting, and (3)\ninherits the state-of-the-art computational complexity of linear solvers for\ndense kernel matrices. The main idea of our method is to approximate the\nsolution of a given PDE with a MAP estimator of a Gaussian process given the\nobservation of the PDE at a finite number of collocation points. Although this\noptimization problem is infinite-dimensional, it can be reduced to a\nfinite-dimensional one by introducing additional variables corresponding to the\nvalues of the derivatives of the solution at collocation points; this\ngeneralizes the representer theorem arising in Gaussian process regression. The\nreduced optimization problem has a quadratic loss and nonlinear constraints,\nand it is in turn solved with a variant of the Gauss-Newton method. The\nresulting algorithm (a) can be interpreted as solving successive linearizations\nof the nonlinear PDE, and (b) is found in practice to converge in a small\nnumber (two to ten) of iterations in experiments conducted on a range of PDEs.\nFor IPs, while the traditional approach has been to iterate between the\nidentifications of parameters in the PDE and the numerical approximation of its\nsolution, our algorithm tackles both simultaneously. Experiments on nonlinear\nelliptic PDEs, Burgers' equation, a regularized Eikonal equation, and an IP for\npermeability identification in Darcy flow illustrate the efficacy and scope of\nour framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:16:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Yifan", ""], ["Hosseini", "Bamdad", ""], ["Owhadi", "Houman", ""], ["Stuart", "Andrew M", ""]]}, {"id": "2103.13059", "submitter": "Richard Combes", "authors": "Wei Huang and Richard Combes and Cindy Trinh", "title": "Towards Optimal Algorithms for Multi-Player Bandits without Collision\n  Sensing Information", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for multi-player multi-armed bandits without\ncollision sensing information. Our algorithm circumvents two problems shared by\nall state-of-the-art algorithms: it does not need as an input a lower bound on\nthe minimal expected reward of an arm, and its performance does not scale\ninversely proportionally to the minimal expected reward. We prove a theoretical\nregret upper bound to justify these claims. We complement our theoretical\nresults with numerical experiments, showing that the proposed algorithm\noutperforms state-of-the-art in practice as well.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:14:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Huang", "Wei", ""], ["Combes", "Richard", ""], ["Trinh", "Cindy", ""]]}, {"id": "2103.13127", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang\n  Su, Jun Zhu", "title": "Black-box Detection of Backdoor Attacks with Limited Information and\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although deep neural networks (DNNs) have made rapid progress in recent\nyears, they are vulnerable in adversarial environments. A malicious backdoor\ncould be embedded in a model by poisoning the training dataset, whose intention\nis to make the infected model give wrong predictions during inference when the\nspecific trigger appears. To mitigate the potential threats of backdoor\nattacks, various backdoor detection and defense methods have been proposed.\nHowever, the existing techniques usually require the poisoned training data or\naccess to the white-box model, which is commonly unavailable in practice. In\nthis paper, we propose a black-box backdoor detection (B3D) method to identify\nbackdoor attacks with only query access to the model. We introduce a\ngradient-free optimization algorithm to reverse-engineer the potential trigger\nfor each class, which helps to reveal the existence of backdoor attacks. In\naddition to backdoor detection, we also propose a simple strategy for reliable\npredictions using the identified backdoored models. Extensive experiments on\nhundreds of DNN models trained on several datasets corroborate the\neffectiveness of our method under the black-box setting against various\nbackdoor attacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:06:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Dong", "Yinpeng", ""], ["Yang", "Xiao", ""], ["Deng", "Zhijie", ""], ["Pang", "Tianyu", ""], ["Xiao", "Zihao", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2103.13192", "submitter": "Tanya Ignatenko", "authors": "Tanya Ignatenko, Kirill Kondrashov, Marco Cox, Bert de Vries", "title": "On Sequential Bayesian Optimization with Pairwise Comparison", "comments": "13 pages, 5 figures (15 with subfigures), submitted for EEE\n  Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of user preference learning on the example\nof parameter setting for a hearing aid (HA). We propose to use an agent that\ninteracts with a HA user, in order to collect the most informative data, and\nlearns user preferences for HA parameter settings, based on these data. We\nmodel the HA system as two interacting sub-systems, one representing a user\nwith his/her preferences and another one representing an agent. In this system,\nthe user responses to HA settings, proposed by the agent. In our user model,\nthe responses are driven by a parametric user preference function. The agent\ncomprises the sequential mechanisms for user model inference and HA parameter\nproposal generation. To infer the user model (preference function), Bayesian\napproximate inference is used in the agent. Here we propose the normalized\nweighted Kullback-Leibler (KL) divergence between true and agent-assigned\npredictive user response distributions as a metric to assess the quality of\nlearned preferences. Moreover, our agent strategy for generating HA parameter\nproposals is to generate HA settings, responses to which help resolving\nuncertainty associated with prediction of the user responses the most. The\nresulting data, consequently, allows for efficient user model learning. The\nnormalized weighted KL-divergence plays an important role here as well, since\nit characterizes the informativeness of the data to be used for probing the\nuser. The efficiency of our approach is validated by numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:46:27 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ignatenko", "Tanya", ""], ["Kondrashov", "Kirill", ""], ["Cox", "Marco", ""], ["de Vries", "Bert", ""]]}, {"id": "2103.13327", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Closed-form geodesics and trust-region method to calculate Riemannian\n  logarithms on Stiefel and its quotient manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.AG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two closed-form geodesic formulas for a family of metrics on\nStiefel manifold, parameterized by two positive numbers, having both the\nembedded and canonical metrics as special cases. The closed-form formulas allow\nus to compute geodesics by matrix exponential in reduced dimension for low-rank\nmanifolds. Combining with the use of Fr{\\'e}chet derivatives to compute the\ngradient of the square Frobenius distance between a geodesic ending point to a\ngiven point on the manifold, we show the logarithm map and geodesic distance\nbetween two endpoints on the manifold could be computed by {\\it minimizing}\nthis square distance by a {\\it trust-region} solver. This leads to a new\nframework to compute the geodesic distance for manifolds with known geodesic\nformula but no closed-form logarithm map. We show the approach works well for\nStiefel as well as flag manifolds. The logarithm map could be used to compute\nthe Riemannian center of mass for these manifolds equipped with the above\nmetrics. We also deduce simple trigonometric formulas for the Riemannian\nexponential and logarithm maps on the Grassmann manifold.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:48:38 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "2103.13342", "submitter": "Nicolas Brunel", "authors": "Salim I. Amoukou, Nicolas J-B. Brunel, Tangi Sala\\\"un", "title": "The Shapley Value of coalition of variables provides better explanations", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While Shapley Values (SV) are one of the gold standard for interpreting\nmachine learning models, we show that they are still poorly understood, in\nparticular in the presence of categorical variables or of variables of low\nimportance. For instance, we show that the popular practice that consists in\nsumming the SV of dummy variables is false as it provides wrong estimates of\nall the SV in the model and implies spurious interpretations. Based on the\nidentification of null and active coalitions, and a coalitional version of the\nSV, we provide a correct computation and inference of important variables.\nMoreover, a Python library (All the experiments and simulations can be\nreproduced with the publicly available library Active Coalition of Variables,\nhttps://www.github.com/salimamoukou/acv00) that computes reliably conditional\nexpectations and SV for tree-based models, is implemented and compared with\nstate-of-the-art algorithms on toy models and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:02:57 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:28:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Amoukou", "Salim I.", ""], ["Brunel", "Nicolas J-B.", ""], ["Sala\u00fcn", "Tangi", ""]]}, {"id": "2103.13357", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li", "title": "A Two-Stage Variable Selection Approach for Correlated High Dimensional\n  Predictors", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models, some predictors are often found to be\ncorrelated with each other, and functioning together. Many group variable\nselection methods are developed to select the groups of predictors that are\nclosely related to the continuous or categorical response. These existing\nmethods usually assume the group structures are well known. For example,\nvariables with similar practical meaning, or dummy variables created by\ncategorical data. However, in practice, it is impractical to know the exact\ngroup structure, especially when the variable dimensional is large. As a\nresult, the group variable selection results may be selected. To solve the\nchallenge, we propose a two-stage approach that combines a variable clustering\nstage and a group variable stage for the group variable selection problem. The\nvariable clustering stage uses information from the data to find a group\nstructure, which improves the performance of the existing group variable\nselection methods. For ultrahigh dimensional data, where the predictors are\nmuch larger than observations, we incorporated a variable screening method in\nthe first stage and shows the advantages of such an approach. In this article,\nwe compared and discussed the performance of four existing group variable\nselection methods under different simulation models, with and without the\nvariable clustering stage. The two-stage method shows a better performance, in\nterms of the prediction accuracy, as well as in the accuracy to select active\npredictors. An athlete's data is also used to show the advantages of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:28:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Zhiyuan", ""]]}, {"id": "2103.13358", "submitter": "Xingang Wang Professor", "authors": "Huawei Fan, Ling-Wei Kong, Ying-Cheng Lai, Xingang Wang", "title": "Anticipating synchronization with machine learning", "comments": "13 pages; 12 figures", "journal-ref": "Phys. Rev. Research 3, 023237 (2021)", "doi": "10.1103/PhysRevResearch.3.023237", "report-no": null, "categories": "nlin.AO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In applications of dynamical systems, situations can arise where it is\ndesired to predict the onset of synchronization as it can lead to\ncharacteristic and significant changes in the system performance and behaviors,\nfor better or worse. In experimental and real settings, the system equations\nare often unknown, raising the need to develop a prediction framework that is\nmodel free and fully data driven. We contemplate that this challenging problem\ncan be addressed with machine learning. In particular, exploiting reservoir\ncomputing or echo state networks, we devise a \"parameter-aware\" scheme to train\nthe neural machine using asynchronous time series, i.e., in the parameter\nregime prior to the onset of synchronization. A properly trained machine will\npossess the power to predict the synchronization transition in that, with a\ngiven amount of parameter drift, whether the system would remain asynchronous\nor exhibit synchronous dynamics can be accurately anticipated. We demonstrate\nthe machine-learning based framework using representative chaotic models and\nsmall network systems that exhibit continuous (second-order) or abrupt\n(first-order) transitions. A remarkable feature is that, for a network system\nexhibiting an explosive (first-order) transition and a hysteresis loop in\nsynchronization, the machine learning scheme is capable of accurately\npredicting these features, including the precise locations of the transition\npoints associated with the forward and backward transition paths.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:51:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fan", "Huawei", ""], ["Kong", "Ling-Wei", ""], ["Lai", "Ying-Cheng", ""], ["Wang", "Xingang", ""]]}, {"id": "2103.13462", "submitter": "Tengyu Ma", "authors": "Tengyu Ma", "title": "Why Do Local Methods Solve Nonconvex Problems?", "comments": "This is the Chapter 21 of the book \"Beyond the Worst-Case Analysis of\n  Algorithms\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization is ubiquitous in modern machine learning. Researchers\ndevise non-convex objective functions and optimize them using off-the-shelf\noptimizers such as stochastic gradient descent and its variants, which leverage\nthe local geometry and update iteratively. Even though solving non-convex\nfunctions is NP-hard in the worst case, the optimization quality in practice is\noften not an issue -- optimizers are largely believed to find approximate\nglobal minima. Researchers hypothesize a unified explanation for this\nintriguing phenomenon: most of the local minima of the practically-used\nobjectives are approximately global minima. We rigorously formalize it for\nconcrete instances of machine learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:34:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ma", "Tengyu", ""]]}, {"id": "2103.13466", "submitter": "Tomohiro Hayase", "authors": "Benoit Collins, Tomohiro Hayase", "title": "Asymptotic Freeness of Layerwise Jacobians Caused by Invariance of\n  Multilayer Perceptron: The Haar Orthogonal Case", "comments": "Any comments are welcomed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free Probability Theory (FPT) provides rich knowledge for handling\nmathematical difficulties caused by random matrices that appear in research\nrelated to deep neural networks (DNNs), such as the dynamical isometry, Fisher\ninformation matrix, and training dynamics. FPT suits these researches because\nthe DNN's parameter-Jacobian and input-Jacobian are polynomials of layerwise\nJacobians. However, the critical assumption of asymptotic freenss of the\nlayerwise Jacobian has not been proven completely so far. The asymptotic\nfreeness assumption plays a fundamental role when propagating spectral\ndistributions through the layers. Haar distributed orthogonal matrices are\nessential for achieving dynamical isometry. In this work, we prove asymptotic\nfreeness of layerwise Jacobian of multilayer perceptrons in this case.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:52:11 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 10:35:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Collins", "Benoit", ""], ["Hayase", "Tomohiro", ""]]}, {"id": "2103.13521", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Terry Soo", "title": "Conditions and Assumptions for Constraint-based Causal Structure\n  Learning", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper formalizes constraint-based structure learning of the \"true\" causal\ngraph from observed data when unobserved variables are also existent. We define\na \"generic\" structure learning algorithm, which provides conditions that, under\nthe faithfulness assumption, the output of all known exact algorithms in the\nliterature must satisfy, and which outputs graphs that are Markov equivalent to\nthe causal graph. More importantly, we provide clear assumptions, weaker than\nfaithfulness, under which the same generic algorithm outputs Markov equivalent\ngraphs to the causal graph. We provide the theory for the general class of\nmodels under the assumption that the distribution is Markovian to the true\ncausal graph, and we specialize the definitions and results for structural\ncausal models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:08:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Soo", "Terry", ""]]}, {"id": "2103.13555", "submitter": "Hyebin Song", "authors": "Hyebin Song, Garvesh Raskutti, Rebecca Willett", "title": "Prediction in the presence of response-dependent missing labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a variety of settings, limitations of sensing technologies or other\nsampling mechanisms result in missing labels, where the likelihood of a missing\nlabel in the training set is an unknown function of the data. For example,\nsatellites used to detect forest fires cannot sense fires below a certain size\nthreshold. In such cases, training datasets consist of positive and\npseudo-negative observations where pseudo-negative observations can be either\ntrue negatives or undetected positives with small magnitudes. We develop a new\nmethodology and non-convex algorithm P(ositive) U(nlabeled) - O(ccurrence)\nM(agnitude) M(ixture) which jointly estimates the occurrence and detection\nlikelihood of positive samples, utilizing prior knowledge of the detection\nmechanism. Our approach uses ideas from positive-unlabeled (PU)-learning and\nzero-inflated models that jointly estimate the magnitude and occurrence of\nevents. We provide conditions under which our model is identifiable and prove\nthat even though our approach leads to a non-convex objective, any local\nminimizer has optimal statistical error (up to a log term) and projected\ngradient descent has geometric convergence rates. We demonstrate on both\nsynthetic data and a California wildfire dataset that our method out-performs\nexisting state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:43:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Song", "Hyebin", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""]]}, {"id": "2103.13565", "submitter": "Haobing Liu", "authors": "Haobing Liu, Yanmin Zhu, Tianzi Zang, Yanan Xu, Jiadi Yu, Feilong Tang", "title": "Jointly Modeling Heterogeneous Student Behaviors and Interactions Among\n  Multiple Prediction Tasks", "comments": null, "journal-ref": "ACM TKDD2021", "doi": "10.1145/3458023", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction tasks about students have practical significance for both student\nand college. Making multiple predictions about students is an important part of\na smart campus. For instance, predicting whether a student will fail to\ngraduate can alert the student affairs office to take predictive measures to\nhelp the student improve his/her academic performance. With the development of\ninformation technology in colleges, we can collect digital footprints which\nencode heterogeneous behaviors continuously. In this paper, we focus on\nmodeling heterogeneous behaviors and making multiple predictions together,\nsince some prediction tasks are related and learning the model for a specific\ntask may have the data sparsity problem. To this end, we propose a variant of\nLSTM and a soft-attention mechanism. The proposed LSTM is able to learn the\nstudent profile-aware representation from heterogeneous behavior sequences. The\nproposed soft-attention mechanism can dynamically learn different importance\ndegrees of different days for every student. In this way, heterogeneous\nbehaviors can be well modeled. In order to model interactions among multiple\nprediction tasks, we propose a co-attention mechanism based unit. With the help\nof the stacked units, we can explicitly control the knowledge transfer among\nmultiple tasks. We design three motivating behavior prediction tasks based on a\nreal-world dataset collected from a college. Qualitative and quantitative\nexperiments on the three prediction tasks have demonstrated the effectiveness\nof our model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:01:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Haobing", ""], ["Zhu", "Yanmin", ""], ["Zang", "Tianzi", ""], ["Xu", "Yanan", ""], ["Yu", "Jiadi", ""], ["Tang", "Feilong", ""]]}, {"id": "2103.13569", "submitter": "Yivan Zhang", "authors": "Yivan Zhang, Masashi Sugiyama", "title": "Approximating Instance-Dependent Noise via Instance-Confidence Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise in multiclass classification is a major obstacle to the\ndeployment of learning systems. However, unlike the widely used\nclass-conditional noise (CCN) assumption that the noisy label is independent of\nthe input feature given the true label, label noise in real-world datasets can\nbe aleatory and heavily dependent on individual instances. In this work, we\ninvestigate the instance-dependent noise (IDN) model and propose an efficient\napproximation of IDN to capture the instance-specific label corruption.\nConcretely, noting the fact that most columns of the IDN transition matrix have\nonly limited influence on the class-posterior estimation, we propose a\nvariational approximation that uses a single-scalar confidence parameter. To\ncope with the situation where the mapping from the instance to its confidence\nvalue could vary significantly for two adjacent instances, we suggest using\ninstance embedding that assigns a trainable parameter to each instance. The\nresulting instance-confidence embedding (ICE) method not only performs well\nunder label noise but also can effectively detect ambiguous or mislabeled\ninstances. We validate its utility on various image and text classification\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:33:30 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Yivan", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.13607", "submitter": "Gautam Gare", "authors": "Gautam Rajendrakumar Gare and John Michael Galeotti", "title": "Exploiting Class Similarity for Machine Learning with Confidence Labels\n  and Projective Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels used for machine learning are relatable to each other, with\ncertain class labels being more similar to each other than others (e.g. images\nof cats and dogs are more similar to each other than those of cats and cars).\nSuch similarity among classes is often the cause of poor model performance due\nto the models confusing between them. Current labeling techniques fail to\nexplicitly capture such similarity information. In this paper, we instead\nexploit the similarity between classes by capturing the similarity information\nwith our novel confidence labels. Confidence labels are probabilistic labels\ndenoting the likelihood of similarity, or confusability, between the classes.\nOften even after models are trained to differentiate between classes in the\nfeature space, the similar classes' latent space still remains clustered. We\nview this type of clustering as valuable information and exploit it with our\nnovel projective loss functions. Our projective loss functions are designed to\nwork with confidence labels with an ability to relax the loss penalty for\nerrors that confuse similar classes. We use our approach to train neural\nnetworks with noisy labels, as we believe noisy labels are partly a result of\nconfusability arising from class similarity. We show improved performance\ncompared to the use of standard loss functions. We conduct a detailed analysis\nusing the CIFAR-10 dataset and show our proposed methods' applicability to\nlarger datasets, such as ImageNet and Food-101N.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:49:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gare", "Gautam Rajendrakumar", ""], ["Galeotti", "John Michael", ""]]}, {"id": "2103.13686", "submitter": "Hugo Manuel Proen\\c{c}a", "authors": "Hugo Manuel Proen\\c{c}a, Thomas B\\\"ack, Matthijs van Leeuwen", "title": "Robust subgroup discovery", "comments": "For associated code, see https://github.com/HMProenca/RuleList ;\n  submitted to Data Mining and Knowledge Discovery Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of robust subgroup discovery, i.e., finding a set of\ninterpretable descriptions of subsets that 1) stand out with respect to one or\nmore target attributes, 2) are statistically robust, and 3) non-redundant. Many\nattempts have been made to mine either locally robust subgroups or to tackle\nthe pattern explosion, but we are the first to address both challenges at the\nsame time from a global perspective.\n  First, we formulate a broad model class of subgroup lists, i.e., ordered sets\nof subgroups, for univariate and multivariate targets that can consist of\nnominal or numeric variables. This novel model class allows us to formalize the\nproblem of optimal robust subgroup discovery using the Minimum Description\nLength (MDL) principle, where we resort to optimal Normalized Maximum\nLikelihood and Bayesian encodings for nominal and numeric targets,\nrespectively. Notably, we show that our problem definition is equal to mining\nthe top-1 subgroup with an information-theoretic quality measure plus a penalty\nfor complexity.\n  Second, as finding optimal subgroup lists is NP-hard, we propose RSD, a\ngreedy heuristic that finds good subgroup lists and guarantees that the most\nsignificant subgroup found according to the MDL criterion is added in each\niteration, which is shown to be equivalent to a Bayesian one-sample\nproportions, multinomial, or t-test between the subgroup and dataset marginal\ntarget distributions plus a multiple hypothesis testing penalty. We empirically\nshow on 54 datasets that RSD outperforms previous subgroup set discovery\nmethods in terms of quality and subgroup list size.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:04:13 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Proen\u00e7a", "Hugo Manuel", ""], ["B\u00e4ck", "Thomas", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "2103.13751", "submitter": "Cl\\'ement Chadebec", "authors": "Cl\\'ement Chadebec and St\\'ephanie Allassonni\\`ere", "title": "Data Generation in Low Sample Size Setting Using Manifold Sampling and a\n  Geometry-Aware VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much efforts have been focused on improving Variational Autoencoders\nthrough richer posterior and prior distributions, little interest was shown in\namending the way we generate the data. In this paper, we develop two non\n\\emph{prior-dependent} generation procedures based on the geometry of the\nlatent space seen as a Riemannian manifold. The first one consists in sampling\nalong geodesic paths which is a natural way to explore the latent space while\nthe second one consists in sampling from the inverse of the metric volume\nelement which is easier to use in practice. Both methods are then compared to\n\\emph{prior-based} methods on various data sets and appear well suited for a\nlimited data regime. Finally, the latter method is used to perform data\naugmentation in a small sample size setting and is validated across various\nstandard and \\emph{real-life} data sets. In particular, this scheme allows to\ngreatly improve classification results on the OASIS database where balanced\naccuracy jumps from 80.7% for a classifier trained with the raw data to 89.1%\nwhen trained only with the synthetic data generated by our method. Such results\nwere also observed on 4 standard data sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:07:10 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chadebec", "Cl\u00e9ment", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""]]}, {"id": "2103.13787", "submitter": "Michael Schmischke", "authors": "Daniel Potts and Michael Schmischke", "title": "Interpretable Approximation of High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply the previously introduced approximation method based\non the ANOVA (analysis of variance) decomposition and Grouped Transformations\nto synthetic and real data. The advantage of this method is the\ninterpretability of the approximation, i.e., the ability to rank the importance\nof the attribute interactions or the variable couplings. Moreover, we are able\nto generate an attribute ranking to identify unimportant variables and reduce\nthe dimensionality of the problem. We compare the method to other approaches on\npublicly available benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:26:55 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Potts", "Daniel", ""], ["Schmischke", "Michael", ""]]}, {"id": "2103.13796", "submitter": "Noa Ben-David", "authors": "Noa Ben-David and Sivan Sabato", "title": "Active Structure Learning of Bayesian Networks in an Observational\n  Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active structure learning of Bayesian networks in an observational\nsetting, in which there are external limitations on the number of variable\nvalues that can be observed from the same sample. Random samples are drawn from\nthe joint distribution of the network variables, and the algorithm iteratively\nselects which variables to observe in the next sample. We propose a new active\nlearning algorithm for this setting, that finds with a high probability a\nstructure with a score that is $\\epsilon$-close to the optimal score. We show\nthat for a class of distributions that we term stable, a sample complexity\nreduction of up to a factor of $\\widetilde{\\Omega}(d^3)$ can be obtained, where\n$d$ is the number of network variables. We further show that in the worst case,\nthe sample complexity of the active algorithm is guaranteed to be almost the\nsame as that of a naive baseline algorithm. To supplement the theoretical\nresults, we report experiments that compare the performance of the new active\nalgorithm to the naive baseline and demonstrate the sample complexity\nimprovements. Code for the algorithm and for the experiments is provided at\nhttps://github.com/noabdavid/activeBNSL.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:50:14 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ben-David", "Noa", ""], ["Sabato", "Sivan", ""]]}, {"id": "2103.13810", "submitter": "Zhaolong Ling", "authors": "Zhaolong Ling, Kui Yu, Hao Wang, Lin Liu, and Jiuyong Li", "title": "Any Part of Bayesian Network Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an interesting and challenging problem, learning any part of a\nBayesian network (BN) structure. In this challenge, it will be computationally\ninefficient using existing global BN structure learning algorithms to find an\nentire BN structure to achieve the part of a BN structure in which we are\ninterested. And local BN structure learning algorithms encounter the false edge\norientation problem when they are directly used to tackle this challenging\nproblem. In this paper, we first present a new concept of Expand-Backtracking\nto explain why local BN structure learning methods have the false edge\norientation problem, then propose APSL, an efficient and accurate Any Part of\nBN Structure Learning algorithm. Specifically, APSL divides the V-structures in\na Markov blanket (MB) into two types: collider V-structure and non-collider\nV-structure, then it starts from a node of interest and recursively finds both\ncollider V-structures and non-collider V-structures in the found MBs, until the\npart of a BN structure in which we are interested are oriented. To improve the\nefficiency of APSL, we further design the APSL-FS algorithm using Feature\nSelection, APSL-FS. Using six benchmark BNs, the extensive experiments have\nvalidated the efficiency and accuracy of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:03:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ling", "Zhaolong", ""], ["Yu", "Kui", ""], ["Wang", "Hao", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""]]}, {"id": "2103.13883", "submitter": "Yaqi Duan", "authors": "Yaqi Duan, Chi Jin, Zhiyuan Li", "title": "Risk Bounds and Rademacher Complexity in Batch Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers batch Reinforcement Learning (RL) with general value\nfunction approximation. Our study investigates the minimal assumptions to\nreliably estimate/minimize Bellman error, and characterizes the generalization\nperformance by (local) Rademacher complexities of general function classes,\nwhich makes initial steps in bridging the gap between statistical learning\ntheory and batch RL. Concretely, we view the Bellman error as a surrogate loss\nfor the optimality gap, and prove the followings: (1) In double sampling\nregime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the\nRademacher complexity of the function class. (2) In the single sampling regime,\nsample-efficient risk minimization is not possible without further assumptions,\nregardless of algorithms. However, with completeness assumptions, the excess\nrisk of FQI and a minimax style algorithm can be again bounded by the\nRademacher complexity of the corresponding function classes. (3) Fast\nstatistical rates can be achieved by using tools of local Rademacher\ncomplexity. Our analysis covers a wide range of function classes, including\nfinite classes, linear spaces, kernel spaces, sparse linear features, etc.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:45:29 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Duan", "Yaqi", ""], ["Jin", "Chi", ""], ["Li", "Zhiyuan", ""]]}, {"id": "2103.13929", "submitter": "Min-hwan Oh", "authors": "Min-hwan Oh, Garud Iyengar", "title": "Multinomial Logit Contextual Bandits: Provable Optimality and\n  Practicality", "comments": "Accepted in AAAI 2021 (Main Technical Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a sequential assortment selection problem where the user choice\nis given by a multinomial logit (MNL) choice model whose parameters are\nunknown. In each period, the learning agent observes a $d$-dimensional\ncontextual information about the user and the $N$ available items, and offers\nan assortment of size $K$ to the user, and observes the bandit feedback of the\nitem chosen from the assortment. We propose upper confidence bound based\nalgorithms for this MNL contextual bandit. The first algorithm is a simple and\npractical method which achieves an $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ regret over\n$T$ rounds. Next, we propose a second algorithm which achieves a\n$\\tilde{\\mathcal{O}}(\\sqrt{dT})$ regret. This matches the lower bound for the\nMNL bandit problem, up to logarithmic terms, and improves on the best known\nresult by a $\\sqrt{d}$ factor. To establish this sharper regret bound, we\npresent a non-asymptotic confidence bound for the maximum likelihood estimator\nof the MNL model that may be of independent interest as its own theoretical\ncontribution. We then revisit the simpler, significantly more practical, first\nalgorithm and show that a simple variant of the algorithm achieves the optimal\nregret for a broad class of important applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:42:25 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Oh", "Min-hwan", ""], ["Iyengar", "Garud", ""]]}, {"id": "2103.13989", "submitter": "Brian Kim", "authors": "Brian Kim and Yalin E. Sagduyu and Tugba Erpek and Sennur Ulukus", "title": "Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G\n  and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provides powerful means to learn from spectrum data and solve\ncomplex tasks in 5G and beyond such as beam selection for initial access (IA)\nin mmWave communications. To establish the IA between the base station (e.g.,\ngNodeB) and user equipment (UE) for directional transmissions, a deep neural\nnetwork (DNN) can predict the beam that is best slanted to each UE by using the\nreceived signal strengths (RSSs) from a subset of possible narrow beams. While\nimproving the latency and reliability of beam selection compared to the\nconventional IA that sweeps all beams, the DNN itself is susceptible to\nadversarial attacks. We present an adversarial attack by generating adversarial\nperturbations to manipulate the over-the-air captured RSSs as the input to the\nDNN. This attack reduces the IA performance significantly and fools the DNN\ninto choosing the beams with small RSSs compared to jamming attacks with\nGaussian or uniform noise.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:25:21 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kim", "Brian", ""], ["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2103.14029", "submitter": "Xiaojie Mao", "authors": "Nathan Kallus, Xiaojie Mao, Masatoshi Uehara", "title": "Causal Inference Under Unmeasured Confounding With Negative Controls: A\n  Minimax Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of causal parameters when not all confounders are\nobserved and instead negative controls are available. Recent work has shown how\nthese can enable identification and efficient estimation via two so-called\nbridge functions. In this paper, we tackle the primary challenge to causal\ninference using negative controls: the identification and estimation of these\nbridge functions. Previous work has relied on uniqueness and completeness\nassumptions on these functions that may be implausible in practice and also\nfocused on their parametric estimation. Instead, we provide a new\nidentification strategy that avoids both uniqueness and completeness. And, we\nprovide a new estimators for these functions based on minimax learning\nformulations. These estimators accommodate general function classes such as\nreproducing Hilbert spaces and neural networks. We study finite-sample\nconvergence results both for estimating bridge function themselves and for the\nfinal estimation of the causal parameter. We do this under a variety of\ncombinations of assumptions that include realizability and closedness\nconditions on the hypothesis and critic classes employed in the minimax\nestimator. Depending on how much we are willing to assume, we obtain different\nconvergence rates. In some cases, we show the estimate for the causal parameter\nmay converge even when our bridge function estimators do not converge to any\nvalid bridge function. And, in other cases, we show we can obtain\nsemiparametric efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:19 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 00:23:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Uehara", "Masatoshi", ""]]}, {"id": "2103.14068", "submitter": "Rachel Cummings", "authors": "Chris Waites and Rachel Cummings", "title": "Differentially Private Normalizing Flows for Privacy-Preserving Density\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flow models have risen as a popular solution to the problem of\ndensity estimation, enabling high-quality synthetic data generation as well as\nexact probability density evaluation. However, in contexts where individuals\nare directly associated with the training data, releasing such a model raises\nprivacy concerns. In this work, we propose the use of normalizing flow models\nthat provide explicit differential privacy guarantees as a novel approach to\nthe problem of privacy-preserving density estimation. We evaluate the efficacy\nof our approach empirically using benchmark datasets, and we demonstrate that\nour method substantially outperforms previous state-of-the-art approaches. We\nadditionally show how our algorithm can be applied to the task of\ndifferentially private anomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:39:51 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Waites", "Chris", ""], ["Cummings", "Rachel", ""]]}, {"id": "2103.14076", "submitter": "Andreas Bock", "authors": "Andreas Bock, Colin J. Cotter", "title": "Learning landmark geodesics using Kalman ensembles", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of diffeomorphometric geodesic landmark matching where\nthe objective is to find a diffeomorphism that via its group action maps\nbetween two sets of landmarks. It is well-known that the motion of the\nlandmarks, and thereby the diffeomorphism, can be encoded by an initial\nmomentum leading to a formulation where the landmark matching problem can be\nsolved as an optimisation problem over such momenta. The novelty of our work\nlies in the application of a derivative-free Bayesian inverse method for\nlearning the optimal momentum encoding the diffeomorphic mapping between the\ntemplate and the target. The method we apply is the ensemble Kalman filter, an\nextension of the Kalman filter to nonlinear observation operators. We describe\nan efficient implementation of the algorithm and show several numerical results\nfor various target shapes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:52:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bock", "Andreas", ""], ["Cotter", "Colin J.", ""]]}, {"id": "2103.14077", "submitter": "Tongzheng Ren", "authors": "Tongzheng Ren, Jialian Li, Bo Dai, Simon S. Du, Sujay Sanghavi", "title": "Nearly Horizon-Free Offline Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit offline reinforcement learning on episodic time-homogeneous\ntabular Markov Decision Processes with $S$ states, $A$ actions and planning\nhorizon $H$. Given the collected $N$ episodes data with minimum cumulative\nreaching probability $d_m$, we obtain the first set of nearly $H$-free sample\ncomplexity bounds for evaluation and planning using the empirical MDPs: 1.For\nthe offline evaluation, we obtain an $\\tilde{O}\\left(\\sqrt{\\frac{1}{Nd_m}}\n\\right)$ error rate, which matches the lower bound and does not have additional\ndependency on $\\poly\\left(S,A\\right)$ in higher-order term, that is different\nfrom previous works~\\citep{yin2020near,yin2020asymptotically}. 2.For the\noffline policy optimization, we obtain an $\\tilde{O}\\left(\\sqrt{\\frac{1}{Nd_m}}\n+ \\frac{S}{Nd_m}\\right)$ error rate, improving upon the best known result by\n\\cite{cui2020plug}, which has additional $H$ and $S$ factors in the main term.\nFurthermore, this bound approaches the\n$\\Omega\\left(\\sqrt{\\frac{1}{Nd_m}}\\right)$ lower bound up to logarithmic\nfactors and a high-order term. To the best of our knowledge, these are the\nfirst set of nearly horizon-free bounds in offline reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:52:17 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Ren", "Tongzheng", ""], ["Li", "Jialian", ""], ["Dai", "Bo", ""], ["Du", "Simon S.", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "2103.14108", "submitter": "Jason W. Rocks", "authors": "Jason W. Rocks and Pankaj Mehta", "title": "The Geometry of Over-parameterized Regression and Adversarial\n  Perturbations", "comments": "11 pages (single column), 4 figures, 10 pages of supporting material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical regression has a simple geometric description in terms of a\nprojection of the training labels onto the column space of the design matrix.\nHowever, for over-parameterized models -- where the number of fit parameters is\nlarge enough to perfectly fit the training data -- this picture becomes\nuninformative. Here, we present an alternative geometric interpretation of\nregression that applies to both under- and over-parameterized models. Unlike\nthe classical picture which takes place in the space of training labels, our\nnew picture resides in the space of input features. This new feature-based\nperspective provides a natural geometric interpretation of the double-descent\nphenomenon in the context of bias and variance, explaining why it can occur\neven in the absence of label noise. Furthermore, we show that adversarial\nperturbations -- small perturbations to the input features that result in large\nchanges in label values -- are a generic feature of biased models, arising from\nthe underlying geometry. We demonstrate these ideas by analyzing three minimal\nmodels for over-parameterized linear least squares regression: without basis\nfunctions (input features equal model features) and with linear or nonlinear\nbasis functions (two-layer neural networks with linear or nonlinear activation\nfunctions, respectively).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:52:08 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 22:11:23 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rocks", "Jason W.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "2103.14203", "submitter": "Chihiro Watanabe", "authors": "Chihiro Watanabe, Taiji Suzuki", "title": "Deep Two-Way Matrix Reordering for Relational Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix reordering is a task to permute the rows and columns of a given\nobserved matrix such that the resulting reordered matrix shows meaningful or\ninterpretable structural patterns. Most existing matrix reordering techniques\nshare the common processes of extracting some feature representations from an\nobserved matrix in a predefined manner, and applying matrix reordering based on\nit. However, in some practical cases, we do not always have prior knowledge\nabout the structural pattern of an observed matrix. To address this problem, we\npropose a new matrix reordering method, called deep two-way matrix reordering\n(DeepTMR), using a neural network model. The trained network can automatically\nextract nonlinear row/column features from an observed matrix, which can then\nbe used for matrix reordering. Moreover, the proposed DeepTMR provides the\ndenoised mean matrix of a given observed matrix as an output of the trained\nnetwork. This denoised mean matrix can be used to visualize the global\nstructure of the reordered observed matrix. We demonstrate the effectiveness of\nthe proposed DeepTMR by applying it to both synthetic and practical datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:31:24 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 15:45:15 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 01:01:39 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 04:06:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Watanabe", "Chihiro", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2103.14224", "submitter": "Gregory Gundersen", "authors": "Gregory W. Gundersen, Diana Cai, Chuteng Zhou, Barbara E. Engelhardt,\n  Ryan P. Adams", "title": "Active multi-fidelity Bayesian online changepoint detection", "comments": "37th Conference on Uncertainty in Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithms for detecting changepoints, or abrupt shifts in the\nbehavior of a time series, are often deployed with limited resources, e.g., to\nedge computing settings such as mobile phones or industrial sensors. In these\nscenarios it may be beneficial to trade the cost of collecting an environmental\nmeasurement against the quality or \"fidelity\" of this measurement and how the\nmeasurement affects changepoint estimation. For instance, one might decide\nbetween inertial measurements or GPS to determine changepoints for motion. A\nBayesian approach to changepoint detection is particularly appealing because we\ncan represent our posterior uncertainty about changepoints and make active,\ncost-sensitive decisions about data fidelity to reduce this posterior\nuncertainty. Moreover, the total cost could be dramatically lowered through\nactive fidelity switching, while remaining robust to changes in data\ndistribution. We propose a multi-fidelity approach that makes cost-sensitive\ndecisions about which data fidelity to collect based on maximizing information\ngain with respect to changepoints. We evaluate this framework on synthetic,\nvideo, and audio data and show that this information-based approach results in\naccurate predictions while reducing total cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:23:54 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 15:29:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gundersen", "Gregory W.", ""], ["Cai", "Diana", ""], ["Zhou", "Chuteng", ""], ["Engelhardt", "Barbara E.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "2103.14238", "submitter": "Wei Chen", "authors": "Wei Chen, Kun Zhang, Ruichu Cai, Biwei Huang, Joseph Ramsey, Zhifeng\n  Hao, Clark Glymour", "title": "FRITL: A Hybrid Method for Causal Discovery in the Presence of Latent\n  Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a particular type of linear\nnon-Gaussian model. Without resorting to the overcomplete Independent Component\nAnalysis (ICA), we show that under some mild assumptions, the model is uniquely\nidentified by a hybrid method. Our method leverages the advantages of\nconstraint-based methods and independent noise-based methods to handle both\nconfounded and unconfounded situations. The first step of our method uses the\nFCI procedure, which allows confounders and is able to produce asymptotically\ncorrect results. The results, unfortunately, usually determine very few\nunconfounded direct causal relations, because whenever it is possible to have a\nconfounder, it will indicate it. The second step of our procedure finds the\nunconfounded causal edges between observed variables among only those adjacent\npairs informed by the FCI results. By making use of the so-called Triad\ncondition, the third step is able to find confounders and their causal\nrelations with other variables. Afterward, we apply ICA on a notably smaller\nset of graphs to identify remaining causal relationships if needed. Extensive\nexperiments on simulated data and real-world data validate the correctness and\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:12:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chen", "Wei", ""], ["Zhang", "Kun", ""], ["Cai", "Ruichu", ""], ["Huang", "Biwei", ""], ["Ramsey", "Joseph", ""], ["Hao", "Zhifeng", ""], ["Glymour", "Clark", ""]]}, {"id": "2103.14350", "submitter": "Gabriel Turinici", "authors": "Gabrel Turinici", "title": "The convergence of the Stochastic Gradient Descent (SGD) : a\n  self-contained proof", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4638695", "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We give here a proof of the convergence of the Stochastic Gradient Descent\n(SGD) in a self-contained manner.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:42:58 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Turinici", "Gabrel", ""]]}, {"id": "2103.14389", "submitter": "Quentin Paris", "authors": "Quentin Paris", "title": "Online learning with exponential weights in metric spaces", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.MG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of online learning in metric spaces using\nexponential weights. We extend the analysis of the exponentially weighted\naverage forecaster, traditionally studied in a Euclidean settings, to a more\nabstract framework. Our results rely on the notion of barycenters, a suitable\nversion of Jensen's inequality and a synthetic notion of lower curvature bound\nin metric spaces known as the measure contraction property. We also adapt the\nonline-to-batch conversion principle to apply our results to a statistical\nlearning framework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:46:10 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Paris", "Quentin", ""]]}, {"id": "2103.14421", "submitter": "Samuel Branders", "authors": "Samuel Branders, Alvaro Pereira, Guillaume Bernard, Marie Ernst,\n  Adelin Albert", "title": "Leveraging Historical Data for High-Dimensional Regression Adjustment, a\n  Composite Covariate Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data collected from patients involved in clinical trials is\ncontinuously growing. All patient characteristics are potential covariates that\ncould be used to improve clinical trial analysis and power. However, the\nrestricted number of patients in phases I and II studies limits the possible\nnumber of covariates included in the analyses. In this paper, we investigate\nthe cost/benefit ratio of including covariates in the analysis of clinical\ntrials. Within this context, we address the long-running question \"What is the\noptimum number of covariates to include in a clinical trial?\" To further\nimprove the cost/benefit ratio of covariates, historical data can be leveraged\nto pre-specify the covariate weights, which can be viewed as the definition of\na new composite covariate. We analyze the use of a composite covariate while\nestimating the treatment effect in small clinical trials. A composite covariate\nlimits the loss of degrees of freedom and the risk of overfitting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:01:17 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Branders", "Samuel", ""], ["Pereira", "Alvaro", ""], ["Bernard", "Guillaume", ""], ["Ernst", "Marie", ""], ["Albert", "Adelin", ""]]}, {"id": "2103.14424", "submitter": "Roger Guimera", "authors": "Oscar Fajardo-Fontiveros, Marta Sales-Pardo, Roger Guimera", "title": "Node metadata can produce predictability transitions in network\n  inference problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG cs.SI physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Network inference is the process of learning the properties of complex\nnetworks from data. Besides using information about known links in the network,\nnode attributes and other forms of network metadata can help to solve network\ninference problems. Indeed, several approaches have been proposed to introduce\nmetadata into probabilistic network models and to use them to make better\ninferences. However, we know little about the effect of such metadata in the\ninference process. Here, we investigate this issue. We find that, rather than\naffecting inference gradually, adding metadata causes abrupt transitions in the\ninference process and in our ability to make accurate predictions, from a\nsituation in which metadata does not play any role to a situation in which\nmetadata completely dominates the inference process. When network data and\nmetadata are partly correlated, metadata optimally contributes to the inference\nprocess at the transition between data-dominated and metadata-dominated\nregimes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:08:07 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fajardo-Fontiveros", "Oscar", ""], ["Sales-Pardo", "Marta", ""], ["Guimera", "Roger", ""]]}, {"id": "2103.14430", "submitter": "Mariana Clare", "authors": "Mariana Clare and Omar Jamil and Cyril Morcrette", "title": "A computationally efficient neural network for predicting weather\n  forecast probabilities", "comments": "19 pages, 13 figures, Github repository:\n  https://github.com/mc4117/ResNet_Weather, Submitted to Quarterly Journal of\n  the Royal Meteorological Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of deep learning techniques over the last decades has opened up a\nnew avenue of research for weather forecasting. Here, we take the novel\napproach of using a neural network to predict probability density functions\nrather than a single output value, thus producing a probabilistic weather\nforecast. This enables the calculation of both uncertainty and skill metrics\nfor the neural network predictions, and overcomes the common difficulty of\ninferring uncertainty from these predictions.\n  This approach is purely data-driven and the neural network is trained on the\nWeatherBench dataset (processed ERA5 data) to forecast geopotential and\ntemperature 3 and 5 days ahead. An extensive data exploration leads to the\nidentification of the most important input variables, which are also found to\nagree with physical reasoning, thereby validating our approach. In order to\nincrease computational efficiency further, each neural network is trained on a\nsmall subset of these variables. The outputs are then combined through a\nstacked neural network, the first time such a technique has been applied to\nweather data. Our approach is found to be more accurate than some numerical\nweather prediction models and as accurate as more complex alternative neural\nnetworks, with the added benefit of providing key probabilistic information\nnecessary for making informed weather forecasts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:28:15 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Clare", "Mariana", ""], ["Jamil", "Omar", ""], ["Morcrette", "Cyril", ""]]}, {"id": "2103.14539", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas\n  Kerren", "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise\n  Selection and Semi-Automatic Extraction Approaches", "comments": "This manuscript is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning (ML) life cycle involves a series of iterative steps,\nfrom the effective gathering and preparation of the data, including complex\nfeature engineering processes, to the presentation and improvement of results,\nwith various algorithms to choose from in every step. Feature engineering in\nparticular can be very beneficial for ML, leading to numerous improvements such\nas boosting the predictive results, decreasing computational times, reducing\nexcessive noise, and increasing the transparency behind the decisions taken\nduring the training. Despite that, while several visual analytics tools exist\nto monitor and control the different stages of the ML life cycle (especially\nthose related to data and algorithms), feature engineering support remains\ninadequate. In this paper, we present FeatureEnVi, a visual analytics system\nspecifically designed to assist with the feature engineering process. Our\nproposed system helps users to choose the most important feature, to transform\nthe original features into powerful alternatives, and to experiment with\ndifferent feature generation combinations. Additionally, data space slicing\nallows users to explore the impact of features on both local and global scales.\nFeatureEnVi utilizes multiple automatic feature selection techniques;\nfurthermore, it visually guides users with statistical evidence about the\ninfluence of each feature (or subsets of features). The final outcome is the\nextraction of heavily engineered features, evaluated by multiple validation\nmetrics. The usefulness and applicability of FeatureEnVi are demonstrated with\ntwo use cases, using a popular red wine quality data set and publicly available\ndata related to vehicle recognition from their silhouettes. We also report\nfeedback from interviews with ML experts and a visualization researcher who\nassessed the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:45:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kucher", "Kostiantyn", ""], ["Kerren", "Andreas", ""]]}, {"id": "2103.14575", "submitter": "Jack Y. Araz", "authors": "Jack Y. Araz, Juan Carlos Criado and Michael Spannowsky", "title": "Elvet -- a neural network-based differential equation and variational\n  problem solver", "comments": "24 pages, 2 figures. typo fixed and added link to Google Colab for\n  examples", "journal-ref": null, "doi": null, "report-no": "IPPP/20/87", "categories": "cs.LG hep-lat hep-ph hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Elvet, a Python package for solving differential equations and\nvariational problems using machine learning methods. Elvet can deal with any\nsystem of coupled ordinary or partial differential equations with arbitrary\ninitial and boundary conditions. It can also minimize any functional that\ndepends on a collection of functions of several variables while imposing\nconstraints on them. The solution to any of these problems is represented as a\nneural network trained to produce the desired function.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:40:49 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 12:46:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Araz", "Jack Y.", ""], ["Criado", "Juan Carlos", ""], ["Spannowsky", "Michael", ""]]}, {"id": "2103.14608", "submitter": "Sebastian Damrich", "authors": "Sebastian Damrich and Fred A. Hamprecht", "title": "On UMAP's true loss function", "comments": "20 pages, 15 figures; minor changes, added run-times and error bars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UMAP has supplanted t-SNE as state-of-the-art for visualizing\nhigh-dimensional datasets in many disciplines, but the reason for its success\nis not well understood. In this work, we investigate UMAP's sampling based\noptimization scheme in detail. We derive UMAP's effective loss function in\nclosed form and find that it differs from the published one. As a consequence,\nwe show that UMAP does not aim to reproduce its theoretically motivated\nhigh-dimensional UMAP similarities. Instead, it tries to reproduce similarities\nthat only encode the shared $k$ nearest neighbor graph, thereby challenging the\nprevious understanding of UMAP's effectiveness. Instead, we claim that the key\nto UMAP's success is its implicit balancing of attraction and repulsion\nresulting from negative sampling. This balancing in turn facilitates\noptimization via gradient descent. We corroborate our theoretical findings on\ntoy and single cell RNA sequencing data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:22:58 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 13:22:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Damrich", "Sebastian", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "2103.14686", "submitter": "Matteo Favoni", "authors": "Srinath Bulusu, Matteo Favoni, Andreas Ipp, David I. M\\\"uller, Daniel\n  Schuh", "title": "Generalization capabilities of translationally equivariant neural\n  networks", "comments": "25 pages, 18 figures, v2: updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising adoption of machine learning in high energy physics and lattice\nfield theory necessitates the re-evaluation of common methods that are widely\nused in computer vision, which, when applied to problems in physics, can lead\nto significant drawbacks in terms of performance and generalizability. One\nparticular example for this is the use of neural network architectures that do\nnot reflect the underlying symmetries of the given physical problem. In this\nwork, we focus on complex scalar field theory on a two-dimensional lattice and\ninvestigate the benefits of using group equivariant convolutional neural\nnetwork architectures based on the translation group. For a meaningful\ncomparison, we conduct a systematic search for equivariant and non-equivariant\nneural network architectures and apply them to various regression and\nclassification tasks. We demonstrate that in most of these tasks our best\nequivariant architectures can perform and generalize significantly better than\ntheir non-equivariant counterparts, which applies not only to physical\nparameters beyond those represented in the training set, but also to different\nlattice sizes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:53:36 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:30:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bulusu", "Srinath", ""], ["Favoni", "Matteo", ""], ["Ipp", "Andreas", ""], ["M\u00fcller", "David I.", ""], ["Schuh", "Daniel", ""]]}, {"id": "2103.14723", "submitter": "Ofer Zeitouni", "authors": "Inbar Seroussi, Ofer Zeitouni", "title": "Lower Bounds on the Generalization Error of Nonlinear Learning Models", "comments": "Minor correction+conference information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study in this paper lower bounds for the generalization error of models\nderived from multi-layer neural networks, in the regime where the size of the\nlayers is commensurate with the number of samples in the training data. We show\nthat unbiased estimators have unacceptable performance for such nonlinear\nnetworks in this regime. We derive explicit generalization lower bounds for\ngeneral biased estimators, in the cases of linear regression and of two-layered\nnetworks. In the linear case the bound is asymptotically tight. In the\nnonlinear case, we provide a comparison of our bounds with an empirical study\nof the stochastic gradient descent algorithm. The analysis uses elements from\nthe theory of large random matrices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:37:54 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 22:45:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Seroussi", "Inbar", ""], ["Zeitouni", "Ofer", ""]]}, {"id": "2103.14749", "submitter": "Curtis Northcutt", "authors": "Curtis G. Northcutt, Anish Athalye, Jonas Mueller", "title": "Pervasive Label Errors in Test Sets Destabilize Machine Learning\n  Benchmarks", "comments": null, "journal-ref": "ICLR 2021 RobustML and Weakly Supervised Learning Workshops;\n  NeurIPS 2020 Workshop on Dataset Curation and Security", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We algorithmically identify label errors in the test sets of 10 of the most\ncommonly-used computer vision, natural language, and audio datasets, and\nsubsequently study the potential for these label errors to affect benchmark\nresults. Errors in test sets are numerous and widespread: we estimate an\naverage of 3.4% errors across the 10 datasets, where for example 2916 label\nerrors comprise 6% of the ImageNet validation set. Putative label errors are\nfound using confident learning and then human-validated via crowdsourcing (54%\nof the algorithmically-flagged candidates are indeed erroneously labeled).\nSurprisingly, we find that lower capacity models may be practically more useful\nthan higher capacity models in real-world datasets with high proportions of\nerroneously labeled data. For example, on ImageNet with corrected labels:\nResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test\nexamples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11\noutperforms VGG-19 if the prevalence of originally mislabeled test examples\nincreases by 5%. Traditionally, ML practitioners choose which model to deploy\nbased on test accuracy -- our findings advise caution here, proposing that\njudging models over correctly labeled test sets may be more useful, especially\nfor noisy real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:54:36 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 02:32:02 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 19:41:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Northcutt", "Curtis G.", ""], ["Athalye", "Anish", ""], ["Mueller", "Jonas", ""]]}, {"id": "2103.14755", "submitter": "Robert Hu", "authors": "David Rindt and Robert Hu and David Steinsaltz and Dino Sejdinovic", "title": "Time-to-event regression using partially monotonic neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel method, termed SuMo-net, that uses partially monotonic\nneural networks to learn a time-to-event distribution from a sample of\ncovariates and right-censored times. SuMo-net models the survival function and\nthe density jointly, and optimizes the likelihood for right-censored data\ninstead of the often used partial likelihood. The method does not make\nassumptions about the true survival distribution and avoids computationally\nexpensive integration of the hazard function. We evaluate the performance of\nthe method on a range of datasets and find competitive performance across\ndifferent metrics and improved computational time of making new predictions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 22:34:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Rindt", "David", ""], ["Hu", "Robert", ""], ["Steinsaltz", "David", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2103.14779", "submitter": "Manish Singh", "authors": "Manish K. Singh, Vassilis Kekatos, and Georgios B. Giannakis", "title": "Learning to Solve the AC-OPF using Sensitivity-Informed Deep Neural\n  Networks", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To shift the computational burden from real-time to offline in delay-critical\npower systems applications, recent works entertain the idea of using a deep\nneural network (DNN) to predict the solutions of the AC optimal power flow\n(AC-OPF) once presented load demands. As network topologies may change,\ntraining this DNN in a sample-efficient manner becomes a necessity. To improve\ndata efficiency, this work utilizes the fact OPF data are not simple training\nlabels, but constitute the solutions of a parametric optimization problem. We\nthus advocate training a sensitivity-informed DNN (SI-DNN) to match not only\nthe OPF optimizers, but also their partial derivatives with respect to the OPF\nparameters (loads). It is shown that the required Jacobian matrices do exist\nunder mild conditions, and can be readily computed from the related primal/dual\nsolutions. The proposed SI-DNN is compatible with a broad range of OPF solvers,\nincluding a non-convex quadratically constrained quadratic program (QCQP), its\nsemidefinite program (SDP) relaxation, and MATPOWER; while SI-DNN can be\nseamlessly integrated in other learning-to-OPF schemes. Numerical tests on\nthree benchmark power systems corroborate the advanced generalization and\nconstraint satisfaction capabilities for the OPF solutions predicted by an\nSI-DNN over a conventionally trained DNN, especially in low-data setups.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 00:45:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Singh", "Manish K.", ""], ["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "2103.14963", "submitter": "Carl Ringqvist Mr", "authors": "Adam Lindhe, Carl Ringqvist and Henrik Hult", "title": "Particle Filter Bridge Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Auto encoding models have been extensively studied in recent years. They\nprovide an efficient framework for sample generation, as well as for analysing\nfeature learning. Furthermore, they are efficient in performing interpolations\nbetween data-points in semantically meaningful ways. In this paper, we build\nfurther on a previously introduced method for generating canonical, dimension\nindependent, stochastic interpolations. Here, the distribution of interpolation\npaths is represented as the distribution of a bridge process constructed from\nan artificial random data generating process in the latent space, having the\nprior distribution as its invariant distribution. As a result the stochastic\ninterpolation paths tend to reside in regions of the latent space where the\nprior has high mass. This is a desirable feature since, generally, such areas\nproduce semantically meaningful samples. In this paper, we extend the bridge\nprocess method by introducing a discriminator network that accurately\nidentifies areas of high latent representation density. The discriminator\nnetwork is incorporated as a change of measure of the underlying bridge process\nand sampling of interpolation paths is implemented using sequential Monte\nCarlo. The resulting sampling procedure allows for greater variability in\ninterpolation paths and stronger drift towards areas of high data density.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:33:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lindhe", "Adam", ""], ["Ringqvist", "Carl", ""], ["Hult", "Henrik", ""]]}, {"id": "2103.14991", "submitter": "Min Chen", "authors": "Min Chen and Zhikun Zhang and Tianhao Wang and Michael Backes and\n  Mathias Humbert and Yang Zhang", "title": "Graph Unlearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The right to be forgotten states that a data subject has the right to erase\ntheir data from an entity storing it. In the context of machine learning (ML),\nit requires the ML model provider to remove the data subject's data from the\ntraining set used to build the ML model, a process known as \\textit{machine\nunlearning}. While straightforward and legitimate, retraining the ML model from\nscratch upon receiving unlearning requests incurs high computational overhead\nwhen the training set is large. To address this issue, a number of approximate\nalgorithms have been proposed in the domain of image and text data, among which\nSISA is the state-of-the-art solution. It randomly partitions the training set\ninto multiple shards and trains a constituent model for each shard. However,\ndirectly applying SISA to the graph data can severely damage the graph\nstructural information, and thereby the resulting ML model utility.\n  In this paper, we propose GraphEraser, a novel machine unlearning method\ntailored to graph data. Its contributions include two novel graph partition\nalgorithms, and a learning-based aggregation method. We conduct extensive\nexperiments on five real-world datasets to illustrate the unlearning efficiency\nand model utility of GraphEraser. We observe that GraphEraser achieves\n2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time\nimprovement compared to retraining from scratch. On the other hand, GraphEraser\nachieves up to $62.5\\%$ higher F1 score than that of random partitioning. In\naddition, our proposed learning-based aggregation method achieves up to $112\\%$\nhigher F1 score than that of the majority vote aggregation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:38:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Min", ""], ["Zhang", "Zhikun", ""], ["Wang", "Tianhao", ""], ["Backes", "Michael", ""], ["Humbert", "Mathias", ""], ["Zhang", "Yang", ""]]}, {"id": "2103.15035", "submitter": "Yaoming Zhen", "authors": "Yaoming Zhen and Junhui Wang", "title": "Community Detection in General Hypergraph via Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data has attracted tremendous attention in recent years, and most\nconventional networks focus on pairwise interactions between two vertices.\nHowever, real-life network data may display more complex structures, and\nmulti-way interactions among vertices arise naturally. In this article, we\npropose a novel method for detecting community structure in general hypergraph\nnetworks, uniform or non-uniform. The proposed method introduces a null vertex\nto augment a non-uniform hypergraph into a uniform multi-hypergraph, and then\nembeds the multi-hypergraph in a low-dimensional vector space such that\nvertices within the same community are close to each other. The resultant\noptimization task can be efficiently tackled by an alternative updating scheme.\nThe asymptotic consistencies of the proposed method are established in terms of\nboth community detection and hypergraph estimation, which are also supported by\nnumerical experiments on some synthetic and real-life hypergraph networks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 03:23:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhen", "Yaoming", ""], ["Wang", "Junhui", ""]]}, {"id": "2103.15106", "submitter": "Alexis Bellot", "authors": "Alexis Bellot, Mihaela van der Schaar", "title": "Deconfounded Score Method: Scoring DAGs with Dense Unobserved\n  Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved confounding is one of the greatest challenges for causal\ndiscovery. The case in which unobserved variables have a widespread effect on\nmany of the observed ones is particularly difficult because most pairs of\nvariables are conditionally dependent given any other subset, rendering the\ncausal effect unidentifiable. In this paper we show that beyond conditional\nindependencies, under the principle of independent mechanisms, unobserved\nconfounding in this setting leaves a statistical footprint in the observed data\ndistribution that allows for disentangling spurious and causal effects. Using\nthis insight, we demonstrate that a sparse linear Gaussian directed acyclic\ngraph among observed variables may be recovered approximately and propose an\nadjusted score-based causal discovery algorithm that may be implemented with\ngeneral purpose solvers and scales to high-dimensional problems. We find, in\naddition, that despite the conditions we pose to guarantee causal recovery,\nperformance in practice is robust to large deviations in model assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 11:07:59 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 09:25:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bellot", "Alexis", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2103.15157", "submitter": "Gabriele Tornetta", "authors": "Gabriele N. Tornetta", "title": "Entropy methods for the confidence assessment of probabilistic\n  classification models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many classification models produce a probability distribution as the outcome\nof a prediction. This information is generally compressed down to the single\nclass with the highest associated probability. In this paper, we argue that\npart of the information that is discarded in this process can be in fact used\nto further evaluate the goodness of models, and in particular the confidence\nwith which each prediction is made. As an application of the ideas presented in\nthis paper, we provide a theoretical explanation of a confidence degradation\nphenomenon observed in the complement approach to the (Bernoulli) Naive Bayes\ngenerative model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:39:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tornetta", "Gabriele N.", ""]]}, {"id": "2103.15261", "submitter": "Vatsal Sharan", "authors": "Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal\n  Sharan, Xin Wang, Qiuyi Zhang", "title": "One Network Fits All? Modular versus Monolithic Task Formulations in\n  Neural Networks", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can deep learning solve multiple tasks simultaneously, even when they are\nunrelated and very different? We investigate how the representations of the\nunderlying tasks affect the ability of a single neural network to learn them\njointly. We present theoretical and empirical findings that a single neural\nnetwork is capable of simultaneously learning multiple tasks from a combined\ndata set, for a variety of methods for representing tasks -- for example, when\nthe distinct tasks are encoded by well-separated clusters or decision trees\nover certain task-code attributes. More concretely, we present a novel analysis\nthat shows that families of simple programming-like constructs for the codes\nencoding the tasks are learnable by two-layer neural networks with standard\ntraining. We study more generally how the complexity of learning such combined\ntasks grows with the complexity of the task codes; we find that combining many\ntasks may incur a sample complexity penalty, even though the individual tasks\nare easy to learn. We provide empirical support for the usefulness of the\nlearning bounds by training networks on clusters, decision trees, and SQL-style\naggregation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 01:16:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Agarwala", "Atish", ""], ["Das", "Abhimanyu", ""], ["Juba", "Brendan", ""], ["Panigrahy", "Rina", ""], ["Sharan", "Vatsal", ""], ["Wang", "Xin", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "2103.15319", "submitter": "Michael Tetelman", "authors": "Michael Tetelman", "title": "Bayesian Attention Networks for Data Compression", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The lossless data compression algorithm based on Bayesian Attention Networks\nis derived from first principles. Bayesian Attention Networks are defined by\nintroducing an attention factor per a training sample loss as a function of two\nsample inputs, from training sample and prediction sample. By using a sharpened\nJensen's inequality we show that the attention factor is completely defined by\na correlation function of the two samples w.r.t. the model weights. Due to the\nattention factor the solution for a prediction sample is mostly defined by a\nfew training samples that are correlated with the prediction sample. Finding a\nspecific solution per prediction sample couples together the training and the\nprediction. To make the approach practical we introduce a latent space to map\neach prediction sample to a latent space and learn all possible solutions as a\nfunction of the latent space along with learning attention as a function of the\nlatent space and a training sample. The latent space plays a role of the\ncontext representation with a prediction sample defining a context and a\nlearned context dependent solution used for the prediction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:11:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tetelman", "Michael", ""]]}, {"id": "2103.15342", "submitter": "Yiming Xu", "authors": "Yiming Xu, Vahid Keshavarzzadeh, Robert M. Kirby, Akil Narayan", "title": "A bandit-learning approach to multifidelity approximation", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifidelity approximation is an important technique in scientific\ncomputation and simulation. In this paper, we introduce a bandit-learning\napproach for leveraging data of varying fidelities to achieve precise estimates\nof the parameters of interest. Under a linear model assumption, we formulate a\nmultifidelity approximation as a modified stochastic bandit, and analyze the\nloss for a class of policies that uniformly explore each model before\nexploiting. Utilizing the estimated conditional mean-squared error, we propose\na consistent algorithm, adaptive Explore-Then-Commit (AETC), and establish a\ncorresponding trajectory-wise optimality result. These results are then\nextended to the case of vector-valued responses, where we demonstrate that the\nalgorithm is efficient without the need to worry about estimating\nhigh-dimensional parameters. The main advantage of our approach is that we\nrequire neither hierarchical model structure nor \\textit{a priori} knowledge of\nstatistical information (e.g., correlations) about or between models. Instead,\nthe AETC algorithm requires only knowledge of which model is a trusted\nhigh-fidelity model, along with (relative) computational cost estimates of\nquerying each model. Numerical experiments are provided at the end to support\nour theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:29:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xu", "Yiming", ""], ["Keshavarzzadeh", "Vahid", ""], ["Kirby", "Robert M.", ""], ["Narayan", "Akil", ""]]}, {"id": "2103.15352", "submitter": "Daogao Liu", "authors": "Janardhan Kulkarni, Yin Tat Lee, Daogao Liu", "title": "Private Non-smooth Empirical Risk Minimization and Stochastic Convex\n  Optimization in Subquadratic Steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the differentially private Empirical Risk Minimization (ERM) and\nStochastic Convex Optimization (SCO) problems for non-smooth convex functions.\nWe get a (nearly) optimal bound on the excess empirical risk and excess\npopulation loss with subquadratic gradient complexity. More precisely, our\ndifferentially private algorithm requires $O(\\frac{N^{3/2}}{d^{1/8}}+\n\\frac{N^2}{d})$ gradient queries for optimal excess empirical risk, which is\nachieved with the help of subsampling and smoothing the function via\nconvolution. This is the first subquadratic algorithm for the non-smooth case\nwhen $d$ is super constant. As a direct application, using the iterative\nlocalization approach of Feldman et al. \\cite{fkt20}, we achieve the optimal\nexcess population loss for stochastic convex optimization problem, with\n$O(\\min\\{N^{5/4}d^{1/8},\\frac{ N^{3/2}}{d^{1/8}}\\})$ gradient queries. Our work\nmakes progress towards resolving a question raised by Bassily et al.\n\\cite{bfgt20}, giving first algorithms for private ERM and SCO with\nsubquadratic steps.\n  We note that independently Asi et al. \\cite{afkt21} gave other algorithms for\nprivate ERM and SCO with subquadratic steps.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:58:56 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 02:45:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kulkarni", "Janardhan", ""], ["Lee", "Yin Tat", ""], ["Liu", "Daogao", ""]]}, {"id": "2103.15532", "submitter": "See Hian Lee", "authors": "See Hian Lee, Feng Ji, Wee Peng Tay", "title": "Learning on heterogeneous graphs using high-order relations", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9413417", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A heterogeneous graph consists of different vertices and edges types.\nLearning on heterogeneous graphs typically employs meta-paths to deal with the\nheterogeneity by reducing the graph to a homogeneous network, guide random\nwalks or capture semantics. These methods are however sensitive to the choice\nof meta-paths, with suboptimal paths leading to poor performance. In this\npaper, we propose an approach for learning on heterogeneous graphs without\nusing meta-paths. Specifically, we decompose a heterogeneous graph into\ndifferent homogeneous relation-type graphs, which are then combined to create\nhigher-order relation-type representations. These representations preserve the\nheterogeneity of edges and retain their edge directions while capturing the\ninteraction of different vertex types multiple hops apart. This is then\ncomplemented with attention mechanisms to distinguish the importance of the\nrelation-type based neighbors and the relation-types themselves. Experiments\ndemonstrate that our model generally outperforms other state-of-the-art\nbaselines in the vertex classification task on three commonly studied\nheterogeneous graph datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:02:47 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lee", "See Hian", ""], ["Ji", "Feng", ""], ["Tay", "Wee Peng", ""]]}, {"id": "2103.15540", "submitter": "Johan Pensar", "authors": "Johan Pensar and Henrik Nyman and Jukka Corander", "title": "Structure Learning of Contextual Markov Networks using Marginal\n  Pseudo-likelihood", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, Vol. 44: 455-479, 2017", "doi": "10.1111/sjos.12260", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are popular models for discrete multivariate systems where\nthe dependence structure of the variables is specified by an undirected graph.\nTo allow for more expressive dependence structures, several generalizations of\nMarkov networks have been proposed. Here we consider the class of contextual\nMarkov networks which takes into account possible context-specific\nindependences among pairs of variables. Structure learning of contextual Markov\nnetworks is very challenging due to the extremely large number of possible\nstructures. One of the main challenges has been to design a score, by which a\nstructure can be assessed in terms of model fit related to complexity, without\nassuming chordality. Here we introduce the marginal pseudo-likelihood as an\nanalytically tractable criterion for general contextual Markov networks. Our\ncriterion is shown to yield a consistent structure estimator. Experiments\ndemonstrate the favorable properties of our method in terms of predictive\naccuracy of the inferred models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:13:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Corander", "Jukka", ""]]}, {"id": "2103.15569", "submitter": "Piyush Kumar", "authors": "Spencer Douglas, Piyush Kumar, R.K. Prasanth", "title": "Risk Bounds for Learning via Hilbert Coresets", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a formalism for constructing stochastic upper bounds on the\nexpected full sample risk for supervised classification tasks via the Hilbert\ncoresets approach within a transductive framework. We explicitly compute tight\nand meaningful bounds for complex datasets and complex hypothesis classes such\nas state-of-the-art deep neural network architectures. The bounds we develop\nexhibit nice properties: i) the bounds are non-uniform in the hypothesis space,\nii) in many practical examples, the bounds become effectively deterministic by\nappropriate choice of prior and training data-dependent posterior distributions\non the hypothesis space, and iii) the bounds become significantly better with\nincrease in the size of the training set. We also lay out some ideas to explore\nfor future research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:39:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Douglas", "Spencer", ""], ["Kumar", "Piyush", ""], ["Prasanth", "R. K.", ""]]}, {"id": "2103.15624", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger and Fabricio Olivetti de Fran\\c{c}a and Bogdan\n  Burlacu and Christian Haider and Michael Kommenda", "title": "Shape-constrained Symbolic Regression -- Improving Extrapolation with\n  Prior Knowledge", "comments": null, "journal-ref": null, "doi": "10.1162/evco_a_00294", "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the addition of constraints on the function image and its\nderivatives for the incorporation of prior knowledge in symbolic regression.\nThe approach is called shape-constrained symbolic regression and allows us to\nenforce e.g. monotonicity of the function over selected inputs. The aim is to\nfind models which conform to expected behaviour and which have improved\nextrapolation capabilities. We demonstrate the feasibility of the idea and\npropose and compare two evolutionary algorithms for shape-constrained symbolic\nregression: i) an extension of tree-based genetic programming which discards\ninfeasible solutions in the selection step, and ii) a two population\nevolutionary algorithm that separates the feasible from the infeasible\nsolutions. In both algorithms we use interval arithmetic to approximate bounds\nfor models and their partial derivatives. The algorithms are tested on a set of\n19 synthetic and four real-world regression problems. Both algorithms are able\nto identify models which conform to shape constraints which is not the case for\nthe unmodified symbolic regression algorithms. However, the predictive accuracy\nof models with constraints is worse on the training set and the test set.\nShape-constrained polynomial regression produces the best results for the test\nset but also significantly larger models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:04:18 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 09:56:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kronberger", "Gabriel", ""], ["de Fran\u00e7a", "Fabricio Olivetti", ""], ["Burlacu", "Bogdan", ""], ["Haider", "Christian", ""], ["Kommenda", "Michael", ""]]}, {"id": "2103.15636", "submitter": "Souvik Chakraborty", "authors": "Shailesh Garg and Ankush Gogoi and Souvik Chakraborty and Budhaditya\n  Hazra", "title": "Machine learning based digital twin for stochastic nonlinear\n  multi-degree of freedom dynamical system", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The potential of digital twin technology is immense, specifically in the\ninfrastructure, aerospace, and automotive sector. However, practical\nimplementation of this technology is not at an expected speed, specifically\nbecause of lack of application-specific details. In this paper, we propose a\nnovel digital twin framework for stochastic nonlinear multi-degree of freedom\n(MDOF) dynamical systems. The approach proposed in this paper strategically\ndecouples the problem into two time-scales -- (a) a fast time-scale governing\nthe system dynamics and (b) a slow time-scale governing the degradation in the\nsystem. The proposed digital twin has four components - (a) a physics-based\nnominal model (low-fidelity), (b) a Bayesian filtering algorithm a (c) a\nsupervised machine learning algorithm and (d) a high-fidelity model for\npredicting future responses. The physics-based nominal model combined with\nBayesian filtering is used combined parameter state estimation and the\nsupervised machine learning algorithm is used for learning the temporal\nevolution of the parameters. While the proposed framework can be used with any\nchoice of Bayesian filtering and machine learning algorithm, we propose to use\nunscented Kalman filter and Gaussian process. Performance of the proposed\napproach is illustrated using two examples. Results obtained indicate the\napplicability and excellent performance of the proposed digital twin framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:14:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Garg", "Shailesh", ""], ["Gogoi", "Ankush", ""], ["Chakraborty", "Souvik", ""], ["Hazra", "Budhaditya", ""]]}, {"id": "2103.15664", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski and Ali H. Sayed", "title": "Competing Adaptive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive networks have the capability to pursue solutions of global\nstochastic optimization problems by relying only on local interactions within\nneighborhoods. The diffusion of information through repeated interactions\nallows for globally optimal behavior, without the need for central\ncoordination. Most existing strategies are developed for cooperative learning\nsettings, where the objective of the network is common to all agents. We\nconsider in this work a team setting, where a subset of the agents form a team\nwith a common goal while competing with the remainder of the network. We\ndevelop an algorithm for decentralized competition among teams of adaptive\nagents, analyze its dynamics and present an application in the decentralized\ntraining of generative adversarial neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:42:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2103.15758", "submitter": "Sebastian Weichwald", "authors": "Eigil F. Rischel, Sebastian Weichwald", "title": "Compositional Abstraction Error and a Category of Causal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventional causal models describe joint distributions over some variables\nused to describe a system, one for each intervention setting. They provide a\nformal recipe for how to move between joint distributions and make predictions\nabout the variables upon intervening on the system. Yet, it is difficult to\nformalise how we may change the underlying variables used to describe the\nsystem, say from fine-grained to coarse-grained variables. Here, we argue that\ncompositionality is a desideratum for model transformations and the associated\nerrors. We develop a framework for model transformations and abstractions with\na notion of error that is compositional: when abstracting a reference model M\nmodularly, first obtaining M' and then further simplifying that to obtain M'',\nthen the composite transformation from M to M'' exists and its error can be\nbounded by the errors incurred by each individual transformation step. Category\ntheory, the study of mathematical objects via the compositional transformations\nbetween them, offers a natural language for developing our framework. We\nintroduce a category of finite interventional causal models and, leveraging\ntheory of enriched categories, prove that our framework enjoys the desired\ncompositionality properties.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:48:12 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rischel", "Eigil F.", ""], ["Weichwald", "Sebastian", ""]]}, {"id": "2103.15783", "submitter": "Sam Polk", "authors": "Sam L. Polk and James M. Murphy", "title": "Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial\n  Diffusion Geometry", "comments": "(6 pages, 2 figures). To appear in Proceedings of IEEE IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms partition a dataset into groups of similar points. The\nprimary contribution of this article is the Multiscale Spatially-Regularized\nDiffusion Learning (M-SRDL) clustering algorithm, which uses\nspatially-regularized diffusion distances to efficiently and accurately learn\nmultiple scales of latent structure in hyperspectral images (HSI). The M-SRDL\nclustering algorithm extracts clusterings at many scales from an HSI and\noutputs these clusterings' variation of information-barycenter as an exemplar\nfor all underlying cluster structure. We show that incorporating spatial\nregularization into a multiscale clustering framework corresponds to smoother\nand more coherent clusters when applied to HSI data and leads to more accurate\nclustering labels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:24:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Polk", "Sam L.", ""], ["Murphy", "James M.", ""]]}, {"id": "2103.15798", "submitter": "Mikhail Khodak", "authors": "Nicholas Roberts and Mikhail Khodak and Tri Dao and Liam Li and\n  Christopher R\\'e and Ameet Talwalkar", "title": "Rethinking Neural Operations for Diverse Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of neural architecture search (NAS) is to automate-away the\ndesign of neural networks on new tasks in under-explored domains. Motivated by\nthis broader vision for NAS, we study the problem of enabling users to discover\nthe right neural operations given data from their specific domain. We introduce\na search space of neural operations called XD-Operations that mimic the\ninductive bias of standard multichannel convolutions while being much more\nexpressive: we prove that XD-operations include many named operations across\nseveral application areas. Starting with any standard backbone network such as\nLeNet or ResNet, we show how to transform it into an architecture search space\nover XD-operations and how to traverse the space using a simple weight-sharing\nscheme. On a diverse set of applications--image classification, solving partial\ndifferential equations (PDEs), and sequence modeling--our approach consistently\nyields models with lower error than baseline networks and sometimes even lower\nerror than expert-designed domain-specific approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:50:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Roberts", "Nicholas", ""], ["Khodak", "Mikhail", ""], ["Dao", "Tri", ""], ["Li", "Liam", ""], ["R\u00e9", "Christopher", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2103.15864", "submitter": "Agnimitra Dasgupta", "authors": "Agnimitra Dasgupta and Carlo Graziani and Zichao Wendy Di", "title": "Gaussian Process for Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic reconstruction, despite its revolutionary impact on a wide range\nof applications, suffers from its ill-posed nature in that there is no unique\nsolution because of limited and noisy measurements. Traditional\noptimization-based reconstruction relies on regularization to address this\nissue; however, it faces its own challenge because the type of regularizer and\nchoice of regularization parameter are a critical but difficult decision.\nMoreover, traditional reconstruction yields point estimates for the\nreconstruction with no further indication of the quality of the solution. In\nthis work we address these challenges by exploring Gaussian processes (GPs).\nOur proposed GP approach yields not only the reconstructed object through the\nposterior mean but also a quantification of the solution uncertainties through\nthe posterior covariance. Furthermore, we explore the flexibility of the GP\nframework to provide a robust model of the information across various length\nscales in the object, as well as the complex noise in the measurements. We\nillustrate the proposed approach on both synthetic and real tomography images\nand show its unique capability of uncertainty quantification in the presence of\nvarious types of noise, as well as reconstruction comparison with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:16:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dasgupta", "Agnimitra", ""], ["Graziani", "Carlo", ""], ["Di", "Zichao Wendy", ""]]}, {"id": "2103.15888", "submitter": "Siqi Zhang", "authors": "Siqi Zhang, Junchi Yang, Crist\\'obal Guzm\\'an, Negar Kiyavash, Niao He", "title": "The Complexity of Nonconvex-Strongly-Concave Minimax Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity for finding approximate stationary points\nof nonconvex-strongly-concave (NC-SC) smooth minimax problems, in both general\nand averaged smooth finite-sum settings. We establish nontrivial lower\ncomplexity bounds of $\\Omega(\\sqrt{\\kappa}\\Delta L\\epsilon^{-2})$ and\n$\\Omega(n+\\sqrt{n\\kappa}\\Delta L\\epsilon^{-2})$ for the two settings,\nrespectively, where $\\kappa$ is the condition number, $L$ is the smoothness\nconstant, and $\\Delta$ is the initial gap. Our result reveals substantial gaps\nbetween these limits and best-known upper bounds in the literature. To close\nthese gaps, we introduce a generic acceleration scheme that deploys existing\ngradient-based methods to solve a sequence of crafted\nstrongly-convex-strongly-concave subproblems. In the general setting, the\ncomplexity of our proposed algorithm nearly matches the lower bound; in\nparticular, it removes an additional poly-logarithmic dependence on accuracy\npresent in previous works. In the averaged smooth finite-sum setting, our\nproposed algorithm improves over previous algorithms by providing a\nnearly-tight dependence on the condition number.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:53:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Siqi", ""], ["Yang", "Junchi", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""], ["Kiyavash", "Negar", ""], ["He", "Niao", ""]]}, {"id": "2103.15917", "submitter": "Nicola Bulso", "authors": "Nicola Bulso, Yasser Roudi", "title": "Restricted Boltzmann Machines as Models of Interacting Variables", "comments": "Supplemental material is available as ancillary file and can be\n  downloaded from a link on the right", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the type of distributions that Restricted Boltzmann Machines (RBMs)\nwith different activation functions can express by investigating the effect of\nthe activation function of the hidden nodes on the marginal distribution they\nimpose on observed binary nodes. We report an exact expression for these\nmarginals in the form of a model of interacting binary variables with the\nexplicit form of the interactions depending on the hidden node activation\nfunction. We study the properties of these interactions in detail and evaluate\nhow the accuracy with which the RBM approximates distributions over binary\nvariables depends on the hidden node activation function and on the number of\nhidden nodes. When the inferred RBM parameters are weak, an intuitive pattern\nis found for the expression of the interaction terms which reduces\nsubstantially the differences across activation functions. We show that the\nweak parameter approximation is a good approximation for different RBMs trained\non the MNIST dataset. Interestingly, in these cases, the mapping reveals that\nthe inferred models are essentially low order interaction models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:52:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bulso", "Nicola", ""], ["Roudi", "Yasser", ""]]}, {"id": "2103.15918", "submitter": "Panagiota Kiourti", "authors": "Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit\n  Jha", "title": "Online Defense of Trojaned Models using Misattributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to detecting neural Trojans on Deep Neural\nNetworks during inference. This approach is based on monitoring the inference\nof a machine learning model, computing the attribution of the model's decision\non different features of the input, and then statistically analyzing these\nattributions to detect whether an input sample contains the Trojan trigger. The\nanomalous attributions, aka misattributions, are then accompanied by\nreverse-engineering of the trigger to evaluate whether the input sample is\ntruly poisoned with a Trojan trigger. We evaluate our approach on several\nbenchmarks, including models trained on MNIST, Fashion MNIST, and German\nTraffic Sign Recognition Benchmark, and demonstrate the state of the art\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:53:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kiourti", "Panagiota", ""], ["Li", "Wenchao", ""], ["Roy", "Anirban", ""], ["Sikka", "Karan", ""], ["Jha", "Susmit", ""]]}, {"id": "2103.15919", "submitter": "Max Goplerud", "authors": "Max Goplerud", "title": "Modelling Heterogeneity Using Bayesian Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to estimate heterogeneity, e.g. the effect of some variable differing\nacross observations, is a key question in political science. Methods for doing\nso make simplifying assumptions about the underlying nature of the\nheterogeneity to draw reliable inferences. This paper allows a common way of\nsimplifying complex phenomenon (placing observations with similar effects into\ndiscrete groups) to be integrated into regression analysis. The framework\nallows researchers to (i) use their prior knowledge to guide which groups are\npermissible and (ii) appropriately quantify uncertainty. The paper does this by\nextending work on \"structured sparsity\" from a traditional penalized likelihood\napproach to a Bayesian one by deriving new theoretical results and inferential\ntechniques. It shows that this method outperforms state-of-the-art methods for\nestimating heterogeneous effects when the underlying heterogeneity is grouped\nand more effectively identifies groups of observations with different effects\nin observational data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:54:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Goplerud", "Max", ""]]}, {"id": "2103.15933", "submitter": "Harvineet Singh", "authors": "Harvineet Singh, Shalmali Joshi, Finale Doshi-Velez, Himabindu\n  Lakkaraju", "title": "Learning Under Adversarial and Interventional Shifts", "comments": "19 pages including 5 pages appendix, 6 figures, 2 tables. Preliminary\n  version presented at Causal Discovery & Causality-Inspired Machine Learning\n  Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are often trained on data from one distribution and\ndeployed on others. So it becomes important to design models that are robust to\ndistribution shifts. Most of the existing work focuses on optimizing for either\nadversarial shifts or interventional shifts. Adversarial methods lack\nexpressivity in representing plausible shifts as they consider shifts to joint\ndistributions in the data. Interventional methods allow more expressivity but\nprovide robustness to unbounded shifts, resulting in overly conservative\nmodels. In this work, we combine the complementary strengths of the two\napproaches and propose a new formulation, RISe, for designing robust models\nagainst a set of distribution shifts that are at the intersection of\nadversarial and interventional shifts. We employ the distributionally robust\noptimization framework to optimize the resulting objective in both supervised\nand reinforcement learning settings. Extensive experimentation with synthetic\nand real world datasets from healthcare demonstrate the efficacy of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:10:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Singh", "Harvineet", ""], ["Joshi", "Shalmali", ""], ["Doshi-Velez", "Finale", ""], ["Lakkaraju", "Himabindu", ""]]}, {"id": "2103.15966", "submitter": "Linfeng Liu", "authors": "Linfeng Liu, Michael C. Hughes, Li-Ping Liu", "title": "Modeling Graph Node Correlations with Neighbor Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new model, the Neighbor Mixture Model (NMM), for modeling node\nlabels in a graph. This model aims to capture correlations between the labels\nof nodes in a local neighborhood. We carefully design the model so it could be\nan alternative to a Markov Random Field but with more affordable computations.\nIn particular, drawing samples and evaluating marginal probabilities of single\nlabels can be done in linear time. To scale computations to large graphs, we\ndevise a variational approximation without introducing extra parameters. We\nfurther use graph neural networks (GNNs) to parameterize the NMM, which reduces\nthe number of learnable parameters while allowing expressive representation\nlearning. The proposed model can be either fit directly to large observed\ngraphs or used to enable scalable inference that preserves correlations for\nother distributions such as deep generative graph models. Across a diverse set\nof node classification, image denoising, and link prediction tasks, we show our\nproposed NMM advances the state-of-the-art in modeling real-world labeled\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:41:56 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 13:07:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Linfeng", ""], ["Hughes", "Michael C.", ""], ["Liu", "Li-Ping", ""]]}, {"id": "2103.15996", "submitter": "Theodor Misiakiewicz Mr.", "authors": "Michael Celentano, Theodor Misiakiewicz, Andrea Montanari", "title": "Minimum complexity interpolation in random features models", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their many appealing properties, kernel methods are heavily affected\nby the curse of dimensionality. For instance, in the case of inner product\nkernels in $\\mathbb{R}^d$, the Reproducing Kernel Hilbert Space (RKHS) norm is\noften very large for functions that depend strongly on a small subset of\ndirections (ridge functions). Correspondingly, such functions are difficult to\nlearn using kernel methods.\n  This observation has motivated the study of generalizations of kernel\nmethods, whereby the RKHS norm -- which is equivalent to a weighted $\\ell_2$\nnorm -- is replaced by a weighted functional $\\ell_p$ norm, which we refer to\nas $\\mathcal{F}_p$ norm. Unfortunately, tractability of these approaches is\nunclear. The kernel trick is not available and minimizing these norms requires\nto solve an infinite-dimensional convex problem.\n  We study random features approximations to these norms and show that, for\n$p>1$, the number of random features required to approximate the original\nlearning problem is upper bounded by a polynomial in the sample size. Hence,\nlearning with $\\mathcal{F}_p$ norms is tractable in these cases. We introduce a\nproof technique based on uniform concentration in the dual, which can be of\nbroader interest in the study of overparametrized models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:00:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Celentano", "Michael", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2103.16082", "submitter": "Puning Zhao", "authors": "Puning Zhao and Lifeng Lai", "title": "Optimal Stochastic Nonconvex Optimization with Bandit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the continuous armed bandit problems for nonconvex\ncost functions under certain smoothness and sublevel set assumptions. We first\nderive an upper bound on the expected cumulative regret of a simple bin\nsplitting method. We then propose an adaptive bin splitting method, which can\nsignificantly improve the performance. Furthermore, a minimax lower bound is\nderived, which shows that our new adaptive method achieves locally minimax\noptimal expected cumulative regret.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:21:12 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhao", "Puning", ""], ["Lai", "Lifeng", ""]]}, {"id": "2103.16091", "submitter": "Gautam Mittal", "authors": "Gautam Mittal, Jesse Engel, Curtis Hawthorne, Ian Simon", "title": "Symbolic Music Generation with Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based generative models and diffusion probabilistic models have been\nsuccessful at generating high-quality samples in continuous domains such as\nimages and audio. However, due to their Langevin-inspired sampling mechanisms,\ntheir application to discrete and sequential data has been limited. In this\nwork, we present a technique for training diffusion models on sequential data\nby parameterizing the discrete domain in the continuous latent space of a\npre-trained variational autoencoder. Our method is non-autoregressive and\nlearns to generate sequences of latent embeddings through the reverse process\nand offers parallel generation with a constant number of iterative refinement\nsteps. We apply this technique to modeling symbolic music and show strong\nunconditional generation and post-hoc conditional infilling results compared to\nautoregressive language models operating over the same continuous embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:48:05 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mittal", "Gautam", ""], ["Engel", "Jesse", ""], ["Hawthorne", "Curtis", ""], ["Simon", "Ian", ""]]}, {"id": "2103.16141", "submitter": "Kazuo Aoyama", "authors": "Kazuo Aoyama and Kazumi Saito", "title": "Structured Inverted-File k-Means Clustering for High-Dimensional Sparse\n  Data", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture-friendly k-means clustering algorithm\ncalled SIVF for a large-scale and high-dimensional sparse data set. Algorithm\nefficiency on time is often measured by the number of costly operations such as\nsimilarity calculations. In practice, however, it depends greatly on how the\nalgorithm adapts to an architecture of the computer system which it is executed\non. Our proposed SIVF employs invariant centroid-pair based filter (ICP) to\ndecrease the number of similarity calculations between a data object and\ncentroids of all the clusters. To maximize the ICP performance, SIVF exploits\nfor a centroid set an inverted-file that is structured so as to reduce pipeline\nhazards. We demonstrate in our experiments on real large-scale document data\nsets that SIVF operates at higher speed and with lower memory consumption than\nexisting algorithms. Our performance analysis reveals that SIVF achieves the\nhigher speed by suppressing performance degradation factors of the number of\ncache misses and branch mispredictions rather than less similarity\ncalculations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:54:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Aoyama", "Kazuo", ""], ["Saito", "Kazumi", ""]]}, {"id": "2103.16210", "submitter": "Harshil Shah", "authors": "Harshil Shah, Tim Xiao, David Barber", "title": "Locally-Contextual Nonlinear CRFs for Sequence Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear chain conditional random fields (CRFs) combined with contextual word\nembeddings have achieved state of the art performance on sequence labeling\ntasks. In many of these tasks, the identity of the neighboring words is often\nthe most useful contextual information when predicting the label of a given\nword. However, contextual embeddings are usually trained in a task-agnostic\nmanner. This means that although they may encode information about the\nneighboring words, it is not guaranteed. It can therefore be beneficial to\ndesign the sequence labeling architecture to directly extract this information\nfrom the embeddings. We propose locally-contextual nonlinear CRFs for sequence\nlabeling. Our approach directly incorporates information from the neighboring\nembeddings when predicting the label for a given word, and parametrizes the\npotential functions using deep neural networks. Our model serves as a drop-in\nreplacement for the linear chain CRF, consistently outperforming it in our\nablation study. On a variety of tasks, our results are competitive with those\nof the best published methods. In particular, we outperform the previous state\nof the art on chunking on CoNLL 2000 and named entity recognition on OntoNotes\n5.0 English.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:43:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shah", "Harshil", ""], ["Xiao", "Tim", ""], ["Barber", "David", ""]]}, {"id": "2103.16336", "submitter": "Ranjan Maitra", "authors": "Emily M. Goren and Ranjan Maitra", "title": "Model-based clustering of partial records", "comments": "18 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.HE cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:30:59 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:18:14 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 12:54:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Goren", "Emily M.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2103.16355", "submitter": "Yuqing Li", "authors": "Yuqing Li, Tao Luo, Chao Ma", "title": "Nonlinear Weighted Directed Acyclic Graph and A Priori Estimates for\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an attempt to better understand structural benefits and generalization\npower of deep neural networks, we firstly present a novel graph theoretical\nformulation of neural network models, including fully connected, residual\nnetwork~(ResNet) and densely connected networks~(DenseNet). Secondly, we extend\nthe error analysis of the population risk for two layer\nnetwork~\\cite{ew2019prioriTwo} and ResNet~\\cite{e2019prioriRes} to DenseNet,\nand show further that for neural networks satisfying certain mild conditions,\nsimilar estimates can be obtained. These estimates are a priori in nature since\nthey depend sorely on the information prior to the training process, in\nparticular, the bounds for the estimation errors are independent of the input\ndimension.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:54:33 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Yuqing", ""], ["Luo", "Tao", ""], ["Ma", "Chao", ""]]}, {"id": "2103.16451", "submitter": "Viet Anh Nguyen", "authors": "Viet Anh Nguyen, Fan Zhang, Jose Blanchet, Erick Delage, Yinyu Ye", "title": "Robustifying Conditional Portfolio Decisions via Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a data-driven portfolio selection model that integrates side\ninformation, conditional estimation and robustness using the framework of\ndistributionally robust optimization. Conditioning on the observed side\ninformation, the portfolio manager solves an allocation problem that minimizes\nthe worst-case conditional risk-return trade-off, subject to all possible\nperturbations of the covariate-return probability distribution in an optimal\ntransport ambiguity set. Despite the non-linearity of the objective function in\nthe probability measure, we show that the distributionally robust portfolio\nallocation with side information problem can be reformulated as a\nfinite-dimensional optimization problem. If portfolio decisions are made based\non either the mean-variance or the mean-Conditional Value-at-Risk criterion,\nthe resulting reformulation can be further simplified to second-order or\nsemi-definite cone programs. Empirical studies in the US and Chinese equity\nmarkets demonstrate the advantage of our integrative framework against other\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:56:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Zhang", "Fan", ""], ["Blanchet", "Jose", ""], ["Delage", "Erick", ""], ["Ye", "Yinyu", ""]]}, {"id": "2103.16547", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Jingjing Liu,\n  Zhangyang Wang", "title": "The Elastic Lottery Ticket Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse\ntrainable subnetworks, or winning tickets, of training, which can be trained in\nisolation to achieve similar or even better performance compared to the full\nmodels. Despite many efforts being made, the most effective method to identify\nsuch winning tickets is still Iterative Magnitude-based Pruning (IMP), which is\ncomputationally expensive and has to be run thoroughly for every different\nnetwork. A natural question that comes in is: can we \"transform\" the winning\nticket found in one network to another with a different architecture, yielding\na winning ticket for the latter at the beginning, without re-doing the\nexpensive IMP? Answering this question is not only practically relevant for\nefficient \"once-for-all\" winning ticket finding, but also theoretically\nappealing for uncovering inherently scalable sparse patterns in networks. We\nconduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety\nof strategies to tweak the winning tickets found from different networks of the\nsame model family (e.g., ResNets). Based on these results, we articulate the\nElastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or\ndropping) and re-ordering layers for one network, its corresponding winning\nticket could be stretched (or squeezed) into a subnetwork for another deeper\n(or shallower) network from the same family, whose performance is nearly the\nsame competitive as the latter's winning ticket directly found by IMP. We have\nalso thoroughly compared E-LTH with pruning-at-initialization and dynamic\nsparse training methods, and discuss the generalizability of E-LTH to different\nmodel families, layer types, or across datasets. Code is available at\nhttps://github.com/VITA-Group/ElasticLTH.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:53:45 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:04:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Xiaohan", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2103.16596", "submitter": "George Tucker", "authors": "Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang,\n  Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral\n  Kumar, Cosmin Paduraru, Sergey Levine, Tom Le Paine", "title": "Benchmarks for Deep Off-Policy Evaluation", "comments": "ICLR 2021 paper. Policies and evaluation code are available at\n  https://github.com/google-research/deep_ope", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Off-policy evaluation (OPE) holds the promise of being able to leverage\nlarge, offline datasets for both evaluating and selecting complex policies for\ndecision making. The ability to learn offline is particularly important in many\nreal-world domains, such as in healthcare, recommender systems, or robotics,\nwhere online data collection is an expensive and potentially dangerous process.\nBeing able to accurately evaluate and select high-performing policies without\nrequiring online interaction could yield significant benefits in safety, time,\nand cost for these applications. While many OPE methods have been proposed in\nrecent years, comparing results between papers is difficult because currently\nthere is a lack of a comprehensive and unified benchmark, and measuring\nalgorithmic progress has been challenging due to the lack of difficult\nevaluation tasks. In order to address this gap, we present a collection of\npolicies that in conjunction with existing offline datasets can be used for\nbenchmarking off-policy evaluation. Our tasks include a range of challenging\nhigh-dimensional continuous control problems, with wide selections of datasets\nand policies for performing policy selection. The goal of our benchmark is to\nprovide a standardized measure of progress that is motivated from a set of\nprinciples designed to challenge and test the limits of existing OPE methods.\nWe perform an evaluation of state-of-the-art algorithms and provide open-source\naccess to our data and code to foster future research in this area.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:09:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Fu", "Justin", ""], ["Norouzi", "Mohammad", ""], ["Nachum", "Ofir", ""], ["Tucker", "George", ""], ["Wang", "Ziyu", ""], ["Novikov", "Alexander", ""], ["Yang", "Mengjiao", ""], ["Zhang", "Michael R.", ""], ["Chen", "Yutian", ""], ["Kumar", "Aviral", ""], ["Paduraru", "Cosmin", ""], ["Levine", "Sergey", ""], ["Paine", "Tom Le", ""]]}, {"id": "2103.16629", "submitter": "Abed AlRahman Al Makdah", "authors": "Abed AlRahman Al Makdah and Vishaal Krishnan and Fabio Pasqualetti", "title": "Learning Robust Feedback Policies from Demonstrations", "comments": "Submitted to CDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose and analyze a new framework to learn feedback control\npolicies that exhibit provable guarantees on the closed-loop performance and\nrobustness to bounded (adversarial) perturbations. These policies are learned\nfrom expert demonstrations without any prior knowledge of the task, its cost\nfunction, and system dynamics. In contrast to the existing algorithms in\nimitation learning and inverse reinforcement learning, we use a\nLipschitz-constrained loss minimization scheme to learn control policies with\ncertified robustness. We establish robust stability of the closed-loop system\nunder the learned control policy and derive an upper bound on its regret, which\nbounds the sub-optimality of the closed-loop performance with respect to the\nexpert policy. We also derive a robustness bound for the deterioration of the\nclosed-loop performance under bounded (adversarial) perturbations on the state\nmeasurements. Ultimately, our results suggest the existence of an underlying\ntradeoff between nominal closed-loop performance and adversarial robustness,\nand that improvements in nominal closed-loop performance can only be made at\nthe expense of robustness to adversarial perturbations. Numerical results\nvalidate our analysis and demonstrate the effectiveness of our robust feedback\npolicy learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:11:05 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Makdah", "Abed AlRahman Al", ""], ["Krishnan", "Vishaal", ""], ["Pasqualetti", "Fabio", ""]]}, {"id": "2103.16649", "submitter": "Rodolphe Le Riche", "authors": "Rodolphe Le Riche, Victor Picheny", "title": "Revisiting Bayesian Optimization in the light of the COCO benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is commonly believed that Bayesian optimization (BO) algorithms are highly\nefficient for optimizing numerically costly functions. However, BO is not often\ncompared to widely different alternatives, and is mostly tested on narrow sets\nof problems (multimodal, low-dimensional functions), which makes it difficult\nto assess where (or if) they actually achieve state-of-the-art performance.\nMoreover, several aspects in the design of these algorithms vary across\nimplementations without a clear recommendation emerging from current practices,\nand many of these design choices are not substantiated by authoritative test\ncampaigns. This article reports a large investigation about the effects on the\nperformance of (Gaussian process based) BO of common and less common design\nchoices. The experiments are carried out with the established COCO (COmparing\nContinuous Optimizers) software. It is found that a small initial budget, a\nquadratic trend, high-quality optimization of the acquisition criterion bring\nconsistent progress. Using the GP mean as an occasional acquisition contributes\nto a negligible additional improvement. Warping degrades performance. The\nMat\\'ern 5/2 kernel is a good default but it may be surpassed by the\nexponential kernel on irregular functions. Overall, the best EGO variants are\ncompetitive or improve over state-of-the-art algorithms in dimensions less or\nequal to 5 for multimodal functions. The code developed for this study makes\nthe new version (v2.1.1) of the R package DiceOptim available on CRAN. The\nstructure of the experiments by function groups allows to define priorities for\nfuture research on Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:45:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 14:17:15 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 08:00:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Riche", "Rodolphe Le", ""], ["Picheny", "Victor", ""]]}, {"id": "2103.16685", "submitter": "Carmen Jim\\'enez-Mesa", "authors": "Carmen Jim\\'enez-Mesa, Javier Ram\\'irez, John Suckling, Jonathan\n  V\\\"oglein, Johannes Levin, Juan Manuel G\\'orriz, Alzheimer's Disease\n  Neuroimaging Initiative ADNI, Dominantly Inherited Alzheimer Network DIAN", "title": "Deep Learning in current Neuroimaging: a multivariate approach with\n  power and type I error control but arguable generalization ability", "comments": "26 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminative analysis in neuroimaging by means of deep/machine learning\ntechniques is usually tested with validation techniques, whereas the associated\nstatistical significance remains largely under-developed due to their\ncomputational complexity. In this work, a non-parametric framework is proposed\nthat estimates the statistical significance of classifications using deep\nlearning architectures. In particular, a combination of autoencoders (AE) and\nsupport vector machines (SVM) is applied to: (i) a one-condition, within-group\ndesigns often of normal controls (NC) and; (ii) a two-condition, between-group\ndesigns which contrast, for example, Alzheimer's disease (AD) patients with NC\n(the extension to multi-class analyses is also included). A random-effects\ninference based on a label permutation test is proposed in both studies using\ncross-validation (CV) and resubstitution with upper bound correction (RUB) as\nvalidation methods. This allows both false positives and classifier overfitting\nto be detected as well as estimating the statistical power of the test. Several\nexperiments were carried out using the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, the Dominantly Inherited Alzheimer Network (DIAN)\ndataset, and a MCI prediction dataset. We found in the permutation test that CV\nand RUB methods offer a false positive rate close to the significance level and\nan acceptable statistical power (although lower using cross-validation). A\nlarge separation between training and test accuracies using CV was observed,\nespecially in one-condition designs. This implies a low generalization ability\nas the model fitted in training is not informative with respect to the test\nset. We propose as solution by applying RUB, whereby similar results are\nobtained to those of the CV test set, but considering the whole set and with a\nlower computational cost per iteration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:15:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Jim\u00e9nez-Mesa", "Carmen", ""], ["Ram\u00edrez", "Javier", ""], ["Suckling", "John", ""], ["V\u00f6glein", "Jonathan", ""], ["Levin", "Johannes", ""], ["G\u00f3rriz", "Juan Manuel", ""], ["ADNI", "Alzheimer's Disease Neuroimaging Initiative", ""], ["DIAN", "Dominantly Inherited Alzheimer Network", ""]]}, {"id": "2103.16700", "submitter": "Siyu Zhou", "authors": "Siyu Zhou and Lucas Mentch", "title": "Trees, Forests, Chickens, and Eggs: When and Why to Prune Trees in a\n  Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to their long-standing reputation as excellent off-the-shelf predictors,\nrandom forests continue remain a go-to model of choice for applied\nstatisticians and data scientists. Despite their widespread use, however, until\nrecently, little was known about their inner-workings and about which aspects\nof the procedure were driving their success. Very recently, two competing\nhypotheses have emerged -- one based on interpolation and the other based on\nregularization. This work argues in favor of the latter by utilizing the\nregularization framework to reexamine the decades-old question of whether\nindividual trees in an ensemble ought to be pruned. Despite the fact that\ndefault constructions of random forests use near full depth trees in most\npopular software packages, here we provide strong evidence that tree depth\nshould be seen as a natural form of regularization across the entire procedure.\nIn particular, our work suggests that random forests with shallow trees are\nadvantageous when the signal-to-noise ratio in the data is low. In building up\nthis argument, we also critique the newly popular notion of \"double descent\" in\nrandom forests by drawing parallels to U-statistics and arguing that the\nnoticeable jumps in random forest accuracy are the result of simple averaging\nrather than interpolation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:57:55 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhou", "Siyu", ""], ["Mentch", "Lucas", ""]]}, {"id": "2103.16714", "submitter": "Subha Maity", "authors": "Subha Maity, Songkai Xue, Mikhail Yurochkin, Yuekai Sun", "title": "Statistical inference for individual fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we rely on machine learning (ML) models to make more consequential\ndecisions, the issue of ML models perpetuating or even exacerbating undesirable\nhistorical biases (e.g., gender and racial biases) has come to the fore of the\npublic's attention. In this paper, we focus on the problem of detecting\nviolations of individual fairness in ML models. We formalize the problem as\nmeasuring the susceptibility of ML models against a form of adversarial attack\nand develop a suite of inference tools for the adversarial cost function. The\ntools allow auditors to assess the individual fairness of ML models in a\nstatistically-principled way: form confidence intervals for the worst-case\nperformance differential between similar individuals and test hypotheses of\nmodel fairness with (asymptotic) non-coverage/Type I error rate control. We\ndemonstrate the utility of our tools in a real-world case study.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:49:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Maity", "Subha", ""], ["Xue", "Songkai", ""], ["Yurochkin", "Mikhail", ""], ["Sun", "Yuekai", ""]]}, {"id": "2103.16785", "submitter": "Fan Zhang", "authors": "Alexander Vargo, Fan Zhang, Mikhail Yurochkin, Yuekai Sun", "title": "Individually Fair Gradient Boosting", "comments": "ICLR Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:06:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Vargo", "Alexander", ""], ["Zhang", "Fan", ""], ["Yurochkin", "Mikhail", ""], ["Sun", "Yuekai", ""]]}, {"id": "2103.16910", "submitter": "Philip Winter", "authors": "Philip Matthias Winter, Sebastian Eder, Johannes Weissenb\\\"ock,\n  Christoph Schwald, Thomas Doms, Tom Vogt, Sepp Hochreiter, Bernhard Nessler", "title": "Trusted Artificial Intelligence: Towards Certification of Machine\n  Learning Applications", "comments": "48 pages, 11 figures, soft-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence is one of the fastest growing technologies of the\n21st century and accompanies us in our daily lives when interacting with\ntechnical applications. However, reliance on such technical systems is crucial\nfor their widespread applicability and acceptance. The societal tools to\nexpress reliance are usually formalized by lawful regulations, i.e., standards,\nnorms, accreditations, and certificates. Therefore, the T\\\"UV AUSTRIA Group in\ncooperation with the Institute for Machine Learning at the Johannes Kepler\nUniversity Linz, proposes a certification process and an audit catalog for\nMachine Learning applications. We are convinced that our approach can serve as\nthe foundation for the certification of applications that use Machine Learning\nand Deep Learning, the techniques that drive the current revolution in\nArtificial Intelligence. While certain high-risk areas, such as fully\nautonomous robots in workspaces shared with humans, are still some time away\nfrom certification, we aim to cover low-risk applications with our\ncertification procedure. Our holistic approach attempts to analyze Machine\nLearning applications from multiple perspectives to evaluate and verify the\naspects of secure software development, functional requirements, data quality,\ndata protection, and ethics. Inspired by existing work, we introduce four\ncriticality levels to map the criticality of a Machine Learning application\nregarding the impact of its decisions on people, environment, and\norganizations. Currently, the audit catalog can be applied to low-risk\napplications within the scope of supervised learning as commonly encountered in\nindustry. Guided by field experience, scientific developments, and market\ndemands, the audit catalog will be extended and modified accordingly.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:59:55 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Winter", "Philip Matthias", ""], ["Eder", "Sebastian", ""], ["Weissenb\u00f6ck", "Johannes", ""], ["Schwald", "Christoph", ""], ["Doms", "Thomas", ""], ["Vogt", "Tom", ""], ["Hochreiter", "Sepp", ""], ["Nessler", "Bernhard", ""]]}, {"id": "2103.16977", "submitter": "Ed Hill", "authors": "Edward Hill, Marco Bardoscia and Arthur Turrell", "title": "Solving Heterogeneous General Equilibrium Economic Models with Deep\n  Reinforcement Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG cs.MA q-fin.EC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General equilibrium macroeconomic models are a core tool used by policymakers\nto understand a nation's economy. They represent the economy as a collection of\nforward-looking actors whose behaviours combine, possibly with stochastic\neffects, to determine global variables (such as prices) in a dynamic\nequilibrium. However, standard semi-analytical techniques for solving these\nmodels make it difficult to include the important effects of heterogeneous\neconomic actors. The COVID-19 pandemic has further highlighted the importance\nof heterogeneity, for example in age and sector of employment, in macroeconomic\noutcomes and the need for models that can more easily incorporate it. We use\ntechniques from reinforcement learning to solve such models incorporating\nheterogeneous agents in a way that is simple, extensible, and computationally\nefficient. We demonstrate the method's accuracy and stability on a toy problem\nfor which there is a known analytical solution, its versatility by solving a\ngeneral equilibrium problem that includes global stochasticity, and its\nflexibility by solving a combined macroeconomic and epidemiological model to\nexplore the economic and health implications of a pandemic. The latter\nsuccessfully captures plausible economic behaviours induced by differential\nhealth risks by age.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 10:55:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hill", "Edward", ""], ["Bardoscia", "Marco", ""], ["Turrell", "Arthur", ""]]}, {"id": "2103.17055", "submitter": "Isabelle Augenstein", "authors": "Sheikh Muhammad Sarwar, Dimitrina Zlatkova, Momchil Hardalov, Yoan\n  Dinkov, Isabelle Augenstein, Preslav Nakov", "title": "A Neighbourhood Framework for Resource-Lean Content Flagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel interpretable framework for cross-lingual content\nflagging, which significantly outperforms prior work both in terms of\npredictive performance and average inference time. The framework is based on a\nnearest-neighbour architecture and is interpretable by design. Moreover, it can\neasily adapt to new instances without the need to retrain it from scratch.\nUnlike prior work, (i) we encode not only the texts, but also the labels in the\nneighbourhood space (which yields better accuracy), and (ii) we use a\nbi-encoder instead of a cross-encoder (which saves computation time). Our\nevaluation results on ten different datasets for abusive language detection in\neight languages shows sizable improvements over the state of the art, as well\nas a speed-up at inference time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:22:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Zlatkova", "Dimitrina", ""], ["Hardalov", "Momchil", ""], ["Dinkov", "Yoan", ""], ["Augenstein", "Isabelle", ""], ["Nakov", "Preslav", ""]]}, {"id": "2103.17060", "submitter": "Masanari Kimura", "authors": "Masanari Kimura and Hideitsu Hino", "title": "$\\alpha$-Geodesical Skew Divergence", "comments": null, "journal-ref": "Entropy. 2021; 23(5):528", "doi": "10.3390/e23050528", "report-no": null, "categories": "cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The asymmetric skew divergence smooths one of the distributions by mixing it,\nto a degree determined by the parameter $\\lambda$, with the other distribution.\nSuch divergence is an approximation of the KL divergence that does not require\nthe target distribution to be absolutely continuous with respect to the source\ndistribution. In this paper, an information geometric generalization of the\nskew divergence called the $\\alpha$-geodesical skew divergence is proposed, and\nits properties are studied.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:27:58 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 05:40:47 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 11:16:15 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 03:08:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kimura", "Masanari", ""], ["Hino", "Hideitsu", ""]]}, {"id": "2103.17096", "submitter": "Axel Oehmichen PhD", "authors": "Axel Oehmichen, Florian Guitton, Cedric Wahl, Bertrand Foing, Damian\n  Tziamtzis, Yike Guo", "title": "MOAI: A methodology for evaluating the impact of indoor airflow in the\n  transmission of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Epidemiology models play a key role in understanding and responding to the\nCOVID-19 pandemic. In order to build those models, scientists need to\nunderstand contributing factors and their relative importance. A large strand\nof literature has identified the importance of airflow to mitigate droplets and\nfar-field aerosol transmission risks. However, the specific factors\ncontributing to higher or lower contamination in various settings have not been\nclearly defined and quantified. As part of the MOAI project\n(https://moaiapp.com), we are developing a privacy-preserving test and trace\napp to enable infection cluster investigators to get in touch with patients\nwithout having to know their identity. This approach allows involving users in\nthe fight against the pandemic by contributing additional information in the\nform of anonymous research questionnaires. We first describe how the\nquestionnaire was designed, and the synthetic data was generated based on a\nreview we carried out on the latest available literature. We then present a\nmodel to evaluate the risk exposition of a user for a given setting. We finally\npropose a temporal addition to the model to evaluate the risk exposure over\ntime for a given user.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:06:09 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Oehmichen", "Axel", ""], ["Guitton", "Florian", ""], ["Wahl", "Cedric", ""], ["Foing", "Bertrand", ""], ["Tziamtzis", "Damian", ""], ["Guo", "Yike", ""]]}, {"id": "2103.17106", "submitter": "Patricia Pauli", "authors": "Patricia Pauli, Dennis Gramlich, Julian Berberich and Frank Allg\\\"ower", "title": "Linear systems with neural network nonlinearities: Improved stability\n  analysis via acausal Zames-Falb multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we analyze the stability of feedback interconnections of a\nlinear time-invariant system with a neural network nonlinearity in discrete\ntime. Our analysis is based on abstracting neural networks using integral\nquadratic constraints (IQCs), exploiting the sector-bounded and\nslope-restricted structure of the underlying activation functions. In contrast\nto existing approaches, we leverage the full potential of dynamic IQCs to\ndescribe the nonlinear activation functions in a less conservative fashion. To\nbe precise, we consider multipliers based on the full-block Yakubovich / circle\ncriterion in combination with acausal Zames-Falb multipliers, leading to linear\nmatrix inequality based stability certificates. Our approach provides a\nflexible and versatile framework for stability analysis of feedback\ninterconnections with neural network nonlinearities, allowing to trade off\ncomputational efficiency and conservatism. Finally, we provide numerical\nexamples that demonstrate the applicability of the proposed framework and the\nachievable improvements over previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:21:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Pauli", "Patricia", ""], ["Gramlich", "Dennis", ""], ["Berberich", "Julian", ""], ["Allg\u00f6wer", "Frank", ""]]}, {"id": "2103.17146", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli and Marco Corneli", "title": "Continuous Latent Position Models for Instantaneous Interactions", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a framework to analyse the timing and frequency of instantaneous\ninteractions between pairs of entities. This type of interaction data is\nespecially common nowadays, and easily available. Examples of instantaneous\ninteractions include email networks, phone call networks and some common types\nof technological and transportation networks. Our framework relies on a novel\nextension of the latent position network model: we assume that the entities are\nembedded in a latent Euclidean space, and that they move along individual\ntrajectories which are continuous over time. These trajectories are used to\ncharacterize the timing and frequency of the pairwise interactions. We discuss\nan inferential framework where we estimate the individual trajectories from the\nobserved interaction data, and propose applications on artificial and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:10:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Corneli", "Marco", ""]]}, {"id": "2103.17171", "submitter": "Joona Pohjonen", "authors": "Joona Pohjonen, Carolin St\\\"urenberg, Antti Rannikko, Tuomas Mirtti,\n  Esa Pitk\\\"anen", "title": "Spectral decoupling allows training transferable neural networks in\n  medical imaging", "comments": "8 pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n  To address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n  We show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n  Our results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:47:01 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:11:58 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 13:36:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pohjonen", "Joona", ""], ["St\u00fcrenberg", "Carolin", ""], ["Rannikko", "Antti", ""], ["Mirtti", "Tuomas", ""], ["Pitk\u00e4nen", "Esa", ""]]}, {"id": "2103.17174", "submitter": "Peter Hinz", "authors": "Peter Hinz", "title": "Using activation histograms to bound the number of affine regions in\n  ReLU feed-forward neural networks", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several current bounds on the maximal number of affine regions of a ReLU\nfeed-forward neural network are special cases of the framework [1] which relies\non layer-wise activation histogram bounds. We analyze and partially solve a\nproblem in algebraic topology the solution of which would fully exploit this\nframework. Our partial solution already induces slightly tighter bounds and\nsuggests insight in how parameter initialization methods can affect the number\nof regions. Furthermore, we extend the framework to allow the composition of\nsubnetwork instead of layer-wise activation histogram bounds to reduce the\nnumber of required compositions which negatively affect the tightness of the\nresulting bound.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:50:44 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 09:19:44 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 09:07:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Hinz", "Peter", ""]]}, {"id": "2103.17203", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "Universal Prediction Band via Semi-Definite Programming", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient method to construct nonparametric,\nheteroskedastic prediction bands for uncertainty quantification, with or\nwithout any user-specified predictive model. The data-adaptive prediction band\nis universally applicable with minimal distributional assumptions, with strong\nnon-asymptotic coverage properties, and easy to implement using standard convex\nprograms. Our approach can be viewed as a novel variance interpolation with\nconfidence and further leverages techniques from semi-definite programming and\nsum-of-squares optimization. Theoretical and numerical performances for the\nproposed approach for uncertainty quantification are analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:30:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Liang", "Tengyuan", ""]]}, {"id": "2103.17233", "submitter": "Stefan Klus", "authors": "Stefan Klus, Patrick Gel{\\ss}, Feliks N\\\"uske, Frank No\\'e", "title": "Symmetric and antisymmetric kernels for machine learning problems in\n  quantum physics and chemistry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math-ph math.MP physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive symmetric and antisymmetric kernels by symmetrizing and\nantisymmetrizing conventional kernels and analyze their properties. In\nparticular, we compute the feature space dimensions of the resulting polynomial\nkernels, prove that the reproducing kernel Hilbert spaces induced by symmetric\nand antisymmetric Gaussian kernels are dense in the space of symmetric and\nantisymmetric functions, and propose a Slater determinant representation of the\nantisymmetric Gaussian kernel, which allows for an efficient evaluation even if\nthe state space is high-dimensional. Furthermore, we show that by exploiting\nsymmetries or antisymmetries the size of the training data set can be\nsignificantly reduced. The results are illustrated with guiding examples and\nsimple quantum physics and chemistry applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:32:27 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 16:56:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Klus", "Stefan", ""], ["Gel\u00df", "Patrick", ""], ["N\u00fcske", "Feliks", ""], ["No\u00e9", "Frank", ""]]}, {"id": "2103.17236", "submitter": "Zichang He", "authors": "Zichang He, Zheng Zhang", "title": "High-Dimensional Uncertainty Quantification via Tensor Regression with\n  Rank Determination and Adaptive Sampling", "comments": "12 pages, accepted by IEEE Trans. Components, Packaging and\n  Manufacturing Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fabrication process variations can significantly influence the performance\nand yield of nano-scale electronic and photonic circuits. Stochastic spectral\nmethods have achieved great success in quantifying the impact of process\nvariations, but they suffer from the curse of dimensionality. Recently,\nlow-rank tensor methods have been developed to mitigate this issue, but two\nfundamental challenges remain open: how to automatically determine the tensor\nrank and how to adaptively pick the informative simulation samples. This paper\nproposes a novel tensor regression method to address these two challenges. We\nuse a $\\ell_{q}/ \\ell_{2}$ group-sparsity regularization to determine the\ntensor rank. The resulting optimization problem can be efficiently solved via\nan alternating minimization solver. We also propose a two-stage adaptive\nsampling method to reduce the simulation cost. Our method considers both\nexploration and exploitation via the estimated Voronoi cell volume and\nnonlinearity measurement respectively. The proposed model is verified with\nsynthetic and some realistic circuit benchmarks, on which our method can well\ncapture the uncertainty caused by 19 to 100 random variables with only 100 to\n600 simulation samples.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:36:07 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 07:20:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["He", "Zichang", ""], ["Zhang", "Zheng", ""]]}, {"id": "2103.17258", "submitter": "Hiroki Furuta", "authors": "Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo,\n  Shixiang Shane Gu", "title": "Co-Adaptation of Algorithmic and Implementational Innovations in\n  Inference-based Deep Reinforcement Learning", "comments": "The implementation is available at:\n  https://github.com/frt03/inference-based-rl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently many algorithms were devised for reinforcement learning (RL) with\nfunction approximation. While they have clear algorithmic distinctions, they\nalso have many implementation differences that are algorithm-independent and\nsometimes under-emphasized. Such mixing of algorithmic novelty and\nimplementation craftsmanship makes rigorous analyses of the sources of\nperformance improvements across algorithms difficult. In this work, we focus on\na series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and\nSAC -- to decouple their algorithmic innovations and implementation decisions.\nWe present unified derivations through a single control-as-inference objective,\nwhere we can categorize each algorithm as based on either\nExpectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence\nminimization and treat the rest of specifications as implementation details. We\nperformed extensive ablation studies, and identified substantial performance\ndrops whenever implementation details are mismatched for algorithmic choices.\nThese results show which implementation details are co-adapted and co-evolved\nwith algorithms, and which are transferable across algorithms: as examples, we\nidentified that tanh Gaussian policy and network sizes are highly adapted to\nalgorithmic types, while layer normalization and ELU are critical for MPO's\nperformances but also transfer to noticeable gains in SAC. We hope our work can\ninspire future work to further demystify sources of performance improvements\nacross multiple algorithms and allow researchers to build on one another's both\nalgorithmic and implementational innovations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:55:20 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:27:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Furuta", "Hiroki", ""], ["Kozuno", "Tadashi", ""], ["Matsushima", "Tatsuya", ""], ["Matsuo", "Yutaka", ""], ["Gu", "Shixiang Shane", ""]]}, {"id": "2103.17268", "submitter": "Zhouxing Shi", "authors": "Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh", "title": "Fast Certified Robust Training with Short Warmup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, bound propagation based certified robust training methods have been\nproposed for training neural networks with certifiable robustness guarantees.\nDespite that state-of-the-art (SOTA) methods including interval bound\npropagation (IBP) and CROWN-IBP have per-batch training complexity similar to\nstandard neural network training, they usually use a long warmup schedule with\nhundreds or thousands epochs to reach SOTA performance and are thus still\ncostly. In this paper, we identify two important issues in existing methods,\nnamely exploded bounds at initialization, and the imbalance in ReLU activation\nstates. These two issues make certified training difficult and unstable, and\nthereby long warmup schedules were needed in prior works. To mitigate these\nissues and conduct certified training with shorter warmup, we propose three\nimprovements: 1) We derive a new weight initialization method for IBP training;\n2) We propose to fully add Batch Normalization (BN) to each layer in the model,\nsince we find BN can reduce the imbalance in ReLU activation states; 3) We also\ndesign regularization to explicitly tighten certified bounds and balance ReLU\nactivation states. In our experiments, we are able to obtain 65.03% verified\nerror on CIFAR-10 ($\\epsilon=\\frac{8}{255}$) and 82.36% verified error on\nTinyImageNet ($\\epsilon=\\frac{1}{255}$) using very short training schedules\n(160 and 80 total epochs, respectively), outperforming literature SOTA trained\nwith hundreds or thousands epochs under the same network architecture.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:58 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:35:36 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 16:07:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shi", "Zhouxing", ""], ["Wang", "Yihan", ""], ["Zhang", "Huan", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""]]}]