[{"id": "1606.00068", "submitter": "Marco Cusumano-Towner", "authors": "Marco F Cusumano-Towner, Vikash K Mansinghka", "title": "Quantifying the probable approximation error of probabilistic inference\n  programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new technique for quantifying the approximation error\nof a broad class of probabilistic inference programs, including ones based on\nboth variational and Monte Carlo approaches. The key idea is to derive a\nsubjective bound on the symmetrized KL divergence between the distribution\nachieved by an approximate inference program and its true target distribution.\nThe bound's validity (and subjectivity) rests on the accuracy of two auxiliary\nprobabilistic programs: (i) a \"reference\" inference program that defines a gold\nstandard of accuracy and (ii) a \"meta-inference\" program that answers the\nquestion \"what internal random choices did the original approximate inference\nprogram probably make given that it produced a particular result?\" The paper\nincludes empirical results on inference problems drawn from linear regression,\nDirichlet process mixture modeling, HMMs, and Bayesian networks. The\nexperiments show that the technique is robust to the quality of the reference\ninference program and that it can detect implementation bugs that are not\napparent from predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 22:37:43 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Cusumano-Towner", "Marco F", ""], ["Mansinghka", "Vikash K", ""]]}, {"id": "1606.00113", "submitter": "Md Ashad Alam PhD", "authors": "Md Ashad Alam and Yu-Ping Wang", "title": "Identifying Outliers using Influence Function of Multiple Kernel\n  Canonical Correlation Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.05563", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging genetic research has essentially focused on discovering unique and\nco-association effects, but typically ignoring to identify outliers or atypical\nobjects in genetic as well as non-genetics variables. Identifying significant\noutliers is an essential and challenging issue for imaging genetics and\nmultiple sources data analysis. Therefore, we need to examine for transcription\nerrors of identified outliers. First, we address the influence function (IF) of\nkernel mean element, kernel covariance operator, kernel cross-covariance\noperator, kernel canonical correlation analysis (kernel CCA) and multiple\nkernel CCA. Second, we propose an IF of multiple kernel CCA, which can be\napplied for more than two datasets. Third, we propose a visualization method to\ndetect influential observations of multiple sources of data based on the IF of\nkernel CCA and multiple kernel CCA. Finally, the proposed methods are capable\nof analyzing outliers of subjects usually found in biomedical applications, in\nwhich the number of dimension is large. To examine the outliers, we use the\nstem-and-leaf display. Experiments on both synthesized and imaging genetics\ndata (e.g., SNP, fMRI, and DNA methylation) demonstrate that the proposed\nvisualization can be applied effectively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 04:45:21 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Alam", "Md Ashad", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1606.00118", "submitter": "Md Ashad Alam PhD", "authors": "Md ashad Alam and Osamu Komori and Yu-Ping Wang", "title": "Gene-Gene association for Imaging Genetics Data using Robust Kernel\n  Canonical Correlation Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.05563", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide interaction studies, to detect gene-gene interactions, most\nmethods are divided into two folds: single nucleotide polymorphisms (SNP) based\nand gene-based methods. Basically, the methods based on the gene are more\neffective than the methods based on a single SNP. Recent years, while the\nkernel canonical correlation analysis (Classical kernel CCA) based U statistic\n(KCCU) has proposed to detect the nonlinear relationship between genes. To\nestimate the variance in KCCU, they have used resampling based methods which\nare highly computationally intensive. In addition, classical kernel CCA is not\nrobust to contaminated data. We, therefore, first discuss robust kernel mean\nelement, the robust kernel covariance, and cross-covariance operators. Second,\nwe propose a method based on influence function to estimate the variance of the\nKCCU. Third, we propose a nonparametric robust KCCU method based on robust\nkernel CCA, which is designed for contaminated data and less sensitive to noise\nthan classical kernel CCA. Finally, we investigate the proposed methods to\nsynthesized data and imaging genetic data set. Based on gene ontology and\npathway analysis, the synthesized and genetics analysis demonstrate that the\nproposed robust method shows the superior performance of the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 05:14:03 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Alam", "Md ashad", ""], ["Komori", "Osamu", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1606.00119", "submitter": "Rajat Sen", "authors": "Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G.\n  Dimakis, and Sanjay Shakkottai", "title": "Contextual Bandits with Latent Confounders: An NMF Approach", "comments": "37 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online recommendation and advertising systems, we consider a\ncausal model for stochastic contextual bandits with a latent low-dimensional\nconfounder. In our model, there are $L$ observed contexts and $K$ arms of the\nbandit. The observed context influences the reward obtained through a latent\nconfounder variable with cardinality $m$ ($m \\ll L,K$). The arm choice and the\nlatent confounder causally determines the reward while the observed context is\ncorrelated with the confounder. Under this model, the $L \\times K$ mean reward\nmatrix $\\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$)\nfactorizes into non-negative factors $\\mathbf{A}$ ($L \\times m$) and\n$\\mathbf{W}$ ($m \\times K$). This insight enables us to propose an\n$\\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions\n(selecting specific arms), that achieves a balance between learning this\nlow-dimensional structure and selecting the best arm to minimize regret. Our\nalgorithm achieves a regret of $\\mathcal{O}\\left(L\\mathrm{poly}(m, \\log K) \\log\nT \\right)$ at time $T$, as compared to $\\mathcal{O}(LK\\log T)$ for conventional\ncontextual bandits, assuming a constant gap between the best arm and the rest\nfor each context. These guarantees are obtained under mild sufficiency\nconditions on the factors that are weaker versions of the well-known\nStatistical RIP condition. We further propose a class of generative models that\nsatisfy our sufficient conditions, and derive a lower bound of\n$\\mathcal{O}\\left(Km\\log T\\right)$. These are the first regret guarantees for\nonline matrix completion with bandit feedback, when the rank is greater than\none. We further compare the performance of our algorithm with the state of the\nart, on synthetic and real world data-sets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 05:21:40 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 05:31:47 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 15:59:58 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1606.00136", "submitter": "Ichiro Takeuchi Prof.", "authors": "Hiroyuki Hanada, Atsushi Shibagaki, Jun Sakuma, Ichiro Takeuchi", "title": "Efficiently Bounding Optimal Solutions after Small Data Modification in\n  Large-Scale Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale classification problems in changing environments where a\nsmall part of the dataset is modified, and the effect of the data modification\nmust be quickly incorporated into the classifier. When the entire dataset is\nlarge, even if the amount of the data modification is fairly small, the\ncomputational cost of re-training the classifier would be prohibitively large.\nIn this paper, we propose a novel method for efficiently incorporating such a\ndata modification effect into the classifier without actually re-training it.\nThe proposed method provides bounds on the unknown optimal classifier with the\ncost only proportional to the size of the data modification. We demonstrate\nthrough numerical experiments that the proposed method provides sufficiently\ntight bounds with negligible computational costs, especially when a small part\nof the dataset is modified in a large-scale classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 06:56:17 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Hanada", "Hiroyuki", ""], ["Shibagaki", "Atsushi", ""], ["Sakuma", "Jun", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1606.00142", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Model selection consistency from the perspective of generalization\n  ability and VC theory with an application to Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is difficult to analyse yet theoretically and empirically\nimportant, especially for high-dimensional data analysis. Recently the least\nabsolute shrinkage and selection operator (Lasso) has been applied in the\nstatistical and econometric literature. Consis- tency of Lasso has been\nestablished under various conditions, some of which are difficult to verify in\npractice. In this paper, we study model selection from the perspective of\ngeneralization ability, under the framework of structural risk minimization\n(SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance\nbetween the in-sample and out-of-sample fit, which can be achieved by using\ncross-validation to select a penalty on model complexity. We show that an exact\nrelationship exists between the generalization ability of a model and model\nselection consistency. By implementing SRM and the VC inequality, we show that\nLasso is L2-consistent for model selection under assumptions similar to those\nimposed on OLS. Furthermore, we derive a probabilistic bound for the distance\nbetween the penalized extremum estimator and the extremum estimator without\npenalty, which is dominated by overfitting. We also propose a new measurement\nof overfitting, GR2, based on generalization ability, that converges to zero if\nmodel selection is consistent. Using simulations, we demonstrate that the\nproposed CV-Lasso algorithm performs well in terms of model selection and\noverfitting control.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 07:22:01 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1606.00226", "submitter": "Richard Combes", "authors": "Thomas Bonald and Richard Combes", "title": "A Minimax Optimal Algorithm for Crowdsourcing", "comments": "19 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accurately estimating the reliability of workers\nbased on noisy labels they provide, which is a fundamental question in\ncrowdsourcing. We propose a novel lower bound on the minimax estimation error\nwhich applies to any estimation procedure. We further propose Triangular\nEstimation (TE), an algorithm for estimating the reliability of workers. TE has\nlow complexity, may be implemented in a streaming setting when labels are\nprovided by workers in real time, and does not rely on an iterative procedure.\nWe further prove that TE is minimax optimal and matches our lower bound. We\nconclude by assessing the performance of TE and other state-of-the-art\nalgorithms on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:18:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 16:19:38 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bonald", "Thomas", ""], ["Combes", "Richard", ""]]}, {"id": "1606.00235", "submitter": "Didier Fraix-Burnet", "authors": "Didier Fraix-Burnet (IPAG)", "title": "Clustering with phylogenetic tools in astrophysics", "comments": "Proceedings of the 60th World Statistics Congress of the\n  International Statistical Institute, ISI2015, Jul 2015, Rio de Janeiro,\n  Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic approaches are finding more and more applications outside the\nfield of biology. Astrophysics is no exception since an overwhelming amount of\nmultivariate data has appeared in the last twenty years or so. In particular,\nthe diversification of galaxies throughout the evolution of the Universe quite\nnaturally invokes phylogenetic approaches. We have demonstrated that Maximum\nParsimony brings useful astrophysical results, and we now proceed toward the\nanalyses of large datasets for galaxies. In this talk I present how we solve\nthe major difficulties for this goal: the choice of the parameters, their\ndiscretization, and the analysis of a high number of objects with an\nunsupervised NP-hard classification technique like cladistics. 1. Introduction\nHow do the galaxy form, and when? How did the galaxy evolve and transform\nthemselves to create the diversity we observe? What are the progenitors to\npresent-day galaxies? To answer these big questions, observations throughout\nthe Universe and the physical modelisation are obvious tools. But between\nthese, there is a key process, without which it would be impossible to extract\nsome digestible information from the complexity of these systems. This is\nclassification. One century ago, galaxies were discovered by Hubble. From\nimages obtained in the visible range of wavelengths, he synthetised his\nobservations through the usual process: classification. With only one parameter\n(the shape) that is qualitative and determined with the eye, he found four\ncategories: ellipticals, spirals, barred spirals and irregulars. This is the\nfamous Hubble classification. He later hypothetized relationships between these\nclasses, building the Hubble Tuning Fork. The Hubble classification has been\nrefined, notably by de Vaucouleurs, and is still used as the only global\nclassification of galaxies. Even though the physical relationships proposed by\nHubble are not retained any more, the Hubble Tuning Fork is nearly always used\nto represent the classification of the galaxy diversity under its new name the\nHubble sequence (e.g. Delgado-Serrano, 2012). Its success is impressive and can\nbe understood by its simplicity, even its beauty, and by the many correlations\nfound between the morphology of galaxies and their other properties. And one\nmust admit that there is no alternative up to now, even though both the Hubble\nclassification and diagram have been recognised to be unsatisfactory. Among the\nmost obvious flaws of this classification, one must mention its monovariate,\nqualitative, subjective and old-fashioned nature, as well as the difficulty to\ncharacterise the morphology of distant galaxies. The first two most significant\nmultivariate studies were by Watanabe et al. (1985) and Whitmore (1984). Since\nthe year 2005, the number of studies attempting to go beyond the Hubble\nclassification has increased largely. Why, despite of this, the Hubble\nclassification and its sequence are still alive and no alternative have yet\nemerged (Sandage, 2005)? My feeling is that the results of the multivariate\nanalyses are not easily integrated into a one-century old practice of modeling\nthe observations. In addition, extragalactic objects like galaxies, stellar\nclusters or stars do evolve. Astronomy now provides data on very distant\nobjects, raising the question of the relationships between those and our\npresent day nearby galaxies. Clearly, this is a phylogenetic problem.\nAstrocladistics 1 aims at exploring the use of phylogenetic tools in\nastrophysics (Fraix-Burnet et al., 2006a,b). We have proved that Maximum\nParsimony (or cladistics) can be applied in astrophysics and provides a new\nexploration tool of the data (Fraix-Burnet et al., 2009, 2012, Cardone \\&\nFraix-Burnet, 2013). As far as the classification of galaxies is concerned, a\nlarger number of objects must now be analysed. In this paper, I\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:34:24 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Fraix-Burnet", "Didier", "", "IPAG"]]}, {"id": "1606.00265", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Finding Singular Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for finding high density, low-dimensional structures in\nnoisy point clouds. These structures are sets with zero Lebesgue measure with\nrespect to the $D$-dimensional ambient space and belong to a $d<D$ dimensional\nspace. We call them \"singular features.\" Hunting for singular features\ncorresponds to finding unexpected or unknown structures hidden in point clouds\nbelonging to $\\R^D$. Our method outputs well defined sets of dimensions $d<D$.\nUnlike spectral clustering, the method works well in the presence of noise. We\nshow how to find singular features by first finding ridges in the estimated\ndensity, followed by a filtering step based on the eigenvalues of the Hessian\nof the density.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:50:12 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1606.00318", "submitter": "Lei Wang", "authors": "Lei Wang", "title": "Discovering Phase Transitions with Unsupervised Learning", "comments": "corrected typos, fixed links in references", "journal-ref": "Phys. Rev. B 94, 195105 (2016)", "doi": "10.1103/PhysRevB.94.195105", "report-no": null, "categories": "cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning is a discipline of machine learning which aims at\ndiscovering patterns in big data sets or classifying the data into several\ncategories without being trained explicitly. We show that unsupervised learning\ntechniques can be readily used to identify phases and phases transitions of\nmany body systems. Starting with raw spin configurations of a prototypical\nIsing model, we use principal component analysis to extract relevant low\ndimensional representations the original data and use clustering analysis to\nidentify distinct phases in the feature space. This approach successfully finds\nout physical concepts such as order parameter and structure factor to be\nindicators of the phase transition. We discuss future prospects of discovering\nmore complex phases and phase transitions using unsupervised learning\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 15:00:22 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 15:19:13 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Wang", "Lei", ""]]}, {"id": "1606.00389", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou and Jeff Bilmes", "title": "Stream Clipper: Scalable Submodular Maximization on Stream", "comments": "17 pages, 12 figures, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a streaming submodular maximization algorithm \"stream clipper\"\nthat performs as well as the offline greedy algorithm on document/video\nsummarization in practice. It adds elements from a stream either to a solution\nset $S$ or to an extra buffer $B$ based on two adaptive thresholds, and\nimproves $S$ by a final greedy step that starts from $S$ adding elements from\n$B$. During this process, swapping elements out of $S$ can occur if doing so\nyields improvements. The thresholds adapt based on if current memory\nutilization exceeds a budget, e.g., it increases the lower threshold, and\nremoves from the buffer $B$ elements below the new lower threshold. We show\nthat, while our approximation factor in the worst case is $1/2$ (like in\nprevious work, and corresponding to the tight bound), we show that there are\ndata-dependent conditions where our bound falls within the range $[1/2,\n1-1/e]$. In news and video summarization experiments, the algorithm\nconsistently outperforms other streaming methods, and, while using\nsignificantly less computation and memory, performs similarly to the offline\ngreedy algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:43:13 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 22:36:44 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 01:50:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhou", "Tianyi", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1606.00398", "submitter": "Sherenaz Al-Haj Baddar", "authors": "Sherenaz W. Al-Haj Baddar", "title": "Short Communication on QUIST: A Quick Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short communication we introduce the quick clustering algorithm\n(QUIST), an efficient hierarchical clustering algorithm based on sorting. QUIST\nis a poly-logarithmic divisive clustering algorithm that does not assume the\nnumber of clusters, and/or the cluster size to be known ahead of time. It is\nalso insensitive to the original ordering of the input.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:57:54 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Baddar", "Sherenaz W. Al-Haj", ""]]}, {"id": "1606.00399", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou, Hua Ouyang, Yi Chang, Jeff Bilmes, Carlos Guestrin", "title": "Scaling Submodular Maximization via Pruned Submodularity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new random pruning method (called \"submodular sparsification\n(SS)\") to reduce the cost of submodular maximization. The pruning is applied\nvia a \"submodularity graph\" over the $n$ ground elements, where each directed\nedge is associated with a pairwise dependency defined by the submodular\nfunction. In each step, SS prunes a $1-1/\\sqrt{c}$ (for $c>1$) fraction of the\nnodes using weights on edges computed based on only a small number ($O(\\log\nn)$) of randomly sampled nodes. The algorithm requires $\\log_{\\sqrt{c}}n$ steps\nwith a small and highly parallelizable per-step computation. An accuracy-speed\ntradeoff parameter $c$, set as $c = 8$, leads to a fast shrink rate\n$\\sqrt{2}/4$ and small iteration complexity $\\log_{2\\sqrt{2}}n$. Analysis shows\nthat w.h.p., the greedy algorithm on the pruned set of size $O(\\log^2 n)$ can\nachieve a guarantee similar to that of processing the original dataset. In news\nand video summarization tasks, SS is able to substantially reduce both\ncomputational costs and memory usage, while maintaining (or even slightly\nexceeding) the quality of the original (and much more costly) greedy algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:58:36 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Zhou", "Tianyi", ""], ["Ouyang", "Hua", ""], ["Chang", "Yi", ""], ["Bilmes", "Jeff", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1606.00411", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Elaine O. Nsoesie, Emily Cohn,\n  Sumiko R. Mekaru, John S. Brownstein and Naren Ramakrishnan", "title": "Temporal Topic Modeling to Assess Associations between News Trends and\n  Infectious Disease Outbreaks", "comments": "This paper has been submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retrospective assessments, internet news reports have been shown to\ncapture early reports of unknown infectious disease transmission prior to\nofficial laboratory confirmation. In general, media interest and reporting\npeaks and wanes during the course of an outbreak. In this study, we quantify\nthe extent to which media interest during infectious disease outbreaks is\nindicative of trends of reported incidence. We introduce an approach that uses\nsupervised temporal topic models to transform large corpora of news articles\ninto temporal topic trends. The key advantages of this approach include,\napplicability to a wide range of diseases, and ability to capture disease\ndynamics - including seasonality, abrupt peaks and troughs. We evaluated the\nmethod using data from multiple infectious disease outbreaks reported in the\nUnited States of America (U.S.), China and India. We noted that temporal topic\ntrends extracted from disease-related news reports successfully captured the\ndynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue\noutbreaks in India (2013) and China (2014). Our observations also suggest that\nefficient modeling of temporal topic trends using time-series regression\ntechniques can estimate disease case counts with increased precision before\nofficial reports by health organizations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 19:30:07 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Nsoesie", "Elaine O.", ""], ["Cohn", "Emily", ""], ["Mekaru", "Sumiko R.", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1606.00451", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "Graph-Guided Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization has become a primary tool for developing reliable estimators\nof the covariance matrix in high-dimensional settings. To curb the curse of\ndimensionality, numerous methods assume that the population covariance (or\ninverse covariance) matrix is sparse, while making no particular structural\nassumptions on the desired pattern of sparsity. A highly-related, yet\ncomplementary, literature studies the specific setting in which the measured\nvariables have a known ordering, in which case a banded population matrix is\noften assumed. While the banded approach is conceptually and computationally\neasier than asking for \"patternless sparsity,\" it is only applicable in very\nspecific situations (such as when data are measured over time or\none-dimensional space). This work proposes a generalization of the notion of\nbandedness that greatly expands the range of problems in which banded\nestimators apply.\n  We develop convex regularizers occupying the broad middle ground between the\nformer approach of \"patternless sparsity\" and the latter reliance on having a\nknown ordering. Our framework defines bandedness with respect to a known graph\non the measured variables. Such a graph is available in diverse situations, and\nwe provide a theoretical, computational, and applied treatment of two new\nestimators. An R package, called ggb, implements these new methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 20:01:02 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 20:27:43 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1606.00546", "submitter": "Florian Ziel", "authors": "Florian Ziel, Carsten Croonenbroeck, Daniel Ambach", "title": "Forecasting wind power - Modeling periodic and non-linear effects under\n  conditional heteroscedasticity", "comments": null, "journal-ref": "Applied Energy, 177 (2016) 285-297", "doi": "10.1016/j.apenergy.2016.05.111", "report-no": null, "categories": "stat.AP stat.CO stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an approach that enables joint wind speed and wind\npower forecasts for a wind park. We combine a multivariate seasonal time\nvarying threshold autoregressive moving average (TVARMA) model with a power\nthreshold generalized autoregressive conditional heteroscedastic (power-TGARCH)\nmodel. The modeling framework incorporates diurnal and annual periodicity\nmodeling by periodic B-splines, conditional heteroscedasticity and a complex\nautoregressive structure with non-linear impacts. In contrast to usually\ntime-consuming estimation approaches as likelihood estimation, we apply a\nhigh-dimensional shrinkage technique. We utilize an iteratively re-weighted\nleast absolute shrinkage and selection operator (lasso) technique. It allows\nfor conditional heteroscedasticity, provides fast computing times and\nguarantees a parsimonious and regularized specification, even though the\nparameter space may be vast. We are able to show that our approach provides\naccurate forecasts of wind power at a turbine-specific level for forecasting\nhorizons of up to 48 h (short- to medium-term forecasts).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:01:14 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Ziel", "Florian", ""], ["Croonenbroeck", "Carsten", ""], ["Ambach", "Daniel", ""]]}, {"id": "1606.00602", "submitter": "Xiyu Yu PhD", "authors": "Xiyu Yu, Dacheng Tao", "title": "Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex\n  Composite optimization", "comments": "This paper has been withdrawn by the author due to an error in the\n  proof of the convergence rate. They will modify this proof as soon as\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study non-convex composite optimization: first, a finite-sum of\nsmooth but non-convex functions, and second, a general function that admits a\nsimple proximal mapping. Most research on stochastic methods for composite\noptimization assumes convexity or strong convexity of each function. In this\npaper, we extend this problem into the non-convex setting using variance\nreduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a\nconstant step size, both prox-SVRG and prox-SAGA are suitable for non-convex\ncomposite optimization, and help the problem converge to a stationary point\nwithin $O(1/\\epsilon)$ iterations. That is similar to the convergence rate seen\nwith the state-of-the-art RSAG method and faster than stochastic gradient\ndescent. Our analysis is also extended into the min-batch setting, which\nlinearly accelerates the convergence. To the best of our knowledge, this is the\nfirst analysis of convergence rate of variance-reduced proximal stochastic\ngradient for non-convex composite optimization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 09:59:16 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 04:15:04 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yu", "Xiyu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1606.00656", "submitter": "Gergo Barta", "authors": "Gergo Barta, Gabor Nagy, Gabor Simon, Gyozo Papp", "title": "Forecasting Framework for Open Access Time Series in Energy", "comments": "6 pages, 6 figures, IEEE Energycon 2016, Leuven, Belgium", "journal-ref": null, "doi": "10.1109/ENERGYCON.2016.7514015", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for automated forecasting of\nenergy-related time series using open access data from European Network of\nTransmission System Operators for Electricity (ENTSO-E). The framework provides\nforecasts for various European countries using publicly available historical\ndata only. Our solution was benchmarked using the actual load data and the\ncountry provided estimates (where available). We conclude that the proposed\nsystem can produce timely forecasts with comparable prediction accuracy in a\nnumber of cases. We also investigate the probabilistic case of forecasting -\nthat is, providing a probability distribution rather than a simple point\nforecast - and incorporate it into a web based API that provides quick and easy\naccess to reliable forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 12:59:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Barta", "Gergo", ""], ["Nagy", "Gabor", ""], ["Simon", "Gabor", ""], ["Papp", "Gyozo", ""]]}, {"id": "1606.00668", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and Yuanyuan Liu and James Cheng", "title": "Unified Scalable Equivalent Formulations for Schatten Quasi-Norms", "comments": "21 pages. CUHK Technical Report CSE-ShangLC20160307, March 7, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schatten quasi-norm can be used to bridge the gap between the nuclear\nnorm and rank function, and is the tighter approximation to matrix rank.\nHowever, most existing Schatten quasi-norm minimization (SQNM) algorithms, as\nwell as for nuclear norm minimization, are too slow or even impractical for\nlarge-scale problems, due to the SVD or EVD of the whole matrix in each\niteration. In this paper, we rigorously prove that for any p, p1, p2>0\nsatisfying 1/p=1/p1+1/p2, the Schatten-p quasi-norm of any matrix is equivalent\nto minimizing the product of the Schatten-p1 norm (or quasi-norm) and\nSchatten-p2 norm (or quasi-norm) of its two factor matrices. Then we present\nand prove the equivalence relationship between the product formula of the\nSchatten quasi-norm and its weighted sum formula for the two cases of p1 and\np2: p1=p2 and p1\\neq p2. In particular, when p>1/2, there is an equivalence\nbetween the Schatten-p quasi-norm of any matrix and the Schatten-2p norms of\nits two factor matrices, where the widely used equivalent formulation of the\nnuclear norm can be viewed as a special case. That is, various SQNM problems\nwith p>1/2 can be transformed into the one only involving smooth, convex norms\nof two factor matrices, which can lead to simpler and more efficient algorithms\nthan conventional methods.\n  We further extend the theoretical results of two factor matrices to the cases\nof three and more factor matrices, from which we can see that for any 0<p<1,\nthe Schatten-p quasi-norm of any matrix is the minimization of the mean of the\nSchatten-(p3+1)p norms of all factor matrices, where p3 denotes the largest\ninteger not exceeding 1/p. In other words, for any 0<p<1, the SQNM problem can\nbe transformed into an optimization problem only involving the smooth, convex\nnorms of multiple factor matrices.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 13:29:49 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 17:09:52 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""]]}, {"id": "1606.00704", "submitter": "Vincent Dumoulin", "authors": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro,\n  Alex Lamb, Martin Arjovsky, Aaron Courville", "title": "Adversarially Learned Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the adversarially learned inference (ALI) model, which jointly\nlearns a generation network and an inference network using an adversarial\nprocess. The generation network maps samples from stochastic latent variables\nto the data space while the inference network maps training examples in data\nspace to the space of latent variables. An adversarial game is cast between\nthese two networks and a discriminative network is trained to distinguish\nbetween joint latent/data-space samples from the generative network and joint\nsamples from the inference network. We illustrate the ability of the model to\nlearn mutually coherent inference and generation networks through the\ninspections of model samples and reconstructions and confirm the usefulness of\nthe learned representations by obtaining a performance competitive with\nstate-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 14:43:37 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 18:05:10 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 18:28:22 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Belghazi", "Ishmael", ""], ["Poole", "Ben", ""], ["Mastropietro", "Olivier", ""], ["Lamb", "Alex", ""], ["Arjovsky", "Martin", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.00709", "submitter": "Sebastian Nowozin", "authors": "Sebastian Nowozin, Botond Cseke, Ryota Tomioka", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence\n  Minimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural samplers are probabilistic models that implement sampling\nusing feedforward neural networks: they take a random input vector and produce\na sample from a probability distribution defined by the network weights. These\nmodels are expressive and allow efficient computation of samples and\nderivatives, but cannot be used for computing likelihoods or for\nmarginalization. The generative-adversarial training method allows to train\nsuch models through the use of an auxiliary discriminative neural network. We\nshow that the generative-adversarial approach is a special case of an existing\nmore general variational divergence estimation approach. We show that any\nf-divergence can be used for training generative neural samplers. We discuss\nthe benefits of various choices of divergence functions on training complexity\nand the quality of the obtained generative models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 14:53:33 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nowozin", "Sebastian", ""], ["Cseke", "Botond", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1606.00720", "submitter": "Michael Smith", "authors": "Michael Thomas Smith, Max Zwiessele, Neil D. Lawrence", "title": "Differentially Private Gaussian Processes", "comments": "9 pages + 4 supplementary material pages, 6 plots grouped into 5\n  figures, accepted at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for machine learning is increasing the availability of data\nwhile respecting the privacy of individuals. Here we combine the provable\nprivacy guarantees of the differential privacy framework with the flexibility\nof Gaussian processes (GPs). We propose a method using GPs to provide\ndifferentially private (DP) regression. We then improve this method by crafting\nthe DP noise covariance structure to efficiently protect the training data,\nwhile minimising the scale of the added noise. We find that this cloaking\nmethod achieves the greatest accuracy, while still providing privacy\nguarantees, and offers practical DP for regression over multi-dimensional\ninputs. Together these methods provide a starter toolkit for combining\ndifferential privacy and GPs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 15:26:53 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 11:56:29 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 16:34:53 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Smith", "Michael Thomas", ""], ["Zwiessele", "Max", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1606.00739", "submitter": "Stefan Riezler", "authors": "Artem Sokolov and Julia Kreutzer and Christopher Lo and Stefan Riezler", "title": "Stochastic Structured Prediction under Bandit Feedback", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic structured prediction under bandit feedback follows a learning\nprotocol where on each of a sequence of iterations, the learner receives an\ninput, predicts an output structure, and receives partial feedback in form of a\ntask loss evaluation of the predicted structure. We present applications of\nthis learning scenario to convex and non-convex objectives for structured\nprediction and analyze them as stochastic first-order methods. We present an\nexperimental evaluation on problems of natural language processing over\nexponential output spaces, and compare convergence speed across different\nobjectives under the practical criterion of optimal task performance on\ndevelopment data and the optimization-theoretic criterion of minimal squared\ngradient norm. Best results under both criteria are obtained for a non-convex\nobjective for pairwise preference learning under bandit feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:06:29 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 16:29:42 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sokolov", "Artem", ""], ["Kreutzer", "Julia", ""], ["Lo", "Christopher", ""], ["Riezler", "Stefan", ""]]}, {"id": "1606.00776", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula,\n  Bowen Zhou, Yoshua Bengio, Aaron Courville", "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue\n  Response Generation", "comments": "21 pages, 2 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 17:37:31 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:01:16 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Klinger", "Tim", ""], ["Tesauro", "Gerald", ""], ["Talamadupula", "Kartik", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.00787", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Eric Xing", "title": "Post-Inference Prior Swapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:20:35 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 18:01:17 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Neiswanger", "Willie", ""], ["Xing", "Eric", ""]]}, {"id": "1606.00800", "submitter": "Brian Mitchell", "authors": "Brian A. Mitchell and Linda R. Petzold", "title": "Multi-View Treelet Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.SI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-view factorization methods make assumptions that are not\nacceptable for many kinds of data, and in particular, for graphical data with\nhierarchical structure. At the same time, current hierarchical methods work\nonly in the single-view setting. We generalize the Treelet Transform to the\nMulti-View Treelet Transform (MVTT) to allow for the capture of hierarchical\nstructure when multiple views are available. Further, we show how this\ngeneralization is consistent with the existing theory and how it might be used\nin denoising empirical networks and in computing the shared response of\nfunctional brain data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:51:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 19:07:46 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mitchell", "Brian A.", ""], ["Petzold", "Linda R.", ""]]}, {"id": "1606.00813", "submitter": "David Inouye", "authors": "David I. Inouye, Pradeep Ravikumar, Inderjit S. Dhillon", "title": "Generalized Root Models: Beyond Pairwise Graphical Models for Univariate\n  Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel k-way high-dimensional graphical model called the\nGeneralized Root Model (GRM) that explicitly models dependencies between\nvariable sets of size k > 2---where k = 2 is the standard pairwise graphical\nmodel. This model is based on taking the k-th root of the original sufficient\nstatistics of any univariate exponential family with positive sufficient\nstatistics, including the Poisson and exponential distributions. As in the\nrecent work with square root graphical (SQR) models [Inouye et al.\n2016]---which was restricted to pairwise dependencies---we give the conditions\nof the parameters that are needed for normalization using the radial\nconditionals similar to the pairwise case [Inouye et al. 2016]. In particular,\nwe show that the Poisson GRM has no restrictions on the parameters and the\nexponential GRM only has a restriction akin to negative definiteness. We\ndevelop a simple but general learning algorithm based on L1-regularized\nnode-wise regressions. We also present a general way of numerically\napproximating the log partition function and associated derivatives of the GRM\nunivariate node conditionals---in contrast to [Inouye et al. 2016], which only\nprovided algorithm for estimating the exponential SQR. To illustrate GRM, we\nmodel word counts with a Poisson GRM and show the associated k-sized variable\nsets. We finish by discussing methods for reducing the parameter space in\nvarious situations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:13:23 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Inouye", "David I.", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1606.00832", "submitter": "Quanquan Gu", "authors": "Jinghui Chen and Quanquan Gu", "title": "High Dimensional Multivariate Regression and Precision Matrix Estimation\n  via Nonconvex Optimization", "comments": "32 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonconvex estimator for joint multivariate regression and\nprecision matrix estimation in the high dimensional regime, under sparsity\nconstraints. A gradient descent algorithm with hard thresholding is developed\nto solve the nonconvex estimator, and it attains a linear rate of convergence\nto the true regression coefficients and precision matrix simultaneously, up to\nthe statistical error. Compared with existing methods along this line of\nresearch, which have little theoretical guarantee, the proposed algorithm not\nonly is computationally much more efficient with provable convergence\nguarantee, but also attains the optimal finite sample statistical rate up to a\nlogarithmic factor. Thorough experiments on both synthetic and real datasets\nback up our theory.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:59:44 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Chen", "Jinghui", ""], ["Gu", "Quanquan", ""]]}, {"id": "1606.00856", "submitter": "Jesus Malo", "authors": "Valero Laparra and Jesus Malo", "title": "Sequential Principal Curves Analysis", "comments": "17 pages, 14 figs., 72 refs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work includes all the technical details of the Sequential Principal\nCurves Analysis (SPCA) in a single document. SPCA is an unsupervised nonlinear\nand invertible feature extraction technique. The identified curvilinear\nfeatures can be interpreted as a set of nonlinear sensors: the response of each\nsensor is the projection onto the corresponding feature. Moreover, it can be\neasily tuned for different optimization criteria; e.g. infomax, error\nminimization, decorrelation; by choosing the right way to measure distances\nalong each curvilinear feature. Even though proposed in [Laparra et al. Neural\nComp. 12] and shown to work in multiple modalities in [Laparra and Malo\nFrontiers Hum. Neuro. 15], the SPCA framework has its original roots in the\nnonlinear ICA algorithm in [Malo and Gutierrez Network 06]. Later on, the SPCA\nphilosophy for nonlinear generalization of PCA originated substantially faster\nalternatives at the cost of introducing different constraints in the model.\nNamely, the Principal Polynomial Analysis (PPA) [Laparra et al. IJNS 14], and\nthe Dimensionality Reduction via Regression (DRR) [Laparra et al. IEEE TGRS\n15]. This report illustrates the reasons why we developed such family and is\nthe appropriate technical companion for the missing details in [Laparra et al.,\nNeCo 12, Laparra and Malo, Front.Hum.Neuro. 15]. See also the data, code and\nexamples in the dedicated sites http://isp.uv.es/spca.html and\nhttp://isp.uv.es/after effects.html\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 20:23:00 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Laparra", "Valero", ""], ["Malo", "Jesus", ""]]}, {"id": "1606.00897", "submitter": "Stefan Bauer", "authors": "Stefan Bauer and Nicolas Carion and Peter Sch\\\"uffler and Thomas Fuchs\n  and Peter Wild and Joachim M. Buhmann", "title": "Multi-Organ Cancer Classification and Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust cell nuclei classification is the cornerstone for a wider\nrange of tasks in digital and Computational Pathology. However, most machine\nlearning systems require extensive labeling from expert pathologists for each\nindividual problem at hand, with no or limited abilities for knowledge transfer\nbetween datasets and organ sites. In this paper we implement and evaluate a\nvariety of deep neural network models and model ensembles for nuclei\nclassification in renal cell cancer (RCC) and prostate cancer (PCa). We propose\na convolutional neural network system based on residual learning which\nsignificantly improves over the state-of-the-art in cell nuclei classification.\nFinally, we show that the combination of tissue types during training increases\nnot only classification accuracy but also overall survival analysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:09:00 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 20:06:14 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Bauer", "Stefan", ""], ["Carion", "Nicolas", ""], ["Sch\u00fcffler", "Peter", ""], ["Fuchs", "Thomas", ""], ["Wild", "Peter", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1606.00906", "submitter": "Qinliang Su", "authors": "Qinliang Su, Xuejun Liao, Changyou Chen, Lawrence Carin", "title": "Nonlinear Statistical Learning with Truncated Gaussian Graphical Models", "comments": "Appeared in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the truncated Gaussian graphical model (TGGM) as a novel\nframework for designing statistical models for nonlinear learning. A TGGM is a\nGaussian graphical model (GGM) with a subset of variables truncated to be\nnonnegative. The truncated variables are assumed latent and integrated out to\ninduce a marginal model. We show that the variables in the marginal model are\nnon-Gaussian distributed and their expected relations are nonlinear. We use\nexpectation-maximization to break the inference of the nonlinear model into a\nsequence of TGGM inference problems, each of which is efficiently solved by\nusing the properties and numerical methods of multivariate Gaussian\ndistributions. We use the TGGM to design models for nonlinear regression and\nclassification, with the performances of these models demonstrated on extensive\nbenchmark datasets and compared to state-of-the-art competing results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:39:40 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 19:22:48 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Su", "Qinliang", ""], ["Liao", "Xuejun", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "1606.00925", "submitter": "Qingyun Sun", "authors": "Qingyun Sun, Mengyuan Yan David Donoho and Stephen Boyd", "title": "Convolutional Imputation of Matrix Networks", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matrix network is a family of matrices, with relatedness modeled by a\nweighted graph. We consider the task of completing a partially observed matrix\nnetwork. We assume a novel sampling scheme where a fraction of matrices might\nbe completely unobserved. How can we recover the entire matrix network from\nincomplete observations? This mathematical problem arises in many applications\nincluding medical imaging and social networks.\n  To recover the matrix network, we propose a structural assumption that the\nmatrices have a graph Fourier transform which is low-rank. We formulate a\nconvex optimization problem and prove an exact recovery guarantee for the\noptimization problem. Furthermore, we numerically characterize the exact\nrecovery regime for varying rank and sampling rate and discover a new phase\ntransition phenomenon. Then we give an iterative imputation algorithm to\nefficiently solve the optimization problem and complete large scale matrix\nnetworks. We demonstrate the algorithm with a variety of applications such as\nMRI and Facebook user network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 22:40:59 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 22:37:05 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 21:01:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sun", "Qingyun", ""], ["Donoho", "Mengyuan Yan David", ""], ["Boyd", "Stephen", ""]]}, {"id": "1606.00931", "submitter": "Jared Katzman", "authors": "Jared Katzman, Uri Shaham, Jonathan Bates, Alexander Cloninger,\n  Tingting Jiang, Yuval Kluger", "title": "DeepSurv: Personalized Treatment Recommender System Using A Cox\n  Proportional Hazards Deep Neural Network", "comments": "Presented at the International Conference of Machine Learning\n  Computational Biology Workshop 2016", "journal-ref": null, "doi": "10.1186/s12874-018-0482-1", "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical practitioners use survival models to explore and understand the\nrelationships between patients' covariates (e.g. clinical and genetic features)\nand the effectiveness of various treatment options. Standard survival models\nlike the linear Cox proportional hazards model require extensive feature\nengineering or prior medical knowledge to model treatment interaction at an\nindividual level. While nonlinear survival methods, such as neural networks and\nsurvival forests, can inherently model these high-level interaction terms, they\nhave yet to be shown as effective treatment recommender systems. We introduce\nDeepSurv, a Cox proportional hazards deep neural network and state-of-the-art\nsurvival method for modeling interactions between a patient's covariates and\ntreatment effectiveness in order to provide personalized treatment\nrecommendations. We perform a number of experiments training DeepSurv on\nsimulated and real survival data. We demonstrate that DeepSurv performs as well\nas or better than other state-of-the-art survival models and validate that\nDeepSurv successfully models increasingly complex relationships between a\npatient's covariates and their risk of failure. We then show how DeepSurv\nmodels the relationship between a patient's features and effectiveness of\ndifferent treatment options to show how DeepSurv can be used to provide\nindividual treatment recommendations. Finally, we train DeepSurv on real\nclinical studies to demonstrate how it's personalized treatment recommendations\nwould increase the survival time of a set of patients. The predictive and\nmodeling capabilities of DeepSurv will enable medical researchers to use deep\nneural networks as a tool in their exploration, understanding, and prediction\nof the effects of a patient's characteristics on their risk of failure.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 23:01:49 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 01:17:22 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 02:58:51 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Katzman", "Jared", ""], ["Shaham", "Uri", ""], ["Bates", "Jonathan", ""], ["Cloninger", "Alexander", ""], ["Jiang", "Tingting", ""], ["Kluger", "Yuval", ""]]}, {"id": "1606.00972", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Song-Chun Zhu, Ying Nian Wu", "title": "Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sequences contain rich dynamic patterns, such as dynamic texture\npatterns that exhibit stationarity in the temporal domain, and action patterns\nthat are non-stationary in either spatial or temporal domain. We show that a\nspatial-temporal generative ConvNet can be used to model and synthesize dynamic\npatterns. The model defines a probability distribution on the video sequence,\nand the log probability is defined by a spatial-temporal ConvNet that consists\nof multiple layers of spatial-temporal filters to capture spatial-temporal\npatterns of different scales. The model can be learned from the training video\nsequences by an \"analysis by synthesis\" learning algorithm that iterates the\nfollowing two steps. Step 1 synthesizes video sequences from the currently\nlearned model. Step 2 then updates the model parameters based on the difference\nbetween the synthesized video sequences and the observed training sequences. We\nshow that the learning algorithm can synthesize realistic dynamic patterns.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 05:36:06 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 23:26:38 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.00985", "submitter": "Enmei Tu", "authors": "Enmei Tu, Yaqian Zhang, Lin Zhu, Jie Yang and Nikola Kasabov", "title": "A Graph-Based Semi-Supervised k Nearest-Neighbor Method for Nonlinear\n  Manifold Distributed Data Classification", "comments": "32 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$ Nearest Neighbors ($k$NN) is one of the most widely used supervised\nlearning algorithms to classify Gaussian distributed data, but it does not\nachieve good results when it is applied to nonlinear manifold distributed data,\nespecially when a very limited amount of labeled samples are available. In this\npaper, we propose a new graph-based $k$NN algorithm which can effectively\nhandle both Gaussian distributed data and nonlinear manifold distributed data.\nTo achieve this goal, we first propose a constrained Tired Random Walk (TRW) by\nconstructing an $R$-level nearest-neighbor strengthened tree over the graph,\nand then compute a TRW matrix for similarity measurement purposes. After this,\nthe nearest neighbors are identified according to the TRW matrix and the class\nlabel of a query point is determined by the sum of all the TRW weights of its\nnearest neighbors. To deal with online situations, we also propose a new\nalgorithm to handle sequential samples based a local neighborhood\nreconstruction. Comparison experiments are conducted on both synthetic data\nsets and real-world data sets to demonstrate the validity of the proposed new\n$k$NN algorithm and its improvements to other version of $k$NN algorithms.\nGiven the widespread appearance of manifold structures in real-world problems\nand the popularity of the traditional $k$NN algorithm, the proposed manifold\nversion $k$NN shows promising potential for classifying manifold-distributed\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 07:09:26 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Tu", "Enmei", ""], ["Zhang", "Yaqian", ""], ["Zhu", "Lin", ""], ["Yang", "Jie", ""], ["Kasabov", "Nikola", ""]]}, {"id": "1606.01039", "submitter": "Pablo A. Alvarado", "authors": "Pablo A. Alvarado, Dan Stowell", "title": "Gaussian Processes for Music Audio Modelling and Content Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real music signals are highly variable, yet they have strong statistical\nstructure. Prior information about the underlying physical mechanisms by which\nsounds are generated and rules by which complex sound structure is constructed\n(notes, chords, a complete musical score), can be naturally unified using\nBayesian modelling techniques. Typically algorithms for Automatic Music\nTranscription independently carry out individual tasks such as multiple-F0\ndetection and beat tracking. The challenge remains to perform joint estimation\nof all parameters. We present a Bayesian approach for modelling music audio,\nand content analysis. The proposed methodology based on Gaussian processes\nseeks joint estimation of multiple music concepts by incorporating into the\nkernel prior information about non-stationary behaviour, dynamics, and rich\nspectral content present in the modelled music signal. We illustrate the\nbenefits of this approach via two tasks: pitch estimation, and inferring\nmissing segments in a polyphonic audio recording.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 10:45:09 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 13:39:45 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Alvarado", "Pablo A.", ""], ["Stowell", "Dan", ""]]}, {"id": "1606.01111", "submitter": "Michalis Michaelides", "authors": "Michalis Michaelides (1), Dimitrios Milios (1), Jane Hillston (1) and\n  Guido Sanguinetti (1 and 2) ((1) School of Informatics, University of\n  Edinburgh, (2) SynthSys, Centre for Synthetic and Systems Biology, University\n  of Edinburgh)", "title": "Property-driven State-Space Coarsening for Continuous Time Markov Chains", "comments": "16 pages, 6 figures, 1 table", "journal-ref": "Lecture Notes in Computer Science 9826 (2016) 3-18", "doi": "10.1007/978-3-319-43425-4_1", "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems with large state-spaces are often expensive to thoroughly\nexplore experimentally. Coarse-graining methods aim to define simpler systems\nwhich are more amenable to analysis and exploration; most current methods,\nhowever, focus on a priori state aggregation based on similarities in\ntransition rates, which is not necessarily reflected in similar behaviours at\nthe level of trajectories. We propose a way to coarsen the state-space of a\nsystem which optimally preserves the satisfaction of a set of logical\nspecifications about the system's trajectories. Our approach is based on\nGaussian Process emulation and Multi-Dimensional Scaling, a dimensionality\nreduction technique which optimally preserves distances in non-Euclidean\nspaces. We show how to obtain low-dimensional visualisations of the system's\nstate-space from the perspective of properties' satisfaction, and how to define\nmacro-states which behave coherently with respect to the specifications. Our\napproach is illustrated on a non-trivial running example, showing promising\nperformance and high computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 14:43:52 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 17:32:57 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Michaelides", "Michalis", "", "1 and 2"], ["Milios", "Dimitrios", "", "1 and 2"], ["Hillston", "Jane", "", "1 and 2"], ["Sanguinetti", "Guido", "", "1 and 2"]]}, {"id": "1606.01128", "submitter": "Bilal Piot", "authors": "Bilal Piot, Matthieu Geist, Olivier Pietquin", "title": "Difference of Convex Functions Programming Applied to Control with\n  Expert Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports applications of Difference of Convex functions (DC)\nprogramming to Learning from Demonstrations (LfD) and Reinforcement Learning\n(RL) with expert data. This is made possible because the norm of the Optimal\nBellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is\nDC. Improvement in performance is demonstrated on two specific algorithms,\nnamely Reward-regularized Classification for Apprenticeship Learning (RCAL) and\nReinforcement Learning with Expert Demonstrations (RLED), through experiments\non generic Markov Decision Processes (MDP), called Garnets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:07:52 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 08:12:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Piot", "Bilal", ""], ["Geist", "Matthieu", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1606.01141", "submitter": "Nils Kriege", "authors": "Nils M. Kriege, Pierre-Louis Giscard, Richard C. Wilson", "title": "On Valid Optimal Assignment Kernels and Applications to Graph\n  Classification", "comments": "9 pages, 4 figures, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of kernel methods has initiated the design of novel positive\nsemidefinite functions, in particular for structured data. A leading design\nparadigm for this is the convolution kernel, which decomposes structured\nobjects into their parts and sums over all pairs of parts. Assignment kernels,\nin contrast, are obtained from an optimal bijection between parts, which can\nprovide a more valid notion of similarity. In general however, optimal\nassignments yield indefinite functions, which complicates their use in kernel\nmethods. We characterize a class of base kernels used to compare parts that\nguarantees positive semidefinite optimal assignment kernels. These base kernels\ngive rise to hierarchies from which the optimal assignment kernels are computed\nin linear time by histogram intersection. We apply these results by developing\nthe Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high\nclassification accuracy on widely-used benchmark data sets improving over the\noriginal Weisfeiler-Lehman kernel.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:32:27 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 12:25:27 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 14:56:20 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kriege", "Nils M.", ""], ["Giscard", "Pierre-Louis", ""], ["Wilson", "Richard C.", ""]]}, {"id": "1606.01160", "submitter": "Dong Huang", "authors": "Dong Huang and Jian-Huang Lai and Chang-Dong Wang", "title": "Robust Ensemble Clustering Using Probability Trajectories", "comments": "The MATLAB code and experimental data of this work are available at:\n  https://www.researchgate.net/publication/284259332", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2016, vol.28,\n  no.5, pp.1312-1326", "doi": "10.1109/TKDE.2015.2503753", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many successful ensemble clustering approaches have been developed\nin recent years, there are still two limitations to most of the existing\napproaches. First, they mostly overlook the issue of uncertain links, which may\nmislead the overall consensus process. Second, they generally lack the ability\nto incorporate global information to refine the local links. To address these\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\nbased on sparse graph representation and probability trajectory analysis. In\nparticular, we present the elite neighbor selection strategy to identify the\nuncertain links by locally adaptive thresholds and build a sparse graph with a\nsmall number of probably reliable links. We argue that a small number of\nprobably reliable links can lead to significantly better consensus results than\nusing all graph links regardless of their reliability. The random walk process\ndriven by a new transition probability matrix is utilized to explore the global\ninformation in the graph. We derive a novel and dense similarity measure from\nthe sparse graph by analyzing the probability trajectories of the random\nwalkers, based on which two consensus functions are further proposed.\nExperimental results on multiple real-world datasets demonstrate the\neffectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:09:32 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Huang", "Dong", ""], ["Lai", "Jian-Huang", ""], ["Wang", "Chang-Dong", ""]]}, {"id": "1606.01164", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory for Pattern Recognition", "comments": "Accepted for publication at NIPS 2016", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016),\n  1172-1180", "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of associative memory is studied, which stores and reliably retrieves\nmany more patterns than the number of neurons in the network. We propose a\nsimple duality between this dense associative memory and neural networks\ncommonly used in deep learning. On the associative memory side of this duality,\na family of models that smoothly interpolates between two limiting cases can be\nconstructed. One limit is referred to as the feature-matching mode of pattern\nrecognition, and the other one as the prototype regime. On the deep learning\nside of the duality, this family corresponds to feedforward neural networks\nwith one hidden layer and various activation functions, which transmit the\nactivities of the visible neurons to the hidden layer. This family of\nactivation functions includes logistics, rectified linear units, and rectified\npolynomials of higher degrees. The proposed duality makes it possible to apply\nenergy-based intuition from associative memory to analyze computational\nproperties of neural networks with unusual activation functions - the higher\nrectified polynomials which until now have not been used in deep learning. The\nutility of the dense memories is illustrated for two test cases: the logical\ngate XOR and the recognition of handwritten digits from the MNIST data set.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:17:01 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 16:05:36 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1606.01245", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and Yuanyuan Liu and James Cheng", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "comments": "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI\n  Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp.\n  2016--2022, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard\nnuclear norm in order to approximate the rank function more accurately.\nHowever, existing Schatten-p quasi-norm minimization algorithms involve\nsingular value decomposition (SVD) or eigenvalue decomposition (EVD) in each\niteration, and thus may become very slow and impractical for large-scale\nproblems. In this paper, we first define two tractable Schatten quasi-norms,\ni.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove\nthat they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,\nwhich lead to the design of very efficient algorithms that only need to update\ntwo much smaller factor matrices. We also design two efficient proximal\nalternating linearized minimization algorithms for solving representative\nmatrix completion problems. Finally, we provide the global convergence and\nperformance guarantees for our algorithms, which have better convergence\nproperties than existing algorithms. Experimental results on synthetic and\nreal-world data show that our algorithms are more accurate than the\nstate-of-the-art methods, and are orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 03:28:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""]]}, {"id": "1606.01284", "submitter": "Wenshuo Wang", "authors": "Wenshuo Wang, Junqiang Xi, and Xiaohan Li", "title": "Statistical Pattern Recognition for Driving Styles Based on Bayesian\n  Probability and Kernel Density Estimation", "comments": "10 pages, 9 figures. Submitted to International Journal of Automotive\n  Technology", "journal-ref": "IET Intelligent Transportation Systems, 2018", "doi": "10.1049/iet-its.2017.0379", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving styles have a great influence on vehicle fuel economy, active safety,\nand drivability. To recognize driving styles of path-tracking behaviors for\ndifferent divers, a statistical pattern-recognition method is developed to deal\nwith the uncertainty of driving styles or characteristics based on probability\ndensity estimation. First, to describe driver path-tracking styles, vehicle\nspeed and throttle opening are selected as the discriminative parameters, and a\nconditional kernel density function of vehicle speed and throttle opening is\nbuilt, respectively, to describe the uncertainty and probability of two\nrepresentative driving styles, e.g., aggressive and normal. Meanwhile, a\nposterior probability of each element in feature vector is obtained using full\nBayesian theory. Second, a Euclidean distance method is involved to decide to\nwhich class the driver should be subject instead of calculating the complex\ncovariance between every two elements of feature vectors. By comparing the\nEuclidean distance between every elements in feature vector, driving styles are\nclassified into seven levels ranging from low normal to high aggressive.\nSubsequently, to show benefits of the proposed pattern-recognition method, a\ncross-validated method is used, compared with a fuzzy logic-based\npattern-recognition method. The experiment results show that the proposed\nstatistical pattern-recognition method for driving styles based on kernel\ndensity estimation is more efficient and stable than the fuzzy logic-based\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:48:53 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Wang", "Wenshuo", ""], ["Xi", "Junqiang", ""], ["Li", "Xiaohan", ""]]}, {"id": "1606.01316", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Srinadh Bhojanapalli, Constantine\n  Caramanis, Sujay Sanghavi", "title": "Provable Burer-Monteiro factorization for a class of norm-constrained\n  matrix problems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the projected gradient descent method on low-rank matrix problems\nwith a strongly convex objective. We use the Burer-Monteiro factorization\napproach to implicitly enforce low-rankness; such factorization introduces\nnon-convexity in the objective. We focus on constraint sets that include both\npositive semi-definite (PSD) constraints and specific matrix norm-constraints.\nSuch criteria appear in quantum state tomography and phase retrieval\napplications.\n  We show that non-convex projected gradient descent favors local linear\nconvergence in the factored space. We build our theory on a novel descent\nlemma, that non-trivially extends recent results on the unconstrained problem.\nThe resulting algorithm is Projected Factored Gradient Descent, abbreviated as\nProjFGD, and shows superior performance compared to state of the art on quantum\nstate tomography and sparse phase retrieval applications.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 02:12:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 02:35:04 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 22:47:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Bhojanapalli", "Srinadh", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.01473", "submitter": "Katelyn Gao", "authors": "Katelyn Gao", "title": "Confidence Intervals for Algorithmic Leveraging in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The age of big data has produced data sets that are computationally expensive\nto analyze and store. Algorithmic leveraging proposes that we sample\nobservations from the original data set to generate a representative data set\nand then perform analysis on the representative data set. In this paper, we\npresent efficient algorithms for constructing finite sample confidence\nintervals for each algorithmic leveraging estimated regression coefficient,\nwith asymptotic coverage guarantees. In simulations, we confirm empirically\nthat the confidence intervals have the desired coverage probabilities, while\nbootstrap confidence intervals may not.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 07:40:08 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 07:04:33 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 06:25:37 GMT"}, {"version": "v4", "created": "Sat, 10 Mar 2018 23:21:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gao", "Katelyn", ""]]}, {"id": "1606.01484", "submitter": "Hideyuki Miyahara", "authors": "Hideyuki Miyahara and Koji Tsumura", "title": "Relaxation of the EM Algorithm via Quantum Annealing", "comments": "6 pages, accepted to ACC 2016, minor revisions after the final\n  submission to ACC 2016", "journal-ref": null, "doi": "10.1109/ACC.2016.7526110", "report-no": null, "categories": "stat.ML cond-mat.stat-mech math.ST physics.comp-ph quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is a novel numerical method to obtain maximum likelihood\nestimates and is often used for practical calculations. However, many of\nmaximum likelihood estimation problems are nonconvex, and it is known that the\nEM algorithm fails to give the optimal estimate by being trapped by local\noptima. In order to deal with this difficulty, we propose a deterministic\nquantum annealing EM algorithm by introducing the mathematical mechanism of\nquantum fluctuations into the conventional EM algorithm because quantum\nfluctuations induce the tunnel effect and are expected to relax the difficulty\nof nonconvex optimization problems in the maximum likelihood estimation\nproblems. We show a theorem that guarantees its convergence and give numerical\nexperiments to verify its efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 09:47:18 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Miyahara", "Hideyuki", ""], ["Tsumura", "Koji", ""]]}, {"id": "1606.01487", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Bounds for Vector-Valued Function Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to derive risk bounds for vector-valued learning with\na broad class of feature maps and loss functions. Multi-task learning and\none-vs-all multi-category learning are treated as examples. We discuss in\ndetail vector-valued functions with one hidden layer, and demonstrate that the\nconditions under which shared representations are beneficial for multi- task\nlearning are equally applicable to multi-category learning.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 09:59:32 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1606.01554", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos", "title": "Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional\n  Estimators", "comments": "16 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide finite-sample analysis of a general framework for using k-nearest\nneighbor statistics to estimate functionals of a nonparametric continuous\nprobability density, including entropies and divergences. Rather than plugging\na consistent density estimate (which requires $k \\to \\infty$ as the sample size\n$n \\to \\infty$) into the functional of interest, the estimators we consider fix\nk and perform a bias correction. This is more efficient computationally, and,\nas we show in certain cases, statistically, leading to faster convergence\nrates. Our framework unifies several previous estimators, for most of which\nours are the first finite sample guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 20:17:29 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1606.01583", "submitter": "Augustus Odena", "authors": "Augustus Odena", "title": "Semi-Supervised Learning with Generative Adversarial Networks", "comments": "Appearing in the Data Efficient Machine Learning workshop at ICML\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Generative Adversarial Networks (GANs) to the semi-supervised\ncontext by forcing the discriminator network to output class labels. We train a\ngenerative model G and a discriminator D on a dataset with inputs belonging to\none of N classes. At training time, D is made to predict which of N+1 classes\nthe input belongs to, where an extra class is added to correspond to the\noutputs of G. We show that this method can be used to create a more\ndata-efficient classifier and that it allows for generating higher quality\nsamples than a regular GAN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 23:42:19 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 01:07:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Odena", "Augustus", ""]]}, {"id": "1606.01735", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Integrated perception with recurrent multi-task neural networks", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": "Advances in Neural Information Processing (NIPS) 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern discriminative predictors have been shown to match natural\nintelligences in specific perceptual tasks in image classification, object and\npart detection, boundary extraction, etc. However, a major advantage that\nnatural intelligences still have is that they work well for \"all\" perceptual\nproblems together, solving them efficiently and coherently in an \"integrated\nmanner\". In order to capture some of these advantages in machine perception, we\nask two questions: whether deep neural networks can learn universal image\nrepresentations, useful not only for a single task but for all of them, and how\nthe solutions to the different tasks can be integrated in this framework. We\nanswer by proposing a new architecture, which we call \"MultiNet\", in which not\nonly deep image features are shared between tasks, but where tasks can interact\nin a recurrent manner by encoding the results of their analysis in a common\nshared representation of the data. In this manner, we show that the performance\nof individual tasks in standard benchmarks can be improved first by sharing\nfeatures between them and then, more significantly, by integrating their\nsolutions in the common representation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:27:25 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:38:00 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1606.01793", "submitter": "Christian Grussler", "authors": "Christian Grussler, Anders Rantzer and Pontus Giselsson", "title": "Low-rank Optimization with Convex Constraints", "comments": "Accepted for publication in IEEE Transactions on Automatic Control", "journal-ref": "IEEE Trans. Automat. Control, 63(11), 4000-4007, 2018", "doi": "10.1109/TAC.2018.2813009", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low-rank approximation with convex constraints, which appears\nin data analysis, system identification, model order reduction, low-order\ncontroller design and low-complexity modelling is considered. Given a matrix,\nthe objective is to find a low-rank approximation that meets rank and convex\nconstraints, while minimizing the distance to the matrix in the squared\nFrobenius norm. In many situations, this non-convex problem is convexified by\nnuclear norm regularization. However, we will see that the approximations\nobtained by this method may be far from optimal. In this paper, we propose an\nalternative convex relaxation that uses the convex envelope of the squared\nFrobenius norm and the rank constraint. With this approach, easily verifiable\nconditions are obtained under which the solutions to the convex relaxation and\nthe original non-convex problem coincide. An SDP representation of the convex\nenvelope is derived, which allows us to apply this approach to several known\nproblems. Our example on optimal low-rank Hankel approximation/model reduction\nillustrates that the proposed convex relaxation performs consistently better\nthan nuclear norm regularization and may outperform balanced truncation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:46:08 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 15:14:26 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 12:47:51 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Grussler", "Christian", ""], ["Rantzer", "Anders", ""], ["Giselsson", "Pontus", ""]]}, {"id": "1606.01800", "submitter": "Ramji Venkataramanan", "authors": "Cynthia Rush and Ramji Venkataramanan", "title": "Finite Sample Analysis of Approximate Message Passing Algorithms", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, vol. 64, no. 11, pp.\n  7264-7286, November 2018", "doi": "10.1109/TIT.2018.2816681", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate message passing (AMP) refers to a class of efficient algorithms\nfor statistical estimation in high-dimensional problems such as compressed\nsensing and low-rank matrix estimation. This paper analyzes the performance of\nAMP in the regime where the problem dimension is large but finite. For\nconcreteness, we consider the setting of high-dimensional regression, where the\ngoal is to estimate a high-dimensional vector $\\beta_0$ from a noisy\nmeasurement $y=A \\beta_0 + w$. AMP is a low-complexity, scalable algorithm for\nthis problem. Under suitable assumptions on the measurement matrix $A$, AMP has\nthe attractive feature that its performance can be accurately characterized in\nthe large system limit by a simple scalar iteration called state evolution.\nPrevious proofs of the validity of state evolution have all been asymptotic\nconvergence results. In this paper, we derive a concentration inequality for\nAMP with i.i.d. Gaussian measurement matrices with finite size $n \\times N$.\nThe result shows that the probability of deviation from the state evolution\nprediction falls exponentially in $n$. This provides theoretical support for\nempirical findings that have demonstrated excellent agreement of AMP\nperformance with state evolution predictions for moderately large dimensions.\nThe concentration inequality also indicates that the number of AMP iterations\n$t$ can grow no faster than order $\\frac{\\log n}{\\log \\log n}$ for the\nperformance to be close to the state evolution predictions with high\nprobability. The analysis can be extended to obtain similar non-asymptotic\nresults for AMP in other settings such as low-rank matrix estimation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:59:14 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 10:56:42 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 10:46:06 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 10:20:14 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Rush", "Cynthia", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1606.01822", "submitter": "Steven Kearnes", "authors": "Steven Kearnes and Vijay Pande", "title": "ROCS-Derived Features for Virtual Screening", "comments": "See \"Version information\" section", "journal-ref": null, "doi": "10.1007/s10822-016-9959-3", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid overlay of chemical structures (ROCS) is a standard tool for the\ncalculation of 3D shape and chemical (\"color\") similarity. ROCS uses unweighted\nsums to combine many aspects of similarity, yielding parameter-free models for\nvirtual screening. In this report, we decompose the ROCS color force field into\n\"color components\" and \"color atom overlaps\", novel color similarity features\nthat can be weighted in a system-specific manner by machine learning\nalgorithms. In cross-validation experiments, these additional features\nsignificantly improve virtual screening performance (ROC AUC scores) relative\nto standard ROCS.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 16:48:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 21:44:50 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 18:23:05 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kearnes", "Steven", ""], ["Pande", "Vijay", ""]]}, {"id": "1606.01855", "submitter": "Aaron Schein", "authors": "Aaron Schein, Mingyuan Zhou, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations", "comments": "To appear in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 18:34:56 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Schein", "Aaron", ""], ["Zhou", "Mingyuan", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1606.01865", "submitter": "Zhengping Che", "authors": "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan\n  Liu", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing\n  Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series data in practical applications, such as health care,\ngeoscience, and biology, are characterized by a variety of missing values. In\ntime series prediction and other related tasks, it has been noted that missing\nvalues and their missing patterns are often correlated with the target labels,\na.k.a., informative missingness. There is very limited work on exploiting the\nmissing patterns for effective imputation and improving prediction performance.\nIn this paper, we develop novel deep learning models, namely GRU-D, as one of\nthe early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a\nstate-of-the-art recurrent neural network. It takes two representations of\nmissing patterns, i.e., masking and time interval, and effectively incorporates\nthem into a deep model architecture so that it not only captures the long-term\ntemporal dependencies in time series, but also utilizes the missing patterns to\nachieve better prediction results. Experiments of time series classification\ntasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic\ndatasets demonstrate that our models achieve state-of-the-art performance and\nprovides useful insights for better understanding and utilization of missing\nvalues in time series analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:08:41 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 20:51:29 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Che", "Zhengping", ""], ["Purushotham", "Sanjay", ""], ["Cho", "Kyunghyun", ""], ["Sontag", "David", ""], ["Liu", "Yan", ""]]}, {"id": "1606.01868", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,\n  David Saxton, Remi Munos", "title": "Unifying Count-Based Exploration and Intrinsic Motivation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an agent's uncertainty about its environment and the problem of\ngeneralizing this uncertainty across observations. Specifically, we focus on\nthe problem of exploration in non-tabular reinforcement learning. Drawing\ninspiration from the intrinsic motivation literature, we use density models to\nmeasure uncertainty, and propose a novel algorithm for deriving a pseudo-count\nfrom an arbitrary density model. This technique enables us to generalize\ncount-based exploration algorithms to the non-tabular case. We apply our ideas\nto Atari 2600 games, providing sensible pseudo-counts from raw pixels. We\ntransform these pseudo-counts into intrinsic rewards and obtain significantly\nimproved exploration in a number of hard games, including the infamously\ndifficult Montezuma's Revenge.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:21:32 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:16:21 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Srinivasan", "Sriram", ""], ["Ostrovski", "Georg", ""], ["Schaul", "Tom", ""], ["Saxton", "David", ""], ["Munos", "Remi", ""]]}, {"id": "1606.01869", "submitter": "Bowei Yan", "authors": "Bowei Yan and Purnamrita Sarkar", "title": "On Robustness of Kernel Clustering", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most important unsupervised problems in machine\nlearning and statistics. Among many existing algorithms, kernel k-means has\ndrawn much research attention due to its ability to find non-linear cluster\nboundaries and its inherent simplicity. There are two main approaches for\nkernel k-means: SVD of the kernel matrix and convex relaxations. Despite the\nattention kernel clustering has received both from theoretical and applied\nquarters, not much is known about robustness of the methods. In this paper we\nfirst introduce a semidefinite programming relaxation for the kernel clustering\nproblem, then prove that under a suitable model specification, both the K-SVD\nand SDP approaches are consistent in the limit, albeit SDP is strongly\nconsistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent,\ni.e. the fraction of misclassified nodes vanish.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:26:23 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 17:15:38 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 01:12:08 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yan", "Bowei", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1606.01885", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Learning to Optimize", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm design is a laborious process and often requires many iterations of\nideation and validation. In this paper, we explore automating algorithm design\nand present a method to learn an optimization algorithm, which we believe to be\nthe first method that can automatically discover a better algorithm. We\napproach this problem from a reinforcement learning perspective and represent\nany particular optimization algorithm as a policy. We learn an optimization\nalgorithm using guided policy search and demonstrate that the resulting\nalgorithm outperforms existing hand-engineered algorithms in terms of\nconvergence speed and/or the final objective value.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 19:50:47 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1606.01984", "submitter": "Linglong Kong", "authors": "Rui Zhu, Di Niu, Linglong Kong, Zongpeng Li", "title": "Expectile Matrix Factorization for Skewed Data Analysis", "comments": "8 page main text with 5 page supplementary documents, published in\n  AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a popular approach to solving matrix estimation\nproblems based on partial observations. Existing matrix factorization is based\non least squares and aims to yield a low-rank matrix to interpret the\nconditional sample means given the observations. However, in many real\napplications with skewed and extreme data, least squares cannot explain their\ncentral tendency or tail distributions, yielding undesired estimates. In this\npaper, we propose \\emph{expectile matrix factorization} by introducing\nasymmetric least squares, a key concept in expectile regression analysis, into\nthe matrix factorization framework. We propose an efficient algorithm to solve\nthe new problem based on alternating minimization and quadratic programming. We\nprove that our algorithm converges to a global optimum and exactly recovers the\ntrue underlying low-rank matrices when noise is zero. For synthetic data with\nskewed noise and a real-world dataset containing web service response times,\nthe proposed scheme achieves lower recovery errors than the existing matrix\nfactorization method based on least squares in a wide range of settings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 00:53:13 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 18:50:48 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 06:04:43 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zhu", "Rui", ""], ["Niu", "Di", ""], ["Kong", "Linglong", ""], ["Li", "Zongpeng", ""]]}, {"id": "1606.02074", "submitter": "Andrey Kormilitzin", "authors": "A. B. Kormilitzin, K. E. A. Saunders, P. J. Harrison, J. R. Geddes, T.\n  J. Lyons", "title": "Application of the Signature Method to Pattern Recognition in the CEQUEL\n  Clinical Trial", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification procedure of streaming data usually requires various ad\nhoc methods or particular heuristic models. We explore a novel non-parametric\nand systematic approach to analysis of heterogeneous sequential data. We\ndemonstrate an application of this method to classification of the delays in\nresponding to the prompts, from subjects with bipolar disorder collected during\na clinical trial, using both synthetic and real examples. We show how this\nmethod can provide a natural and systematic way to extract characteristic\nfeatures from sequential data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 09:36:29 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Kormilitzin", "A. B.", ""], ["Saunders", "K. E. A.", ""], ["Harrison", "P. J.", ""], ["Geddes", "J. R.", ""], ["Lyons", "T. J.", ""]]}, {"id": "1606.02077", "submitter": "Nagarajan Natarajan", "authors": "Prateek Jain and Nagarajan Natarajan", "title": "Regret Bounds for Non-decomposable Metrics with Missing Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recommending relevant labels (items) for a given\ndata point (user). In particular, we are interested in the practically\nimportant setting where the evaluation is with respect to non-decomposable\n(over labels) performance metrics like the $F_1$ measure, and the training data\nhas missing labels. To this end, we propose a generic framework that given a\nperformance metric $\\Psi$, can devise a regularized objective function and a\nthreshold such that all the values in the predicted score vector above and only\nabove the threshold are selected to be positive. We show that the regret or\ngeneralization error in the given metric $\\Psi$ is bounded ultimately by\nestimation error of certain underlying parameters. In particular, we derive\nregret bounds under three popular settings: a) collaborative filtering, b)\nmultilabel classification, and c) PU (positive-unlabeled) learning. For each of\nthe above problems, we can obtain precise non-asymptotic regret bound which is\nsmall even when a large fraction of labels is missing. Our empirical results on\nsynthetic and benchmark datasets demonstrate that by explicitly modeling for\nmissing labels and optimizing the desired performance metric, our algorithm\nindeed achieves significantly better performance (like $F_1$ score) when\ncompared to methods that do not model missing label information carefully.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 10:00:30 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jain", "Prateek", ""], ["Natarajan", "Nagarajan", ""]]}, {"id": "1606.02109", "submitter": "Onur Dikmen", "authors": "Antti Honkela, Mrinal Das, Arttu Nieminen, Onur Dikmen and Samuel\n  Kaski", "title": "Efficient differentially private learning improves drug sensitivity\n  prediction", "comments": "14 pages + 13 pages supplementary information, 3 + 3 figures", "journal-ref": "Biology Direct (2018) 13:1", "doi": "10.1186/s13062-017-0203-4", "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of a personalised recommendation system face a dilemma: recommendations\ncan be improved by learning from data, but only if the other users are willing\nto share their private information. Good personalised predictions are vitally\nimportant in precision medicine, but genomic information on which the\npredictions are based is also particularly sensitive, as it directly identifies\nthe patients and hence cannot easily be anonymised. Differential privacy has\nemerged as a potentially promising solution: privacy is considered sufficient\nif presence of individual patients cannot be distinguished. However,\ndifferentially private learning with current methods does not improve\npredictions with feasible data sizes and dimensionalities. Here we show that\nuseful predictors can be learned under powerful differential privacy\nguarantees, and even from moderately-sized data sets, by demonstrating\nsignificant improvements with a new robust private regression method in the\naccuracy of private drug sensitivity prediction. The method combines two key\nproperties not present even in recent proposals, which can be generalised to\nother predictors: we prove it is asymptotically consistently and efficiently\nprivate, and demonstrate that it performs well on finite data. Good finite data\nperformance is achieved by limiting the sharing of private information by\ndecreasing the dimensionality and by projecting outliers to fit tighter bounds,\ntherefore needing to add less noise for equal privacy. As already the\nsimple-to-implement method shows promise on the challenging genomic data, we\nanticipate rapid progress towards practical applications in many fields, such\nas mobile sensing and social media, in addition to the badly needed precision\nmedicine solutions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 11:52:28 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:38:00 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Honkela", "Antti", ""], ["Das", "Mrinal", ""], ["Nieminen", "Arttu", ""], ["Dikmen", "Onur", ""], ["Kaski", "Samuel", ""]]}, {"id": "1606.02185", "submitter": "Harrison Edwards", "authors": "Harrison Edwards, Amos Storkey", "title": "Towards a Neural Statistician", "comments": "Updated to camera ready version for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient learner is one who reuses what they already know to tackle a new\nproblem. For a machine learner, this means understanding the similarities\namongst datasets. In order to do this, one must take seriously the idea of\nworking with datasets, rather than datapoints, as the key objects to model.\nTowards this goal, we demonstrate an extension of a variational autoencoder\nthat can learn a method for computing representations, or statistics, of\ndatasets in an unsupervised fashion. The network is trained to produce\nstatistics that encapsulate a generative model for each dataset. Hence the\nnetwork enables efficient learning from new datasets for both unsupervised and\nsupervised tasks. We show that we are able to learn statistics that can be used\nfor: clustering datasets, transferring generative models to new datasets,\nselecting representative samples of datasets and classifying previously unseen\nclasses. We refer to our model as a neural statistician, and by this we mean a\nneural network that can learn to compute summary statistics of datasets without\nsupervision.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 15:36:39 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 17:18:16 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Edwards", "Harrison", ""], ["Storkey", "Amos", ""]]}, {"id": "1606.02206", "submitter": "Farzan Farnia", "authors": "Farzan Farnia, David Tse", "title": "A Minimax Approach to Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of\nprobability distributions $\\Gamma$ on $(X,Y)$, what is the optimal decision\nrule minimizing the worst-case expected loss over $\\Gamma$? In this paper, we\naddress this question by introducing a generalization of the principle of\nmaximum entropy. Applying this principle to sets of distributions with marginal\non $X$ constrained to be the empirical marginal from the data, we develop a\ngeneral minimax approach for supervised learning problems. While for some loss\nfunctions such as squared-error and log loss, the minimax approach rederives\nwell-knwon regression models, for the 0-1 loss it results in a new linear\nclassifier which we call the maximum entropy machine. The maximum entropy\nmachine minimizes the worst-case 0-1 loss over the structured set of\ndistribution, and by our numerical experiments can outperform other well-known\nlinear classifiers such as SVM. We also prove a bound on the generalization\nworst-case error in the minimax approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 16:39:09 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 01:20:30 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 23:03:43 GMT"}, {"version": "v4", "created": "Sun, 6 Nov 2016 23:19:58 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 01:56:04 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Farnia", "Farzan", ""], ["Tse", "David", ""]]}, {"id": "1606.02261", "submitter": "Brendan Tracey", "authors": "Brendan D. Tracey and David H. Wolpert", "title": "Reducing the error of Monte Carlo Algorithms by Learning Control\n  Variates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) sampling algorithms are an extremely widely-used technique\nto estimate expectations of functions f(x), especially in high dimensions.\nControl variates are a very powerful technique to reduce the error of such\nestimates, but in their conventional form rely on having an accurate\napproximation of f, a priori. Stacked Monte Carlo (StackMC) is a recently\nintroduced technique designed to overcome this limitation by fitting a control\nvariate to the data samples themselves. Done naively, forming a control variate\nto the data would result in overfitting, typically worsening the MC algorithm's\nperformance. StackMC uses in-sample / out-sample techniques to remove this\noverfitting. Crucially, it is a post-processing technique, requiring no\nadditional samples, and can be applied to data generated by any MC estimator.\nOur preliminary experiments demonstrated that StackMC improved the estimates of\nexpectations when it was used to post-process samples produces by a \"simple\nsampling\" MC estimator. Here we substantially extend this earlier work. We\nprovide an in-depth analysis of the StackMC algorithm, which we use to\nconstruct an improved version of the original algorithm, with lower estimation\nerror. We then perform experiments of StackMC on several additional kinds of MC\nestimators, demonstrating improved performance when the samples are generated\nvia importance sampling, Latin-hypercube sampling and quasi-Monte Carlo\nsampling. We also show how to extend StackMC to combine multiple fitting\nfunctions, and how to apply it to discrete input spaces x.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 18:52:38 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Tracey", "Brendan D.", ""], ["Wolpert", "David H.", ""]]}, {"id": "1606.02275", "submitter": "Roger Grosse", "authors": "Roger B. Grosse and Siddharth Ancha and Daniel M. Roy", "title": "Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:39:02 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Grosse", "Roger B.", ""], ["Ancha", "Siddharth", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1606.02307", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg, Shuyang Gao, Kyle Reing, Aram Galstyan", "title": "Sifting Common Information from Many Variables", "comments": "In Proceedings of the 26th International Joint Conference on\n  Artificial Intelligence (IJCAI-17). 8 pages, 7 figures. v4: Typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the relationship between any pair of variables is a rich and active\narea of research that is central to scientific practice. In contrast,\ncharacterizing the common information among any group of variables is typically\na theoretical exercise with few practical methods for high-dimensional data. A\npromising solution would be a multivariate generalization of the famous Wyner\ncommon information, but this approach relies on solving an apparently\nintractable optimization problem. We leverage the recently introduced\ninformation sieve decomposition to formulate an incremental version of the\ncommon information problem that admits a simple fixed point solution, fast\nconvergence, and complexity that is linear in the number of variables. This\nscalable approach allows us to demonstrate the usefulness of common information\nin high-dimensional learning problems. The sieve outperforms standard methods\non dimensionality reduction tasks, solves a blind source separation problem\nthat cannot be solved with ICA, and accurately recovers structure in brain\nimaging data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 20:00:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 16:46:05 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 23:09:55 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 20:00:35 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Gao", "Shuyang", ""], ["Reing", "Kyle", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.02321", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Karl Pichotta, James G. Scott", "title": "Better Conditional Density Estimation for Neural Networks", "comments": "12 pages, 3 figures, code available soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of the neural network literature focuses on predicting\npoint values for a given set of response variables, conditioned on a feature\nvector. In many cases we need to model the full joint conditional distribution\nover the response variables rather than simply making point predictions. In\nthis paper, we present two novel approaches to such conditional density\nestimation (CDE): Multiscale Nets (MSNs) and CDE Trend Filtering. Multiscale\nnets transform the CDE regression task into a hierarchical classification task\nby decomposing the density into a series of half-spaces and learning boolean\nprobabilities of each split. CDE Trend Filtering applies a k-th order graph\ntrend filtering penalty to the unnormalized logits of a multinomial classifier\nnetwork, with each edge in the graph corresponding to a neighboring point on a\ndiscretized version of the density. We compare both methods against plain\nmultinomial classifier networks and mixture density networks (MDNs) on a\nsimulated dataset and three real-world datasets. The results suggest the two\nmethods are complementary: MSNs work well in a high-data-per-feature regime and\nCDE-TF is well suited for few-samples-per-feature scenarios where overfitting\nis a primary concern.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 20:19:59 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Tansey", "Wesley", ""], ["Pichotta", "Karl", ""], ["Scott", "James G.", ""]]}, {"id": "1606.02344", "submitter": "Tammo Rukat", "authors": "Tammo Rukat, Adam Baker, Andrew Quinn, Mark Woolrich", "title": "Resting state brain networks from EEG: Hidden Markov states vs.\n  classical microstates", "comments": "Presented at MLINI-2015 workshop, 2015 (arXiv:abs/1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/09", "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional brain networks exhibit dynamics on the sub-second temporal scale\nand are often assumed to embody the physiological substrate of cognitive\nprocesses. Here we analyse the temporal and spatial dynamics of these states,\nas measured by EEG, with a hidden Markov model and compare this approach to\nclassical EEG microstate analysis. We find dominating state lifetimes of\n100--150\\,ms for both approaches. The state topographies show obvious\nsimilarities. However, they also feature distinct spatial and especially\ntemporal properties. These differences may carry physiological meaningful\ninformation originating from patterns in the data that the HMM is able to\nintegrate while the microstate analysis is not. This hypothesis is supported by\na consistently high pairwise correlation of the temporal evolution of EEG\nmicrostates which is not observed for the HMM states and which seems unlikely\nto be a good description of the underlying physiology. However, further\ninvestigation is required to determine the robustness and the functional and\nclinical relevance of EEG HMM states in comparison to EEG microstates.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 21:59:31 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Rukat", "Tammo", ""], ["Baker", "Adam", ""], ["Quinn", "Andrew", ""], ["Woolrich", "Mark", ""]]}, {"id": "1606.02346", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz, Kristian Kersting", "title": "How is a data-driven approach better than random choice in label space\n  division for multi-label classification?", "comments": null, "journal-ref": null, "doi": "10.3390/e18080282", "report-no": null, "categories": "cs.LG cs.PF cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose using five data-driven community detection approaches from social\nnetworks to partition the label space for the task of multi-label\nclassification as an alternative to random partitioning into equal subsets as\nperformed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector,\ninfomap, walktrap and label propagation algorithms. We construct a label\nco-occurence graph (both weighted an unweighted versions) based on training\ndata and perform community detection to partition the label set. We include\nBinary Relevance and Label Powerset classification methods for comparison. We\nuse gini-index based Decision Trees as the base classifier. We compare educated\napproaches to label space divisions against random baselines on 12 benchmark\ndata sets over five evaluation measures. We show that in almost all cases seven\neducated guess approaches are more likely to outperform RAkELd than otherwise\nin all measures, but Hamming Loss. We show that fastgreedy and walktrap\ncommunity detection methods on weighted label co-occurence graphs are 85-92%\nmore likely to yield better F1 scores than random partitioning. Infomap on the\nunweighted label co-occurence graphs is on average 90% of the times better than\nrandom paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard\nsimilarity. Weighted fastgreedy is better on average than RAkELd when it comes\nto Hamming Loss.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 22:17:30 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""], ["Kersting", "Kristian", ""]]}, {"id": "1606.02349", "submitter": "Marius C\\u{a}t\\u{a}lin Iordan", "authors": "Marius C\\u{a}t\\u{a}lin Iordan, Armand Joulin, Diane M. Beck, Li\n  Fei-Fei", "title": "Locally-Optimized Inter-Subject Alignment of Functional Cortical Regions", "comments": "Presented at MLINI-2015 workshop, 2015 (arXiv:cs/0101200)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/04", "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-subject registration of cortical areas is necessary in functional\nimaging (fMRI) studies for making inferences about equivalent brain function\nacross a population. However, many high-level visual brain areas are defined as\npeaks of functional contrasts whose cortical position is highly variable. As\nsuch, most alignment methods fail to accurately map functional regions of\ninterest (ROIs) across participants. To address this problem, we propose a\nlocally optimized registration method that directly predicts the location of a\nseed ROI on a separate target cortical sheet by maximizing the functional\ncorrelation between their time courses, while simultaneously allowing for\nnon-smooth local deformations in region topology. Our method outperforms the\ntwo most commonly used alternatives (anatomical landmark-based AFNI alignment\nand cortical convexity-based FreeSurfer alignment) in overlap between predicted\nregion and functionally-defined LOC. Furthermore, the maps obtained using our\nmethod are more consistent across subjects than both baseline measures.\nCritically, our method represents an important step forward towards predicting\nbrain regions without explicit localizer scans and deciphering the poorly\nunderstood relationship between the location of functional regions, their\nanatomical extent, and the consistency of computations those regions perform\nacross people.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 22:40:30 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Iordan", "Marius C\u0103t\u0103lin", ""], ["Joulin", "Armand", ""], ["Beck", "Diane M.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1606.02355", "submitter": "Tommaso Furlanello", "authors": "Tommaso Furlanello, Jiaping Zhao, Andrew M. Saxe, Laurent Itti, Bosco\n  S. Tjan", "title": "Active Long Term Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual Learning in artificial neural networks suffers from interference\nand forgetting when different tasks are learned sequentially. This paper\nintroduces the Active Long Term Memory Networks (A-LTM), a model of sequential\nmulti-task deep learning that is able to maintain previously learned\nassociation between sensory input and behavioral output while acquiring knew\nknowledge. A-LTM exploits the non-convex nature of deep neural networks and\nactively maintains knowledge of previously learned, inactive tasks using a\ndistillation loss. Distortions of the learned input-output map are penalized\nbut hidden layers are free to transverse towards new local optima that are more\nfavorable for the multi-task objective. We re-frame the McClelland's seminal\nHippocampal theory with respect to Catastrophic Inference (CI) behavior\nexhibited by modern deep architectures trained with back-propagation and\ninhomogeneous sampling of latent factors across epochs. We present empirical\nresults of non-trivial CI during continual learning in Deep Linear Networks\ntrained on the same task, in Convolutional Neural Networks when the task shifts\nfrom predicting semantic to graphical factors and during domain adaptation from\nsimple to complex environments. We present results of the A-LTM model's ability\nto maintain viewpoint recognition learned in the highly controlled iLab-20M\ndataset with 10 object categories and 88 camera viewpoints, while adapting to\nthe unstructured domain of Imagenet with 1,000 object categories.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 23:43:42 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Furlanello", "Tommaso", ""], ["Zhao", "Jiaping", ""], ["Saxe", "Andrew M.", ""], ["Itti", "Laurent", ""], ["Tjan", "Bosco S.", ""]]}, {"id": "1606.02359", "submitter": "Mathias Drton", "authors": "Mathias Drton and Marloes H. Maathuis", "title": "Structure Learning in Graphical Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical model is a statistical model that is associated to a graph whose\nnodes correspond to variables of interest. The edges of the graph reflect\nallowed conditional dependencies among the variables. Graphical models admit\ncomputationally convenient factorization properties and have long been a\nvaluable tool for tractable modeling of multivariate distributions. More\nrecently, applications such as reconstructing gene regulatory networks from\ngene expression data have driven major advances in structure learning, that is,\nestimating the graph underlying a model. We review some of these advances and\ndiscuss methods such as the graphical lasso and neighborhood selection for\nundirected graphical models (or Markov random fields), and the PC algorithm and\nscore-based search methods for directed graphical models (or Bayesian\nnetworks). We further review extensions that account for effects of latent\nvariables and heterogeneous data sources.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 23:58:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Drton", "Mathias", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1606.02396", "submitter": "Ardavan Saeedi", "authors": "Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman", "title": "Deep Successor Reinforcement Learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robust value functions given raw observations and rewards is now\npossible with model-free and model-based deep reinforcement learning\nalgorithms. There is a third alternative, called Successor Representations\n(SR), which decomposes the value function into two components -- a reward\npredictor and a successor map. The successor map represents the expected future\nstate occupancy from any given state and the reward predictor maps states to\nscalar rewards. The value function of a state can be computed as the inner\nproduct between the successor map and the reward weights. In this paper, we\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\nlearning framework. DSR has several appealing properties including: increased\nsensitivity to distal reward changes due to factorization of reward and world\ndynamics, and the ability to extract bottleneck states (subgoals) given\nsuccessor maps trained under a random policy. We show the efficacy of our\napproach on two diverse environments given raw pixel observations -- simple\ngrid-world domains (MazeBase) and the Doom game engine.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 04:48:49 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Saeedi", "Ardavan", ""], ["Gautam", "Simanta", ""], ["Gershman", "Samuel J.", ""]]}, {"id": "1606.02401", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin", "title": "On clustering network-valued data", "comments": "Updated title, added new materials; 21 pages, 3 figures, 3 tables;\n  conference version to appear in NIPS-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection, which focuses on clustering nodes or detecting\ncommunities in (mostly) a single network, is a problem of considerable\npractical interest and has received a great deal of attention in the research\ncommunity. While being able to cluster within a network is important, there are\nemerging needs to be able to cluster multiple networks. This is largely\nmotivated by the routine collection of network data that are generated from\npotentially different populations. These networks may or may not have node\ncorrespondence. When node correspondence is present, we cluster networks by\nsummarizing a network by its graphon estimate, whereas when node correspondence\nis not present, we propose a novel solution for clustering such networks by\nassociating a computationally feasible feature vector to each network based on\ntrace of powers of the adjacency matrix. We illustrate our methods using both\nsimulated and real data sets, and theoretical justifications are provided in\nterms of consistency.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:25:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 09:24:48 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 11:35:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Lin", "Lizhen", ""]]}, {"id": "1606.02404", "submitter": "Hassan Ashtiani", "authors": "Hassan Ashtiani, Shrinu Kushagra and Shai Ben-David", "title": "Clustering with Same-Cluster Queries", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Semi-Supervised Active Clustering framework\n(SSAC), where the learner is allowed to interact with a domain expert, asking\nwhether two given instances belong to the same cluster or not. We study the\nquery and computational complexity of clustering in this framework. We consider\na setting where the expert conforms to a center-based clustering with a notion\nof margin. We show that there is a trade off between computational complexity\nand query complexity; We prove that for the case of $k$-means clustering (i.e.,\nwhen the expert conforms to a solution of $k$-means), having access to\nrelatively few such queries allows efficient solutions to otherwise NP hard\nproblems.\n  In particular, we provide a probabilistic polynomial-time (BPP) algorithm for\nclustering in this setting that asks $O\\big(k^2\\log k + k\\log n)$ same-cluster\nqueries and runs with time complexity $O\\big(kn\\log n)$ (where $k$ is the\nnumber of clusters and $n$ is the number of instances). The algorithm succeeds\nwith high probability for data satisfying margin conditions under which,\nwithout queries, we show that the problem is NP hard. We also prove a lower\nbound on the number of queries needed to have a computationally efficient\nclustering algorithm in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:28:14 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 18:16:44 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Ashtiani", "Hassan", ""], ["Kushagra", "Shrinu", ""], ["Ben-David", "Shai", ""]]}, {"id": "1606.02421", "submitter": "Igor Colin", "authors": "Igor Colin, Aur\\'elien Bellet, Joseph Salmon, St\\'ephan\n  Cl\\'emen\\c{c}on", "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In decentralized networks (of sensors, connected objects, etc.), there is an\nimportant need for efficient algorithms to optimize a global cost function, for\ninstance to learn a global model from the local data collected by each\ncomputing unit. In this paper, we address the problem of decentralized\nminimization of pairwise functions of the data points, where these points are\ndistributed over the nodes of a graph defining the communication topology of\nthe network. This general problem finds applications in ranking, distance\nmetric learning and graph inference, among others. We propose new gossip\nalgorithms based on dual averaging which aims at solving such problems both in\nsynchronous and asynchronous settings. The proposed framework is flexible\nenough to deal with constrained and regularized variants of the optimization\nproblem. Our theoretical analysis reveals that the proposed algorithms preserve\nthe convergence rate of centralized dual averaging up to an additive bias term.\nWe present numerical simulations on Area Under the ROC Curve (AUC) maximization\nand metric learning problems which illustrate the practical interest of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:01:47 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Colin", "Igor", ""], ["Bellet", "Aur\u00e9lien", ""], ["Salmon", "Joseph", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""]]}, {"id": "1606.02518", "submitter": "Georgios Arvanitidis", "authors": "Georgios Arvanitidis, Lars Kai Hansen, S{\\o}ren Hauberg", "title": "A Locally Adaptive Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate normal density is a monotonic function of the distance to\nthe mean, and its ellipsoidal shape is due to the underlying Euclidean metric.\nWe suggest to replace this metric with a locally adaptive, smoothly changing\n(Riemannian) metric that favors regions of high local density. The resulting\nlocally adaptive normal distribution (LAND) is a generalization of the normal\ndistribution to the \"manifold\" setting, where data is assumed to lie near a\npotentially low-dimensional manifold embedded in $\\mathbb{R}^D$. The LAND is\nparametric, depending only on a mean and a covariance, and is the maximum\nentropy distribution under the given metric. The underlying metric is, however,\nnon-parametric. We develop a maximum likelihood algorithm to infer the\ndistribution parameters that relies on a combination of gradient descent and\nMonte Carlo integration. We further extend the LAND to mixture models, and\nprovide the corresponding EM algorithm. We demonstrate the efficiency of the\nLAND to fit non-trivial probability distributions over both synthetic data, and\nEEG measurements of human sleep.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 11:49:08 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 11:45:33 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 12:40:51 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Arvanitidis", "Georgios", ""], ["Hansen", "Lars Kai", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1606.02566", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Igor Pr\\\"unster", "title": "A moment-matching Ferguson and Klass algorithm", "comments": "24 pages, 6 figures, 5 tables", "journal-ref": "Statistics and Computing, 27(1):3--17, 2017", "doi": "10.1007/s11222-016-9676-8", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely random measures (CRM) represent the key building block of a wide\nvariety of popular stochastic models and play a pivotal role in modern Bayesian\nNonparametrics. A popular representation of CRMs as a random series with\ndecreasing jumps is due to Ferguson and Klass (1972). This can immediately be\nturned into an algorithm for sampling realizations of CRMs or more elaborate\nmodels involving transformed CRMs. However, concrete implementation requires to\ntruncate the random series at some threshold resulting in an approximation\nerror. The goal of this paper is to quantify the quality of the approximation\nby a moment-matching criterion, which consists in evaluating a measure of\ndiscrepancy between actual moments and moments based on the simulation output.\nSeen as a function of the truncation level, the methodology can be used to\ndetermine the truncation level needed to reach a certain level of precision.\nThe resulting moment-matching \\FK algorithm is then implemented and illustrated\non several popular Bayesian nonparametric models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:18:21 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Arbel", "Julyan", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "1606.02647", "submitter": "Marc G. Bellemare", "authors": "R\\'emi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare", "title": "Safe and Efficient Off-Policy Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we take a fresh look at some old and new algorithms for\noff-policy, return-based reinforcement learning. Expressing these in a common\nform, we derive a novel algorithm, Retrace($\\lambda$), with three desired\nproperties: (1) it has low variance; (2) it safely uses samples collected from\nany behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is\nefficient as it makes the best use of samples collected from near on-policy\nbehaviour policies. We analyze the contractive nature of the related operator\nunder both off-policy policy evaluation and control settings and derive online\nsample-based algorithms. We believe this is the first return-based off-policy\ncontrol algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy\nin the Limit with Infinite Exploration). As a corollary, we prove the\nconvergence of Watkins' Q($\\lambda$), which was an open problem since 1989. We\nillustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 17:34:13 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:26:31 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Munos", "R\u00e9mi", ""], ["Stepleton", "Tom", ""], ["Harutyunyan", "Anna", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "1606.02679", "submitter": "Daniel Romero", "authors": "Daniel Romero, Seung-Jun Kim, Georgios B. Giannakis, Roberto\n  Lopez-Valcarce", "title": "Learning Power Spectrum Maps from Quantized Power Measurements", "comments": "Submitted Jun. 2016", "journal-ref": null, "doi": "10.1109/TSP.2017.2666775", "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power spectral density (PSD) maps providing the distribution of RF power\nacross space and frequency are constructed using power measurements collected\nby a network of low-cost sensors. By introducing linear compression and\nquantization to a small number of bits, sensor measurements can be communicated\nto the fusion center with minimal bandwidth requirements. Strengths of data-\nand model-driven approaches are combined to develop estimators capable of\nincorporating multiple forms of spectral and propagation prior information\nwhile fitting the rapid variations of shadow fading across space. To this end,\nnovel nonparametric and semiparametric formulations are investigated. It is\nshown that PSD maps can be obtained using support vector machine-type solvers.\nIn addition to batch approaches, an online algorithm attuned to real-time\noperation is developed. Numerical tests assess the performance of the novel\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 01:30:19 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 15:59:29 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Romero", "Daniel", ""], ["Kim", "Seung-Jun", ""], ["Giannakis", "Georgios B.", ""], ["Lopez-Valcarce", "Roberto", ""]]}, {"id": "1606.02702", "submitter": "Joseph  Salmon", "authors": "Eugene Ndiaye and Olivier Fercoq and Alexandre Gramfort and Vincent\n  Lecl\\`ere and Joseph Salmon", "title": "Efficient Smoothed Concomitant Lasso Estimation for High Dimensional\n  Regression", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/904/1/012006", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, sparse structures are crucial for efficiency,\nboth in term of memory, computation and performance. It is customary to\nconsider $\\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity\nenforcing methods, the Lasso being a canonical example, are popular candidates\nto address high dimension. For efficiency, they rely on tuning a parameter\ntrading data fitting versus sparsity. For the Lasso theory to hold this tuning\nparameter should be proportional to the noise level, yet the latter is often\nunknown in practice. A possible remedy is to jointly optimize over the\nregression parameter as well as over the noise level. This has been considered\nunder several names in the literature: Scaled-Lasso, Square-root Lasso,\nConcomitant Lasso estimation for instance, and could be of interest for\nconfidence sets or uncertainty quantification. In this work, after illustrating\nnumerical difficulties for the Smoothed Concomitant Lasso formulation, we\npropose a modification we coined Smoothed Concomitant Lasso, aimed at\nincreasing numerical stability. We propose an efficient and accurate solver\nleading to a computational cost no more expansive than the one for the Lasso.\nWe leverage on standard ingredients behind the success of fast Lasso solvers: a\ncoordinate descent algorithm, combined with safe screening rules to achieve\nspeed efficiency, by eliminating early irrelevant features.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:51:47 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Lecl\u00e8re", "Vincent", ""], ["Salmon", "Joseph", ""]]}, {"id": "1606.02827", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Greg Ver Steeg, Aram Galstyan", "title": "Variational Information Maximization for Feature Selection", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is one of the most fundamental problems in machine\nlearning. An extensive body of work on information-theoretic feature selection\nexists which is based on maximizing mutual information between subsets of\nfeatures and class labels. Practical methods are forced to rely on\napproximations due to the difficulty of estimating mutual information. We\ndemonstrate that approximations made by existing methods are based on\nunrealistic assumptions. We formulate a more flexible and general class of\nassumptions based on variational distributions and use them to tractably\ngenerate lower bounds for mutual information. These bounds define a novel\ninformation-theoretic framework for feature selection, which we prove to be\noptimal under tree graphical models with proper choice of variational\ndistributions. Our experiments demonstrate that the proposed method strongly\noutperforms existing information-theoretic feature selection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 05:19:23 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Gao", "Shuyang", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.02838", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven (UR1, PANAMA), Anthony Bourrier (GIPSA-lab), R\\'emi\n  Gribonval (PANAMA), Patrick P\\'erez", "title": "Sketching for Large-Scale Learning of Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning parameters from voluminous data can be prohibitive in terms of\nmemory and computational requirements. We propose a \"compressive learning\"\nframework where we estimate model parameters from a sketch of the training\ndata. This sketch is a collection of generalized moments of the underlying\nprobability distribution of the data. It can be computed in a single pass on\nthe training set, and is easily computable on streams or distributed datasets.\nThe proposed framework shares similarities with compressive sensing, which aims\nat drastically reducing the dimension of high-dimensional signals while\npreserving the ability to reconstruct them. To perform the estimation task, we\nderive an iterative algorithm analogous to sparse reconstruction algorithms in\nthe context of linear inverse problems. We exemplify our framework with the\ncompressive estimation of a Gaussian Mixture Model (GMM), providing heuristics\non the choice of the sketching procedure and theoretical guarantees of\nreconstruction. We experimentally show on synthetic data that the proposed\nalgorithm yields results comparable to the classical Expectation-Maximization\n(EM) technique while requiring significantly less memory and fewer computations\nwhen the number of database elements is large. We further demonstrate the\npotential of the approach on real large-scale data (over 10 8 training samples)\nfor the task of model-based speaker verification. Finally, we draw some\nconnections between the proposed framework and approximate Hilbert space\nembedding of probability distributions using random features. We show that the\nproposed sketching operator can be seen as an innovative method to design\ntranslation-invariant kernels adapted to the analysis of GMMs. We also use this\ntheoretical framework to derive information preservation guarantees, in the\nspirit of infinite-dimensional compressive sensing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 06:59:19 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 11:22:44 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Keriven", "Nicolas", "", "UR1, PANAMA"], ["Bourrier", "Anthony", "", "GIPSA-lab"], ["Gribonval", "R\u00e9mi", "", "PANAMA"], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1606.02960", "submitter": "Sam Wiseman", "authors": "Sam Wiseman and Alexander M. Rush", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "comments": "EMNLP 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 13:29:34 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 03:45:30 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wiseman", "Sam", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.02979", "submitter": "Shaohua Li", "authors": "Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao", "title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)", "comments": "13 pages. The original version has been accepted in ACL 2016 as a\n  long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:45:39 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:49:07 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Li", "Shaohua", ""], ["Chua", "Tat-Seng", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1606.03063", "submitter": "William Levy Ph. D.", "authors": "William B Levy, Toby Berger, Mustafa Sungkar", "title": "Neural computation from first principles: Using the maximum entropy\n  method to obtain an optimal bits-per-joule neuron", "comments": null, "journal-ref": "IEEE Trans. Molecular, Biological, and Multi-scale Communication,\n  v2, Dec. 2016, 154-165", "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization results are one method for understanding neural computation from\nNature's perspective and for defining the physical limits on neuron-like\nengineering. Earlier work looks at individual properties or performance\ncriteria and occasionally a combination of two, such as energy and information.\nHere we make use of Jaynes' maximum entropy method and combine a larger set of\nconstraints, possibly dimensionally distinct, each expressible as an\nexpectation. The method identifies a likelihood-function and a sufficient\nstatistic arising from each such optimization. This likelihood is a\nfirst-hitting time distribution in the exponential class. Particular constraint\nsets are identified that, from an optimal inference perspective, justify\nearlier neurocomputational models. Interactions between constraints, mediated\nthrough the inferred likelihood, restrict constraint-set parameterizations,\ne.g., the energy-budget limits estimation performance which, in turn, matches\nan axonal communication constraint. Such linkages are, for biologists,\nexperimental predictions of the method. In addition to the related likelihood,\nat least one type of constraint set implies marginal distributions, and in this\ncase, a Shannon bits/joule statement arises.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 23:00:17 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 21:36:06 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Levy", "William B", ""], ["Berger", "Toby", ""], ["Sungkar", "Mustafa", ""]]}, {"id": "1606.03141", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen", "title": "Mutual Exclusivity Loss for Semi-Supervised Deep Learning", "comments": "5 pages, 1 figures, ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of semi-supervised learning with deep\nConvolutional Neural Networks (ConvNets). Semi-supervised learning is motivated\non the observation that unlabeled data is cheap and can be used to improve the\naccuracy of classifiers. In this paper we propose an unsupervised\nregularization term that explicitly forces the classifier's prediction for\nmultiple classes to be mutually-exclusive and effectively guides the decision\nboundary to lie on the low density space between the manifolds corresponding to\ndifferent classes of data. Our proposed approach is general and can be used\nwith any backpropagation-based learning method. We show through different\nexperiments that our method can improve the object recognition performance of\nConvNets using unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:15:16 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.03203", "submitter": "Finnian Lattimore", "authors": "Finnian Lattimore and Tor Lattimore and Mark D. Reid", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of using causal models to improve the rate at which good\ninterventions can be learned online in a stochastic environment. Our formalism\ncombines multi-arm bandits and causal inference to model a novel type of bandit\nfeedback that is not exploited by existing approaches. We propose a new\nalgorithm that exploits the causal feedback and prove a bound on its simple\nregret that is strictly better (in all quantities) than algorithms that do not\nuse the additional causal information.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:19:32 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Lattimore", "Finnian", ""], ["Lattimore", "Tor", ""], ["Reid", "Mark D.", ""]]}, {"id": "1606.03276", "submitter": "Shaona Ghosh", "authors": "Shaona Ghosh, Kevin Page and David De Roure", "title": "An Application of Network Lasso Optimization For Ride Sharing Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ride sharing has important implications in terms of environmental, social and\nindividual goals by reducing carbon footprints, fostering social interactions\nand economizing commuter costs. The ride sharing systems that are commonly\navailable lack adaptive and scalable techniques that can simultaneously learn\nfrom the large scale data and predict in real-time dynamic fashion. In this\npaper, we study such a problem towards a smart city initiative, where a generic\nride sharing system is conceived capable of making predictions about ride share\nopportunities based on the historically recorded data while satisfying\nreal-time ride requests. Underpinning the system is an application of a\npowerful machine learning convex optimization framework called Network Lasso\nthat uses the Alternate Direction Method of Multipliers (ADMM) optimization for\nlearning and dynamic prediction. We propose an application of a robust and\nscalable unified optimization framework within the ride sharing case-study. The\napplication of Network Lasso framework is capable of jointly optimizing and\nclustering different rides based on their spatial and model similarity. The\nprediction from the framework clusters new ride requests, making accurate price\nprediction based on the clusters, detecting hidden correlations in the data and\nallowing fast convergence due to the network topology. We provide an empirical\nevaluation of the application of ADMM network Lasso on real trip record and\nsimulated data, proving their effectiveness since the mean squared error of the\nalgorithm's prediction is minimized on the test rides.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 11:29:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 20:42:29 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 15:09:56 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Ghosh", "Shaona", ""], ["Page", "Kevin", ""], ["De Roure", "David", ""]]}, {"id": "1606.03352", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona,\n  Pei-Hao Su, Stefan Ultes, David Vandyke, Steve Young", "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a variety of LSTM-based conditional language models (LM) have been\napplied across a range of language generation tasks. In this work we study\nvarious model architectures and different ways to represent and aggregate the\nsource information in an end-to-end neural dialogue system framework. A method\ncalled snapshot learning is also proposed to facilitate learning from\nsupervised sequential signals by applying a companion cross-entropy objective\nfunction to the conditioning vector. The experimental and analytical results\ndemonstrate firstly that competition occurs between the conditioning vector and\nthe LM, and the differing architectures provide different trade-offs between\nthe two. Secondly, the discriminative power and transparency of the\nconditioning vector is key to providing both model interpretability and better\nperformance. Thirdly, snapshot learning leads to consistent performance\nimprovements independent of which architecture is used.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:56:19 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina M.", ""], ["Su", "Pei-Hao", ""], ["Ultes", "Stefan", ""], ["Vandyke", "David", ""], ["Young", "Steve", ""]]}, {"id": "1606.03358", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh", "title": "Extended Gauss-Newton and ADMM-Gauss-Newton Algorithms for Low-Rank\n  Matrix Optimization", "comments": "35 pages, 5 figures and 5 tables. The code can be found at\n  http://www.trandinhquoc.com, UNC-STAT&OR - Tech. Report 2016.a", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a variant of the well-known Gauss-Newton (GN)\nmethod to solve a class of nonconvex optimization problems involving low-rank\nmatrix variables. As opposed to the standard GN method, our algorithm allows\none to handle general smooth convex objective function. We show, under mild\nconditions, that the proposed algorithm globally and locally converges to a\nstationary point of the original problem. We also show empirically that the GN\nalgorithm achieves higher accurate solutions than the alternating minimization\nalgorithm (AMA). Then, we specify our GN scheme to handle the symmetric case\nand prove its convergence, where AMA is not applicable. Next, we incorporate\nour GN scheme into the alternating direction method of multipliers (ADMM) to\ndevelop an ADMM-GN algorithm. We prove that, under mild conditions and a proper\nchoice of the penalty parameter, our ADMM-GN globally converges to a stationary\npoint of the original problem. Finally, we provide several numerical\nexperiments to illustrate the proposed algorithms. Our results show that the\nnew algorithms have encouraging performance compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 15:18:55 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 18:30:06 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 04:34:22 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Tran-Dinh", "Quoc", ""]]}, {"id": "1606.03432", "submitter": "Bryan He", "authors": "Bryan He, Christopher De Sa, Ioannis Mitliagkas, Christopher R\\'e", "title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on\n  How Much", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a Markov Chain Monte Carlo sampling technique that\niteratively samples variables from their conditional distributions. There are\ntwo common scan orders for the variables: random scan and systematic scan. Due\nto the benefits of locality in hardware, systematic scan is commonly used, even\nthough most statistical guarantees are only for random scan. While it has been\nconjectured that the mixing times of random scan and systematic scan do not\ndiffer by more than a logarithmic factor, we show by counterexample that this\nis not the case, and we prove that that the mixing times do not differ by more\nthan a polynomial factor under mild conditions. To prove these relative bounds,\nwe introduce a method of augmenting the state space to study systematic scan\nusing conductance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 19:24:10 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["He", "Bryan", ""], ["De Sa", "Christopher", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.03439", "submitter": "Taesup Kim", "authors": "Taesup Kim, Yoshua Bengio", "title": "Deep Directed Generative Models with Energy-Based Probability Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training energy-based probabilistic models is confronted with apparently\nintractable sums, whose Monte Carlo estimation requires sampling from the\nestimated probability distribution in the inner loop of training. This can be\napproximately achieved by Markov chain Monte Carlo methods, but may still face\na formidable obstacle that is the difficulty of mixing between modes with sharp\nconcentrations of probability. Whereas an MCMC process is usually derived from\na given energy function based on mathematical considerations and requires an\narbitrarily long time to obtain good and varied samples, we propose to train a\ndeep directed generative model (not a Markov chain) so that its sampling\ndistribution approximately matches the energy function that is being trained.\nInspired by generative adversarial networks, the proposed framework involves\ntraining of two models that represent dual views of the estimated probability\ndistribution: the energy function (mapping an input configuration to a scalar\nenergy value) and the generator (mapping a noise vector to a generated\nconfiguration), both represented by deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 19:42:57 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Kim", "Taesup", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1606.03475", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner, Peter Szolovits", "title": "De-identification of Patient Notes with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Patient notes in electronic health records (EHRs) may contain\ncritical information for medical investigations. However, the vast majority of\nmedical investigators can only access de-identified notes, in order to protect\nthe confidentiality of patients. In the United States, the Health Insurance\nPortability and Accountability Act (HIPAA) defines 18 types of protected health\ninformation (PHI) that needs to be removed to de-identify patient notes. Manual\nde-identification is impractical given the size of EHR databases, the limited\nnumber of researchers with access to the non-de-identified notes, and the\nfrequent mistakes of human annotators. A reliable automated de-identification\nsystem would consequently be of high value.\n  Materials and Methods: We introduce the first de-identification system based\non artificial neural networks (ANNs), which requires no handcrafted features or\nrules, unlike existing systems. We compare the performance of the system with\nstate-of-the-art systems on two datasets: the i2b2 2014 de-identification\nchallenge dataset, which is the largest publicly available de-identification\ndataset, and the MIMIC de-identification dataset, which we assembled and is\ntwice as large as the i2b2 2014 dataset.\n  Results: Our ANN model outperforms the state-of-the-art systems. It yields an\nF1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision\nof 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with\na recall 99.25 and a precision of 99.06.\n  Conclusion: Our findings support the use of ANNs for de-identification of\npatient notes, as they show better performance than previously published\nsystems while requiring no feature engineering.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 20:45:30 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Uzuner", "Ozlem", ""], ["Szolovits", "Peter", ""]]}, {"id": "1606.03490", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton", "title": "The Mythos of Model Interpretability", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models boast remarkable predictive capabilities.\nBut can you trust your model? Will it work in deployment? What else can it tell\nyou about the world? We want models to be not only good, but interpretable. And\nyet the task of interpretation appears underspecified. Papers provide diverse\nand sometimes non-overlapping motivations for interpretability, and offer\nmyriad notions of what attributes render models interpretable. Despite this\nambiguity, many papers proclaim interpretability axiomatically, absent further\nexplanation. In this paper, we seek to refine the discourse on\ninterpretability. First, we examine the motivations underlying interest in\ninterpretability, finding them to be diverse and occasionally discordant. Then,\nwe address model properties and techniques thought to confer interpretability,\nidentifying transparency to humans and post-hoc explanations as competing\nnotions. Throughout, we discuss the feasibility and desirability of different\nnotions, and question the oft-made assertions that linear models are\ninterpretable and that deep neural networks are not.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 21:28:47 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:21:04 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 08:51:10 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lipton", "Zachary C.", ""]]}, {"id": "1606.03504", "submitter": "Ming Yuan", "authors": "Ming Yuan and Cun-Hui Zhang", "title": "Incoherent Tensor Norms and Their Applications in Higher Order Tensor\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the sample size requirement for a general class\nof nuclear norm minimization methods for higher order tensor completion. We\nintroduce a class of tensor norms by allowing for different levels of\ncoherence, which allows us to leverage the incoherence of a tensor. In\nparticular, we show that a $k$th order tensor of rank $r$ and dimension\n$d\\times\\cdots\\times d$ can be recovered perfectly from as few as\n$O((r^{(k-1)/2}d^{3/2}+r^{k-1}d)(\\log(d))^2)$ uniformly sampled entries through\nan appropriate incoherent nuclear norm minimization. Our results demonstrate\nsome key differences between completing a matrix and a higher order tensor:\nThey not only point to potential room for improvement over the usual nuclear\nnorm minimization but also highlight the importance of explicitly accounting\nfor incoherence, when dealing with higher order tensors.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 23:59:12 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Yuan", "Ming", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1606.03601", "submitter": "Mohamed Aly", "authors": "Mohamed Aly, Guangming Zang, Wolfgang Heidrich, Peter Wonka", "title": "TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse\n  View X-Ray Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRex, a flexible and robust Tomographic Reconstruction framework\nusing proximal algorithms. We provide an overview and perform an experimental\ncomparison between the famous iterative reconstruction methods in terms of\nreconstruction quality in sparse view situations. We then derive the proximal\noperators for the four best methods. We show the flexibility of our framework\nby deriving solvers for two noise models: Gaussian and Poisson; and by plugging\nin three powerful regularizers. We compare our framework to state of the art\nmethods, and show superior quality on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 14:19:28 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Aly", "Mohamed", ""], ["Zang", "Guangming", ""], ["Heidrich", "Wolfgang", ""], ["Wonka", "Peter", ""]]}, {"id": "1606.03623", "submitter": "Muhammad Ammad-ud-din Mr.", "authors": "Muhammad Ammad-ud-din, Suleiman A.Khan, Disha Malani, Astrid\n  Murum\\\"agi, Olli Kallioniemi, Tero Aittokallio and Samuel Kaski", "title": "Drug response prediction by inferring pathway-response associations with\n  Kernelized Bayesian Matrix Factorization", "comments": "Accepted in European Conference in Computational Biology, to be\n  published in Bioinformatics 2016", "journal-ref": "2016 Bioinformatics Published by Oxford University Press", "doi": "10.1093/bioinformatics/btw433.", "report-no": "32(17):i455-i463", "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of computational personalized medicine is to systematically\nutilize genomic and other molecular features of samples to predict drug\nresponses for a previously unseen sample. Such predictions are valuable for\ndeveloping hypotheses for selecting therapies tailored for individual patients.\nThis is especially valuable in oncology, where molecular and genetic\nheterogeneity of the cells has a major impact on the response. However, the\nprediction task is extremely challenging, raising the need for methods that can\neffectively model and predict drug responses. In this study, we propose a novel\nformulation of multi-task matrix factorization that allows selective data\nintegration for predicting drug responses. To solve the modeling task, we\nextend the state-of-the-art kernelized Bayesian matrix factorization (KBMF)\nmethod with component-wise multiple kernel learning. In addition, our approach\nexploits the known pathway information in a novel and biologically meaningful\nfashion to learn the drug response associations. Our method quantitatively\noutperforms the state of the art on predicting drug responses in two publicly\navailable cancer data sets as well as on a synthetic data set. In addition, we\nvalidated our model predictions with lab experiments using an in-house cancer\ncell line panel. We finally show the practical applicability of the proposed\nmethod by utilizing prior knowledge to infer pathway-drug response\nassociations, opening up the opportunity for elucidating drug action\nmechanisms. We demonstrate that pathway-response associations can be learned by\nthe proposed model for the well known EGFR and MEK inhibitors.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 20:50:53 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ammad-ud-din", "Muhammad", ""], ["Khan", "Suleiman A.", ""], ["Malani", "Disha", ""], ["Murum\u00e4gi", "Astrid", ""], ["Kallioniemi", "Olli", ""], ["Aittokallio", "Tero", ""], ["Kaski", "Samuel", ""]]}, {"id": "1606.03657", "submitter": "Xi Chen", "authors": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,\n  Pieter Abbeel", "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 02:14:31 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chen", "Xi", ""], ["Duan", "Yan", ""], ["Houthooft", "Rein", ""], ["Schulman", "John", ""], ["Sutskever", "Ilya", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1606.03672", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili and Farokh Marvasti", "title": "Comparison of Several Sparse Recovery Methods for Low Rank Matrices with\n  Random Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will investigate the efficacy of IMAT (Iterative Method of\nAdaptive Thresholding) in recovering the sparse signal (parameters) for linear\nmodels with missing data. Sparse recovery rises in compressed sensing and\nmachine learning problems and has various applications necessitating viable\nreconstruction methods specifically when we work with big data. This paper will\nfocus on comparing the power of IMAT in reconstruction of the desired sparse\nsignal with LASSO. Additionally, we will assume the model has random missing\ninformation. Missing data has been recently of interest in big data and machine\nlearning problems since they appear in many cases including but not limited to\nmedical imaging datasets, hospital datasets, and massive MIMO. The dominance of\nIMAT over the well-known LASSO will be taken into account in different\nscenarios. Simulations and numerical results are also provided to verify the\narguments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 07:05:22 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1606.03685", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis and Spyridon Pougkakiotis, Sergios Theodoridis", "title": "Efficient KLMS and KRLS Algorithms: A Random Fourier Feature Perspective", "comments": "presented in the 2016 IEEE Workshop on Statistical Signal Processing\n  (SSP 16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for online Least Squares algorithms for nonlinear\nmodeling in RKH spaces (RKHS). Instead of implicitly mapping the data to a RKHS\n(e.g., kernel trick), we map the data to a finite dimensional Euclidean space,\nusing random features of the kernel's Fourier transform. The advantage is that,\nthe inner product of the mapped data approximates the kernel function. The\nresulting \"linear\" algorithm does not require any form of sparsification,\nsince, in contrast to all existing algorithms, the solution's size remains\nfixed and does not increase with the iteration steps. As a result, the obtained\nalgorithms are computationally significantly more efficient compared to\npreviously derived variants, while, at the same time, they converge at similar\nspeeds and to similar error floors.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 08:59:45 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Pougkakiotis", "Spyridon", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1606.03802", "submitter": "Pedro Ribeiro Mendes J\\'unior", "authors": "Pedro Ribeiro Mendes J\\'unior, Terrance E. Boult, Jacques Wainer, and\n  Anderson Rocha", "title": "Specialized Support Vector Machines for Open-set Recognition", "comments": "Some additional information were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Often, when dealing with real-world recognition problems, we do not need, and\noften cannot have, knowledge of the entire set of possible classes that might\nappear during operational testing. In such cases, we need to think of robust\nclassification methods able to deal with the \"unknown\" and properly reject\nsamples belonging to classes never seen during training. Notwithstanding,\nalmost all existing classifiers to date were mostly developed for the\nclosed-set scenario, i.e., the classification setup in which it is assumed that\nall test samples belong to one of the classes with which the classifier was\ntrained. In the open-set scenario, however, a test sample can belong to none of\nthe known classes and the classifier must properly reject it by classifying it\nas unknown. In this work, we extend upon the well-known Support Vector Machines\n(SVM) classifier and introduce the Specialized Support Vector Machines (SSVM),\nwhich is suitable for recognition in open-set setups. SSVM balances the\nempirical risk and the risk of the unknown and ensures that the region of the\nfeature space in which a test sample would be classified as known (one of the\nknown classes) is always bounded, ensuring a finite risk of the unknown. In\nthis work, we also highlight the properties of the SVM classifier related to\nthe open-set scenario, and provide necessary and sufficient conditions for an\nRBF SVM to have bounded open-space risk.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 03:46:17 GMT"}, {"version": "v10", "created": "Tue, 21 Apr 2020 22:45:04 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 22:54:27 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 16:30:28 GMT"}, {"version": "v4", "created": "Thu, 1 Mar 2018 22:47:18 GMT"}, {"version": "v5", "created": "Tue, 26 Jun 2018 17:33:55 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2018 13:30:56 GMT"}, {"version": "v7", "created": "Wed, 14 Nov 2018 15:09:14 GMT"}, {"version": "v8", "created": "Thu, 2 May 2019 13:20:51 GMT"}, {"version": "v9", "created": "Wed, 13 Nov 2019 18:44:31 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["J\u00fanior", "Pedro Ribeiro Mendes", ""], ["Boult", "Terrance E.", ""], ["Wainer", "Jacques", ""], ["Rocha", "Anderson", ""]]}, {"id": "1606.03803", "submitter": "Zhao Ren", "authors": "Zhao Ren, Yongjian Kang, Yingying Fan, Jinchi Lv", "title": "Tuning-Free Heterogeneity Pursuit in Massive Networks", "comments": "29 pages for the main text including 1 figure and 7 tables, 28 pages\n  for the Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is often natural in many contemporary applications involving\nmassive data. While posing new challenges to effective learning, it can play a\ncrucial role in powering meaningful scientific discoveries through the\nunderstanding of important differences among subpopulations of interest. In\nthis paper, we exploit multiple networks with Gaussian graphs to encode the\nconnectivity patterns of a large number of features on the subpopulations. To\nuncover the heterogeneity of these structures across subpopulations, we suggest\na new framework of tuning-free heterogeneity pursuit (THP) via large-scale\ninference, where the number of networks is allowed to diverge. In particular,\ntwo new tests, the chi-based test and the linear functional-based test, are\nintroduced and their asymptotic null distributions are established. Under mild\nregularity conditions, we establish that both tests are optimal in achieving\nthe testable region boundary and the sample size requirement for the latter\ntest is minimal. Both theoretical guarantees and the tuning-free feature stem\nfrom efficient multiple-network estimation by our newly suggested approach of\nheterogeneous group square-root Lasso (HGSL) for high-dimensional\nmulti-response regression with heterogeneous noises. To solve this convex\nprogram, we further introduce a tuning-free algorithm that is scalable and\nenjoys provable convergence to the global optimum. Both computational and\ntheoretical advantages of our procedure are elucidated through simulation and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 03:58:23 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Ren", "Zhao", ""], ["Kang", "Yongjian", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1606.03841", "submitter": "Quanming Yao", "authors": "Quanming Yao and James.T Kwok", "title": "Efficient Learning with a Family of Nonconvex Regularizers by\n  Redistributing Nonconvexity", "comments": "Journal version of previous conference paper appeared at ICML-2016\n  with same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of convex regularizers allows for easy optimization, though they\noften produce biased estimation and inferior prediction performance. Recently,\nnonconvex regularizers have attracted a lot of attention and outperformed\nconvex ones. However, the resultant optimization problem is much harder. In\nthis paper, for a large class of nonconvex regularizers, we propose to move the\nnonconvexity from the regularizer to the loss. The nonconvex regularizer is\nthen transformed to a familiar convex regularizer, while the resultant loss\nfunction can still be guaranteed to be smooth. Learning with the convexified\nregularizer can be performed by existing efficient algorithms originally\ndesigned for convex regularizers (such as the proximal algorithm, Frank-Wolfe\nalgorithm, alternating direction method of multipliers and stochastic gradient\ndescent). Extensions are made when the convexified regularizer does not have\nclosed-form proximal step, and when the loss function is nonconvex, nonsmooth.\nExtensive experiments on a variety of machine learning application scenarios\nshow that optimizing the transformed problem is much faster than running the\nstate-of-the-art on the original problem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 07:21:31 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 05:57:58 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 01:27:29 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James. T", ""]]}, {"id": "1606.03860", "submitter": "Yixin Wang", "authors": "Yixin Wang, Alp Kucukelbir, David M. Blei", "title": "Robust Probabilistic Modeling with Bayesian Data Reweighting", "comments": "In ICML 2017. Updated related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models analyze data by relying on a set of assumptions. Data\nthat exhibit deviations from these assumptions can undermine inference and\nprediction quality. Robust models offer protection against mismatch between a\nmodel's assumptions and reality. We propose a way to systematically detect and\nmitigate mismatch of a large class of probabilistic models. The idea is to\nraise the likelihood of each observation to a weight and then to infer both the\nlatent variables and the weights from data. Inferring the weights allows a\nmodel to identify observations that match its assumptions and down-weight\nothers. This enables robust inference and improves predictive accuracy. We\nstudy four different forms of mismatch with reality, ranging from missing\nlatent groups to structure misspecification. A Poisson factorization analysis\nof the Movielens 1M dataset shows the benefits of this approach in a practical\nscenario.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 08:56:35 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 21:13:37 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 16:44:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Wang", "Yixin", ""], ["Kucukelbir", "Alp", ""], ["Blei", "David M.", ""]]}, {"id": "1606.03865", "submitter": "Johan W{\\aa}gberg", "authors": "Johan W{\\aa}gberg, Dave Zachariah, Thomas B. Sch\\\"on, Petre Stoica", "title": "Prediction performance after learning in Gaussian process regression", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the quantification of the prediction performance in\nGaussian process regression. The standard approach is to base the prediction\nerror bars on the theoretical predictive variance, which is a lower bound on\nthe mean square-error (MSE). This approach, however, does not take into account\nthat the statistical model is learned from the data. We show that this omission\nleads to a systematic underestimation of the prediction errors. Starting from a\ngeneralization of the Cram\\'er-Rao bound, we derive a more accurate MSE bound\nwhich provides a measure of uncertainty for prediction of Gaussian processes.\nThe improved bound is easily computed and we illustrate it using synthetic and\nreal data examples. of uncertainty for prediction of Gaussian processes and\nillustrate it using synthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 09:16:25 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 11:20:29 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 08:34:22 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["W\u00e5gberg", "Johan", ""], ["Zachariah", "Dave", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Stoica", "Petre", ""]]}, {"id": "1606.03956", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Andre Manoel and Francesco Caltagirone and Marylou\n  Gabri\\'e and Florent Krzakala", "title": "Inferring Sparsity: Compressed Sensing using Generalized Restricted\n  Boltzmann Machines", "comments": "IEEE Information Theory Workshop, 2016", "journal-ref": "2016 IEEE Information Theory Workshop (ITW), Pages: 265 - 269", "doi": "10.1109/ITW.2016.7606837", "report-no": null, "categories": "cs.IT cond-mat.dis-nn cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider compressed sensing reconstruction from $M$\nmeasurements of $K$-sparse structured signals which do not possess a writable\ncorrelation model. Assuming that a generative statistical model, such as a\nBoltzmann machine, can be trained in an unsupervised manner on example signals,\nwe demonstrate how this signal model can be used within a Bayesian framework of\nsignal reconstruction. By deriving a message-passing inference for general\ndistribution restricted Boltzmann machines, we are able to integrate these\ninferred signal models into approximate message passing for compressed sensing\nreconstruction. Finally, we show for the MNIST dataset that this approach can\nbe very effective, even for $M < K$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:03:50 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Tramel", "Eric W.", ""], ["Manoel", "Andre", ""], ["Caltagirone", "Francesco", ""], ["Gabri\u00e9", "Marylou", ""], ["Krzakala", "Florent", ""]]}, {"id": "1606.03976", "submitter": "Fredrik D. Johansson", "authors": "Uri Shalit, Fredrik D. Johansson, David Sontag", "title": "Estimating individual treatment effect: generalization bounds and\n  algorithms", "comments": "Added name \"TARNet\" to refer to version with alpha = 0. Removed supp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is intense interest in applying machine learning to problems of causal\ninference in fields such as healthcare, economics and education. In particular,\nindividual-level causal inference has important applications such as precision\nmedicine. We give a new theoretical analysis and family of algorithms for\npredicting individual treatment effect (ITE) from observational data, under the\nassumption known as strong ignorability. The algorithms learn a \"balanced\"\nrepresentation such that the induced treated and control distributions look\nsimilar. We give a novel, simple and intuitive generalization-error bound\nshowing that the expected ITE estimation error of a representation is bounded\nby a sum of the standard generalization-error of that representation and the\ndistance between the treated and control distributions induced by the\nrepresentation. We use Integral Probability Metrics to measure distances\nbetween distributions, deriving explicit bounds for the Wasserstein and Maximum\nMean Discrepancy (MMD) distances. Experiments on real and simulated data show\nthe new algorithms match or outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:40:57 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 13:13:05 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 18:17:38 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 15:44:15 GMT"}, {"version": "v5", "created": "Tue, 16 May 2017 15:11:15 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Shalit", "Uri", ""], ["Johansson", "Fredrik D.", ""], ["Sontag", "David", ""]]}, {"id": "1606.04052", "submitter": "Julien Perez", "authors": "Julien Perez and Fei Liu", "title": "Dialog state tracking, a machine reading approach using Memory Network", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an end-to-end dialog system, the aim of dialog state tracking is to\naccurately estimate a compact representation of the current dialog status from\na sequence of noisy observations produced by the speech recognition and the\nnatural language understanding modules. This paper introduces a novel method of\ndialog state tracking based on the general paradigm of machine reading and\nproposes to solve it using an End-to-End Memory Network, MemN2N, a\nmemory-enhanced neural network architecture. We evaluate the proposed approach\non the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has\nbeen converted for the occasion in order to frame the hidden state variable\ninference as a question-answering task based on a sequence of utterances\nextracted from a dialog. We show that the proposed tracker gives encouraging\nresults. Then, we propose to extend the DSTC-2 dataset with specific reasoning\ncapabilities requirement like counting, list maintenance, yes-no question\nanswering and indefinite knowledge management. Finally, we present encouraging\nresults using our proposed MemN2N based tracking model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 18:09:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 06:42:04 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 00:07:41 GMT"}, {"version": "v4", "created": "Thu, 13 Oct 2016 19:23:00 GMT"}, {"version": "v5", "created": "Thu, 2 Mar 2017 20:17:23 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Perez", "Julien", ""], ["Liu", "Fei", ""]]}, {"id": "1606.04080", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals and Charles Blundell and Timothy Lillicrap and Koray\n  Kavukcuoglu and Daan Wierstra", "title": "Matching Networks for One Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:34:22 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 17:45:19 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Vinyals", "Oriol", ""], ["Blundell", "Charles", ""], ["Lillicrap", "Timothy", ""], ["Kavukcuoglu", "Koray", ""], ["Wierstra", "Daan", ""]]}, {"id": "1606.04130", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, David C. Kale, Randall Wetzel", "title": "Modeling Missing Data in Clinical Time Series with RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a simple strategy to cope with missing data in sequential\ninputs, addressing the task of multilabel classification of diagnoses given\nclinical time series. Collected from the pediatric intensive care unit (PICU)\nat Children's Hospital Los Angeles, our data consists of multivariate time\nseries of observations. The measurements are irregularly spaced, leading to\nmissingness patterns in temporally discretized sequences. While these artifacts\nare typically handled by imputation, we achieve superior predictive performance\nby treating the artifacts as features. Unlike linear models, recurrent neural\nnetworks can realize this improvement using only simple binary indicators of\nmissingness. For linear models, we show an alternative strategy to capture this\nsignal. Training models on missingness patterns only, we show that for some\ndiseases, what tests are run can be as predictive as the results themselves.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 20:34:35 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 09:04:23 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 08:51:35 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 01:10:14 GMT"}, {"version": "v5", "created": "Fri, 11 Nov 2016 12:46:53 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Kale", "David C.", ""], ["Wetzel", "Randall", ""]]}, {"id": "1606.04160", "submitter": "Richard Nock", "authors": "Richard Nock, Giorgio Patrini, Finnian Lattimore, Tiberio Caetano", "title": "The Crossover Process: Learnability and Data Protection from Inference\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is usual to consider data protection and learnability as conflicting\nobjectives. This is not always the case: we show how to jointly control\ninference --- seen as the attack --- and learnability by a noise-free process\nthat mixes training examples, the Crossover Process (cp). One key point is that\nthe cp~is typically able to alter joint distributions without touching on\nmarginals, nor altering the sufficient statistic for the class. In other words,\nit saves (and sometimes improves) generalization for supervised learning, but\ncan alter the relationship between covariates --- and therefore fool measures\nof nonlinear independence and causal inference into misleading ad-hoc\nconclusions. For example, a cp~can increase / decrease odds ratios, bring\nfairness or break fairness, tamper with disparate impact, strengthen, weaken or\nreverse causal directions, change observed statistical measures of dependence.\nFor each of these, we quantify changes brought by a cp, as well as its\nstatistical impact on generalization abilities via a new complexity measure\nthat we call the Rademacher cp~complexity. Experiments on a dozen readily\navailable domains validate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:27:36 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 21:41:50 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Nock", "Richard", ""], ["Patrini", "Giorgio", ""], ["Lattimore", "Finnian", ""], ["Caetano", "Tiberio", ""]]}, {"id": "1606.04166", "submitter": "Heinrich Jiang", "authors": "Heinrich Jiang, Samory Kpotufe", "title": "Modal-set estimation with an application to clustering", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, in PMLR 54:1197-1206, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first procedure that can estimate -- with statistical\nconsistency guarantees -- any local-maxima of a density, under benign\ndistributional conditions. The procedure estimates all such local maxima, or\n$\\textit{modal-sets}$, of any bounded shape or dimension, including usual\npoint-modes. In practice, modal-sets can arise as dense low-dimensional\nstructures in noisy data, and more generally serve to better model the rich\nvariety of locally-high-density structures in data.\n  The procedure is then shown to be competitive on clustering applications, and\nmoreover is quite stable to a wide range of settings of its tuning parameter.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:44:39 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Jiang", "Heinrich", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1606.04268", "submitter": "Or Yair", "authors": "Or Yair, Ronen Talmon", "title": "Local Canonical Correlation Analysis for Nonlinear Common Variables\n  Discovery", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2628348", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of hidden common variables discovery\nfrom multimodal data sets of nonlinear high-dimensional observations. We\npresent a metric based on local applications of canonical correlation analysis\n(CCA) and incorporate it in a kernel-based manifold learning technique.We show\nthat this metric discovers the hidden common variables underlying the\nmultimodal observations by estimating the Euclidean distance between them. Our\napproach can be viewed both as an extension of CCA to a nonlinear setting as\nwell as an extension of manifold learning to multiple data sets. Experimental\nresults show that our method indeed discovers the common variables underlying\nhigh-dimensional nonlinear observations without assuming prior rigid model\nassumptions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:18:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Yair", "Or", ""], ["Talmon", "Ronen", ""]]}, {"id": "1606.04316", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli, Giorgio Corani, Janez Demsar, Marco Zaffalon", "title": "Time for a change: a tutorial for comparing multiple classifiers through\n  Bayesian analysis", "comments": "This paper has been published in the Journal of Machine Learning\n  Research (JMLR) vol.18, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community adopted the use of null hypothesis\nsignificance testing (NHST) in order to ensure the statistical validity of\nresults. Many scientific fields however realized the shortcomings of\nfrequentist reasoning and in the most radical cases even banned its use in\npublications. We should do the same: just as we have embraced the Bayesian\nparadigm in the development of new machine learning methods, so we should also\nuse it in the analysis of our own results. We argue for abandonment of NHST by\nexposing its fallacies and, more importantly, offer better - more sound and\nuseful - alternatives for it.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 11:35:35 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 11:16:35 GMT"}, {"version": "v3", "created": "Sat, 15 Jul 2017 15:16:48 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Benavoli", "Alessio", ""], ["Corani", "Giorgio", ""], ["Demsar", "Janez", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1606.04317", "submitter": "David van Leeuwen", "authors": "David A. van Leeuwen and Joost van Doremalen", "title": "Calibration of Phone Likelihoods in Automatic Speech Recognition", "comments": "Rejected by Interspeech 2016. I would love to include the reviews,\n  but there is no space for that here (400 characters)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the probabilistic properties of the posteriors in a\nspeech recognition system that uses a deep neural network (DNN) for acoustic\nmodeling. We do this by reducing Kaldi's DNN shared pdf-id posteriors to phone\nlikelihoods, and using test set forced alignments to evaluate these using a\ncalibration sensitive metric. Individual frame posteriors are in principle\nwell-calibrated, because the DNN is trained using cross entropy as the\nobjective function, which is a proper scoring rule. When entire phones are\nassessed, we observe that it is best to average the log likelihoods over the\nduration of the phone. Further scaling of the average log likelihoods by the\nlogarithm of the duration slightly improves the calibration, and this\nimprovement is retained when tested on independent test data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 11:44:31 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["van Leeuwen", "David A.", ""], ["van Doremalen", "Joost", ""]]}, {"id": "1606.04335", "submitter": "Maria Kalantzi", "authors": "Maria Kalantzi", "title": "LLFR: A Lanczos-Based Latent Factor Recommender for Big Data Scenarios", "comments": "65 pages, MSc Thesis (in Greek)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose if this master's thesis is to study and develop a new algorithmic\nframework for Collaborative Filtering to produce recommendations in the top-N\nrecommendation problem. Thus, we propose Lanczos Latent Factor Recommender\n(LLFR); a novel \"big data friendly\" collaborative filtering algorithm for top-N\nrecommendation. Using a computationally efficient Lanczos-based procedure, LLFR\nbuilds a low dimensional item similarity model, that can be readily exploited\nto produce personalized ranking vectors over the item space. A number of\nexperiments on real datasets indicate that LLFR outperforms other\nstate-of-the-art top-N recommendation methods from a computational as well as a\nqualitative perspective. Our experimental results also show that its relative\nperformance gains, compared to competing methods, increase as the data get\nsparser, as in the Cold Start Problem. More specifically, this is true both\nwhen the sparsity is generalized - as in the New Community Problem, a very\ncommon problem faced by real recommender systems in their beginning stages,\nwhen there is not sufficient number of ratings for the collaborative filtering\nalgorithms to uncover similarities between items or users - and in the very\ninteresting case where the sparsity is localized in a small fraction of the\ndataset - as in the New Users Problem, where new users are introduced to the\nsystem, they have not rated many items and thus, the CF algorithm can not make\nreliable personalized recommendations yet.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:04:57 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Kalantzi", "Maria", ""]]}, {"id": "1606.04366", "submitter": "Per Mattsson", "authors": "Per Mattsson, Dave Zachariah and Petre Stoica", "title": "Recursive nonlinear-system identification using latent variables", "comments": "10 pages, 4 figures", "journal-ref": "P. Mattsson, D. Zachariah, P. Stoica, Recursive nonlinear-system\n  identification using latent variables, Automatica, Volume 93, Pages 343-351,\n  2018", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a method for learning nonlinear systems with\nmultiple outputs and inputs. We begin by modelling the errors of a nominal\npredictor of the system using a latent variable framework. Then using the\nmaximum likelihood principle we derive a criterion for learning the model. The\nresulting optimization problem is tackled using a majorization-minimization\napproach. Finally, we develop a convex majorization technique and show that it\nenables a recursive identification method. The method learns parsimonious\npredictive models and is tested on both synthetic and real nonlinear systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:46:21 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 15:38:50 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 12:03:34 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Mattsson", "Per", ""], ["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "1606.04393", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Akshaya Mishra, and Alexander Wong", "title": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural\n  Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking inspiration from biological evolution, we explore the idea of \"Can\ndeep neural networks evolve naturally over successive generations into highly\nefficient deep neural networks?\" by introducing the notion of synthesizing new\nhighly efficient, yet powerful deep neural networks over successive generations\nvia an evolutionary process from ancestor deep neural networks. The\narchitectural traits of ancestor deep neural networks are encoded using\nsynaptic probability models, which can be viewed as the `DNA' of these\nnetworks. New descendant networks with differing network architectures are\nsynthesized based on these synaptic probability models from the ancestor\nnetworks and computational environmental factor models, in a random manner to\nmimic heredity, natural selection, and random mutation. These offspring\nnetworks are then trained into fully functional networks, like one would train\na newborn, and have more efficient, more diverse network architectures than\ntheir ancestor networks, while achieving powerful modeling capabilities.\nExperimental results for the task of visual saliency demonstrated that the\nsynthesized `evolved' offspring networks can achieve state-of-the-art\nperformance while having network architectures that are significantly more\nefficient (with a staggering $\\sim$48-fold decrease in synapses by the fourth\ngeneration) compared to the original ancestor network.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:36:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 16:15:40 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 22:51:33 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Mishra", "Akshaya", ""], ["Wong", "Alexander", ""]]}, {"id": "1606.04414", "submitter": "Jian Wu", "authors": "Jian Wu, Peter I. Frazier", "title": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization", "comments": "Minor edits and typo fixes. Please cite \"J. Wu and P. Frazier. The\n  parallel knowledge gradient method for batch bayesian optimization. In\n  Advances In Neural Information Processing Systems, pp. 3126-3134. 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of black-box optimization, one can evaluate multiple\npoints simultaneously, e.g. when evaluating the performances of several\ndifferent neural network architectures in a parallel computing environment. In\nthis paper, we develop a novel batch Bayesian optimization algorithm --- the\nparallel knowledge gradient method. By construction, this method provides the\none-step Bayes-optimal batch of points to sample. We provide an efficient\nstrategy for computing this Bayes-optimal batch of points, and we demonstrate\nthat the parallel knowledge gradient method finds global optima significantly\nfaster than previous batch Bayesian optimization algorithms on both synthetic\ntest functions and when tuning hyperparameters of practical machine learning\nalgorithms, especially when function evaluations are noisy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:12:01 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 05:47:09 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 20:49:18 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2018 23:44:15 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wu", "Jian", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1606.04443", "submitter": "Steve Li", "authors": "Steven Cheng-Xian Li, Benjamin Marlin", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled\n  time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for classification of sparse and\nirregularly-sampled time series. The properties of such time series can result\nin substantial uncertainty about the values of the underlying temporal\nprocesses, while making the data difficult to deal with using standard\nclassification methods that assume fixed-dimensional feature spaces. To address\nthese challenges, we propose an uncertainty-aware classification framework\nbased on a special computational layer we refer to as the Gaussian process\nadapter that can connect irregularly sampled time series data to any black-box\nclassifier learnable using gradient descent. We show how to scale up the\nrequired computations based on combining the structured kernel interpolation\nframework and the Lanczos approximation method, and how to discriminatively\ntrain the Gaussian process adapter in combination with a number of classifiers\nend-to-end using backpropagation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:31:14 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 20:05:02 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Steven Cheng-Xian", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1606.04449", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "Recurrent neural network training with preconditioned stochastic\n  gradient descent", "comments": "Supplemental materials including Matlab code are put at\n  https://sites.google.com/site/lixilinx/home/psgd", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the performance of a recently proposed preconditioned\nstochastic gradient descent (PSGD) algorithm on recurrent neural network (RNN)\ntraining. PSGD adaptively estimates a preconditioner to accelerate gradient\ndescent, and is designed to be simple, general and easy to use, as stochastic\ngradient descent (SGD). RNNs, especially the ones requiring extremely long term\nmemories, are difficult to train. We have tested PSGD on a set of synthetic\npathological RNN learning problems and the real world MNIST handwritten digit\nrecognition task. Experimental results suggest that PSGD is able to achieve\nhighly competitive performance without using any trick like preprocessing,\npretraining or parameter tweaking.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:40:38 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 18:56:38 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1606.04460", "submitter": "Charles Blundell", "authors": "Charles Blundell and Benigno Uria and Alexander Pritzel and Yazhe Li\n  and Avraham Ruderman and Joel Z Leibo and Jack Rae and Daan Wierstra and\n  Demis Hassabis", "title": "Model-Free Episodic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art deep reinforcement learning algorithms take many millions of\ninteractions to attain human-level performance. Humans, on the other hand, can\nvery quickly exploit highly rewarding nuances of an environment upon first\ndiscovery. In the brain, such rapid learning is thought to depend on the\nhippocampus and its capacity for episodic memory. Here we investigate whether a\nsimple model of hippocampal episodic control can learn to solve difficult\nsequential decision-making tasks. We demonstrate that it not only attains a\nhighly rewarding strategy significantly faster than state-of-the-art deep\nreinforcement learning algorithms, but also achieves a higher overall reward on\nsome of the more challenging domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:03:46 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Blundell", "Charles", ""], ["Uria", "Benigno", ""], ["Pritzel", "Alexander", ""], ["Li", "Yazhe", ""], ["Ruderman", "Avraham", ""], ["Leibo", "Joel Z", ""], ["Rae", "Jack", ""], ["Wierstra", "Daan", ""], ["Hassabis", "Demis", ""]]}, {"id": "1606.04464", "submitter": "Maruti Mudunuru", "authors": "M. K. Mudunuru, S. Karra, N. Makedonska, T. Chen", "title": "Sequential geophysical and flow inversion to characterize fracture\n  networks in subsurface systems", "comments": "32 pages, 14 figures", "journal-ref": null, "doi": "10.1002/sam.11356", "report-no": null, "categories": "cs.CE math.NA physics.comp-ph physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsurface applications including geothermal, geological carbon\nsequestration, oil and gas, etc., typically involve maximizing either the\nextraction of energy or the storage of fluids. Characterizing the subsurface is\nextremely complex due to heterogeneity and anisotropy. Due to this complexity,\nthere are uncertainties in the subsurface parameters, which need to be\nestimated from multiple diverse as well as fragmented data streams. In this\npaper, we present a non-intrusive sequential inversion framework, for\nintegrating data from geophysical and flow sources to constraint subsurface\nDiscrete Fracture Networks (DFN). In this approach, we first estimate bounds on\nthe statistics for the DFN fracture orientations using microseismic data. These\nbounds are estimated through a combination of a focal mechanism (physics-based\napproach) and clustering analysis (statistical approach) of seismic data. Then,\nthe fracture lengths are constrained based on the flow data. The efficacy of\nthis multi-physics based sequential inversion is demonstrated through a\nrepresentative synthetic example.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:18:06 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 16:05:06 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 01:04:40 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Mudunuru", "M. K.", ""], ["Karra", "S.", ""], ["Makedonska", "N.", ""], ["Chen", "T.", ""]]}, {"id": "1606.04478", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Alexander Vandenberg-Rodes, Babak Shahbaba", "title": "Bayesian Inference on Matrix Manifolds for Linear Dimensionality\n  Reduction", "comments": "All datasets and computer programs are publicly available at\n  http://www.ics.uci.edu/~babaks/Site/Codes.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reframe linear dimensionality reduction as a problem of Bayesian inference\non matrix manifolds. This natural paradigm extends the Bayesian framework to\ndimensionality reduction tasks in higher dimensions with simpler models at\ngreater speeds. Here an orthogonal basis is treated as a single point on a\nmanifold and is associated with a linear subspace on which observations vary\nmaximally. Throughout this paper, we employ the Grassmann and Stiefel manifolds\nfor various dimensionality reduction problems, explore the connection between\nthe two manifolds, and use Hybrid Monte Carlo for posterior sampling on the\nGrassmannian for the first time. We delineate in which situations either\nmanifold should be considered. Further, matrix manifold models are used to\nyield scientific insight in the context of cognitive neuroscience, and we\nconclude that our methods are suitable for basic inference as well as accurate\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:58:49 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Holbrook", "Andrew", ""], ["Vandenberg-Rodes", "Alexander", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1606.04567", "submitter": "Maruti Mudunuru", "authors": "M. K. Mudunuru, S. Karra, D. R. Harp, G. D. Guthrie, H. S. Viswanathan", "title": "Regression-based reduced-order models to predict transient thermal\n  output for enhanced geothermal systems", "comments": "25 pages, 8 figures", "journal-ref": "M.K. Mudunuru, S. Karra, D.R. Harp, G.D. Guthrie, H.S.\n  Viswanathan, Regression-based reduced-order models to predict transient\n  thermal output for enhanced geothermal systems, Geothermics, Volume 70, 2017,\n  Pages 192-205", "doi": "10.1016/j.geothermics.2017.06.013", "report-no": null, "categories": "cs.CE math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to assess the utility of Reduced-Order Models\n(ROMs) developed from 3D physics-based models for predicting transient thermal\npower output for an enhanced geothermal reservoir while explicitly accounting\nfor uncertainties in the subsurface system and site-specific details. Numerical\nsimulations are performed based on Latin Hypercube Sampling (LHS) of model\ninputs drawn from uniform probability distributions. Key sensitive parameters\nare identified from these simulations, which are fracture zone permeability,\nwell/skin factor, bottom hole pressure, and injection flow rate. The inputs for\nROMs are based on these key sensitive parameters. The ROMs are then used to\nevaluate the influence of subsurface attributes on thermal power production\ncurves. The resulting ROMs are compared with field-data and the detailed\nphysics-based numerical simulations. We propose three different ROMs with\ndifferent levels of model parsimony, each describing key and essential features\nof the power production curves. ROM-1 is able to accurately reproduce the power\noutput of numerical simulations for low values of permeabilities and certain\nfeatures of the field-scale data, and is relatively parsimonious. ROM-2 is a\nmore complex model than ROM-1 but it accurately describes the field-data. At\nhigher permeabilities, ROM-2 reproduces numerical results better than ROM-1,\nhowever, there is a considerable deviation at low fracture zone permeabilities.\nROM-3 is developed by taking the best aspects of ROM-1 and ROM-2 and provides a\nmiddle ground for model parsimony. It is able to describe various features of\nnumerical simulations and field-data. From the proposed workflow, we\ndemonstrate that the proposed simple ROMs are able to capture various complex\nfeatures of the power production curves of Fenton Hill HDR system. For typical\nEGS applications, ROM-2 and ROM-3 outperform ROM-1.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 21:05:16 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 05:22:16 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 19:05:32 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Mudunuru", "M. K.", ""], ["Karra", "S.", ""], ["Harp", "D. R.", ""], ["Guthrie", "G. D.", ""], ["Viswanathan", "H. S.", ""]]}, {"id": "1606.04618", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Marco F. Duarte", "title": "Masking Strategies for Image Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting an optimal mask for an image manifold,\ni.e., choosing a subset of the pixels of the image that preserves the\nmanifold's geometric structure present in the original data. Such masking\nimplements a form of compressive sensing through emerging imaging sensor\nplatforms for which the power expense grows with the number of pixels acquired.\nOur goal is for the manifold learned from masked images to resemble its full\nimage counterpart as closely as possible. More precisely, we show that one can\nindeed accurately learn an image manifold without having to consider a large\nmajority of the image pixels. In doing so, we consider two masking methods that\npreserve the local and global geometric structure of the manifold,\nrespectively. In each case, the process of finding the optimal masking pattern\ncan be cast as a binary integer program, which is computationally expensive but\ncan be approximated by a fast greedy algorithm. Numerical experiments show that\nthe relevant manifold structure is preserved through the data-dependent masking\nprocess, even for modest mask sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 02:03:05 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1606.04722", "submitter": "Xi Wu", "authors": "Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, Jeffrey\n  F. Naughton", "title": "Bolt-on Differential Privacy for Scalable Stochastic Gradient\n  Descent-based Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made separately on analytics systems for\nscalable stochastic gradient descent (SGD) and private SGD, none of the major\nscalable analytics frameworks have incorporated differentially private SGD.\nThere are two inter-related issues for this disconnect between research and\npractice: (1) low model accuracy due to added noise to guarantee privacy, and\n(2) high development and runtime overhead of the private algorithms. This paper\ntakes a first step to remedy this disconnect and proposes a private SGD\nalgorithm to address \\emph{both} issues in an integrated manner. In contrast to\nthe white-box approach adopted by previous work, we revisit and use the\nclassical technique of {\\em output perturbation} to devise a novel \"bolt-on\"\napproach to private SGD. While our approach trivially addresses (2), it makes\n(1) even more challenging. We address this challenge by providing a novel\nanalysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy\nguarantees, better convergence of SGD when only a constant number of passes can\nbe made over the data. We integrate our algorithm, as well as other\nstate-of-the-art differentially private SGD, into Bismarck, a popular scalable\nSGD-based analytics system on top of an RDBMS. Extensive experiments show that\nour algorithm can be easily integrated, incurs virtually no overhead, scales\nwell, and most importantly, yields substantially better (up to 4X) test\naccuracy than the state-of-the-art algorithms on many real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 11:14:29 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 16:26:59 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 17:35:09 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Wu", "Xi", ""], ["Li", "Fengan", ""], ["Kumar", "Arun", ""], ["Chaudhuri", "Kamalika", ""], ["Jha", "Somesh", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1606.04753", "submitter": "Matteo Turchetta", "authors": "Matteo Turchetta, Felix Berkenkamp, Andreas Krause", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian\n  Processes", "comments": "15 pages, extended version with proofs", "journal-ref": "Proc. of Advances in Neural Information Processing Systems (NIPS),\n  2016, pp. 4305-4313", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical reinforcement learning, when exploring an environment, agents\naccept arbitrary short term loss for long term gain. This is infeasible for\nsafety critical applications, such as robotics, where even a single unsafe\naction may cause system failure. In this paper, we address the problem of\nsafely exploring finite Markov decision processes (MDP). We define safety in\nterms of an, a priori unknown, safety constraint that depends on states and\nactions. We aim to explore the MDP under this constraint, assuming that the\nunknown function satisfies regularity conditions expressed via a Gaussian\nprocess prior. We develop a novel algorithm for this task and prove that it is\nable to completely explore the safely reachable part of the MDP without\nviolating the safety constraint. To achieve this, it cautiously explores safe\nstates and actions in order to gain statistical confidence about the safety of\nunvisited state-action pairs from noisy observations collected while navigating\nthe environment. Moreover, the algorithm explicitly considers reachability when\nexploring the MDP, ensuring that it does not get stuck in any state with no\nsafe way out. We demonstrate our method on digital terrain models for the task\nof exploring an unknown map with a rover.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:18:30 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 14:00:11 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Turchetta", "Matteo", ""], ["Berkenkamp", "Felix", ""], ["Krause", "Andreas", ""]]}, {"id": "1606.04789", "submitter": "Soheil Feizi", "authors": "Soheil Feizi, Ali Makhdoumi, Ken Duffy, Muriel Medard and Manolis\n  Kellis", "title": "Network Maximal Correlation", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Network Maximal Correlation (NMC) as a multivariate measure of\nnonlinear association among random variables. NMC is defined via an\noptimization that infers transformations of variables by maximizing aggregate\ninner products between transformed variables. For finite discrete and jointly\nGaussian random variables, we characterize a solution of the NMC optimization\nusing basis expansion of functions over appropriate basis functions. For finite\ndiscrete variables, we propose an algorithm based on alternating conditional\nexpectation to determine NMC. Moreover we propose a distributed algorithm to\ncompute an approximation of NMC for large and dense graphs using graph\npartitioning. For finite discrete variables, we show that the probability of\ndiscrepancy greater than any given level between NMC and NMC computed using\nempirical distributions decays exponentially fast as the sample size grows. For\njointly Gaussian variables, we show that under some conditions the NMC\noptimization is an instance of the Max-Cut problem. We then illustrate an\napplication of NMC in inference of graphical model for bijective functions of\njointly Gaussian variables. Finally, we show NMC's utility in a data\napplication of learning nonlinear dependencies among genes in a cancer dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:45:16 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 23:46:37 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Feizi", "Soheil", ""], ["Makhdoumi", "Ali", ""], ["Duffy", "Ken", ""], ["Medard", "Muriel", ""], ["Kellis", "Manolis", ""]]}, {"id": "1606.04809", "submitter": "R\\'emi Leblond", "authors": "R\\'emi Leblond, Fabian Pedregosa and Simon Lacoste-Julien", "title": "ASAGA: Asynchronous Parallel SAGA", "comments": "Appears in: Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017), 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ASAGA, an asynchronous parallel version of the incremental\ngradient algorithm SAGA that enjoys fast linear convergence rates. Through a\nnovel perspective, we revisit and clarify a subtle but important technical\nissue present in a large fraction of the recent convergence rate proofs for\nasynchronous parallel optimization algorithms, and propose a simplification of\nthe recently introduced \"perturbed iterate\" framework that resolves it. We\nthereby prove that ASAGA can obtain a theoretical linear speedup on multi-core\nsystems even without sparsity assumptions. We present results of an\nimplementation on a 40-core architecture illustrating the practical speedup as\nwell as the hardware overhead.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 15:12:01 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 21:32:53 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 12:38:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Leblond", "R\u00e9mi", ""], ["Pedregosa", "Fabian", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1606.04820", "submitter": "Matthias Bauer", "authors": "Matthias Bauer, Mark van der Wilk, Carl Edward Rasmussen", "title": "Understanding Probabilistic Sparse Gaussian Process Approximations", "comments": "published in Advances in Neural Information Processing Systems 29\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good sparse approximations are essential for practical inference in Gaussian\nProcesses as the computational cost of exact methods is prohibitive for large\ndatasets. The Fully Independent Training Conditional (FITC) and the Variational\nFree Energy (VFE) approximations are two recent popular methods. Despite\nsuperficial similarities, these approximations have surprisingly different\ntheoretical properties and behave differently in practice. We thoroughly\ninvestigate the two methods for regression both analytically and through\nillustrative examples, and draw conclusions to guide practical application.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 15:33:27 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 12:42:51 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Bauer", "Matthias", ""], ["van der Wilk", "Mark", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1606.04838", "submitter": "Frank E. Curtis", "authors": "L\\'eon Bottou, Frank E. Curtis, Jorge Nocedal", "title": "Optimization Methods for Large-Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review and commentary on the past, present, and future\nof numerical optimization algorithms in the context of machine learning\napplications. Through case studies on text classification and the training of\ndeep neural networks, we discuss how optimization problems arise in machine\nlearning and what makes them challenging. A major theme of our study is that\nlarge-scale machine learning represents a distinctive setting in which the\nstochastic gradient (SG) method has traditionally played a central role while\nconventional gradient-based nonlinear optimization techniques typically falter.\nBased on this viewpoint, we present a comprehensive theory of a\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance.\nThis leads to a discussion about the next generation of optimization methods\nfor large-scale machine learning, including an investigation of two main\nstreams of research on techniques that diminish noise in the stochastic\ndirections and methods that make use of second-order derivative approximations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:15:53 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 20:08:27 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 20:40:22 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bottou", "L\u00e9on", ""], ["Curtis", "Frank E.", ""], ["Nocedal", "Jorge", ""]]}, {"id": "1606.04934", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya\n  Sutskever and Max Welling", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 19:46:36 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 20:36:01 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Salimans", "Tim", ""], ["Jozefowicz", "Rafal", ""], ["Chen", "Xi", ""], ["Sutskever", "Ilya", ""], ["Welling", "Max", ""]]}, {"id": "1606.04985", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, Laetitia Chapel, S\\'ebastien Lef\\`evre", "title": "Combining multiscale features for classification of hyperspectral\n  images: a sequence based kernel approach", "comments": "8th IEEE GRSS Workshop on Hyperspectral Image and Signal Processing:\n  Evolution in Remote Sensing (WHISPERS 2016), UCLA in Los Angeles, California,\n  U.S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, hyperspectral image classification widely copes with spatial\ninformation to improve accuracy. One of the most popular way to integrate such\ninformation is to extract hierarchical features from a multiscale segmentation.\nIn the classification context, the extracted features are commonly concatenated\ninto a long vector (also called stacked vector), on which is applied a\nconventional vector-based machine learning technique (e.g. SVM with Gaussian\nkernel). In this paper, we rather propose to use a sequence structured kernel:\nthe spectrum kernel. We show that the conventional stacked vector-based kernel\nis actually a special case of this kernel. Experiments conducted on various\npublicly available hyperspectral datasets illustrate the improvement of the\nproposed kernel w.r.t. conventional ones using the same hierarchical spatial\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:19:54 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Cui", "Yanwei", ""], ["Chapel", "Laetitia", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1606.04988", "submitter": "Paul Mineiro", "authors": "Hal Daume III, Nikos Karampatziakis, John Langford, Paul Mineiro", "title": "Logarithmic Time One-Against-Some", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a new online reduction of multiclass classification to binary\nclassification for which training and prediction time scale logarithmically\nwith the number of classes. Compared to previous approaches, we obtain\nsubstantially better statistical performance for two reasons: First, we prove a\ntighter and more complete boosting theorem, and second we translate the results\nmore directly into an algorithm. We show that several simple techniques give\nrise to an algorithm that can compete with one-against-all in both space and\npredictive power while offering exponential improvements in speed when the\nnumber of classes is large.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:27:43 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 02:09:04 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Daume", "Hal", "III"], ["Karampatziakis", "Nikos", ""], ["Langford", "John", ""], ["Mineiro", "Paul", ""]]}, {"id": "1606.04991", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alec Koppel and Alejandro Ribeiro", "title": "A Class of Parallel Doubly Stochastic Algorithms for Large-Scale\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.06782", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems over training sets in which both, the number of\ntraining examples and the dimension of the feature vectors, are large. To solve\nthese problems we propose the random parallel stochastic algorithm (RAPSA). We\ncall the algorithm random parallel because it utilizes multiple parallel\nprocessors to operate on a randomly chosen subset of blocks of the feature\nvector. We call the algorithm stochastic because processors choose training\nsubsets uniformly at random. Algorithms that are parallel in either of these\ndimensions exist, but RAPSA is the first attempt at a methodology that is\nparallel in both the selection of blocks and the selection of elements of the\ntraining set. In RAPSA, processors utilize the randomly chosen functions to\ncompute the stochastic gradient component associated with a randomly chosen\nblock. The technical contribution of this paper is to show that this minimally\ncoordinated algorithm converges to the optimal classifier when the training\nobjective is convex. Moreover, we present an accelerated version of RAPSA\n(ARAPSA) that incorporates the objective function curvature information by\npremultiplying the descent direction by a Hessian approximation matrix. We\nfurther extend the results for asynchronous settings and show that if the\nprocessors perform their updates without any coordination the algorithms are\nstill convergent to the optimal argument. RAPSA and its extensions are then\nnumerically evaluated on a linear estimation problem and a binary image\nclassification task using the MNIST handwritten digit dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:34:46 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Koppel", "Alec", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1606.04995", "submitter": "Tan Le Thanh", "authors": "Le Thanh Tan and Long Bao Le", "title": "Joint Data Compression and MAC Protocol Design for Smartgrids with\n  Renewable Energy", "comments": "https://arxiv.org/admin/q/1589135, Wireless Communications and Mobile\n  Computing, 2016. arXiv admin note: substantial text overlap with\n  arXiv:1506.08318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the joint design of data compression and\n802.15.4-based medium access control (MAC) protocol for smartgrids with\nrenewable energy. We study the setting where a number of nodes, each of which\ncomprises electricity load and/or renewable sources, report periodically their\ninjected powers to a data concentrator. Our design exploits the correlation of\nthe reported data in both time and space to efficiently design the data\ncompression using the compressed sensing (CS) technique and theMAC protocol so\nthat the reported data can be recovered reliably within minimum reporting time.\nSpecifically, we perform the following design tasks: i) we employ the\ntwo-dimensional (2D) CS technique to compress the reported data in the\ndistributed manner; ii) we propose to adapt the 802.15.4 MAC protocol frame\nstructure to enable efficient data transmission and reliable data\nreconstruction; and iii) we develop an analytical model based on which we can\nobtain efficient MAC parameter configuration to minimize the reporting delay.\nFinally, numerical results are presented to demonstrate the effectiveness of\nour proposed framework compared to existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 22:14:15 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Tan", "Le Thanh", ""], ["Le", "Long Bao", ""]]}, {"id": "1606.05018", "submitter": "Stefan Hosein", "authors": "Stefan Hosein and Patrick Hosein", "title": "Improving Power Generation Efficiency using Deep Neural Networks", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant research on power generation,\ndistribution and transmission efficiency especially in the case of renewable\nresources. The main objective is reduction of energy losses and this requires\nimprovements on data acquisition and analysis. In this paper we address these\nconcerns by using consumers' electrical smart meter readings to estimate\nnetwork loading and this information can then be used for better capacity\nplanning. We compare Deep Neural Network (DNN) methods with traditional methods\nfor load forecasting. Our results indicate that DNN methods outperform most\ntraditional methods. This comes at the cost of additional computational\ncomplexity but this can be addressed with the use of cloud resources. We also\nillustrate how these results can be used to better support dynamic pricing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 00:53:56 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Hosein", "Stefan", ""], ["Hosein", "Patrick", ""]]}, {"id": "1606.05027", "submitter": "Jonas Mueller", "authors": "Jonas Mueller, David N. Reshef, George Du, Tommi Jaakkola", "title": "Learning Optimal Interventions", "comments": "AISTATS 2017", "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54:1039-1047, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to identify beneficial interventions from observational data. We\nconsider interventions that are narrowly focused (impacting few covariates) and\nmay be tailored to each individual or globally enacted over a population. For\napplications where harmful intervention is drastically worse than proposing no\nchange, we propose a conservative definition of the optimal intervention.\nAssuming the underlying relationship remains invariant under intervention, we\ndevelop efficient algorithms to identify the optimal intervention policy from\nlimited data and provide theoretical guarantees for our approach in a Gaussian\nProcess setting. Although our methods assume covariates can be precisely\nadjusted, they remain capable of improving outcomes in misspecified settings\nwhere interventions incur unintentional downstream effects. Empirically, our\napproach identifies good interventions in two practical applications: gene\nperturbation and writing improvement.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 01:55:33 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 06:12:56 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mueller", "Jonas", ""], ["Reshef", "David N.", ""], ["Du", "George", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1606.05060", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Pruning Random Forests for Prediction on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to prune a random forest (RF) for resource-constrained prediction.\nWe first construct a RF and then prune it to optimize expected feature cost &\naccuracy. We pose pruning RFs as a novel 0-1 integer program with linear\nconstraints that encourages feature re-use. We establish total unimodularity of\nthe constraint set to prove that the corresponding LP relaxation solves the\noriginal integer program. We then exploit connections to combinatorial\noptimization and develop an efficient primal-dual algorithm, scalable to large\ndatasets. In contrast to our bottom-up approach, which benefits from good RF\ninitialization, conventional methods are top-down acquiring features based on\ntheir utility value and is generally intractable, requiring heuristics.\nEmpirically, our pruning algorithm outperforms existing state-of-the-art\nresource-constrained algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 05:56:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1606.05105", "submitter": "Muhammad  Khan", "authors": "Muhammad Raza Khan and Joshua E. Blumenstock", "title": "Machine Learning Across Cultures: Modeling the Adoption of Financial\n  Services for the Poor", "comments": "This workshop paper summarizes results in a longer paper to be\n  published in the proceedings of KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, mobile operators in many developing economies have launched \"Mobile\nMoney\" platforms that deliver basic financial services over the mobile phone\nnetwork. While many believe that these services can improve the lives of the\npoor, a consistent difficulty has been identifying individuals most likely to\nbenefit from access to the new technology. Here, we combine terabyte-scale data\nfrom three different mobile phone operators from Ghana, Pakistan, and Zambia,\nto better understand the behavioral determinants of mobile money adoption. Our\nsupervised learning models provide insight into the best predictors of adoption\nin three very distinct cultures. We find that models fit on one population fail\nto generalize to another, and in general are highly context-dependent. These\nfindings highlight the need for a nuanced approach to understanding the role\nand potential of financial services for the poor.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 09:21:55 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Khan", "Muhammad Raza", ""], ["Blumenstock", "Joshua E.", ""]]}, {"id": "1606.05110", "submitter": "Elena Erdmann", "authors": "Elena Erdmann, Karin Boczek, Lars Koppers, Gerret von Nordheim,\n  Christian P\\\"olitz, Alejandro Molina, Katharina Morik, Henrik M\\\"uller,\n  J\\\"org Rahnenf\\\"uhrer, Kristian Kersting", "title": "Machine Learning meets Data-Driven Journalism: Boosting International\n  Understanding and Transparency in News Coverage", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Migration crisis, climate change or tax havens: Global challenges need global\nsolutions. But agreeing on a joint approach is difficult without a common\nground for discussion. Public spheres are highly segmented because news are\nmainly produced and received on a national level. Gain- ing a global view on\ninternational debates about important issues is hindered by the enormous\nquantity of news and by language barriers. Media analysis usually focuses only\non qualitative re- search. In this position statement, we argue that it is\nimperative to pool methods from machine learning, journalism studies and\nstatistics to help bridging the segmented data of the international public\nsphere, using the Transatlantic Trade and Investment Partnership (TTIP) as a\ncase study.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 09:31:12 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Erdmann", "Elena", ""], ["Boczek", "Karin", ""], ["Koppers", "Lars", ""], ["von Nordheim", "Gerret", ""], ["P\u00f6litz", "Christian", ""], ["Molina", "Alejandro", ""], ["Morik", "Katharina", ""], ["M\u00fcller", "Henrik", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""], ["Kersting", "Kristian", ""]]}, {"id": "1606.05158", "submitter": "Joseph  Salmon", "authors": "C-A. Deledalle and N. Papadakis and J. Salmon and S. Vaiter", "title": "CLEAR: Covariant LEAst-square Re-fitting with applications to image\n  restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework to remove parts of the systematic\nerrors affecting popular restoration algorithms, with a special focus for image\nprocessing tasks. Generalizing ideas that emerged for $\\ell_1$ regularization,\nwe develop an approach re-fitting the results of standard methods towards the\ninput data. Total variation regularizations and non-local means are special\ncases of interest. We identify important covariant information that should be\npreserved by the re-fitting method, and emphasize the importance of preserving\nthe Jacobian (w.r.t. the observed signal) of the original estimator. Then, we\nprovide an approach that has a \"twicing\" flavor and allows re-fitting the\nrestored signal by adding back a local affine transformation of the residual\nterm. We illustrate the benefits of our method on numerical simulations for\nimage restoration tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 12:23:55 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:45:02 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Deledalle", "C-A.", ""], ["Papadakis", "N.", ""], ["Salmon", "J.", ""], ["Vaiter", "S.", ""]]}, {"id": "1606.05201", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL, NEUROSPIN), Pradeep Reddy Raamana, Denis\n  Engemann (UPMC), Andr\\'es Hoyos-Idrobo (NEUROSPIN, PARIETAL), Yannick\n  Schwartz (PARIETAL, NEUROSPIN), Bertrand Thirion (PARIETAL)", "title": "Assessing and tuning brain decoders: cross-validation, caveats, and\n  guidelines", "comments": "NeuroImage, Elsevier, 2016", "journal-ref": null, "doi": "10.1016/j.neuroimage.2016.10.038", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding, ie prediction from brain images or signals, calls for empirical\nevaluation of its predictive power. Such evaluation is achieved via\ncross-validation, a method also used to tune decoders' hyper-parameters. This\npaper is a review on cross-validation procedures for decoding in neuroimaging.\nIt includes a didactic overview of the relevant theoretical considerations.\nPractical aspects are highlighted with an extensive empirical study of the\ncommon decoders in within-and across-subject predictions, on multiple datasets\n--anatomical and functional MRI and MEG-- and simulations. Theory and\nexperiments outline that the popular \" leave-one-out \" strategy leads to\nunstable and biased estimates, and a repeated random splits method should be\npreferred. Experiments outline the large error bars of cross-validation in\nneuroimaging settings: typical confidence intervals of 10%. Nested\ncross-validation can tune decoders' parameters while avoiding circularity bias.\nHowever we find that it can be more favorable to use sane defaults, in\nparticular for non-sparse decoders.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:29:28 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:40:46 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"], ["Raamana", "Pradeep Reddy", "", "UPMC"], ["Engemann", "Denis", "", "UPMC"], ["Hoyos-Idrobo", "Andr\u00e9s", "", "NEUROSPIN, PARIETAL"], ["Schwartz", "Yannick", "", "PARIETAL, NEUROSPIN"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1606.05228", "submitter": "Charles Zheng", "authors": "Charles Y. Zheng, Rakesh Achanta, and Yuval Benjamini", "title": "How many faces can be recognized? Performance extrapolation for\n  multi-class classification", "comments": "Submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of multi-class classification generally increases with the\nnumber of classes. Using data from a subset of the classes, can we predict how\nwell a classifier will scale with an increased number of classes? Under the\nassumption that the classes are sampled exchangeably, and under the assumption\nthat the classifier is generative (e.g. QDA or Naive Bayes), we show that the\nexpected accuracy when the classifier is trained on $k$ classes is the $k-1$st\nmoment of a \\emph{conditional accuracy distribution}, which can be estimated\nfrom data. This provides the theoretical foundation for performance\nextrapolation based on pseudolikelihood, unbiased estimation, and\nhigh-dimensional asymptotics. We investigate the robustness of our methods to\nnon-generative classifiers in simulations and one optical character recognition\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:38:20 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Zheng", "Charles Y.", ""], ["Achanta", "Rakesh", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1606.05229", "submitter": "Charles Zheng", "authors": "Charles Y. Zheng and Yuval Benjamini", "title": "Estimating mutual information in high dimensions via classification\n  error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate pattern analyses approaches in neuroimaging are fundamentally\nconcerned with investigating the quantity and type of information processed by\nvarious regions of the human brain; typically, estimates of classification\naccuracy are used to quantify information. While a extensive and powerful\nlibrary of methods can be applied to train and assess classifiers, it is not\nalways clear how to use the resulting measures of classification performance to\ndraw scientific conclusions: e.g. for the purpose of evaluating redundancy\nbetween brain regions. An additional confound for interpreting classification\nperformance is the dependence of the error rate on the number and choice of\ndistinct classes obtained for the classification task. In contrast, mutual\ninformation is a quantity defined independently of the experimental design, and\nhas ideal properties for comparative analyses. Unfortunately, estimating the\nmutual information based on observations becomes statistically infeasible in\nhigh dimensions without some kind of assumption or prior.\n  In this paper, we construct a novel classification-based estimator of mutual\ninformation based on high-dimensional asymptotics. We show that in a particular\nlimiting regime, the mutual information is an invertible function of the\nexpected $k$-class Bayes error. While the theory is based on a large-sample,\nhigh-dimensional limit, we demonstrate through simulations that our proposed\nestimator has superior performance to the alternatives in problems of moderate\ndimensionality.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:40:21 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 17:12:03 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zheng", "Charles Y.", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1606.05241", "submitter": "Matej Balog", "authors": "Matej Balog, Balaji Lakshminarayanan, Zoubin Ghahramani, Daniel M.\n  Roy, Yee Whye Teh", "title": "The Mondrian Kernel", "comments": "Accepted for presentation at the 32nd Conference on Uncertainty in\n  Artificial Intelligence (UAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Mondrian kernel, a fast random feature approximation to the\nLaplace kernel. It is suitable for both batch and online learning, and admits a\nfast kernel-width-selection procedure as the random features can be re-used\nefficiently for all kernel widths. The features are constructed by sampling\ntrees via a Mondrian process [Roy and Teh, 2009], and we highlight the\nconnection to Mondrian forests [Lakshminarayanan et al., 2014], where trees are\nalso sampled via a Mondrian process, but fit independently. This link provides\na new insight into the relationship between kernel methods and random forests.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 16:14:32 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Balog", "Matej", ""], ["Lakshminarayanan", "Balaji", ""], ["Ghahramani", "Zoubin", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1606.05273", "submitter": "Will Ruth", "authors": "Will Ruth and Thomas Loughin", "title": "The Effect of Heteroscedasticity on Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression trees are becoming increasingly popular as omnibus predicting\ntools and as the basis of numerous modern statistical learning ensembles. Part\nof their popularity is their ability to create a regression prediction without\never specifying a structure for the mean model. However, the method implicitly\nassumes homogeneous variance across the entire explanatory-variable space. It\nis unknown how the algorithm behaves when faced with heteroscedastic data. In\nthis study, we assess the performance of the most popular regression-tree\nalgorithm in a single-variable setting under a very simple step-function model\nfor heteroscedasticity. We use simulation to show that the locations of splits,\nand hence the ability to accurately predict means, are both adversely\ninfluenced by the change in variance. We identify the pruning algorithm as the\nmain concern, although the effects on the splitting algorithm may be meaningful\nin some applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:11:27 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Ruth", "Will", ""], ["Loughin", "Thomas", ""]]}, {"id": "1606.05275", "submitter": "Janardan Misra", "authors": "Sanjay Podder, Janardan Misra, Senthil Kumaresan, Neville Dubash,\n  Indrani Bhattacharya", "title": "Designing Intelligent Automation based Solutions for Complex Social\n  Problems", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding effective and timely preventive measures against complex social\nproblems affecting relatively low income geographies is a difficult challenge.\nThere is a strong need to adopt intelligent automation based solutions with low\ncost imprints to tackle these problems at larger scales. Starting with the\nhypothesis that analytical modelling and analysis of social phenomena with high\naccuracy is in general inherently hard, in this paper we propose design\nframework to enable data-driven machine learning based adaptive solution\napproach towards enabling more effective preventive measures. We use survey\ndata collected from a socio-economically backward region of India about\nadolescent girls to illustrate the design approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:15:45 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Podder", "Sanjay", ""], ["Misra", "Janardan", ""], ["Kumaresan", "Senthil", ""], ["Dubash", "Neville", ""], ["Bhattacharya", "Indrani", ""]]}, {"id": "1606.05286", "submitter": "Julien Perez", "authors": "Julien Perez", "title": "Spectral decomposition method of dialog state tracking via collective\n  matrix factorization", "comments": "13 pages, 3 figures, 1 Table. arXiv admin note: substantial text\n  overlap with arXiv:1606.04052", "journal-ref": "Dialogue & Discourse 7(3) (2016)", "doi": "10.5087/dad.2016.304", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of dialog management is commonly decomposed into two sequential\nsubtasks: dialog state tracking and dialog policy learning. In an end-to-end\ndialog system, the aim of dialog state tracking is to accurately estimate the\ntrue dialog state from noisy observations produced by the speech recognition\nand the natural language understanding modules. The state tracking task is\nprimarily meant to support a dialog policy. From a probabilistic perspective,\nthis is achieved by maintaining a posterior distribution over hidden dialog\nstates composed of a set of context dependent variables. Once a dialog policy\nis learned, it strives to select an optimal dialog act given the estimated\ndialog state and a defined reward function. This paper introduces a novel\nmethod of dialog state tracking based on a bilinear algebric decomposition\nmodel that provides an efficient inference schema through collective matrix\nfactorization. We evaluate the proposed approach on the second Dialog State\nTracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives\nencouraging results compared to the state-of-the-art trackers that participated\nin this standard benchmark. Finally, we show that the prediction schema is\ncomputationally efficient in comparison to the previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:31:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Perez", "Julien", ""]]}, {"id": "1606.05313", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Percy Liang", "title": "Unsupervised Risk Estimation Using Only Conditional Independence\n  Structure", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to estimate a model's test error from unlabeled data, on\ndistributions very different from the training distribution, while assuming\nonly that certain conditional independencies are preserved between train and\ntest. We do not need to assume that the optimal predictor is the same between\ntrain and test, or that the true distribution lies in any parametric family. We\ncan also efficiently differentiate the error estimate to perform unsupervised\ndiscriminative learning. Our technical tool is the method of moments, which\nallows us to exploit conditional independencies in the absence of a\nfully-specified model. Our framework encompasses a large family of losses\nincluding the log and exponential loss, and extends to structured output\nsettings such as hidden Markov models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 18:48:51 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1606.05320", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Finale Doshi-Velez", "title": "Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks (RNNs), state of the art\nmodels in speech recognition and translation. Our approach to increasing\ninterpretability is by combining an RNN with a hidden Markov model (HMM), a\nsimpler and more transparent model. We explore various combinations of RNNs and\nHMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained\nfirst, then a small LSTM is given HMM state distributions and trained to fill\nin gaps in the HMM's performance; and a jointly trained hybrid model. We find\nthat the LSTM and HMM learn complementary information about the features in the\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:13:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 22:20:39 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1606.05325", "submitter": "Yubin Park", "authors": "Yubin Park and Joyce Ho and Joydeep Ghosh", "title": "ACDC: $\\alpha$-Carving Decision Chain for Risk Stratification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many healthcare settings, intuitive decision rules for risk stratification\ncan help effective hospital resource allocation. This paper introduces a novel\nvariant of decision tree algorithms that produces a chain of decisions, not a\ngeneral tree. Our algorithm, $\\alpha$-Carving Decision Chain (ACDC),\nsequentially carves out \"pure\" subsets of the majority class examples. The\nresulting chain of decision rules yields a pure subset of the minority class\nexamples. Our approach is particularly effective in exploring large and\nclass-imbalanced health datasets. Moreover, ACDC provides an interactive\ninterpretation in conjunction with visual performance metrics such as Receiver\nOperating Characteristics curve and Lift chart.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:36:51 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Park", "Yubin", ""], ["Ho", "Joyce", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1606.05336", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, Jascha\n  Sohl-Dickstein", "title": "On the Expressive Power of Deep Neural Networks", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of neural network expressivity,\nwhich seeks to characterize how structural properties of a neural network\nfamily affect the functions it is able to compute. Our approach is based on an\ninterrelated set of measures of expressivity, unified by the novel notion of\ntrajectory length, which measures how the output of a network changes as the\ninput sweeps along a one-dimensional path. Our findings can be summarized as\nfollows:\n  (1) The complexity of the computed function grows exponentially with depth.\n  (2) All weights are not equal: trained networks are more sensitive to their\nlower (initial) layer weights.\n  (3) Regularizing on trajectory length (trajectory regularization) is a\nsimpler alternative to batch normalization, with the same performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:55:29 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 20:26:47 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2016 22:21:25 GMT"}, {"version": "v4", "created": "Mon, 3 Oct 2016 15:44:39 GMT"}, {"version": "v5", "created": "Wed, 1 Mar 2017 03:00:26 GMT"}, {"version": "v6", "created": "Sun, 18 Jun 2017 13:24:34 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Raghu", "Maithra", ""], ["Poole", "Ben", ""], ["Kleinberg", "Jon", ""], ["Ganguli", "Surya", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1606.05340", "submitter": "Subhaneil Lahiri", "authors": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein,\n  Surya Ganguli", "title": "Exponential expressivity in deep neural networks through transient chaos", "comments": "Fixed equation references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Riemannian geometry with the mean field theory of high dimensional\nchaos to study the nature of signal propagation in generic, deep neural\nnetworks with random weights. Our results reveal an order-to-chaos expressivity\nphase transition, with networks in the chaotic phase computing nonlinear\nfunctions whose global curvature grows exponentially with depth but not width.\nWe prove this generic class of deep random functions cannot be efficiently\ncomputed by any shallow network, going beyond prior work restricted to the\nanalysis of single functions. Moreover, we formalize and quantitatively\ndemonstrate the long conjectured idea that deep networks can disentangle highly\ncurved manifolds in input space into flat manifolds in hidden space. Our\ntheoretical analysis of the expressive power of deep networks broadly applies\nto arbitrary nonlinearities, and provides a quantitative underpinning for\npreviously abstract notions about the geometry of deep functions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:59:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 18:13:20 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Poole", "Ben", ""], ["Lahiri", "Subhaneil", ""], ["Raghu", "Maithra", ""], ["Sohl-Dickstein", "Jascha", ""], ["Ganguli", "Surya", ""]]}, {"id": "1606.05363", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou", "title": "Predicting Ambulance Demand: Challenges and Methods", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting ambulance demand accurately at a fine resolution in time and space\n(e.g., every hour and 1 km$^2$) is critical for staff / fleet management and\ndynamic deployment. There are several challenges: though the dataset is\ntypically large-scale, demand per time period and locality is almost always\nzero. The demand arises from complex urban geography and exhibits complex\nspatio-temporal patterns, both of which need to captured and exploited. To\naddress these challenges, we propose three methods based on Gaussian mixture\nmodels, kernel density estimation, and kernel warping. These methods provide\nspatio-temporal predictions for Toronto and Melbourne that are significantly\nmore accurate than the current industry practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 20:25:47 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhou", "Zhengyi", ""]]}, {"id": "1606.05382", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Deovrat Kakde, Maria Jahja, Wei Xiao, Hansi Jiang,\n  Seunghyun Kong, Sergiy Peredriy", "title": "Sampling Method for Fast Training of Support Vector Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2018.8463127", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a popular outlier detection\ntechnique which constructs a flexible description of the input data. SVDD\ncomputation time is high for large training datasets which limits its use in\nbig-data process-monitoring applications. We propose a new iterative\nsampling-based method for SVDD training. The method incrementally learns the\ntraining data description at each iteration by computing SVDD on an independent\nrandom sample selected with replacement from the training data set. The\nexperimental results indicate that the proposed method is extremely fast and\nprovides a good data description .\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:18:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 17:30:57 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 22:15:38 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""], ["Jahja", "Maria", ""], ["Xiao", "Wei", ""], ["Jiang", "Hansi", ""], ["Kong", "Seunghyun", ""], ["Peredriy", "Sergiy", ""]]}, {"id": "1606.05386", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin", "title": "Model-Agnostic Interpretability of Machine Learning", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why machine learning models behave the way they do empowers\nboth system designers and end-users in many ways: in model selection, feature\nengineering, in order to trust and act upon the predictions, and in more\nintuitive user interfaces. Thus, interpretability has become a vital concern in\nmachine learning, and work in the area of interpretable models has found\nrenewed interest. In some applications, such models are as accurate as\nnon-interpretable ones, and thus are preferred for their transparency. Even\nwhen they are not accurate, they may still be preferred when interpretability\nis of paramount importance. However, restricting machine learning to\ninterpretable models is often a severe limitation. In this paper we argue for\nexplaining machine learning predictions using model-agnostic approaches. By\ntreating the machine learning models as black-box functions, these approaches\nprovide crucial flexibility in the choice of models, explanations, and\nrepresentations, improving debugging, comparison, and interfaces for a variety\nof users and models. We also outline the main challenges for such methods, and\nreview a recently-introduced model-agnostic explanation approach (LIME) that\naddresses these challenges.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:39:41 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1606.05390", "submitter": "Satoshi Hara", "authors": "Satoshi Hara, Kohei Hayashi", "title": "Making Tree Ensembles Interpretable", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensembles, such as random forest and boosted trees, are renowned for\ntheir high prediction performance, whereas their interpretability is critically\nlimited. In this paper, we propose a post processing method that improves the\nmodel interpretability of tree ensembles. After learning a complex tree\nensembles in a standard way, we approximate it by a simpler model that is\ninterpretable for human. To obtain the simpler model, we derive the EM\nalgorithm minimizing the KL divergence from the complex ensemble. A synthetic\nexperiment showed that a complicated tree ensemble was approximated reasonably\nas interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 00:33:03 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hara", "Satoshi", ""], ["Hayashi", "Kohei", ""]]}, {"id": "1606.05400", "submitter": "Cesar Comin M.Sc.", "authors": "Cesar H. Comin, Thomas K. DM. Peron, Filipi N. Silva, Diego R.\n  Amancio, Francisco A. Rodrigues, Luciano da F. Costa", "title": "Complex systems: features, similarity and connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing interest in complex networks research has been a consequence\nof several intrinsic features of this area, such as the generality of the\napproach to represent and model virtually any discrete system, and the\nincorporation of concepts and methods deriving from many areas, from\nstatistical physics to sociology, which are often used in an independent way.\nYet, for this same reason, it would be desirable to integrate these various\naspects into a more coherent and organic framework, which would imply in\nseveral benefits normally allowed by the systematization in science, including\nthe identification of new types of problems and the cross-fertilization between\nfields. More specifically, the identification of the main areas to which the\nconcepts frequently used in complex networks can be applied paves the way to\nadopting and applying a larger set of concepts and methods deriving from those\nrespective areas. Among the several areas that have been used in complex\nnetworks research, pattern recognition, optimization, linear algebra, and time\nseries analysis seem to play a more basic and recurrent role. In the present\nmanuscript, we propose a systematic way to integrate the concepts from these\ndiverse areas regarding complex networks research. In order to do so, we start\nby grouping the multidisciplinary concepts into three main groups, namely\nfeatures, similarity, and network connectivity. Then we show that several of\nthe analysis and modeling approaches to complex networks can be thought as a\ncomposition of maps between these three groups, with emphasis on nine main\ntypes of mappings, which are presented and illustrated. Such a systematization\nof principles and approaches also provides an opportunity to review some of the\nmost closely related works in the literature, which is also developed in this\narticle.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 01:48:16 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Comin", "Cesar H.", ""], ["Peron", "Thomas K. DM.", ""], ["Silva", "Filipi N.", ""], ["Amancio", "Diego R.", ""], ["Rodrigues", "Francisco A.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1606.05492", "submitter": "Neeraj Bokde", "authors": "Neeraj Bokde, Gualberto Asencio-Cort\\'es, Francisco\n  Mart\\'inez-\\'Alvarez and Kishore Kulat", "title": "PSF : Introduction to R Package for Pattern Sequence Based Forecasting\n  Algorithm", "comments": "Available at:\n  https://journal.r-project.org/archive/2017/RJ-2017-021/index.html, The R\n  Journal 2017", "journal-ref": null, "doi": "10.32614/RJ-2017-021", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses about an R package that implements the Pattern Sequence\nbased Forecasting (PSF) algorithm, which was developed for univariate time\nseries forecasting. This algorithm has been successfully applied to many\ndifferent fields. The PSF algorithm consists of two major parts: clustering and\nprediction. The clustering part includes selection of the optimum number of\nclusters. It labels time series data with reference to such clusters. The\nprediction part includes functions like optimum window size selection for\nspecific patterns and prediction of future values with reference to past\npattern sequences. The PSF package consists of various functions to implement\nthe PSF algorithm. It also contains a function which automates all other\nfunctions to obtain optimized prediction results. The aim of this package is to\npromote the PSF algorithm and to ease its implementation with minimum efforts.\nThis paper describes all the functions in the PSF package with their syntax. It\nalso provides a simple example of usage. Finally, the usefulness of this\npackage is discussed by comparing it to auto.arima and ets, well-known time\nseries forecasting functions available on CRAN repository.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 12:00:21 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 09:58:47 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 12:29:26 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Bokde", "Neeraj", ""], ["Asencio-Cort\u00e9s", "Gualberto", ""], ["Mart\u00ednez-\u00c1lvarez", "Francisco", ""], ["Kulat", "Kishore", ""]]}, {"id": "1606.05560", "submitter": "Boram Yoon", "authors": "Boram Yoon", "title": "Estimation of matrix trace using machine learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-24126", "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new trace estimator of the matrix whose explicit form is not\ngiven but its matrix multiplication to a vector is available. The form of the\nestimator is similar to the Hutchison stochastic trace estimator, but instead\nof the random noise vectors in Hutchison estimator, we use small number of\nprobing vectors determined by machine learning. Evaluation of the quality of\nestimates and bias correction are discussed. An unbiased estimator is proposed\nfor the calculation of the expectation value of a function of traces. In the\nnumerical experiments with random matrices, it is shown that the precision of\ntrace estimates with $\\mathcal{O}(10)$ probing vectors determined by the\nmachine learning is similar to that with $\\mathcal{O}(10000)$ random noise\nvectors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:50:25 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Yoon", "Boram", ""]]}, {"id": "1606.05572", "submitter": "Haizi Yu", "authors": "Haizi Yu, Lav R. Varshney, Guy E. Garnett, Ranjitha Kumar", "title": "Learning Interpretable Musical Compositional Rules and Traces", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout music history, theorists have identified and documented\ninterpretable rules that capture the decisions of composers. This paper asks,\n\"Can a machine behave like a music theorist?\" It presents MUS-ROVER, a\nself-learning system for automatically discovering rules from symbolic music.\nMUS-ROVER performs feature learning via $n$-gram models to extract\ncompositional rules --- statistical patterns over the resulting features. We\nevaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover\nknown rules, as well as identify new, characteristic patterns for further\nstudy. We discuss how the extracted rules can be used in both machine and human\ncomposition.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 15:58:24 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Yu", "Haizi", ""], ["Varshney", "Lav R.", ""], ["Garnett", "Guy E.", ""], ["Kumar", "Ranjitha", ""]]}, {"id": "1606.05579", "submitter": "Irina Higgins", "authors": "Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria,\n  Charles Blundell, Shakir Mohamed, Alexander Lerchner", "title": "Early Visual Concept Learning with Unsupervised Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated discovery of early visual concepts from raw image data is a major\nopen challenge in AI research. Addressing this problem, we propose an\nunsupervised approach for learning disentangled representations of the\nunderlying factors of variation. We draw inspiration from neuroscience, and\nshow how this can be achieved in an unsupervised generative model by applying\nthe same learning pressures as have been suggested to act in the ventral visual\nstream in the brain. By enforcing redundancy reduction, encouraging statistical\nindependence, and exposure to data with transform continuities analogous to\nthose to which human infants are exposed, we obtain a variational autoencoder\n(VAE) framework capable of learning disentangled factors. Our approach makes\nfew assumptions and works well across a wide variety of datasets. Furthermore,\nour solution has useful emergent properties, such as zero-shot inference and an\nintuitive understanding of \"objectness\".\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 16:19:46 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 19:50:49 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 09:30:26 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Higgins", "Irina", ""], ["Matthey", "Loic", ""], ["Glorot", "Xavier", ""], ["Pal", "Arka", ""], ["Uria", "Benigno", ""], ["Blundell", "Charles", ""], ["Mohamed", "Shakir", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1606.05589", "submitter": "Abhishek Das", "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv\n  Batra", "title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "comments": "5 pages, 4 figures, 3 tables, presented at 2016 ICML Workshop on\n  Human Interpretability in Machine Learning (WHI 2016), New York, NY. arXiv\n  admin note: substantial text overlap with arXiv:1606.03556", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:00:02 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Das", "Abhishek", ""], ["Agrawal", "Harsh", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.05596", "submitter": "Yang Lei", "authors": "Yang Lei, James C. Bezdek, Simone Romano, Nguyen Xuan Vinh, Jeffrey\n  Chan and James Bailey", "title": "Ground Truth Bias in External Cluster Validity Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been noticed that some external CVIs exhibit a preferential bias\ntowards a larger or smaller number of clusters which is monotonic (directly or\ninversely) in the number of clusters in candidate partitions. This type of bias\nis caused by the functional form of the CVI model. For example, the popular\nRand index (RI) exhibits a monotone increasing (NCinc) bias, while the Jaccard\nIndex (JI) index suffers from a monotone decreasing (NCdec) bias. This type of\nbias has been previously recognized in the literature. In this work, we\nidentify a new type of bias arising from the distribution of the ground truth\n(reference) partition against which candidate partitions are compared. We call\nthis new type of bias ground truth (GT) bias. This type of bias occurs if a\nchange in the reference partition causes a change in the bias status (e.g.,\nNCinc, NCdec) of a CVI. For example, NCinc bias in the RI can be changed to\nNCdec bias by skewing the distribution of clusters in the ground truth\npartition. It is important for users to be aware of this new type of biased\nbehaviour, since it may affect the interpretations of CVI results. The\nobjective of this article is to study the empirical and theoretical\nimplications of GT bias. To the best of our knowledge, this is the first\nextensive study of such a property for external cluster validity indices.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:31:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lei", "Yang", ""], ["Bezdek", "James C.", ""], ["Romano", "Simone", ""], ["Vinh", "Nguyen Xuan", ""], ["Chan", "Jeffrey", ""], ["Bailey", "James", ""]]}, {"id": "1606.05642", "submitter": "Mohammad Javad Faraji", "authors": "Mohammadjavad Faraji, Kerstin Preuschoff, Wulfram Gerstner", "title": "Balancing New Against Old Information: The Role of Surprise in Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surprise describes a range of phenomena from unexpected events to behavioral\nresponses. We propose a measure of surprise and use it for surprise-driven\nlearning. Our surprise measure takes into account data likelihood as well as\nthe degree of commitment to a belief via the entropy of the belief\ndistribution. We find that surprise-minimizing learning dynamically adjusts the\nbalance between new and old information without the need of knowledge about the\ntemporal statistics of the environment. We apply our framework to a dynamic\ndecision-making task and a maze exploration task. Our surprise minimizing\nframework is suitable for learning in complex environments, even if the\nenvironment undergoes gradual or sudden changes and could eventually provide a\nframework to study the behavior of humans and animals encountering surprising\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 19:54:43 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 20:31:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Faraji", "Mohammadjavad", ""], ["Preuschoff", "Kerstin", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1606.05672", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Andrea Passerini", "title": "Interpretability in Linear Brain Decoding", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the interpretability of brain decoding approaches is of primary\ninterest in many neuroimaging studies. Despite extensive studies of this type,\nat present, there is no formal definition for interpretability of brain\ndecoding models. As a consequence, there is no quantitative measure for\nevaluating the interpretability of different brain decoding methods. In this\npaper, we present a simple definition for interpretability of linear brain\ndecoding models. Then, we propose to combine the interpretability and the\nperformance of the brain decoding into a new multi-objective criterion for\nmodel selection. Our preliminary results on the toy data show that optimizing\nthe hyper-parameters of the regularized linear classifier based on the proposed\ncriterion results in more informative linear models. The presented definition\nprovides the theoretical background for quantitative evaluation of\ninterpretability in linear brain decoding.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 20:34:04 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Passerini", "Andrea", ""]]}, {"id": "1606.05685", "submitter": "Josua Krause", "authors": "Josua Krause, Adam Perer, Enrico Bertini", "title": "Using Visual Analytics to Interpret Predictive Machine Learning Models", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly believed that increasing the interpretability of a machine\nlearning model may decrease its predictive power. However, inspecting\ninput-output relationships of those models using visual analytics, while\ntreating them as black-box, can help to understand the reasoning behind\noutcomes without sacrificing predictive quality. We identify a space of\npossible solutions and provide two examples of where such techniques have been\nsuccessfully used in practice.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 21:56:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 18:06:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Krause", "Josua", ""], ["Perer", "Adam", ""], ["Bertini", "Enrico", ""]]}, {"id": "1606.05693", "submitter": "Nicholas Johnson", "authors": "Nicholas Johnson, Vidyashankar Sivakumar, Arindam Banerjee", "title": "Structured Stochastic Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic linear bandit problem proceeds in rounds where at each round\nthe algorithm selects a vector from a decision set after which it receives a\nnoisy linear loss parameterized by an unknown vector. The goal in such a\nproblem is to minimize the (pseudo) regret which is the difference between the\ntotal expected loss of the algorithm and the total expected loss of the best\nfixed vector in hindsight. In this paper, we consider settings where the\nunknown parameter has structure, e.g., sparse, group sparse, low-rank, which\ncan be captured by a norm, e.g., $L_1$, $L_{(1,2)}$, nuclear norm. We focus on\nconstructing confidence ellipsoids which contain the unknown parameter across\nall rounds with high-probability. We show the radius of such ellipsoids depend\non the Gaussian width of sets associated with the norm capturing the structure.\nSuch characterization leads to tighter confidence ellipsoids and, therefore,\nsharper regret bounds compared to bounds in the existing literature which are\nbased on the ambient dimensionality.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:31:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Johnson", "Nicholas", ""], ["Sivakumar", "Vidyashankar", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1606.05725", "submitter": "Amirhossein Akbarnejad", "authors": "Amirhossein Akbarnejad, Mahdieh Soleymani Baghshah", "title": "An Efficient Large-scale Semi-supervised Multi-label Classifier Capable\n  of Handling Missing labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification has received considerable interest in recent\nyears. Multi-label classifiers have to address many problems including:\nhandling large-scale datasets with many instances and a large set of labels,\ncompensating missing label assignments in the training set, considering\ncorrelations between labels, as well as exploiting unlabeled data to improve\nprediction performance. To tackle datasets with a large set of labels,\nembedding-based methods have been proposed which seek to represent the label\nassignments in a low-dimensional space. Many state-of-the-art embedding-based\nmethods use a linear dimensionality reduction to represent the label\nassignments in a low-dimensional space. However, by doing so, these methods\nactually neglect the tail labels - labels that are infrequently assigned to\ninstances. We propose an embedding-based method that non-linearly embeds the\nlabel vectors using an stochastic approach, thereby predicting the tail labels\nmore accurately. Moreover, the proposed method have excellent mechanisms for\nhandling missing labels, dealing with large-scale datasets, as well as\nexploiting unlabeled data. With the best of our knowledge, our proposed method\nis the first multi-label classifier that simultaneously addresses all of the\nmentioned challenges. Experiments on real-world datasets show that our method\noutperforms stateof-the-art multi-label classifiers by a large margin, in terms\nof prediction performance, as well as training time.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 07:49:13 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Akbarnejad", "Amirhossein", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1606.05798", "submitter": "Guolong Su", "authors": "Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": "WHI 2016 submission", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a contribution to interpretable machine learning research, we develop a\nnovel optimization framework for learning accurate and sparse two-level Boolean\nrules. We consider rules in both conjunctive normal form (AND-of-ORs) and\ndisjunctive normal form (OR-of-ANDs). A principled objective function is\nproposed to trade classification accuracy and interpretability, where we use\nHamming loss to characterize accuracy and sparsity to characterize\ninterpretability. We propose efficient procedures to optimize these objectives\nbased on linear programming (LP) relaxation, block coordinate descent, and\nalternating minimization. Experiments show that our new algorithms provide very\ngood tradeoffs between accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 19:37:26 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Su", "Guolong", ""], ["Wei", "Dennis", ""], ["Varshney", "Kush R.", ""], ["Malioutov", "Dmitry M.", ""]]}, {"id": "1606.05819", "submitter": "Sechan Oh", "authors": "Amit Dhurandhar, Sechan Oh, Marek Petrik", "title": "Building an Interpretable Recommender via Loss-Preserving Transformation", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for building an interpretable recommender system for\npersonalizing online content and promotions. Historical data available for the\nsystem consists of customer features, provided content (promotions), and user\nresponses. Unlike in a standard multi-class classification setting,\nmisclassification costs depend on both recommended actions and customers. Our\nmethod transforms such a data set to a new set which can be used with standard\ninterpretable multi-class classification algorithms. The transformation has the\ndesirable property that minimizing the standard misclassification penalty in\nthis new space is equivalent to minimizing the custom cost function.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 01:37:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Oh", "Sechan", ""], ["Petrik", "Marek", ""]]}, {"id": "1606.05850", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Guaranteed bounds on the Kullback-Leibler divergence of univariate\n  mixtures using piecewise log-sum-exp inequalities", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": "10.3390/e18120442", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic measures such as the entropy, cross-entropy and the\nKullback-Leibler divergence between two mixture models is a core primitive in\nmany signal processing tasks. Since the Kullback-Leibler divergence of mixtures\nprovably does not admit a closed-form formula, it is in practice either\nestimated using costly Monte-Carlo stochastic integration, approximated, or\nbounded using various techniques. We present a fast and generic method that\nbuilds algorithmically closed-form lower and upper bounds on the entropy, the\ncross-entropy and the Kullback-Leibler divergence of mixtures. We illustrate\nthe versatile method by reporting on our experiments for approximating the\nKullback-Leibler divergence between univariate exponential mixtures, Gaussian\nmixtures, Rayleigh mixtures, and Gamma mixtures.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 09:39:30 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 00:24:48 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1606.05889", "submitter": "Mathukumalli Vidyasagar", "authors": "Shashank Ranjan and Mathukumalli Vidyasagar", "title": "Tight Performance Bounds for Compressed Sensing With Conventional and\n  Group Sparsity", "comments": "26 pages, one table, no figures. Revised version of a paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of recovering a group sparse vector from\na small number of linear measurements. In the past the common approach has been\nto use various \"group sparsity-inducing\" norms such as the Group LASSO norm for\nthis purpose. By using the theory of convex relaxations, we show that it is\nalso possible to use $\\ell_1$-norm minimization for group sparse recovery. We\nintroduce a new concept called group robust null space property (GRNSP), and\nshow that, under suitable conditions, a group version of the restricted\nisometry property (GRIP) implies the GRNSP, and thus leads to group sparse\nrecovery. When all groups are of equal size, our bounds are less conservative\nthan known bounds. Moreover, our results apply even to situations where where\nthe groups have different sizes. When specialized to conventional sparsity, our\nbounds reduce to one of the well-known \"best possible\" conditions for sparse\nrecovery. This relationship between GRNSP and GRIP is new even for conventional\nsparsity, and substantially streamlines the proofs of some known results. Using\nthis relationship, we derive bounds on the $\\ell_p$-norm of the residual error\nvector for all $p \\in [1,2]$, and not just when $p = 2$. When the measurement\nmatrix consists of random samples of a sub-Gaussian random variable, we present\nbounds on the number of measurements, which are less conservative than\ncurrently known bounds.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 17:01:31 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 06:25:41 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ranjan", "Shashank", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1606.05896", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, James Zou, Ryan P. Adams and Charles Sutton", "title": "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when they see\none. We present a new approach to interactive clustering for data exploration\ncalled TINDER, based on a particularly simple feedback mechanism, in which an\nanalyst can reject a given clustering and request a new one, which is chosen to\nbe different from the previous clustering while fitting the data well. We\nformalize this interaction in a Bayesian framework as a method for prior\nelicitation, in which each different clustering is produced by a prior\ndistribution that is modified to discourage previously rejected clusterings. We\nshow that TINDER successfully produces a diverse set of clusterings, each of\nequivalent quality, that are much more diverse than would be obtained by\nrandomized restarts.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 18:07:15 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Srivastava", "Akash", ""], ["Zou", "James", ""], ["Adams", "Ryan P.", ""], ["Sutton", "Charles", ""]]}, {"id": "1606.05908", "submitter": "Carl Doersch", "authors": "Carl Doersch", "title": "Tutorial on Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In just three years, Variational Autoencoders (VAEs) have emerged as one of\nthe most popular approaches to unsupervised learning of complicated\ndistributions. VAEs are appealing because they are built on top of standard\nfunction approximators (neural networks), and can be trained with stochastic\ngradient descent. VAEs have already shown promise in generating many kinds of\ncomplicated data, including handwritten digits, faces, house numbers, CIFAR\nimages, physical models of scenes, segmentation, and predicting the future from\nstatic images. This tutorial introduces the intuitions behind VAEs, explains\nthe mathematics behind them, and describes some empirical behavior. No prior\nknowledge of variational Bayesian methods is assumed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 21:02:30 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 12:33:43 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 16:56:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Doersch", "Carl", ""]]}, {"id": "1606.05925", "submitter": "Vikrant Singh Tomar", "authors": "Vikrant Singh Tomar and Richard C. Rose", "title": "Graph based manifold regularized deep neural networks for automatic\n  speech recognition", "comments": "12 pages including citations, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been successfully applied to a wide variety\nof acoustic modeling tasks in recent years. These include the applications of\nDNNs either in a discriminative feature extraction or in a hybrid acoustic\nmodeling scenario. Despite the rapid progress in this area, a number of\nchallenges remain in training DNNs. This paper presents an effective way of\ntraining DNNs using a manifold learning based regularization framework. In this\nframework, the parameters of the network are optimized to preserve underlying\nmanifold based relationships between speech feature vectors while minimizing a\nmeasure of loss between network outputs and targets. This is achieved by\nincorporating manifold based locality constraints in the objective criterion of\nDNNs. Empirical evidence is provided to demonstrate that training a network\nwith manifold constraints preserves structural compactness in the hidden layers\nof the network. Manifold regularization is applied to train bottleneck DNNs for\nfeature extraction in hidden Markov model (HMM) based speech recognition. The\nexperiments in this work are conducted on the Aurora-2 spoken digits and the\nAurora-4 read news large vocabulary continuous speech recognition tasks. The\nperformance is measured in terms of word error rate (WER) on these tasks. It is\nshown that the manifold regularized DNNs result in up to 37% reduction in WER\nrelative to standard DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 23:40:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Tomar", "Vikrant Singh", ""], ["Rose", "Richard C.", ""]]}, {"id": "1606.05988", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung", "title": "Continuum directions for supervised dimension reduction", "comments": null, "journal-ref": "Comput. Stat. Data Anal. 125 (2018) 27-43", "doi": "10.1016/j.csda.2018.03.015", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction of multivariate data supervised by auxiliary information\nis considered. A series of basis for dimension reduction is obtained as\nminimizers of a novel criterion. The proposed method is akin to continuum\nregression, and the resulting basis is called continuum directions. With a\npresence of binary supervision data, these directions continuously bridge the\nprincipal component, mean difference and linear discriminant directions, thus\nranging from unsupervised to fully supervised dimension reduction.\nHigh-dimensional asymptotic studies of continuum directions for binary\nsupervision reveal several interesting facts. The conditions under which the\nsample continuum directions are inconsistent, but their classification\nperformance is good, are specified. While the proposed method can be directly\nused for binary and multi-category classification, its generalizations to\nincorporate any form of auxiliary data are also presented. The proposed method\nenjoys fast computation, and the performance is better or on par with more\ncomputer-intensive alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 06:52:41 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:43:32 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 17:04:18 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Jung", "Sungkyu", ""]]}, {"id": "1606.06121", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Quantifying and Reducing Stereotypes in Word Embeddings", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:58:45 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1606.06126", "submitter": "Josiah Hanna", "authors": "Josiah P. Hanna, Peter Stone, Scott Niekum", "title": "Bootstrapping with Models: Confidence Intervals for Off-Policy\n  Evaluation", "comments": "Published in proceedings of the 16th International Conference on\n  Autonomous Agents and Multi-agent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an autonomous agent, executing a poor policy may be costly or even\ndangerous. For such agents, it is desirable to determine confidence interval\nlower bounds on the performance of any given policy without executing said\npolicy. Current methods for exact high confidence off-policy evaluation that\nuse importance sampling require a substantial amount of data to achieve a tight\nlower bound. Existing model-based methods only address the problem in discrete\nstate spaces. Since exact bounds are intractable for many domains we trade off\nstrict guarantees of safety for more data-efficient approximate bounds. In this\ncontext, we propose two bootstrapping off-policy evaluation methods which use\nlearned MDP transition models in order to estimate lower confidence bounds on\npolicy performance with limited data in both continuous and discrete state\nspaces. Since direct use of a model may introduce bias, we derive a theoretical\nupper bound on model bias for when the model transition function is estimated\nwith i.i.d. trajectories. This bound broadens our understanding of the\nconditions under which model-based methods have high bias. Finally, we\nempirically evaluate our proposed methods and analyze the settings in which\ndifferent bootstrapping off-policy confidence interval methods succeed and\nfail.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:06:22 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 23:26:07 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 17:13:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hanna", "Josiah P.", ""], ["Stone", "Peter", ""], ["Niekum", "Scott", ""]]}, {"id": "1606.06177", "submitter": "Kush Varshney", "authors": "Prasanna Sattigeri, Aur\\'elie Lozano, Aleksandra Mojsilovi\\'c, Kush R.\n  Varshney, Mahmoud Naghshineh", "title": "Understanding Innovation to Drive Sustainable Development", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation is among the key factors driving a country's economic and social\ngrowth. But what are the factors that make a country innovative? How do they\ndiffer across different parts of the world and different stages of development?\nIn this work done in collaboration with the World Economic Forum (WEF), we\nanalyze the scores obtained through executive opinion surveys that constitute\nthe WEF's Global Competitiveness Index in conjunction with other country-level\nmetrics and indicators to identify actionable levers of innovation. The\nfindings can help country leaders and organizations shape the policies to drive\ndevelopmental activities and increase the capacity of innovation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 19:14:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Sattigeri", "Prasanna", ""], ["Lozano", "Aur\u00e9lie", ""], ["Mojsilovi\u0107", "Aleksandra", ""], ["Varshney", "Kush R.", ""], ["Naghshineh", "Mahmoud", ""]]}, {"id": "1606.06179", "submitter": "Arnak Dalalyan S.", "authors": "Pierre C. Bellec and Arnak S. Dalalyan and Edwin Grappin and Quentin\n  Paris", "title": "On the prediction loss of the lasso in the partially labeled setting", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the risk bounds of the lasso estimator in the\ncontext of transductive and semi-supervised learning. In other terms, the\nsetting under consideration is that of regression with random design under\npartial labeling. The main goal is to obtain user-friendly bounds on the\noff-sample prediction risk. To this end, the simple setting of bounded response\nvariable and bounded (high-dimensional) covariates is considered. We propose\nsome new adaptations of the lasso to these settings and establish oracle\ninequalities both in expectation and in deviation. These results provide\nnon-asymptotic upper bounds on the risk that highlight the interplay between\nthe bias due to the mis-specification of the linear model, the bias due to the\napproximate sparsity and the variance. They also demonstrate that the presence\nof a large number of unlabeled features may have significant positive impact in\nthe situations where the restricted eigenvalue of the design matrix vanishes or\nis very small.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:38:59 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 12:49:32 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Bellec", "Pierre C.", ""], ["Dalalyan", "Arnak S.", ""], ["Grappin", "Edwin", ""], ["Paris", "Quentin", ""]]}, {"id": "1606.06237", "submitter": "Yining Wang", "authors": "Yining Wang, Animashree Anandkumar", "title": "Online and Differentially-Private Tensor Decomposition", "comments": "19 pages, 9 figures. To appear at the 30th Annual Conference on\n  Advances in Neural Information Processing Systems (NIPS 2016), to be held at\n  Barcelona, Spain. Fix small typos in proofs of Lemmas C.5 and C.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we resolve many of the key algorithmic questions regarding\nrobustness, memory efficiency, and differential privacy of tensor\ndecomposition. We propose simple variants of the tensor power method which\nenjoy these strong properties. We present the first guarantees for online\ntensor power method which has a linear memory requirement. Moreover, we present\na noise calibrated tensor power method with efficient privacy guarantees. At\nthe heart of all these guarantees lies a careful perturbation analysis derived\nin this paper which improves up on the existing results significantly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:30:10 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 22:30:01 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 21:56:58 GMT"}, {"version": "v4", "created": "Thu, 15 Dec 2016 13:35:22 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Wang", "Yining", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1606.06250", "submitter": "Weiwei Pan", "authors": "Arjumand Masood and Weiwei Pan and Finale Doshi-Velez", "title": "An Empirical Comparison of Sampling Quality Metrics: A Case Study for\n  Bayesian Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we empirically explore the question: how can we assess the\nquality of samples from some target distribution? We assume that the samples\nare provided by some valid Monte Carlo procedure, so we are guaranteed that the\ncollection of samples will asymptotically approximate the true distribution.\nMost current evaluation approaches focus on two questions: (1) Has the chain\nmixed, that is, is it sampling from the distribution? and (2) How independent\nare the samples (as MCMC procedures produce correlated samples)? Focusing on\nthe case of Bayesian nonnegative matrix factorization, we empirically evaluate\nstandard metrics of sampler quality as well as propose new metrics to capture\naspects that these measures fail to expose. The aspect of sampling that is of\nparticular interest to us is the ability (or inability) of sampling methods to\nmove between multiple optima in NMF problems. As a proxy, we propose and study\na number of metrics that might quantify the diversity of a set of NMF\nfactorizations obtained by a sampler through quantifying the coverage of the\nposterior distribution. We compare the performance of a number of standard\nsampling methods for NMF in terms of these new metrics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:59:34 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Masood", "Arjumand", ""], ["Pan", "Weiwei", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1606.06343", "submitter": "Mark Dredze", "authors": "Mark Dredze and Manuel Garc\\'ia-Herranz and Alex Rutherford and Gideon\n  Mann", "title": "Twitter as a Source of Global Mobility Patterns for Social Good", "comments": "Presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data on human spatial distribution and movement is essential for\nunderstanding and analyzing social systems. However existing sources for this\ndata are lacking in various ways; difficult to access, biased, have poor\ngeographical or temporal resolution, or are significantly delayed. In this\npaper, we describe how geolocation data from Twitter can be used to estimate\nglobal mobility patterns and address these shortcomings. These findings will\ninform how this novel data source can be harnessed to address humanitarian and\ndevelopment efforts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 21:39:51 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Dredze", "Mark", ""], ["Garc\u00eda-Herranz", "Manuel", ""], ["Rutherford", "Alex", ""], ["Mann", "Gideon", ""]]}, {"id": "1606.06352", "submitter": "Brendan O'Connor", "authors": "Abram Handler, Su Lin Blodgett, Brendan O'Connor", "title": "Visualizing textual models with in-text and word-as-pixel highlighting", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:30:19 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Handler", "Abram", ""], ["Blodgett", "Su Lin", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1606.06357", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon, Johannes Welbl, Sebastian Riedel, \\'Eric Gaussier,\n  Guillaume Bouchard", "title": "Complex Embeddings for Simple Link Prediction", "comments": "10+2 pages, accepted at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical relational learning, the link prediction problem is key to\nautomatically understand the structure of large knowledge bases. As in previous\nstudies, we propose to solve this problem through latent factorization.\nHowever, here we make use of complex valued embeddings. The composition of\ncomplex embeddings can handle a large variety of binary relations, among them\nsymmetric and antisymmetric relations. Compared to state-of-the-art models such\nas Neural Tensor Network and Holographic Embeddings, our approach based on\ncomplex embeddings is arguably simpler, as it only uses the Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors. Our approach is scalable to large datasets as it remains linear in\nboth space and time, while consistently outperforming alternative approaches on\nstandard link prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:52:48 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""], ["Gaussier", "\u00c9ric", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1606.06361", "submitter": "Abulhair Saparov", "authors": "Abulhair Saparov, Tom M. Mitchell", "title": "A Probabilistic Generative Grammar for Semantic Parsing", "comments": "[manuscript draft]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework that couples the syntax and semantics of natural\nlanguage sentences in a generative model, in order to develop a semantic parser\nthat jointly infers the syntactic, morphological, and semantic representations\nof a given sentence under the guidance of background knowledge. To generate a\nsentence in our framework, a semantic statement is first sampled from a prior,\nsuch as from a set of beliefs in a knowledge base. Given this semantic\nstatement, a grammar probabilistically generates the output sentence. A joint\nsemantic-syntactic parser is derived that returns the $k$-best semantic and\nsyntactic parses for a given sentence. The semantic prior is flexible, and can\nbe used to incorporate background knowledge during parsing, in ways unlike\nprevious semantic parsing approaches. For example, semantic statements\ncorresponding to beliefs in a knowledge base can be given higher prior\nprobability, type-correct statements can be given somewhat lower probability,\nand beliefs outside the knowledge base can be given lower probability. The\nconstruction of our grammar invokes a novel application of hierarchical\nDirichlet processes (HDPs), which in turn, requires a novel and efficient\ninference approach. We present experimental results showing, for a simple\ngrammar, that our parser outperforms a state-of-the-art CCG semantic parser and\nscales to knowledge bases with millions of beliefs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:29:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Saparov", "Abulhair", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "1606.06364", "submitter": "Lovenoor Aulck", "authors": "Lovenoor Aulck and Nishant Velagapudi and Joshua Blumenstock and Jevin\n  West", "title": "Predicting Student Dropout in Higher Education", "comments": "Presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, roughly 30% of first-year students at US baccalaureate\ninstitutions do not return for their second year and over $9 billion is spent\neducating these students. Yet, little quantitative research has analyzed the\ncauses and possible remedies for student attrition. Here, we describe initial\nefforts to model student dropout using the largest known dataset on higher\neducation attrition, which tracks over 32,500 students' demographics and\ntranscript records at one of the nation's largest public universities. Our\nresults highlight several early indicators of student attrition and show that\ndropout can be accurately predicted even when predictions are based on a single\nterm of academic transcript data. These results highlight the potential for\nmachine learning to have an impact on student retention and success while\npointing to several promising directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:41:19 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 00:50:55 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 21:41:47 GMT"}, {"version": "v4", "created": "Tue, 7 Mar 2017 22:50:28 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Aulck", "Lovenoor", ""], ["Velagapudi", "Nishant", ""], ["Blumenstock", "Joshua", ""], ["West", "Jevin", ""]]}, {"id": "1606.06366", "submitter": "Bo Tang", "authors": "Bo Tang, Haibo He", "title": "FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text\n  Categorization", "comments": "8 pages, 6 figures, World Congress on Intelligent Control and\n  Automation, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new wrapper feature selection approach based on\nJensen-Shannon (JS) divergence, termed feature selection with maximum\nJS-divergence (FSMJ), for text categorization. Unlike most existing feature\nselection approaches, the proposed FSMJ approach is based on real-valued\nfeatures which provide more information for discrimination than binary-valued\nfeatures used in conventional approaches. We show that the FSMJ is a greedy\napproach and the JS-divergence monotonically increases when more features are\nselected. We conduct several experiments on real-life data sets, compared with\nthe state-of-the-art feature selection approaches for text categorization. The\nsuperior performance of the proposed FSMJ approach demonstrates its\neffectiveness and further indicates its wide potential applications on data\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:58:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tang", "Bo", ""], ["He", "Haibo", ""]]}, {"id": "1606.06377", "submitter": "Bo Tang", "authors": "Bo Tang, Paul M. Baggenstoss, Haibo He", "title": "Kernel-based Generative Learning in Distortion Feature Space", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel kernel-based generative classifier which is\ndefined in a distortion subspace using polynomial series expansion, named\nKernel-Distortion (KD) classifier. An iterative kernel selection algorithm is\ndeveloped to steadily improve classification performance by repeatedly removing\nand adding kernels. The experimental results on character recognition\napplication not only show that the proposed generative classifier performs\nbetter than many existing classifiers, but also illustrate that it has\ndifferent recognition capability compared to the state-of-the-art\ndiscriminative classifier - deep belief network. The recognition diversity\nindicates that a hybrid combination of the proposed generative classifier and\nthe discriminative classifier could further improve the classification\nperformance. Two hybrid combination methods, cascading and stacking, have been\nimplemented to verify the diversity and the improvement of the proposed\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 00:45:35 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tang", "Bo", ""], ["Baggenstoss", "Paul M.", ""], ["He", "Haibo", ""]]}, {"id": "1606.06439", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL, NEUROSPIN), Matthieu Kowalski (PARIETAL,\n  L2S), Bertrand Thirion (NEUROSPIN, PARIETAL)", "title": "Social-sparsity brain decoders: faster spatial sparsity", "comments": "in Pattern Recognition in NeuroImaging, Jun 2016, Trento, Italy. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-sparse predictors are good models for brain decoding: they give\naccurate predictions and their weight maps are interpretable as they focus on a\nsmall number of regions. However, the state of the art, based on total\nvariation or graph-net, is computationally costly. Here we introduce sparsity\nin the local neighborhood of each voxel with social-sparsity, a structured\nshrinkage operator. We find that, on brain imaging classification problems,\nsocial-sparsity performs almost as well as total-variation models and better\nthan graph-net, for a fraction of the computational cost. It also very clearly\noutlines predictive regions. We give details of the model and the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 06:51:57 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"], ["Kowalski", "Matthieu", "", "PARIETAL,\n  L2S"], ["Thirion", "Bertrand", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1606.06564", "submitter": "Alessandro Fontana", "authors": "Alessandro Fontana", "title": "An artificial neural network to find correlation patterns in an\n  arbitrary number of variables", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods to find correlation among variables are of interest to many\ndisciplines, including statistics, machine learning, (big) data mining and\nneurosciences. Parameters that measure correlation between two variables are of\nlimited utility when used with multiple variables. In this work, I propose a\nsimple criterion to measure correlation among an arbitrary number of variables,\nbased on a data set. The central idea is to i) design a function of the\nvariables that can take different forms depending on a set of parameters, ii)\ncalculate the difference between a statistics associated to the function\ncomputed on the data set and the same statistics computed on a randomised\nversion of the data set, called \"scrambled\" data set, and iii) optimise the\nparameters to maximise this difference. Many such functions can be organised in\nlayers, which can in turn be stacked one on top of the other, forming a neural\nnetwork. The function parameters are searched with an enhanced genetic\nalgortihm called POET and the resulting method is tested on a cancer gene data\nset. The method may have potential implications for some issues that affect the\nfield of neural networks, such as overfitting, the need to process huge amounts\nof data for training and the presence of \"adversarial examples\".\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:35:43 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 17:52:10 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Fontana", "Alessandro", ""]]}, {"id": "1606.06959", "submitter": "David Barber", "authors": "David Barber, Aleksandar Botev", "title": "Dealing with a large number of classes -- Likelihood, Discrimination or\n  Ranking?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider training probabilistic classifiers in the case of a large number\nof classes. The number of classes is assumed too large to perform exact\nnormalisation over all classes. To account for this we consider a simple\napproach that directly approximates the likelihood. We show that this simple\napproach works well on toy problems and is competitive with recently introduced\nalternative non-likelihood based approximations. Furthermore, we relate this\napproach to a simple ranking objective. This leads us to suggest a specific\nsetting for the optimal threshold in the ranking objective.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 14:10:47 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 12:18:40 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Barber", "David", ""], ["Botev", "Aleksandar", ""]]}, {"id": "1606.06962", "submitter": "Andreas Loukas", "authors": "Nathanael Perraudin and Andreas Loukas and Francesco Grassi and Pierre\n  Vandergheynst", "title": "Towards stationary time-vertex signal processing", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods for signal processing have shown promise for the analysis\nof data exhibiting irregular structure, such as those found in social,\ntransportation, and sensor networks. Yet, though these systems are often\ndynamic, state-of-the-art methods for signal processing on graphs ignore the\ndimension of time, treating successive graph signals independently or taking a\nglobal average. To address this shortcoming, this paper considers the\nstatistical analysis of time-varying graph signals. We introduce a novel\ndefinition of joint (time-vertex) stationarity, which generalizes the classical\ndefinition of time stationarity and the more recent definition appropriate for\ngraphs. Joint stationarity gives rise to a scalable Wiener optimization\nframework for joint denoising, semi-supervised learning, or more generally\ninversing a linear operator, that is provably optimal. Experimental results on\nreal weather data demonstrate that taking into account graph and time\ndimensions jointly can yield significant accuracy improvements in the\nreconstruction effort.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 14:33:15 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Loukas", "Andreas", ""], ["Grassi", "Francesco", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1606.06997", "submitter": "Charles Garfinkle", "authors": "Charles J. Garfinkle and Christopher J. Hillar", "title": "On the uniqueness and stability of dictionaries for sparse\n  representation of noisy signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning optimal dictionaries for sparse coding has exposed characteristic\nsparse features of many natural signals. However, universal guarantees of the\nstability of such features in the presence of noise are lacking. Here, we\nprovide very general conditions guaranteeing when dictionaries yielding the\nsparsest encodings are unique and stable with respect to measurement or\nmodeling error. We demonstrate that some or all original dictionary elements\nare recoverable from noisy data even if the dictionary fails to satisfy the\nspark condition, its size is overestimated, or only a polynomial number of\ndistinct sparse supports appear in the data. Importantly, we derive these\nguarantees without requiring any constraints on the recovered dictionary beyond\na natural upper bound on its size. Our results also yield an effective\nprocedure sufficient to affirm if a proposed solution to the dictionary\nlearning problem is unique within bounds commensurate with the noise. We\nsuggest applications to data analysis, engineering, and neuroscience and close\nwith some remaining challenges left open by our work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 16:06:29 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 07:14:12 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 07:11:28 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 20:31:01 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Garfinkle", "Charles J.", ""], ["Hillar", "Christopher J.", ""]]}, {"id": "1606.07025", "submitter": "Luis Mu\\~noz-Gonz\\'alez", "authors": "Luis Mu\\~noz-Gonz\\'alez, Daniele Sgandurra, Andrea Paudice, Emil C.\n  Lupu", "title": "Efficient Attack Graph Analysis through Approximate Inference", "comments": "30 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attack graphs provide compact representations of the attack paths that an\nattacker can follow to compromise network resources by analysing network\nvulnerabilities and topology. These representations are a powerful tool for\nsecurity risk assessment. Bayesian inference on attack graphs enables the\nestimation of the risk of compromise to the system's components given their\nvulnerabilities and interconnections, and accounts for multi-step attacks\nspreading through the system. Whilst static analysis considers the risk posture\nat rest, dynamic analysis also accounts for evidence of compromise, e.g. from\nSIEM software or forensic investigation. However, in this context, exact\nBayesian inference techniques do not scale well. In this paper we show how\nLoopy Belief Propagation - an approximate inference technique - can be applied\nto attack graphs, and that it scales linearly in the number of nodes for both\nstatic and dynamic analysis, making such analyses viable for larger networks.\nWe experiment with different topologies and network clustering on synthetic\nBayesian attack graphs with thousands of nodes to show that the algorithm's\naccuracy is acceptable and converge to a stable solution. We compare sequential\nand parallel versions of Loopy Belief Propagation with exact inference\ntechniques for both static and dynamic analysis, showing the advantages of\napproximate inference techniques to scale to larger attack graphs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 17:48:17 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Sgandurra", "Daniele", ""], ["Paudice", "Andrea", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1606.07035", "submitter": "Sara Magliacane", "authors": "Sara Magliacane, Tom Claassen, Joris M. Mooij", "title": "Ancestral Causal Inference", "comments": "In Proceedings of Advances in Neural Information Processing Systems\n  29 (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based causal discovery from limited data is a notoriously\ndifficult challenge due to the many borderline independence test decisions.\nSeveral approaches to improve the reliability of the predictions by exploiting\nredundancy in the independence information have been proposed recently. Though\npromising, existing approaches can still be greatly improved in terms of\naccuracy and scalability. We present a novel method that reduces the\ncombinatorial explosion of the search space by using a more coarse-grained\nrepresentation of causal information, drastically reducing computation time.\nAdditionally, we propose a method to score causal predictions based on their\nconfidence. Crucially, our implementation also allows one to easily combine\nobservational and interventional data and to incorporate various types of\navailable background knowledge. We prove soundness and asymptotic consistency\nof our method and demonstrate that it can outperform the state-of-the-art on\nsynthetic data, achieving a speedup of several orders of magnitude. We\nillustrate its practical feasibility by applying it on a challenging protein\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 18:26:27 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 22:23:32 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 14:26:27 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Magliacane", "Sara", ""], ["Claassen", "Tom", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1606.07043", "submitter": "David Kale", "authors": "Kyle Reing, David C. Kale, Greg Ver Steeg, Aram Galstyan", "title": "Toward Interpretable Topic Discovery via Anchored Correlation\n  Explanation", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many predictive tasks, such as diagnosing a patient based on their medical\nchart, are ultimately defined by the decisions of human experts. Unfortunately,\nencoding experts' knowledge is often time consuming and expensive. We propose a\nsimple way to use fuzzy and informal knowledge from experts to guide discovery\nof interpretable latent topics in text. The underlying intuition of our\napproach is that latent factors should be informative about both correlations\nin the data and a set of relevance variables specified by an expert.\nMathematically, this approach is a combination of the information bottleneck\nand Total Correlation Explanation (CorEx). We give a preliminary evaluation of\nAnchored CorEx, showing that it produces more coherent and interpretable topics\non two distinct corpora.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:00:38 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Reing", "Kyle", ""], ["Kale", "David C.", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.07081", "submitter": "Lalit Jain", "authors": "Lalit Jain, Kevin Jamieson, Robert Nowak", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of ordinal embedding is to represent items as points in a\nlow-dimensional Euclidean space given a set of constraints in the form of\ndistance comparisons like \"item $i$ is closer to item $j$ than item $k$\".\nOrdinal constraints like this often come from human judgments. To account for\nerrors and variation in judgments, we consider the noisy situation in which the\ngiven constraints are independently corrupted by reversing the correct\nconstraint with some probability. This paper makes several new contributions to\nthis problem. First, we derive prediction error bounds for ordinal embedding\nwith noise by exploiting the fact that the rank of a distance matrix of points\nin $\\mathbb{R}^d$ is at most $d+2$. These bounds characterize how well a\nlearned embedding predicts new comparative judgments. Second, we investigate\nthe special case of a known noise model and study the Maximum Likelihood\nestimator. Third, knowledge of the noise model enables us to relate prediction\nerrors to embedding accuracy. This relationship is highly non-trivial since we\nshow that the linear map corresponding to distance comparisons is\nnon-invertible, but there exists a nonlinear map that is invertible. Fourth,\ntwo new algorithms for ordinal embedding are proposed and evaluated in\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:06:10 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Jain", "Lalit", ""], ["Jamieson", "Kevin", ""], ["Nowak", "Robert", ""]]}, {"id": "1606.07112", "submitter": "Tom Zahavy", "authors": "Nir Ben Zrihem, Tom Zahavy, Shie Mannor", "title": "Visualizing Dynamics: from t-SNE to SEMI-MDPs", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) is a trending field of research, showing\ngreat promise in many challenging problems such as playing Atari, solving Go\nand controlling robots. While DRL agents perform well in practice we are still\nmissing the tools to analayze their performance and visualize the temporal\nabstractions that they learn. In this paper, we present a novel method that\nautomatically discovers an internal Semi Markov Decision Process (SMDP) model\nin the Deep Q Network's (DQN) learned representation. We suggest a novel\nvisualization method that represents the SMDP model by a directed graph and\nvisualize it above a t-SNE map. We show how can we interpret the agent's policy\nand give evidence for the hierarchical state aggregation that DQNs are learning\nautomatically. Our algorithm is fully automatic, does not require any domain\nspecific knowledge and is evaluated by a novel likelihood based evaluation\ncriteria.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 21:18:50 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Zrihem", "Nir Ben", ""], ["Zahavy", "Tom", ""], ["Mannor", "Shie", ""]]}, {"id": "1606.07129", "submitter": "Behnoush Abdollahi", "authors": "Behnoush Abdollahi, Olfa Nasraoui", "title": "Explainable Restricted Boltzmann Machines for Collaborative Filtering", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most accurate recommender systems are black-box models, hiding the reasoning\nbehind their recommendations. Yet explanations have been shown to increase the\nuser's trust in the system in addition to providing other benefits such as\nscrutability, meaning the ability to verify the validity of recommendations.\nThis gap between accuracy and transparency or explainability has generated an\ninterest in automated explanation generation methods. Restricted Boltzmann\nMachines (RBM) are accurate models for CF that also lack interpretability. In\nthis paper, we focus on RBM based collaborative filtering recommendations, and\nfurther assume the absence of any additional data source, such as item content\nor user attributes. We thus propose a new Explainable RBM technique that\ncomputes the top-n recommendation list from items that are explainable.\nExperimental results show that our method is effective in generating accurate\nand explainable recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 22:24:30 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Abdollahi", "Behnoush", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "1606.07153", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Rachael Meager, Jonathan Huggins,\n  Michael Jordan", "title": "Fast robustness quantification with variational Bayes", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:19:17 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Meager", "Rachael", ""], ["Huggins", "Jonathan", ""], ["Jordan", "Michael", ""]]}, {"id": "1606.07163", "submitter": "William Souillard-Mandar", "authors": "William Souillard-Mandar, Randall Davis, Cynthia Rudin, Rhoda Au, Dana\n  Penney", "title": "Interpretable Machine Learning Models for the Digital Clock Drawing Test", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular\nneuropsychological screening tool for cognitive conditions. The Digital Clock\nDrawing Test (dCDT) uses novel software to analyze data from a digitizing\nballpoint pen that reports its position with considerable spatial and temporal\nprecision, making possible the analysis of both the drawing process and final\nproduct. We developed methodology to analyze pen stroke data from these\ndrawings, and computed a large collection of features which were then analyzed\nwith a variety of machine learning techniques. The resulting scoring systems\nwere designed to be more accurate than the systems currently used by\nclinicians, but just as interpretable and easy to use. The systems also allow\nus to quantify the tradeoff between accuracy and interpretability. We created\nautomated versions of the CDT scoring systems currently used by clinicians,\nallowing us to benchmark our models, which indicated that our machine learning\nmodels substantially outperformed the existing scoring systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 02:08:58 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Souillard-Mandar", "William", ""], ["Davis", "Randall", ""], ["Rudin", "Cynthia", ""], ["Au", "Rhoda", ""], ["Penney", "Dana", ""]]}, {"id": "1606.07240", "submitter": "Emilie Morvant", "authors": "Anil Goyal (AMA, LHC), Emilie Morvant (LHC), Pascal Germain (SIERRA),\n  Massih-Reza Amini (AMA)", "title": "PAC-Bayesian Analysis for a two-step Hierarchical Multiview Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a two-level multiview learning with more than two views under the\nPAC-Bayesian framework. This approach, sometimes referred as late fusion,\nconsists in learning sequentially multiple view-specific classifiers at the\nfirst level, and then combining these view-specific classifiers at the second\nlevel. Our main theoretical result is a generalization bound on the risk of the\nmajority vote which exhibits a term of diversity in the predictions of the\nview-specific classifiers. From this result it comes out that controlling the\ntrade-off between diversity and accuracy is a key element for multiview\nlearning, which complements other results in multiview learning. Finally, we\nexperiment our principle on multiview datasets extracted from the Reuters\nRCV1/RCV2 collection.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:29:53 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 13:50:41 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 13:27:04 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Goyal", "Anil", "", "AMA, LHC"], ["Morvant", "Emilie", "", "LHC"], ["Germain", "Pascal", "", "SIERRA"], ["Amini", "Massih-Reza", "", "AMA"]]}, {"id": "1606.07251", "submitter": "Florian Colombo", "authors": "Florian Colombo, Samuel P. Muscinelli, Alexander Seeholzer, Johanni\n  Brea and Wulfram Gerstner", "title": "Algorithmic Composition of Melodies with Deep Recurrent Neural Networks", "comments": "Proceeding of the 1st Conference on Computer Simulation of Musical\n  Creativity, Huddersfield University", "journal-ref": null, "doi": "10.13140/RG.2.1.2436.5683", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big challenge in algorithmic composition is to devise a model that is both\neasily trainable and able to reproduce the long-range temporal dependencies\ntypical of music. Here we investigate how artificial neural networks can be\ntrained on a large corpus of melodies and turned into automated music composers\nable to generate new melodies coherent with the style they have been trained\non. We employ gated recurrent unit networks that have been shown to be\nparticularly efficient in learning complex sequential activations with\narbitrary long time lags. Our model processes rhythm and melody in parallel\nwhile modeling the relation between these two features. Using such an approach,\nwe were able to generate interesting complete melodies or suggest possible\ncontinuations of a melody fragment that is coherent with the characteristics of\nthe fragment itself.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:53:30 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Colombo", "Florian", ""], ["Muscinelli", "Samuel P.", ""], ["Seeholzer", "Alexander", ""], ["Brea", "Johanni", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1606.07268", "submitter": "Anru Zhang", "authors": "Anru Zhang and Lawrence D. Brown and T. Tony Cai", "title": "Semi-supervised Inference: General Theory and Estimation of Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a general semi-supervised inference framework focused on the\nestimation of the population mean. As usual in semi-supervised settings, there\nexists an unlabeled sample of covariate vectors and a labeled sample consisting\nof covariate vectors along with real-valued responses (\"labels\"). Otherwise,\nthe formulation is \"assumption-lean\" in that no major conditions are imposed on\nthe statistical or functional form of the data. We consider both the ideal\nsemi-supervised setting where infinitely many unlabeled samples are available,\nas well as the ordinary semi-supervised setting in which only a finite number\nof unlabeled samples is available.\n  Estimators are proposed along with corresponding confidence intervals for the\npopulation mean. Theoretical analysis on both the asymptotic distribution and\n$\\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed\nestimators, based on a simple form of the least squares method, outperform the\nordinary sample mean. The simple, transparent form of the estimator lends\nconfidence to the perception that its asymptotic improvement over the ordinary\nsample mean also nearly holds even for moderate size samples. The method is\nfurther extended to a nonparametric setting, in which the oracle rate can be\nachieved asymptotically. The proposed estimators are further illustrated by\nsimulation studies and a real data example involving estimation of the homeless\npopulation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:53:05 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:07:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhang", "Anru", ""], ["Brown", "Lawrence D.", ""], ["Cai", "T. Tony", ""]]}, {"id": "1606.07279", "submitter": "Remi Flamary", "authors": "Devis Tuia, R\\'emi Flamary, Nicolas Courty", "title": "Multiclass feature learning for hyperspectral image classification:\n  sparse and hierarchical solutions", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 105,\n  July 2015, Pages 272-285", "doi": "10.1016/j.isprsjprs.2015.01.006", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the question of discovering an effective set of\nspatial filters to solve hyperspectral classification problems. Instead of\nfixing a priori the filters and their parameters using expert knowledge, we let\nthe model find them within random draws in the (possibly infinite) space of\npossible filters. We define an active set feature learner that includes in the\nmodel only features that improve the classifier. To this end, we consider a\nfast and linear classifier, multiclass logistic classification, and show that\nwith a good representation (the filters discovered), such a simple classifier\ncan reach at least state of the art performances. We apply the proposed active\nset learner in four hyperspectral image classification problems, including\nagricultural and urban classification at different resolutions, as well as\nmultimodal data. We also propose a hierarchical setting, which allows to\ngenerate more complex banks of features that can better describe the\nnonlinearities present in the data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:05:23 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tuia", "Devis", ""], ["Flamary", "R\u00e9mi", ""], ["Courty", "Nicolas", ""]]}, {"id": "1606.07282", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Concha Bielza and Pedro Larra\\~naga", "title": "A review of Gaussian Markov models for conditional independence", "comments": "Fix author signature", "journal-ref": "Journal of Statistical Planning and Inference, 206:127-144, 2020", "doi": "10.1016/j.jspi.2019.09.008", "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov models lie at the interface between statistical independence in a\nprobability distribution and graph separation properties. We review model\nselection and estimation in directed and undirected Markov models with Gaussian\nparametrization, emphasizing the main similarities and differences. These two\nmodel classes are similar but not equivalent, although they share a common\nintersection. We present the existing results from a historical perspective,\ntaking into account the amount of literature existing from both the artificial\nintelligence and statistics research communities, where these models were\noriginated. We cover classical topics such as maximum likelihood estimation and\nmodel selection via hypothesis testing, but also more modern approaches like\nregularization and Bayesian methods. We also discuss how the Markov models\nreviewed fit in the rich hierarchy of other, higher level Markov model classes.\nFinally, we close the paper overviewing relaxations of the Gaussian assumption\nand pointing out the main areas of application where these Markov models are\nnowadays used.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:12:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 16:27:04 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 11:10:43 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 08:35:01 GMT"}, {"version": "v5", "created": "Wed, 2 Oct 2019 08:33:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1606.07285", "submitter": "Wojciech Samek", "authors": "Farhad Arbabzadah and Gr\\'egoire Montavon and Klaus-Robert M\\\"uller\n  and Wojciech Samek", "title": "Identifying individual facial expressions by deconstructing a neural\n  network", "comments": "12 pages, 7 figures, Paper accepted for GCPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of explaining predictions of psychological\nattributes such as attractiveness, happiness, confidence and intelligence from\nface photographs using deep neural networks. Since psychological attribute\ndatasets typically suffer from small sample sizes, we apply transfer learning\nwith two base models to avoid overfitting. These models were trained on an age\nand gender prediction task, respectively. Using a novel explanation method we\nextract heatmaps that highlight the parts of the image most responsible for the\nprediction. We further observe that the explanation method provides important\ninsights into the nature of features of the base model, which allow one to\nassess the aptitude of the base model for a given transfer learning task.\nFinally, we observe that the multiclass model is more feature rich than its\nbinary counterpart. The experimental evaluation is performed on the 2222 images\nfrom the 10k US faces dataset containing psychological attribute labels as well\nas on a subset of KDEF images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:24:45 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 00:41:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Arbabzadah", "Farhad", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07289", "submitter": "Remi Flamary", "authors": "Devis Tuia, Remi Flamary, Michel Barlaud", "title": "Non-convex regularization in remote sensing", "comments": "11 pages, 11 figures", "journal-ref": "Geoscience and Remote Sensing, IEEE Transactions on, 2016", "doi": "10.1109/TGRS.2016.2585201", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effect of different regularizers and their\nimplications in high dimensional image classification and sparse linear\nunmixing. Although kernelization or sparse methods are globally accepted\nsolutions for processing data in high dimensions, we present here a study on\nthe impact of the form of regularization used and its parametrization. We\nconsider regularization via traditional squared (2) and sparsity-promoting (1)\nnorms, as well as more unconventional nonconvex regularizers (p and Log Sum\nPenalty). We compare their properties and advantages on several classification\nand linear unmixing tasks and provide advices on the choice of the best\nregularizer for the problem at hand. Finally, we also provide a fully\nfunctional toolbox for the community.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:36:01 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Tuia", "Devis", ""], ["Flamary", "Remi", ""], ["Barlaud", "Michel", ""]]}, {"id": "1606.07298", "submitter": "Wojciech Samek", "authors": "Leila Arras and Franziska Horn and Gr\\'egoire Montavon and\n  Klaus-Robert M\\\"uller and Wojciech Samek", "title": "Explaining Predictions of Non-Linear Classifiers in NLP", "comments": "7 pages, 3 figures, Paper accepted for 1st Workshop on Representation\n  Learning for NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:53:31 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07312", "submitter": "Maximilian Karl", "authors": "Maximilian Karl, Justin Bayer, Patrick van der Smagt", "title": "Unsupervised preprocessing for Tactile Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile information is important for gripping, stable grasp, and in-hand\nmanipulation, yet the complexity of tactile data prevents widespread use of\nsuch sensors. We make use of an unsupervised learning algorithm that transforms\nthe complex tactile data into a compact, latent representation without the need\nto record ground truth reference data. These compact representations can either\nbe used directly in a reinforcement learning based controller or can be used to\ncalibrate the tactile sensor to physical quantities with only a few datapoints.\nWe show the quality of our latent representation by predicting important\nfeatures and with a simple control task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:44:28 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Karl", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1606.07326", "submitter": "Wei Pan", "authors": "Wei Pan and Hao Dong and Yike Guo", "title": "DropNeuron: Simplifying the Structure of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning using multi-layer neural networks (NNs) architecture manifests\nsuperb power in modern machine learning systems. The trained Deep Neural\nNetworks (DNNs) are typically large. The question we would like to address is\nwhether it is possible to simplify the NN during training process to achieve a\nreasonable performance within an acceptable computational time. We presented a\nnovel approach of optimising a deep neural network through regularisation of\nnet- work architecture. We proposed regularisers which support a simple\nmechanism of dropping neurons during a network training process. The method\nsupports the construction of a simpler deep neural networks with compatible\nperformance with its simplified version. As a proof of concept, we evaluate the\nproposed method with examples including sparse linear regression, deep\nautoencoder and convolutional neural network. The valuations demonstrate\nexcellent performance.\n  The code for this work can be found in\nhttp://www.github.com/panweihit/DropNeuron\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 14:30:36 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 12:50:27 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 09:39:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pan", "Wei", ""], ["Dong", "Hao", ""], ["Guo", "Yike", ""]]}, {"id": "1606.07365", "submitter": "Jian Zhang", "authors": "Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, Christopher R\\'e", "title": "Parallel SGD: When does averaging help?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a number of workers running SGD independently on the same pool of\ndata and averaging the models every once in a while -- a common but not well\nunderstood practice. We study model averaging as a variance-reducing mechanism\nand describe two ways in which the frequency of averaging affects convergence.\nFor convex objectives, we show the benefit of frequent averaging depends on the\ngradient variance envelope. For non-convex objectives, we illustrate that this\nbenefit depends on the presence of multiple globally optimal points. We\ncomplement our findings with multicore experiments on both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:23:35 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Zhang", "Jian", ""], ["De Sa", "Christopher", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.07369", "submitter": "David Dooling", "authors": "David Dooling, Angela Kim, Barbara McAneny, Jennifer Webster", "title": "Personalized Prognostic Models for Oncology: A Machine Learning Approach", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have applied a little-known data transformation to subsets of the\nSurveillance, Epidemiology, and End Results (SEER) publically available data of\nthe National Cancer Institute (NCI) to make it suitable input to standard\nmachine learning classifiers. This transformation properly treats the\nright-censored data in the SEER data and the resulting Random Forest and\nMulti-Layer Perceptron models predict full survival curves. Treating the 6, 12,\nand 60 months points of the resulting survival curves as 3 binary classifiers,\nthe 18 resulting classifiers have AUC values ranging from .765 to .885. Further\nevidence that the models have generalized well from the training data is\nprovided by the extremely high levels of agreement between the random forest\nand neural network models predictions on the 6, 12, and 60 month binary\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:55:22 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Dooling", "David", ""], ["Kim", "Angela", ""], ["McAneny", "Barbara", ""], ["Webster", "Jennifer", ""]]}, {"id": "1606.07470", "submitter": "Shankar Kumar", "authors": "Babak Damavandi, Shankar Kumar, Noam Shazeer and Antoine Bruguier", "title": "NN-grams: Unifying neural network and n-gram language models for Speech\n  Recognition", "comments": "To be published in the proceedings of INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NN-grams, a novel, hybrid language model integrating n-grams and\nneural networks (NN) for speech recognition. The model takes as input both word\nhistories as well as n-gram counts. Thus, it combines the memorization capacity\nand scalability of an n-gram model with the generalization ability of neural\nnetworks. We report experiments where the model is trained on 26B words.\nNN-grams are efficient at run-time since they do not include an output soft-max\nlayer. The model is trained using noise contrastive estimation (NCE), an\napproach that transforms the estimation problem of neural networks into one of\nbinary classification between data samples and noise samples. We present\nresults with noise samples derived from either an n-gram distribution or from\nspeech recognition lattices. NN-grams outperforms an n-gram model on an Italian\nspeech recognition dictation task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 20:37:06 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Damavandi", "Babak", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Bruguier", "Antoine", ""]]}, {"id": "1606.07545", "submitter": "Camille Jandot", "authors": "Camille Jandot, Patrice Simard, Max Chickering, David Grangier, Jina\n  Suh", "title": "Interactive Semantic Featuring for Text Classification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In text classification, dictionaries can be used to define\nhuman-comprehensible features. We propose an improvement to dictionary features\ncalled smoothed dictionary features. These features recognize document contexts\ninstead of n-grams. We describe a principled methodology to solicit dictionary\nfeatures from a teacher, and present results showing that models built using\nthese human-comprehensible features are competitive with models trained with\nBag of Words features.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 02:28:24 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Jandot", "Camille", ""], ["Simard", "Patrice", ""], ["Chickering", "Max", ""], ["Grangier", "David", ""], ["Suh", "Jina", ""]]}, {"id": "1606.07575", "submitter": "Arash Shahriari", "authors": "Arash Shahriari", "title": "Multipartite Ranking-Selection of Low-Dimensional Instances by\n  Supervised Projection to High-Dimensional Space", "comments": "15 pages, 1 figure, 2 tables, 3 algorithms, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning of redundant or irrelevant instances of data is a key to every\nsuccessful solution for pattern recognition. In this paper, we present a novel\nranking-selection framework for low-length but highly correlated instances.\nInstead of working in the low-dimensional instance space, we learn a supervised\nprojection to high-dimensional space spanned by the number of classes in the\ndataset under study. Imposing higher distinctions via exposing the notion of\nlabels to the instances, lets to deploy one versus all ranking for each\nindividual classes and selecting quality instances via adaptive thresholding of\nthe overall scores. To prove the efficiency of our paradigm, we employ it for\nthe purpose of texture understanding which is a hard recognition challenge due\nto high similarity of texture pixels and low dimensionality of their color\nfeatures. Our experiments show considerable improvements in recognition\nperformance over other local descriptors on several publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:15:45 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Shahriari", "Arash", ""]]}, {"id": "1606.07578", "submitter": "Bienvenue Kouwaye", "authors": "Bienvenue Kouway\\`e", "title": "Regression Trees and Random forest based feature selection for malaria\n  risk exposure prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with prediction of anopheles number, the main vector of\nmalaria risk, using environmental and climate variables. The variables\nselection is based on an automatic machine learning method using regression\ntrees, and random forests combined with stratified two levels cross validation.\nThe minimum threshold of variables importance is accessed using the quadratic\ndistance of variables importance while the optimal subset of selected variables\nis used to perform predictions. Finally the results revealed to be\nqualitatively better, at the selection, the prediction , and the CPU time point\nof view than those obtained by GLM-Lasso method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:34:17 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Kouway\u00e8", "Bienvenue", ""]]}, {"id": "1606.07636", "submitter": "Matthieu Geist", "authors": "Matthieu Geist and Bilal Piot and Olivier Pietquin", "title": "Is the Bellman residual a bad proxy?", "comments": "Final NIPS 2017 version (title, among other things, changed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at theoretically and empirically comparing two standard\noptimization criteria for Reinforcement Learning: i) maximization of the mean\nvalue and ii) minimization of the Bellman residual. For that purpose, we place\nourselves in the framework of policy search algorithms, that are usually\ndesigned to maximize the mean value, and derive a method that minimizes the\nresidual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis\nshows how good this proxy is to policy optimization, and notably that it is\nbetter than its value-based counterpart. We also propose experiments on\nrandomly generated generic Markov decision processes, specifically designed for\nstudying the influence of the involved concentrability coefficient. They show\nthat the Bellman residual is generally a bad proxy to policy optimization and\nthat directly maximizing the mean value is much better, despite the current\nlack of deep theoretical analysis. This might seem obvious, as directly\naddressing the problem of interest is usually better, but given the prevalence\nof (projected) Bellman residual minimization in value-based reinforcement\nlearning, we believe that this question is worth to be considered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 10:54:41 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 11:17:04 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 14:17:46 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Geist", "Matthieu", ""], ["Piot", "Bilal", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1606.07686", "submitter": "Lei Zhang", "authors": "Houman Owhadi and Lei Zhang", "title": "Gamblets for opening the complexity-bottleneck of implicit schemes for\n  hyperbolic and parabolic ODEs/PDEs with rough coefficients", "comments": "55 pages. 26 figures", "journal-ref": "Journal of Computational Physics, 347, 99-128, 2017", "doi": "10.1016/j.jcp.2017.06.037", "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit schemes are popular methods for the integration of time dependent\nPDEs such as hyperbolic and parabolic PDEs. However the necessity to solve\ncorresponding linear systems at each time step constitutes a complexity\nbottleneck in their application to PDEs with rough coefficients. We present a\ngeneralization of gamblets introduced in \\cite{OwhadiMultigrid:2015} enabling\nthe resolution of these implicit systems in near-linear complexity and provide\nrigorous a-priori error bounds on the resulting numerical approximations of\nhyperbolic and parabolic PDEs. These generalized gamblets induce a\nmultiresolution decomposition of the solution space that is adapted to both the\nunderlying (hyperbolic and parabolic) PDE (and the system of ODEs resulting\nfrom space discretization) and to the time-steps of the numerical scheme.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 13:56:45 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 07:14:47 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Owhadi", "Houman", ""], ["Zhang", "Lei", ""]]}, {"id": "1606.07781", "submitter": "Peter Bull", "authors": "Peter Bull, Isaac Slavitt, Greg Lipstein", "title": "Harnessing the Power of the Crowd to Increase Capacity for Data Science\n  in the Social Sector", "comments": "Presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present three case studies of organizations using a data science\ncompetition to answer a pressing question. The first is in education where a\nnonprofit that creates smart school budgets wanted to automatically tag budget\nline items. The second is in public health, where a low-cost, nonprofit women's\nhealth care provider wanted to understand the effect of demographic and\nbehavioral questions on predicting which services a woman would need. The third\nand final example is in government innovation: using online restaurant reviews\nfrom Yelp, competitors built models to forecast which restaurants were most\nlikely to have hygiene violations when visited by health inspectors. Finally,\nwe reflect on the unique benefits of the open, public competition model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 18:30:35 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Bull", "Peter", ""], ["Slavitt", "Isaac", ""], ["Lipstein", "Greg", ""]]}, {"id": "1606.07792", "submitter": "Heng-Tze Cheng", "authors": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar\n  Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa\n  Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,\n  Hemal Shah", "title": "Wide & Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide & Deep learning---jointly trained wide linear models and deep\nneural networks---to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide & Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 19:07:02 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cheng", "Heng-Tze", ""], ["Koc", "Levent", ""], ["Harmsen", "Jeremiah", ""], ["Shaked", "Tal", ""], ["Chandra", "Tushar", ""], ["Aradhye", "Hrishi", ""], ["Anderson", "Glen", ""], ["Corrado", "Greg", ""], ["Chai", "Wei", ""], ["Ispir", "Mustafa", ""], ["Anil", "Rohan", ""], ["Haque", "Zakaria", ""], ["Hong", "Lichan", ""], ["Jain", "Vihan", ""], ["Liu", "Xiaobing", ""], ["Shah", "Hemal", ""]]}, {"id": "1606.07840", "submitter": "Lin Li", "authors": "Lin Li and Ananthram Swami and Anna Scaglione", "title": "Modeling Group Dynamics Using Probabilistic Tensor Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic modeling framework for learning the dynamic\npatterns in the collective behaviors of social agents and developing profiles\nfor different behavioral groups, using data collected from multiple information\nsources. The proposed model is based on a hierarchical Bayesian process, in\nwhich each observation is a finite mixture of an set of latent groups and the\nmixture proportions (i.e., group probabilities) are drawn randomly. Each group\nis associated with some distributions over a finite set of outcomes. Moreover,\nas time evolves, the structure of these groups also changes; we model the\nchange in the group structure by a hidden Markov model (HMM) with a fixed\ntransition probability. We present an efficient inference method based on\ntensor decompositions and the expectation-maximization (EM) algorithm for\nparameter estimation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 21:51:24 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Li", "Lin", ""], ["Swami", "Ananthram", ""], ["Scaglione", "Anna", ""]]}, {"id": "1606.07845", "submitter": "Kamiar Rahnama Rad", "authors": "Kamiar Rahnama Rad and Timothy A. Machado and Liam Paninski", "title": "Robust and scalable Bayesian analysis of spatial neural tuning function\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common analytical problem in neuroscience is the interpretation of neural\nactivity with respect to sensory input or behavioral output. This is typically\nachieved by regressing measured neural activity against known stimuli or\nbehavioral variables to produce a \"tuning function\" for each neuron.\nUnfortunately, because this approach handles neurons individually, it cannot\ntake advantage of simultaneous measurements from spatially adjacent neurons\nthat often have similar tuning properties. On the other hand, sharing\ninformation between adjacent neurons can errantly degrade estimates of tuning\nfunctions across space if there are sharp discontinuities in tuning between\nnearby neurons. In this paper, we develop a computationally efficient block\nGibbs sampler that effectively pools information between neurons to de-noise\ntuning function estimates while simultaneously preserving sharp discontinuities\nthat might exist in the organization of tuning across space. This method is\nfully Bayesian and its computational cost per iteration scales\nsub-quadratically with total parameter dimensionality. We demonstrate the\nrobustness and scalability of this approach by applying it to both real and\nsynthetic datasets. In particular, an application to data from the spinal cord\nillustrates that the proposed methods can dramatically decrease the\nexperimental time required to accurately estimate tuning functions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 22:15:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Rad", "Kamiar Rahnama", ""], ["Machado", "Timothy A.", ""], ["Paninski", "Liam", ""]]}, {"id": "1606.07855", "submitter": "Weisi Deng Weisi Deng", "authors": "Weisi Deng, Yuting Ji, and Lang Tong", "title": "Probabilistic Forecasting and Simulation of Electricity Markets via\n  Online Dictionary Learning", "comments": "8 pages, 6 figures, Hawaii International Conference on System\n  Sciences 2017 (HICSS-50)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of probabilistic forecasting and online simulation of real-time\nelectricity market with stochastic generation and demand is considered. By\nexploiting the parametric structure of the direct current optimal power flow, a\nnew technique based on online dictionary learning (ODL) is proposed. The ODL\napproach incorporates real-time measurements and historical traces to produce\nforecasts of joint and marginal probability distributions of future locational\nmarginal prices, power flows, and dispatch levels, conditional on the system\nstate at the time of forecasting. Compared with standard Monte Carlo simulation\ntechniques, the ODL approach offers several orders of magnitude improvement in\ncomputation time, making it feasible for online forecasting of market\noperations. Numerical simulations on large and moderate size power systems\nillustrate its performance and complexity features and its potential as a tool\nfor system operators.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 00:23:56 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Deng", "Weisi", ""], ["Ji", "Yuting", ""], ["Tong", "Lang", ""]]}, {"id": "1606.07892", "submitter": "Qinyi Zhang", "authors": "Qinyi Zhang and Sarah Filippi and Arthur Gretton and Dino Sejdinovic", "title": "Large-Scale Kernel Methods for Independence Testing", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11222-016-9721-7", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations of probability measures in reproducing kernel Hilbert spaces\nprovide a flexible framework for fully nonparametric hypothesis tests of\nindependence, which can capture any type of departure from independence,\nincluding nonlinear associations and multivariate interactions. However, these\napproaches come with an at least quadratic computational cost in the number of\nobservations, which can be prohibitive in many applications. Arguably, it is\nexactly in such large-scale datasets that capturing any type of dependence is\nof interest, so striking a favourable tradeoff between computational efficiency\nand test performance for kernel independence tests would have a direct impact\non their applicability in practice. In this contribution, we provide an\nextensive study of the use of large-scale kernel approximations in the context\nof independence testing, contrasting block-based, Nystrom and random Fourier\nfeature approaches. Through a variety of synthetic data experiments, it is\ndemonstrated that our novel large scale methods give comparable performance\nwith existing methods whilst using significantly less computation time and\nmemory.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 10:09:03 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhang", "Qinyi", ""], ["Filippi", "Sarah", ""], ["Gretton", "Arthur", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1606.08009", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili, Arash Amini, and Farokh Marvasti", "title": "Fast Methods for Recovering Sparse Parameters in Linear Low Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the recovery of a sparse weight vector\n(parameters vector) from a set of noisy linear combinations. However, only\npartial information about the matrix representing the linear combinations is\navailable. Assuming a low-rank structure for the matrix, one natural solution\nwould be to first apply a matrix completion on the data, and then to solve the\nresulting compressed sensing problem. In big data applications such as massive\nMIMO and medical data, the matrix completion step imposes a huge computational\nburden. Here, we propose to reduce the computational cost of the completion\ntask by ignoring the columns corresponding to zero elements in the sparse\nvector. To this end, we employ a technique to initially approximate the support\nof the sparse vector. We further propose to unify the partial matrix completion\nand sparse vector recovery into an augmented four-step problem. Simulation\nresults reveal that the augmented approach achieves the best performance, while\nboth proposed methods outperform the natural two-step technique with\nsubstantially less computational requirements.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 08:27:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 14:46:39 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Amini", "Arash", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1606.08046", "submitter": "Eric Lock", "authors": "Tianmeng Lyu, Eric F. Lock, and Lynn E. Eberly", "title": "Discriminating sample groups with multi-way data", "comments": "25 pages, 5 figures, 2 tables", "journal-ref": "Biostatistics 18(3), 434-450, 2017", "doi": "10.1093/biostatistics/kxw057", "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear classifiers, such as the support vector machine (SVM)\nand distance weighted discrimination (DWD), are commonly used in biomedical\nresearch to distinguish groups of subjects based on a large number of features.\nHowever, their use is limited to applications where a single vector of features\nis measured for each subject. In practice data are often multi-way, or measured\nover multiple dimensions. For example, metabolite abundance may be measured\nover multiple regions or tissues, or gene expression may be measured over\nmultiple time points, for the same subjects. We propose a framework for linear\nclassification of high-dimensional multi-way data, in which coefficients can be\nfactorized into weights that are specific to each dimension. More generally,\nthe coefficients for each measurement in a multi-way dataset are assumed to\nhave low-rank structure. This framework extends existing classification\ntechniques, and we have implemented multi-way versions of SVM and DWD. We\ndescribe informative simulation results, and apply multi-way DWD to data for\ntwo very different clinical research studies. The first study uses metabolite\nmagnetic resonance spectroscopy data over multiple brain regions to compare\npatients with and without spinocerebellar ataxia, the second uses publicly\navailable gene expression time-course data to compare treatment responses for\npatients with multiple sclerosis. Our method improves performance and\nsimplifies interpretation over naive applications of full rank linear\nclassification to multi-way data. An R package is available at\nhttps://github.com/lockEF/MultiwayClassification .\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 15:39:04 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Lyu", "Tianmeng", ""], ["Lock", "Eric F.", ""], ["Eberly", "Lynn E.", ""]]}, {"id": "1606.08063", "submitter": "Robert Moakler", "authors": "Daizhuo Chen, Samuel P. Fraiberger, Robert Moakler, Foster Provost", "title": "Enhancing Transparency and Control when Drawing Data-Driven Inferences\n  about Individuals", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that information disclosed on social network sites\n(such as Facebook) can be used to predict personal characteristics with\nsurprisingly high accuracy. In this paper we examine a method to give online\nusers transparency into why certain inferences are made about them by\nstatistical models, and control to inhibit those inferences by hiding\n(\"cloaking\") certain personal information from inference. We use this method to\nexamine whether such transparency and control would be a reasonable goal by\nassessing how difficult it would be for users to actually inhibit inferences.\nApplying the method to data from a large collection of real users on Facebook,\nwe show that a user must cloak only a small portion of her Facebook Likes in\norder to inhibit inferences about their personal characteristics. However, we\nalso show that in response a firm could change its modeling of users to make\ncloaking more difficult.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 18:10:03 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Chen", "Daizhuo", ""], ["Fraiberger", "Samuel P.", ""], ["Moakler", "Robert", ""], ["Provost", "Foster", ""]]}, {"id": "1606.08084", "submitter": "Elaheh Raisi", "authors": "Elaheh Raisi and Bert Huang", "title": "Cyberbullying Identification Using Participant-Vocabulary Consistency", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of social media, people can now form relationships and\ncommunities easily regardless of location, race, ethnicity, or gender. However,\nthe power of social media simultaneously enables harmful online behavior such\nas harassment and bullying. Cyberbullying is a serious social problem, making\nit an important topic in social network analysis. Machine learning methods can\npotentially help provide better understanding of this phenomenon, but they must\naddress several key challenges: the rapidly changing vocabulary involved in\ncyber- bullying, the role of social network structure, and the scale of the\ndata. In this study, we propose a model that simultaneously discovers\ninstigators and victims of bullying as well as new bullying vocabulary by\nstarting with a corpus of social interactions and a seed dictionary of bullying\nindicators. We formulate an objective function based on participant-vocabulary\nconsistency. We evaluate this approach on Twitter and Ask.fm data sets and show\nthat the proposed method can detect new bullying vocabulary as well as victims\nand bullies.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 20:41:52 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Raisi", "Elaheh", ""], ["Huang", "Bert", ""]]}, {"id": "1606.08105", "submitter": "Cheng Luo", "authors": "Cheng Luo, Richard Yi Da Xu and Yang Xiang", "title": "The Dependent Random Measures with Independent Increments in Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When observations are organized into groups where commonalties exist amongst\nthem, the dependent random measures can be an ideal choice for modeling. One of\nthe propositions of the dependent random measures is that the atoms of the\nposterior distribution are shared amongst groups, and hence groups can borrow\ninformation from each other. When normalized dependent random measures prior\nwith independent increments are applied, we can derive appropriate exchangeable\nprobability partition function (EPPF), and subsequently also deduce its\ninference algorithm given any mixture model likelihood. We provide all\nnecessary derivation and solution to this framework. For demonstration, we used\nmixture of Gaussians likelihood in combination with a dependent structure\nconstructed by linear combinations of CRMs. Our experiments show superior\nperformance when using this framework, where the inferred values including the\nmixing weights and the number of clusters both respond appropriately to the\nnumber of completely random measure used.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 01:24:02 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Luo", "Cheng", ""], ["Da Xu", "Richard Yi", ""], ["Xiang", "Yang", ""]]}, {"id": "1606.08282", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Marco F. Duarte and Benjamin Marlin", "title": "Out-of-Sample Extension for Dimensionality Reduction of Noisy Time\n  Series", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2735189", "report-no": null, "categories": "stat.ML cs.CG cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an out-of-sample extension framework for a global\nmanifold learning algorithm (Isomap) that uses temporal information in\nout-of-sample points in order to make the embedding more robust to noise and\nartifacts. Given a set of noise-free training data and its embedding, the\nproposed framework extends the embedding for a noisy time series. This is\nachieved by adding a spatio-temporal compactness term to the optimization\nobjective of the embedding. To the best of our knowledge, this is the first\nmethod for out-of-sample extension of manifold embeddings that leverages timing\ninformation available for the extension set. Experimental results demonstrate\nthat our out-of-sample extension algorithm renders a more robust and accurate\nembedding of sequentially ordered image data in the presence of various noise\nand artifacts when compared to other timing-aware embeddings. Additionally, we\nshow that an out-of-sample extension framework based on the proposed algorithm\noutperforms the state of the art in eye-gaze estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:03:40 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 20:13:17 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 01:37:40 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Duarte", "Marco F.", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1606.08288", "submitter": "Cristina Gallego", "authors": "Cristina Gallego-Ortiz and Anne L. Martel", "title": "Interpreting extracted rules from ensemble of trees: Application to\n  computer-aided diagnosis of breast MRI", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High predictive performance and ease of use and interpretability are\nimportant requirements for the applicability of a computer-aided diagnosis\n(CAD) to human reading studies. We propose a CAD system specifically designed\nto be more comprehensible to the radiologist reviewing screening breast MRI\nstudies. Multiparametric imaging features are combined to produce a CAD system\nfor differentiating cancerous and non-cancerous lesions. The complete system\nuses a rule-extraction algorithm to present lesion classification results in an\neasy to understand graph visualization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:35:09 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Gallego-Ortiz", "Cristina", ""], ["Martel", "Anne L.", ""]]}, {"id": "1606.08455", "submitter": "Olga Isupova", "authors": "Olga Isupova, Danil Kuzin, Lyudmila Mihaylova", "title": "Anomaly detection in video with Bayesian nonparametrics", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel dynamic Bayesian nonparametric topic model for anomaly detection in\nvideo is proposed in this paper. Batch and online Gibbs samplers are developed\nfor inference. The paper introduces a new abnormality measure for decision\nmaking. The proposed method is evaluated on both synthetic and real data. The\ncomparison with a non-dynamic model shows the superiority of the proposed\ndynamic one in terms of the classification performance for anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 20:01:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Isupova", "Olga", ""], ["Kuzin", "Danil", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1606.08476", "submitter": "Olga Isupova", "authors": "Olga Isupova, Danil Kuzin, Lyudmila Mihaylova", "title": "Dynamic Hierarchical Dirichlet Process for Abnormal Behaviour Detection\n  in Video", "comments": "8 pages, International Conference on Information Fusion 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel dynamic Hierarchical Dirichlet Process topic\nmodel that considers the dependence between successive observations.\nConventional posterior inference algorithms for this kind of models require\nprocessing of the whole data through several passes. It is computationally\nintractable for massive or sequential data. We design the batch and online\ninference algorithms, based on the Gibbs sampling, for the proposed model. It\nallows to process sequential data, incrementally updating the model by a new\nobservation. The model is applied to abnormal behaviour detection in video\nsequences. A new abnormality measure is proposed for decision making. The\nproposed method is compared with the method based on the non- dynamic\nHierarchical Dirichlet Process, for which we also derive the online Gibbs\nsampler and the abnormality measure. The results with synthetic and real data\nshow that the consideration of the dynamics in a topic model improves the\nclassification performance for abnormal behaviour detection.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 20:29:42 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Isupova", "Olga", ""], ["Kuzin", "Danil", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1606.08531", "submitter": "Bahare Fatemi", "authors": "Bahare Fatemi, Seyed Mehran Kazemi, David Poole", "title": "A Learning Algorithm for Relational Logistic Regression: Preliminary\n  Results", "comments": "In IJCAI-16 Statistical Relational AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Relational logistic regression (RLR) is a representation of conditional\nprobability in terms of weighted formulae for modelling multi-relational data.\nIn this paper, we develop a learning algorithm for RLR models. Learning an RLR\nmodel from data consists of two steps: 1- learning the set of formulae to be\nused in the model (a.k.a. structure learning) and learning the weight of each\nformula (a.k.a. parameter learning). For structure learning, we deploy Schmidt\nand Murphy's hierarchical assumption: first we learn a model with simple\nformulae, then more complex formulae are added iteratively only if all their\nsub-formulae have proven effective in previous learned models. For parameter\nlearning, we convert the problem into a non-relational learning problem and use\nan off-the-shelf logistic regression learning algorithm from Weka, an\nopen-source machine learning tool, to learn the weights. We also indicate how\nhidden features about the individuals can be incorporated into RLR to boost the\nlearning performance. We compare our learning algorithm to other structure and\nparameter learning algorithms in the literature, and compare the performance of\nRLR models to standard logistic regression and RDN-Boost on a modified version\nof the MovieLens data-set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 01:43:38 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Fatemi", "Bahare", ""], ["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1606.08538", "submitter": "Bo Tang", "authors": "Bo Tang and Haibo He", "title": "A Local Density-Based Approach for Local Outlier Detection", "comments": "22 pages, 14 figures, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple but effective density-based outlier detection\napproach with the local kernel density estimation (KDE). A Relative\nDensity-based Outlier Score (RDOS) is introduced to measure the local\noutlierness of objects, in which the density distribution at the location of an\nobject is estimated with a local KDE method based on extended nearest neighbors\nof the object. Instead of using only $k$ nearest neighbors, we further consider\nreverse nearest neighbors and shared nearest neighbors of an object for density\ndistribution estimation. Some theoretical properties of the proposed RDOS\nincluding its expected value and false alarm probability are derived. A\ncomprehensive experimental study on both synthetic and real-life data sets\ndemonstrates that our approach is more effective than state-of-the-art outlier\ndetection methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 02:23:58 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Tang", "Bo", ""], ["He", "Haibo", ""]]}, {"id": "1606.08549", "submitter": "Alexander Moreno Alexander Moreno", "authors": "Alexander Moreno, Tameem Adel, Edward Meeds, James M. Rehg, Max\n  Welling", "title": "Automatic Variational ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a framework for performing\nlikelihood-free posterior inference for simulation models. Stochastic\nVariational inference (SVI) is an appealing alternative to the inefficient\nsampling approaches commonly used in ABC. However, SVI is highly sensitive to\nthe variance of the gradient estimators, and this problem is exacerbated by\napproximating the likelihood. We draw upon recent advances in variance\nreduction for SV and likelihood-free inference using deterministic simulations\nto produce low variance gradient estimators of the variational lower-bound. By\nthen exploiting automatic differentiation libraries we can avoid nearly all\nmodel-specific derivations. We demonstrate performance on three problems and\ncompare to existing SVI algorithms. Our results demonstrate the correctness and\nefficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 04:16:25 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Moreno", "Alexander", ""], ["Adel", "Tameem", ""], ["Meeds", "Edward", ""], ["Rehg", "James M.", ""], ["Welling", "Max", ""]]}, {"id": "1606.08561", "submitter": "Shantanu Jain", "authors": "Shantanu Jain, Martha White, Predrag Radivojac", "title": "Estimating the class prior and posterior from noisy positives and\n  unlabeled data", "comments": "Fixed a typo in the MSGMM update equations in the appendix. Other\n  minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a classification algorithm for estimating posterior distributions\nfrom positive-unlabeled data, that is robust to noise in the positive labels\nand effective for high-dimensional data. In recent years, several algorithms\nhave been proposed to learn from positive-unlabeled data; however, many of\nthese contributions remain theoretical, performing poorly on real\nhigh-dimensional data that is typically contaminated with noise. We build on\nthis previous work to develop two practical classification algorithms that\nexplicitly model the noise in the positive labels and utilize univariate\ntransforms built on discriminative classifiers. We prove that these univariate\ntransforms preserve the class prior, enabling estimation in the univariate\nspace and avoiding kernel density estimation for high-dimensional data. The\ntheoretical development and both parametric and nonparametric algorithms\nproposed here constitutes an important step towards wide-spread use of robust\nclassification algorithms for positive-unlabeled data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 05:29:25 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 19:25:14 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Jain", "Shantanu", ""], ["White", "Martha", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1606.08571", "submitter": "Yang Lu", "authors": "Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu", "title": "Alternating Back-Propagation for Generator Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternating back-propagation algorithm for learning\nthe generator network model. The model is a non-linear generalization of factor\nanalysis. In this model, the mapping from the continuous latent factors to the\nobserved signal is parametrized by a convolutional neural network. The\nalternating back-propagation algorithm iterates the following two steps: (1)\nInferential back-propagation, which infers the latent factors by Langevin\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\nparameters given the inferred latent factors by gradient descent. The gradient\ncomputations in both steps are powered by back-propagation, and they share most\nof their code in common. We show that the alternating back-propagation\nalgorithm can learn realistic generator models of natural images, video\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\nor indirect training data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 06:46:05 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 15:11:00 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 04:38:01 GMT"}, {"version": "v4", "created": "Tue, 6 Dec 2016 04:04:19 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Han", "Tian", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.08658", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Hendrik Blockeel", "title": "Clustering-Based Relational Unsupervised Representation Learning with an\n  Explicit Distributed Representation", "comments": "8 pages, 1 figure, 2 tables, StaRAI 2016 submission, final version", "journal-ref": null, "doi": "10.24963/ijcai.2017/226", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of unsupervised representation learning is to extract a new\nrepresentation of data, such that solving many different tasks becomes easier.\nExisting methods typically focus on vectorized data and offer little support\nfor relational data, which additionally describe relationships among instances.\nIn this work we introduce an approach for relational unsupervised\nrepresentation learning. Viewing a relational dataset as a hypergraph, new\nfeatures are obtained by clustering vertices and hyperedges. To find a\nrepresentation suited for many relational learning tasks, a wide range of\nsimilarities between relational objects is considered, e.g. feature and\nstructural similarities. We experimentally evaluate the proposed approach and\nshow that models learned on such latent representations perform better, have\nlower complexity, and outperform the existing approaches on classification\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:37:45 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 07:35:46 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 09:21:40 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1606.08660", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Wannes Meert and Hendrik Blockeel", "title": "Theory reconstruction: a representation learning view on predicate\n  invention", "comments": "3 pages, StaRAI 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With this positional paper we present a representation learning view on\npredicate invention. The intention of this proposal is to bridge the relational\nand deep learning communities on the problem of predicate invention. We propose\na theory reconstruction approach, a formalism that extends autoencoder approach\nto representation learning to the relational settings. Our intention is to\nstart a discussion to define a unifying framework for predicate invention and\ntheory revision.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 11:41:03 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 11:29:35 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Meert", "Wannes", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1606.08698", "submitter": "Guillem Collell", "authors": "Guillem Collell, Drazen Prelec, Kaustubh Patil", "title": "Reviving Threshold-Moving: a Simple Plug-in Bagging Ensemble for Binary\n  and Multiclass Imbalanced Data", "comments": "Typo in the proof fixed. TP/(P+N)=P(y=1) replaced by P/(P+N)=P(y=1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance presents a major hurdle in the application of data mining\nmethods. A common practice to deal with it is to create ensembles of\nclassifiers that learn from resampled balanced data. For example, bagged\ndecision trees combined with random undersampling (RUS) or the synthetic\nminority oversampling technique (SMOTE). However, most of the resampling\nmethods entail asymmetric changes to the examples of different classes, which\nin turn can introduce its own biases in the model. Furthermore, those methods\nrequire a performance measure to be specified a priori before learning. An\nalternative is to use a so-called threshold-moving method that a posteriori\nchanges the decision threshold of a model to counteract the imbalance, thus has\na potential to adapt to the performance measure of interest. Surprisingly,\nlittle attention has been paid to the potential of combining bagging ensemble\nwith threshold-moving. In this paper, we present probability thresholding\nbagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary\nto usual rebalancing practice, our method preserves the natural class\ndistribution of the data resulting in well calibrated posterior probabilities.\nWe also extend the proposed method to handle multiclass data. The method is\nvalidated on binary and multiclass benchmark data sets. We perform analyses\nthat provide insights into the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:49:30 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 18:53:14 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 08:09:37 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Collell", "Guillem", ""], ["Prelec", "Drazen", ""], ["Patil", "Kaustubh", ""]]}, {"id": "1606.08793", "submitter": "Steven Kearnes", "authors": "Steven Kearnes, Brian Goldman, Vijay Pande", "title": "Modeling Industrial ADMET Data with Multitask Networks", "comments": "See \"Version information\" section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods such as multitask neural networks have recently been\napplied to ligand-based virtual screening and other drug discovery\napplications. Using a set of industrial ADMET datasets, we compare neural\nnetworks to standard baseline models and analyze multitask learning effects\nwith both random cross-validation and a more relevant temporal validation\nscheme. We confirm that multitask learning can provide modest benefits over\nsingle-task models and show that smaller datasets tend to benefit more than\nlarger datasets from multitask learning. Additionally, we find that adding\nmassive amounts of side information is not guaranteed to improve performance\nrelative to simpler multitask learning. Our results emphasize that multitask\neffects are highly dataset-dependent, suggesting the use of dataset-specific\nmodels to maximize overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 17:22:29 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 20:09:43 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 01:04:14 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Kearnes", "Steven", ""], ["Goldman", "Brian", ""], ["Pande", "Vijay", ""]]}, {"id": "1606.08813", "submitter": "Seth Flaxman", "authors": "Bryce Goodman and Seth Flaxman", "title": "European Union regulations on algorithmic decision-making and a \"right\n  to explanation\"", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": "AI Magazine, Vol 38, No 3, 2017", "doi": "10.1609/aimag.v38i3.2741", "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize the potential impact that the European Union's new General Data\nProtection Regulation will have on the routine use of machine learning\nalgorithms. Slated to take effect as law across the EU in 2018, it will\nrestrict automated individual decision-making (that is, algorithms that make\ndecisions based on user-level predictors) which \"significantly affect\" users.\nThe law will also effectively create a \"right to explanation,\" whereby a user\ncan ask for an explanation of an algorithmic decision that was made about them.\nWe argue that while this law will pose large challenges for industry, it\nhighlights opportunities for computer scientists to take the lead in designing\nalgorithms and evaluation frameworks which avoid discrimination and enable\nexplanation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:20:06 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 15:42:30 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 12:30:13 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Goodman", "Bryce", ""], ["Flaxman", "Seth", ""]]}, {"id": "1606.08819", "submitter": "Moshe Salhov", "authors": "Moshe Salhov, Ofir Lindenbaum, Yariv Aizenbud, Avi Silberschatz, Yoel\n  Shkolnisky, Amir Averbuch", "title": "Multi-View Kernel Consensus For Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input data features set for many data driven tasks is high-dimensional\nwhile the intrinsic dimension of the data is low. Data analysis methods aim to\nuncover the underlying low dimensional structure imposed by the low dimensional\nhidden parameters by utilizing distance metrics that consider the set of\nattributes as a single monolithic set. However, the transformation of the low\ndimensional phenomena into the measured high dimensional observations might\ndistort the distance metric, This distortion can effect the desired estimated\nlow dimensional geometric structure. In this paper, we suggest to utilize the\nredundancy in the attribute domain by partitioning the attributes into multiple\nsubsets we call views. The proposed methods utilize the agreement also called\nconsensus between different views to extract valuable geometric information\nthat unifies multiple views about the intrinsic relationships among several\ndifferent observations. This unification enhances the information that a single\nview or a simple concatenations of views provides.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:32:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 11:11:43 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Salhov", "Moshe", ""], ["Lindenbaum", "Ofir", ""], ["Aizenbud", "Yariv", ""], ["Silberschatz", "Avi", ""], ["Shkolnisky", "Yoel", ""], ["Averbuch", "Amir", ""]]}, {"id": "1606.08842", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Nihar B. Shah and Kannan Ramchandran and Martin J.\n  Wainwright", "title": "Active Ranking from Pairwise Comparisons and when Parametric Assumptions\n  Don't Help", "comments": "improved log factor in main result; added discussion on comparison\n  probabilities close to zero; added numerical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential or active ranking of a set of n items based on noisy\npairwise comparisons. Items are ranked according to the probability that a\ngiven item beats a randomly chosen item, and ranking refers to partitioning the\nitems into sets of pre-specified sizes according to their scores. This notion\nof ranking includes as special cases the identification of the top-k items and\nthe total ordering of the items. We first analyze a sequential ranking\nalgorithm that counts the number of comparisons won, and uses these counts to\ndecide whether to stop, or to compare another pair of items, chosen based on\nconfidence intervals specified by the data collected up to that point. We prove\nthat this algorithm succeeds in recovering the ranking using a number of\ncomparisons that is optimal up to logarithmic factors. This guarantee does not\nrequire any structural properties of the underlying pairwise probability\nmatrix, unlike a significant body of past work on pairwise ranking based on\nparametric models such as the Thurstone or Bradley-Terry-Luce models. It has\nbeen a long-standing open question as to whether or not imposing these\nparametric assumptions allows for improved ranking algorithms. For stochastic\ncomparison models, in which the pairwise probabilities are bounded away from\nzero, our second contribution is to resolve this issue by proving a lower bound\nfor parametric models. This shows, perhaps surprisingly, that these popular\nparametric modeling choices offer at most logarithmic gains for stochastic\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 19:59:52 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 15:55:44 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Heckel", "Reinhard", ""], ["Shah", "Nihar B.", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1606.08882", "submitter": "Brian Baingana Dr", "authors": "Brian Baingana, Georgios B. Giannakis", "title": "Tracking Switched Dynamic Network Topologies from Information Cascades", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2628354", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contagions such as the spread of popular news stories, or infectious\ndiseases, propagate in cascades over dynamic networks with unobservable\ntopologies. However, \"social signals\" such as product purchase time, or blog\nentry timestamps are measurable, and implicitly depend on the underlying\ntopology, making it possible to track it over time. Interestingly, network\ntopologies often \"jump\" between discrete states that may account for sudden\nchanges in the observed signals. The present paper advocates a switched dynamic\nstructural equation model to capture the topology-dependent cascade evolution,\nas well as the discrete states driving the underlying topologies. Conditions\nunder which the proposed switched model is identifiable are established.\nLeveraging the edge sparsity inherent to social networks, a recursive\n$\\ell_1$-norm regularized least-squares estimator is put forth to jointly track\nthe states and network topologies. An efficient first-order proximal-gradient\nalgorithm is developed to solve the resulting optimization problem. Numerical\nexperiments on both synthetic data and real cascades measured over the span of\none year are conducted, and test results corroborate the efficacy of the\nadvocated approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:49:00 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1606.08957", "submitter": "Sheng Chen", "authors": "Sheng Chen and Arindam Banerjee", "title": "Alternating Estimation for Structured High-Dimensional Multi-Response\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning high-dimensional multi-response linear models with\nstructured parameters. By exploiting the noise correlations among responses, we\npropose an alternating estimation (AltEst) procedure to estimate the model\nparameters based on the generalized Dantzig selector. Under suitable sample\nsize and resampling assumptions, we show that the error of the estimates\ngenerated by AltEst, with high probability, converges linearly to certain\nminimum achievable level, which can be tersely expressed by a few geometric\nmeasures, such as Gaussian width of sets related to the parameter structure. To\nthe best of our knowledge, this is the first non-asymptotic statistical\nguarantee for such AltEst-type algorithm applied to estimation problem with\ngeneral structures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 05:35:20 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chen", "Sheng", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1606.08963", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Mihajlo Grbovic, Vladan Radosavljevic, Narayan\n  Bhamidipati, Slobodan Vucetic", "title": "Non-linear Label Ranking for Large-scale Prediction of Long-Term User\n  Interests", "comments": "28th AAAI Conference on Artificial Intelligence (AAAI-14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of personalization of online services from the\nviewpoint of ad targeting, where we seek to find the best ad categories to be\nshown to each user, resulting in improved user experience and increased\nadvertisers' revenue. We propose to address this problem as a task of ranking\nthe ad categories depending on a user's preference, and introduce a novel label\nranking approach capable of efficiently learning non-linear, highly accurate\nmodels in large-scale settings. Experiments on a real-world advertising data\nset with more than 3.2 million users show that the proposed algorithm\noutperforms the existing solutions in terms of both rank loss and top-K\nretrieval performance, strongly suggesting the benefit of using the proposed\nmodel on large-scale ranking problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 06:00:35 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Djuric", "Nemanja", ""], ["Grbovic", "Mihajlo", ""], ["Radosavljevic", "Vladan", ""], ["Bhamidipati", "Narayan", ""], ["Vucetic", "Slobodan", ""]]}, {"id": "1606.09066", "submitter": "Satoshi Hara", "authors": "Satoshi Hara, Kohei Hayashi", "title": "Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensembles, such as random forests and boosted trees, are renowned for\ntheir high prediction performance. However, their interpretability is\ncritically limited due to the enormous complexity. In this study, we present a\nmethod to make a complex tree ensemble interpretable by simplifying the model.\nSpecifically, we formalize the simplification of tree ensembles as a model\nselection problem. Given a complex tree ensemble, we aim at obtaining the\nsimplest representation that is essentially equivalent to the original one. To\nthis end, we derive a Bayesian model selection algorithm that optimizes the\nsimplified model while maintaining the prediction performance. Our numerical\nexperiments on several datasets showed that complicated tree ensembles were\nreasonably approximated as interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 12:24:03 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 04:38:49 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 11:37:29 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Hara", "Satoshi", ""], ["Hayashi", "Kohei", ""]]}, {"id": "1606.09155", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Accelerated first-order primal-dual proximal methods for linearly\n  constrained composite convex programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by big data applications, first-order methods have been extremely\npopular in recent years. However, naive gradient methods generally converge\nslowly. Hence, much efforts have been made to accelerate various first-order\nmethods. This paper proposes two accelerated methods towards solving structured\nlinearly constrained convex programming, for which we assume composite convex\nobjective.\n  The first method is the accelerated linearized augmented Lagrangian method\n(LALM). At each update to the primal variable, it allows linearization to the\ndifferentiable function and also the augmented term, and thus it enables easy\nsubproblems. Assuming merely weak convexity, we show that LALM owns $O(1/t)$\nconvergence if parameters are kept fixed during all the iterations and can be\naccelerated to $O(1/t^2)$ if the parameters are adapted, where $t$ is the\nnumber of total iterations.\n  The second method is the accelerated linearized alternating direction method\nof multipliers (LADMM). In addition to the composite convexity, it further\nassumes two-block structure on the objective. Different from classic ADMM, our\nmethod allows linearization to the objective and also augmented term to make\nthe update simple. Assuming strong convexity on one block variable, we show\nthat LADMM also enjoys $O(1/t^2)$ convergence with adaptive parameters. This\nresult is a significant improvement over that in [Goldstein et. al, SIIMS'14],\nwhich requires strong convexity on both block variables and no linearization to\nthe objective or augmented term.\n  Numerical experiments are performed on quadratic programming, image\ndenoising, and support vector machine. The proposed accelerated methods are\ncompared to nonaccelerated ones and also existing accelerated methods. The\nresults demonstrate the validness of acceleration and superior performance of\nthe proposed methods over existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:26:55 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1606.09163", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme\n  Recognition for Low Latency Processing", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic analysis on the performance of a phonetic recogniser\nwhen the window of input features is not symmetric with respect to the current\nframe. The recogniser is based on Context Dependent Deep Neural Networks\n(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the\nlatency of the system by reducing the number of future feature frames required\nto estimate the current output. Our tests performed on the TIMIT database show\nthat the performance does not degrade when the input window is shifted up to 5\nframes in the past compared to common practice (no future frame). This\ncorresponds to improving the latency by 50 ms in our settings. Our tests also\nshow that the best results are not obtained with the symmetric window commonly\nemployed, but with an asymmetric window with eight past and two future context\nframes, although this observation should be confirmed on other data sets. The\nreduction in latency suggested by our results is critical for specific\napplications such as real-time lip synchronisation for tele-presence, but may\nalso be beneficial in general applications to improve the lag in human-machine\nspoken interaction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:51:44 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1606.09184", "submitter": "Peter Schulam", "authors": "Peter Schulam and Raman Arora", "title": "Disease Trajectory Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical researchers are coming to appreciate that many diseases are in fact\ncomplex, heterogeneous syndromes composed of subpopulations that express\ndifferent variants of a related complication. Time series data extracted from\nindividual electronic health records (EHR) offer an exciting new way to study\nsubtle differences in the way these diseases progress over time. In this paper,\nwe focus on answering two questions that can be asked using these databases of\ntime series. First, we want to understand whether there are individuals with\nsimilar disease trajectories and whether there are a small number of degrees of\nfreedom that account for differences in trajectories across the population.\nSecond, we want to understand how important clinical outcomes are associated\nwith disease trajectories. To answer these questions, we propose the Disease\nTrajectory Map (DTM), a novel probabilistic model that learns low-dimensional\nrepresentations of sparse and irregularly sampled time series. We propose a\nstochastic variational inference algorithm for learning the DTM that allows the\nmodel to scale to large modern medical datasets. To demonstrate the DTM, we\nanalyze data collected on patients with the complex autoimmune disease,\nscleroderma. We find that DTM learns meaningful representations of disease\ntrajectories and that the representations are significantly associated with\nimportant clinical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:06:45 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Schulam", "Peter", ""], ["Arora", "Raman", ""]]}, {"id": "1606.09187", "submitter": "Wojciech Samek", "authors": "Jing Yu Koh and Wojciech Samek and Klaus-Robert M\\\"uller and Alexander\n  Binder", "title": "Object Boundary Detection and Classification with Image-level Labels", "comments": "12 pages, 2 figures, accepted for GCPR 2017 - 39th German Conference\n  on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic boundary and edge detection aims at simultaneously detecting object\nedge pixels in images and assigning class labels to them. Systematic training\nof predictors for this task requires the labeling of edges in images which is a\nparticularly tedious task. We propose a novel strategy for solving this task,\nwhen pixel-level annotations are not available, performing it in an almost\nzero-shot manner by relying on conventional whole image neural net classifiers\nthat were trained using large bounding boxes. Our method performs the following\ntwo steps at test time. Firstly it predicts the class labels by applying the\ntrained whole image network to the test images. Secondly, it computes\npixel-wise scores from the obtained predictions by applying backprop gradients\nas well as recent visualization algorithms such as deconvolution and layer-wise\nrelevance propagation. We show that high pixel-wise scores are indicative for\nthe location of semantic boundaries, which suggests that the semantic boundary\nproblem can be approached without using edge labels during the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:08:58 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 17:13:43 GMT"}, {"version": "v3", "created": "Sun, 25 Jun 2017 12:50:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Koh", "Jing Yu", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Binder", "Alexander", ""]]}, {"id": "1606.09190", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Cl\\'ement Dombry and Adrien Faivre", "title": "A Semi-Definite Programming approach to low dimensional embedding for\n  unsupervised clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a variant of the method of Gu\\'edon and Verhynin for\nestimating the cluster matrix in the Mixture of Gaussians framework via\nSemi-Definite Programming. A clustering oriented embedding is deduced from this\nestimate. The procedure is suitable for very high dimensional data because it\nis based on pairwise distances only. Theoretical garantees are provided and an\neigenvalue optimisation approach is proposed for computing the embedding. The\nperformance of the method is illustrated via Monte Carlo experiements and\ncomparisons with other embeddings from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:20:39 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Dombry", "Cl\u00e9ment", ""], ["Faivre", "Adrien", ""]]}, {"id": "1606.09193", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien and Zhen Wai Olivier Ho", "title": "Small coherence implies the weak Null Space Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Compressed Sensing community, it is well known that given a matrix $X\n\\in \\mathbb R^{n\\times p}$ with $\\ell_2$ normalized columns, the Restricted\nIsometry Property (RIP) implies the Null Space Property (NSP). It is also well\nknown that a small Coherence $\\mu$ implies a weak RIP, i.e. the singular values\nof $X_T$ lie between $1-\\delta$ and $1+\\delta$ for \"most\" index subsets $T\n\\subset \\{1,\\ldots,p\\}$ with size governed by $\\mu$ and $\\delta$. In this short\nnote, we show that a small Coherence implies a weak Null Space Property, i.e.\n$\\Vert h_T\\Vert_2 \\le C \\ \\Vert h_{T^c}\\Vert_1/\\sqrt{s}$ for most $T \\subset\n\\{1,\\ldots,p\\}$ with cardinality $|T|\\le s$. We moreover prove some singular\nvalue perturbation bounds that may also prove useful for other applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:29:05 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Ho", "Zhen Wai Olivier", ""]]}, {"id": "1606.09202", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux", "title": "Tighter bounds lead to improved classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to supervised classification involves the minimization\nof a log-loss as an upper bound to the classification error. While this is a\ntight bound early on in the optimization, it overemphasizes the influence of\nincorrectly classified examples far from the decision boundary. Updating the\nupper bound during the optimization leads to improved classification rates\nwhile transforming the learning into a sequence of minimization problems. In\naddition, in the context where the classifier is part of a larger system, this\nmodification makes it possible to link the performance of the classifier to\nthat of the whole system, allowing the seamless introduction of external\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 18:01:15 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 19:54:19 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Roux", "Nicolas Le", ""]]}, {"id": "1606.09282", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Derek Hoiem", "title": "Learning without Forgetting", "comments": "Conference version appears in ECCV 2016; updated with journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building a unified vision system or gradually adding new capabilities to\na system, the usual assumption is that training data for all tasks is always\navailable. However, as the number of tasks grows, storing and retraining on\nsuch data becomes infeasible. A new problem arises where we add new\ncapabilities to a Convolutional Neural Network (CNN), but the training data for\nits existing capabilities are unavailable. We propose our Learning without\nForgetting method, which uses only new task data to train the network while\npreserving the original capabilities. Our method performs favorably compared to\ncommonly used feature extraction and fine-tuning adaption techniques and\nperforms similarly to multitask learning that uses original task data we assume\nunavailable. A more surprising observation is that Learning without Forgetting\nmay be able to replace fine-tuning with similar old and new task datasets for\nimproved new task performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:54:04 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 22:12:43 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 22:32:30 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Li", "Zhizhong", ""], ["Hoiem", "Derek", ""]]}, {"id": "1606.09375", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Xavier Bresson, Pierre Vandergheynst", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral\n  Filtering", "comments": "NIPS 2016 final revision", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 07:42:13 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:24:49 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 17:04:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1606.09388", "submitter": "Emilie Kaufmann", "authors": "Alexander Luedtke, Emilie Kaufmann (CRIStAL), Antoine Chambaz (MAP5 -\n  UMR 8145)", "title": "Asymptotically Optimal Algorithms for Budgeted Multiple Play Bandits", "comments": null, "journal-ref": "Machine Learning Journal, Springer, In press", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the multi-armed bandit problem with multiple\nplays where there is a cost associated with pulling each arm and the agent has\na budget at each time that dictates how much she can expect to spend. We derive\nan asymptotic regret lower bound for any uniformly efficient algorithm in our\nsetting. We then study a variant of Thompson sampling for Bernoulli rewards and\na variant of KL-UCB for both single-parameter exponential families and bounded,\nfinitely supported rewards. We show these algorithms are asymptotically\noptimal, both in rateand leading problem-dependent constants, including in the\nthick margin setting where multiple arms fall on the decision boundary.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 08:30:57 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 07:38:48 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 12:20:09 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Luedtke", "Alexander", "", "CRIStAL"], ["Kaufmann", "Emilie", "", "CRIStAL"], ["Chambaz", "Antoine", "", "MAP5 -\n  UMR 8145"]]}, {"id": "1606.09458", "submitter": "Maryam Sabzevari", "authors": "Maryam Sabzevari, Gonzalo Mart\\'inez-Mu\\~noz, Alberto Su\\'arez", "title": "Vote-boosting ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vote-boosting is a sequential ensemble learning method in which the\nindividual classifiers are built on different weighted versions of the training\ndata. To build a new classifier, the weight of each training instance is\ndetermined in terms of the degree of disagreement among the current ensemble\npredictions for that instance. For low class-label noise levels, especially\nwhen simple base learners are used, emphasis should be made on instances for\nwhich the disagreement rate is high. When more flexible classifiers are used\nand as the noise level increases, the emphasis on these uncertain instances\nshould be reduced. In fact, at sufficiently high levels of class-label noise,\nthe focus should be on instances on which the ensemble classifiers agree. The\noptimal type of emphasis can be automatically determined using\ncross-validation. An extensive empirical analysis using the beta distribution\nas emphasis function illustrates that vote-boosting is an effective method to\ngenerate ensembles that are both accurate and robust.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 12:24:04 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 12:31:01 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sabzevari", "Maryam", ""], ["Mart\u00ednez-Mu\u00f1oz", "Gonzalo", ""], ["Su\u00e1rez", "Alberto", ""]]}, {"id": "1606.09517", "submitter": "Ryan Turner", "authors": "Ryan Turner", "title": "A Model Explanation System: Latest Updates and Extensions", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general model explanation system (MES) for \"explaining\" the\noutput of black box classifiers. This paper describes extensions to Turner\n(2015), which is referred to frequently in the text. We use the motivating\nexample of a classifier trained to detect fraud in a credit card transaction\nhistory. The key aspect is that we provide explanations applicable to a single\nprediction, rather than provide an interpretable set of parameters. We focus on\nexplaining positive predictions (alerts). However, the presented methodology is\nsymmetrically applicable to negative predictions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 14:44:26 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Turner", "Ryan", ""]]}, {"id": "1606.09632", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "A Permutation-based Model for Crowd Labeling: Optimal Estimation and\n  Robustness", "comments": "in IEEE Transactions on Information Theory (online), 2020", "journal-ref": null, "doi": "10.1109/TIT.2020.3045613", "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of aggregating and denoising crowd-labeled data has gained increased\nsignificance with the advent of crowdsourcing platforms and massive datasets.\nWe propose a permutation-based model for crowd labeled data that is a\nsignificant generalization of the classical Dawid-Skene model, and introduce a\nnew error metric by which to compare different estimators. We derive global\nminimax rates for the permutation-based model that are sharp up to logarithmic\nfactors, and match the minimax lower bounds derived under the simpler\nDawid-Skene model. We then design two computationally-efficient estimators: the\nWAN estimator for the setting where the ordering of workers in terms of their\nabilities is approximately known, and the OBI-WAN estimator where that is not\nknown. For each of these estimators, we provide non-asymptotic bounds on their\nperformance. We conduct synthetic simulations and experiments on real-world\ncrowdsourcing data, and the experimental results corroborate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 19:40:56 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 04:58:30 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 18:18:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}]