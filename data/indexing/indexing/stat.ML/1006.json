[{"id": "1006.0234", "submitter": "Manuel Gomez Rodriguez", "authors": "Manuel Gomez-Rodriguez, Jure Leskovec, Andreas Krause", "title": "Inferring Networks of Diffusion and Influence", "comments": "Short version appeared in ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD), 2010. Long version submitted to\n  ACM Transactions on Knowledge Discovery from Data (TKDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion and virus propagation are fundamental processes taking\nplace in networks. While it is often possible to directly observe when nodes\nbecome infected with a virus or adopt the information, observing individual\ntransmissions (i.e., who infects whom, or who influences whom) is typically\nvery difficult. Furthermore, in many applications, the underlying network over\nwhich the diffusions and propagations spread is actually unobserved. We tackle\nthese challenges by developing a method for tracing paths of diffusion and\ninfluence through networks and inferring the networks over which contagions\npropagate. Given the times when nodes adopt pieces of information or become\ninfected, we identify the optimal network that best explains the observed\ninfection times. Since the optimization problem is NP-hard to solve exactly, we\ndevelop an efficient approximation algorithm that scales to large datasets and\nfinds provably near-optimal networks.\n  We demonstrate the effectiveness of our approach by tracing information\ndiffusion in a set of 170 million blogs and news articles over a one year\nperiod to infer how information flows through the online media space. We find\nthat the diffusion network of news for the top 1,000 media sites and blogs\ntends to have a core-periphery structure with a small set of core media sites\nthat diffuse information to the rest of the Web. These sites tend to have\nstable circles of influence with more general news media sites acting as\nconnectors between them.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 20:02:31 GMT"}, {"version": "v2", "created": "Tue, 7 Dec 2010 20:35:08 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2011 18:56:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gomez-Rodriguez", "Manuel", ""], ["Leskovec", "Jure", ""], ["Krause", "Andreas", ""]]}, {"id": "1006.0375", "submitter": "Joachim Buhmann M", "authors": "Joachim M. Buhmann", "title": "Information theoretic model validation for clustering", "comments": "9 pages, 2 figures, International Symposium on Information Theory\n  2010 (ISIT10 E-Mo-4.2), June 13-18 in Austin, TX}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection in clustering requires (i) to specify a suitable clustering\nprinciple and (ii) to control the model order complexity by choosing an\nappropriate number of clusters depending on the noise level in the data. We\nadvocate an information theoretic perspective where the uncertainty in the\nmeasurements quantizes the set of data partitionings and, thereby, induces\nuncertainty in the solution space of clusterings. A clustering model, which can\ntolerate a higher level of fluctuations in the measurements than alternative\nmodels, is considered to be superior provided that the clustering solution is\nequally informative. This tradeoff between \\emph{informativeness} and\n\\emph{robustness} is used as a model selection criterion. The requirement that\ndata partitionings should generalize from one data set to an equally probable\nsecond data set gives rise to a new notion of structure induced information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 13:47:12 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Buhmann", "Joachim M.", ""]]}, {"id": "1006.0719", "submitter": "Waheed Bajwa", "authors": "Waheed U. Bajwa, Robert Calderbank, and Sina Jafarpour", "title": "Why Gabor Frames? Two Fundamental Measures of Coherence and Their Role\n  in Model Selection", "comments": "31 pages, 4 figures; This paper is a full-length journal version of a\n  shorter paper that was presented at the IEEE International Symposium on\n  Information Theory, Austin, TX, June 2010", "journal-ref": "J. Commun. Netw., vol. 12, no. 4, pp. 289-307, Aug. 2010", "doi": "10.1109/JCN.2010.6388466", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies non-asymptotic model selection for the general case of\narbitrary design matrices and arbitrary nonzero entries of the signal. In this\nregard, it generalizes the notion of incoherence in the existing literature on\nmodel selection and introduces two fundamental measures of coherence---termed\nas the worst-case coherence and the average coherence---among the columns of a\ndesign matrix. It utilizes these two measures of coherence to provide an\nin-depth analysis of a simple, model-order agnostic one-step thresholding (OST)\nalgorithm for model selection and proves that OST is feasible for exact as well\nas partial model selection as long as the design matrix obeys an easily\nverifiable property. One of the key insights offered by the ensuing analysis in\nthis regard is that OST can successfully carry out model selection even when\nmethods based on convex optimization such as the lasso fail due to the rank\ndeficiency of the submatrices of the design matrix. In addition, the paper\nestablishes that if the design matrix has reasonably small worst-case and\naverage coherence then OST performs near-optimally when either (i) the energy\nof any nonzero entry of the signal is close to the average signal energy per\nnonzero entry or (ii) the signal-to-noise ratio in the measurement system is\nnot too high. Finally, two other key contributions of the paper are that (i) it\nprovides bounds on the average coherence of Gaussian matrices and Gabor frames,\nand (ii) it extends the results on model selection using OST to low-complexity,\nmodel-order agnostic recovery of sparse signals with arbitrary nonzero entries.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 19:22:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2010 12:16:47 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2010 19:08:37 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bajwa", "Waheed U.", ""], ["Calderbank", "Robert", ""], ["Jafarpour", "Sina", ""]]}, {"id": "1006.0849", "submitter": "Nicholas Fyson", "authors": "Nick Fyson, Tijl De Bie and Nello Cristianini", "title": "Reconstruction of Causal Networks by Set Covering", "comments": "Under consideration for the ECML PKDD 2010 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the reconstruction of networks, based on the order of\nnodes visited by a stochastic branching process. Our algorithm reconstructs a\nnetwork of minimal size that ensures consistency with the data. Crucially, we\nshow that global consistency with the data can be achieved through purely local\nconsiderations, inferring the neighbourhood of each node in turn. The\noptimisation problem solved for each individual node can be reduced to a Set\nCovering Problem, which is known to be NP-hard but can be approximated well in\npractice. We then extend our approach to account for noisy data, based on the\nMinimum Description Length principle. We demonstrate our algorithms on\nsynthetic data, generated by an SIR-like epidemiological model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 10:33:49 GMT"}], "update_date": "2010-06-07", "authors_parsed": [["Fyson", "Nick", ""], ["De Bie", "Tijl", ""], ["Cristianini", "Nello", ""]]}, {"id": "1006.0868", "submitter": "Iain Murray", "authors": "Iain Murray and Ryan Prescott Adams", "title": "Slice sampling covariance hyperparameters of latent Gaussian models", "comments": "9 pages, 4 figures, 4 algorithms. Minor corrections to previous\n  version. This version to appear in Advances in Neural Information Processing\n  Systems (NIPS) 23, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process (GP) is a popular way to specify dependencies between\nrandom variables in a probabilistic model. In the Bayesian framework the\ncovariance structure can be specified using unknown hyperparameters.\nIntegrating over these hyperparameters considers different possible\nexplanations for the data when making predictions. This integration is often\nperformed using Markov chain Monte Carlo (MCMC) sampling. However, with\nnon-Gaussian observations standard hyperparameter sampling approaches require\ncareful tuning and may converge slowly. In this paper we present a slice\nsampling approach that requires little tuning while mixing well in both strong-\nand weak-data regimes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 11:32:16 GMT"}, {"version": "v2", "created": "Thu, 28 Oct 2010 20:47:52 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Murray", "Iain", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1006.1029", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin, Dimitar Hristovski", "title": "Chi-square-based scoring function for categorization of MEDLINE\n  citations", "comments": "34 pages, 2 figures", "journal-ref": "Methods of Information in Medicine, 2010;49(4):371-380", "doi": "10.3414/ME09-01-0009", "report-no": null, "categories": "cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Text categorization has been used in biomedical informatics for\nidentifying documents containing relevant topics of interest. We developed a\nsimple method that uses a chi-square-based scoring function to determine the\nlikelihood of MEDLINE citations containing genetic relevant topic. Methods: Our\nprocedure requires construction of a genetic and a nongenetic domain document\ncorpus. We used MeSH descriptors assigned to MEDLINE citations for this\ncategorization task. We compared frequencies of MeSH descriptors between two\ncorpora applying chi-square test. A MeSH descriptor was considered to be a\npositive indicator if its relative observed frequency in the genetic domain\ncorpus was greater than its relative observed frequency in the nongenetic\ndomain corpus. The output of the proposed method is a list of scores for all\nthe citations, with the highest score given to those citations containing MeSH\ndescriptors typical for the genetic domain. Results: Validation was done on a\nset of 734 manually annotated MEDLINE citations. It achieved predictive\naccuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method\nby comparing it to three machine learning algorithms (support vector machines,\ndecision trees, na\\\"ive Bayes). Although the differences were not statistically\nsignificantly different, results showed that our chi-square scoring performs as\ngood as compared machine learning algorithms. Conclusions: We suggest that the\nchi-square scoring is an effective solution to help categorize MEDLINE\ncitations. The algorithm is implemented in the BITOLA literature-based\ndiscovery support system as a preprocessor for gene symbol disambiguation\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:14:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""], ["Hristovski", "Dimitar", ""]]}, {"id": "1006.1030", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin", "title": "Rasch-based high-dimensionality data reduction and class prediction with\n  applications to microarray gene expression data", "comments": null, "journal-ref": "Expert Systems with Applications, 2010;37(7):5178-5185", "doi": "10.1016/j.eswa.2009.12.074", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class prediction is an important application of microarray gene expression\ndata analysis. The high-dimensionality of microarray data, where number of\ngenes (variables) is very large compared to the number of samples (obser-\nvations), makes the application of many prediction techniques (e.g., logistic\nregression, discriminant analysis) difficult. An efficient way to solve this\nprob- lem is by using dimension reduction statistical techniques. Increasingly\nused in psychology-related applications, Rasch model (RM) provides an appealing\nframework for handling high-dimensional microarray data. In this paper, we\nstudy the potential of RM-based modeling in dimensionality reduction with\nbinarized microarray gene expression data and investigate its prediction ac-\ncuracy in the context of class prediction using linear discriminant analysis.\nTwo different publicly available microarray data sets are used to illustrate a\ngeneral framework of the approach. Performance of the proposed method is\nassessed by re-randomization scheme using principal component analysis (PCA) as\na benchmark method. Our results show that RM-based dimension reduction is as\neffective as PCA-based dimension reduction. The method is general and can be\napplied to the other high-dimensional data problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:27:29 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""]]}, {"id": "1006.1062", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, Zoubin Ghahramani, Michael I. Jordan", "title": "Tree-Structured Stick Breaking Processes for Hierarchical Data", "comments": "16 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data are naturally modeled by an unobserved hierarchical structure. In\nthis paper we propose a flexible nonparametric prior over unknown data\nhierarchies. The approach uses nested stick-breaking processes to allow for\ntrees of unbounded width and depth, where data can live at any node and are\ninfinitely exchangeable. One can view our model as providing infinite mixtures\nwhere the components have a dependency structure corresponding to an\nevolutionary diffusion down a tree. By using a stick-breaking approach, we can\napply Markov chain Monte Carlo methods based on slice sampling to perform\nBayesian inference and simulate from the posterior distribution on trees. We\napply our method to hierarchical clustering of images and topic modeling of\ntext data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 18:52:13 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Ghahramani", "Zoubin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1006.1138", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari", "title": "Online Learning via Sequential Complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential prediction and provide tools to study\nthe minimax value of the associated game. Classical statistical learning theory\nprovides several useful complexity measures to study learning with i.i.d. data.\nOur proposed sequential complexities can be seen as extensions of these\nmeasures to the sequential setting. The developed theory is shown to yield\nprecise learning guarantees for the problem of sequential prediction. In\nparticular, we show necessary and sufficient conditions for online learnability\nin the setting of supervised learning. Several examples show the utility of our\nframework: we can establish learnability without having to exhibit an explicit\nonline learning algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2010 21:05:27 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 15:41:11 GMT"}, {"version": "v3", "created": "Tue, 12 Aug 2014 16:44:00 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1006.1328", "submitter": "Jonathan Huang", "authors": "Jonathan Huang and Carlos Guestrin", "title": "Uncovering the Riffled Independence Structure of Rankings", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing distributions over permutations can be a daunting task due to\nthe fact that the number of permutations of $n$ objects scales factorially in\n$n$. One recent way that has been used to reduce storage complexity has been to\nexploit probabilistic independence, but as we argue, full independence\nassumptions impose strong sparsity constraints on distributions and are\nunsuitable for modeling rankings. We identify a novel class of independence\nstructures, called \\emph{riffled independence}, encompassing a more expressive\nfamily of distributions while retaining many of the properties necessary for\nperforming efficient inference and reducing sample complexity. In riffled\nindependence, one draws two permutations independently, then performs the\n\\emph{riffle shuffle}, common in card games, to combine the two permutations to\nform a single permutation. Within the context of ranking, riffled independence\ncorresponds to ranking disjoint sets of objects independently, then\ninterleaving those rankings. In this paper, we provide a formal introduction to\nriffled independence and present algorithms for using riffled independence\nwithin Fourier-theoretic frameworks which have been explored by a number of\nrecent papers. Additionally, we propose an automated method for discovering\nsets of items which are riffle independent from a training set of rankings. We\nshow that our clustering-like algorithms can be used to discover meaningful\nlatent coalitions from real preference ranking datasets and to learn the\nstructure of hierarchically decomposable models based on riffled independence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 18:45:46 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Huang", "Jonathan", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1006.1343", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Adam Ganz", "title": "Segmentation and Nodal Points in Narrative: Study of Multiple Variations\n  of a Ballad", "comments": "27 pp., 13 figures. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lady Maisry ballads afford us a framework within which to segment a\nstoryline into its major components. Segments and as a consequence nodal points\nare discussed for nine different variants of the Lady Maisry story of a (young)\nwoman being burnt to death by her family, on account of her becoming pregnant\nby a foreign personage. We motivate the importance of nodal points in textual\nand literary analysis. We show too how the openings of the nine variants can be\nanalyzed comparatively, and also the conclusions of the ballads.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 19:36:18 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Murtagh", "Fionn", ""], ["Ganz", "Adam", ""]]}, {"id": "1006.1346", "submitter": "Ignacio Ramirez", "authors": "Pablo Sprechmann, Ignacio Ram\\'irez, Guillermo Sapiro, Yonina Eldar", "title": "C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2157912", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse modeling is a powerful framework for data analysis and processing.\nTraditionally, encoding in this framework is performed by solving an\nL1-regularized linear regression problem, commonly referred to as Lasso or\nBasis Pursuit. In this work we combine the sparsity-inducing property of the\nLasso model at the individual feature level, with the block-sparsity property\nof the Group Lasso model, where sparse groups of features are jointly encoded,\nobtaining a sparsity pattern hierarchically structured. This results in the\nHierarchical Lasso (HiLasso), which shows important practical modeling\nadvantages. We then extend this approach to the collaborative case, where a set\nof simultaneously coded signals share the same sparsity pattern at the higher\n(group) level, but not necessarily at the lower (inside the group) level,\nobtaining the collaborative HiLasso model (C-HiLasso). Such signals then share\nthe same active groups, or classes, but not necessarily the same active set.\nThis model is very well suited for applications such as source identification\nand separation. An efficient optimization procedure, which guarantees\nconvergence to the global optimum, is developed for these new models. The\nunderlying presentation of the new framework and optimization approach is\ncomplemented with experimental examples and theoretical results regarding\nrecovery guarantees for the proposed models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 19:46:56 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2011 11:58:16 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Sprechmann", "Pablo", ""], ["Ram\u00edrez", "Ignacio", ""], ["Sapiro", "Guillermo", ""], ["Eldar", "Yonina", ""]]}, {"id": "1006.1350", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zoubin Ghahramani", "title": "Copula Processes", "comments": "11 pages, 1 table, 1 figure. Submitted for publication. Since last\n  version: minor edits and reformatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR q-fin.CP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a copula process which describes the dependencies between\narbitrarily many random variables independently of their marginal\ndistributions. As an example, we develop a stochastic volatility model,\nGaussian Copula Process Volatility (GCPV), to predict the latent standard\ndeviations of a sequence of random variables. To make predictions we use\nBayesian inference, with the Laplace approximation, and with Markov chain Monte\nCarlo as an alternative. We find both methods comparable. We also find our\nmodel can outperform GARCH on simulated and financial data. And unlike GARCH,\nGCPV can easily handle missing data, incorporate covariates other than time,\nand model a rich class of covariance structures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 19:59:50 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2010 18:17:41 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1006.1673", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Nithin Michael, Ao Kevin Tang, and Ananthram\n  Swami", "title": "Distributed Algorithms for Learning and Cognitive Medium Access with\n  Logarithmic Regret", "comments": "Submitted to IEEE JSAC on Advances in Cognitive Radio Networking and\n  Communications, Dec. 2009, Revised May 2010", "journal-ref": null, "doi": "10.1109/JSAC.2011.110406", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distributed learning and channel access is considered in a\ncognitive network with multiple secondary users. The availability statistics of\nthe channels are initially unknown to the secondary users and are estimated\nusing sensing decisions. There is no explicit information exchange or prior\nagreement among the secondary users. We propose policies for distributed\nlearning and access which achieve order-optimal cognitive system throughput\n(number of successful secondary transmissions) under self play, i.e., when\nimplemented at all the secondary users. Equivalently, our policies minimize the\nregret in distributed learning and access. We first consider the scenario when\nthe number of secondary users is known to the policy, and prove that the total\nregret is logarithmic in the number of transmission slots. Our distributed\nlearning and access policy achieves order-optimal regret by comparing to an\nasymptotic lower bound for regret under any uniformly-good learning and access\npolicy. We then consider the case when the number of secondary users is fixed\nbut unknown, and is estimated through feedback. We propose a policy in this\nscenario whose asymptotic sum regret which grows slightly faster than\nlogarithmic in the number of transmission slots.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 23:24:13 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Michael", "Nithin", ""], ["Tang", "Ao Kevin", ""], ["Swami", "Ananthram", ""]]}, {"id": "1006.1746", "submitter": "Vianney Perchet", "authors": "Vianney Perchet (EC)", "title": "Calibration and Internal no-Regret with Partial Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrated strategies can be obtained by performing strategies that have no\ninternal regret in some auxiliary game. Such strategies can be constructed\nexplicitly with the use of Blackwell's approachability theorem, in an other\nauxiliary game. We establish the converse: a strategy that approaches a convex\n$B$-set can be derived from the construction of a calibrated strategy. We\ndevelop these tools in the framework of a game with partial monitoring, where\nplayers do not observe the actions of their opponents but receive random\nsignals, to define a notion of internal regret and construct strategies that\nhave no such regret.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 09:02:44 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Perchet", "Vianney", "", "EC"]]}, {"id": "1006.1828", "submitter": "Dariusz Plewczynski", "authors": "Dariusz Plewczynski", "title": "Landau Theory of Adaptive Integration in Computational Intelligence", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI nlin.AO q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Intelligence (CI) is a sub-branch of Artificial Intelligence\nparadigm focusing on the study of adaptive mechanisms to enable or facilitate\nintelligent behavior in complex and changing environments. There are several\nparadigms of CI [like artificial neural networks, evolutionary computations,\nswarm intelligence, artificial immune systems, fuzzy systems and many others],\neach of these has its origins in biological systems [biological neural systems,\nnatural Darwinian evolution, social behavior, immune system, interactions of\norganisms with their environment]. Most of those paradigms evolved into\nseparate machine learning (ML) techniques, where probabilistic methods are used\ncomplementary with CI techniques in order to effectively combine elements of\nlearning, adaptation, evolution and Fuzzy logic to create heuristic algorithms\nthat are, in some sense, intelligent. The current trend is to develop consensus\ntechniques, since no single machine learning algorithms is superior to others\nin all possible situations. In order to overcome this problem several\nmeta-approaches were proposed in ML focusing on the integration of results from\ndifferent methods into single prediction. We discuss here the Landau theory for\nthe nonlinear equation that can describe the adaptive integration of\ninformation acquired from an ensemble of independent learning agents. The\ninfluence of each individual agent on other learners is described similarly to\nthe social impact theory. The final decision outcome for the consensus system\nis calculated using majority rule in the stationary limit, yet the minority\nsolutions can survive inside the majority population as the complex\nintermittent clusters of opposite opinion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 15:11:14 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Plewczynski", "Dariusz", ""]]}, {"id": "1006.2165", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Henrik Ohlsson", "title": "A Probabilistic Perspective on Gaussian Filtering and Smoothing", "comments": "14 pages. Extended version of conference paper (ACC 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.RO cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general probabilistic perspective on Gaussian filtering and\nsmoothing. This allows us to show that common approaches to Gaussian\nfiltering/smoothing can be distinguished solely by their methods of\ncomputing/approximating the means and covariances of joint probabilities. This\nimplies that novel filters and smoothers can be derived straightforwardly by\nproviding methods for computing these moments. Based on this insight, we derive\nthe cubature Kalman smoother and propose a novel robust filtering and smoothing\nalgorithm based on Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 22:23:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2010 00:14:05 GMT"}, {"version": "v3", "created": "Mon, 9 Aug 2010 01:36:08 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2011 20:54:13 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2011 06:15:34 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Ohlsson", "Henrik", ""]]}, {"id": "1006.2940", "submitter": "Zhou Fang", "authors": "Zhou Fang and Nicolai Meinshausen", "title": "LASSO ISOtone for High Dimensional Additive Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive isotonic regression attempts to determine the relationship between a\nmulti-dimensional observation variable and a response, under the constraint\nthat the estimate is the additive sum of univariate component effects that are\nmonotonically increasing. In this article, we present a new method for such\nregression called LASSO Isotone (LISO). LISO adapts ideas from sparse linear\nmodelling to additive isotonic regression. Thus, it is viable in many\nsituations with high dimensional predictor variables, where selection of\nsignificant versus insignificant variables are required. We suggest an\nalgorithm involving a modification of the backfitting algorithm CPAV. We give a\nnumerical convergence result, and finally examine some of its properties\nthrough simulations. We also suggest some possible extensions that improve\nperformance, and allow calculation to be carried out when the direction of the\nmonotonicity is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 09:52:14 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Fang", "Zhou", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1006.3316", "submitter": "Han Liu", "authors": "Han Liu, Kathryn Roeder, Larry Wasserman", "title": "Stability Approach to Regularization Selection (StARS) for High\n  Dimensional Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenging problem in estimating high-dimensional graphical models is to\nchoose the regularization parameter in a data-dependent way. The standard\ntechniques include $K$-fold cross-validation ($K$-CV), Akaike information\ncriterion (AIC), and Bayesian information criterion (BIC). Though these methods\nwork well for low-dimensional problems, they are not suitable in high\ndimensional settings. In this paper, we present StARS: a new stability-based\nmethod for choosing the regularization parameter in high dimensional inference\nfor undirected graphs. The method has a clear interpretation: we use the least\namount of regularization that simultaneously makes a graph sparse and\nreplicable under random sampling. This interpretation requires essentially no\nconditions. Under mild conditions, we show that StARS is partially sparsistent\nin terms of graph estimation: i.e. with high probability, all the true edges\nwill be included in the selected model even when the graph size diverges with\nthe sample size. Empirically, the performance of StARS is compared with the\nstate-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on\nboth synthetic data and a real microarray dataset. StARS outperforms all these\ncompeting procedures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 20:07:20 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Liu", "Han", ""], ["Roeder", "Kathryn", ""], ["Wasserman", "Larry", ""]]}, {"id": "1006.3640", "submitter": "Hannes Nickisch", "authors": "Hannes Nickisch and Carl Edward Rasmussen", "title": "Gaussian Mixture Modeling with Gaussian Process Latent Variable Models", "comments": "11 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density modeling is notoriously difficult for high dimensional data. One\napproach to the problem is to search for a lower dimensional manifold which\ncaptures the main characteristics of the data. Recently, the Gaussian Process\nLatent Variable Model (GPLVM) has successfully been used to find low\ndimensional manifolds in a variety of complex data. The GPLVM consists of a set\nof points in a low dimensional latent space, and a stochastic map to the\nobserved space. We show how it can be interpreted as a density model in the\nobserved space. However, the GPLVM is not trained as a density model and\ntherefore yields bad density estimates. We propose a new training strategy and\nobtain improved generalisation performance and better density estimates in\ncomparative evaluations on several benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 08:55:28 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2010 08:14:46 GMT"}], "update_date": "2010-07-14", "authors_parsed": [["Nickisch", "Hannes", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1006.3901", "submitter": "Fabian Wauthier", "authors": "Fabian L. Wauthier and Michael I. Jordan", "title": "Heavy-Tailed Processes for Selective Shrinkage", "comments": "10 pages, 4 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions are frequently used to enhance the robustness of\nregression and classification methods to outliers in output space. Often,\nhowever, we are confronted with \"outliers\" in input space, which are isolated\nobservations in sparsely populated regions. We show that heavy-tailed\nstochastic processes (which we construct from Gaussian processes via a copula),\ncan be used to improve robustness of regression and classification estimators\nto such outliers by selectively shrinking them more strongly in sparse regions\nthan in dense regions. We carry out a theoretical analysis to show that\nselective shrinkage occurs, provided the marginals of the heavy-tailed process\nhave sufficiently heavy tails. The analysis is complemented by experiments on\nbiological data which indicate significant improvements of estimates in sparse\nregions while producing competitive results in dense regions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2010 23:34:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2010 09:09:01 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wauthier", "Fabian L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1006.3972", "submitter": "John Lafferty", "authors": "Han Liu, Xi Chen, John Lafferty and Larry Wasserman", "title": "Graph-Valued Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models encode in a graph $G$ the dependency structure of\na random vector $Y$. In many applications, it is of interest to model $Y$ given\nanother random vector $X$ as input. We refer to the problem of estimating the\ngraph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In\nthis paper, we propose a semiparametric method for estimating $G(x)$ that\nbuilds a tree on the $X$ space just as in CART (classification and regression\ntrees), but at each leaf of the tree estimates a graph. We call the method\n``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of\nGo-CART using dyadic partitioning trees, establishing oracle inequalities on\nrisk minimization and tree partition consistency. We also demonstrate the\napplication of Go-CART to a meteorological dataset, showing how graph-valued\nregression can provide a useful tool for analyzing complex data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 00:56:37 GMT"}], "update_date": "2010-06-22", "authors_parsed": [["Liu", "Han", ""], ["Chen", "Xi", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "1006.4046", "submitter": "Benjamin Recht", "authors": "Laura Balzano and Robert Nowak and Benjamin Recht", "title": "Online Identification and Tracking of Subspaces from Highly Incomplete\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents GROUSE (Grassmanian Rank-One Update Subspace Estimation),\nan efficient online algorithm for tracking subspaces from highly incomplete\nobservations. GROUSE requires only basic linear algebraic manipulations at each\niteration, and each subspace update can be performed in linear time in the\ndimension of the subspace. The algorithm is derived by analyzing incremental\ngradient descent on the Grassmannian manifold of subspaces. With a slight\nmodification, GROUSE can also be used as an online incremental algorithm for\nthe matrix completion problem of imputing missing entries of a low-rank matrix.\nGROUSE performs exceptionally well in practice both in tracking subspaces and\nas an online algorithm for matrix completion.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 12:12:27 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2011 20:19:54 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Balzano", "Laura", ""], ["Nowak", "Robert", ""], ["Recht", "Benjamin", ""]]}, {"id": "1006.4338", "submitter": "Lauren Hannah", "authors": "Lauren A. Hannah, Warren B. Powell, David M. Blei", "title": "Stochastic Search with an Observable State Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study convex stochastic search problems where a noisy\nobjective function value is observed after a decision is made. There are many\nstochastic search problems whose behavior depends on an exogenous state\nvariable which affects the shape of the objective function. Currently, there is\nno general purpose algorithm to solve this class of problems. We use\nnonparametric density estimation to take observations from the joint\nstate-outcome distribution and use them to infer the optimal decision for a\ngiven query state. We propose two solution methods that depend on the problem\ncharacteristics: function-based and gradient-based optimization. We examine two\nweighting schemes, kernel-based weights and Dirichlet process-based weights,\nfor use with the solution methods. The weights and solution methods are tested\non a synthetic multi-product newsvendor problem and the hour-ahead wind\ncommitment problem. Our results show that in some cases Dirichlet process\nweights offer substantial benefits over kernel based weights and more generally\nthat nonparametric estimation methods provide good solutions to otherwise\nintractable problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 17:36:51 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2010 16:15:05 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Hannah", "Lauren A.", ""], ["Powell", "Warren B.", ""], ["Blei", "David M.", ""]]}, {"id": "1006.5051", "submitter": "Ping Li", "authors": "Ping Li", "title": "Fast ABC-Boost for Multi-Class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abc-boost is a new line of boosting algorithms for multi-class\nclassification, by utilizing the commonly used sum-to-zero constraint. To\nimplement abc-boost, a base class must be identified at each boosting step.\nPrior studies used a very expensive procedure based on exhaustive search for\ndetermining the base class at each boosting step. Good testing performances of\nabc-boost (implemented as abc-mart and abc-logitboost) on a variety of datasets\nwere reported.\n  For large datasets, however, the exhaustive search strategy adopted in prior\nabc-boost algorithms can be too prohibitive. To overcome this serious\nlimitation, this paper suggests a heuristic by introducing Gaps when computing\nthe base class during training. That is, we update the choice of the base class\nonly for every $G$ boosting steps (i.e., G=1 in prior studies). We test this\nidea on large datasets (Covertype and Poker) as well as datasets of moderate\nsizes. Our preliminary results are very encouraging. On the large datasets,\neven with G=100 (or larger), there is essentially no loss of test accuracy. On\nthe moderate datasets, no obvious loss of test accuracy is observed when G<=\n20~50. Therefore, aided by this heuristic, it is promising that abc-boost will\nbe a practical tool for accurate multi-class classification.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 19:48:50 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1006.5060", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye and Xiaohui Xie", "title": "Learning sparse gradients for variable selection and dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection and dimension reduction are two commonly adopted\napproaches for high-dimensional data analysis, but have traditionally been\ntreated separately. Here we propose an integrated approach, called sparse\ngradient learning (SGL), for variable selection and dimension reduction via\nlearning the gradients of the prediction function directly from samples. By\nimposing a sparsity constraint on the gradients, variable selection is achieved\nby selecting variables corresponding to non-zero partial derivatives, and\neffective dimensions are extracted based on the eigenvectors of the derived\nsparse empirical gradient covariance matrix. An error analysis is given for the\nconvergence of the estimated gradients to the true ones in both the Euclidean\nand the manifold setting. We also develop an efficient forward-backward\nsplitting algorithm to solve the SGL problem, making the framework practically\nscalable for medium or large datasets. The utility of SGL for variable\nselection and feature extraction is explicitly given and illustrated on\nartificial data as well as real-world examples. The main advantages of our\nmethod include variable selection for both linear and nonlinear predictions,\neffective dimension reduction with sparse loadings, and an efficient algorithm\nfor large p, small n problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 20:27:00 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2010 05:06:43 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1006.5831", "submitter": "Eric Laber", "authors": "Eric B. Laber and Min Qian and Dan J. Lizotte, William E. Pelham and\n  Susan A. Murphy", "title": "Statistical Inference in Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic treatment regimes are of growing interest across the clinical\nsciences as these regimes provide one way to operationalize and thus inform\nsequential personalized clinical decision making. A dynamic treatment regime is\na sequence of decision rules, with a decision rule per stage of clinical\nintervention; each decision rule maps up-to-date patient information to a\nrecommended treatment. We briefly review a variety of approaches for using data\nto construct the decision rules. We then review an interesting challenge, that\nof nonregularity that often arises in this area. By nonregularity, we mean the\nparameters indexing the optimal dynamic treatment regime are nonsmooth\nfunctionals of the underlying generative distribution.\n  A consequence is that no regular or asymptotically unbiased estimator of\nthese parameters exists. Nonregularity arises in inference for parameters in\nthe optimal dynamic treatment regime; we illustrate the effect of nonregularity\non asymptotic bias and via sensitivity of asymptotic, limiting, distributions\nto local perturbations. We propose and evaluate a locally consistent Adaptive\nConfidence Interval (ACI) for the parameters of the optimal dynamic treatment\nregime. We use data from the Adaptive Interventions for Children with ADHD\nstudy as an illustrative example. We conclude by highlighting and discussing\nemerging theoretical problems in this area.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 11:10:09 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 21:00:14 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 16:54:55 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Laber", "Eric B.", ""], ["Qian", "Min", ""], ["Lizotte", "Dan J.", ""], ["Pelham", "William E.", ""], ["Murphy", "Susan A.", ""]]}]