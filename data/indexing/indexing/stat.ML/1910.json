[{"id": "1910.00019", "submitter": "Sho Yaida", "authors": "Sho Yaida", "title": "Non-Gaussian processes and neural networks at finite widths", "comments": "33 pages, 3 figures; v2: final version accepted at MSML 2020, with\n  some clarification on the connection to renormalization-group flow", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are ubiquitous in nature and engineering. A case in point\nis a class of neural networks in the infinite-width limit, whose priors\ncorrespond to Gaussian processes. Here we perturbatively extend this\ncorrespondence to finite-width neural networks, yielding non-Gaussian processes\nas priors. The methodology developed herein allows us to track the flow of\npreactivation distributions by progressively integrating out random variables\nfrom lower to higher layers, reminiscent of renormalization-group flow. We\nfurther develop a perturbative procedure to perform Bayesian inference with\nweakly non-Gaussian priors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 22:50:38 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yaida", "Sho", ""]]}, {"id": "1910.00024", "submitter": "Shuohui Li", "authors": "Shuo-Hui Li, Chen-Xiao Dong, Linfeng Zhang, and Lei Wang", "title": "Neural Canonical Transformation with Symplectic Flows", "comments": "Main text: 9 pages, 8 figures. Supplement: 2 page, 1 figure. GitHub\n  link: https://github.com/li012589/neuralCT", "journal-ref": "Phys. Rev. X 10, 021020 (2020)", "doi": "10.1103/PhysRevX.10.021020", "report-no": null, "categories": "cond-mat.stat-mech cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical transformation plays a fundamental role in simplifying and solving\nclassical Hamiltonian systems. We construct flexible and powerful canonical\ntransformations as generative models using symplectic neural networks. The\nmodel transforms physical variables towards a latent representation with an\nindependent harmonic oscillator Hamiltonian. Correspondingly, the phase space\ndensity of the physical system flows towards a factorized Gaussian distribution\nin the latent space. Since the canonical transformation preserves the\nHamiltonian evolution, the model captures nonlinear collective modes in the\nlearned latent representation. We present an efficient implementation of\nsymplectic neural coordinate transformations and two ways to train the model.\nThe variational free energy calculation is based on the analytical form of\nphysical Hamiltonian. While the phase space density estimation only requires\nsamples in the coordinate space for separable Hamiltonians. We demonstrate\nappealing features of neural canonical transformation using toy problems\nincluding two-dimensional ring potential and harmonic chain. Finally, we apply\nthe approach to real-world problems such as identifying slow collective modes\nin alanine dipeptide and conceptual compression of the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:00:05 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 14:59:18 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 15:16:17 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Li", "Shuo-Hui", ""], ["Dong", "Chen-Xiao", ""], ["Zhang", "Linfeng", ""], ["Wang", "Lei", ""]]}, {"id": "1910.00054", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Daniel Hsu, Luis Gravano", "title": "Weakly Supervised Attention Networks for Fine-Grained Opinion Mining and\n  Public Health", "comments": "Accepted for the 5th Workshop on Noisy User-generated Text (W-NUT\n  2019), held in conjunction with EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many review classification applications, a fine-grained analysis of the\nreviews is desirable, because different segments (e.g., sentences) of a review\nmay focus on different aspects of the entity in question. However, training\nsupervised models for segment-level classification requires segment labels,\nwhich may be more difficult or expensive to obtain than review labels. In this\npaper, we employ Multiple Instance Learning (MIL) and use only weak supervision\nin the form of a single label per review. First, we show that when\ninappropriate MIL aggregation functions are used, then MIL-based networks are\noutperformed by simpler baselines. Second, we propose a new aggregation\nfunction based on the sigmoid attention mechanism and show that our proposed\nmodel outperforms the state-of-the-art models for segment-level sentiment\nclassification (by up to 9.8% in F1). Finally, we highlight the importance of\nfine-grained predictions in an important public-health application: finding\nactionable reports of foodborne illness. We show that our model achieves 48.6%\nhigher recall compared to previous models, thus increasing the chance of\nidentifying previously unknown foodborne outbreaks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:40:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Hsu", "Daniel", ""], ["Gravano", "Luis", ""]]}, {"id": "1910.00062", "submitter": "Georgi Nalbantov", "authors": "Georgi Nalbantov, Svetoslav Ivanov", "title": "Tutorial on Implied Posterior Probability for SVMs", "comments": "20 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implied posterior probability of a given model (say, Support Vector Machines\n(SVM)) at a point $\\bf{x}$ is an estimate of the class posterior probability\npertaining to the class of functions of the model applied to a given dataset.\nIt can be regarded as a score (or estimate) for the true posterior probability,\nwhich can then be calibrated/mapped onto expected (non-implied by the model)\nposterior probability implied by the underlying functions, which have generated\nthe data. In this tutorial we discuss how to compute implied posterior\nprobabilities of SVMs for the binary classification case as well as how to\ncalibrate them via a standard method of isotonic regression.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:13:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Nalbantov", "Georgi", ""], ["Ivanov", "Svetoslav", ""]]}, {"id": "1910.00063", "submitter": "Daniel Larsson", "authors": "Daniel T. Larsson, Dipankar Maity, Panagiotis Tsiotras", "title": "Q-Search Trees: An Information-Theoretic Approach Towards Hierarchical\n  Abstractions for Agents with Computational Limitations", "comments": null, "journal-ref": "2020 IEEE Transactions on Robotics (T-RO)", "doi": "10.1109/TRO.2020.3003219", "report-no": null, "categories": "cs.RO cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a framework to obtain graph abstractions for\ndecision-making by an agent where the abstractions emerge as a function of the\nagent's limited computational resources. We discuss the connection of the\nproposed approach with information-theoretic signal compression, and formulate\na novel optimization problem to obtain tree-based abstractions as a function of\nthe agent's computational resources. The structural properties of the new\nproblem are discussed in detail, and two algorithmic approaches are proposed to\nobtain solutions to this optimization problem. We discuss the quality of, and\nprove relationships between, solutions obtained by the two proposed algorithms.\nThe framework is demonstrated to generate a hierarchy of abstractions for a\nnon-trivial environment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:21:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Larsson", "Daniel T.", ""], ["Maity", "Dipankar", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1910.00067", "submitter": "Cory Stephenson", "authors": "Cory Stephenson, Gokce Keskin, Anil Thomas, Oguz H. Elibol", "title": "Semi-supervised voice conversion with amortized variational inference", "comments": "Accepted for publication at Interspeech 2019", "journal-ref": "Proc. Interspeech 2019 (2019): 729-733", "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a semi-supervised approach to the voice conversion\nproblem, in which speech from a source speaker is converted into speech of a\ntarget speaker. The proposed method makes use of both parallel and non-parallel\nutterances from the source and target simultaneously during training. This\napproach can be used to extend existing parallel data voice conversion systems\nsuch that they can be trained with semi-supervision. We show that incorporating\nsemi-supervision improves the voice conversion performance compared to fully\nsupervised training when the number of parallel utterances is limited as in\nmany practical applications. Additionally, we find that increasing the number\nnon-parallel utterances used in training continues to improve performance when\nthe amount of parallel training data is held constant.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:39:57 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Stephenson", "Cory", ""], ["Keskin", "Gokce", ""], ["Thomas", "Anil", ""], ["Elibol", "Oguz H.", ""]]}, {"id": "1910.00069", "submitter": "Robert Bamler", "authors": "Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt", "title": "Tightening Bounds for Variational Inference by Revisiting Perturbation\n  Theory", "comments": "To appear in Journal of Statistical Mechanics: Theory and Experiment\n  (JSTAT), 2019", "journal-ref": null, "doi": "10.1088/1742-5468/ab43d3", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has become one of the most widely used methods in\nlatent variable modeling. In its basic form, variational inference employs a\nfully factorized variational distribution and minimizes its KL divergence to\nthe posterior. As the minimization can only be carried out approximately, this\napproximation induces a bias. In this paper, we revisit perturbation theory as\na powerful way of improving the variational approximation. Perturbation theory\nrelies on a form of Taylor expansion of the log marginal likelihood, vaguely in\nterms of the log ratio of the true posterior and its variational approximation.\nWhile first order terms give the classical variational bound, higher-order\nterms yield corrections that tighten it. However, traditional perturbation\ntheory does not provide a lower bound, making it inapt for stochastic\noptimization. In this paper, we present a similar yet alternative way of\nderiving corrections to the ELBO that resemble perturbation theory, but that\nresult in a valid bound. We show in experiments on Gaussian Processes and\nVariational Autoencoders that the new bounds are more mass covering, and that\nthe resulting posterior covariances are closer to the true posterior and lead\nto higher likelihoods on held-out data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:44:18 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Bamler", "Robert", ""], ["Zhang", "Cheng", ""], ["Opper", "Manfred", ""], ["Mandt", "Stephan", ""]]}, {"id": "1910.00078", "submitter": "Jehandad Khan", "authors": "Jehandad Khan, Paul Fultz, Artem Tamazov, Daniel Lowell, Chao Liu,\n  Michael Melesse, Murali Nandhimandalam, Kamil Nasyrov, Ilya Perminov, Tejash\n  Shah, Vasilii Filippov, Jing Zhang, Jing Zhou, Bragadeesh Natarajan, Mayank\n  Daga", "title": "MIOpen: An Open Source Library For Deep Learning Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has established itself to be a common occurrence in the\nbusiness lexicon. The unprecedented success of deep learning in recent years\ncan be attributed to: abundance of data, availability of gargantuan compute\ncapabilities offered by GPUs, and adoption of open-source philosophy by the\nresearchers and industry. Deep neural networks can be decomposed into a series\nof different operators. MIOpen, AMD's open-source deep learning primitives\nlibrary for GPUs, provides highly optimized implementations of such operators,\nshielding researchers from internal implementation details and hence,\naccelerating the time to discovery. This paper introduces MIOpen and provides\ndetails about the internal workings of the library and supported features.\nMIOpen innovates on several fronts, such as implementing fusion to optimize for\nmemory bandwidth and GPU launch overheads, providing an auto-tuning\ninfrastructure to overcome the large design space of problem configurations,\nand implementing different algorithms to optimize convolutions for different\nfilter and input sizes. MIOpen is one of the first libraries to publicly\nsupport the bfloat16 data-type for convolutions, allowing efficient training at\nlower precision without the loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 20:07:36 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Khan", "Jehandad", ""], ["Fultz", "Paul", ""], ["Tamazov", "Artem", ""], ["Lowell", "Daniel", ""], ["Liu", "Chao", ""], ["Melesse", "Michael", ""], ["Nandhimandalam", "Murali", ""], ["Nasyrov", "Kamil", ""], ["Perminov", "Ilya", ""], ["Shah", "Tejash", ""], ["Filippov", "Vasilii", ""], ["Zhang", "Jing", ""], ["Zhou", "Jing", ""], ["Natarajan", "Bragadeesh", ""], ["Daga", "Mayank", ""]]}, {"id": "1910.00084", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, Ni Lao", "title": "Contextual Graph Attention for Answering Logical Queries over Incomplete\n  Knowledge Graphs", "comments": "8 pages, 3 figures, camera ready version of article accepted to K-CAP\n  2019, Marina del Rey, California, United States", "journal-ref": "K-CAP 2019, Nov. 19 - 21, 2019, Marina del Rey, CA, USA", "doi": "10.1145/3360901.3364432", "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several studies have explored methods for using KG embedding to\nanswer logical queries. These approaches either treat embedding learning and\nquery answering as two separated learning tasks, or fail to deal with the\nvariability of contributions from different query paths. We proposed to\nleverage a graph attention mechanism to handle the unequal contribution of\ndifferent query paths. However, commonly used graph attention assumes that the\ncenter node embedding is provided, which is unavailable in this task since the\ncenter node is to be predicted. To solve this problem we propose a multi-head\nattention-based end-to-end logical query answering model, called Contextual\nGraph Attention model(CGA), which uses an initial neighborhood aggregation\nlayer to generate the center embedding, and the whole model is trained jointly\non the original KG structure as well as the sampled query-answer pairs. We also\nintroduce two new datasets, DB18 and WikiGeo19, which are rather large in size\ncompared to the existing datasets and contain many more relation types, and use\nthem to evaluate the performance of the proposed model. Our result shows that\nthe proposed CGA with fewer learnable parameters consistently outperforms the\nbaseline models on both datasets as well as Bio dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 20:20:48 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Yan", "Bo", ""], ["Zhu", "Rui", ""], ["Cai", "Ling", ""], ["Lao", "Ni", ""]]}, {"id": "1910.00099", "submitter": "Wenhao Ding", "authors": "Wenhao Ding, Mengdi Xu, Ding Zhao", "title": "CMTS: Conditional Multiple Trajectory Synthesizer for Generating\n  Safety-critical Driving Scenarios", "comments": "Submitted to ICRA 2020, 8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturalistic driving trajectories are crucial for the performance of\nautonomous driving algorithms. However, most of the data is collected in safe\nscenarios leading to the duplication of trajectories which are easy to be\nhandled by currently developed algorithms. When considering safety, testing\nalgorithms in near-miss scenarios that rarely show up in off-the-shelf datasets\nis a vital part of the evaluation. As a remedy, we propose a near-miss data\nsynthesizing framework based on Variational Bayesian methods and term it as\nConditional Multiple Trajectory Synthesizer (CMTS). We leverage a generative\nmodel conditioned on road maps to bridge safe and collision driving data by\nrepresenting their distribution in the latent space. By sampling from the\nnear-miss distribution, we can synthesize safety-critical data crucial for\nunderstanding traffic scenarios but not shown in neither the original dataset\nnor the collision dataset. Our experimental results demonstrate that the\naugmented dataset covers more kinds of driving scenarios, especially the\nnear-miss ones, which help improve the trajectory prediction accuracy and the\ncapability of dealing with risky driving scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:43:14 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 18:03:36 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ding", "Wenhao", ""], ["Xu", "Mengdi", ""], ["Zhao", "Ding", ""]]}, {"id": "1910.00100", "submitter": "Jiatong Li", "authors": "Jiatong Li, Ricardo Guerrero, Vladimir Pavlovic", "title": "Deep Cooking: Predicting Relative Food Ingredient Amounts from Images", "comments": null, "journal-ref": null, "doi": "10.1145/3347448.3357164", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the novel problem of not only predicting ingredients\nfrom a food image, but also predicting the relative amounts of the detected\ningredients. We propose two prediction-based models using deep learning that\noutput sparse and dense predictions, coupled with important semi-automatic\nmulti-database integrative data pre-processing, to solve the problem.\nExperiments on a dataset of recipes collected from the Internet show the models\ngenerate encouraging experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:49:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Li", "Jiatong", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1910.00101", "submitter": "Maymoonah Toubeh", "authors": "Maymoonah Toubeh and Pratap Tokekar", "title": "Risk-Aware Planning by Confidence Estimation using Deep Learning-Based\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes the use of Bayesian approximations of uncertainty from\ndeep learning in a robot planner, showing that this produces more cautious\nactions in safety-critical scenarios. The case study investigated is motivated\nby a setup where an aerial robot acts as a \"scout\" for a ground robot. This is\nuseful when the below area is unknown or dangerous, with applications in space\nexploration, military, or search-and-rescue. Images taken from the aerial view\nare used to provide a less obstructed map to guide the navigation of the robot\non the ground. Experiments are conducted using a deep learning semantic image\nsegmentation, followed by a path planner based on the resulting cost map, to\nprovide an empirical analysis of the proposed method. A comparison with similar\napproaches is presented to portray the usefulness of certain techniques, or\nvariations within a technique, in similar experimental settings. The method is\nanalyzed to assess the impact of variations in the uncertainty extraction, as\nwell as the absence of an uncertainty metric, on the overall system with the\nuse of a defined metric which measures surprise to the planner. The analysis is\nperformed on multiple datasets, showing a similar trend of lower surprise when\nuncertainty information is incorporated in the planning, given threshold values\nof the hyperparameters in the uncertainty extraction have been met. We find\nthat taking uncertainty into account leads to paths that could be 18% less\nrisky on an average.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:20:41 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Toubeh", "Maymoonah", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1910.00105", "submitter": "Kuno Kim", "authors": "Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, Stefano Ermon", "title": "Domain Adaptive Imitation Learning", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of how to imitate tasks across domains with\ndiscrepancies such as embodiment, viewpoint, and dynamics mismatch. Many prior\nworks require paired, aligned demonstrations and an additional RL step that\nrequires environment interactions. However, paired, aligned demonstrations are\nseldom obtainable and RL procedures are expensive. We formalize the Domain\nAdaptive Imitation Learning (DAIL) problem, which is a unified framework for\nimitation learning in the presence of viewpoint, embodiment, and dynamics\nmismatch. Informally, DAIL is the process of learning how to perform a task\noptimally, given demonstrations of the task in a distinct domain. We propose a\ntwo step approach to DAIL: alignment followed by adaptation. In the alignment\nstep we execute a novel unsupervised MDP alignment algorithm, Generative\nAdversarial MDP Alignment (GAMA), to learn state and action correspondences\nfrom \\emph{unpaired, unaligned} demonstrations. In the adaptation step we\nleverage the correspondences to zero-shot imitate tasks across domains. To\ndescribe when DAIL is feasible via alignment and adaptation, we introduce a\ntheory of MDP alignability. We experimentally evaluate GAMA against baselines\nin embodiment, viewpoint, and dynamics mismatch scenarios where aligned\ndemonstrations don't exist and show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 20:58:55 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 18:36:20 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Kuno", ""], ["Gu", "Yihong", ""], ["Song", "Jiaming", ""], ["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.00125", "submitter": "Rasool Fakoor", "authors": "Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J. Smola", "title": "Meta-Q-Learning", "comments": "ICLR 2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for\nmeta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas.\nFirst, we show that Q-learning is competitive with state-of-the-art meta-RL\nalgorithms if given access to a context variable that is a representation of\nthe past trajectory. Second, a multi-task objective to maximize the average\nreward across the training tasks is an effective method to meta-train RL\npolicies. Third, past data from the meta-training replay buffer can be recycled\nto adapt the policy on a new task using off-policy updates. MQL draws upon\nideas in propensity estimation to do so and thereby amplifies the amount of\navailable data for adaptation. Experiments on standard continuous-control\nbenchmarks suggest that MQL compares favorably with the state of the art in\nmeta-RL.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 21:50:32 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 10:38:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Fakoor", "Rasool", ""], ["Chaudhari", "Pratik", ""], ["Soatto", "Stefano", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1910.00152", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Marco Cuturi and Michael I. Jordan", "title": "On the Complexity of Approximating Multimarginal Optimal Transport", "comments": "Improve the paper significantly; 39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating the multimarginal optimal transport\n(MOT) distance, a generalization of the classical optimal transport distance,\nconsidered here between $m$ discrete probability distributions supported each\non $n$ support points. First, we show that the standard linear programming (LP)\nrepresentation of the MOT problem is not a minimum-cost flow problem when $m\n\\geq 3$. This negative result implies that some combinatorial algorithms, e.g.,\nnetwork simplex method, are not suitable for approximating the MOT problem,\nwhile the worst-case complexity bound for the deterministic interior-point\nalgorithm remains a quantity of $\\tilde{O}(n^{3m})$. We then propose two simple\nand \\textit{deterministic} algorithms for approximating the MOT problem. The\nfirst algorithm, which we refer to as \\textit{multimarginal Sinkhorn}\nalgorithm, is a provably efficient multimarginal generalization of the Sinkhorn\nalgorithm. We show that it achieves a complexity bound of\n$\\tilde{O}(m^3n^m\\varepsilon^{-2})$ for a tolerance $\\varepsilon \\in (0, 1)$.\nThis provides a first \\textit{near-linear time} complexity bound guarantee for\napproximating the MOT problem and matches the best known complexity bound for\nthe Sinkhorn algorithm in the classical OT setting when $m = 2$. The second\nalgorithm, which we refer to as \\textit{accelerated multimarginal Sinkhorn}\nalgorithm, achieves the acceleration by incorporating an estimate sequence and\nthe complexity bound is $\\tilde{O}(m^3n^{m+1/3}\\varepsilon^{-4/3})$. This bound\nis better than that of the first algorithm in terms of $1/\\varepsilon$, and\naccelerated alternating minimization\nalgorithm~\\citep{Tupitsa-2020-Multimarginal} in terms of $n$. Finally, we\ncompare our new algorithms with the commercial LP solver \\textsc{Gurobi}.\nPreliminary results on synthetic data and real images demonstrate the\neffectiveness and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:43:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 23:00:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Cuturi", "Marco", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1910.00164", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Caiming Xiong, Richard Socher", "title": "Predicting with High Correlation Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that instead of learning actual object features, deep\nnetworks tend to exploit non-robust (spurious) discriminative features that are\nshared between training and test sets. Therefore, while they achieve state of\nthe art performance on such test sets, they achieve poor generalization on out\nof distribution (OOD) samples where the IID (independent, identical\ndistribution) assumption breaks and the distribution of non-robust features\nshifts. In this paper, we consider distribution shift as a shift in the\ndistribution of input features during test time that exhibit low correlation\nwith targets in the training set. Under this definition, we evaluate existing\nrobust feature learning methods and regularization methods and compare them\nagainst a baseline designed to specifically capture high correlation features\nin training set. As a controlled test-bed, we design a colored MNIST (C-MNIST)\ndataset and find that existing methods trained on this set fail to generalize\nwell on an OOD version this dataset, showing that they overfit the low\ncorrelation color features. This is avoided by the baseline method trained on\nthe same C-MNIST data, which is designed to learn high correlation features,\nand is able to generalize on the test sets of vanilla MNIST, MNIST-M and SVHN\ndatasets. Our code is available at\n\\url{https://github.com/salesforce/corr_based_prediction}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 00:48:27 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 19:39:27 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Arpit", "Devansh", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1910.00174", "submitter": "Luke Merrick", "authors": "Luke Merrick", "title": "Randomized Ablation Feature Importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a model $f$ that predicts a target $y$ from a vector of input features\n$\\pmb{x} = x_1, x_2, \\ldots, x_M$, we seek to measure the importance of each\nfeature with respect to the model's ability to make a good prediction. To this\nend, we consider how (on average) some measure of goodness or badness of\nprediction (which we term \"loss\" $\\ell$), changes when we hide or ablate each\nfeature from the model. To ablate a feature, we replace its value with another\npossible value randomly. By averaging over many points and many possible\nreplacements, we measure the importance of a feature on the model's ability to\nmake good predictions. Furthermore, we present statistical measures of\nuncertainty that quantify how confident we are that the feature importance we\nmeasure from our finite dataset and finite number of ablations is close to the\ntheoretical true importance value.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 01:56:46 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 02:09:43 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Merrick", "Luke", ""]]}, {"id": "1910.00177", "submitter": "Xue Bin Peng", "authors": "Xue Bin Peng, Aviral Kumar, Grace Zhang, Sergey Levine", "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop a simple and scalable reinforcement learning\nalgorithm that uses standard supervised learning methods as subroutines. Our\ngoal is an algorithm that utilizes only simple and convergent maximum\nlikelihood loss functions, while also being able to leverage off-policy data.\nOur proposed approach, which we refer to as advantage-weighted regression\n(AWR), consists of two standard supervised learning steps: one to regress onto\ntarget values for a value function, and another to regress onto weighted target\nactions for the policy. The method is simple and general, can accommodate\ncontinuous and discrete actions, and can be implemented in just a few lines of\ncode on top of standard supervised learning methods. We provide a theoretical\nmotivation for AWR and analyze its properties when incorporating off-policy\ndata from experience replay. We evaluate AWR on a suite of standard OpenAI Gym\nbenchmark tasks, and show that it achieves competitive performance compared to\na number of well-established state-of-the-art RL algorithms. AWR is also able\nto acquire more effective policies than most off-policy algorithms when\nlearning from purely static datasets with no additional environmental\ninteractions. Furthermore, we demonstrate our algorithm on challenging\ncontinuous control tasks with highly complex simulated characters.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 02:23:38 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 03:56:32 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 20:23:21 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Peng", "Xue Bin", ""], ["Kumar", "Aviral", ""], ["Zhang", "Grace", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.00185", "submitter": "Xiang Li", "authors": "Jiaming Guo, Wei Qiu, Xiang Li, Xuandong Zhao, Ning Guo, Quanzheng Li", "title": "Predicting Alzheimer's Disease by Hierarchical Graph Convolution from\n  Positron Emission Tomography Imaging", "comments": "Jiaming Guo, Wei Qiu and Xiang Li contribute equally to this work", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9005971", "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging-based early diagnosis of Alzheimer Disease (AD) has become an\neffective approach, especially by using nuclear medicine imaging techniques\nsuch as Positron Emission Topography (PET). In various literature it has been\nfound that PET images can be better modeled as signals (e.g. uptake of\nflorbetapir) defined on a network (non-Euclidean) structure which is governed\nby its underlying graph patterns of pathological progression and metabolic\nconnectivity. In order to effectively apply deep learning framework for PET\nimage analysis to overcome its limitation on Euclidean grid, we develop a\nsolution for 3D PET image representation and analysis under a generalized,\ngraph-based CNN architecture (PETNet), which analyzes PET signals defined on a\ngroup-wise inferred graph structure. Computations in PETNet are defined in\nnon-Euclidean, graph (network) domain, as it performs feature extraction by\nconvolution operations on spectral-filtered signals on the graph and pooling\noperations based on hierarchical graph clustering. Effectiveness of the PETNet\nis evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset,\nwhich shows improved performance over both deep learning and other machine\nlearning-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 03:05:18 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Guo", "Jiaming", ""], ["Qiu", "Wei", ""], ["Li", "Xiang", ""], ["Zhao", "Xuandong", ""], ["Guo", "Ning", ""], ["Li", "Quanzheng", ""]]}, {"id": "1910.00189", "submitter": "Kevin Hsieh", "authors": "Kevin Hsieh, Amar Phanishayee, Onur Mutlu, Phillip B. Gibbons", "title": "The Non-IID Data Quagmire of Decentralized Machine Learning", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML), 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning (ML) applications need to perform\ndecentralized learning over datasets generated at different devices and\nlocations. Such datasets pose a significant challenge to decentralized learning\nbecause their different contexts result in significant data distribution skew\nacross devices/locations. In this paper, we take a step toward better\nunderstanding this challenge by presenting a detailed experimental study of\ndecentralized DNN training on a common type of data skew: skewed distribution\nof data labels across devices/locations. Our study shows that: (i) skewed data\nlabels are a fundamental and pervasive problem for decentralized learning,\ncausing significant accuracy loss across many ML applications, DNN models,\ntraining datasets, and decentralized learning algorithms; (ii) the problem is\nparticularly challenging for DNN models with batch normalization; and (iii) the\ndegree of data skew is a key determinant of the difficulty of the problem.\nBased on these findings, we present SkewScout, a system-level approach that\nadapts the communication frequency of decentralized learning algorithms to the\n(skew-induced) accuracy loss between data partitions. We also show that group\nnormalization can recover much of the accuracy loss of batch normalization.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 03:52:47 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 00:58:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hsieh", "Kevin", ""], ["Phanishayee", "Amar", ""], ["Mutlu", "Onur", ""], ["Gibbons", "Phillip B.", ""]]}, {"id": "1910.00195", "submitter": "Mingwei Wei", "authors": "Mingwei Wei, David J Schwab", "title": "How noise affects the Hessian spectrum in overparameterized neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) forms the core optimization method for deep\nneural networks. While some theoretical progress has been made, it still\nremains unclear why SGD leads the learning dynamics in overparameterized\nnetworks to solutions that generalize well. Here we show that for\noverparameterized networks with a degenerate valley in their loss landscape,\nSGD on average decreases the trace of the Hessian of the loss. We also\ngeneralize this result to other noise structures and show that isotropic noise\nin the non-degenerate subspace of the Hessian decreases its determinant. In\naddition to explaining SGDs role in sculpting the Hessian spectrum, this opens\nthe door to new optimization approaches that may confer better generalization\nperformance. We test our results with experiments on toy models and deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 04:13:27 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:41:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wei", "Mingwei", ""], ["Schwab", "David J", ""]]}, {"id": "1910.00201", "submitter": "Guangyuan Zhao", "authors": "Yunhao Ba, Guangyuan Zhao and Achuta Kadambi", "title": "Blending Diverse Physical Priors with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning in context of physical systems merits a re-examination of\nthe learning strategy. In addition to data, one can leverage a vast library of\nphysical prior models (e.g. kinematics, fluid flow, etc) to perform more robust\ninference. The nascent sub-field of \\emph{physics-based learning} (PBL) studies\nthe blending of neural networks with physical priors. While previous PBL\nalgorithms have been applied successfully to specific tasks, it is hard to\ngeneralize existing PBL methods to a wide range of physics-based problems. Such\ngeneralization would require an architecture that can adapt to variations in\nthe correctness of the physics, or in the quality of training data. No such\narchitecture exists. In this paper, we aim to generalize PBL, by making a first\nattempt to bring neural architecture search (NAS) to the realm of PBL. We\nintroduce a new method known as physics-based neural architecture search\n(PhysicsNAS) that is a top-performer across a diverse range of quality in the\nphysical model and the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 05:01:19 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Ba", "Yunhao", ""], ["Zhao", "Guangyuan", ""], ["Kadambi", "Achuta", ""]]}, {"id": "1910.00204", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Manfred K. Warmuth", "title": "TriMap: Large-scale Dimensionality Reduction Using Triplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ``TriMap''; a dimensionality reduction technique based on\ntriplet constraints that preserves the global accuracy of the data better than\nthe other commonly used methods such as t-SNE, LargeVis, and UMAP. To quantify\nthe global accuracy, we introduce a score which roughly reflects the relative\nplacement of the clusters rather than the individual points. We empirically\nshow the excellent performance of TriMap on a large variety of datasets in\nterms of the quality of the embedding as well as the runtime. On our\nperformance benchmarks, TriMap easily scales to millions of points without\ndepleting the memory and clearly outperforms t-SNE, LargeVis, and UMAP in terms\nof runtime.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 05:28:57 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Amid", "Ehsan", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1910.00216", "submitter": "Akihiro Nakamura", "authors": "Akihiro Nakamura, Tatsuya Harada", "title": "Revisiting Fine-tuning for Few-shot Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is the process of learning novel classes using only a few\nexamples and it remains a challenging task in machine learning. Many\nsophisticated few-shot learning algorithms have been proposed based on the\nnotion that networks can easily overfit to novel examples if they are simply\nfine-tuned using only a few examples. In this study, we show that in the\ncommonly used low-resolution mini-ImageNet dataset, the fine-tuning method\nachieves higher accuracy than common few-shot learning algorithms in the 1-shot\ntask and nearly the same accuracy as that of the state-of-the-art algorithm in\nthe 5-shot task. We then evaluate our method with more practical tasks, namely\nthe high-resolution single-domain and cross-domain tasks. With both tasks, we\nshow that our method achieves higher accuracy than common few-shot learning\nalgorithms. We further analyze the experimental results and show that: 1) the\nretraining process can be stabilized by employing a low learning rate, 2) using\nadaptive gradient optimizers during fine-tuning can increase test accuracy, and\n3) test accuracy can be improved by updating the entire network when a large\ndomain-shift exists between base and novel classes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:21:50 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 04:53:10 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Nakamura", "Akihiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1910.00237", "submitter": "Irene Unceta", "authors": "Irene Unceta, Diego Palacios, Jordi Nin, Oriol Pujol", "title": "Sampling Unknown Decision Functions to Build Classifier Copies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copies have been proposed as a viable alternative to endow machine learning\nmodels with properties and features that adapt them to changing needs. A\nfundamental step of the copying process is generating an unlabelled set of\npoints to explore the decision behavior of the targeted classifier throughout\nthe input space. In this article we propose two sampling strategies to produce\nsuch sets. We validate them in six well-known problems and compare them with\ntwo standard methods. We evaluate our proposals in terms of both their accuracy\nperformance and their computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 07:54:26 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Unceta", "Irene", ""], ["Palacios", "Diego", ""], ["Nin", "Jordi", ""], ["Pujol", "Oriol", ""]]}, {"id": "1910.00270", "submitter": "Daniel Greenfeld", "authors": "Daniel Greenfeld, Uri Shalit", "title": "Robust Learning with the Hilbert-Schmidt Independence Criterion", "comments": "Proceedings of the 37th International Conference on Machine Learning\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of a non-parametric independence measure, the\nHilbert-Schmidt Independence Criterion (HSIC), as a loss-function for learning\nrobust regression and classification models. This loss-function encourages\nlearning models where the distribution of the residuals between the label and\nthe model prediction is statistically independent of the distribution of the\ninstances themselves. This loss-function was first proposed by Mooij et al.\n(2009) in the context of learning causal graphs. We adapt it to the task of\nlearning for unsupervised covariate shift: learning on a source domain without\naccess to any instances or labels from the unknown target domain, but with the\nassumption that $p(y|x)$ (the conditional probability of labels given\ninstances) remains the same in the target domain. We show that the proposed\nloss is expected to give rise to models that generalize well on a class of\ntarget domains characterised by the complexity of their description within a\nreproducing kernel Hilbert space. Experiments on unsupervised covariate shift\ntasks demonstrate that models learned with the proposed loss-function\noutperform models learned with standard loss functions, achieving\nstate-of-the-art results on a challenging cell-microscopy unsupervised\ncovariate shift task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:27:33 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 15:41:43 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 12:55:51 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 14:46:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Greenfeld", "Daniel", ""], ["Shalit", "Uri", ""]]}, {"id": "1910.00292", "submitter": "Florian Schmidt", "authors": "Florian Schmidt", "title": "Generalization in Generation: A closer look at Exposure Bias", "comments": "wngt2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure bias refers to the train-test discrepancy that seemingly arises when\nan autoregressive generative model uses only ground-truth contexts at training\ntime but generated ones at test time. We separate the contributions of the\nmodel and the learning framework to clarify the debate on consequences and\nreview proposed counter-measures. In this light, we argue that generalization\nis the underlying property to address and propose unconditional generation as\nits fundamental benchmark. Finally, we combine latent variable modeling with a\nrecent formulation of exploration in reinforcement learning to obtain a\nrigorous handling of true and generated contexts. Results on language modeling\nand variational sentence auto-encoding confirm the model's generalization\ncapability.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 10:28:32 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 06:55:36 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Schmidt", "Florian", ""]]}, {"id": "1910.00314", "submitter": "Yatin Chaudhary", "authors": "Yatin Chaudhary, Pankaj Gupta, Hinrich Sch\\\"utze", "title": "BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using\n  Topics and Attention Based Query-Document-Sentence Interactions", "comments": "EMNLP2019, 10 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our system details and results of participation in the\nRDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a\nmulti-dimensional and broad framework to describe mental health disorders by\ncombining knowledge from genomics to behaviour. Non-availability of RDoC\nlabelled dataset and tedious labelling process hinders the use of RDoC\nframework to reach its full potential in Biomedical research community and\nHealthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed\nabstracts relevant to a given RDoC construct and Task-2 aims at extraction of\nthe most relevant sentence from a given PubMed abstract. We investigate (1)\nattention based supervised neural topic model and SVM for retrieval and ranking\nof PubMed abstracts and, further utilize BM25 and other relevance measures for\nre-ranking, (2) supervised and unsupervised sentence ranking models utilizing\nmulti-view representations comprising of query-aware attention-based sentence\nrepresentation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved\n1st rank and scored 0.86 mean average precision (mAP) and 0.58 macro average\naccuracy (MAA) in Task-1 and Task-2 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:47:36 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 08:06:02 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Chaudhary", "Yatin", ""], ["Gupta", "Pankaj", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1910.00330", "submitter": "Sayed Soroush Haj Zargarbashi", "authors": "S. Soroush Haj Zargarbashi, Bagher Babaali", "title": "A Multi-Modal Feature Embedding Approach to Diagnose Alzheimer Disease\n  from Spoken Language", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Alzheimer's disease is a type of dementia in which early\ndiagnosis plays a major rule in the quality of treatment. Among new works in\nthe diagnosis of Alzheimer's disease, there are many of them analyzing the\nvoice stream acoustically, syntactically or both. The mostly used tools to\nperform these analysis usually include machine learning techniques. Objective:\nDesigning an automatic machine learning based diagnosis system will help in the\nprocedure of early detection. Also, systems, using noninvasive data are\npreferable. Methods: We used are classification system based on spoken\nlanguage. We use three (statistical and neural) approaches to classify audio\nsignals from spoken language into two classes of dementia and control. Result:\nThis work designs a multi-modal feature embedding on the spoken language audio\nsignal using three approaches; N-gram, i-vector, and x-vector. The evaluation\nof the system is done on the cookie picture description task from Pitt Corpus\ndementia bank with the accuracy of 83:6\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:03:24 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zargarbashi", "S. Soroush Haj", ""], ["Babaali", "Bagher", ""]]}, {"id": "1910.00341", "submitter": "Myunghun Jung", "authors": "Myunghun Jung, Hyungjun Lim, Jahyun Goo, Youngmoon Jung, and Hoirin\n  Kim", "title": "Additional Shared Decoder on Siamese Multi-view Encoders for Learning\n  Acoustic Word Embeddings", "comments": "Accepted at 2019 IEEE Automatic Speech Recognition and Understanding\n  Workshop (ASRU 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic word embeddings --- fixed-dimensional vector representations of\narbitrary-length words --- have attracted increasing interest in\nquery-by-example spoken term detection. Recently, on the fact that the\northography of text labels partly reflects the phonetic similarity between the\nwords' pronunciation, a multi-view approach has been introduced that jointly\nlearns acoustic and text embeddings. It showed that it is possible to learn\ndiscriminative embeddings by designing the objective which takes text labels as\nwell as word segments. In this paper, we propose a network architecture that\nexpands the multi-view approach by combining the Siamese multi-view encoders\nwith a shared decoder network to maximize the effect of the relationship\nbetween acoustic and text embeddings in embedding space. Discriminatively\ntrained with multi-view triplet loss and decoding loss, our proposed approach\nachieves better performance on acoustic word discrimination task with the WSJ\ndataset, resulting in 11.1% relative improvement in average precision. We also\npresent experimental results on cross-view word discrimination and word level\nspeech recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:36:18 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Jung", "Myunghun", ""], ["Lim", "Hyungjun", ""], ["Goo", "Jahyun", ""], ["Jung", "Youngmoon", ""], ["Kim", "Hoirin", ""]]}, {"id": "1910.00359", "submitter": "Jonas Geiping", "authors": "Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller and\n  Tom Goldstein", "title": "Truth or Backpropaganda? An Empirical Investigation of Deep Learning\n  Theory", "comments": "18 pages, 6 figures. First two authors contributed equally. Published\n  as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically evaluate common assumptions about neural networks that are\nwidely held by practitioners and theorists alike. In this work, we: (1) prove\nthe widespread existence of suboptimal local minima in the loss landscape of\nneural networks, and we use our theory to find examples; (2) show that\nsmall-norm parameters are not optimal for generalization; (3) demonstrate that\nResNets do not conform to wide-network theories, such as the neural tangent\nkernel, and that the interaction between skip connections and batch\nnormalization plays a role; (4) find that rank does not correlate with\ngeneralization or robustness in a practical setting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:09:46 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 13:26:16 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 16:29:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Goldblum", "Micah", ""], ["Geiping", "Jonas", ""], ["Schwarzschild", "Avi", ""], ["Moeller", "Michael", ""], ["Goldstein", "Tom", ""]]}, {"id": "1910.00370", "submitter": "Yijun Bian", "authors": "Yijun Bian, Qingquan Song, Mengnan Du, Jun Yao, Huanhuan Chen, Xia Hu", "title": "Sub-Architecture Ensemble Pruning in Neural Architecture Search", "comments": "Accepted by TNNLS. This work was done when the first author was a\n  visiting research scholar at Texas A&M University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) is gaining more and more attention in recent\nyears due to its flexibility and remarkable capability to reduce the burden of\nneural network design. To achieve better performance, however, the searching\nprocess usually costs massive computations that might not be affordable for\nresearchers and practitioners. While recent attempts have employed ensemble\nlearning methods to mitigate the enormous computational cost, however, they\nneglect a key property of ensemble methods, namely diversity, which leads to\ncollecting more similar sub-architectures with potential redundancy in the\nfinal design. To tackle this problem, we propose a pruning method for NAS\nensembles called \"Sub-Architecture Ensemble Pruning in Neural Architecture\nSearch (SAEP).\" It targets to leverage diversity and to achieve sub-ensemble\narchitectures at a smaller size with comparable performance to ensemble\narchitectures that are not pruned. Three possible solutions are proposed to\ndecide which sub-architectures to prune during the searching process.\nExperimental results exhibit the effectiveness of the proposed method by\nlargely reducing the number of sub-architectures without degrading the\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:26:54 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:37:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bian", "Yijun", ""], ["Song", "Qingquan", ""], ["Du", "Mengnan", ""], ["Yao", "Jun", ""], ["Chen", "Huanhuan", ""], ["Hu", "Xia", ""]]}, {"id": "1910.00387", "submitter": "Fei Wu", "authors": "Fei Wu, Thomas Michel and Alexandre Briot", "title": "Leveraging Model Interpretability and Stability to increase Model\n  Robustness", "comments": "2019 ICCV workshop on Interpreting and Explaining Visual AI models; 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art Deep Neural Networks (DNN) can now achieve above human level\naccuracy on image classification tasks. However their outstanding performances\ncome along with a complex inference mechanism making them arduously\ninterpretable models. In order to understand the underlying prediction rules of\nDNNs, Dhamdhere et al. propose an interpretability method to break down a DNN\nprediction score as sum of its hidden unit contributions, in the form of a\nmetric called conductance. Analyzing conductances of DNN hidden units, we find\nout there is a difference in how wrong and correct predictions are inferred. We\nidentify distinguishable patterns of hidden unit activations for wrong and\ncorrect predictions. We then use an error detector in the form of a binary\nclassifier on top of the DNN to automatically discriminate wrong and correct\npredictions of the DNN based on their hidden unit activations. Detected wrong\npredictions are discarded, increasing the model robustness. A different\napproach to distinguish wrong and correct predictions of DNNs is proposed by\nWang et al. whose method is based on the premise that input samples leading a\nDNN into making wrong predictions are less stable to the DNN weight changes\nthan correctly classified input samples. In our study, we compare both methods\nand find out by combining them that better detection of wrong predictions can\nbe achieved.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:51:56 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 23:23:04 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Wu", "Fei", ""], ["Michel", "Thomas", ""], ["Briot", "Alexandre", ""]]}, {"id": "1910.00391", "submitter": "Jacob S{\\o}gaard Larsen Mr.", "authors": "Jacob S{\\o}gaard Larsen, Line Clemmensen", "title": "Deep learning for Chemometric and non-translational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to train deep convolutional neural networks which\nlearn from multiple data sets of varying input sizes through weight sharing.\nThis is an advantage in chemometrics where individual measurements represent\nexact chemical compounds and thus signals cannot be translated or resized\nwithout disturbing their interpretation. Our approach show superior performance\ncompared to transfer learning when a medium sized and a small data set are\ntrained together. While we observe a small improvement compared to individual\ntraining when two medium sized data sets are trained together, in particular\nthrough a reduction in the variance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:55:44 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 12:37:08 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 11:30:12 GMT"}, {"version": "v4", "created": "Thu, 7 Nov 2019 13:54:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Larsen", "Jacob S\u00f8gaard", ""], ["Clemmensen", "Line", ""]]}, {"id": "1910.00393", "submitter": "Johannes Haupt", "authors": "Johannes Haupt, Daniel Jacob, Robin M. Gubela, Stefan Lessmann", "title": "Affordable Uplift: Supervised Randomization in Controlled Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer scoring models are the core of scalable direct marketing. Uplift\nmodels provide an estimate of the incremental benefit from a treatment that is\nused for operational decision-making. Training and monitoring of uplift models\nrequire experimental data. However, the collection of data under randomized\ntreatment assignment is costly, since random targeting deviates from an\nestablished targeting policy. To increase the cost-efficiency of\nexperimentation and facilitate frequent data collection and model training, we\nintroduce supervised randomization. It is a novel approach that integrates\nexisting scoring models into randomized trials to target relevant customers,\nwhile ensuring consistent estimates of treatment effects through correction for\nactive sample selection. An empirical Monte Carlo study shows that data\ncollection under supervised randomization is cost-efficient, while downstream\nuplift models perform competitively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:01:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Haupt", "Johannes", ""], ["Jacob", "Daniel", ""], ["Gubela", "Robin M.", ""], ["Lessmann", "Stefan", ""]]}, {"id": "1910.00399", "submitter": "David Isele", "authors": "David Isele, Alireza Nakhaei, and Kikuo Fujimura", "title": "Safe Reinforcement Learning on Autonomous Vehicles", "comments": null, "journal-ref": "IROS 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been numerous advances in reinforcement learning, but the\ntypically unconstrained exploration of the learning process prevents the\nadoption of these methods in many safety critical applications. Recent work in\nsafe reinforcement learning uses idealized models to achieve their guarantees,\nbut these models do not easily accommodate the stochasticity or\nhigh-dimensionality of real world systems. We investigate how prediction\nprovides a general and intuitive framework to constraint exploration, and show\nhow it can be used to safely learn intersection handling behaviors on an\nautonomous vehicle.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 20:36:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Isele", "David", ""], ["Nakhaei", "Alireza", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1910.00406", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Junlin Yang, James S.\n  Duncan", "title": "Decision Explanation and Feature Importance for Invertible Networks", "comments": "Correct notations", "journal-ref": "ICCVW 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks and hard to\ninterpret because of their black-box nature. The recently proposed invertible\nnetwork is able to accurately reconstruct the inputs to a layer from its\noutputs, thus has the potential to unravel the black-box model. An invertible\nnetwork classifier can be viewed as a two-stage model: (1) invertible\ntransformation from input space to the feature space; (2) a linear classifier\nin the feature space. We can determine the decision boundary of a linear\nclassifier in the feature space; since the transform is invertible, we can\ninvert the decision boundary from the feature space to the input space.\nFurthermore, we propose to determine the projection of a data point onto the\ndecision boundary, and define explanation as the difference between data and\nits projection. Finally, we propose to locally approximate a neural network\nwith its first-order Taylor expansion, and define feature importance using a\nlocal linear model. We provide the implementation of our method:\n\\url{https://github.com/juntang-zhuang/explain_invertible}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 01:01:58 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 03:34:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhuang", "Juntang", ""], ["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Yang", "Junlin", ""], ["Duncan", "James S.", ""]]}, {"id": "1910.00411", "submitter": "Jiachun Liao", "authors": "Peter Kairouz and Jiachun Liao and Chong Huang and Maunil Vyas and\n  Monica Welfert and Lalitha Sankar", "title": "Generating Fair Universal Representations using Adversarial Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven framework for learning fair universal\nrepresentations (FUR) that guarantee statistical fairness for any learning task\nthat may not be known a priori. Our framework leverages recent advances in\nadversarial learning to allow a data holder to learn representations in which a\nset of sensitive attributes are decoupled from the rest of the dataset. We\nformulate this as a constrained minimax game between an encoder and an\nadversary where the constraint ensures a measure of usefulness (utility) of the\nrepresentation. The resulting problem is that of censoring, i.e., finding a\nrepresentation that is least informative about the sensitive attributes given a\nutility constraint. For appropriately chosen adversarial loss functions, our\ncensoring framework precisely clarifies the optimal adversarial strategy\nagainst strong information-theoretic adversaries; it also achieves the fairness\nmeasure of demographic parity for the resulting constrained representations. We\nevaluate the performance of our proposed framework on both synthetic and\npublicly available datasets. For these datasets, we use two tradeoff measures:\ncensoring vs. representation fidelity and fairness vs. utility for downstream\ntasks, to amply demonstrate that multiple sensitive features can be effectively\ncensored even as the resulting fair representations ensure accuracy for\nmultiple downstream tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 23:06:09 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 18:45:36 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 23:58:08 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 19:49:50 GMT"}, {"version": "v5", "created": "Tue, 24 Mar 2020 05:56:28 GMT"}, {"version": "v6", "created": "Fri, 30 Apr 2021 01:07:44 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kairouz", "Peter", ""], ["Liao", "Jiachun", ""], ["Huang", "Chong", ""], ["Vyas", "Maunil", ""], ["Welfert", "Monica", ""], ["Sankar", "Lalitha", ""]]}, {"id": "1910.00423", "submitter": "Keith Levin", "authors": "Keith Levin, Fred Roosta, Minh Tang, Michael W. Mahoney, Carey E.\n  Priebe", "title": "Limit theorems for out-of-sample extensions of the adjacency and\n  Laplacian spectral embeddings", "comments": "Portions of this work originally appeared in ICML2018 as\n  \"Out-of-sample extension of graph adjacency spectral embedding\" (accompanying\n  technical report available at arXiv:1802.06307). This work extends the\n  results of that earlier paper to a second graph embedding technique called\n  the Laplacian spectral embedding and presents additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings, a class of dimensionality reduction techniques designed for\nrelational data, have proven useful in exploring and modeling network\nstructure. Most dimensionality reduction methods allow out-of-sample\nextensions, by which an embedding can be applied to observations not present in\nthe training set. Applied to graphs, the out-of-sample extension problem\nconcerns how to compute the embedding of a vertex that is added to the graph\nafter an embedding has already been computed. In this paper, we consider the\nout-of-sample extension problem for two graph embedding procedures: the\nadjacency spectral embedding and the Laplacian spectral embedding. In both\ncases, we prove that when the underlying graph is generated according to a\nlatent space model called the random dot product graph, which includes the\npopular stochastic block model as a special case, an out-of-sample extension\nbased on a least-squares objective obeys a central limit theorem about the true\nlatent position of the out-of-sample vertex. In addition, we prove a\nconcentration inequality for the out-of-sample extension of the adjacency\nspectral embedding based on a maximum-likelihood objective. Our results also\nyield a convenient framework in which to analyze trade-offs between estimation\naccuracy and computational expense, which we explore briefly.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 04:02:10 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Levin", "Keith", ""], ["Roosta", "Fred", ""], ["Tang", "Minh", ""], ["Mahoney", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1910.00452", "submitter": "Balasubramaniam Srinivasan", "authors": "Balasubramaniam Srinivasan and Bruno Ribeiro", "title": "On the Equivalence between Positional Node Embeddings and Structural\n  Graph Representations", "comments": "This version corrects some typos in the definition of \\Sigma, it\n  should be \\Sigma_n. Code available at\n  https://github.com/PurdueMINDS/Equivalence", "journal-ref": "Published as a conference paper at the Eighth International\n  Conference on Learning Representations (ICLR 2020)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides the first unifying theoretical framework for node\n(positional) embeddings and structural graph representations, bridging methods\nlike matrix factorization and graph neural networks. Using invariant theory, we\nshow that the relationship between structural representations and node\nembeddings is analogous to that of a distribution and its samples. We prove\nthat all tasks that can be performed by node embeddings can also be performed\nby structural representations and vice-versa. We also show that the concept of\ntransductive and inductive learning is unrelated to node embeddings and graph\nrepresentations, clearing another source of confusion in the literature.\nFinally, we introduce new practical guidelines to generating and using node\nembeddings, which fixes significant shortcomings of standard operating\nprocedures used today.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:37:22 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 18:26:37 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 01:08:22 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Srinivasan", "Balasubramaniam", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1910.00462", "submitter": "Ivan Donadello", "authors": "Ivan Donadello and Luciano Serafini", "title": "Compensating Supervision Incompleteness with Prior Knowledge in Semantic\n  Image Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Image Interpretation is the task of extracting a structured semantic\ndescription from images. This requires the detection of visual relationships:\ntriples (subject,relation,object) describing a semantic relation between a\nsubject and an object. A pure supervised approach to visual relationship\ndetection requires a complete and balanced training set for all the possible\ncombinations of (subject, relation, object). However, such training sets are\nnot available and would require a prohibitive human effort. This implies the\nability of predicting triples which do not appear in the training set. This\nproblem is called zero-shot learning. State-of-the-art approaches to zero-shot\nlearning exploit similarities among relationships in the training set or\nexternal linguistic knowledge. In this paper, we perform zero-shot learning by\nusing Logic Tensor Networks, a novel Statistical Relational Learning framework\nthat exploits both the similarities with other seen relationships and\nbackground knowledge, expressed with logical constraints between subjects,\nrelations and objects. The experiments on the Visual Relationship Dataset show\nthat the use of logical constraints outperforms the current methods. This\nimplies that background knowledge can be used to alleviate the incompleteness\nof training sets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:56:08 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Donadello", "Ivan", ""], ["Serafini", "Luciano", ""]]}, {"id": "1910.00482", "submitter": "Di Wang", "authors": "Di Wang and Lijie Hu and Huanyu Zhang and Marco Gaboardi and Jinhui Xu", "title": "Estimating Smooth GLM in Non-interactive Local Differential Privacy\n  Model with Public Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the problem of estimating smooth Generalized Linear\nModels (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.\nDifferent from its classical setting, our model allows the server to access\nsome additional public but unlabeled data. By using Stein's lemma and its\nvariants, we first show that there is an $(\\epsilon, \\delta)$-NLDP algorithm\nfor GLM (under some mild assumptions), if each data record is i.i.d sampled\nfrom some sub-Gaussian distribution with bounded $\\ell_1$-norm. Then with high\nprobability, the sample complexity of the public and private data, for the\nalgorithm to achieve an $\\alpha$ estimation error (in $\\ell_\\infty$-norm), is\n$O(p^2\\alpha^{-2})$ and ${O}(p^2\\alpha^{-2}\\epsilon^{-2})$, respectively, if\n$\\alpha$ is not too small ({\\em i.e.,} $\\alpha\\geq\n\\Omega(\\frac{1}{\\sqrt{p}})$), where $p$ is the dimensionality of the data. This\nis a significant improvement over the previously known quasi-polynomial (in\n$\\alpha$) or exponential (in $p$) complexity of GLM with no public data. Also,\nour algorithm can answer multiple (at most $\\exp(O(p))$) GLM queries with the\nsame sample complexities as in the one GLM query case with at least constant\nprobability. We then extend our idea to the non-linear regression problem and\nshow a similar phenomenon for it. Finally, we demonstrate the effectiveness of\nour algorithms through experiments on both synthetic and real world datasets.\nTo our best knowledge, this is the first paper showing the existence of\nefficient and effective algorithms for GLM and non-linear regression in the\nNLDP model with public unlabeled data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:31:15 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 16:15:36 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 14:08:52 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Di", ""], ["Hu", "Lijie", ""], ["Zhang", "Huanyu", ""], ["Gaboardi", "Marco", ""], ["Xu", "Jinhui", ""]]}, {"id": "1910.00511", "submitter": "Yang Zhang", "authors": "Yang Zhang, Shiyu Chang, Mo Yu, Kaizhi Qian", "title": "An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major paradigms of white-box adversarial attacks that attempt\nto impose input perturbations. The first paradigm, called the fix-perturbation\nattack, crafts adversarial samples within a given perturbation level. The\nsecond paradigm, called the zero-confidence attack, finds the smallest\nperturbation needed to cause mis-classification, also known as the margin of an\ninput feature. While the former paradigm is well-resolved, the latter is not.\nExisting zero-confidence attacks either introduce significant ap-proximation\nerrors, or are too time-consuming. We therefore propose MARGINATTACK, a\nzero-confidence attack framework that is able to compute the margin with\nimproved accuracy and efficiency. Our experiments show that MARGINATTACK is\nable to compute a smaller margin than the state-of-the-art zero-confidence\nattacks, and matches the state-of-the-art fix-perturbation at-tacks. In\naddition, it runs significantly faster than the Carlini-Wagner attack,\ncurrently the most ac-curate zero-confidence attack algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:59:52 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhang", "Yang", ""], ["Chang", "Shiyu", ""], ["Yu", "Mo", ""], ["Qian", "Kaizhi", ""]]}, {"id": "1910.00528", "submitter": "Shruti Mishra", "authors": "Shruti Mishra, Abbas Abdolmaleki, Arthur Guez, Piotr Trochim, Doina\n  Precup", "title": "Augmenting learning using symmetry in a biologically-inspired domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Invariances to translation, rotation and other spatial transformations are a\nhallmark of the laws of motion, and have widespread use in the natural sciences\nto reduce the dimensionality of systems of equations. In supervised learning,\nsuch as in image classification tasks, rotation, translation and scale\ninvariances are used to augment training datasets. In this work, we use data\naugmentation in a similar way, exploiting symmetry in the quadruped domain of\nthe DeepMind control suite (Tassa et al. 2018) to add to the trajectories\nexperienced by the actor in the actor-critic algorithm of Abdolmaleki et al.\n(2018). In a data-limited regime, the agent using a set of experiences\naugmented through symmetry is able to learn faster. Our approach can be used to\ninject knowledge of invariances in the domain and task to augment learning in\nrobots, and more generally, to speed up learning in realistic robotics\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:29:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mishra", "Shruti", ""], ["Abdolmaleki", "Abbas", ""], ["Guez", "Arthur", ""], ["Trochim", "Piotr", ""], ["Precup", "Doina", ""]]}, {"id": "1910.00535", "submitter": "Vaios Laschos Dr", "authors": "Vaios Laschos, Jan Tinapp, Klaus Obermayer", "title": "Training Generative Networks with general Optimal Transport distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm that uses an auxiliary neural network to express\nthe potential of the optimal transport map between two data distributions. In\nthe sequel, we use the aforementioned map to train generative networks. Unlike\nWGANs, where the Euclidean distance is ${\\it implicitly}$ used, this new method\nallows to ${\\it explicitly}$ use ${\\it any}$ transportation cost function that\ncan be chosen to match the problem at hand. For example, it allows to use the\nsquared distance as a transportation cost function, giving rise to the\nWasserstein-2 metric for probability distributions, which results in fast and\nstable gradient descends. It also allows to use image centered distances, like\nthe structure similarity index, with notable differences in the results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:39:50 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 17:37:34 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 10:03:04 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Laschos", "Vaios", ""], ["Tinapp", "Jan", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1910.00547", "submitter": "Bruno Ribeiro", "authors": "S Chandra Mouli, Leonardo Teixeira, Jennifer Neville, Bruno Ribeiro", "title": "Deep Lifetime Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of lifetime clustering is to develop an inductive model that maps\nsubjects into $K$ clusters according to their underlying (unobserved) lifetime\ndistribution. We introduce a neural-network based lifetime clustering model\nthat can find cluster assignments by directly maximizing the divergence between\nthe empirical lifetime distributions of the clusters. Accordingly, we define a\nnovel clustering loss function over the lifetime distributions (of entire\nclusters) based on a tight upper bound of the two-sample Kuiper test p-value.\nThe resultant model is robust to the modeling issues associated with the\nunobservability of termination signals, and does not assume proportional\nhazards. Our results in real and synthetic datasets show significantly better\nlifetime clusters (as evaluated by C-index, Brier Score, Logrank score and\nadjusted Rand index) as compared to competing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:10:16 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 02:57:43 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mouli", "S Chandra", ""], ["Teixeira", "Leonardo", ""], ["Neville", "Jennifer", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1910.00551", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, Peter L.\n  Bartlett", "title": "An Efficient Sampling Algorithm for Non-smooth Composite Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a density of the form $p(x) \\propto\n\\exp(-f(x)- g(x))$, where $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a smooth\nand strongly convex function and $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a\nconvex and Lipschitz function. We propose a new algorithm based on the\nMetropolis-Hastings framework, and prove that it mixes to within TV distance\n$\\varepsilon$ of the target density in at most $O(d \\log (d/\\varepsilon))$\niterations. This guarantee extends previous results on sampling from\ndistributions with smooth log densities ($g = 0$) to the more general composite\nnon-smooth case, with the same mixing time up to a multiple of the condition\nnumber. Our method is based on a novel proximal-based proposal distribution\nthat can be efficiently computed for a large class of non-smooth functions $g$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:25:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mou", "Wenlong", ""], ["Flammarion", "Nicolas", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1910.00577", "submitter": "Uri Alon", "authors": "Uri Alon, Roy Sadaka, Omer Levy, Eran Yahav", "title": "Structural Language Models of Code", "comments": "Appeared in ICML'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of any-code completion - generating a missing piece of\nsource code in a given program without any restriction on the vocabulary or\nstructure. We introduce a new approach to any-code completion that leverages\nthe strict syntax of programming languages to model a code snippet as a tree -\nstructural language modeling (SLM). SLM estimates the probability of the\nprogram's abstract syntax tree (AST) by decomposing it into a product of\nconditional probabilities over its nodes. We present a neural model that\ncomputes these conditional probabilities by considering all AST paths leading\nto a target node. Unlike previous techniques that have severely restricted the\nkinds of expressions that can be generated in this task, our approach can\ngenerate arbitrary code in any programming language. Our model significantly\noutperforms both seq2seq and a variety of structured approaches in generating\nJava and C# code. Our code, data, and trained models are available at\nhttp://github.com/tech-srl/slm-code-generation/ . An online demo is available\nat http://AnyCodeGen.org .\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:54:07 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 09:07:27 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:04:07 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 12:15:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Alon", "Uri", ""], ["Sadaka", "Roy", ""], ["Levy", "Omer", ""], ["Yahav", "Eran", ""]]}, {"id": "1910.00582", "submitter": "Xi Yang", "authors": "Xi Yang, Yan Gong, Nida Waheed, Keith March, Jiang Bian, William R.\n  Hogan, Yonghui Wu", "title": "Identifying Cancer Patients at Risk for Heart Failure Using Machine\n  Learning Methods", "comments": "6 pages, 1 figure, 3 tables, accepted by AMIA 2019", "journal-ref": "AMIA Annu Symp Proc (2019) 933-941", "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiotoxicity related to cancer therapies has become a serious issue,\ndiminishing cancer treatment outcomes and quality of life. Early detection of\ncancer patients at risk for cardiotoxicity before cardiotoxic treatments and\nproviding preventive measures are potential solutions to improve cancer\npatients's quality of life. This study focuses on predicting the development of\nheart failure in cancer patients after cancer diagnoses using historical\nelectronic health record (EHR) data. We examined four machine learning\nalgorithms using 143,199 cancer patients from the University of Florida Health\n(UF Health) Integrated Data Repository (IDR). We identified a total number of\n1,958 qualified cases and matched them to 15,488 controls by gender, age, race,\nand major cancer type. Two feature encoding strategies were compared to encode\nvariables as machine learning features. The gradient boosting (GB) based model\nachieved the best AUC score of 0.9077 (with a sensitivity of 0.8520 and a\nspecificity of 0.8138), outperforming other machine learning methods. We also\nlooked into the subgroup of cancer patients with exposure to chemotherapy drugs\nand observed a lower specificity score (0.7089). The experimental results show\nthat machine learning methods are able to capture clinical factors that are\nknown to be associated with heart failure and that it is feasible to use\nmachine learning methods to identify cancer patients at risk for cancer\ntherapy-related heart failure.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:13:04 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Yang", "Xi", ""], ["Gong", "Yan", ""], ["Waheed", "Nida", ""], ["March", "Keith", ""], ["Bian", "Jiang", ""], ["Hogan", "William R.", ""], ["Wu", "Yonghui", ""]]}, {"id": "1910.00584", "submitter": "Arpan Kusari", "authors": "Arpan Kusari", "title": "CWAE-IRL: Formulating a supervised approach to Inverse Reinforcement\n  Learning problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning (IRL) is used to infer the reward function\nfrom the actions of an expert running a Markov Decision Process (MDP). A novel\napproach using variational inference for learning the reward function is\nproposed in this research. Using this technique, the intractable posterior\ndistribution of the continuous latent variable (the reward function in this\ncase) is analytically approximated to appear to be as close to the prior belief\nwhile trying to reconstruct the future state conditioned on the current state\nand action. The reward function is derived using a well-known deep generative\nmodel known as Conditional Variational Auto-encoder (CVAE) with Wasserstein\nloss function, thus referred to as Conditional Wasserstein Auto-encoder-IRL\n(CWAE-IRL), which can be analyzed as a combination of the backward and forward\ninference. This can then form an efficient alternative to the previous\napproaches to IRL while having no knowledge of the system dynamics of the\nagent. Experimental results on standard benchmarks such as objectworld and\npendulum show that the proposed algorithm can effectively learn the latent\nreward function in complex, high-dimensional environments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:06:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kusari", "Arpan", ""]]}, {"id": "1910.00641", "submitter": "Martin Huber", "authors": "Martin Huber", "title": "An introduction to flexible methods for policy evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter covers different approaches to policy evaluation for assessing\nthe causal effect of a treatment or intervention on an outcome of interest. As\nan introduction to causal inference, the discussion starts with the\nexperimental evaluation of a randomized treatment. It then reviews evaluation\nmethods based on selection on observables (assuming a quasi-random treatment\ngiven observed covariates), instrumental variables (inducing a quasi-random\nshift in the treatment), difference-in-differences and changes-in-changes\n(exploiting changes in outcomes over time), as well as regression\ndiscontinuities and kinks (using changes in the treatment assignment at some\nthreshold of a running variable). The chapter discusses methods particularly\nsuited for data with many observations for a flexible (i.e. semi- or\nnonparametric) modeling of treatment effects, and/or many (i.e. high\ndimensional) observed covariates by applying machine learning to select and\ncontrol for covariates in a data-driven way. This is not only useful for\ntackling confounding by controlling for instance for factors jointly affecting\nthe treatment and the outcome, but also for learning effect heterogeneities\nacross subgroups defined upon observable covariates and optimally targeting\nthose groups for which the treatment is most effective.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 19:59:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Huber", "Martin", ""]]}, {"id": "1910.00643", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Vinayak Tantia, Nicolas Ballas, Michael Rabbat", "title": "SlowMo: Improving Communication-Efficient Distributed SGD with Slow\n  Momentum", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization is essential for training large models on large\ndatasets. Multiple approaches have been proposed to reduce the communication\noverhead in distributed training, such as synchronizing only after performing\nmultiple local SGD steps, and decentralized methods (e.g., using gossip\nalgorithms) to decouple communications among workers. Although these methods\nrun faster than AllReduce-based methods, which use blocking communication\nbefore every update, the resulting models may be less accurate after the same\nnumber of updates. Inspired by the BMUF method of Chen & Huo (2016), we propose\na slow momentum (SlowMo) framework, where workers periodically synchronize and\nperform a momentum update, after multiple iterations of a base optimization\nalgorithm. Experiments on image classification and machine translation tasks\ndemonstrate that SlowMo consistently yields improvements in optimization and\ngeneralization performance relative to the base optimizer, even when the\nadditional overhead is amortized over many updates so that the SlowMo runtime\nis on par with that of the base optimizer. We provide theoretical convergence\nguarantees showing that SlowMo converges to a stationary point of smooth\nnon-convex losses. Since BMUF can be expressed through the SlowMo framework,\nour results also correspond to the first theoretical convergence guarantees for\nBMUF.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:06:48 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:00:02 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Jianyu", ""], ["Tantia", "Vinayak", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "1910.00659", "submitter": "Aaron Griffith", "authors": "Aaron Griffith, Andrew Pomerance, Daniel J. Gauthier", "title": "Forecasting Chaotic Systems with Very Low Connectivity Reservoir\n  Computers", "comments": null, "journal-ref": null, "doi": "10.1063/1.5120710", "report-no": null, "categories": "cs.LG nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the hyperparameter space of reservoir computers used for\nforecasting of the chaotic Lorenz '63 attractor with Bayesian optimization. We\nuse a new measure of reservoir performance, designed to emphasize learning the\nglobal climate of the forecasted system rather than short-term prediction. We\nfind that optimizing over this measure more quickly excludes reservoirs that\nfail to reproduce the climate. The results of optimization are surprising: the\noptimized parameters often specify a reservoir network with very low\nconnectivity. Inspired by this observation, we explore reservoir designs with\neven simpler structure, and find well-performing reservoirs that have zero\nspectral radius and no recurrence. These simple reservoirs provide\ncounterexamples to widely used heuristics in the field, and may be useful for\nhardware implementations of reservoir computers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:39:25 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 19:51:35 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Griffith", "Aaron", ""], ["Pomerance", "Andrew", ""], ["Gauthier", "Daniel J.", ""]]}, {"id": "1910.00662", "submitter": "Hao-Chih Lee", "authors": "Hao-Chih Lee, Sarah T Cherng, Riccardo Miotto, Joel T Dudley", "title": "Enhancing high-content imaging for studying microtubule networks at\n  large-scale", "comments": "accepted and presented in Machine Learning for Healthcare 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given the crucial role of microtubules for cell survival, many researchers\nhave found success using microtubule-targeting agents in the search for\neffective cancer therapeutics. Understanding microtubule responses to targeted\ninterventions requires that the microtubule network within cells can be\nconsistently observed across a large sample of images. However, fluorescence\nnoise sources captured simultaneously with biological signals while using\nwide-field microscopes can obfuscate fine microtubule structures. Such\nrequirements are particularly challenging for high-throughput imaging, where\nresearchers must make decisions related to the trade-off between imaging\nquality and speed. Here, we propose a computational framework to enhance the\nquality of high-throughput imaging data to achieve fast speed and high quality\nsimultaneously. Using CycleGAN, we learn an image model from low-throughput,\nhigh-resolution images to enhance features, such as microtubule networks in\nhigh-throughput low-resolution images. We show that CycleGAN is effective in\nidentifying microtubules with 0.93+ AUC-ROC and that these results are robust\nto different kinds of image noise. We further apply CycleGAN to quantify the\nchanges in microtubule density as a result of the application of drug\ncompounds, and show that the quantified responses correspond well with known\ndrug effects\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:43:39 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Lee", "Hao-Chih", ""], ["Cherng", "Sarah T", ""], ["Miotto", "Riccardo", ""], ["Dudley", "Joel T", ""]]}, {"id": "1910.00668", "submitter": "Andrew Carr", "authors": "Andrew Carr and Jared Nielsen and David Wingate", "title": "Wasserstein Neural Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Processes (NPs) are a class of models that learn a mapping from a\ncontext set of input-output pairs to a distribution over functions. They are\ntraditionally trained using maximum likelihood with a KL divergence\nregularization term. We show that there are desirable classes of problems where\nNPs, with this loss, fail to learn any reasonable distribution. We also show\nthat this drawback is solved by using approximations of Wasserstein distance\nwhich calculates optimal transport distances even for distributions of disjoint\nsupport. We give experimental justification for our method and demonstrate\nperformance. These Wasserstein Neural Processes (WNPs) maintain all of the\nbenefits of traditional NPs while being able to approximate a new class of\nfunction mappings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:13:10 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 00:28:06 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Carr", "Andrew", ""], ["Nielsen", "Jared", ""], ["Wingate", "David", ""]]}, {"id": "1910.00696", "submitter": "Eric Carver", "authors": "Eric Carver, Zhenzhen Dai, Evan Liang, James Snyder, Ning Wen", "title": "Improvement of Multiparametric MR Image Segmentation by Augmenting the\n  Data with Generative Adversarial Networks for Glioma Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year thousands of patients are diagnosed with a glioma, a type of\nmalignant brain tumor. Physicians use MR images as a key tool in the diagnosis\nand treatment of these patients. Neural networks show great potential to aid\nphysicians in the medical image analysis. This study investigates the use of\nvarying amounts of synthetic brain T1-weighted (T1), post-contrast T1-weighted\n(T1Gd), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR) MR\nimages created by a generative adversarial network to overcome the lack of\nannotated medical image data in training separate 2D U-Nets to segment\nenhancing tumor, peritumoral edema, and necrosis (non-enhancing tumor core)\nregions on gliomas. These synthetic MR images were assessed quantitively\n(SSIM=0.79) and qualitatively by a physician who found that the synthetic\nimages seem stronger for delineation of structural boundaries but struggle more\nwhen gradient is significant, (e.g. edema signal in T2 modalities). Multiple 2D\nU-Nets were trained with original BraTS data and differing subsets of a\nquarter, half, three-quarters, and all synthetic MR images. There was not an\nobvious correlation between the improvement of values of the metrics in\nseparate validation dataset for each structure and amount of synthetic data\nadded, there is a strong correlation between the amount of synthetic data added\nand the number of best overall validation metrics. In summary, this study\nshowed ability to generate high quality synthetic Flair, T2, T1, and T1CE MR\nimages using the GAN. Using the synthetic MR images showed encouraging results\nto improve the U-Net segmentation performance which has the potential to\naddress the scarcity of readily available medical images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:14:25 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Carver", "Eric", ""], ["Dai", "Zhenzhen", ""], ["Liang", "Evan", ""], ["Snyder", "James", ""], ["Wen", "Ning", ""]]}, {"id": "1910.00698", "submitter": "Chaochao Yan", "authors": "Chaochao Yan and Sheng Wang and Jinyu Yang and Tingyang Xu and Junzhou\n  Huang", "title": "Re-balancing Variational Autoencoder Loss for Molecule Sequence\n  Generation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecule generation is to design new molecules with specific chemical\nproperties and further to optimize the desired chemical properties. Following\nprevious work, we encode molecules into continuous vectors in the latent space\nand then decode the vectors into molecules under the variational autoencoder\n(VAE) framework. We investigate the posterior collapse problem of current\nRNN-based VAEs for molecule sequence generation. For the first time, we find\nthat underestimated reconstruction loss leads to posterior collapse, and\nprovide both theoretical and experimental evidence. We propose an effective and\nefficient solution to fix the problem and avoid posterior collapse. Without\nbells and whistles, our method achieves SOTA reconstruction accuracy and\ncompetitive validity on the ZINC 250K dataset. When generating 10,000 unique\nvalid SMILES from random prior sampling, it costs JT-VAE1450s while our method\nonly needs 9s. Our implementation is at\nhttps://github.com/chaoyan1037/Re-balanced-VAE.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:17:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 11:25:44 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Yan", "Chaochao", ""], ["Wang", "Sheng", ""], ["Yang", "Jinyu", ""], ["Xu", "Tingyang", ""], ["Huang", "Junzhou", ""]]}, {"id": "1910.00700", "submitter": "Ali Mirzaeian", "authors": "Ali Mirzaeian, Houman Homayoun, Avesta Sasan", "title": "NESTA: Hamming Weight Compression-Based Neural Proc. Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we present NESTA, a specialized Neural engine that\nsignificantly accelerates the computation of convolution layers in a deep\nconvolutional neural network, while reducing the computational energy. NESTA\nreformats Convolutions into $3 \\times 3$ batches and uses a hierarchy of\nHamming Weight Compressors to process each batch. Besides, when processing the\nconvolution across multiple channels, NESTA, rather than computing the precise\nresult of a convolution per channel, quickly computes an approximation of its\npartial sum, and a residual value such that if added to the approximate partial\nsum, generates the accurate output. Then, instead of immediately adding the\nresidual, it uses (consumes) the residual when processing the next batch in the\nhamming weight compressors with available capacity. This mechanism shortens the\ncritical path by avoiding the need to propagate carry signals during each round\nof computation and speeds up the convolution of each channel. In the last stage\nof computation, when the partial sum of the last channel is computed, NESTA\nterminates by adding the residual bits to the approximate output to generate a\ncorrect result.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:32:35 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mirzaeian", "Ali", ""], ["Homayoun", "Houman", ""], ["Sasan", "Avesta", ""]]}, {"id": "1910.00701", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Han Zhang, Sercan O. Arik, Honglak Lee, Tomas Pfister", "title": "Distilling Effective Supervision from Severe Label Noise", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting large-scale data with clean labels for supervised training of\nneural networks is practically challenging. Although noisy labels are usually\ncheap to acquire, existing methods suffer a lot from label noise. This paper\ntargets at the challenge of robust training at high label noise regimes. The\nkey insight to achieve this goal is to wisely leverage a small trusted set to\nestimate exemplar weights and pseudo labels for noisy data in order to reuse\nthem for supervised training. We present a holistic framework to train deep\nneural networks in a way that is highly invulnerable to label noise. Our method\nsets the new state of the art on various types of label noise and achieves\nexcellent performance on large-scale datasets with real-world label noise. For\ninstance, on CIFAR100 with a $40\\%$ uniform noise ratio and only 10 trusted\nlabeled data per class, our method achieves $80.2{\\pm}0.3\\%$ classification\naccuracy, where the error rate is only $1.4\\%$ higher than a neural network\ntrained without label noise. Moreover, increasing the noise ratio to $80\\%$,\nour method still maintains a high accuracy of $75.5{\\pm}0.2\\%$, compared to the\nprevious best accuracy $48.2\\%$.\n  Source code available:\nhttps://github.com/google-research/google-research/tree/master/ieg\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:34:29 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 22:06:28 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 23:50:48 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 16:59:37 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 23:58:13 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Zizhao", ""], ["Zhang", "Han", ""], ["Arik", "Sercan O.", ""], ["Lee", "Honglak", ""], ["Pfister", "Tomas", ""]]}, {"id": "1910.00702", "submitter": "Ling Cai", "authors": "Ling Cai, Bo Yan, Gengchen Mai, Krzysztof Janowicz, Rui Zhu", "title": "TransGCN:Coupling Transformation Assumptions with Graph Convolutional\n  Networks for Link Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3360901.3364441", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is an important and frequently studied task that contributes\nto an understanding of the structure of knowledge graphs (KGs) in statistical\nrelational learning. Inspired by the success of graph convolutional networks\n(GCN) in modeling graph data, we propose a unified GCN framework, named\nTransGCN, to address this task, in which relation and entity embeddings are\nlearned simultaneously. To handle heterogeneous relations in KGs, we introduce\na novel way of representing heterogeneous neighborhood by introducing\ntransformation assumptions on the relationship between the subject, the\nrelation, and the object of a triple. Specifically, a relation is treated as a\ntransformation operator transforming a head entity to a tail entity. Both\ntranslation assumption in TransE and rotation assumption in RotatE are explored\nin our framework. Additionally, instead of only learning entity embeddings in\nthe convolution-based encoder while learning relation embeddings in the decoder\nas done by the state-of-art models, e.g., R-GCN, the TransGCN framework trains\nrelation embeddings and entity embeddings simultaneously during the graph\nconvolution operation, thus having fewer parameters compared with R-GCN.\nExperiments show that our models outperform the-state-of-arts methods on both\nFB15K-237 and WN18RR.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:34:40 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Cai", "Ling", ""], ["Yan", "Bo", ""], ["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Zhu", "Rui", ""]]}, {"id": "1910.00727", "submitter": "Varun Chandrasekaran", "authors": "Lakshya Jain, Varun Chandrasekaran, Uyeong Jang, Wilson Wu, Andrew\n  Lee, Andy Yan, Steven Chen, Somesh Jha, Sanjit A. Seshia", "title": "Analyzing and Improving Neural Networks by Generating Semantic\n  Counterexamples through Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even as deep neural networks (DNNs) have achieved remarkable success on\nvision-related tasks, their performance is brittle to transformations in the\ninput. Of particular interest are semantic transformations that model changes\nthat have a basis in the physical world, such as rotations, translations,\nchanges in lighting or camera pose. In this paper, we show how differentiable\nrendering can be utilized to generate images that are informative, yet\nrealistic, and which can be used to analyze DNN performance and improve its\nrobustness through data augmentation. Given a differentiable renderer and a\nDNN, we show how to use off-the-shelf attacks from adversarial machine learning\nto generate semantic counterexamples -- images where semantic features are\nchanged as to produce misclassifications or misdetections. We validate our\napproach on DNNs for image classification and object detection. For\nclassification, we show that semantic counterexamples, when used to augment the\ndataset, (i) improve generalization performance (ii) enhance robustness to\nsemantic transformations, and (iii) transfer between models. Additionally, in\ncomparison to sampling-based semantic augmentation, our technique generates\nmore informative data in a sample efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:47:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 22:08:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jain", "Lakshya", ""], ["Chandrasekaran", "Varun", ""], ["Jang", "Uyeong", ""], ["Wu", "Wilson", ""], ["Lee", "Andrew", ""], ["Yan", "Andy", ""], ["Chen", "Steven", ""], ["Jha", "Somesh", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1910.00744", "submitter": "David Rolnick", "authors": "David Rolnick and Konrad P. Kording", "title": "Reverse-Engineering Deep ReLU Networks", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely assumed that a neural network cannot be recovered from its\noutputs, as the network depends on its parameters in a highly nonlinear way.\nHere, we prove that in fact it is often possible to identify the architecture,\nweights, and biases of an unknown deep ReLU network by observing only its\noutput. Every ReLU network defines a piecewise linear function, where the\nboundaries between linear regions correspond to inputs for which some neuron in\nthe network switches between inactive and active ReLU states. By dissecting the\nset of region boundaries into components associated with particular neurons, we\nshow both theoretically and empirically that it is possible to recover the\nweights of neurons and their arrangement within the network, up to isomorphism.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:53:15 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 16:40:31 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Rolnick", "David", ""], ["Kording", "Konrad P.", ""]]}, {"id": "1910.00748", "submitter": "Nikita Srivatsan", "authors": "Nikita Srivatsan, Jonathan T. Barron, Dan Klein, Taylor\n  Berg-Kirkpatrick", "title": "A Deep Factorization of Style and Structure in Fonts", "comments": "EMNLP 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep factorization model for typographic analysis that\ndisentangles content from style. Specifically, a variational inference\nprocedure factors each training glyph into the combination of a\ncharacter-specific content embedding and a latent font-specific style variable.\nThe underlying generative model combines these factors through an asymmetric\ntranspose convolutional process to generate the image of the glyph itself. When\ntrained on corpora of fonts, our model learns a manifold over font styles that\ncan be used to analyze or reconstruct new, unseen fonts. On the task of\nreconstructing missing glyphs from an unknown font given only a small number of\nobservations, our model outperforms both a strong nearest neighbors baseline\nand a state-of-the-art discriminative model from prior work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:24:12 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 01:43:18 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Srivatsan", "Nikita", ""], ["Barron", "Jonathan T.", ""], ["Klein", "Dan", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1910.00752", "submitter": "Daniel Severo", "authors": "Daniel Severo, Fl\\'avio Amaro, Estevam R. Hruschka Jr, Andr\\'e Soares\n  de Moura Costa", "title": "Ward2ICU: A Vital Signs Dataset of Inpatients from the General Ward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proxy dataset of vital signs with class labels indicating\npatient transitions from the ward to intensive care units called Ward2ICU.\nPatient privacy is protected using a Wasserstein Generative Adversarial Network\nto implicitly learn an approximation of the data distribution, allowing us to\nsample synthetic data. The quality of data generation is assessed directly on\nthe binary classification task by comparing specificity and sensitivity of an\nLSTM classifier on proxy and original datasets. We initialize a discussion of\nunintentionally disclosing commercial sensitive information and propose a\nsolution for a special case through class label balancing\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:38:33 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Severo", "Daniel", ""], ["Amaro", "Fl\u00e1vio", ""], ["Hruschka", "Estevam R.", "Jr"], ["Costa", "Andr\u00e9 Soares de Moura", ""]]}, {"id": "1910.00753", "submitter": "Jonas K\\\"ohler", "authors": "Jonas K\\\"ohler, Leon Klein, Frank No\\'e", "title": "Equivariant Flows: sampling configurations for multi-body systems with\n  symmetric energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.chem-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flows are exact-likelihood generative neural networks that transform samples\nfrom a simple prior distribution to the samples of the probability distribution\nof interest. Boltzmann Generators (BG) combine flows and statistical mechanics\nto sample equilibrium states of strongly interacting many-body systems such as\nproteins with 1000 atoms. In order to scale and generalize these results, it is\nessential that the natural symmetries of the probability density - in physics\ndefined by the invariances of the energy function - are built into the flow.\nHere we develop theoretical tools for constructing such equivariant flows and\ndemonstrate that a BG that is equivariant with respect to rotations and\nparticle permutations can generalize to sampling nontrivially new\nconfigurations where a nonequivariant BG cannot.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:42:34 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["K\u00f6hler", "Jonas", ""], ["Klein", "Leon", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1910.00760", "submitter": "Renjie Liao", "authors": "Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William\n  L. Hamilton, David Duvenaud, Raquel Urtasun, Richard S. Zemel", "title": "Efficient Graph Generation with Graph Recurrent Attention Networks", "comments": "Neural Information Processing Systems (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of efficient and expressive deep generative models of\ngraphs, called Graph Recurrent Attention Networks (GRANs). Our model generates\ngraphs one block of nodes and associated edges at a time. The block size and\nsampling stride allow us to trade off sample quality for efficiency. Compared\nto previous RNN-based graph generative models, our framework better captures\nthe auto-regressive conditioning between the already-generated and\nto-be-generated parts of the graph using Graph Neural Networks (GNNs) with\nattention. This not only reduces the dependency on node ordering but also\nbypasses the long-term bottleneck caused by the sequential nature of RNNs.\nMoreover, we parameterize the output distribution per block using a mixture of\nBernoulli, which captures the correlations among generated edges within the\nblock. Finally, we propose to handle node orderings in generation by\nmarginalizing over a family of canonical orderings. On standard benchmarks, we\nachieve state-of-the-art time efficiency and sample quality compared to\nprevious models. Additionally, we show our model is capable of generating large\ngraphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN\nis the first deep graph generative model that can scale to this size. Our code\nis released at: https://github.com/lrjconan/GRAN.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:28:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 18:49:57 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 20:25:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liao", "Renjie", ""], ["Li", "Yujia", ""], ["Song", "Yang", ""], ["Wang", "Shenlong", ""], ["Nash", "Charlie", ""], ["Hamilton", "William L.", ""], ["Duvenaud", "David", ""], ["Urtasun", "Raquel", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1910.00762", "submitter": "Angela Jiang", "authors": "Angela H. Jiang, Daniel L.-K. Wong, Giulio Zhou, David G. Andersen,\n  Jeffrey Dean, Gregory R. Ganger, Gauri Joshi, Michael Kaminksy, Michael\n  Kozuch, Zachary C. Lipton, Padmanabhan Pillai", "title": "Accelerating Deep Learning by Focusing on the Biggest Losers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Selective-Backprop, a technique that accelerates the\ntraining of deep neural networks (DNNs) by prioritizing examples with high loss\nat each iteration. Selective-Backprop uses the output of a training example's\nforward pass to decide whether to use that example to compute gradients and\nupdate parameters, or to skip immediately to the next example. By reducing the\nnumber of computationally-expensive backpropagation steps performed,\nSelective-Backprop accelerates training. Evaluation on CIFAR10, CIFAR100, and\nSVHN, across a variety of modern image models, shows that Selective-Backprop\nconverges to target error rates up to 3.5x faster than with standard SGD and\nbetween 1.02--1.8x faster than a state-of-the-art importance sampling approach.\nFurther acceleration of 26% can be achieved by using stale forward pass results\nfor selection, thus also skipping forward passes of low priority examples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:34:29 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Jiang", "Angela H.", ""], ["Wong", "Daniel L. -K.", ""], ["Zhou", "Giulio", ""], ["Andersen", "David G.", ""], ["Dean", "Jeffrey", ""], ["Ganger", "Gregory R.", ""], ["Joshi", "Gauri", ""], ["Kaminksy", "Michael", ""], ["Kozuch", "Michael", ""], ["Lipton", "Zachary C.", ""], ["Pillai", "Padmanabhan", ""]]}, {"id": "1910.00768", "submitter": "Zijian Zhang", "authors": "Zijian Zhang, Fan Yang, Haofan Wang, Xia Hu", "title": "Contextual Local Explanation for Black Box Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model-agnostic explanation technique which explains the\nprediction of any classifier called CLE. CLE gives an faithful and\ninterpretable explanation to the prediction, by approximating the model locally\nusing an interpretable model. We demonstrate the flexibility of CLE by\nexplaining different models for text, tabular and image classification, and the\nfidelity of it by doing simulated user experiments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:58:12 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Zhang", "Zijian", ""], ["Yang", "Fan", ""], ["Wang", "Haofan", ""], ["Hu", "Xia", ""]]}, {"id": "1910.00775", "submitter": "Taesup Kim", "authors": "Taesup Kim, Sungjin Ahn, Yoshua Bengio", "title": "Variational Temporal Abstraction", "comments": "Accepted in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variational approach to learning and inference of temporally\nhierarchical structure and representation for sequential data. We propose the\nVariational Temporal Abstraction (VTA), a hierarchical recurrent state space\nmodel that can infer the latent temporal structure and thus perform the\nstochastic state transition hierarchically. We also propose to apply this model\nto implement the jumpy-imagination ability in imagination-augmented\nagent-learning in order to improve the efficiency of the imagination. In\nexperiments, we demonstrate that our proposed method can model 2D and 3D visual\nsequence datasets with interpretable temporal structure discovery and that its\napplication to jumpy imagination enables more efficient agent-learning in a 3D\nnavigation task.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 04:37:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kim", "Taesup", ""], ["Ahn", "Sungjin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1910.00780", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Guihong Li, Radu Marculescu", "title": "How does topology influence gradient propagation and model performance\n  of deep networks with DenseNet-type skip connections?", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DenseNets introduce concatenation-type skip connections that achieve\nstate-of-the-art accuracy in several computer vision tasks. In this paper, we\nreveal that the topology of the concatenation-type skip connections is closely\nrelated to the gradient propagation which, in turn, enables a predictable\nbehavior of DNNs' test performance. To this end, we introduce a new metric\ncalled NN-Mass to quantify how effectively information flows through DNNs.\nMoreover, we empirically show that NN-Mass also works for other types of skip\nconnections, e.g., for ResNets, Wide-ResNets (WRNs), and MobileNets, which\ncontain addition-type skip connections (i.e., residuals or inverted residuals).\nAs such, for both DenseNet-like CNNs and ResNets/WRNs/MobileNets, our\ntheoretically grounded NN-Mass can identify models with similar accuracy,\ndespite having significantly different size/compute requirements. Detailed\nexperiments on both synthetic and real datasets (e.g., MNIST, CIFAR-10,\nCIFAR-100, ImageNet) provide extensive evidence for our insights. Finally, the\nclosed-form equation of our NN-Mass enables us to design significantly\ncompressed DenseNets (for CIFAR-10) and MobileNets (for ImageNet) directly at\ninitialization without time-consuming training and/or searching.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 05:25:47 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 02:51:57 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 00:04:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Li", "Guihong", ""], ["Marculescu", "Radu", ""]]}, {"id": "1910.00821", "submitter": "Nicolas Gillis", "authors": "Pierre De Handschutter, Nicolas Gillis, Arnaud Vandaele, Xavier\n  Siebert", "title": "Near-Convex Archetypal Analysis", "comments": "10 pages, 3 figures", "journal-ref": "IEEE Signal Processing Letters 27 (1), pp. 81-85, 2020", "doi": "10.1109/LSP.2019.2957604", "report-no": null, "categories": "eess.SP cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a widely used linear dimensionality\nreduction technique for nonnegative data. NMF requires that each data point is\napproximated by a convex combination of basis elements. Archetypal analysis\n(AA), also referred to as convex NMF, is a well-known NMF variant imposing that\nthe basis elements are themselves convex combinations of the data points. AA\nhas the advantage to be more interpretable than NMF because the basis elements\nare directly constructed from the data points. However, it usually suffers from\na high data fitting error because the basis elements are constrained to be\ncontained in the convex cone of the data points. In this letter, we introduce\nnear-convex archetypal analysis (NCAA) which combines the advantages of both AA\nand NMF. As for AA, the basis vectors are required to be linear combinations of\nthe data points and hence are easily interpretable. As for NMF, the additional\nflexibility in choosing the basis elements allows NCAA to have a low data\nfitting error. We show that NCAA compares favorably with a state-of-the-art\nminimum-volume NMF method on synthetic datasets and on a real-world\nhyperspectral image.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 08:16:14 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["De Handschutter", "Pierre", ""], ["Gillis", "Nicolas", ""], ["Vandaele", "Arnaud", ""], ["Siebert", "Xavier", ""]]}, {"id": "1910.00877", "submitter": "David Rohde", "authors": "Otmane Sakhi, Stephen Bonner, David Rohde, Flavian Vasile", "title": "Reconsidering Analytical Variational Bounds for Output Layers of Deep\n  Networks", "comments": "8 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of the re-parameterization trick with the use of variational\nauto-encoders has caused a sensation in Bayesian deep learning, allowing the\ntraining of realistic generative models of images and has considerably\nincreased our ability to use scalable latent variable models. The\nre-parameterization trick is necessary for models in which no analytical\nvariational bound is available and allows noisy gradients to be computed for\narbitrary models. However, for certain standard output layers of a neural\nnetwork, analytical bounds are available and the variational auto-encoder may\nbe used both without the re-parameterization trick or the need for any Monte\nCarlo approximation. In this work, we show that using Jaakola and Jordan bound,\nwe can produce a binary classification layer that allows a Bayesian output\nlayer to be trained, using the standard stochastic gradient descent algorithm.\nWe further demonstrate that a latent variable model utilizing the Bouchard\nbound for multi-class classification allows for fast training of a fully\nprobabilistic latent factor model, even when the number of classes is very\nlarge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:06:47 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 13:21:41 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Sakhi", "Otmane", ""], ["Bonner", "Stephen", ""], ["Rohde", "David", ""], ["Vasile", "Flavian", ""]]}, {"id": "1910.00879", "submitter": "Isaac Matthews", "authors": "Tom Ryder, Dennis Prangle, Andrew Golightly, Isaac Matthews", "title": "The Neural Moving Average Model for Scalable Variational Inference of\n  State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has had great success in scaling approximate Bayesian\ninference to big data by exploiting mini-batch training. To date, however, this\nstrategy has been most applicable to models of independent data. We propose an\nextension to state space models of time series data based on a novel generative\nmodel for latent temporal states: the neural moving average model. This permits\na subsequence to be sampled without drawing from the entire distribution,\nenabling training iterations to use mini-batches of the time series at low\ncomputational cost. We illustrate our method on autoregressive, Lotka-Volterra,\nFitzHugh-Nagumo and stochastic volatility models, achieving accurate parameter\nestimation in a short time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:28:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:34:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ryder", "Tom", ""], ["Prangle", "Dennis", ""], ["Golightly", "Andrew", ""], ["Matthews", "Isaac", ""]]}, {"id": "1910.00888", "submitter": "Thomas Pinetz", "authors": "Thomas Pinetz, Daniel Soukup and Thomas Pock", "title": "On the estimation of the Wasserstein distance in generative models", "comments": "Accepted and presented at GCPR 2019 (http://gcpr2019.tu-dortmund.de/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been used to model the underlying\nprobability distribution of sample based datasets. GANs are notoriuos for\ntraining difficulties and their dependence on arbitrary hyperparameters. One\nrecent improvement in GAN literature is to use the Wasserstein distance as loss\nfunction leading to Wasserstein Generative Adversarial Networks (WGANs). Using\nthis as a basis, we show various ways in which the Wasserstein distance is\nestimated for the task of generative modelling. Additionally, the secrets in\ntraining such models are shown and summarized at the end of this work. Where\napplicable, we extend current works to different algorithms, different cost\nfunctions, and different regularization schemes to improve generative models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:49:00 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Pinetz", "Thomas", ""], ["Soukup", "Daniel", ""], ["Pock", "Thomas", ""]]}, {"id": "1910.00925", "submitter": "Ali Siahkoohi", "authors": "Ali Siahkoohi, Mathias Louboutin, Felix J. Herrmann", "title": "Neural network augmented wave-equation simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.geo-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate forward modeling is important for solving inverse problems. An\ninaccurate wave-equation simulation, as a forward operator, will offset the\nresults obtained via inversion. In this work, we consider the case where we\ndeal with incomplete physics. One proxy of incomplete physics is an inaccurate\ndiscretization of Laplacian in simulation of wave equation via\nfinite-difference method. We exploit intrinsic one-to-one similarities between\ntimestepping algorithm with Convolutional Neural Networks (CNNs), and propose\nto intersperse CNNs between low-fidelity timesteps. Augmenting neural networks\nwith low-fidelity timestepping algorithms may allow us to take large timesteps\nwhile limiting the numerical dispersion artifacts. While simulating the\nwave-equation with low-fidelity timestepping algorithm, by correcting the\nwavefield several time during propagation, we hope to limit the numerical\ndispersion artifact introduced by a poor discretization of the Laplacian. As a\nproof of concept, we demonstrate this principle by correcting for numerical\ndispersion by keeping the velocity model fixed, and varying the source\nlocations to generate training and testing pairs for our supervised learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:00:51 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:08:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Siahkoohi", "Ali", ""], ["Louboutin", "Mathias", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1910.00935", "submitter": "Yuanming Hu", "authors": "Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan\n  Ragan-Kelley, Fr\\'edo Durand", "title": "DiffTaichi: Differentiable Programming for Physical Simulation", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DiffTaichi, a new differentiable programming language tailored for\nbuilding high-performance differentiable physical simulators. Based on an\nimperative programming language, DiffTaichi generates gradients of simulation\nsteps using source code transformations that preserve arithmetic intensity and\nparallelism. A light-weight tape is used to record the whole simulation program\nstructure and replay the gradient kernels in a reversed order, for end-to-end\nbackpropagation. We demonstrate the performance and productivity of our\nlanguage in gradient-based learning and optimization tasks on 10 different\nphysical simulators. For example, a differentiable elastic object simulator\nwritten in our language is 4.2x shorter than the hand-engineered CUDA version\nyet runs as fast, and is 188x faster than the TensorFlow implementation. Using\nour differentiable programs, neural network controllers are typically optimized\nwithin only tens of iterations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 05:00:26 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 13:13:28 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 06:21:07 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hu", "Yuanming", ""], ["Anderson", "Luke", ""], ["Li", "Tzu-Mao", ""], ["Sun", "Qi", ""], ["Carr", "Nathan", ""], ["Ragan-Kelley", "Jonathan", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1910.00942", "submitter": "Guillaume Salha", "authors": "Guillaume Salha, Romain Hennequin, Michalis Vazirgiannis", "title": "Keep It Simple: Graph Autoencoders Without Graph Convolutional Networks", "comments": "NeurIPS 2019 Graph Representation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged\nas powerful node embedding methods, with promising performances on challenging\ntasks such as link prediction and node clustering. Graph AE, VAE and most of\ntheir extensions rely on graph convolutional networks (GCN) to learn vector\nspace representations of nodes. In this paper, we propose to replace the GCN\nencoder by a simple linear model w.r.t. the adjacency matrix of the graph. For\nthe two aforementioned tasks, we empirically show that this approach\nconsistently reaches competitive performances w.r.t. GCN-based models for\nnumerous real-world graphs, including the widely used Cora, Citeseer and Pubmed\ncitation networks that became the de facto benchmark datasets for evaluating\ngraph AE and VAE. This result questions the relevance of repeatedly using these\nthree datasets to compare complex graph AE and VAE models. It also emphasizes\nthe effectiveness of simple node encoding schemes for many real-world\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:30:08 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Salha", "Guillaume", ""], ["Hennequin", "Romain", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1910.00943", "submitter": "Jos\\'e Ant\\'onio Ferreira", "authors": "Jos\\'e A. Ferreira", "title": "Data-generating models under which the random forest algorithm performs\n  badly", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples are given of data-generating models under which some versions of the\nrandom forest algorithm may fail to be consistent or be extremely slow to\nconverge to the optimal predictor. The evidence provided for these properties\nis based on mostly intuitive arguments, similar to those used earlier with\nsimpler examples, and on numerical experiments. Although one can always choose\na model under which random forests perform very badly, it is shown that when\nsubstantial improvement is possible simple methods based on statistics of\n'variable use' and 'variable importance' may indicate a better predictor based\non a sort of mixture of random forests; thus, by acknowledging the difficulties\nposed by some models one may improve the performance of random forests in some\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:33:33 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 08:15:42 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 07:52:44 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 10:23:39 GMT"}, {"version": "v5", "created": "Tue, 14 Jan 2020 12:21:14 GMT"}, {"version": "v6", "created": "Fri, 7 May 2021 20:17:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ferreira", "Jos\u00e9 A.", ""]]}, {"id": "1910.00964", "submitter": "Venet Osmani", "authors": "Seyedmostafa Sheikhalishahi, Vevake Balaraman, Venet Osmani", "title": "Benchmarking machine learning models on multi-centre eICU critical care\n  dataset", "comments": "Source code to replicate the results\n  https://github.com/mostafaalishahi/eICU_Benchmark", "journal-ref": null, "doi": "10.1371/journal.pone.0235424", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress of machine learning in critical care has been difficult to track, in\npart due to absence of public benchmarks. Other fields of research (such as\ncomputer vision and natural language processing) have established various\ncompetitions and public benchmarks. Recent availability of large clinical\ndatasets has enabled the possibility of establishing public benchmarks. Taking\nadvantage of this opportunity, we propose a public benchmark suite to address\nfour areas of critical care, namely mortality prediction, estimation of length\nof stay, patient phenotyping and risk of decompensation. We define each task\nand compare the performance of both clinical models as well as baseline and\ndeep learning models using eICU critical care dataset of around 73,000\npatients. This is the first public benchmark on a multi-centre critical care\ndataset, comparing the performance of clinical gold standard with our\npredictive model. We also investigate the impact of numerical variables as well\nas handling of categorical variables on each of the defined tasks. The source\ncode, detailing our methods and experiments is publicly available such that\nanyone can replicate our results and build upon our work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:04:24 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:44:40 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Sheikhalishahi", "Seyedmostafa", ""], ["Balaraman", "Vevake", ""], ["Osmani", "Venet", ""]]}, {"id": "1910.00965", "submitter": "Ozgur Emre Sivrikaya", "authors": "Mert Yuksekgonul, Ozgur Emre Sivrikaya, Mustafa Gokce Baydogan", "title": "Learning Maximally Predictive Prototypes in Multiple Instance Learning", "comments": "Sets & Partitions Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple model that provides permutation invariant\nmaximally predictive prototype generator from a given dataset, which leads to\ninterpretability of the solution and concrete insights to the nature and the\nsolution of a problem. Our aim is to find out prototypes in the feature space\nto map the collection of instances (i.e. bags) to a distance feature space and\nsimultaneously learn a linear classifier for multiple instance learning (MIL).\nOur experiments on classical MIL benchmark datasets demonstrate that proposed\nframework is an accurate and efficient classifier compared to the existing\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:04:48 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 07:46:13 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 11:49:34 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 11:56:24 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Yuksekgonul", "Mert", ""], ["Sivrikaya", "Ozgur Emre", ""], ["Baydogan", "Mustafa Gokce", ""]]}, {"id": "1910.00969", "submitter": "Andreas Hinterreiter", "authors": "Andreas Hinterreiter, Peter Ruch, Holger Stitz, Martin Ennemoser,\n  J\\\"urgen Bernard, Hendrik Strobelt, Marc Streit", "title": "ConfusionFlow: A model-agnostic visualization for temporal analysis of\n  classifier confusion", "comments": "Changes compared to previous version: Reintroduced NN pruning use\n  case; restructured Evaluation section; several additional minor revisions.\n  Submitted as Minor Revision to IEEE TVCG on 2020-07-02", "journal-ref": null, "doi": "10.1109/TVCG.2020.3012063", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers are among the most widely used supervised machine learning\nalgorithms. Many classification models exist, and choosing the right one for a\ngiven task is difficult. During model selection and debugging, data scientists\nneed to assess classifiers' performances, evaluate their learning behavior over\ntime, and compare different models. Typically, this analysis is based on\nsingle-number performance measures such as accuracy. A more detailed evaluation\nof classifiers is possible by inspecting class errors. The confusion matrix is\nan established way for visualizing these class errors, but it was not designed\nwith temporal or comparative analysis in mind. More generally, established\nperformance analysis systems do not allow a combined temporal and comparative\nanalysis of class-level information. To address this issue, we propose\nConfusionFlow, an interactive, comparative visualization tool that combines the\nbenefits of class confusion matrices with the visualization of performance\ncharacteristics over time. ConfusionFlow is model-agnostic and can be used to\ncompare performances for different model types, model architectures, and/or\ntraining and test datasets. We demonstrate the usefulness of ConfusionFlow in a\ncase study on instance selection strategies in active learning. We further\nassess the scalability of ConfusionFlow and present a use case in the context\nof neural network pruning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:18:09 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 11:35:57 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 17:01:55 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hinterreiter", "Andreas", ""], ["Ruch", "Peter", ""], ["Stitz", "Holger", ""], ["Ennemoser", "Martin", ""], ["Bernard", "J\u00fcrgen", ""], ["Strobelt", "Hendrik", ""], ["Streit", "Marc", ""]]}, {"id": "1910.00982", "submitter": "Micah Goldblum", "authors": "Micah Goldblum, Liam Fowl, Tom Goldstein", "title": "Adversarially Robust Few-Shot Learning: A Meta-Learning Approach", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on adversarially robust neural networks for image\nclassification requires large training sets and computationally expensive\ntraining procedures. On the other hand, few-shot learning methods are highly\nvulnerable to adversarial examples. The goal of our work is to produce networks\nwhich both perform well at few-shot classification tasks and are simultaneously\nrobust to adversarial examples. We develop an algorithm, called Adversarial\nQuerying (AQ), for producing adversarially robust meta-learners, and we\nthoroughly investigate the causes for adversarial vulnerability. Moreover, our\nmethod achieves far superior robust performance on few-shot image\nclassification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:39:21 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 03:33:24 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 15:09:38 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Goldblum", "Micah", ""], ["Fowl", "Liam", ""], ["Goldstein", "Tom", ""]]}, {"id": "1910.01007", "submitter": "John Mellor", "authors": "John F. J. Mellor, Eunbyung Park, Yaroslav Ganin, Igor Babuschkin,\n  Tejas Kulkarni, Dan Rosenbaum, Andy Ballard, Theophane Weber, Oriol Vinyals,\n  S. M. Ali Eslami", "title": "Unsupervised Doodling and Painting with Improved SPIRAL", "comments": "See https://learning-to-paint.github.io for an interactive version of\n  this paper, with videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using reinforcement learning agents as generative models of\nimages (extending arXiv:1804.01118). A generative agent controls a simulated\npainting environment, and is trained with rewards provided by a discriminator\nnetwork simultaneously trained to assess the realism of the agent's samples,\neither unconditional or reconstructions. Compared to prior work, we make a\nnumber of improvements to the architectures of the agents and discriminators\nthat lead to intriguing and at times surprising results. We find that when\nsufficiently constrained, generative agents can learn to produce images with a\ndegree of visual abstraction, despite having only ever seen real photographs\n(no human brush strokes). And given enough time with the painting environment,\nthey can produce images with considerable realism. These results show that,\nunder the right circumstances, some aspects of human drawing can emerge from\nsimulated embodiment, without the need for external supervision, imitation or\nsocial cues. Finally, we note the framework's potential for use in creative\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:12:06 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mellor", "John F. J.", ""], ["Park", "Eunbyung", ""], ["Ganin", "Yaroslav", ""], ["Babuschkin", "Igor", ""], ["Kulkarni", "Tejas", ""], ["Rosenbaum", "Dan", ""], ["Ballard", "Andy", ""], ["Weber", "Theophane", ""], ["Vinyals", "Oriol", ""], ["Eslami", "S. M. Ali", ""]]}, {"id": "1910.01059", "submitter": "Hyeryung Jang", "authors": "Hyeryung Jang, Osvaldo Simeone, Brian Gardner, and Andr\\'e Gr\\\"uning", "title": "An Introduction to Probabilistic Spiking Neural Networks: Probabilistic\n  Models, Learning Rules, and Applications", "comments": "Published in IEEE Signal Processing Magazine, Vol. 36, No. 6, pp.\n  64-77 (subsumes arXiv:1812.03929), Author's Accepted Manuscript", "journal-ref": null, "doi": "10.1109/MSP.2019.2935234", "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are distributed trainable systems whose\ncomputing elements, or neurons, are characterized by internal analog dynamics\nand by digital and sparse synaptic communications. The sparsity of the synaptic\nspiking inputs and the corresponding event-driven nature of neural processing\ncan be leveraged by energy-efficient hardware implementations, which can offer\nsignificant energy reductions as compared to conventional artificial neural\nnetworks (ANNs). The design of training algorithms lags behind the hardware\nimplementations. Most existing training algorithms for SNNs have been designed\neither for biological plausibility or through conversion from pretrained ANNs\nvia rate encoding. This article provides an introduction to SNNs by focusing on\na probabilistic signal processing methodology that enables the direct\nderivation of learning rules by leveraging the unique time-encoding\ncapabilities of SNNs. We adopt discrete-time probabilistic models for networked\nspiking neurons and derive supervised and unsupervised learning rules from\nfirst principles via variational inference. Examples and open research problems\nare also provided.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:28:34 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 18:14:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""], ["Gardner", "Brian", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}, {"id": "1910.01062", "submitter": "Chen Tessler", "authors": "Chen Tessler, Nadav Merlis and Shie Mannor", "title": "Stabilizing Deep Reinforcement Learning with Conservative Updates", "comments": "Under review at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in deep learning have enabled the application of\nreinforcement learning algorithms in complex domains. However, they lack the\ntheoretical guarantees which are present in the tabular setting and suffer from\nmany stability and reproducibility problems \\citep{henderson2018deep}. In this\nwork, we suggest a simple approach for improving stability and providing\nprobabilistic performance improvement in off-policy actor-critic deep\nreinforcement learning regimes. Experiments on continuous action spaces, in the\nMuJoCo control suite, show that our proposed method reduces the variance of the\nprocess and improves the overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:32:25 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 09:56:55 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tessler", "Chen", ""], ["Merlis", "Nadav", ""], ["Mannor", "Shie", ""]]}, {"id": "1910.01064", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem", "title": "Concept Drift Detection and Adaptation with Weak Supervision on\n  Streaming Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift in learning and classification occurs when the statistical\nproperties of either the data features or target change over time; evidence of\ndrift has appeared in search data, medical research, malware, web data, and\nvideo. Drift adaptation has not yet been addressed in high dimensional, noisy,\nlow-context data such as streaming text, video, or images due to the unique\nchallenges these domains present. We present a two-fold approach to deal with\nconcept drift in these domains: a density-based clustering approach to deal\nwith virtual concept drift (change in statistical properties of features) and a\nweak-supervision step to deal with real concept drift (change in statistical\nproperties of target). Our density-based clustering avoids problems posed by\nthe curse of dimensionality to create an evolving 'map' of the live data space,\nthereby addressing virtual drift in features. Our weak-supervision step\nleverages high-confidence labels (oracle or heuristic labels) to generate\nweighted training sets to generalize and update existing deep learners to adapt\nto changing decision boundaries (real drift) and create new deep learners for\nunseen regions of the data space. Our results show that our two-fold approach\nperforms well with >90% precision in 2018, four years after initial deployment\nin 2014, without any human intervention.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:33:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Suprem", "Abhijit", ""]]}, {"id": "1910.01067", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Marco Valtorta, Pooyan Jamshidi", "title": "Order-Independent Structure Learning of Multivariate Regression Chain\n  Graphs", "comments": "This paper is an extended version of the accepted paper for SUM 2019\n  that will appear in the proceedings published by Springer in the Lecture\n  Notes in Artificial Intelligence (LNAI) series", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with multivariate regression chain graphs (MVR CGs), which\nwere introduced by Cox and Wermuth [3,4] to represent linear causal models with\ncorrelated errors. We consider the PC-like algorithm for structure learning of\nMVR CGs, which is a constraint-based method proposed by Sonntag and Pe\\~{n}a in\n[18]. We show that the PC-like algorithm is order-dependent, in the sense that\nthe output can depend on the order in which the variables are given. This\norder-dependence is a minor issue in low-dimensional settings. However, it can\nbe very pronounced in high-dimensional settings, where it can lead to highly\nvariable results. We propose two modifications of the PC-like algorithm that\nremove part or all of this order-dependence. Simulations under a variety of\nsettings demonstrate the competitive performance of our algorithms in\ncomparison with the original PC-like algorithm in low-dimensional settings and\nimproved performance in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:07:55 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "1910.01074", "submitter": "Eleanor Quint", "authors": "Eleanor Quint and Dong Xu and Samuel Flint and Stephen Scott and\n  Matthew Dwyer", "title": "Formal Language Constraints for Markov Decision Processes", "comments": "NeurIPS 2019 Workshop on Safety and Robustness in Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to satisfy safety conditions, an agent may be constrained from\nacting freely. A safe controller can be designed a priori if an environment is\nwell understood, but not when learning is employed. In particular,\nreinforcement learned (RL) controllers require exploration, which can be\nhazardous in safety critical situations. We study the benefits of giving\nstructure to the constraints of a constrained Markov decision process by\nspecifying them in formal languages as a step towards using safety methods from\nsoftware engineering and controller synthesis. We instantiate these constraints\nas finite automata to efficiently recognise constraint violations. Constraint\nstates are then used to augment the underlying MDP state and to learn a dense\ncost function, easing the problem of quickly learning joint MDP/constraint\ndynamics. We empirically evaluate the effect of these methods on training a\nvariety of RL algorithms over several constraints specified in Safety Gym,\nMuJoCo, and Atari environments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:45:23 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 22:38:36 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 18:00:26 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Quint", "Eleanor", ""], ["Xu", "Dong", ""], ["Flint", "Samuel", ""], ["Scott", "Stephen", ""], ["Dwyer", "Matthew", ""]]}, {"id": "1910.01075", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo\n  Larochelle, Bernhard Sch\\\"olkopf, Michael C. Mozer, Chris Pal, Yoshua Bengio", "title": "Learning Neural Causal Models from Unknown Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promising results have driven a recent surge of interest in continuous\noptimization methods for Bayesian network structure learning from observational\ndata. However, there are theoretical limitations on the identifiability of\nunderlying structures obtained from observational data alone. Interventional\ndata provides much richer information about the underlying data-generating\nprocess. However, the extension and application of methods designed for\nobservational data to include interventions is not straightforward and remains\nan open problem. In this paper we provide a general framework based on\ncontinuous optimization and neural networks to create models for the\ncombination of observational and interventional data. The proposed method is\neven applicable in the challenging and realistic case that the identity of the\nintervened upon variable is unknown. We examine the proposed method in the\nsetting of graph recovery both de novo and from a partially-known edge set. We\nestablish strong benchmark results on several structure learning tasks,\nincluding structure recovery of both synthetic graphs as well as standard\ngraphs from the Bayesian Network Repository.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:50:15 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 04:23:05 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Bilaniuk", "Olexa", ""], ["Goyal", "Anirudh", ""], ["Bauer", "Stefan", ""], ["Larochelle", "Hugo", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Mozer", "Michael C.", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1910.01077", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo,\n  David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, Ziyu Wang", "title": "Task-Relevant Adversarial Imitation Learning", "comments": "Accepted to CoRL 2020 (see presentation here:\n  https://youtu.be/ZgQvFGuEgFU )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a critical vulnerability in adversarial imitation is the\ntendency of discriminator networks to learn spurious associations between\nvisual features and expert labels. When the discriminator focuses on\ntask-irrelevant features, it does not provide an informative reward signal,\nleading to poor task performance. We analyze this problem in detail and propose\na solution that outperforms standard Generative Adversarial Imitation Learning\n(GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning\n(TRAIL), uses constrained discriminator optimization to learn informative\nrewards. In comprehensive experiments, we show that TRAIL can solve challenging\nrobotic manipulation tasks from pixels by imitating human operators without\naccess to any task rewards, and clearly outperforms comparable baseline\nimitation agents, including those trained via behaviour cloning and\nconventional GAIL.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:53:37 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:30:23 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zolna", "Konrad", ""], ["Reed", "Scott", ""], ["Novikov", "Alexander", ""], ["Colmenarejo", "Sergio Gomez", ""], ["Budden", "David", ""], ["Cabi", "Serkan", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Wang", "Ziyu", ""]]}, {"id": "1910.01112", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Krishna Kumar Singh, Cho-Jui Hsieh, Yong Jae Lee", "title": "Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in\n  Class-Imbalanced Data", "comments": "Camera ready version for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised generative model that learns to disentangle\nobject identity from other low-level aspects in class-imbalanced data. We first\ninvestigate the issues surrounding the assumptions about uniformity made by\nInfoGAN, and demonstrate its ineffectiveness to properly disentangle object\nidentity in imbalanced data. Our key idea is to make the discovery of the\ndiscrete latent factor of variation invariant to identity-preserving\ntransformations in real images, and use that as a signal to learn the\nappropriate latent distribution representing object identity. Experiments on\nboth artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world\n(YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method\nin disentangling object identity as a latent factor of variation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:50:44 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 10:48:53 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Singh", "Krishna Kumar", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1910.01113", "submitter": "Johannes Leuschner", "authors": "Johannes Leuschner, Maximilian Schmidt, Daniel Otero Baguer, Peter\n  Maa{\\ss}", "title": "The LoDoPaB-CT Dataset: A Benchmark Dataset for Low-Dose CT\n  Reconstruction Methods", "comments": null, "journal-ref": "Scientific Data Volume 8, Article number: 109 (2021)", "doi": "10.1038/s41597-021-00893-z", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning approaches for solving Inverse Problems in imaging have become\nvery effective and are demonstrated to be quite competitive in the field.\nComparing these approaches is a challenging task since they highly rely on the\ndata and the setup that is used for training. We provide a public dataset of\ncomputed tomography images and simulated low-dose measurements suitable for\ntraining this kind of methods. With the LoDoPaB-CT Dataset we aim to create a\nbenchmark that allows for a fair comparison. It contains over 40,000 scan\nslices from around 800 patients selected from the LIDC/IDRI Database. In this\npaper we describe how we processed the original slices and how we simulated the\nmeasurements. We also include first baseline results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 18:59:45 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 15:00:35 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Leuschner", "Johannes", ""], ["Schmidt", "Maximilian", ""], ["Baguer", "Daniel Otero", ""], ["Maa\u00df", "Peter", ""]]}, {"id": "1910.01116", "submitter": "Irene Y. Chen", "authors": "Irene Y. Chen, Monica Agrawal, Steven Horng, David Sontag", "title": "Robustly Extracting Medical Knowledge from EHRs: A Case Study of\n  Learning a Health Knowledge Graph", "comments": "12 pages, presented at PSB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly large electronic health records (EHRs) provide an opportunity to\nalgorithmically learn medical knowledge. In one prominent example, a causal\nhealth knowledge graph could learn relationships between diseases and symptoms\nand then serve as a diagnostic tool to be refined with additional clinical\ninput. Prior research has demonstrated the ability to construct such a graph\nfrom over 270,000 emergency department patient visits. In this work, we\ndescribe methods to evaluate a health knowledge graph for robustness. Moving\nbeyond precision and recall, we analyze for which diseases and for which\npatients the graph is most accurate. We identify sample size and unmeasured\nconfounders as major sources of error in the health knowledge graph. We\nintroduce a method to leverage non-linear functions in building the causal\ngraph to better understand existing model assumptions. Finally, to assess model\ngeneralizability, we extend to a larger set of complete patient visits within a\nhospital system. We conclude with a discussion on how to robustly extract\nmedical knowledge from EHRs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:42:03 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Irene Y.", ""], ["Agrawal", "Monica", ""], ["Horng", "Steven", ""], ["Sontag", "David", ""]]}, {"id": "1910.01150", "submitter": "Deovrat Kakde", "authors": "Kai Shen, Anya Mcguirk, Yuwei Liao, Arin Chaudhuri and Deovrat Kakde", "title": "Fault Detection Using Nonlinear Low-Dimensional Representation of Sensor\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor data analysis plays a key role in health assessment of critical\nequipment. Such data are multivariate and exhibit nonlinear relationships. This\npaper describes how one can exploit nonlinear dimension reduction techniques,\nsuch as the t-distributed stochastic neighbor embedding (t-SNE) and kernel\nprincipal component analysis (KPCA) for fault detection. We show that using\nanomaly detection with low dimensional representations provides better\ninterpretability and is conducive to edge processing in IoT applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:14:04 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Shen", "Kai", ""], ["Mcguirk", "Anya", ""], ["Liao", "Yuwei", ""], ["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""]]}, {"id": "1910.01161", "submitter": "Siddhant Garg", "authors": "Siddhant Garg, Aditya Kumar Akash", "title": "Stochastic Bandits with Delayed Composite Anonymous Feedback", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS)\n  Workshop on Machine Learning with Guarantees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore a novel setting of the Multi-Armed Bandit (MAB) problem inspired\nfrom real world applications which we call bandits with \"stochastic delayed\ncomposite anonymous feedback (SDCAF)\". In SDCAF, the rewards on pulling arms\nare stochastic with respect to time but spread over a fixed number of time\nsteps in the future after pulling the arm. The complexity of this problem stems\nfrom the anonymous feedback to the player and the stochastic generation of the\nreward. Due to the aggregated nature of the rewards, the player is unable to\nassociate the reward to a particular time step from the past. We present two\nalgorithms for this more complicated setting of SDCAF using phase based\nextensions of the UCB algorithm. We perform regret analysis to show sub-linear\ntheoretical guarantees on both the algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:49:16 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 06:05:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Garg", "Siddhant", ""], ["Akash", "Aditya Kumar", ""]]}, {"id": "1910.01177", "submitter": "Augustus Odena", "authors": "Zhengli Zhao, Nicolas Papernot, Sameer Singh, Neoklis Polyzotis,\n  Augustus Odena", "title": "Improving Differentially Private Models with Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broad adoption of machine learning techniques has increased privacy concerns\nfor models trained on sensitive data such as medical records. Existing\ntechniques for training differentially private (DP) models give rigorous\nprivacy guarantees, but applying these techniques to neural networks can\nseverely degrade model performance. This performance reduction is an obstacle\nto deploying private models in the real world. In this work, we improve the\nperformance of DP models by fine-tuning them through active learning on public\ndata. We introduce two new techniques - DIVERSEPUBLIC and NEARPRIVATE - for\ndoing this fine-tuning in a privacy-aware way. For the MNIST and SVHN datasets,\nthese techniques improve state-of-the-art accuracy for DP models while\nretaining privacy guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:24:31 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Zhao", "Zhengli", ""], ["Papernot", "Nicolas", ""], ["Singh", "Sameer", ""], ["Polyzotis", "Neoklis", ""], ["Odena", "Augustus", ""]]}, {"id": "1910.01179", "submitter": "Eric Zhan", "authors": "Eric Zhan, Albert Tseng, Yisong Yue, Adith Swaminathan, Matthew\n  Hausknecht", "title": "Learning Calibratable Policies using Programmatic Style-Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of controllable generation of long-term sequential\nbehaviors, where the goal is to calibrate to multiple behavior styles\nsimultaneously. In contrast to the well-studied areas of controllable\ngeneration of images, text, and speech, there are two questions that pose\nsignificant challenges when generating long-term behaviors: how should we\nspecify the factors of variation to control, and how can we ensure that the\ngenerated behavior faithfully demonstrates combinatorially many styles? We\nleverage programmatic labeling functions to specify controllable styles, and\nderive a formal notion of style-consistency as a learning objective, which can\nthen be solved using conventional policy learning approaches. We evaluate our\nframework using demonstrations from professional basketball players and agents\nin the MuJoCo physics environment, and show that existing approaches that do\nnot explicitly enforce style-consistency fail to generate diverse behaviors\nwhereas our learned policies can be calibrated for up to 1024 distinct style\ncombinations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:34:51 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 00:26:26 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 04:42:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhan", "Eric", ""], ["Tseng", "Albert", ""], ["Yue", "Yisong", ""], ["Swaminathan", "Adith", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "1910.01180", "submitter": "Thomas Magelinski", "authors": "Thomas Magelinski, David Beskow, Kathleen M. Carley", "title": "Graph-Hist: Graph Classification from Latent Feature Histograms With\n  Application to Bot Detection", "comments": null, "journal-ref": "AAAI 2020 (pp. 5134-5141)", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are increasingly used for graph classification in a variety\nof contexts. Social media is a critical application area in this space, however\nthe characteristics of social media graphs differ from those seen in most\npopular benchmark datasets. Social networks tend to be large and sparse, while\nbenchmarks are small and dense. Classically, large and sparse networks are\nanalyzed by studying the distribution of local properties. Inspired by this, we\nintroduce Graph-Hist: an end-to-end architecture that extracts a graph's latent\nlocal features, bins nodes together along 1-D cross sections of the feature\nspace, and classifies the graph based on this multi-channel histogram. We show\nthat Graph-Hist improves state of the art performance on true social media\nbenchmark datasets, while still performing well on other benchmarks. Finally,\nwe demonstrate Graph-Hist's performance by conducting bot detection in social\nmedia. While sophisticated bot and cyborg accounts increasingly evade\ntraditional detection methods, they leave artificial artifacts in their\nconversational graph that are detected through graph classification. We apply\nGraph-Hist to classify these conversational graphs. In the process, we confirm\nthat social media graphs are different than most baselines and that Graph-Hist\noutperforms existing bot-detection models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:35:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Magelinski", "Thomas", ""], ["Beskow", "David", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "1910.01182", "submitter": "Salimeh Yasaei Sekeh", "authors": "Salimeh Yasaei Sekeh, Madan Ravi Ganesh, Shurjo Banerjee, Jason J.\n  Corso, and Alfred O. Hero", "title": "A Geometric Approach to Online Streaming Feature Selection", "comments": "10 page, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Streaming Feature Selection (OSFS) is a sequential learning problem\nwhere individual features across all samples are made available to algorithms\nin a streaming fashion. In this work, firstly, we assert that OSFS's main\nassumption of having data from all the samples available at runtime is\nunrealistic and introduce a new setting where features and samples are streamed\nconcurrently called OSFS with Streaming Samples (OSFS-SS). Secondly, the\nprimary OSFS method, SAOLA utilizes an unbounded mutual information measure and\nrequires multiple comparison steps between the stored and incoming feature sets\nto evaluate a feature's importance. We introduce Geometric Online Adaption, an\nalgorithm that requires relatively less feature comparison steps and uses a\nbounded conditional geometric dependency measure. Our algorithm outperforms\nseveral OSFS baselines including SAOLA on a variety of datasets. We also extend\nSAOLA to work in the OSFS-SS setting and show that GOA continues to achieve the\nbest results. Thirdly, the current paradigm of the OSFS algorithm comparison is\nflawed. Algorithms are measured by comparing the number of features used and\nthe accuracy obtained by the learner, two properties that are fundamentally at\nodds with one another. Without fixing a limit on either of these properties,\nthe qualities of features obtained by different algorithms are incomparable. We\ntry to rectify this inconsistency by fixing the maximum number of features\navailable to the learner and comparing algorithms in terms of their accuracy.\nAdditionally, we characterize the behaviour of SAOLA and GOA on feature sets\nderived from popular deep convolutional featurizers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:36:46 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:49:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sekeh", "Salimeh Yasaei", ""], ["Ganesh", "Madan Ravi", ""], ["Banerjee", "Shurjo", ""], ["Corso", "Jason J.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1910.01196", "submitter": "Chih-Chieh Yang", "authors": "Chih-Chieh Yang and Guojing Cong", "title": "Accelerating Data Loading in Deep Neural Network Training", "comments": "11 pages, 12 figures, accepted for publication in IEEE International\n  Conference on High Performance Computing, Data and Analytics (HiPC) 2019", "journal-ref": null, "doi": "10.1109/HiPC.2019.00037", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data loading can dominate deep neural network training time on large-scale\nsystems. We present a comprehensive study on accelerating data loading\nperformance in large-scale distributed training. We first identify performance\nand scalability issues in current data loading implementations. We then propose\noptimizations that utilize CPU resources to the data loader design. We use an\nanalytical model to characterize the impact of data loading on the overall\ntraining time and establish the performance trend as we scale up distributed\ntraining. Our model suggests that I/O rate limits the scalability of\ndistributed training, which inspires us to design a locality-aware data loading\nmethod. By utilizing software caches, our method can drastically reduce the\ndata loading communication volume in comparison with the original data loading\nimplementation. Finally, we evaluate the proposed optimizations with various\nexperiments. We achieved more than 30x speedup in data loading using 256 nodes\nwith 1,024 learners.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:03:02 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Yang", "Chih-Chieh", ""], ["Cong", "Guojing", ""]]}, {"id": "1910.01213", "submitter": "Kyri Baker", "authors": "Ahmed Zamzam and Kyri Baker", "title": "Learning Optimal Solutions for Extremely Fast AC Optimal Power Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SP eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an online method that leverages machine learning to\nobtain feasible solutions to the AC optimal power flow (OPF) problem with\nnegligible optimality gaps on extremely fast timescales (e.g., milliseconds),\nbypassing solving an AC OPF altogether. This is motivated by the fact that as\nthe power grid experiences increasing amounts of renewable power generation,\ncontrollable loads, and other inverter-interfaced devices, faster system\ndynamics and quicker fluctuations in the power supply are likely to occur.\nCurrently, grid operators typically solve AC OPF every 15 minutes to determine\neconomic generator settings while ensuring grid constraints are satisfied. Due\nto the computational challenges with solving this nonconvex problem, many\nefforts have focused on linearizing or approximating the problem in order to\nsolve the AC OPF on faster timescales. However, many of these approximations\ncan be fairly poor representations of the actual system state and still require\nsolving an optimization problem, which can be time consuming for large\nnetworks. In this work, we leverage historical data to learn a mapping between\nthe system loading and optimal generation values, enabling us to find\nnear-optimal and feasible AC OPF solutions on extremely fast timescales without\nactually solving an optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 17:47:10 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Zamzam", "Ahmed", ""], ["Baker", "Kyri", ""]]}, {"id": "1910.01215", "submitter": "Xingyou Song", "authors": "Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo\n  Pacchiano, Yunhao Tang", "title": "ES-MAML: Simple Hessian-Free Meta Learning", "comments": "Published as a conference paper in ICLR 2020. Code can be found in\n  http://github.com/google-research/google-research/tree/master/es_maml", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ES-MAML, a new framework for solving the model agnostic meta\nlearning (MAML) problem based on Evolution Strategies (ES). Existing algorithms\nfor MAML are based on policy gradients, and incur significant difficulties when\nattempting to estimate second derivatives using backpropagation on stochastic\npolicies. We show how ES can be applied to MAML to obtain an algorithm which\navoids the problem of estimating second derivatives, and is also conceptually\nsimple and easy to implement. Moreover, ES-MAML can handle new types of\nnonsmooth adaptation operators, and other techniques for improving performance\nand estimation of ES methods become applicable. We show empirically that\nES-MAML is competitive with existing methods and often yields better adaptation\nwith fewer queries.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:28:33 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 20:39:22 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 15:30:11 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 15:49:16 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Song", "Xingyou", ""], ["Gao", "Wenbo", ""], ["Yang", "Yuxiang", ""], ["Choromanski", "Krzysztof", ""], ["Pacchiano", "Aldo", ""], ["Tang", "Yunhao", ""]]}, {"id": "1910.01226", "submitter": "Emily Wenger", "authors": "Huiying Li, Emily Wenger, Shawn Shan, Ben Y. Zhao, Haitao Zheng", "title": "Piracy Resistant Watermarks for Deep Neural Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As companies continue to invest heavily in larger, more accurate and more\nrobust deep learning models, they are exploring approaches to monetize their\nmodels while protecting their intellectual property. Model licensing is\npromising, but requires a robust tool for owners to claim ownership of models,\ni.e. a watermark. Unfortunately, current designs have not been able to address\npiracy attacks, where third parties falsely claim model ownership by embedding\ntheir own \"pirate watermarks\" into an already-watermarked model. We observe\nthat resistance to piracy attacks is fundamentally at odds with the current use\nof incremental training to embed watermarks into models. In this work, we\npropose null embedding, a new way to build piracy-resistant watermarks into\nDNNs that can only take place at a model's initial training. A null embedding\ntakes a bit string (watermark value) as input, and builds strong dependencies\nbetween the model's normal classification accuracy and the watermark. As a\nresult, attackers cannot remove an embedded watermark via tuning or incremental\ntraining, and cannot add new pirate watermarks to already watermarked models.\nWe empirically show that our proposed watermarks achieve piracy resistance and\nother watermark properties, over a wide range of tasks and models. Finally, we\nexplore a number of adaptive counter-measures, and show our watermark remains\nrobust against a variety of model modifications, including model fine-tuning,\ncompression, and existing methods to detect/remove backdoors. Our watermarked\nmodels are also amenable to transfer learning without losing their watermark\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:20:06 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:48:01 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 21:22:45 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Li", "Huiying", ""], ["Wenger", "Emily", ""], ["Shan", "Shawn", ""], ["Zhao", "Ben Y.", ""], ["Zheng", "Haitao", ""]]}, {"id": "1910.01240", "submitter": "Shresth Verma", "authors": "Shresth Verma, Haritha S. Nair, Gaurav Agarwal, Joydip Dhar, Anupam\n  Shukla", "title": "Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in\n  Damaged Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotics has proved to be an indispensable tool in many industrial as well as\nsocial applications, such as warehouse automation, manufacturing, disaster\nrobotics, etc. In most of these scenarios, damage to the agent while\naccomplishing mission-critical tasks can result in failure. To enable robotic\nadaptation in such situations, the agent needs to adopt policies which are\nrobust to a diverse set of damages and must do so with minimum computational\ncomplexity. We thus propose a damage aware control architecture which diagnoses\nthe damage prior to gait selection while also incorporating domain\nrandomization in the damage space for learning a robust policy. To implement\ndamage awareness, we have used a Long Short Term Memory based supervised\nlearning network which diagnoses the damage and predicts the type of damage.\nThe main novelty of this approach is that only a single policy is trained to\nadapt against a wide variety of damages and the diagnosis is done in a single\ntrial at the time of damage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 22:16:40 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Verma", "Shresth", ""], ["Nair", "Haritha S.", ""], ["Agarwal", "Gaurav", ""], ["Dhar", "Joydip", ""], ["Shukla", "Anupam", ""]]}, {"id": "1910.01249", "submitter": "James Preiss", "authors": "James A. Preiss, S\\'ebastien M. R. Arnold, Chen-Yu Wei, Marius Kloft", "title": "Analyzing the Variance of Policy Gradient Estimators for the\n  Linear-Quadratic Regulator", "comments": "Accepted at NeurIPS 2019 Workshop on Optimization Foundations for\n  Reinforcement Learning. 7 pages + 6 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variance of the REINFORCE policy gradient estimator in\nenvironments with continuous state and action spaces, linear dynamics,\nquadratic cost, and Gaussian noise. These simple environments allow us to\nderive bounds on the estimator variance in terms of the environment and noise\nparameters. We compare the predictions of our bounds to the empirical variance\nin simulation experiments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:18:59 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Preiss", "James A.", ""], ["Arnold", "S\u00e9bastien M. R.", ""], ["Wei", "Chen-Yu", ""], ["Kloft", "Marius", ""]]}, {"id": "1910.01254", "submitter": "Masih Aminbeidokhti", "authors": "Masih Aminbeidokhti, Marco Pedersoli, Patrick Cardinal, Eric Granger", "title": "Emotion Recognition with Spatial Attention and Temporal Softmax Pooling", "comments": "9 pages; 2 figures; 2 tables; Best paper award at ICIAR 2019", "journal-ref": null, "doi": "10.1007/978-3-030-27202-9_29", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based emotion recognition is a challenging task because it requires to\ndistinguish the small deformations of the human face that represent emotions,\nwhile being invariant to stronger visual differences due to different\nidentities. State-of-the-art methods normally use complex deep learning models\nsuch as recurrent neural networks (RNNs, LSTMs, GRUs), convolutional neural\nnetworks (CNNs, C3D, residual networks) and their combination. In this paper,\nwe propose a simpler approach that combines a CNN pre-trained on a public\ndataset of facial images with (1) a spatial attention mechanism, to localize\nthe most important regions of the face for a given emotion, and (2) temporal\nsoftmax pooling, to select the most important frames of the given video.\nResults on the challenging EmotiW dataset show that this approach can achieve\nhigher accuracy than more complex approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:53:10 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 02:52:19 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Aminbeidokhti", "Masih", ""], ["Pedersoli", "Marco", ""], ["Cardinal", "Patrick", ""], ["Granger", "Eric", ""]]}, {"id": "1910.01255", "submitter": "Yiping Lu", "authors": "Bin Dong, Jikai Hou, Yiping Lu, Zhihua Zhang", "title": "Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge\n  Utilizing Anisotropic Information Retrieval For Overparameterized Neural\n  Network", "comments": "Accepted by NeurIPS 2019 Workshop on Machine Learning with\n  Guarantees. Submitted to other places", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distillation is a method to transfer knowledge from one model to another and\noften achieves higher accuracy with the same capacity. In this paper, we aim to\nprovide a theoretical understanding on what mainly helps with the distillation.\nOur answer is \"early stopping\". Assuming that the teacher network is\noverparameterized, we argue that the teacher network is essentially harvesting\ndark knowledge from the data via early stopping. This can be justified by a new\nconcept, {Anisotropic Information Retrieval (AIR)}, which means that the neural\nnetwork tends to fit the informative information first and the non-informative\ninformation (including noise) later. Motivated by the recent development on\ntheoretically analyzing overparameterized neural networks, we can characterize\nAIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new\nunderstanding of distillation. With that, we further utilize distillation to\nrefine noisy labels. We propose a self-distillation algorithm to sequentially\ndistill knowledge from the network in the previous training epoch to avoid\nmemorizing the wrong labels. We also demonstrate, both theoretically and\nempirically, that self-distillation can benefit from more than just early\nstopping. Theoretically, we prove convergence of the proposed algorithm to the\nground truth labels for randomly initialized overparameterized neural networks\nin terms of $\\ell_2$ distance, while the previous result was on convergence in\n$0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a\nmargin on the training data which leads to better generalization. Empirically,\nwe achieve better testing accuracy and entirely avoid early stopping which\nmakes the algorithm more user-friendly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:53:39 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Dong", "Bin", ""], ["Hou", "Jikai", ""], ["Lu", "Yiping", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1910.01277", "submitter": "Vaneet Aggarwal", "authors": "Qinbo Bai and Mridul Agarwal and Vaneet Aggarwal", "title": "Escaping Saddle Points for Zeroth-order Nonconvex Optimization using\n  Estimated Gradient Descent", "comments": "arXiv admin note: text overlap with arXiv:1703.00887 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent and its variants are widely used in machine learning.\nHowever, oracle access of gradient may not be available in many applications,\nlimiting the direct use of gradient descent. This paper proposes a method of\nestimating gradient to perform gradient descent, that converges to a stationary\npoint for general non-convex optimization problems. Beyond the first-order\nstationary properties, the second-order stationary properties are important in\nmachine learning applications to achieve better performance. We show that the\nproposed model-free non-convex optimization algorithm returns an\n$\\epsilon$-second-order stationary point with\n$\\widetilde{O}(\\frac{d^{2+\\frac{\\theta}{2}}}{\\epsilon^{8+\\theta}})$ queries of\nthe function for any arbitrary $\\theta>0$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 02:02:32 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Bai", "Qinbo", ""], ["Agarwal", "Mridul", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1910.01288", "submitter": "Shuai Yang", "authors": "Shuai Yang and Hao Wang and Kui Yu and Fuyuan Cao and Xindong Wu", "title": "Towards Efficient Local Causal Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local causal structure learning aims to discover and distinguish direct\ncauses (parents) and direct effects (children) of a variable of interest from\ndata. While emerging successes have been made, existing methods need to search\na large space to distinguish direct causes from direct effects of a target\nvariable \\emph{T}. To tackle this issue, we propose a novel Efficient Local\nCausal Structure learning algorithm, named ELCS. Specifically, we first propose\nthe concept of N-structures, then design an efficient Markov Blanket (MB)\ndiscovery subroutine to integrate MB learning with N-structures to learn the MB\nof \\emph{T} and simultaneously distinguish direct causes from direct effects of\n\\emph{T}. With the proposed MB subroutine, ELCS starts from the target\nvariable, sequentially finds MBs of variables connected to the target variable\nand simultaneously constructs local causal structures over MBs until the direct\ncauses and direct effects of the target variable have been distinguished. Using\neight Bayesian networks the extensive experiments have validated that ELCS\nachieves better accuracy and efficiency than the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 03:15:51 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 03:49:48 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 02:23:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yang", "Shuai", ""], ["Wang", "Hao", ""], ["Yu", "Kui", ""], ["Cao", "Fuyuan", ""], ["Wu", "Xindong", ""]]}, {"id": "1910.01312", "submitter": "Chengjing Wang", "authors": "Dunbiao Niu, Chengjing Wang, Peipei Tang, Qingsong Wang, and Enbin\n  Song", "title": "A sparse semismooth Newton based augmented Lagrangian method for\n  large-scale support vector machines", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are successful modeling and prediction tools\nwith a variety of applications. Previous work has demonstrated the superiority\nof the SVMs in dealing with the high dimensional, low sample size problems.\nHowever, the numerical difficulties of the SVMs will become severe with the\nincrease of the sample size. Although there exist many solvers for the SVMs,\nonly few of them are designed by exploiting the special structures of the SVMs.\nIn this paper, we propose a highly efficient sparse semismooth Newton based\naugmented Lagrangian method for solving a large-scale convex quadratic\nprogramming problem with a linear equality constraint and a simple box\nconstraint, which is generated from the dual problems of the SVMs. By\nleveraging the primal-dual error bound result, the fast local convergence rate\nof the augmented Lagrangian method can be guaranteed. Furthermore, by\nexploiting the second-order sparsity of the problem when using the semismooth\nNewton method,the algorithm can efficiently solve the aforementioned difficult\nproblems. Finally, numerical comparisons demonstrate that the proposed\nalgorithm outperforms the current state-of-the-art solvers for the large-scale\nSVMs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:51:47 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:08:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Niu", "Dunbiao", ""], ["Wang", "Chengjing", ""], ["Tang", "Peipei", ""], ["Wang", "Qingsong", ""], ["Song", "Enbin", ""]]}, {"id": "1910.01319", "submitter": "Tiago Ramalho", "authors": "Tiago Ramalho, Thierry Sousbie, Stefano Peluchetti", "title": "An empirical study of pretrained representations for few-shot\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent algorithms with state-of-the-art few-shot classification results start\ntheir procedure by computing data features output by a large pretrained model.\nIn this paper we systematically investigate which models provide the best\nrepresentations for a few-shot image classification task when pretrained on the\nImagenet dataset. We test their representations when used as the starting point\nfor different few-shot classification algorithms. We observe that models\ntrained on a supervised classification task have higher performance than models\ntrained in an unsupervised manner even when transferred to out-of-distribution\ndatasets. Models trained with adversarial robustness transfer better, while\nhaving slightly lower accuracy than supervised models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 06:31:58 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ramalho", "Tiago", ""], ["Sousbie", "Thierry", ""], ["Peluchetti", "Stefano", ""]]}, {"id": "1910.01329", "submitter": "He Zhao", "authors": "He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, Dinh\n  Phung", "title": "Perturbations are not Enough: Generating Adversarial Examples with\n  Spatial Distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural network image classifiers are reported to be susceptible to\nadversarial evasion attacks, which use carefully crafted images created to\nmislead a classifier. Recently, various kinds of adversarial attack methods\nhave been proposed, most of which focus on adding small perturbations to input\nimages. Despite the success of existing approaches, the way to generate\nrealistic adversarial images with small perturbations remains a challenging\nproblem. In this paper, we aim to address this problem by proposing a novel\nadversarial method, which generates adversarial examples by imposing not only\nperturbations but also spatial distortions on input images, including scaling,\nrotation, shear, and translation. As humans are less susceptible to small\nspatial distortions, the proposed approach can produce visually more realistic\nattacks with smaller perturbations, able to deceive classifiers without\naffecting human predictions. We learn our method by amortized techniques with\nneural networks and generate adversarial examples efficiently by a forward pass\nof the networks. Extensive experiments on attacking different types of\nnon-robustified classifiers and robust classifiers with defence show that our\nmethod has state-of-the-art performance in comparison with advanced attack\nparallels.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:15:40 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Zhao", "He", ""], ["Le", "Trung", ""], ["Montague", "Paul", ""], ["De Vel", "Olivier", ""], ["Abraham", "Tamas", ""], ["Phung", "Dinh", ""]]}, {"id": "1910.01347", "submitter": "Samuel Paradis", "authors": "Samuel Paradis and Michael Whitmeyer", "title": "Pay Attention: Leveraging Sequence Models to Predict the Useful Life of\n  Batteries", "comments": "6 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use data on 124 batteries released by Stanford University to first try to\nsolve the binary classification problem of determining if a battery is \"good\"\nor \"bad\" given only the first 5 cycles of data (i.e., will it last longer than\na certain threshold of cycles), as well as the prediction problem of\ndetermining the exact number of cycles a battery will last given the first 100\ncycles of data. We approach the problem from a purely data-driven standpoint,\nhoping to use deep learning to learn the patterns in the sequences of data that\nthe Stanford team engineered by hand. For both problems, we used a similar deep\nnetwork design, that included an optional 1-D convolution, LSTMs, an optional\nAttention layer, followed by fully connected layers to produce our output. For\nthe classification task, we were able to achieve very competitive results, with\nvalidation accuracies above 90%, and a test accuracy of 95%, compared to the\n97.5% test accuracy of the current leading model. For the prediction task, we\nwere also able to achieve competitive results, with a test MAPE error of 12.5%\nas compared with a 9.1% MAPE error achieved by the current leading model\n(Severson et al. 2019).\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:14:02 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 20:32:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Paradis", "Samuel", ""], ["Whitmeyer", "Michael", ""]]}, {"id": "1910.01382", "submitter": "Zhe Hou", "authors": "Hadrien Bride, Zhe Hou, Jie Dong, Jin Song Dong and Ali Mirjalili", "title": "Silas: High Performance, Explainable and Verifiable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new classification tool named Silas, which is built\nto provide a more transparent and dependable data analytics service. A focus of\nSilas is on providing a formal foundation of decision trees in order to support\nlogical analysis and verification of learned prediction models. This paper\ndescribes the distinct features of Silas: The Model Audit module formally\nverifies the prediction model against user specifications, the Enforcement\nLearning module trains prediction models that are guaranteed correct, the Model\nInsight and Prediction Insight modules reason about the prediction model and\nexplain the decision-making of predictions. We also discuss implementation\ndetails ranging from programming paradigm to memory management that help\nachieve high-performance computation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:17:50 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Bride", "Hadrien", ""], ["Hou", "Zhe", ""], ["Dong", "Jie", ""], ["Dong", "Jin Song", ""], ["Mirjalili", "Ali", ""]]}, {"id": "1910.01396", "submitter": "Subhro Ghosh", "authors": "Subhro Ghosh and Sanjay Chaudhuri", "title": "Maximum Likelihood under constraints: Degeneracies and Random Critical\n  Points", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of semi-parametric maximum likelihood under\nconstraints on summary statistics. Such a procedure results in a discrete\nprobability distribution that maximises the likelihood among all such\ndistributions under the specified constraints (called estimating equations),\nand is an approximation to the underlying population distribution. The study of\nsuch empirical likelihood originates from the seminal work of Owen. We\ninvestigate this procedure in the setting of mis-specified (or biased)\nestimating equations, i.e. when the null hypothesis is not true. We establish\nthat the behaviour of the optimal distribution under such mis-specification\ndiffer markedly from their properties under the null, i.e. when the estimating\nequations are unbiased and correctly specified. This is manifested by certain\ndegeneracies in the optimal distribution which define the likelihood. Such\ndegeneracies are not observed under the null. Furthermore, we establish an\nanomalous behaviour of the log-likelihood based Wilks statistic, which, unlike\nunder the null, does not exhibit a chi-squared limit. In the Bayesian setting,\nwe rigorously establish the posterior consistency of procedures based on these\nideas, where instead of a parametric likelihood, an empirical likelihood is\nused to define the posterior distribution. In particular, we show that this\nposterior, as a random probability measure, rapidly converges to the delta\nmeasure at the true parameter value. A novel feature of our approach is the\ninvestigation of critical points of random functions in the context of such\nempirical likelihood. In particular, we obtain the location and the mass of the\ndegenerate optimal weights as the leading and sub-leading terms in a canonical\nexpansion of a particular critical point of a random function that is naturally\nassociated with the model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:50:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:59:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ghosh", "Subhro", ""], ["Chaudhuri", "Sanjay", ""]]}, {"id": "1910.01400", "submitter": "Kieran Woodward Mr", "authors": "Kieran Woodward, Eiman Kanjo and Andreas Oikonomou", "title": "LabelSens: Enabling Real-time Sensor Data Labelling at the point of\n  Collection on Edge Computing", "comments": "Pers Ubiquit Comput (2020)", "journal-ref": null, "doi": "10.1007/s00779-020-01427-x", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning has developed rapidly, enabling the\ndevelopment of applications with high levels of recognition accuracy relating\nto the use of speech and images. However, other types of data to which these\nmodels can be applied have not yet been explored as thoroughly. Labelling is an\nindispensable stage of data pre-processing that can be particularly\nchallenging, especially when applied to single or multi-model real-time sensor\ndata collection approaches. Currently, real-time sensor data labelling is an\nunwieldy process, with a limited range of tools available and poor performance\ncharacteristics, which can lead to the performance of the machine learning\nmodels being compromised. In this paper, we introduce new techniques for\nlabelling at the point of collection coupled with a pilot study and a\nsystematic performance comparison of two popular types of deep neural networks\nrunning on five custom built devices and a comparative mobile app (68.5-89%\naccuracy within-device GRU model, 92.8% highest LSTM model accuracy). These\ndevices are designed to enable real-time labelling with various buttons, slide\npotentiometer and force sensors. This exploratory work illustrates several key\nfeatures that inform the design of data collection tools that can help\nresearchers select and apply appropriate labelling techniques to their work. We\nalso identify common bottlenecks in each architecture and provide field tested\nguidelines to assist in building adaptive, high-performance edge solutions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:54:15 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 08:55:23 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 13:39:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Woodward", "Kieran", ""], ["Kanjo", "Eiman", ""], ["Oikonomou", "Andreas", ""]]}, {"id": "1910.01409", "submitter": "Dexuan Zhang", "authors": "Dexuan Zhang, Tatsuya Harada", "title": "A General Upper Bound for Unsupervised Domain Adaptation", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel upper bound of target error to address the\nproblem for unsupervised domain adaptation. Recent studies reveal that a deep\nneural network can learn transferable features which generalize well to novel\ntasks. Furthermore, a theory proposed by Ben-David et al. (2010) provides a\nupper bound for target error when transferring the knowledge, which can be\nsummarized as minimizing the source error and distance between marginal\ndistributions simultaneously. However, common methods based on the theory\nusually ignore the joint error such that samples from different classes might\nbe mixed together when matching marginal distribution. And in such case, no\nmatter how we minimize the marginal discrepancy, the target error is not\nbounded due to an increasing joint error. To address this problem, we propose a\ngeneral upper bound taking joint error into account, such that the undesirable\ncase can be properly penalized. In addition, we utilize constrained hypothesis\nspace to further formalize a tighter bound as well as a novel cross margin\ndiscrepancy to measure the dissimilarity between hypotheses which alleviates\ninstability during adversarial learning. Extensive empirical evidence shows\nthat our proposal outperforms related approaches in image classification error\nrates on standard domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:31:14 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 07:40:18 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhang", "Dexuan", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1910.01417", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Exploiting multi-CNN features in CNN-RNN based Dimensional Emotion\n  Recognition on the OMG in-the-wild Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel CNN-RNN based approach, which exploits multiple\nCNN features for dimensional emotion recognition in-the-wild, utilizing the\nOne-Minute Gradual-Emotion (OMG-Emotion) dataset. Our approach includes first\npre-training with the relevant and large in size, Aff-Wild and Aff-Wild2\nemotion databases. Low-, mid- and high-level features are extracted from the\ntrained CNN component and are exploited by RNN subnets in a multi-task\nframework. Their outputs constitute an intermediate level prediction; final\nestimates are obtained as the mean or median values of these predictions.\nFusion of the networks is also examined for boosting the obtained performance,\nat Decision-, or at Model-level; in the latter case a RNN was used for the\nfusion. Our approach, although using only the visual modality, outperformed\nstate-of-the-art methods that utilized audio and visual modalities. Some of our\ndevelopments have been submitted to the OMG-Emotion Challenge, ranking second\namong the technologies which used only visual information for valence\nestimation; ranking third overall. Through extensive experimentation, we\nfurther show that arousal estimation is greatly improved when low-level\nfeatures are combined with high-level ones.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:56:41 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 10:28:25 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.01432", "submitter": "Erwan Le Merrer", "authors": "Erwan Le Merrer and Gilles Tredan", "title": "The Bouncer Problem: Challenges to Remote Explainability", "comments": null, "journal-ref": "Nat Mach Intell (2020)", "doi": "10.1038/s42256-020-0216-z", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of explainability is envisioned to satisfy society's demands for\ntransparency on machine learning decisions. The concept is simple: like humans,\nalgorithms should explain the rationale behind their decisions so that their\nfairness can be assessed. While this approach is promising in a local context\n(e.g. to explain a model during debugging at training time), we argue that this\nreasoning cannot simply be transposed in a remote context, where a trained\nmodel by a service provider is only accessible through its API. This is\nproblematic as it constitutes precisely the target use-case requiring\ntransparency from a societal perspective. Through an analogy with a club\nbouncer (which may provide untruthful explanations upon customer reject), we\nshow that providing explanations cannot prevent a remote service from lying\nabout the true reasons leading to its decisions. More precisely, we prove the\nimpossibility of remote explainability for single explanations, by constructing\nan attack on explanations that hides discriminatory features to the querying\nuser. We provide an example implementation of this attack. We then show that\nthe probability that an observer spots the attack, using several explanations\nfor attempting to find incoherences, is low in practical settings. This\nundermines the very concept of remote explainability in general.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 12:54:00 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 10:56:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Merrer", "Erwan Le", ""], ["Tredan", "Gilles", ""]]}, {"id": "1910.01444", "submitter": "Yuta Saito", "authors": "Yuta Saito", "title": "Asymmetric Tri-training for Debiasing Missing-Not-At-Random Explicit\n  Feedback", "comments": "43rd International ACM SIGIR Conference on Research and Development\n  in Information Retrieval (SIGIR '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real-world recommender systems, the observed rating data are subject\nto selection bias, and the data are thus missing-not-at-random. Developing a\nmethod to facilitate the learning of a recommender with biased feedback is one\nof the most challenging problems, as it is widely known that naive approaches\nunder selection bias often lead to suboptimal results. A well-established\nsolution for the problem is using propensity scoring techniques. The propensity\nscore is the probability of each data being observed, and unbiased performance\nestimation is possible by weighting each data by the inverse of its propensity.\nHowever, the performance of the propensity-based unbiased estimation approach\nis often affected by choice of the propensity estimation model or the high\nvariance problem. To overcome these limitations, we propose a model-agnostic\nmeta-learning method inspired by the asymmetric tri-training framework for\nunsupervised domain adaptation. The proposed method utilizes two predictors to\ngenerate data with reliable pseudo-ratings and another predictor to make the\nfinal predictions. In a theoretical analysis, a propensity-independent upper\nbound of the true performance metric is derived, and it is demonstrated that\nthe proposed method can minimize this bound. We conduct comprehensive\nexperiments using public real-world datasets. The results suggest that the\nprevious propensity-based methods are largely affected by the choice of\npropensity models and the variance problem caused by the inverse propensity\nweighting. Moreover, we show that the proposed meta-learning method is robust\nto these issues and can facilitate in developing effective recommendations from\nbiased explicit feedback.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 07:23:46 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 06:45:48 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 02:08:10 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 13:34:38 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 01:27:10 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2020 19:21:41 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Saito", "Yuta", ""]]}, {"id": "1910.01449", "submitter": "Ramiro Camino", "authors": "Ramiro Camino, Christof Ferreira Torres, Mathis Baden, Radu State", "title": "A Data Science Approach for Honeypot Detection in Ethereum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum smart contracts have recently drawn a considerable amount of\nattention from the media, the financial industry and academia. With the\nincrease in popularity, malicious users found new opportunities to profit by\ndeceiving newcomers. Consequently, attackers started luring other attackers\ninto contracts that seem to have exploitable flaws, but that actually contain a\ncomplex hidden trap that in the end benefits the contract creator. In the\nblockchain community, these contracts are known as honeypots. A recent study\npresented a tool called HONEYBADGER that uses symbolic execution to detect\nhoneypots by analyzing contract bytecode. In this paper, we present a data\nscience detection approach based foremost on the contract transaction behavior.\nWe create a partition of all the possible cases of fund movements between the\ncontract creator, the contract, the transaction sender and other participants.\nTo this end, we add transaction aggregated features, such as the number of\ntransactions and the corresponding mean value and other contract features, for\nexample compilation information and source code length. We find that all\naforementioned categories of features contain useful information for the\ndetection of honeypots. Moreover, our approach allows us to detect new,\npreviously undetected honeypots of already known techniques. We furthermore\nemploy our method to test the detection of unknown honeypot techniques by\nsequentially removing one technique from the training set. We show that our\nmethod is capable of discovering the removed honeypot techniques. Finally, we\ndiscovered two new techniques that were previously not known.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:21:35 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 20:16:58 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Camino", "Ramiro", ""], ["Torres", "Christof Ferreira", ""], ["Baden", "Mathis", ""], ["State", "Radu", ""]]}, {"id": "1910.01453", "submitter": "Hao Xu", "authors": "Hao Xu", "title": "D2D-LSTM based Prediction of the D2D Diffusion Path in Mobile Social\n  Networks", "comments": "9 pages, 10 fighures. arXiv admin note: text overlap with\n  arXiv:1705.09275 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, how to expand data transmission to reduce cell data and repeated\ncell transmission has received more and more research attention. In mobile\nsocial networks, content popularity prediction has always been an important\npart of traffic offloading and expanding data dissemination. However, current\nmainstream content popularity prediction methods only use the number of\ndownloads and shares or the distribution of user interests, which do not\nconsider important time and geographic location information in mobile social\nnetworks, and all of data is from OSN which is not same as MSN. In this work,\nwe propose D2D Long Short-Term Memory (D2D-LSTM), a deep neural network based\non LSTM, which is designed to predict a complete D2D diffusion path. Our work\nis the first attempt in the world to use real data of MSN to predict diffusion\npath with deep neural networks which conforms to the D2D structure. Compared to\nlinear sequence networks, only learn users' social features without time\ndistribution or GPS distribution and files' content features, our model can\npredict the propagation path more accurately (up to 85.858\\%) and can reach\nconvergence faster (less than 100 steps) because of the neural network that\nconforms to the D2D structure and combines user social features and files\nfeatures. Moreover, we can simulate generating a D2D propagation tree. After\nexperiment and comparison, it is found to be very similar to the ground-truth\ntrees. Finally, we define a user prototype refinement that can more accurately\ndescribe the propagation sharing habits of a prototype user (including content\npreferences, time preferences, and geographic location preferences), and\nexperimentally validate the predictions when the user prototype is added to\n1000 classes, it is almost identical to the 50 categories.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 03:03:09 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Xu", "Hao", ""]]}, {"id": "1910.01458", "submitter": "Sansiri Tarnpradab", "authors": "Sansiri Tarnpradab, Kien A. Hua", "title": "Attention Based Neural Architecture for Rumor Detection with Author\n  Context Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of social media has made information sharing possible across\nthe globe. The downside, unfortunately, is the wide spread of misinformation.\nMethods applied in most previous rumor classifiers give an equal weight, or\nattention, to words in the microblog, and do not take the context beyond\nmicroblog contents into account; therefore, the accuracy becomes plateaued. In\nthis research, we propose an ensemble neural architecture to detect rumor on\nTwitter. The architecture incorporates word attention and context from the\nauthor to enhance the classification performance. In particular, the word-level\nattention mechanism enables the architecture to put more emphasis on important\nwords when constructing the text representation. To derive further context,\nmicroblog posts composed by individual authors are exploited since they can\nreflect style and characteristics in spreading information, which are\nsignificant cues to help classify whether the shared content is rumor or\nlegitimate news. The experiment on the real-world Twitter dataset collected\nfrom two well-known rumor tracking websites demonstrates promising results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 21:48:03 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Tarnpradab", "Sansiri", ""], ["Hua", "Kien A.", ""]]}, {"id": "1910.01465", "submitter": "Johannes Ackermann", "authors": "Johannes Ackermann, Volker Gabler, Takayuki Osa, Masashi Sugiyama", "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double\n  Centralized Critics", "comments": "Accepted for the Deep RL Workshop at NeurIPS 2019; Changes for v2:\n  Changed Figures 3,4, due to an error in the implementation of MATD3. Please\n  refer to this version for fair evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world tasks require multiple agents to work together. Multi-agent\nreinforcement learning (RL) methods have been proposed in recent years to solve\nthese tasks, but current methods often fail to efficiently learn policies. We\nthus investigate the presence of a common weakness in single-agent RL, namely\nvalue function overestimation bias, in the multi-agent setting. Based on our\nfindings, we propose an approach that reduces this bias by using double\ncentralized critics. We evaluate it on six mixed cooperative-competitive tasks,\nshowing a significant advantage over current methods. Finally, we investigate\nthe application of multi-agent methods to high-dimensional robotic tasks and\nshow that our approach can be used to learn decentralized policies in this\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:40:46 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:00:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ackermann", "Johannes", ""], ["Gabler", "Volker", ""], ["Osa", "Takayuki", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.01473", "submitter": "Behrooz Mamandipoor", "authors": "Behrooz Mamandipoor, Mahshid Majd, Monica Moz, Venet Osmani", "title": "Blood lactate concentration prediction in critical care patients:\n  handling missing values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blood lactate concentration is a strong indicator of mortality risk in\ncritically ill patients. While frequent lactate measurements are necessary to\nassess patient's health state, the measurement is an invasive procedure that\ncan increase risk of hospital-acquired infections. For this reason we formally\ndefine the problem of lactate prediction as a clinically relevant benchmark\nproblem for machine learning community so as to assist clinical decision making\nin blood lactate testing. Accordingly, we demonstrate the relevant challenges\nof the problem and its data in addition to the adopted solutions. Also, we\nevaluate the performance of different prediction algorithms on a large dataset\nof ICU patients from the multi-centre eICU database. More specifically, we\nfocus on investigating the impact of missing value imputation methods in\nlactate prediction for each algorithm. The experimental analysis shows\npromising prediction results that encourages further investigation of this\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:56:50 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Mamandipoor", "Behrooz", ""], ["Majd", "Mahshid", ""], ["Moz", "Monica", ""], ["Osmani", "Venet", ""]]}, {"id": "1910.01487", "submitter": "Jingwei Zhang", "authors": "Shan Lin, Jingwei Zhang", "title": "Generalization Bounds for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved breakthrough performances\nin a wide range of applications including image classification, semantic\nsegmentation, and object detection. Previous research on characterizing the\ngeneralization ability of neural networks mostly focuses on fully connected\nneural networks (FNNs), regarding CNNs as a special case of FNNs without taking\ninto account the special structure of convolutional layers. In this work, we\npropose a tighter generalization bound for CNNs by exploiting the sparse and\npermutation structure of its weight matrices. As the generalization bound\nrelies on the spectral norm of weight matrices, we further study spectral norms\nof three commonly used convolution operations including standard convolution,\ndepthwise convolution, and pointwise convolution. Theoretical and experimental\nresults both demonstrate that our bounds for CNNs are tighter than existing\nbounds.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:08:57 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Lin", "Shan", ""], ["Zhang", "Jingwei", ""]]}, {"id": "1910.01491", "submitter": "Kei Nakagawa", "authors": "Kei Nakagawa, Masaya Abe, Junpei Komiyama", "title": "A Robust Transferable Deep Learning Framework for Cross-sectional\n  Investment Strategy", "comments": null, "journal-ref": null, "doi": "10.1109/DSAA49011.2020.00051", "report-no": null, "categories": "q-fin.ST cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock return predictability is an important research theme as it reflects our\neconomic and social organization, and significant efforts are made to explain\nthe dynamism therein. Statistics of strong explanative power, called \"factor\"\nhave been proposed to summarize the essence of predictive stock returns.\nAlthough machine learning methods are increasingly popular in stock return\nprediction, an inference of the stock returns is highly elusive, and still most\ninvestors, if partly, rely on their intuition to build a better decision\nmaking. The challenge here is to make an investment strategy that is consistent\nover a reasonably long period, with the minimum human decision on the entire\nprocess. To this end, we propose a new stock return prediction framework that\nwe call Ranked Information Coefficient Neural Network (RIC-NN). RIC-NN is a\ndeep learning approach and includes the following three novel ideas: (1)\nnonlinear multi-factor approach, (2) stopping criteria with ranked information\ncoefficient (rank IC), and (3) deep transfer learning among multiple regions.\nExperimental comparison with the stocks in the Morgan Stanley Capital\nInternational (MSCI) indices shows that RIC-NN outperforms not only\noff-the-shelf machine learning methods but also the average return of major\nequity investment funds in the last fourteen years.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:02:57 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Nakagawa", "Kei", ""], ["Abe", "Masaya", ""], ["Komiyama", "Junpei", ""]]}, {"id": "1910.01493", "submitter": "Duc Le", "authors": "Duc Le, Xiaohui Zhang, Weiyi Zheng, Christian F\\\"ugen, Geoffrey Zweig,\n  Michael L. Seltzer", "title": "From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid\n  Speech Recognition", "comments": "To appear at ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an implicit assumption that traditional hybrid approaches for\nautomatic speech recognition (ASR) cannot directly model graphemes and need to\nrely on phonetic lexicons to get competitive performance, especially on English\nwhich has poor grapheme-phoneme correspondence. In this work, we show for the\nfirst time that, on English, hybrid ASR systems can in fact model graphemes\neffectively by leveraging tied context-dependent graphemes, i.e., chenones. Our\nchenone-based systems significantly outperform equivalent senone baselines by\n4.5% to 11.1% relative on three different English datasets. Our results on\nLibrispeech are state-of-the-art compared to other hybrid approaches and\ncompetitive with previously published end-to-end numbers. Further analysis\nshows that chenones can better utilize powerful acoustic models and large\ntraining data, and require context- and position-dependent modeling to work\nwell. Chenone-based systems also outperform senone baselines on proper noun and\nrare word recognition, an area where the latter is traditionally thought to\nhave an advantage. Our work provides an alternative for end-to-end ASR and\nestablishes that hybrid systems can be improved by dropping the reliance on\nphonetic knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 04:17:46 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 21:45:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Le", "Duc", ""], ["Zhang", "Xiaohui", ""], ["Zheng", "Weiyi", ""], ["F\u00fcgen", "Christian", ""], ["Zweig", "Geoffrey", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "1910.01500", "submitter": "Cody Coleman", "authors": "Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius\n  Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor\n  Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim\n  Hazelwood, Andrew Hock, Xinyuan Huang, Atsushi Ike, Bill Jia, Daniel Kang,\n  David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo\n  Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor\n  Robie, Tom St. John, Tsuguchika Tabaru, Carole-Jean Wu, Lingjie Xu, Masafumi\n  Yamazaki, Cliff Young, Matei Zaharia", "title": "MLPerf Training Benchmark", "comments": "MLSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) needs industry-standard performance benchmarks to\nsupport design and competitive evaluation of the many emerging software and\nhardware solutions for ML. But ML training presents three unique benchmarking\nchallenges absent from other domains: optimizations that improve training\nthroughput can increase the time to solution, training is stochastic and time\nto solution exhibits high variance, and software and hardware systems are so\ndiverse that fair benchmarking with the same binary, code, and even\nhyperparameters is difficult. We therefore present MLPerf, an ML benchmark that\novercomes these challenges. Our analysis quantitatively evaluates MLPerf's\nefficacy at driving performance and scalability improvements across two rounds\nof results from multiple vendors.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:55:34 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 19:31:47 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:22:28 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Mattson", "Peter", ""], ["Cheng", "Christine", ""], ["Coleman", "Cody", ""], ["Diamos", "Greg", ""], ["Micikevicius", "Paulius", ""], ["Patterson", "David", ""], ["Tang", "Hanlin", ""], ["Wei", "Gu-Yeon", ""], ["Bailis", "Peter", ""], ["Bittorf", "Victor", ""], ["Brooks", "David", ""], ["Chen", "Dehao", ""], ["Dutta", "Debojyoti", ""], ["Gupta", "Udit", ""], ["Hazelwood", "Kim", ""], ["Hock", "Andrew", ""], ["Huang", "Xinyuan", ""], ["Ike", "Atsushi", ""], ["Jia", "Bill", ""], ["Kang", "Daniel", ""], ["Kanter", "David", ""], ["Kumar", "Naveen", ""], ["Liao", "Jeffery", ""], ["Ma", "Guokai", ""], ["Narayanan", "Deepak", ""], ["Oguntebi", "Tayo", ""], ["Pekhimenko", "Gennady", ""], ["Pentecost", "Lillian", ""], ["Reddi", "Vijay Janapa", ""], ["Robie", "Taylor", ""], ["John", "Tom St.", ""], ["Tabaru", "Tsuguchika", ""], ["Wu", "Carole-Jean", ""], ["Xu", "Lingjie", ""], ["Yamazaki", "Masafumi", ""], ["Young", "Cliff", ""], ["Zaharia", "Matei", ""]]}, {"id": "1910.01510", "submitter": "David Rohde", "authors": "Finnian Lattimore, David Rohde", "title": "Causal inference with Bayes rule", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1906.07125", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of causality has a controversial history. The question of whether\nit is possible to represent and address causal problems with probability\ntheory, or if fundamentally new mathematics such as the do-calculus is required\nhas been hotly debated, In this paper we demonstrate that, while it is critical\nto explicitly model our assumptions on the impact of intervening in a system,\nprovided we do so, estimating causal effects can be done entirely within the\nstandard Bayesian paradigm. The invariance assumptions underlying causal\ngraphical models can be encoded in ordinary Probabilistic graphical models,\nallowing causal estimation with Bayesian statistics, equivalent to the\ndo-calculus.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:32:19 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 22:19:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Lattimore", "Finnian", ""], ["Rohde", "David", ""]]}, {"id": "1910.01526", "submitter": "Jianan Wang", "authors": "Joel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand,\n  Christopher Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang,\n  Peter Toth, Simon Schmitt, Marcus Hutter", "title": "Gated Linear Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.01897", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new family of backpropagation-free neural\narchitectures, Gated Linear Networks (GLNs). What distinguishes GLNs from\ncontemporary neural networks is the distributed and local nature of their\ncredit assignment mechanism; each neuron directly predicts the target, forgoing\nthe ability to learn feature representations in favor of rapid online learning.\nIndividual neurons can model nonlinear functions via the use of data-dependent\ngating in conjunction with online convex optimization. We show that this\narchitecture gives rise to universal learning capabilities in the limit, with\neffective model capacity increasing as a function of network size in a manner\ncomparable with deep ReLU networks. Furthermore, we demonstrate that the GLN\nlearning mechanism possesses extraordinary resilience to catastrophic\nforgetting, performing comparably to a MLP with dropout and Elastic Weight\nConsolidation on standard benchmarks. These desirable theoretical and empirical\nproperties position GLNs as a complementary technique to contemporary offline\ndeep learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:02:26 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:34:55 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Veness", "Joel", ""], ["Lattimore", "Tor", ""], ["Budden", "David", ""], ["Bhoopchand", "Avishkar", ""], ["Mattern", "Christopher", ""], ["Grabska-Barwinska", "Agnieszka", ""], ["Sezener", "Eren", ""], ["Wang", "Jianan", ""], ["Toth", "Peter", ""], ["Schmitt", "Simon", ""], ["Hutter", "Marcus", ""]]}, {"id": "1910.01544", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Peter Stoica", "title": "Robust Risk Minimization for Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general statistical learning problem where an unknown fraction\nof the training data is corrupted. We develop a robust learning method that\nonly requires specifying an upper bound on the corrupted data fraction. The\nmethod minimizes a risk function defined by a non-parametric distribution with\nunknown probability weights. We derive and analyse the optimal weights and show\nhow they provide robustness against corrupted data. Furthermore, we give a\ncomputationally efficient coordinate descent algorithm to solve the risk\nminimization problem. We demonstrate the wide range applicability of the\nmethod, including regression, classification, unsupervised learning and classic\nparameter estimation, with state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:10:09 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 13:50:51 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Stoica", "Peter", ""]]}, {"id": "1910.01545", "submitter": "William Guss", "authors": "William H. Guss, Ruslan Salakhutdinov", "title": "On Universal Approximation by Neural Networks with Uniform Guarantees on\n  Approximation of Infinite Dimensional Maps", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of universal approximation of arbitrary functions $f: \\mathcal{X}\n\\to \\mathcal{Y}$ by neural networks has a rich and thorough history dating back\nto Kolmogorov (1957). In the case of learning finite dimensional maps, many\nauthors have shown various forms of the universality of both fixed depth and\nfixed width neural networks. However, in many cases, these classical results\nfail to extend to the recent use of approximations of neural networks with\ninfinitely many units for functional data analysis, dynamical systems\nidentification, and other applications where either $\\mathcal{X}$ or\n$\\mathcal{Y}$ become infinite dimensional. Two questions naturally arise: which\ninfinite dimensional analogues of neural networks are sufficient to approximate\nany map $f: \\mathcal{X} \\to \\mathcal{Y}$, and when do the finite approximations\nto these analogues used in practice approximate $f$ uniformly over its infinite\ndimensional domain $\\mathcal{X}$?\n  In this paper, we answer the open question of universal approximation of\nnonlinear operators when $\\mathcal{X}$ and $\\mathcal{Y}$ are both infinite\ndimensional. We show that for a large class of different infinite analogues of\nneural networks, any continuous map can be approximated arbitrarily closely\nwith some mild topological conditions on $\\mathcal{X}$. Additionally, we\nprovide the first lower-bound on the minimal number of input and output units\nrequired by a finite approximation to an infinite neural network to guarantee\nthat it can uniformly approximate any nonlinear operator using samples from its\ninputs and outputs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:10:43 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Guss", "William H.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1910.01570", "submitter": "Athanasios Vlontzos", "authors": "Kara Lamb, Garima Malhotra, Athanasios Vlontzos, Edward Wagstaff,\n  At{\\i}l{\\i}m G\\\"unes Baydin, Anahita Bhiwandiwalla, Yarin Gal, Alfredo\n  Kalaitzis, Anthony Reina, Asti Bhatt", "title": "Prediction of GNSS Phase Scintillations: A Machine Learning Approach", "comments": "First 4 authors contributed equally Paper accepted in Machine\n  Learning for the Physical Sciences workshop of NeurIPS 2019 Camera Ready\n  Version to Follow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Global Navigation Satellite System (GNSS) uses a constellation of\nsatellites around the earth for accurate navigation, timing, and positioning.\nNatural phenomena like space weather introduce irregularities in the Earth's\nionosphere, disrupting the propagation of the radio signals that GNSS relies\nupon. Such disruptions affect both the amplitude and the phase of the\npropagated waves. No physics-based model currently exists to predict the time\nand location of these disruptions with sufficient accuracy and at relevant\nscales. In this paper, we focus on predicting the phase fluctuations of GNSS\nradio waves, known as phase scintillations. We propose a novel architecture and\nloss function to predict 1 hour in advance the magnitude of phase\nscintillations within a time window of plus-minus 5 minutes with\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:15:55 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Lamb", "Kara", ""], ["Malhotra", "Garima", ""], ["Vlontzos", "Athanasios", ""], ["Wagstaff", "Edward", ""], ["Baydin", "At\u0131l\u0131m G\u00fcnes", ""], ["Bhiwandiwalla", "Anahita", ""], ["Gal", "Yarin", ""], ["Kalaitzis", "Alfredo", ""], ["Reina", "Anthony", ""], ["Bhatt", "Asti", ""]]}, {"id": "1910.01578", "submitter": "Yanqi Zhou", "authors": "Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter C. Ma,\n  Qiumin Xu, Ming Zhong, Hanxiao Liu, Anna Goldie, Azalia Mirhoseini, James\n  Laudon", "title": "GDP: Generalized Device Placement for Dataflow Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime and scalability of large neural networks can be significantly\naffected by the placement of operations in their dataflow graphs on suitable\ndevices. With increasingly complex neural network architectures and\nheterogeneous device characteristics, finding a reasonable placement is\nextremely challenging even for domain experts. Most existing automated device\nplacement approaches are impractical due to the significant amount of compute\nrequired and their inability to generalize to new, previously held-out graphs.\nTo address both limitations, we propose an efficient end-to-end method based on\na scalable sequential attention mechanism over a graph neural network that is\ntransferable to new graphs. On a diverse set of representative deep learning\nmodels, including Inception-v3, AmoebaNet, Transformer-XL, and WaveNet, our\nmethod on average achieves 16% improvement over human experts and 9.2%\nimprovement over the prior art with 15 times faster convergence. To further\nreduce the computation cost, we pre-train the policy network on a set of\ndataflow graphs and use a superposition network to fine-tune it on each\nindividual graph, achieving state-of-the-art performance on large hold-out\ngraphs with over 50k nodes, such as an 8-layer GNMT.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 04:13:57 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Zhou", "Yanqi", ""], ["Roy", "Sudip", ""], ["Abdolrashidi", "Amirali", ""], ["Wong", "Daniel", ""], ["Ma", "Peter C.", ""], ["Xu", "Qiumin", ""], ["Zhong", "Ming", ""], ["Liu", "Hanxiao", ""], ["Goldie", "Anna", ""], ["Mirhoseini", "Azalia", ""], ["Laudon", "James", ""]]}, {"id": "1910.01589", "submitter": "Ping Li", "authors": "Mostafa Rahmani and Ping Li", "title": "Graph Analysis and Graph Pooling in the Spatial Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial convolution layer which is widely used in the Graph Neural\nNetworks (GNNs) aggregates the feature vector of each node with the feature\nvectors of its neighboring nodes. The GNN is not aware of the locations of the\nnodes in the global structure of the graph and when the local structures\ncorresponding to different nodes are similar to each other, the convolution\nlayer maps all those nodes to similar or same feature vectors in the continuous\nfeature space. Therefore, the GNN cannot distinguish two graphs if their\ndifference is not in their local structures. In addition, when the nodes are\nnot labeled/attributed the convolution layers can fail to distinguish even\ndifferent local structures. In this paper, we propose an effective solution to\naddress this problem of the GNNs. The proposed approach leverages a spatial\nrepresentation of the graph which makes the neural network aware of the\ndifferences between the nodes and also their locations in the graph. The\nspatial representation which is equivalent to a point-cloud representation of\nthe graph is obtained by a graph embedding method. Using the proposed approach,\nthe local feature extractor of the GNN distinguishes similar local structures\nin different locations of the graph and the GNN infers the topological\nstructure of the graph from the spatial distribution of the locally extracted\nfeature vectors. Moreover, the spatial representation is utilized to simplify\nthe graph down-sampling problem. A new graph pooling method is proposed and it\nis shown that the proposed pooling method achieves competitive or better\nresults in comparison with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:44:21 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Li", "Ping", ""]]}, {"id": "1910.01590", "submitter": "Vincent Fortuin", "authors": "Laura Manduchi, Matthias H\\\"user, Julia Vogt, Gunnar R\\\"atsch, Vincent\n  Fortuin", "title": "DPSOM: Deep Probabilistic Clustering with Self-Organizing Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating interpretable visualizations from complex data is a common problem\nin many applications. Two key ingredients for tackling this issue are\nclustering and representation learning. However, current methods do not yet\nsuccessfully combine the strengths of these two approaches. Existing\nrepresentation learning models which rely on latent topological structure such\nas self-organising maps, exhibit markedly lower clustering performance compared\nto recent deep clustering methods. To close this performance gap, we (a)\npresent a novel way to fit self-organizing maps with probabilistic cluster\nassignments (PSOM), (b) propose a new deep architecture for probabilistic\nclustering (DPSOM) using a VAE, and (c) extend our architecture for time-series\nclustering (T-DPSOM), which also allows forecasting in the latent space using\nLSTMs. We show that DPSOM achieves superior clustering performance compared to\ncurrent deep clustering methods on MNIST/Fashion-MNIST, while maintaining the\nfavourable visualization properties of SOMs. On medical time series, we show\nthat T-DPSOM outperforms baseline methods in time series clustering and time\nseries forecasting, while providing interpretable visualizations of patient\nstate trajectories and uncertainty estimation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:47:33 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 14:24:57 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 08:34:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Manduchi", "Laura", ""], ["H\u00fcser", "Matthias", ""], ["Vogt", "Julia", ""], ["R\u00e4tsch", "Gunnar", ""], ["Fortuin", "Vincent", ""]]}, {"id": "1910.01612", "submitter": "Adam Oberman", "authors": "Adam M Oberman", "title": "Partial differential equation regularization for supervised machine\n  learning", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an overview of supervised machine learning problems for\nregression and classification. Topics include: kernel methods, training by\nstochastic gradient descent, deep learning architecture, losses for\nclassification, statistical learning theory, and dimension independent\ngeneralization bounds. Implicit regularization in deep learning examples are\npresented, including data augmentation, adversarial training, and additive\nnoise. These methods are reframed as explicit gradient regularization.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:29:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Oberman", "Adam M", ""]]}, {"id": "1910.01618", "submitter": "Alexandre Ren\\'e", "authors": "Alexandre Ren\\'e, Andr\\'e Longtin and Jakob H. Macke", "title": "Inference of a mesoscopic population model from population spike trains", "comments": "1st revision: 48 pages, 13 figures Improved statistical validation of\n  results. Rewrite of Section 4.2 to clarify the link between the mesoscopic\n  model and a transport equation. Multiple small improvements to the\n  presentation Original: 46 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand how rich dynamics emerge in neural populations, we require\nmodels exhibiting a wide range of activity patterns while remaining\ninterpretable in terms of connectivity and single-neuron dynamics. However, it\nhas been challenging to fit such mechanistic spiking networks at the single\nneuron scale to empirical population data. To close this gap, we propose to fit\nsuch data at a meso scale, using a mechanistic but low-dimensional and hence\nstatistically tractable model. The mesoscopic representation is obtained by\napproximating a population of neurons as multiple homogeneous `pools' of\nneurons, and modelling the dynamics of the aggregate population activity within\neach pool. We derive the likelihood of both single-neuron and connectivity\nparameters given this activity, which can then be used to either optimize\nparameters by gradient ascent on the log-likelihood, or to perform Bayesian\ninference using Markov Chain Monte Carlo (MCMC) sampling. We illustrate this\napproach using a model of generalized integrate-and-fire neurons for which\nmesoscopic dynamics have been previously derived, and show that both\nsingle-neuron and connectivity parameters can be recovered from simulated data.\nIn particular, our inference method extracts posterior correlations between\nmodel parameters, which define parameter subsets able to reproduce the data. We\ncompute the Bayesian posterior for combinations of parameters using MCMC\nsampling and investigate how the approximations inherent to a mesoscopic\npopulation model impact the accuracy of the inferred single-neuron parameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:37:42 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 22:40:14 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ren\u00e9", "Alexandre", ""], ["Longtin", "Andr\u00e9", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1910.01619", "submitter": "Yu Bai", "authors": "Yu Bai, Jason D. Lee", "title": "Beyond Linearization: On Quadratic and Higher-Order Approximation of\n  Wide Neural Networks", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work has established connections between over-parametrized\nneural networks and linearized models governed by he Neural Tangent Kernels\n(NTKs). NTK theory leads to concrete convergence and generalization results,\nyet the empirical performance of neural networks are observed to exceed their\nlinearized models, suggesting insufficiency of this theory.\n  Towards closing this gap, we investigate the training of over-parametrized\nneural networks that are beyond the NTK regime yet still governed by the Taylor\nexpansion of the network. We bring forward the idea of \\emph{randomizing} the\nneural networks, which allows them to escape their NTK and couple with\nquadratic models. We show that the optimization landscape of randomized\ntwo-layer networks are nice and amenable to escaping-saddle algorithms. We\nprove concrete generalization and expressivity results on these randomized\nnetworks, which lead to sample complexity bounds (of learning certain simple\nfunctions) that match the NTK and can in addition be better by a dimension\nfactor when mild distributional assumptions are present. We demonstrate that\nour randomization technique can be generalized systematically beyond the\nquadratic case, by using it to find networks that are coupled with higher-order\nterms in their Taylor series.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:38:10 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:55:51 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bai", "Yu", ""], ["Lee", "Jason D.", ""]]}, {"id": "1910.01623", "submitter": "Martin Slawski", "authors": "Martin Slawski, Guoqing Diao, Emanuel Ben-David", "title": "A Pseudo-Likelihood Approach to Linear Regression with Partially\n  Shuffled Data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant interest in linear regression in the\nsituation where predictors and responses are not observed in matching pairs\ncorresponding to the same statistical unit as a consequence of separate data\ncollection and uncertainty in data integration. Mismatched pairs can\nconsiderably impact the model fit and disrupt the estimation of regression\nparameters. In this paper, we present a method to adjust for such mismatches\nunder ``partial shuffling\" in which a sufficiently large fraction of\n(predictors, response)-pairs are observed in their correct correspondence. The\nproposed approach is based on a pseudo-likelihood in which each term takes the\nform of a two-component mixture density. Expectation-Maximization schemes are\nproposed for optimization, which (i) scale favorably in the number of samples,\nand (ii) achieve excellent statistical performance relative to an oracle that\nhas access to the correct pairings as certified by simulations and case\nstudies. In particular, the proposed approach can tolerate considerably larger\nfraction of mismatches than existing approaches, and enables estimation of the\nnoise level as well as the fraction of mismatches. Inference for the resulting\nestimator (standard errors, confidence intervals) can be based on established\ntheory for composite likelihood estimation. Along the way, we also propose a\nstatistical test for the presence of mismatches and establish its consistency\nunder suitable conditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:43:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Slawski", "Martin", ""], ["Diao", "Guoqing", ""], ["Ben-David", "Emanuel", ""]]}, {"id": "1910.01634", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan,\n  Kyle M. Champley", "title": "Improving Limited Angle CT Reconstruction with a Robust GAN Prior", "comments": "NeurIPS 2019 Workshop on Deep Inverse Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited angle CT reconstruction is an under-determined linear inverse problem\nthat requires appropriate regularization techniques to be solved. In this work\nwe study how pre-trained generative adversarial networks (GANs) can be used to\nclean noisy, highly artifact laden reconstructions from conventional\ntechniques, by effectively projecting onto the inferred image manifold. In\nparticular, we use a robust version of the popularly used GAN prior for inverse\nproblems, based on a recent technique called corruption mimicking, that\nsignificantly improves the reconstruction quality. The proposed approach\noperates in the image space directly, as a result of which it does not need to\nbe trained or require access to the measurement model, is scanner agnostic, and\ncan work over a wide range of sensing scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:52:14 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 23:19:21 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 23:22:24 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 17:40:27 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Anirudh", "Rushil", ""], ["Kim", "Hyojin", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Mohan", "K. Aditya", ""], ["Champley", "Kyle M.", ""]]}, {"id": "1910.01635", "submitter": "Greg Ongie", "authors": "Greg Ongie, Rebecca Willett, Daniel Soudry, Nathan Srebro", "title": "A Function Space View of Bounded Norm Infinite Width ReLU Nets: The\n  Multivariate Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element of understanding the efficacy of overparameterized neural\nnetworks is characterizing how they represent functions as the number of\nweights in the network approaches infinity. In this paper, we characterize the\nnorm required to realize a function $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ as a\nsingle hidden-layer ReLU network with an unbounded number of units (infinite\nwidth), but where the Euclidean norm of the weights is bounded, including\nprecisely characterizing which functions can be realized with finite norm. This\nwas settled for univariate univariate functions in Savarese et al. (2019),\nwhere it was shown that the required norm is determined by the L1-norm of the\nsecond derivative of the function. We extend the characterization to\nmultivariate functions (i.e., networks with d input units), relating the\nrequired norm to the L1-norm of the Radon transform of a (d+1)/2-power\nLaplacian of the function. This characterization allows us to show that all\nfunctions in Sobolev spaces $W^{s,1}(\\mathbb{R})$, $s\\geq d+1$, can be\nrepresented with bounded norm, to calculate the required norm for several\nspecific functions, and to obtain a depth separation result. These results have\nimportant implications for understanding generalization performance and the\ndistinction between neural networks and more traditional kernel learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:56:10 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ongie", "Greg", ""], ["Willett", "Rebecca", ""], ["Soudry", "Daniel", ""], ["Srebro", "Nathan", ""]]}, {"id": "1910.01663", "submitter": "Dingli Yu", "authors": "Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong\n  Wang, Dingli Yu", "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "comments": "Code for UCI experiments:\n  https://github.com/LeoYu/neural-tangent-kernel-UCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that the following two models are equivalent: (a)\ninfinitely wide neural networks (NNs) trained under l2 loss by gradient descent\nwith infinitesimally small learning rate (b) kernel regression with respect to\nso-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient\nalgorithm to compute the NTK, as well as its convolutional counterparts,\nappears in Arora et al. (2019a), which allowed studying performance of\ninfinitely wide nets on datasets like CIFAR-10. However, super-quadratic\nrunning time of kernel methods makes them best suited for small-data tasks. We\nreport results suggesting neural tangent kernels perform strongly on low-data\ntasks.\n  1. On a standard testbed of classification/regression tasks from the UCI\ndatabase, NTK SVM beats the previous gold standard, Random Forests (RF), and\nalso the corresponding finite nets.\n  2. On CIFAR-10 with 10 - 640 training samples, Convolutional NTK consistently\nbeats ResNet-34 by 1% - 3%.\n  3. On VOC07 testbed for few-shot image classification tasks on ImageNet with\ntransfer learning (Goyal et al., 2019), replacing the linear SVM currently used\nwith a Convolutional NTK SVM consistently improves performance.\n  4. Comparing the performance of NTK with the finite-width net it was derived\nfrom, NTK behavior starts at lower net widths than suggested by theoretical\nanalysis(Arora et al., 2019a). NTK's efficacy may trace to lower variance of\noutput.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:04:17 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:51:25 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 17:53:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Arora", "Sanjeev", ""], ["Du", "Simon S.", ""], ["Li", "Zhiyuan", ""], ["Salakhutdinov", "Ruslan", ""], ["Wang", "Ruosong", ""], ["Yu", "Dingli", ""]]}, {"id": "1910.01666", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Shusen Liu, Peer-Timo\n  Bremer, Brian K. Spears", "title": "Exploring Generative Physics Models with Scientific Priors in Inertial\n  Confinement Fusion", "comments": "Machine Learning for Physical Sciences Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant interest in using modern neural networks for scientific\napplications due to their effectiveness in modeling highly complex, non-linear\nproblems in a data-driven fashion. However, a common challenge is to verify the\nscientific plausibility or validity of outputs predicted by a neural network.\nThis work advocates the use of known scientific constraints as a lens into\nevaluating, exploring, and understanding such predictions for the problem of\ninertial confinement fusion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:08:31 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Liu", "Shusen", ""], ["Bremer", "Peer-Timo", ""], ["Spears", "Brian K.", ""]]}, {"id": "1910.01671", "submitter": "Matthew Trager", "authors": "Matthew Trager, Kathl\\'en Kohn, Joan Bruna", "title": "Pure and Spurious Critical Points: a Geometric Study of Linear Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The critical locus of the loss function of a neural network is determined by\nthe geometry of the functional space and by the parameterization of this space\nby the network's weights. We introduce a natural distinction between pure\ncritical points, which only depend on the functional space, and spurious\ncritical points, which arise from the parameterization. We apply this\nperspective to revisit and extend the literature on the loss function of linear\nneural networks. For this type of network, the functional space is either the\nset of all linear maps from input to output space, or a determinantal variety,\ni.e., a set of linear maps with bounded rank. We use geometric properties of\ndeterminantal varieties to derive new results on the landscape of linear\nnetworks with different loss functions and different parameterizations. Our\nanalysis clearly illustrates that the absence of \"bad\" local minima in the loss\nlandscape of linear networks is due to two distinct phenomena that apply in\ndifferent settings: it is true for arbitrary smooth convex losses in the case\nof architectures that can express all linear maps (\"filling architectures\") but\nit holds only for the quadratic loss when the functional space is a\ndeterminantal variety (\"non-filling architectures\"). Without any assumption on\nthe architecture, smooth convex losses may lead to landscapes with many bad\nminima.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:22:30 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 02:46:46 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Trager", "Matthew", ""], ["Kohn", "Kathl\u00e9n", ""], ["Bruna", "Joan", ""]]}, {"id": "1910.01688", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Daniel Apley, Wei Chen", "title": "Bayesian Optimization for Materials Design with Mixed Quantitative and\n  Qualitative Variables", "comments": "29 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.mtrl-sci cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although Bayesian Optimization (BO) has been employed for accelerating\nmaterials design in computational materials engineering, existing works are\nrestricted to problems with quantitative variables. However, real designs of\nmaterials systems involve both qualitative and quantitative design variables\nrepresenting material compositions, microstructure morphology, and processing\nconditions. For mixed-variable problems, existing Bayesian Optimization (BO)\napproaches represent qualitative factors by dummy variables first and then fit\na standard Gaussian process (GP) model with numerical variables as the\nsurrogate model. This approach is restrictive theoretically and fails to\ncapture complex correlations between qualitative levels. We present in this\npaper the integration of a novel latent-variable (LV) approach for\nmixed-variable GP modeling with the BO framework for materials design. LVGP is\na fundamentally different approach that maps qualitative design variables to\nunderlying numerical LV in GP, which has strong physical justification. It\nprovides flexible parameterization and representation of qualitative factors\nand shows superior modeling accuracy compared to the existing methods. We\ndemonstrate our approach through testing with numerical examples and materials\ndesign examples. It is found that in all test examples the mapped LVs provide\nintuitive visualization and substantial insight into the nature and effects of\nthe qualitative factors. Though materials designs are used as examples, the\nmethod presented is generic and can be utilized for other mixed variable design\noptimization problems that involve expensive physics-based simulations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:05:20 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhang", "Yichi", ""], ["Apley", "Daniel", ""], ["Chen", "Wei", ""]]}, {"id": "1910.01694", "submitter": "Jingrong Lin", "authors": "Jingrong Lin, Keegan Lensink, Eldad Haber", "title": "Fluid Flow Mass Transport for Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have been shown to be powerful in generating\ncontent. To this end, they have been studied intensively in the last few years.\nNonetheless, training these networks requires solving a saddle point problem\nthat is difficult to solve and slowly converging. Motivated from techniques in\nthe registration of point clouds and by the fluid flow formulation of mass\ntransport, we investigate a new formulation that is based on strict\nminimization, without the need for the maximization. The formulation views the\nproblem as a matching problem rather than an adversarial one and thus allows us\nto quickly converge and obtain meaningful metrics in the optimization path.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:14:52 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:04:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Lin", "Jingrong", ""], ["Lensink", "Keegan", ""], ["Haber", "Eldad", ""]]}, {"id": "1910.01705", "submitter": "Khurram Javed Mr", "authors": "Khurram Javed, Hengshuai Yao, Martha White", "title": "Is Fast Adaptation All You Need?", "comments": "Meta Learning Workshop, NeurIPS 2019, 2 figures, MRCL, MAML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based meta-learning has proven to be highly effective at learning\nmodel initializations, representations, and update rules that allow fast\nadaptation from a few samples. The core idea behind these approaches is to use\nfast adaptation and generalization -- two second-order metrics -- as training\nsignals on a meta-training dataset. However, little attention has been given to\nother possible second-order metrics. In this paper, we investigate a different\ntraining signal -- robustness to catastrophic interference -- and demonstrate\nthat representations learned by directing minimizing interference are more\nconducive to incremental learning than those learned by just maximizing fast\nadaptation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:52:25 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Javed", "Khurram", ""], ["Yao", "Hengshuai", ""], ["White", "Martha", ""]]}, {"id": "1910.01708", "submitter": "Scott Fujimoto", "authors": "Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, Joelle Pineau", "title": "Benchmarking Batch Deep Reinforcement Learning Algorithms", "comments": "Deep RL Workshop NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely-used deep reinforcement learning algorithms have been shown to fail in\nthe batch setting--learning from a fixed data set without interaction with the\nenvironment. Following this result, there have been several papers showing\nreasonable performances under a variety of environments and batch settings. In\nthis paper, we benchmark the performance of recent off-policy and batch\nreinforcement learning algorithms under unified settings on the Atari domain,\nwith data generated by a single partially-trained behavioral policy. We find\nthat under these conditions, many of these algorithms underperform DQN trained\nonline with the same amount of data, as well as the partially-trained\nbehavioral policy. To introduce a strong baseline, we adapt the\nBatch-Constrained Q-learning algorithm to a discrete-action setting, and show\nit outperforms all existing algorithms at this task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:15:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Fujimoto", "Scott", ""], ["Conti", "Edoardo", ""], ["Ghavamzadeh", "Mohammad", ""], ["Pineau", "Joelle", ""]]}, {"id": "1910.01713", "submitter": "Vadim Arzamasov", "authors": "Vadim Arzamasov and Klemens B\\\"ohm", "title": "Scenario Discovery via Rule Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario discovery is the process of finding areas of interest, commonly\nreferred to as scenarios, in data spaces resulting from simulations. For\ninstance, one might search for conditions - which are inputs of the simulation\nmodel - where the system under investigation is unstable. A commonly used\nalgorithm for scenario discovery is PRIM. It yields scenarios in the form of\nhyper-rectangles which are human-comprehensible. When the simulation model has\nmany inputs, and the simulations are computationally expensive, PRIM may not\nproduce good results, given the affordable volume of data. So we propose a new\nprocedure for scenario discovery - we train an intermediate statistical model\nwhich generalizes fast, and use it to label (a lot of) data for PRIM. We\nprovide the statistical intuition behind our idea. Our experimental study shows\nthat this method is much better than PRIM itself. Specifically, our method\nreduces the number of simulations runs necessary by 75% on average.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:40:18 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Arzamasov", "Vadim", ""], ["B\u00f6hm", "Klemens", ""]]}, {"id": "1910.01716", "submitter": "Khaza Anuarul Hoque", "authors": "Gautam Raj Mode, Prasad Calyam, Khaza Anuarul Hoque", "title": "False Data Injection Attacks in Internet of Things and Deep Learning\n  enabled Predictive Analytics", "comments": "extended version of the manuscript entitled \"Impact of False Data\n  Injection Attacks on Deep Learning enabled Predictive Analytics\" accepted for\n  publication in the IEEE NOMS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is the latest industrial revolution primarily merging automation\nwith advanced manufacturing to reduce direct human effort and resources.\nPredictive maintenance (PdM) is an industry 4.0 solution, which facilitates\npredicting faults in a component or a system powered by state-of-the-art\nmachine learning (ML) algorithms and the Internet-of-Things (IoT) sensors.\nHowever, IoT sensors and deep learning (DL) algorithms, both are known for\ntheir vulnerabilities to cyber-attacks. In the context of PdM systems, such\nattacks can have catastrophic consequences as they are hard to detect due to\nthe nature of the attack. To date, the majority of the published literature\nfocuses on the accuracy of DL enabled PdM systems and often ignores the effect\nof such attacks. In this paper, we demonstrate the effect of IoT sensor attacks\non a PdM system. At first, we use three state-of-the-art DL algorithms,\nspecifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and\nConvolutional Neural Network (CNN) for predicting the Remaining Useful Life\n(RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results\nshow that the GRU-based PdM model outperforms some of the recent literature on\nRUL prediction using the C-MAPSS dataset. Afterward, we model two different\ntypes of false data injection attacks (FDIA) on turbofan engine sensor data and\nevaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained\nresults demonstrate that FDI attacks on even a few IoT sensors can strongly\ndefect the RUL prediction. However, the GRU-based PdM model performs better in\nterms of accuracy and resiliency. Lastly, we perform a study on the GRU-based\nPdM model using four different GRU networks with different sequence lengths.\nOur experiments reveal an interesting relationship between the accuracy,\nresiliency and sequence length for the GRU-based PdM models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:50:08 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 21:26:54 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 02:57:28 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 00:07:14 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Mode", "Gautam Raj", ""], ["Calyam", "Prasad", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "1910.01723", "submitter": "Kolby Nottingham", "authors": "Kolby Nottingham, Anand Balakrishnan, Jyotirmoy Deshmukh, Connor\n  Christopherson, Joshua Greaves, David Wingate", "title": "Using Logical Specifications of Objectives in Multi-Objective\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multi-objective reinforcement learning (MORL) paradigm, the relative\nimportance of each environment objective is often unknown prior to training, so\nagents must learn to specialize their behavior to optimize different\ncombinations of environment objectives that are specified post-training. These\nare typically linear combinations, so the agent is effectively parameterized by\na weight vector that describes how to balance competing environment objectives.\nHowever, many real world behaviors require non-linear combinations of\nobjectives. Additionally, the conversion between desired behavior and\nweightings is often unclear.\n  In this work, we explore the use of a language based on propositional logic\nwith quantitative semantics--in place of weight vectors--for specifying\nnon-linear behaviors in an interpretable way. We use a recurrent encoder to\nencode logical combinations of objectives, and train a MORL agent to generalize\nover these encodings. We test our agent in several environments with various\nobjectives and show that our agent can generalize to many never-before-seen\nspecifications with performance comparable to single policy baseline agents. We\nalso demonstrate our agent's ability to generate meaningful policies when\npresented with novel specifications and quickly specialize to novel\nspecifications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:16:04 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:15:34 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Nottingham", "Kolby", ""], ["Balakrishnan", "Anand", ""], ["Deshmukh", "Jyotirmoy", ""], ["Christopherson", "Connor", ""], ["Greaves", "Joshua", ""], ["Wingate", "David", ""]]}, {"id": "1910.01727", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem\n  Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, Soumith Chintala", "title": "Generalized Inner Loop Meta-Learning", "comments": "17 pages, 3 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many (but not all) approaches self-qualifying as \"meta-learning\" in deep\nlearning and reinforcement learning fit a common pattern of approximating the\nsolution to a nested optimization problem. In this paper, we give a\nformalization of this shared pattern, which we call GIMLI, prove its general\nrequirements, and derive a general-purpose algorithm for implementing similar\napproaches. Based on this analysis and algorithm, we describe a library of our\ndesign, higher, which we share with the community to assist and enable future\nresearch into these kinds of meta-learning approaches. We end the paper by\nshowcasing the practical applications of this framework and library through\nillustrative experiments and ablation studies which they facilitate.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:41:56 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 15:42:14 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Grefenstette", "Edward", ""], ["Amos", "Brandon", ""], ["Yarats", "Denis", ""], ["Htut", "Phu Mon", ""], ["Molchanov", "Artem", ""], ["Meier", "Franziska", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""], ["Chintala", "Soumith", ""]]}, {"id": "1910.01728", "submitter": "Jaya Kumar Alageshan", "authors": "Jaya Kumar Alageshan, Akhilesh Kumar Verma, J\\'er\\'emie Bec, Rahul\n  Pandit", "title": "Machine learning strategies for path-planning microswimmers in turbulent\n  flows", "comments": "8 pages, 10 figures", "journal-ref": "Phys. Rev. E 101, 043110 (2020)", "doi": "10.1103/PhysRevE.101.043110", "report-no": null, "categories": "physics.flu-dyn math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop an adversarial-reinforcement learning scheme for microswimmers in\nstatistically homogeneous and isotropic turbulent fluid flows, in both two (2D)\nand three dimensions (3D). We show that this scheme allows microswimmers to\nfind non-trivial paths, which enable them to reach a target on average in less\ntime than a naive microswimmer, which tries, at any instant of time and at a\ngiven position in space, to swim in the direction of the target. We use\npseudospectral direct numerical simulations (DNSs) of the 2D and 3D\n(incompressible) Navier-Stokes equations to obtain the turbulent flows. We then\nintroduce passive microswimmers that try to swim along a given direction in\nthese flows; the microswimmers do not affect the flow, but they are advected by\nit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:47:08 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 15:55:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Alageshan", "Jaya Kumar", ""], ["Verma", "Akhilesh Kumar", ""], ["Bec", "J\u00e9r\u00e9mie", ""], ["Pandit", "Rahul", ""]]}, {"id": "1910.01734", "submitter": "Jinchi Lv", "authors": "Jianqing Fan, Yingying Fan, Xiao Han, Jinchi Lv", "title": "SIMPLE: Statistical Inference on Membership Profiles in Large Networks", "comments": "60 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data is prevalent in many contemporary big data applications in which\na common interest is to unveil important latent links between different pairs\nof nodes. Yet a simple fundamental question of how to precisely quantify the\nstatistical uncertainty associated with the identification of latent links\nstill remains largely unexplored. In this paper, we propose the method of\nstatistical inference on membership profiles in large networks (SIMPLE) in the\nsetting of degree-corrected mixed membership model, where the null hypothesis\nassumes that the pair of nodes share the same profile of community memberships.\nIn the simpler case of no degree heterogeneity, the model reduces to the mixed\nmembership model for which an alternative more robust test is also proposed.\nBoth tests are of the Hotelling-type statistics based on the rows of empirical\neigenvectors or their ratios, whose asymptotic covariance matrices are very\nchallenging to derive and estimate. Nevertheless, their analytical expressions\nare unveiled and the unknown covariance matrices are consistently estimated.\nUnder some mild regularity conditions, we establish the exact limiting\ndistributions of the two forms of SIMPLE test statistics under the null\nhypothesis and contiguous alternative hypothesis. They are the chi-square\ndistributions and the noncentral chi-square distributions, respectively, with\ndegrees of freedom depending on whether the degrees are corrected or not. We\nalso address the important issue of estimating the unknown number of\ncommunities and establish the asymptotic properties of the associated test\nstatistics. The advantages and practical utility of our new procedures in terms\nof both size and power are demonstrated through several simulation examples and\nreal network applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:01:39 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Fan", "Jianqing", ""], ["Fan", "Yingying", ""], ["Han", "Xiao", ""], ["Lv", "Jinchi", ""]]}, {"id": "1910.01735", "submitter": "Bo Jiang", "authors": "Bo Jiang, Beibei Wang, Jin Tang and Bin Luo", "title": "GmCN: Graph Mask Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have shown very powerful for graph data\nrepresentation and learning tasks. Existing GCNs usually conduct feature\naggregation on a fixed neighborhood graph in which each node computes its\nrepresentation by aggregating the feature representations of all its neighbors\nwhich is biased by its own representation. However, this fixed aggregation\nstrategy is not guaranteed to be optimal for GCN based graph learning and also\ncan be affected by some graph structure noises, such as incorrect or undesired\nedge connections. To address these issues, we propose a novel Graph mask\nConvolutional Network (GmCN) in which nodes can adaptively select the optimal\nneighbors in their feature aggregation to better serve GCN learning. GmCN can\nbe theoretically interpreted by a regularization framework, based on which we\nderive a simple update algorithm to determine the optimal mask adaptively in\nGmCN training process. Experiments on several datasets validate the\neffectiveness of GmCN.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 02:59:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 08:50:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Beibei", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1910.01736", "submitter": "Bo Jiang", "authors": "Bo Jiang, Leiling Wang, Jin Tang and Bin Luo", "title": "Context-Aware Graph Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been widely studied for graph data\nrepresentation and learning. However, existing GNNs generally conduct\ncontext-aware learning on node feature representation only which usually\nignores the learning of edge (weight) representation. In this paper, we propose\na novel unified GNN model, named Context-aware Adaptive Graph Attention Network\n(CaGAT). CaGAT aims to learn a context-aware attention representation for each\ngraph edge by further exploiting the context relationships among different\nedges. In particular, CaGAT conducts context-aware learning on both node\nfeature representation and edge (weight) representation simultaneously and\ncooperatively in a unified manner which can boost their respective performance\nin network training. We apply CaGAT on semi-supervised learning tasks.\nPromising experimental results on several benchmark datasets demonstrate the\neffectiveness and benefits of CaGAT.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 03:17:30 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Leiling", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1910.01738", "submitter": "Astrid Merckling", "authors": "Astrid Merckling, Alexandre Coninx, Loic Cressot, Stephane Doncieux\n  and Nicolas Perrin", "title": "State Representation Learning from Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context where several policies can be observed as black boxes on\ndifferent instances of a control task, we propose a method to derive a state\nrepresentation that can be relied on to reproduce any of the observed policies.\nWe do so via imitation learning on a multi-head neural network consisting of a\nfirst part that outputs a common state representation and then one head per\npolicy to imitate. If the demonstrations contain enough diversity, the state\nrepresentation is general and can be transferred to learn new instances of the\ntask. We present a proof of concept with experimental results on a simulated 2D\nrobotic arm performing a reaching task, with noisy image inputs containing a\ndistractor, and show that the state representations learned provide a greater\nspeed up to end-to-end reinforcement learning on new instances of the task than\nwith other classical representations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 14:43:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Merckling", "Astrid", ""], ["Coninx", "Alexandre", ""], ["Cressot", "Loic", ""], ["Doncieux", "Stephane", ""], ["Perrin", "Nicolas", ""]]}, {"id": "1910.01739", "submitter": "David Eriksson", "authors": "David Eriksson, Michael Pearce, Jacob R Gardner, Ryan Turner, Matthias\n  Poloczek", "title": "Scalable Global Optimization via Local Bayesian Optimization", "comments": "Appears in NeurIPS 2019 as a spotlight paper", "journal-ref": "In Advances in Neural Information Processing Systems 32, pages\n  5497-5508. 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has recently emerged as a popular method for the\nsample-efficient optimization of expensive black-box functions. However, the\napplication to high-dimensional problems with several thousand observations\nremains challenging, and on difficult problems Bayesian optimization is often\nnot competitive with other paradigms. In this paper we take the view that this\nis due to the implicit homogeneity of the global probabilistic models and an\noveremphasized exploration that results from global acquisition. This motivates\nthe design of a local probabilistic approach for global optimization of\nlarge-scale high-dimensional problems. We propose the $\\texttt{TuRBO}$\nalgorithm that fits a collection of local models and performs a principled\nglobal allocation of samples across these models via an implicit bandit\napproach. A comprehensive evaluation demonstrates that $\\texttt{TuRBO}$\noutperforms state-of-the-art methods from machine learning and operations\nresearch on problems spanning reinforcement learning, robotics, and the natural\nsciences.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:05:46 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 20:53:18 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 02:56:38 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 20:59:28 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Eriksson", "David", ""], ["Pearce", "Michael", ""], ["Gardner", "Jacob R", ""], ["Turner", "Ryan", ""], ["Poloczek", "Matthias", ""]]}, {"id": "1910.01740", "submitter": "Harsh Shrivastava", "authors": "Samyam Rajbhandari, Harsh Shrivastava, Yuxiong He", "title": "AntMan: Sparse Low-Rank Compression to Accelerate RNN inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide adoption of complex RNN based models is hindered by their inference\nperformance, cost and memory requirements. To address this issue, we develop\nAntMan, combining structured sparsity with low-rank decomposition\nsynergistically, to reduce model computation, size and execution time of RNNs\nwhile attaining desired accuracy. AntMan extends knowledge distillation based\ntraining to learn the compressed models efficiently. Our evaluation shows that\nAntMan offers up to 100x computation reduction with less than 1pt accuracy drop\nfor language and machine reading comprehension models. Our evaluation also\nshows that for a given accuracy target, AntMan produces 5x smaller models than\nthe state-of-art. Lastly, we show that AntMan offers super-linear speed gains\ncompared to theoretical speedup, demonstrating its practical value on commodity\nhardware.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:31:09 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Shrivastava", "Harsh", ""], ["He", "Yuxiong", ""]]}, {"id": "1910.01741", "submitter": "Denis Yarats", "authors": "Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau,\n  Rob Fergus", "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training an agent to solve control tasks directly from high-dimensional\nimages with model-free reinforcement learning (RL) has proven difficult. A\npromising approach is to learn a latent representation together with the\ncontrol policy. However, fitting a high-capacity encoder using a scarce reward\nsignal is sample inefficient and leads to poor performance. Prior work has\nshown that auxiliary losses, such as image reconstruction, can aid efficient\nrepresentation learning. However, incorporating reconstruction loss into an\noff-policy learning algorithm often leads to training instability. We explore\nthe underlying reasons and identify variational autoencoders, used by previous\ninvestigations, as the cause of the divergence. Following these findings, we\npropose effective techniques to improve training stability. This results in a\nsimple approach capable of matching state-of-the-art model-free and model-based\nalgorithms on MuJoCo control tasks. Furthermore, our approach demonstrates\nrobustness to observational noise, surpassing existing approaches in this\nsetting. Code, results, and videos are anonymously available at\nhttps://sites.google.com/view/sac-ae/home.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:50:03 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 00:34:50 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 15:42:09 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yarats", "Denis", ""], ["Zhang", "Amy", ""], ["Kostrikov", "Ilya", ""], ["Amos", "Brandon", ""], ["Pineau", "Joelle", ""], ["Fergus", "Rob", ""]]}, {"id": "1910.01742", "submitter": "Wenqi Wei", "authors": "Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Emre Gursoy, Stacey\n  Truex, Yanzhao Wu", "title": "Cross-Layer Strategic Ensemble Defense Against Adversarial Examples", "comments": "To appear in IEEE ICNC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) has demonstrated its success in multiple domains.\nHowever, DNN models are inherently vulnerable to adversarial examples, which\nare generated by adding adversarial perturbations to benign inputs to fool the\nDNN model to misclassify. In this paper, we present a cross-layer strategic\nensemble framework and a suite of robust defense algorithms, which are\nattack-independent, and capable of auto-repairing and auto-verifying the target\nmodel being attacked. Our strategic ensemble approach makes three original\ncontributions. First, we employ input-transformation diversity to design the\ninput-layer strategic transformation ensemble algorithms. Second, we utilize\nmodel-disagreement diversity to develop the output-layer strategic model\nensemble algorithms. Finally, we create an input-output cross-layer strategic\nensemble defense that strengthens the defensibility by combining diverse input\ntransformation based model ensembles with diverse output verification model\nensembles. Evaluated over 10 attacks on ImageNet dataset, we show that our\nstrategic ensemble defense algorithms can achieve high defense success rates\nand are more robust with high attack prevention success rates and low benign\nfalse negative rates, compared to existing representative defense methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 07:57:42 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wei", "Wenqi", ""], ["Liu", "Ling", ""], ["Loper", "Margaret", ""], ["Chow", "Ka-Ho", ""], ["Gursoy", "Emre", ""], ["Truex", "Stacey", ""], ["Wu", "Yanzhao", ""]]}, {"id": "1910.01743", "submitter": "Shih-Yang Su", "authors": "Shih-Yang Su, Hossein Hajimirsadeghi, Greg Mori", "title": "Graph Generation with Variational Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating graph structures is a challenging problem due to the diverse\nrepresentations and complex dependencies among nodes. In this paper, we\nintroduce Graph Variational Recurrent Neural Network (GraphVRNN), a\nprobabilistic autoregressive model for graph generation. Through modeling the\nlatent variables of graph data, GraphVRNN can capture the joint distributions\nof graph structures and the underlying node attributes. We conduct experiments\non the proposed GraphVRNN in both graph structure learning and attribute\ngeneration tasks. The evaluation results show that the variational component\nallows our network to model complicated distributions, as well as generate\nplausible structures and node attributes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:23:14 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Su", "Shih-Yang", ""], ["Hajimirsadeghi", "Hossein", ""], ["Mori", "Greg", ""]]}, {"id": "1910.01751", "submitter": "Suraj Nair", "authors": "Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei", "title": "Causal Induction from Visual Observations for Goal Directed Tasks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal reasoning has been an indispensable capability for humans and other\nintelligent animals to interact with the physical world. In this work, we\npropose to endow an artificial agent with the capability of causal reasoning\nfor completing goal-directed tasks. We develop learning-based approaches to\ninducing causal knowledge in the form of directed acyclic graphs, which can be\nused to contextualize a learned goal-conditional policy to perform tasks in\nnovel environments with latent causal structures. We leverage attention\nmechanisms in our causal induction model and goal-conditional policy, enabling\nus to incrementally generate the causal graph from the agent's visual\nobservations and to selectively use the induced graph for determining actions.\nOur experiments show that our method effectively generalizes towards completing\nnew tasks in novel environments with previously unseen causal structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:32:40 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Nair", "Suraj", ""], ["Zhu", "Yuke", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1910.01769", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee and Ahmed Hassan Awadallah", "title": "Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data", "comments": "Multilingual version of this work, namely XtremeDistil\n  (https://aka.ms/XtremeDistil) appears at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in pre-training huge models on large amounts of text through\nself supervision have obtained state-of-the-art results in various natural\nlanguage processing tasks. However, these huge and expensive models are\ndifficult to use in practise for downstream tasks. Some recent efforts use\nknowledge distillation to compress these models. However, we see a gap between\nthe performance of the smaller student models as compared to that of the large\nteacher. In this work, we leverage large amounts of in-domain unlabeled\ntransfer data in addition to a limited amount of labeled training instances to\nbridge this gap for distilling BERT. We show that simple RNN based student\nmodels even with hard distillation can perform at par with the huge teachers\ngiven the transfer set. The student performance can be further improved with\nsoft distillation and leveraging teacher intermediate representations. We show\nthat our student models can compress the huge teacher by up to 26x while still\nmatching or even marginally exceeding the teacher performance in low-resource\nsettings with small amount of labeled data. Additionally, for the multilingual\nextension of this work with XtremeDistil (Mukherjee and Hassan Awadallah,\n2020), we demonstrate massive distillation of multilingual BERT-like teacher\nmodels by upto 35x in terms of parameter compression and 51x in terms of\nlatency speedup for batch inference while retaining 95% of its F1-score for NER\nover 41 languages.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:01:26 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 01:44:14 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "1910.01784", "submitter": "Lu Wang", "authors": "Lu Wang, Wenchao Yu, Wei Wang, Wei Cheng, Wei Zhang, Hongyuan Zha,\n  Xiaofeng He, Haifeng Chen", "title": "Learning Robust Representations with Graph Denoising Policy Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning, aiming to learn low-dimensional\nrepresentations which capture the geometric dependencies between nodes in the\noriginal graph, has gained increasing popularity in a variety of graph analysis\ntasks, including node classification and link prediction. Existing\nrepresentation learning methods based on graph neural networks and their\nvariants rely on the aggregation of neighborhood information, which makes it\nsensitive to noises in the graph. In this paper, we propose Graph Denoising\nPolicy Network (short for GDPNet) to learn robust representations from noisy\ngraph data through reinforcement learning. GDPNet first selects signal\nneighborhoods for each node, and then aggregates the information from the\nselected neighborhoods to learn node representations for the down-stream tasks.\nSpecifically, in the signal neighborhood selection phase, GDPNet optimizes the\nneighborhood for each target node by formulating the process of removing noisy\nneighborhoods as a Markov decision process and learning a policy with\ntask-specific rewards received from the representation learning phase. In the\nrepresentation learning phase, GDPNet aggregates features from signal neighbors\nto generate node representations for down-stream tasks, and provides\ntask-specific rewards to the signal neighbor selection phase. These two phases\nare jointly trained to select optimal sets of neighbors for target nodes with\nmaximum cumulative task-specific rewards, and to learn robust representations\nfor nodes. Experimental results on node classification task demonstrate the\neffectiveness of GDNet, outperforming the state-of-the-art graph representation\nlearning methods on several well-studied datasets. Additionally, GDPNet is\nmathematically equivalent to solving the submodular maximizing problem, which\ntheoretically guarantees the best approximation to the optimal solution with\nGDPNet.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:22:17 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wang", "Lu", ""], ["Yu", "Wenchao", ""], ["Wang", "Wei", ""], ["Cheng", "Wei", ""], ["Zhang", "Wei", ""], ["Zha", "Hongyuan", ""], ["He", "Xiaofeng", ""], ["Chen", "Haifeng", ""]]}, {"id": "1910.01786", "submitter": "Zhen Zeng Dr.", "authors": "Zhen Zeng, Yuefeng Lu, Judong Shen, Wei Zheng, Peter Shaw, Mary Beth\n  Dorr", "title": "A Random Interaction Forest for Prioritizing Predictive Biomarkers", "comments": "15 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine is becoming a focus in medical research recently, as its\nimplementation brings values to all stakeholders in the healthcare system.\nVarious statistical methodologies have been developed tackling problems in\ndifferent aspects of this field, e.g., assessing treatment heterogeneity,\nidentifying patient subgroups, or building treatment decision models. However,\nthere is a lack of new tools devoted to selecting and prioritizing predictive\nbiomarkers. We propose a novel tree-based ensemble method, random interaction\nforest (RIF), to generate predictive importance scores and prioritize candidate\nbiomarkers for constructing refined treatment decision models. RIF was\nevaluated by comparing with the conventional random forest and univariable\nregression methods and showed favorable properties under various simulation\nscenarios. We applied the proposed RIF method to a biomarker dataset from two\nphase III clinical trials of bezlotoxumab on $\\textit{Clostridium difficile}$\ninfection recurrence and obtained biologically meaningful results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:28:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zeng", "Zhen", ""], ["Lu", "Yuefeng", ""], ["Shen", "Judong", ""], ["Zheng", "Wei", ""], ["Shaw", "Peter", ""], ["Dorr", "Mary Beth", ""]]}, {"id": "1910.01791", "submitter": "Mohammad Lotfollahi", "authors": "Mohammad Lotfollahi, Mohsen Naghipourfar, Fabian J. Theis, F.\n  Alexander Wolf", "title": "Conditional out-of-sample generation for unpaired data using trVAE", "comments": "Added reference to Johansson et al. (2016) and removed sentences from\n  Lopez et al. (2018) in the background section (see acknowledgements)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.CB q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative models have shown great success in generating\nhigh-dimensional samples conditional on low-dimensional descriptors (learning\ne.g. stroke thickness in MNIST, hair color in CelebA, or speaker identity in\nWavenet), their generation out-of-sample poses fundamental problems. The\nconditional variational autoencoder (CVAE) as a simple conditional generative\nmodel does not explicitly relate conditions during training and, hence, has no\nincentive of learning a compact joint distribution across conditions. We\novercome this limitation by matching their distributions using maximum mean\ndiscrepancy (MMD) in the decoder layer that follows the bottleneck. This\nintroduces a strong regularization both for reconstructing samples within the\nsame condition and for transforming samples across conditions, resulting in\nmuch improved generalization. We refer to the architecture as\n\\emph{transformer} VAE (trVAE). Benchmarking trVAE on high-dimensional image\nand tabular data, we demonstrate higher robustness and higher accuracy than\nexisting approaches. In particular, we show qualitatively improved predictions\nfor cellular perturbation response to treatment and disease based on\nhigh-dimensional single-cell gene expression data, by tackling previously\nproblematic minority classes and multiple conditions. For generic tasks, we\nimprove Pearson correlations of high-dimensional estimated means and variances\nwith their ground truths from 0.89 to 0.97 and 0.75 to 0.87, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 03:39:35 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 11:49:58 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Lotfollahi", "Mohammad", ""], ["Naghipourfar", "Mohsen", ""], ["Theis", "Fabian J.", ""], ["Wolf", "F. Alexander", ""]]}, {"id": "1910.01799", "submitter": "Matt Wand", "authors": "Marianne Menictas, Gioia Di Credico and Matt P. Wand", "title": "Streamlined Variational Inference for Linear Mixed Models with Crossed\n  Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive streamlined mean field variational Bayes algorithms for fitting\nlinear mixed models with crossed random effects. In the most general situation,\nwhere the dimensions of the crossed groups are arbitrarily large, streamlining\nis hindered by lack of sparseness in the underlying least squares system.\nBecause of this fact we also consider a hierarchy of relaxations of the mean\nfield product restriction. The least stringent product restriction delivers a\nhigh degree of inferential accuracy. However, this accuracy must be mitigated\nagainst its higher storage and computing demands. Faster sparse storage and\ncomputing alternatives are also provided, but come with the price of diminished\ninferential accuracy. This article provides full algorithmic details of three\nvariational inference strategies, presents detailed empirical results on their\npros and cons and, thus, guides the users on their choice of variational\ninference approach depending on the problem size and computing resources.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 04:49:49 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 22:27:08 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Menictas", "Marianne", ""], ["Di Credico", "Gioia", ""], ["Wand", "Matt P.", ""]]}, {"id": "1910.01803", "submitter": "Sajad Darabi", "authors": "Sajad Darabi, Mohammad Kachuee, Majid Sarrafzadeh", "title": "Unsupervised Representation for EHR Signals and Codes as Patient Status\n  Vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective modeling of electronic health records presents many challenges as\nthey contain large amounts of irregularity most of which are due to the varying\nprocedures and diagnosis a patient may have. Despite the recent progress in\nmachine learning, unsupervised learning remains largely at open, especially in\nthe healthcare domain. In this work, we present a two-step unsupervised\nrepresentation learning scheme to summarize the multi-modal clinical time\nseries consisting of signals and medical codes into a patient status vector.\nFirst, an auto-encoder step is used to reduce sparse medical codes and clinical\ntime series into a distributed representation. Subsequently, the concatenation\nof the distributed representations is further fine-tuned using a forecasting\ntask. We evaluate the usefulness of the representation on two downstream tasks:\nmortality and readmission. Our proposed method shows improved generalization\nperformance for both short duration ICU visits and long duration ICU visits.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 05:42:50 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Darabi", "Sajad", ""], ["Kachuee", "Mohammad", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1910.01805", "submitter": "Alexander Jung", "authors": "Alexander Jung", "title": "On the Duality between Network Flows and Network Lasso", "comments": "networks, clustering, machine learning, optimization, duality, Lasso", "journal-ref": null, "doi": "10.1109/LSP.2020.2998400", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications generate data with an intrinsic network structure such as\ntime series data, image data or social network data. The network Lasso (nLasso)\nhas been proposed recently as a method for joint clustering and optimization of\nmachine learning models for networked data. The nLasso extends the Lasso from\nsparse linear models to clustered graph signals. This paper explores the\nduality of nLasso and network flow optimization. We show that, in a very\nprecise sense, nLasso is equivalent to a minimum-cost flow problem on the data\nnetwork structure. Our main technical result is a concise characterization of\nnLasso solutions via existence of certain network flows. The main conceptual\nresult is a useful link between nLasso methods and basic graph algorithms such\nas clustering or maximum flow.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 06:04:45 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 07:16:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Jung", "Alexander", ""]]}, {"id": "1910.01833", "submitter": "Tanner Bohn", "authors": "Tanner Bohn, Yining Hu, Charles X. Ling", "title": "Few-Shot Abstract Visual Reasoning With Spectral Features", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image preprocessing technique capable of improving the\nperformance of few-shot classifiers on abstract visual reasoning tasks. Many\nvisual reasoning tasks with abstract features are easy for humans to learn with\nfew examples but very difficult for computer vision approaches with the same\nnumber of samples, despite the ability for deep learning models to learn\nabstract features. Same-different (SD) problems represent a type of visual\nreasoning task requiring knowledge of pattern repetition within individual\nimages, and modern computer vision approaches have largely faltered on these\nclassification problems, even when provided with vast amounts of training data.\nWe propose a simple method for solving these problems based on the insight that\nremoving peaks from the amplitude spectrum of an image is capable of\nemphasizing the unique parts of the image. When combined with several\nclassifiers, our method performs well on the SD SVRT tasks with few-shot\nlearning, improving upon the best comparable results on all tasks, with average\nabsolute accuracy increases nearly 40% for some classifiers. In particular, we\nfind that combining Relational Networks with this image preprocessing approach\nimproves their performance from chance-level to over 90% accuracy on several SD\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:15:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Bohn", "Tanner", ""], ["Hu", "Yining", ""], ["Ling", "Charles X.", ""]]}, {"id": "1910.01837", "submitter": "Johannes Rabold", "authors": "Johannes Rabold, Hannah Deininger, Michael Siebers, Ute Schmid", "title": "Enriching Visual with Verbal Explanations for Relational Concepts --\n  Combining LIME with Aleph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of deep learning applications, there is a growing\ndemand for explanations. Visual explanations provide information about which\nparts of an image are relevant for a classifier's decision. However,\nhighlighting of image parts (e.g., an eye) cannot capture the relevance of a\nspecific feature value for a class (e.g., that the eye is wide open).\nFurthermore, highlighting cannot convey whether the classification depends on\nthe mere presence of parts or on a specific spatial relation between them.\nConsequently, we present an approach that is capable of explaining a\nclassifier's decision in terms of logic rules obtained by the Inductive Logic\nProgramming system Aleph. The examples and the background knowledge needed for\nAleph are based on the explanation generation method LIME. We demonstrate our\napproach with images of a blocksworld domain. First, we show that our approach\nis capable of identifying a single relation as important explanatory construct.\nAfterwards, we present the more complex relational concept of towers. Finally,\nwe show how the generated relational rules can be explicitly related with the\ninput image, resulting in richer explanations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:51:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Rabold", "Johannes", ""], ["Deininger", "Hannah", ""], ["Siebers", "Michael", ""], ["Schmid", "Ute", ""]]}, {"id": "1910.01842", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi\n  Hoai Phuong Nguyen, Laura Beggel, Thomas Brox", "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been shown to over-fit a dataset when being\ntrained with noisy labels for a long enough time. To overcome this problem, we\npresent a simple and effective method self-ensemble label filtering (SELF) to\nprogressively filter out the wrong labels during training. Our method improves\nthe task performance by gradually allowing supervision only from the\npotentially non-noisy (clean) labels and stops learning on the filtered noisy\nlabels. For the filtering, we form running averages of predictions over the\nentire training dataset using the network output at different training epochs.\nWe show that these ensemble estimates yield more accurate identification of\ninconsistent predictions throughout training than the single estimates of the\nnetwork at the most recent training epoch. While filtered samples are removed\nentirely from the supervised training loss, we dynamically leverage them via\nsemi-supervised learning in the unsupervised loss. We demonstrate the positive\neffect of such an approach on various image classification tasks under both\nsymmetric and asymmetric label noise and at different noise ratios. It\nsubstantially outperforms all previous works on noise-aware learning across\ndifferent datasets and can be applied to a broad set of network architectures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:59:54 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Mummadi", "Chaithanya Kumar", ""], ["Ngo", "Thi Phuong Nhung", ""], ["Nguyen", "Thi Hoai Phuong", ""], ["Beggel", "Laura", ""], ["Brox", "Thomas", ""]]}, {"id": "1910.01845", "submitter": "Yoel Drori", "authors": "Yoel Drori and Ohad Shamir", "title": "The Complexity of Finding Stationary Points with Stochastic Gradient\n  Descent", "comments": "Corrected the attribution of Ghadimi and Lan's result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the iteration complexity of stochastic gradient descent (SGD) for\nminimizing the gradient norm of smooth, possibly nonconvex functions. We\nprovide several results, implying that the $\\mathcal{O}(\\epsilon^{-4})$ upper\nbound of Ghadimi and Lan~\\cite{ghadimi2013stochastic} (for making the average\ngradient norm less than $\\epsilon$) cannot be improved upon, unless a\ncombination of additional assumptions is made. Notably, this holds even if we\nlimit ourselves to convex quadratic functions. We also show that for nonconvex\nfunctions, the feasibility of minimizing gradients with SGD is surprisingly\nsensitive to the choice of optimality criteria.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:10:11 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:56:15 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 06:58:32 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Drori", "Yoel", ""], ["Shamir", "Ohad", ""]]}, {"id": "1910.01847", "submitter": "Yuta Saito", "authors": "Yuta Saito, Gota Morishita, Shota Yasui", "title": "Dual Learning Algorithm for Delayed Conversions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In display advertising, predicting the conversion rate (CVR), meaning the\nprobability that a user takes a predefined action on an advertiser's website,\nis a fundamental task for estimating the value of displaying an advertisement\nto a user. There are two main challenges in CVR prediction due to delayed\nfeedback. First, some positive labels are not correctly observed in training\ndata because some conversions do not occur immediately after a click. Second,\ndelay mechanisms are not uniform among instances, meaning some positive\nfeedback are much more frequently observed than others. It is widely\nacknowledged that these problems lead to severe bias in CVR prediction. To\novercome these challenges, we propose two unbiased estimators: one for CVR\nprediction and the other for bias estimation. Subsequently, we propose a dual\nlearning algorithm in which a CVR predictor and a bias estimator are trained in\nalternating fashion using only observable conversions. The proposed algorithm\nis the first of its kind to address the two major challenges in a theoretically\nsophisticated manner. Empirical evaluations using synthetic datasets\ndemonstrate the practical value of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:26:48 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 06:44:09 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 01:11:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Saito", "Yuta", ""], ["Morishita", "Gota", ""], ["Yasui", "Shota", ""]]}, {"id": "1910.01849", "submitter": "Sylvain Courtain", "authors": "Sylvain Courtain, Pierre Leleux, Ilkka Kivimaki, Guillaume Guex, Marco\n  Saerens", "title": "Randomized Shortest Paths with Net Flows and Capacity Constraints", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2020.10.005", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends the randomized shortest paths (RSP) model by investigating\nthe net flow RSP and adding capacity constraints on edge flows. The standard\nRSP is a model of movement, or spread, through a network interpolating between\na random-walk and a shortest-path behavior [30, 42, 49]. The framework assumes\na unit flow injected into a source node and collected from a target node with\nflows minimizing the expected transportation cost, together with a relative\nentropy regularization term. In this context, the present work first develops\nthe net flow RSP model considering that edge flows in opposite directions\nneutralize each other (as in electric networks), and proposes an algorithm for\ncomputing the expected routing costs between all pairs of nodes. This quantity\nis called the net flow RSP dissimilarity measure between nodes. Experimental\ncomparisons on node clustering tasks indicate that the net flow RSP\ndissimilarity is competitive with other state-of-the-art dissimilarities. In\nthe second part of the paper, it is shown how to introduce capacity constraints\non edge flows, and a procedure is developed to solve this constrained problem\nby exploiting Lagrangian duality. These two extensions should improve\nsignificantly the scope of applications of the RSP framework.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:35:16 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 09:43:03 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 09:15:12 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Courtain", "Sylvain", ""], ["Leleux", "Pierre", ""], ["Kivimaki", "Ilkka", ""], ["Guex", "Guillaume", ""], ["Saerens", "Marco", ""]]}, {"id": "1910.01907", "submitter": "Adith Boloor", "authors": "Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy\n  Vorobeychik, Xuan Zhang", "title": "Attacking Vision-based Perception in End-to-End Autonomous Driving\n  Models", "comments": "under review in the Journal of Systems Architecture 2019. arXiv admin\n  note: substantial text overlap with arXiv:1903.05157", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning, especially techniques such as deep\nneural networks, are enabling a range of emerging applications. One such\nexample is autonomous driving, which often relies on deep learning for\nperception. However, deep learning-based perception has been shown to be\nvulnerable to a host of subtle adversarial manipulations of images.\nNevertheless, the vast majority of such demonstrations focus on perception that\nis disembodied from end-to-end control. We present novel end-to-end attacks on\nautonomous driving in simulation, using simple physically realizable attacks:\nthe painting of black lines on the road. These attacks target deep neural\nnetwork models for end-to-end autonomous driving control. A systematic\ninvestigation shows that such attacks are easy to engineer, and we describe\nscenarios (e.g., right turns) in which they are highly effective. We define\nseveral objective functions that quantify the success of an attack and develop\ntechniques based on Bayesian Optimization to efficiently traverse the search\nspace of higher dimensional attacks. Additionally, we define a novel class of\nhijacking attacks, where painted lines on the road cause the driver-less car to\nfollow a target path. Through the use of network deconvolution, we provide\ninsights into the successful attacks, which appear to work by mimicking\nactivations of entirely different scenarios. Our code is available at\nhttps://github.com/xz-group/AdverseDrive\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:56:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Boloor", "Adith", ""], ["Garimella", "Karthik", ""], ["He", "Xin", ""], ["Gill", "Christopher", ""], ["Vorobeychik", "Yevgeniy", ""], ["Zhang", "Xuan", ""]]}, {"id": "1910.01911", "submitter": "Junghoon Seo", "authors": "Junghoon Seo, Seungwon Lee, Beomsu Kim, Taegyun Jeon", "title": "Revisiting Classical Bagging with Modern Transfer Learning for\n  On-the-fly Disaster Damage Detector", "comments": "Accepted at the 2019 NeurIPS Workshop on Artificial Intelligence for\n  Humanitarian Assistance and Disaster Response(AI+HADR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic post-disaster damage detection using aerial imagery is crucial for\nquick assessment of damage caused by disaster and development of a recovery\nplan. The main problem preventing us from creating an applicable model in\npractice is that damaged (positive) examples we are trying to detect are much\nharder to obtain than undamaged (negative) examples, especially in short time.\nIn this paper, we revisit the classical bootstrap aggregating approach in the\ncontext of modern transfer learning for data-efficient disaster damage\ndetection. Unlike previous classical ensemble learning articles, our work\npoints out the effectiveness of simple bagging in deep transfer learning that\nhas been underestimated in the context of imbalanced classification. Benchmark\nresults on the AIST Building Change Detection dataset show that our approach\nsignificantly outperforms existing methodologies, including the recently\nproposed disentanglement learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:47:58 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Seo", "Junghoon", ""], ["Lee", "Seungwon", ""], ["Kim", "Beomsu", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1910.01913", "submitter": "Benjamin Eysenbach", "authors": "Benjamin Eysenbach and Sergey Levine", "title": "If MaxEnt RL is the Answer, What is the Question?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimentally, it has been observed that humans and animals often make\ndecisions that do not maximize their expected utility, but rather choose\noutcomes randomly, with probability proportional to expected utility.\nProbability matching, as this strategy is called, is equivalent to maximum\nentropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not\noptimize expected utility. In this paper, we formally show that MaxEnt RL does\noptimally solve certain classes of control problems with variability in the\nreward function. In particular, we show (1) that MaxEnt RL can be used to solve\na certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player\ngame where an adversary chooses the reward function. These results suggest a\ndeeper connection between MaxEnt RL, robust control, and POMDPs, and provide\ninsight for the types of problems for which we might expect MaxEnt RL to\nproduce effective solutions. Specifically, our results suggest that domains\nwith uncertainty in the task goal may be especially well-suited for MaxEnt RL\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:50:34 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Eysenbach", "Benjamin", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.01914", "submitter": "Hicham Janati", "authors": "Hicham Janati, Thomas Bazeille, Bertrand Thirion, Marco Cuturi,\n  Alexandre Gramfort", "title": "Multi-subject MEG/EEG source imaging with sparse multi-task regression", "comments": "version 2. arXiv admin note: text overlap with arXiv:1902.04812", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalography and electroencephalography (M/EEG) are non-invasive\nmodalities that measure the weak electromagnetic fields generated by neural\nactivity. Estimating the location and magnitude of the current sources that\ngenerated these electromagnetic fields is a challenging ill-posed regression\nproblem known as \\emph{source imaging}. When considering a group study, a\ncommon approach consists in carrying out the regression tasks independently for\neach subject. An alternative is to jointly localize sources for all subjects\ntaken together, while enforcing some similarity between them. By pooling all\nmeasurements in a single multi-task regression, one makes the problem better\nposed, offering the ability to identify more sources and with greater\nprecision. The Minimum Wasserstein Estimates (MWE) promotes focal activations\nthat do not perfectly overlap for all subjects, thanks to a regularizer based\non Optimal Transport (OT) metrics. MWE promotes spatial proximity on the\ncortical mantel while coping with the varying noise levels across subjects. On\nrealistic simulations, MWE decreases the localization error by up to 4 mm per\nsource compared to individual solutions. Experiments on the Cam-CAN dataset\nshow a considerable improvement in spatial specificity in population imaging.\nOur analysis of a multimodal dataset shows how multi-subject source\nlocalization closes the gap between MEG and fMRI for brain mapping.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:20:29 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:38:09 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Janati", "Hicham", ""], ["Bazeille", "Thomas", ""], ["Thirion", "Bertrand", ""], ["Cuturi", "Marco", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1910.01921", "submitter": "Behnaz Moradi-Jamei", "authors": "Behnaz Moradi-Jamei, Heman Shakeri, Pietro Poggi-Corradini and Michael\n  J. Higgins", "title": "A new method for quantifying network cyclic structure to improve\n  community detection", "comments": "arXiv admin note: This paper is the new version of arXiv:1805.07484", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distinguishing property of communities in networks is that cycles are more\nprevalent within communities than across communities. Thus, the detection of\nthese communities may be aided through the incorporation of measures of the\nlocal \"richness\" of the cyclic structure. In this paper, we introduce renewal\nnon-backtracking random walks (RNBRW) as a way of quantifying this structure.\nRNBRW gives a weight to each edge equal to the probability that a\nnon-backtracking random walk completes a cycle with that edge. Hence, edges\nwith larger weights may be thought of as more important to the formation of\ncycles. Of note, since separate random walks can be performed in parallel,\nRNBRW weights can be estimated very quickly, even for large graphs. We give\nsimulation results showing that pre-weighting edges through RNBRW may\nsubstantially improve the performance of common community detection algorithms.\nOur results suggest that RNBRW is especially efficient for the challenging case\nof detecting communities in sparse graphs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:49:10 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 20:07:20 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Moradi-Jamei", "Behnaz", ""], ["Shakeri", "Heman", ""], ["Poggi-Corradini", "Pietro", ""], ["Higgins", "Michael J.", ""]]}, {"id": "1910.01931", "submitter": "Marianna Pensky", "authors": "Majid Noroozi, Marianna Pensky and Ramchandra Rimal", "title": "Sparse Popularity Adjusted Stochastic Block Model", "comments": "4 figures. arXiv admin note: text overlap with arXiv:1902.00431", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we study a sparse stochastic network enabled with a\nblock structure. The popular Stochastic Block Model (SBM) and the Degree\nCorrected Block Model (DCBM) address sparsity by placing an upper bound on the\nmaximum probability of connections between any pair of nodes. As a result,\nsparsity describes only the behavior of network as a whole, without\ndistinguishing between the block-dependent sparsity patterns. To the best of\nour knowledge, the recently introduced Popularity Adjusted Block Model (PABM)\nis the only block model that allows to introduce a {\\it structural sparsity}\nwhere some probabilities of connections are identically equal to zero while the\nrest of them remain above a certain threshold. The latter presents a more\nnuanced view of the network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:20:05 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:06:34 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Noroozi", "Majid", ""], ["Pensky", "Marianna", ""], ["Rimal", "Ramchandra", ""]]}, {"id": "1910.01963", "submitter": "Sedigheh Mahdavi", "authors": "Sedigheh Mahdavi, Shima Khoshraftar, and Aijun An", "title": "Dynamic Joint Variational Graph Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning network representations is a fundamental task for many graph\napplications such as link prediction, node classification, graph clustering,\nand graph visualization. Many real-world networks are interpreted as dynamic\nnetworks and evolve over time. Most existing graph embedding algorithms were\ndeveloped for static graphs mainly and cannot capture the evolution of a large\ndynamic network. In this paper, we propose Dynamic joint Variational Graph\nAutoencoders (Dyn-VGAE) that can learn both local structures and temporal\nevolutionary patterns in a dynamic network. Dyn-VGAE provides a joint learning\nframework for computing temporal representations of all graph snapshots\nsimultaneously. Each auto-encoder embeds a graph snapshot based on its local\nstructure and can also learn temporal dependencies by collaborating with other\nautoencoders. We conduct experimental studies on dynamic real-world graph\ndatasets and the results demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:13:46 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mahdavi", "Sedigheh", ""], ["Khoshraftar", "Shima", ""], ["An", "Aijun", ""]]}, {"id": "1910.01991", "submitter": "Felix Sattler", "authors": "Felix Sattler, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task\n  Optimization under Privacy Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is currently the most widely adopted framework for\ncollaborative training of (deep) machine learning models under privacy\nconstraints. Albeit it's popularity, it has been observed that Federated\nLearning yields suboptimal results if the local clients' data distributions\ndiverge. To address this issue, we present Clustered Federated Learning (CFL),\na novel Federated Multi-Task Learning (FMTL) framework, which exploits\ngeometric properties of the FL loss surface, to group the client population\ninto clusters with jointly trainable data distributions. In contrast to\nexisting FMTL approaches, CFL does not require any modifications to the FL\ncommunication protocol to be made, is applicable to general non-convex\nobjectives (in particular deep neural networks) and comes with strong\nmathematical guarantees on the clustering quality. CFL is flexible enough to\nhandle client populations that vary over time and can be implemented in a\nprivacy preserving way. As clustering is only performed after Federated\nLearning has converged to a stationary point, CFL can be viewed as a\npost-processing method that will always achieve greater or equal performance\nthan conventional FL by allowing clients to arrive at more specialized models.\nWe verify our theoretical analysis in experiments with deep convolutional and\nrecurrent neural networks on commonly used Federated Learning datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:31:09 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Sattler", "Felix", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1910.01992", "submitter": "Zhen Huang", "authors": "Zhen Huang, Tim Ng, Leo Liu, Henry Mason, Xiaodan Zhuang, Daben Liu", "title": "SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units\n  for speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep CNNs achieve state-of-the-art results in both computer vision and\nspeech recognition, but are difficult to train. The most popular way to train\nvery deep CNNs is to use shortcut connections (SC) together with batch\nnormalization (BN). Inspired by Self- Normalizing Neural Networks, we propose\nthe self-normalizing deep CNN (SNDCNN) based acoustic model topology, by\nremoving the SC/BN and replacing the typical RELU activations with scaled\nexponential linear unit (SELU) in ResNet-50. SELU activations make the network\nself-normalizing and remove the need for both shortcut connections and batch\nnormalization. Compared to ResNet- 50, we can achieve the same or lower (up to\n4.5% relative) word error rate (WER) while boosting both training and inference\nspeed by 60%-80%. We also explore other model inference optimization schemes to\nfurther reduce latency for production use.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:31:48 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 14:39:52 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 20:39:17 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Huang", "Zhen", ""], ["Ng", "Tim", ""], ["Liu", "Leo", ""], ["Mason", "Henry", ""], ["Zhuang", "Xiaodan", ""], ["Liu", "Daben", ""]]}, {"id": "1910.02007", "submitter": "Yi Liu", "authors": "Yi Liu, Jialiang Peng, James J.Q Yu, Yi Wu", "title": "PPGAN: Privacy-preserving Generative Adversarial Network", "comments": "This paper was accepted by IEEE ICPADS 2019 Workshop. This paper\n  contains 10 pages, 3 figures", "journal-ref": null, "doi": "10.1109/ICPADS47876.2019.00150", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Network (GAN) and its variants serve as a perfect\nrepresentation of the data generation model, providing researchers with a large\namount of high-quality generated data. They illustrate a promising direction\nfor research with limited data availability. When GAN learns the semantic-rich\ndata distribution from a dataset, the density of the generated distribution\ntends to concentrate on the training data. Due to the gradient parameters of\nthe deep neural network contain the data distribution of the training samples,\nthey can easily remember the training samples. When GAN is applied to private\nor sensitive data, for instance, patient medical records, as private\ninformation may be leakage. To address this issue, we propose a\nPrivacy-preserving Generative Adversarial Network (PPGAN) model, in which we\nachieve differential privacy in GANs by adding well-designed noise to the\ngradient during the model learning procedure. Besides, we introduced the\nMoments Accountant strategy in the PPGAN training process to improve the\nstability and compatibility of the model by controlling privacy loss. We also\ngive a mathematical proof of the differential privacy discriminator. Through\nextensive case studies of the benchmark datasets, we demonstrate that PPGAN can\ngenerate high-quality synthetic data while retaining the required data\navailable under a reasonable privacy budget.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:01:22 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Liu", "Yi", ""], ["Peng", "Jialiang", ""], ["Yu", "James J. Q", ""], ["Wu", "Yi", ""]]}, {"id": "1910.02008", "submitter": "Ying Zhang", "authors": "Ying Zhang, \\\"Omer Deniz Akyildiz, Theodoros Damoulas, Sotirios\n  Sabanis", "title": "Nonasymptotic estimates for Stochastic Gradient Langevin Dynamics under\n  local conditions in nonconvex optimization", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of empirical risk minimization, see Raginsky, Rakhlin, and\nTelgarsky (2017), we are concerned with a non-asymptotic analysis of sampling\nalgorithms used in optimization. In particular, we obtain non-asymptotic error\nbounds for a popular class of algorithms called Stochastic Gradient Langevin\nDynamics (SGLD). These results are derived in Wasserstein-1 and Wasserstein-2\ndistances in the absence of log-concavity of the target distribution. More\nprecisely, the stochastic gradient $H(\\theta, x)$ is assumed to be locally\nLipschitz continuous in both variables, and furthermore, the dissipativity\ncondition is relaxed by removing its uniform dependence in $x$. This relaxation\nallows us to present two key paradigms within the framework of scalable\nposterior sampling for Bayesian inference and of nonconvex optimization;\nnamely, examples from minibatch logistic regression and from variational\ninference are given by providing theoretical guarantees for the sampling\nbehaviour of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:02:44 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 15:37:03 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 19:46:42 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 05:26:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhang", "Ying", ""], ["Akyildiz", "\u00d6mer Deniz", ""], ["Damoulas", "Theodoros", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "1910.02021", "submitter": "Viktor Zenkov", "authors": "Viktor Zenkov and Jason Laska", "title": "Dynamic data fusion using multi-input models for malware classification", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Criminals use malware to disrupt cyber-systems. The number of these\nmalware-vulnerable systems is increasing quickly as common systems, such as\nvehicles, routers, and lightbulbs, become increasingly interconnected\ncyber-systems. To address the scale of this problem, analysts divide malware\ninto classes and develop, for each class, a specialized defense. In this\nproject we classified malware with machine learning. In particular, we used a\nsupervised multi-class long short term memory (LSTM) model. We trained the\nalgorithm with thousands of malware files annotated with class labels (the\ntraining set), and the algorithm learned patterns indicative of each class. We\nused disassembled malware files (provided by Microsoft) and separated the\nconstituent data into parsed instructions, which look like human-readable\nmachine code text, and raw bytes, which are hexadecimal values. We are\ninterested in which format, text or hex, is more valuable as input for\nclassification. To solve this, we investigated four cases: a text-only model, a\nhexadecimal-only model, a multi-input model using both text and hexadecimal\ninputs, and a model based on combining the individual results. We performed\nthis investigation using the machine learning Python package Keras, which\nallows easily configurable deep learning architectures and training. We hoped\nto understand the trade-offs between the different formats. Due to the class\nimbalance in the data, we used multiple methods to compare the formats, using\ntest accuracies, balanced accuracies (taking into account weights of classes),\nand an accuracy derived from tables of confusion. We found that the multi-input\nmodel, which allows learning on both input types simultaneously, resulted in\nthe best performance. Our finding expedites malware classification research by\nproviding researchers a suitable deep learning architecture to train a tailored\nversion to their malware.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 16:00:29 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zenkov", "Viktor", ""], ["Laska", "Jason", ""]]}, {"id": "1910.02034", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Ahmed Farahat, Chetan Gupta", "title": "Generative Adversarial Networks for Failure Prediction", "comments": "ECML PKDD 2019 (The European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostics and Health Management (PHM) is an emerging engineering discipline\nwhich is concerned with the analysis and prediction of equipment health and\nperformance. One of the key challenges in PHM is to accurately predict\nimpending failures in the equipment. In recent years, solutions for failure\nprediction have evolved from building complex physical models to the use of\nmachine learning algorithms that leverage the data generated by the equipment.\nHowever, failure prediction problems pose a set of unique challenges that make\ndirect application of traditional classification and prediction algorithms\nimpractical. These challenges include the highly imbalanced training data, the\nextremely high cost of collecting more failure samples, and the complexity of\nthe failure patterns. Traditional oversampling techniques will not be able to\ncapture such complexity and accordingly result in overfitting the training\ndata. This paper addresses these challenges by proposing a novel algorithm for\nfailure prediction using Generative Adversarial Networks (GAN-FP). GAN-FP first\nutilizes two GAN networks to simultaneously generate training samples and build\nan inference network that can be used to predict failures for new samples.\nGAN-FP first adopts an infoGAN to generate realistic failure and non-failure\nsamples, and initialize the weights of the first few layers of the inference\nnetwork. The inference network is then tuned by optimizing a weighted loss\nobjective using only real failure and non-failure samples. The inference\nnetwork is further tuned using a second GAN whose purpose is to guarantee the\nconsistency between the generated samples and corresponding labels. GAN-FP can\nbe used for other imbalanced classification problems as well.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:51:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zheng", "Shuai", ""], ["Farahat", "Ahmed", ""], ["Gupta", "Chetan", ""]]}, {"id": "1910.02035", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Chetan Gupta, Susumu Serita", "title": "Manufacturing Dispatching using Reinforcement and Transfer Learning", "comments": "ECML PKDD 2019 (The European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient dispatching rule in manufacturing industry is key to ensure product\non-time delivery and minimum past-due and inventory cost. Manufacturing,\nespecially in the developed world, is moving towards on-demand manufacturing\nmeaning a high mix, low volume product mix. This requires efficient dispatching\nthat can work in dynamic and stochastic environments, meaning it allows for\nquick response to new orders received and can work over a disparate set of shop\nfloor settings. In this paper we address this problem of dispatching in\nmanufacturing. Using reinforcement learning (RL), we propose a new design to\nformulate the shop floor state as a 2-D matrix, incorporate job slack time into\nstate representation, and design lateness and tardiness rewards function for\ndispatching purpose. However, maintaining a separate RL model for each\nproduction line on a manufacturing shop floor is costly and often infeasible.\nTo address this, we enhance our deep RL model with an approach for dispatching\npolicy transfer. This increases policy generalization and saves time and cost\nfor model training and data collection. Experiments show that: (1) our approach\nperforms the best in terms of total discounted reward and average lateness,\ntardiness, (2) the proposed policy transfer approach reduces training time and\nincreases policy generalization.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:52:46 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zheng", "Shuai", ""], ["Gupta", "Chetan", ""], ["Serita", "Susumu", ""]]}, {"id": "1910.02043", "submitter": "Eduardo Soares Mr", "authors": "Eduardo Soares, Plamen Angelov", "title": "Fair-by-design explainable models for prediction of recidivism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction provides decision makers with an assessment of the\nlikelihood that a criminal defendant will reoffend that can be used in\npre-trial decision-making. It can also be used for prediction of locations\nwhere crimes most occur, profiles that are more likely to commit violent\ncrimes. While such instruments are gaining increasing popularity, their use is\ncontroversial as they may present potential discriminatory bias in the risk\nassessment. In this paper we propose a new fair-by-design approach to predict\nrecidivism. It is prototype-based, learns locally and extracts empirically the\ndata distribution. The results show that the proposed method is able to reduce\nthe bias and provide human interpretable rules to assist specialists in the\nexplanation of the given results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:13:20 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Soares", "Eduardo", ""], ["Angelov", "Plamen", ""]]}, {"id": "1910.02047", "submitter": "Erich Kummerfeld", "authors": "Erich Kummerfeld and Alexander Rix", "title": "Simulations evaluating resampling methods for causal discovery: ensemble\n  performance and calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery can be a powerful tool for investigating causality when a\nsystem can be observed but is inaccessible to experiments in practice. Despite\nthis, it is rarely used in any scientific or medical fields. One of the major\nhurdles preventing the field of causal discovery from having a larger impact is\nthat it is difficult to determine when the output of a causal discovery method\ncan be trusted in a real-world setting. Trust is especially critical when human\nhealth is on the line.\n  In this paper, we report the results of a series of simulation studies\ninvestigating the performance of different resampling methods as indicators of\nconfidence in discovered graph features. We found that subsampling and sampling\nwith replacement both performed surprisingly well, suggesting that they can\nserve as grounds for confidence in graph features. We also found that the\ncalibration of subsampling and sampling with replacement had different\nconvergence properties, suggesting that one's choice of which to use should\ndepend on the sample size.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:19:27 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kummerfeld", "Erich", ""], ["Rix", "Alexander", ""]]}, {"id": "1910.02050", "submitter": "Junho Cho", "authors": "Junho Cho, Sethumadhavan Chandrasekhar, Erixhen Sula, Samuel Olsson,\n  Ellsworth Burrows, Greg Raybon, Roland Ryf, Nicolas Fontaine, Jean-Christophe\n  Antona, Steve Grubb, Peter Winzer, Andrew Chraplyvy", "title": "Supply-Power-Constrained Cable Capacity Maximization Using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We experimentally achieve a 19% capacity gain per Watt of electrical supply\npower in a 12-span link by eliminating gain flattening filters and optimizing\nlaunch powers using machine learning by deep neural networks in a massively\nparallel fiber context.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:37:35 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Cho", "Junho", ""], ["Chandrasekhar", "Sethumadhavan", ""], ["Sula", "Erixhen", ""], ["Olsson", "Samuel", ""], ["Burrows", "Ellsworth", ""], ["Raybon", "Greg", ""], ["Ryf", "Roland", ""], ["Fontaine", "Nicolas", ""], ["Antona", "Jean-Christophe", ""], ["Grubb", "Steve", ""], ["Winzer", "Peter", ""], ["Chraplyvy", "Andrew", ""]]}, {"id": "1910.02054", "submitter": "Jeff Rasley", "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He", "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deep learning models offer significant accuracy gains, but training\nbillions to trillions of parameters is challenging. Existing solutions such as\ndata and model parallelisms exhibit fundamental limitations to fit these models\ninto limited device memory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero Redundancy Optimizer\n(ZeRO), to optimize memory, vastly improving training speed while increasing\nthe model size that can be efficiently trained. ZeRO eliminates memory\nredundancies in data- and model-parallel training while retaining low\ncommunication volume and high computational granularity, allowing us to scale\nthe model size proportional to the number of devices with sustained high\nefficiency. Our analysis on memory requirements and communication volume\ndemonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters\nusing today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter\nwith super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.\nThis represents an 8x increase in model size and 10x increase in achievable\nperformance over state-of-the-art. In terms of usability, ZeRO can train large\nmodels of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)\nwithout requiring model parallelism which is harder for scientists to apply.\nLast but not the least, researchers have used the system breakthroughs of ZeRO\nto create the world's largest language model (Turing-NLG, 17B parameters) with\nrecord breaking accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:29:39 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 16:55:13 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 06:45:15 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Rasley", "Jeff", ""], ["Ruwase", "Olatunji", ""], ["He", "Yuxiong", ""]]}, {"id": "1910.02078", "submitter": "Mathieu Seurin", "authors": "Mathieu Seurin, Philippe Preux, Olivier Pietquin", "title": "I'm sorry Dave, I'm afraid I can't do that, Deep Q-learning from\n  forbidden action", "comments": "Accepted at Internationnal Joint Conference on Neural Networks\n  (IJCNN'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Reinforcement Learning (RL) is still restricted to simulation or\nto enhance human-operated systems through recommendations. Real-world\nenvironments (e.g. industrial robots or power grids) are generally designed\nwith safety constraints in mind implemented in the shape of valid actions masks\nor contingency controllers. For example, the range of motion and the angles of\nthe motors of a robot can be limited to physical boundaries. Violating\nconstraints thus results in rejected actions or entering in a safe mode driven\nby an external controller, making RL agents incapable of learning from their\nmistakes. In this paper, we propose a simple modification of a state-of-the-art\ndeep RL algorithm (DQN), enabling learning from forbidden actions. To do so,\nthe standard Q-learning update is enhanced with an extra safety loss inspired\nby structured classification. We empirically show that it reduces the number of\nhit constraints during the learning phase and accelerates convergence to\nnear-optimal policies compared to using standard DQN. Experiments are done on a\nVisual Grid World Environment and Text-World domain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:43:06 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 15:24:06 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 14:58:53 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 12:21:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Seurin", "Mathieu", ""], ["Preux", "Philippe", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1910.02095", "submitter": "Andrew Lohn", "authors": "Gavin S. Hartnett, Andrew J. Lohn, Alexander P. Sedlack", "title": "Adversarial Examples for Cost-Sensitive Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by safety-critical classification problems, we investigate\nadversarial attacks against cost-sensitive classifiers. We use current\nstate-of-the-art adversarially-resistant neural network classifiers [1] as the\nunderlying models. Cost-sensitive predictions are then achieved via a final\nprocessing step in the feed-forward evaluation of the network. We evaluate the\neffectiveness of cost-sensitive classifiers against a variety of attacks and we\nintroduce a new cost-sensitive attack which performs better than targeted\nattacks in some cases. We also explored the measures a defender can take in\norder to limit their vulnerability to these attacks. This attacker/defender\nscenario is naturally framed as a two-player zero-sum finite game which we\nanalyze using game theory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:16:11 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hartnett", "Gavin S.", ""], ["Lohn", "Andrew J.", ""], ["Sedlack", "Alexander P.", ""]]}, {"id": "1910.02096", "submitter": "Hongteng Xu", "authors": "Dixin Luo, Hongteng Xu, Lawrence Carin", "title": "Fused Gromov-Wasserstein Alignment for Hawkes Processes", "comments": "The workshop on learning with temporal point processes in NeurIPS\n  2019 (WTPP19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fused Gromov-Wasserstein alignment method to jointly learn\nthe Hawkes processes in different event spaces, and align their event types.\nGiven two Hawkes processes, we use fused Gromov-Wasserstein discrepancy to\nmeasure their dissimilarity, which considers both the Wasserstein discrepancy\nbased on their base intensities and the Gromov-Wasserstein discrepancy based on\ntheir infectivity matrices. Accordingly, the learned optimal transport reflects\nthe correspondence between the event types of these two Hawkes processes. The\nHawkes processes and their optimal transport are learned jointly via maximum\nlikelihood estimation, with a fused Gromov-Wasserstein regularizer.\nExperimental results show that the proposed method works well on synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:17:43 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Luo", "Dixin", ""], ["Xu", "Hongteng", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.02097", "submitter": "Heinrich Jiang", "authors": "Ofir Nachum, Heinrich Jiang", "title": "Group-based Fair Learning Leads to Counter-intuitive Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of machine learning (ML) methods have been proposed recently to\nmaximize model predictive accuracy while enforcing notions of group parity or\nfairness across sub-populations. We propose a desirable property for these\nprocedures, slack-consistency: For any individual, the predictions of the model\nshould be monotonic with respect to allowed slack (i.e., maximum allowed\ngroup-parity violation). Such monotonicity can be useful for individuals to\nunderstand the impact of enforcing fairness on their predictions. Surprisingly,\nwe find that standard ML methods for enforcing fairness violate this basic\nproperty. Moreover, this undesirable behavior arises in situations agnostic to\nthe complexity of the underlying model or approximate optimizations, suggesting\nthat the simple act of incorporating a constraint can lead to drastically\nunintended behavior in ML. We present a simple theoretical method for enforcing\nslack-consistency, while encouraging further discussions on the unintended\nbehaviors potentially induced when enforcing group-based parity.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:20:50 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Nachum", "Ofir", ""], ["Jiang", "Heinrich", ""]]}, {"id": "1910.02100", "submitter": "Abishek Sankararaman", "authors": "Abishek Sankararaman, Ayalvadi Ganesh, Sanjay Shakkottai", "title": "Social Learning in Multi Agent Multi Armed Bandits", "comments": "Minor Corrections from before", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI cs.SI math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a distributed version of the classical stochastic\nMulti-Arm Bandit (MAB) problem. Our setting consists of a large number of\nagents $n$ that collaboratively and simultaneously solve the same instance of\n$K$ armed MAB to minimize the average cumulative regret over all agents. The\nagents can communicate and collaborate among each other \\emph{only} through a\npairwise asynchronous gossip based protocol that exchange a limited number of\nbits. In our model, agents at each point decide on (i) which arm to play, (ii)\nwhether to, and if so (iii) what and whom to communicate with. Agents in our\nmodel are decentralized, namely their actions only depend on their observed\nhistory in the past.\n  We develop a novel algorithm in which agents, whenever they choose,\ncommunicate only arm-ids and not samples, with another agent chosen uniformly\nand independently at random. The per-agent regret scaling achieved by our\nalgorithm is $O \\left( \\frac{\\lceil\\frac{K}{n}\\rceil+\\log(n)}{\\Delta}\n  \\log(T) + \\frac{\\log^3(n) \\log \\log(n)}{\\Delta^2}\n  \\right)$. Furthermore, any agent in our algorithm communicates only a total\nof $\\Theta(\\log(T))$ times over a time interval of $T$.\n  We compare our results to two benchmarks - one where there is no\ncommunication among agents and one corresponding to complete interaction. We\nshow both theoretically and empirically, that our algorithm experiences a\nsignificant reduction both in per-agent regret when compared to the case when\nagents do not collaborate and in communication complexity when compared to the\nfull interaction setting which requires $T$ communication attempts by an agent\nover $T$ arm pulls.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:34:04 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 15:12:18 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 01:20:10 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Sankararaman", "Abishek", ""], ["Ganesh", "Ayalvadi", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1910.02107", "submitter": "Tengfei Ma", "authors": "Tengfei Ma, Junyuan Shang, Cao Xiao, Jimeng Sun", "title": "GENN: Predicting Correlated Drug-drug Interactions with Graph Energy\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaining more comprehensive knowledge about drug-drug interactions (DDIs) is\none of the most important tasks in drug development and medical practice.\nRecently graph neural networks have achieved great success in this task by\nmodeling drugs as nodes and drug-drug interactions as links and casting DDI\npredictions as link prediction problems. However, correlations between link\nlabels (e.g., DDI types) were rarely considered in existing works. We propose\nthe graph energy neural network (GENN) to explicitly model link type\ncorrelations. We formulate the DDI prediction task as a structure prediction\nproblem and introduce a new energy-based model where the energy function is\ndefined by graph neural networks. Experiments on two real-world DDI datasets\ndemonstrated that GENN is superior to many baselines without consideration of\nlink type correlations and achieved $13.77\\%$ and $5.01\\%$ PR-AUC improvement\non the two datasets, respectively. We also present a case study in which \\mname\ncan better capture meaningful DDI correlations compared with baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:03:12 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 02:50:56 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Ma", "Tengfei", ""], ["Shang", "Junyuan", ""], ["Xiao", "Cao", ""], ["Sun", "Jimeng", ""]]}, {"id": "1910.02114", "submitter": "Katherine Kempfert", "authors": "Katherine C. Kempfert, Yishi Wang, Cuixian Chen, and Samuel W.K. Wong", "title": "A Comparison Study on Nonlinear Dimension Reduction Methods with Kernel\n  Variations: Visualization, Optimization and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of high dimensionality, correlation among covariates, and noise\ncontained in data, dimension reduction (DR) techniques are often employed to\nthe application of machine learning algorithms. Principal Component Analysis\n(PCA), Linear Discriminant Analysis (LDA), and their kernel variants (KPCA,\nKLDA) are among the most popular DR methods. Recently, Supervised Kernel\nPrincipal Component Analysis (SKPCA) has been shown as another successful\nalternative. In this paper, brief reviews of these popular techniques are\npresented first. We then conduct a comparative performance study based on three\nsimulated datasets, after which the performance of the techniques are evaluated\nthrough application to a pattern recognition problem in face image analysis.\nThe gender classification problem is considered on MORPH-II and FG-NET, two\npopular longitudinal face aging databases. Several feature extraction methods\nare used, including biologically-inspired features (BIF), local binary patterns\n(LBP), histogram of oriented gradients (HOG), and the Active Appearance Model\n(AAM). After applications of DR methods, a linear support vector machine (SVM)\nis deployed with gender classification accuracy rates exceeding 95% on\nMORPH-II, competitive with benchmark results. A parallel computational approach\nis also proposed, attaining faster processing speeds and similar recognition\nrates on MORPH-II. Our computational approach can be applied to practical\ngender classification systems and generalized to other face analysis tasks,\nsuch as race classification and age prediction.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:33:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kempfert", "Katherine C.", ""], ["Wang", "Yishi", ""], ["Chen", "Cuixian", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1910.02119", "submitter": "Hao Yan", "authors": "Hao Yan, Kamran Paynabar, Jianjun Shi", "title": "AKM$^2$D : An Adaptive Framework for Online Sensing and Anomaly\n  Quantification", "comments": "Under review in IISE Transaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In point-based sensing systems such as coordinate measuring machines (CMM)\nand laser ultrasonics where complete sensing is impractical due to the high\nsensing time and cost, adaptive sensing through a systematic exploration is\nvital for online inspection and anomaly quantification. Most of the existing\nsequential sampling methodologies focus on reducing the overall fitting error\nfor the entire sampling space. However, in many anomaly quantification\napplications, the main goal is to estimate sparse anomalous regions in the\npixel-level accurately. In this paper, we develop a novel framework named\nAdaptive Kernelized Maximum-Minimum Distance AKM$^2$D to speed up the\ninspection and anomaly detection process through an intelligent sequential\nsampling scheme integrated with fast estimation and detection. The proposed\nmethod balances the sampling efforts between the space-filling sampling\n(exploration) and focused sampling near the anomalous region (exploitation).\nThe proposed methodology is validated by conducting simulations and a case\nstudy of anomaly detection in composite sheets using a guided wave test.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:45:58 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yan", "Hao", ""], ["Paynabar", "Kamran", ""], ["Shi", "Jianjun", ""]]}, {"id": "1910.02120", "submitter": "Binhang Yuan", "authors": "Binhang Yuan and Cameron R. Wolfe and Chen Dun and Yuxin Tang and\n  Anastasios Kyrillidis and Christopher M. Jermaine", "title": "Distributed Learning of Deep Neural Networks using Independent Subnet\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning (ML) can bring more computational resources to\nbear than single-machine learning, reducing training time. Further,\ndistribution allows models to be partitioned over many machines, allowing very\nlarge models to be trained -- models that may be much larger than the available\nmemory of any individual machine. However, in practice, distributed ML remains\nchallenging, primarily due to high communication costs. We propose a new\napproach to distributed neural network learning, called independent subnet\ntraining (IST). In IST, a neural network is decomposed into a set of\nsubnetworks of the same depth as the original network, each of which is trained\nlocally, before the various subnets are exchanged and the process is repeated.\nIST training has many advantages over standard data parallel approaches.\nBecause the subsets are independent, communication frequency is reduced.\nBecause the original network is decomposed into independent parts,\ncommunication volume is reduced. Further, the decomposition makes IST naturally\nmodel parallel, and so IST scales to very large models that cannot fit on any\nsingle machine. We show experimentally that IST results in training time that\nare much lower than data parallel approaches to distributed learning, and that\nit scales to large models that cannot be learned using standard approaches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:46:16 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 20:01:11 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 02:17:29 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 23:29:57 GMT"}, {"version": "v5", "created": "Thu, 5 Nov 2020 15:11:47 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yuan", "Binhang", ""], ["Wolfe", "Cameron R.", ""], ["Dun", "Chen", ""], ["Tang", "Yuxin", ""], ["Kyrillidis", "Anastasios", ""], ["Jermaine", "Christopher M.", ""]]}, {"id": "1910.02133", "submitter": "Biswadip Dey", "authors": "Akshay Iyer, Biswadip Dey, Arindam Dasgupta, Wei Chen, Amit\n  Chakraborty", "title": "A Conditional Generative Model for Predicting Material Microstructures\n  from Processing Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microstructures of a material form the bridge linking processing conditions -\nwhich can be controlled, to the material property - which is the primary\ninterest in engineering applications. Thus a critical task in material design\nis establishing the processing-structure relationship, which requires domain\nexpertise and techniques that can model the high-dimensional material\nmicrostructure. This work proposes a deep learning based approach that models\nthe processing-structure relationship as a conditional image synthesis problem.\nIn particular, we develop an auxiliary classifier Wasserstein GAN with gradient\npenalty (ACWGAN-GP) to synthesize microstructures under a given processing\ncondition. This approach is free of feature engineering, requires modest domain\nknowledge and is applicable to a wide range of material systems. We demonstrate\nthis approach using the ultra high carbon steel (UHCS) database, where each\nmicrostructure is annotated with a label describing the cooling method it was\nsubjected to. Our results show that ACWGAN-GP can synthesize high-quality\nmultiphase microstructures for a given cooling method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 20:13:11 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Iyer", "Akshay", ""], ["Dey", "Biswadip", ""], ["Dasgupta", "Arindam", ""], ["Chen", "Wei", ""], ["Chakraborty", "Amit", ""]]}, {"id": "1910.02136", "submitter": "Sumit Mukherjee", "authors": "Anusua Trivedi, Sumit Mukherjee, Edmund Tse, Anne Ewing, Juan Lavista\n  Ferres", "title": "Risks of Using Non-verified Open Data: A case study on using Machine\n  Learning techniques for predicting Pregnancy Outcomes in India", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) has evolved considerably in the last few years.\nWhile applications of AI is now becoming more common in fields like retail and\nmarketing, application of AI in solving problems related to developing\ncountries is still an emerging topic. Specially, AI applications in\nresource-poor settings remains relatively nascent. There is a huge scope of AI\nbeing used in such settings. For example, researchers have started exploring AI\napplications to reduce poverty and deliver a broad range of critical public\nservices. However, despite many promising use cases, there are many dataset\nrelated challenges that one has to overcome in such projects. These challenges\noften take the form of missing data, incorrectly collected data and improperly\nlabeled variables, among other factors. As a result, we can often end up using\ndata that is not representative of the problem we are trying to solve. In this\ncase study, we explore the challenges of using such an open dataset from India,\nto predict an important health outcome. We highlight how the use of AI without\nproper understanding of reporting metrics can lead to erroneous conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 20:27:20 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:21:56 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Trivedi", "Anusua", ""], ["Mukherjee", "Sumit", ""], ["Tse", "Edmund", ""], ["Ewing", "Anne", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "1910.02150", "submitter": "Stefan Klus", "authors": "Stefan Klus, Patrick Gel{\\ss}", "title": "Tensor-based algorithms for image classification", "comments": null, "journal-ref": "Algorithms, 12(11), 240, 2019", "doi": "10.3390/a12110240", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in machine learning with tensor networks has been growing\nrapidly in recent years. We show that tensor-based methods developed for\nlearning the governing equations of dynamical systems from data can, in the\nsame way, be used for supervised learning problems and propose two novel\napproaches for image classification. One is a kernel-based reformulation of the\npreviously introduced MANDy (multidimensional approximation of nonlinear\ndynamics), the other an alternating ridge regression in the tensor-train\nformat. We apply both methods to the MNIST and fashion MNIST data set and show\nthat the approaches are competitive with state-of-the-art neural network-based\nclassifiers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 21:16:33 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 22:31:02 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Klus", "Stefan", ""], ["Gel\u00df", "Patrick", ""]]}, {"id": "1910.02155", "submitter": "Abdallah Chehade", "authors": "Abdallah Chehade, Zunya Shi", "title": "The Sparse Reverse of Principal Component Analysis for Fast Low-Rank\n  Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion constantly receives tremendous attention from many research\nfields. It is commonly applied for recommender systems such as movie ratings,\ncomputer vision such as image reconstruction or completion, multi-task learning\nsuch as collaboratively modeling time-series trends of multiple sensors, and\nmany other applications. Matrix completion techniques are usually\ncomputationally exhaustive and/or fail to capture the heterogeneity in the\ndata. For example, images usually contain a heterogeneous set of objects, and\nthus it is a challenging task to reconstruct images with high levels of missing\ndata. In this paper, we propose the sparse reverse of principal component\nanalysis for matrix completion. The proposed approach maintains smoothness\nacross the matrix, produces accurate estimates of the missing data, converges\niteratively, and it is computationally tractable with a controllable upper\nbound on the number of iterations until convergence. The accuracy of the\nproposed technique is validated on natural images, movie ratings, and\nmultisensor data. It is also compared with common benchmark methods used for\nmatrix completion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 21:44:55 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chehade", "Abdallah", ""], ["Shi", "Zunya", ""]]}, {"id": "1910.02160", "submitter": "Satabdi Saha", "authors": "Satabdi Saha, Duchwan Ryu and Nader Ebrahimi", "title": "Variable Selection with Random Survival Forest and Bayesian Additive\n  Regression Tree for Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we utilize a survival analysis methodology incorporating\nBayesian additive regression trees to account for nonlinear and additive\ncovariate effects. We compare the performance of Bayesian additive regression\ntrees, Cox proportional hazards and random survival forests models for censored\nsurvival data, using simulation studies and survival analysis for breast cancer\nwith U.S. SEER database for the year 2005. In simulation studies, we compare\nthe three models across varying sample sizes and censoring rates on the basis\nof bias and prediction accuracy. In survival analysis for breast cancer, we\nretrospectively analyze a subset of 1500 patients having invasive ductal\ncarcinoma that is a common form of breast cancer mostly affecting older woman.\nPredictive potential of the three models are then compared using some widely\nused performance assessment measures in survival literature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 22:18:17 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 05:38:12 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Saha", "Satabdi", ""], ["Ryu", "Duchwan", ""], ["Ebrahimi", "Nader", ""]]}, {"id": "1910.02170", "submitter": "Evan Rosenman", "authors": "Evan Rosenman and Karthik Rajkumar", "title": "Optimized Partial Identification Bounds for Regression Discontinuity\n  Designs with Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity (RD) design is one of the most popular\nquasi-experimental methods for applied causal inference. In practice, the\nmethod is quite sensitive to the assumption that individuals cannot control\ntheir value of a \"running variable\" that determines treatment status precisely.\nIf individuals are able to precisely manipulate their scores, then point\nidentification is lost. We propose a procedure for obtaining partial\nidentification bounds in the case of a discrete running variable where\nmanipulation is present. Our method relies on two stages: first, we derive the\ndistribution of non-manipulators under several assumptions about the data.\nSecond, we obtain bounds on the causal effect via a sequential convex\nprogramming approach. We also propose methods for tightening the partial\nidentification bounds using an auxiliary covariate, and derive confidence\nintervals via the bootstrap. We demonstrate the utility of our method on a\nsimulated dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 23:32:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rosenman", "Evan", ""], ["Rajkumar", "Karthik", ""]]}, {"id": "1910.02175", "submitter": "Deepta Rajan", "authors": "Deepta Rajan, David Beymer, Shafiqul Abedin and Ehsan Dehghan", "title": "Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely\n  Annotated 3D CT Images", "comments": "2019 NeurIPS ML4H (Proceedings of Machine Learning Research)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary embolisms (PE) are known to be one of the leading causes for\ncardiac-related mortality. Due to inherent variabilities in how PE manifests\nand the cumbersome nature of manual diagnosis, there is growing interest in\nleveraging AI tools for detecting PE. In this paper, we build a two-stage\ndetection pipeline that is accurate, computationally efficient, robust to\nvariations in PE types and kernels used for CT reconstruction, and most\nimportantly, does not require dense annotations. Given the challenges in\nacquiring expert annotations in large-scale datasets, our approach produces\nstate-of-the-art results with very sparse emboli contours (at 10mm slice\nspacing), while using models with significantly lower number of parameters. We\nachieve AUC scores of 0.94 on the validation set and 0.85 on the test set of\nhighly severe PEs. Using a large, real-world dataset characterized by complex\nPE types and patients from multiple hospitals, we present an elaborate\nempirical study and provide guidelines for designing highly generalizable\npipelines.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:01:28 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 16:37:22 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 20:56:02 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Rajan", "Deepta", ""], ["Beymer", "David", ""], ["Abedin", "Shafiqul", ""], ["Dehghan", "Ehsan", ""]]}, {"id": "1910.02176", "submitter": "Pengyu Cheng", "authors": "Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao and\n  Lawrence Carin", "title": "Straight-Through Estimator as Projected Wasserstein Gradient Flow", "comments": "Accepted as NeurIPS 2018 Bayesian Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Straight-Through (ST) estimator is a widely used technique for\nback-propagating gradients through discrete random variables. However, this\neffective method lacks theoretical justification. In this paper, we show that\nST can be interpreted as the simulation of the projected Wasserstein gradient\nflow (pWGF). Based on this understanding, a theoretical foundation is\nestablished to justify the convergence properties of ST. Further, another pWGF\nestimator variant is proposed, which exhibits superior performance on\ndistributions with infinite support,e.g., Poisson distributions. Empirically,\nwe show that ST and our proposed estimator, while applied to different types of\ndiscrete structures (including both Bernoulli and Poisson latent variables),\nexhibit comparable or even better performances relative to other\nstate-of-the-art methods. Our results uncover the origin of the widespread\nadoption of the ST estimator and represent a helpful step towards exploring\nalternative gradient estimators for discrete variables.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:06:34 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Cheng", "Pengyu", ""], ["Liu", "Chang", ""], ["Li", "Chunyuan", ""], ["Shen", "Dinghan", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.02182", "submitter": "Pasha Khosravi", "authors": "Pasha Khosravi, YooJung Choi, Yitao Liang, Antonio Vergari, Guy Van\n  den Broeck", "title": "On Tractable Computation of Expected Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing expected predictions of discriminative models is a fundamental task\nin machine learning that appears in many interesting applications such as\nfairness, handling missing values, and data analysis. Unfortunately, computing\nexpectations of a discriminative model with respect to a probability\ndistribution defined by an arbitrary generative model has been proven to be\nhard in general. In fact, the task is intractable even for simple models such\nas logistic regression and a naive Bayes distribution. In this paper, we\nidentify a pair of generative and discriminative models that enables tractable\ncomputation of expectations, as well as moments of any order, of the latter\nwith respect to the former in case of regression. Specifically, we consider\nexpressive probabilistic circuits with certain structural constraints that\nsupport tractable probabilistic inference. Moreover, we exploit the tractable\ncomputation of high-order moments to derive an algorithm to approximate the\nexpectations for classification scenarios in which exact computations are\nintractable. Our framework to compute expected predictions allows for handling\nof missing data during prediction time in a principled and accurate way and\nenables reasoning about the behavior of discriminative models. We empirically\nshow our algorithm to consistently outperform standard imputation techniques on\na variety of datasets. Finally, we illustrate how our framework can be used for\nexploratory data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:20:41 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 21:47:20 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Khosravi", "Pasha", ""], ["Choi", "YooJung", ""], ["Liang", "Yitao", ""], ["Vergari", "Antonio", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1910.02187", "submitter": "Pengyu Cheng", "authors": "Pengyu Cheng, Yitong Li, Xinyuan Zhang, Liqun Cheng, David Carlson,\n  Lawrence Carin", "title": "Dynamic Embedding on Textual Networks via a Gaussian Process", "comments": "Accepted for presentation at the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual network embedding aims to learn low-dimensional representations of\ntext-annotated nodes in a graph. Prior work in this area has typically focused\non fixed graph structures; however, real-world networks are often dynamic. We\naddress this challenge with a novel end-to-end node-embedding model, called\nDynamic Embedding for Textual Networks with a Gaussian Process (DetGP). After\ntraining, DetGP can be applied efficiently to dynamic graphs without\nre-training or backpropagation. The learned representation of each node is a\ncombination of textual and structural embeddings. Because the structure is\nallowed to be dynamic, our method uses the Gaussian process to take advantage\nof its non-parametric properties. To use both local and global graph\nstructures, diffusion is used to model multiple hops between neighbors. The\nrelative importance of global versus local structure for the embeddings is\nlearned automatically. With the non-parametric nature of the Gaussian process,\nupdating the embeddings for a changed graph structure requires only a forward\npass through the learned model. Considering link prediction and node\nclassification, experiments demonstrate the empirical effectiveness of our\nmethod compared to baseline approaches. We further show that DetGP can be\nstraightforwardly and efficiently applied to dynamic textual networks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 01:16:33 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 20:44:42 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 20:52:01 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cheng", "Pengyu", ""], ["Li", "Yitong", ""], ["Zhang", "Xinyuan", ""], ["Cheng", "Liqun", ""], ["Carlson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.02208", "submitter": "Yanqiu Wu", "authors": "Che Wang, Yanqiu Wu, Quan Vuong, Keith Ross", "title": "Striving for Simplicity and Performance in Off-Policy DRL: Output\n  Normalization and Non-Uniform Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to develop off-policy DRL algorithms that not only exceed\nstate-of-the-art performance but are also simple and minimalistic. For standard\ncontinuous control benchmarks, Soft Actor-Critic (SAC), which employs entropy\nmaximization, currently provides state-of-the-art performance. We first\ndemonstrate that the entropy term in SAC addresses action saturation due to the\nbounded nature of the action spaces, with this insight, we propose a\nstreamlined algorithm with a simple normalization scheme or with inverted\ngradients. We show that both approaches can match SAC's sample efficiency\nperformance without the need of entropy maximization, we then propose a simple\nnon-uniform sampling method for selecting transitions from the replay buffer\nduring training. Extensive experimental results demonstrate that our proposed\nsampling scheme leads to state of the art sample efficiency on challenging\ncontinuous control tasks. We combine all of our findings into one simple\nalgorithm, which we call Streamlined Off Policy with Emphasizing Recent\nExperience, for which we provide robust public-domain code.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 04:22:35 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 14:02:08 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 15:22:40 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2020 09:02:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Che", ""], ["Wu", "Yanqiu", ""], ["Vuong", "Quan", ""], ["Ross", "Keith", ""]]}, {"id": "1910.02217", "submitter": "Hari Prasanna Das", "authors": "Hari Prasanna Das, Ioannis C. Konstantakopoulos, Aummul Baneen\n  Manasawala, Tanya Veeravalli, Huihan Liu and Costas J. Spanos", "title": "A Novel Graphical Lasso based approach towards Segmentation Analysis in\n  Energy Game-Theoretic Frameworks", "comments": "Proceedings of the Special Session on Machine Learning in Energy\n  Application, International Conference on Machine Learning and Applications\n  (ICMLA) 2019. arXiv admin note: text overlap with arXiv:1810.10533", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy game-theoretic frameworks have emerged to be a successful strategy to\nencourage energy efficient behavior in large scale by leveraging\nhuman-in-the-loop strategy. A number of such frameworks have been introduced\nover the years which formulate the energy saving process as a competitive game\nwith appropriate incentives for energy efficient players. However, prior works\ninvolve an incentive design mechanism which is dependent on knowledge of\nutility functions for all the players in the game, which is hard to compute\nespecially when the number of players is high, common in energy game-theoretic\nframeworks. Our research proposes that the utilities of players in such a\nframework can be grouped together to a relatively small number of clusters, and\nthe clusters can then be targeted with tailored incentives. The key to above\nsegmentation analysis is to learn the features leading to human decision making\ntowards energy usage in competitive environments. We propose a novel graphical\nlasso based approach to perform such segmentation, by studying the feature\ncorrelations in a real-world energy social game dataset. To further improve the\nexplainability of the model, we perform causality study using grangers\ncausality. Proposed segmentation analysis results in characteristic clusters\ndemonstrating different energy usage behaviors. We also present avenues to\nimplement intelligent incentive design using proposed segmentation method.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 06:04:40 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Das", "Hari Prasanna", ""], ["Konstantakopoulos", "Ioannis C.", ""], ["Manasawala", "Aummul Baneen", ""], ["Veeravalli", "Tanya", ""], ["Liu", "Huihan", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1910.02249", "submitter": "Bingzhe Wu", "authors": "Bingzhe Wu, Chaochao Chen, Shiwan Zhao, Cen Chen, Yuan Yao, Guangyu\n  Sun, Li Wang, Xiaolu Zhang, Jun Zhou", "title": "Characterizing Membership Privacy in Stochastic Gradient Langevin\n  Dynamics", "comments": "Under review of AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian deep learning is recently regarded as an intrinsic way to\ncharacterize the weight uncertainty of deep neural networks~(DNNs). Stochastic\nGradient Langevin Dynamics~(SGLD) is an effective method to enable Bayesian\ndeep learning on large-scale datasets. Previous theoretical studies have shown\nvarious appealing properties of SGLD, ranging from the convergence properties\nto the generalization bounds. In this paper, we study the properties of SGLD\nfrom a novel perspective of membership privacy protection (i.e., preventing the\nmembership attack). The membership attack, which aims to determine whether a\nspecific sample is used for training a given DNN model, has emerged as a common\nthreat against deep learning algorithms. To this end, we build a theoretical\nframework to analyze the information leakage (w.r.t. the training dataset) of a\nmodel trained using SGLD. Based on this framework, we demonstrate that SGLD can\nprevent the information leakage of the training dataset to a certain extent.\nMoreover, our theoretical analysis can be naturally extended to other types of\nStochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical\nresults on different datasets and models verify our theoretical findings and\nsuggest that the SGLD algorithm can not only reduce the information leakage but\nalso improve the generalization ability of the DNN models in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 11:26:54 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wu", "Bingzhe", ""], ["Chen", "Chaochao", ""], ["Zhao", "Shiwan", ""], ["Chen", "Cen", ""], ["Yao", "Yuan", ""], ["Sun", "Guangyu", ""], ["Wang", "Li", ""], ["Zhang", "Xiaolu", ""], ["Zhou", "Jun", ""]]}, {"id": "1910.02290", "submitter": "Anna Kruspe", "authors": "Anna Kruspe", "title": "Few-shot tweet detection in emerging disaster events", "comments": "Accepted to AI+HADR workshop @ NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media sources can provide crucial information in crisis situations,\nbut discovering relevant messages is not trivial. Methods have so far focused\non universal detection models for all kinds of crises or for certain crisis\ntypes (e.g. floods). Event-specific models could implement a more focused\nsearch area, but collecting data and training new models for a crisis that is\nalready in progress is costly and may take too much time for a prompt response.\nAs a compromise, manually collecting a small amount of example messages is\nfeasible. Few-shot models can generalize to unseen classes with such a small\nhandful of examples, and do not need be trained anew for each event. We compare\nhow few-shot approaches (matching networks and prototypical networks) perform\nfor this task. Since this is essentially a one-class problem, we also\ndemonstrate how a modified one-class version of prototypical models can be used\nfor this application.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 16:25:56 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kruspe", "Anna", ""]]}, {"id": "1910.02301", "submitter": "Isuru Hewapathirana", "authors": "Isuru Udayangani Hewapathirana, Dominic Lee, Elena Moltchanova and\n  Jeanette McLeod", "title": "Change Detection in Noisy Dynamic Networks: A Spectral Embedding\n  Approach", "comments": "44 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in dynamic networks is an important problem in many areas,\nsuch as fraud detection, cyber intrusion detection and health care monitoring.\nIt is a challenging problem because it involves a time sequence of graphs, each\nof which is usually very large and sparse with heterogeneous vertex degrees,\nresulting in a complex, high dimensional mathematical object. Spectral\nembedding methods provide an effective way to transform a graph to a lower\ndimensional latent Euclidean space that preserves the underlying structure of\nthe network. Although change detection methods that use spectral embedding are\navailable, they do not address sparsity and degree heterogeneity that usually\noccur in noisy real-world graphs and a majority of these methods focus on\nchanges in the behaviour of the overall network.\n  In this paper, we adapt previously developed techniques in spectral graph\ntheory and propose a novel concept of applying Procrustes techniques to\nembedded points for vertices in a graph to detect changes in entity behaviour.\nOur spectral embedding approach not only addresses sparsity and degree\nheterogeneity issues, but also obtains an estimate of the appropriate embedding\ndimension. We call this method CDP (change detection using Procrustes\nanalysis). We demonstrate the performance of CDP through extensive simulation\nexperiments and a real-world application. CDP successfully detects various\ntypes of vertex-based changes including (i) changes in vertex degree, (ii)\nchanges in community membership of vertices, and (iii) unusual increase or\ndecrease in edge weight between vertices. The change detection performance of\nCDP is compared with two other baseline methods that employ alternative\nspectral embedding approaches. In both cases, CDP generally shows superior\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 18:02:18 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hewapathirana", "Isuru Udayangani", ""], ["Lee", "Dominic", ""], ["Moltchanova", "Elena", ""], ["McLeod", "Jeanette", ""]]}, {"id": "1910.02304", "submitter": "Nazreen P M", "authors": "Nazreen P.M., Shantanu Chakrabartty and Chetan Singh Thakur", "title": "Multiplierless and Sparse Machine Learning based on Margin Propagation\n  Networks", "comments": "New results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new generation of machine learning processors have evolved from\nmulti-core and parallel architectures that were designed to efficiently\nimplement matrix-vector-multiplications (MVMs). This is because at the\nfundamental level, neural network and machine learning operations extensively\nuse MVM operations and hardware compilers exploit the inherent parallelism in\nMVM operations to achieve hardware acceleration on GPUs and FPGAs. However,\nmany IoT and edge computing platforms require embedded ML devices close to the\nnetwork in order to compensate for communication cost and latency. Hence a\nnatural question to ask is whether MVM operations are even necessary to\nimplement ML algorithms and whether simpler hardware primitives can be used to\nimplement an ultra-energy-efficient ML processor/architecture. In this paper we\npropose an alternate hardware-software codesign of ML and neural network\narchitectures where instead of using MVM operations and non-linear activation\nfunctions, the architecture only uses simple addition and thresholding\noperations to implement inference and learning. At the core of the proposed\napproach is margin-propagation (MP) based computation that maps multiplications\ninto additions and additions into a dynamic rectifying-linear-unit (ReLU)\noperations. This mapping results in significant improvement in computational\nand hence energy cost. In this paper, we show how the MP network formulation\ncan be applied for designing linear classifiers, shallow multi-layer\nperceptrons and support vector networks suitable fot IoT platforms and tiny ML\napplications. We show that these MP based classifiers give comparable results\nto that of their traditional counterparts for benchmark UCI datasets, with the\nadded advantage of reduction in computational complexity enabling an\nimprovement in energy efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 18:09:57 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 12:43:48 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["M.", "Nazreen P.", ""], ["Chakrabartty", "Shantanu", ""], ["Thakur", "Chetan Singh", ""]]}, {"id": "1910.02321", "submitter": "Nuno Louren\\c{c}o", "authors": "In\\^es Valentim, Nuno Louren\\c{c}o, Nuno Antunes", "title": "The Impact of Data Preparation on the Fairness of Software Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are widely adopted in scenarios that directly affect\npeople. The development of software systems based on these models raises\nsocietal and legal concerns, as their decisions may lead to the unfair\ntreatment of individuals based on attributes like race or gender. Data\npreparation is key in any machine learning pipeline, but its effect on fairness\nis yet to be studied in detail. In this paper, we evaluate how the fairness and\neffectiveness of the learned models are affected by the removal of the\nsensitive attribute, the encoding of the categorical attributes, and instance\nselection methods (including cross-validators and random undersampling). We\nused the Adult Income and the German Credit Data datasets, which are widely\nstudied and known to have fairness concerns. We applied each data preparation\ntechnique individually to analyse the difference in predictive performance and\nfairness, using statistical parity difference, disparate impact, and the\nnormalised prejudice index. The results show that fairness is affected by\ntransformations made to the training data, particularly in imbalanced datasets.\nRemoving the sensitive attribute is insufficient to eliminate all the\nunfairness in the predictions, as expected, but it is key to achieve fairer\nmodels. Additionally, the standard random undersampling with respect to the\ntrue labels is sometimes more prejudicial than performing no random\nundersampling.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 19:50:16 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Valentim", "In\u00eas", ""], ["Louren\u00e7o", "Nuno", ""], ["Antunes", "Nuno", ""]]}, {"id": "1910.02330", "submitter": "Adish Singla", "authors": "Ahana Ghosh, Sebastian Tschiatschek, Hamed Mahdavi, Adish Singla", "title": "Towards Deployment of Robust AI Agents for Human-Machine Partnerships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of designing AI agents that can robustly cooperate with\npeople in human-machine partnerships. Our work is inspired by real-life\nscenarios in which an AI agent, e.g., a virtual assistant, has to cooperate\nwith new users after its deployment. We model this problem via a parametric MDP\nframework where the parameters correspond to a user's type and characterize her\nbehavior. In the test phase, the AI agent has to interact with a user of\nunknown type. Our approach to designing a robust AI agent relies on observing\nthe user's actions to make inferences about the user's type and adapting its\npolicy to facilitate efficient cooperation. We show that without being\nadaptive, an AI agent can end up performing arbitrarily bad in the test phase.\nWe develop two algorithms for computing policies that automatically adapt to\nthe user in the test phase. We demonstrate the effectiveness of our approach in\nsolving a two-agent collaborative task.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 21:04:27 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:19:40 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Ghosh", "Ahana", ""], ["Tschiatschek", "Sebastian", ""], ["Mahdavi", "Hamed", ""], ["Singla", "Adish", ""]]}, {"id": "1910.02332", "submitter": "Mhd Wesam Al-Nabki", "authors": "Mhd Wesam Al-Nabki, Eduardo Fidalgo, Enrique Alegre, and Deisy Chaves", "title": "Content-Based Features to Rank Influential Hidden Services of the Tor\n  Darknet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unevenness importance of criminal activities in the onion domains of the\nTor Darknet and the different levels of their appeal to the end-user make them\ntangled to measure their influence. To this end, this paper presents a novel\ncontent-based ranking framework to detect the most influential onion domains.\nOur approach comprises a modeling unit that represents an onion domain using\nforty features extracted from five different resources: user-visible text, HTML\nmarkup, Named Entities, network topology, and visual content. And also, a\nranking unit that, using the Learning-to-Rank (LtR) approach, automatically\nlearns a ranking function by integrating the previously obtained features.\nUsing a case-study based on drugs-related onion domains, we obtained the\nfollowing results. (1) Among the explored LtR schemes, the listwise approach\noutperforms the benchmarked methods with an NDCG of 0.95 for the top-10 ranked\ndomains. (2) We proved quantitatively that our framework surpasses the\nlink-based ranking techniques. Also, (3) with the selected feature, we observed\nthat the textual content, composed by text, NER, and HTML features, is the most\nbalanced approach, in terms of efficiency and score obtained. The proposed\nframework might support Law Enforcement Agencies in detecting the most\ninfluential domains related to possible suspicious activities.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 21:39:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Fidalgo", "Eduardo", ""], ["Alegre", "Enrique", ""], ["Chaves", "Deisy", ""]]}, {"id": "1910.02333", "submitter": "Rahul Parhi", "authors": "Rahul Parhi and Robert D. Nowak", "title": "The Role of Neural Network Activation Functions", "comments": "update to published version", "journal-ref": "IEEE Signal Processing Letters, vol. 27, pp. 1779-1783, 2020", "doi": "10.1109/LSP.2020.3027517", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of activation functions have been proposed for neural\nnetworks. The Rectified Linear Unit (ReLU) is especially popular today. There\nare many practical reasons that motivate the use of the ReLU. This paper\nprovides new theoretical characterizations that support the use of the ReLU,\nits variants such as the leaky ReLU, as well as other activation functions in\nthe case of univariate, single-hidden layer feedforward neural networks. Our\nresults also explain the importance of commonly used strategies in the design\nand training of neural networks such as \"weight decay\" and \"path-norm\"\nregularization, and provide a new justification for the use of \"skip\nconnections\" in network architectures. These new insights are obtained through\nthe lens of spline theory. In particular, we show how neural network training\nproblems are related to infinite-dimensional optimizations posed over Banach\nspaces of functions whose solutions are well-known to be fractional and\npolynomial splines, where the particular Banach space (which controls the order\nof the spline) depends on the choice of activation function.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 21:57:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 17:26:37 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 14:37:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Parhi", "Rahul", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1910.02338", "submitter": "Amirhossein Taghvaei", "authors": "Amirhossein Taghvaei, Prashant G. Mehta", "title": "An Optimal Transport Formulation of the Ensemble Kalman Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled interacting particle systems such as the ensemble Kalman filter\n(EnKF) and the feedback particle filter (FPF) are numerical algorithms to\napproximate the solution of the nonlinear filtering problem in continuous time.\nThe distinguishing feature of these algorithms is that the Bayesian update step\nis implemented using a feedback control law. It has been noted in the\nliterature that the control law is not unique. This is the main problem\naddressed in this paper. To obtain a unique control law, the filtering problem\nis formulated here as an optimal transportation problem. An explicit formula\nfor the (mean-field type) optimal control law is derived in the linear Gaussian\nsetting. Comparisons are made with the control laws for different types of EnKF\nalgorithms described in the literature. Via empirical approximation of the\nmean-field control law, a finite-$N$ controlled interacting particle algorithm\nis obtained. For this algorithm, the equations for empirical mean and\ncovariance are derived and shown to be identical to the Kalman filter. This\nallows strong conclusions on convergence and error properties based on the\nclassical filter stability theory for the Kalman filter. It is shown that,\nunder certain technical conditions, the mean squared error (m.s.e.) converges\nto zero even with a finite number of particles. A detailed propagation of chaos\nanalysis is carried out for the finite-$N$ algorithm. The analysis is used to\nprove weak convergence of the empirical distribution as $N\\rightarrow\\infty$.\nFor a certain simplified filtering problem, analytical comparison of the m.s.e.\nwith the importance sampling-based algorithms is described. The analysis helps\nexplain the favorable scaling properties of the control-based algorithms\nreported in several numerical studies in recent literature.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 22:55:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Taghvaei", "Amirhossein", ""], ["Mehta", "Prashant G.", ""]]}, {"id": "1910.02342", "submitter": "Keith Dillon", "authors": "Keith Dillon", "title": "Clustering Gaussian Graphical Models", "comments": "arXiv admin note: text overlap with arXiv:1903.07181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an efficient method to perform clustering of nodes in Gaussian\ngraphical models directly from sample data. Nodes are clustered based on the\nsimilarity of their network neighborhoods, with edge weights defined by partial\ncorrelations. In the limited-data scenario, where the covariance matrix would\nbe rank-deficient, we are able to make use of matrix factors, and never need to\nestimate the actual covariance or precision matrix. We demonstrate the method\non functional MRI data from the Human Connectome Project. A matlab\nimplementation of the algorithm is provided.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 23:48:05 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dillon", "Keith", ""]]}, {"id": "1910.02344", "submitter": "Jae Hyun Lim", "authors": "Jae Hyun Lim, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher Pal,\n  Sungjin Ahn", "title": "Neural Multisensory Scene Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For embodied agents to infer representations of the underlying 3D physical\nworld they inhabit, they should efficiently combine multisensory cues from\nnumerous trials, e.g., by looking at and touching objects. Despite its\nimportance, multisensory 3D scene representation learning has received less\nattention compared to the unimodal setting. In this paper, we propose the\nGenerative Multisensory Network (GMN) for learning latent representations of 3D\nscenes which are partially observable through multiple sensory modalities. We\nalso introduce a novel method, called the Amortized Product-of-Experts, to\nimprove the computational efficiency and the robustness to unseen combinations\nof modalities at test time. Experimental results demonstrate that the proposed\nmodel can efficiently infer robust modality-invariant 3D-scene representations\nfrom arbitrary combinations of modalities and perform accurate cross-modal\ngeneration. To perform this exploration, we also develop the Multisensory\nEmbodied 3D-Scene Environment (MESE).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 00:14:38 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:07:33 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Lim", "Jae Hyun", ""], ["Pinheiro", "Pedro O.", ""], ["Rostamzadeh", "Negar", ""], ["Pal", "Christopher", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1910.02352", "submitter": "Zenan Li", "authors": "Zenan Li, Xiaoxing Ma, Chang Xu, Jingwei Xu, Chun Cao and Jian L\\\"u", "title": "Operational Calibration: Debugging Confidence Errors for DNNs in the\n  Field", "comments": "Published in the Proceedings of the 28th ACM Joint European Software\n  Engineering Conference and Symposium on the Foundations of Software\n  Engineering (ESEC/FSE 2020)", "journal-ref": null, "doi": "10.1145/3368089.3409696", "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained DNN models are increasingly adopted as integral parts of software\nsystems, but they often perform deficiently in the field. A particularly\ndamaging problem is that DNN models often give false predictions with high\nconfidence, due to the unavoidable slight divergences between operation data\nand training data. To minimize the loss caused by inaccurate confidence,\noperational calibration, i.e., calibrating the confidence function of a DNN\nclassifier against its operation domain, becomes a necessary debugging step in\nthe engineering of the whole system.\n  Operational calibration is difficult considering the limited budget of\nlabeling operation data and the weak interpretability of DNN models. We propose\na Bayesian approach to operational calibration that gradually corrects the\nconfidence given by the model under calibration with a small number of labeled\noperation data deliberately selected from a larger set of unlabeled operation\ndata. The approach is made effective and efficient by leveraging the locality\nof the learned representation of the DNN model and modeling the calibration as\nGaussian Process Regression. Comprehensive experiments with various practical\ndatasets and DNN models show that it significantly outperformed alternative\nmethods, and in some difficult tasks it eliminated about 71% to 97%\nhigh-confidence (>0.9) errors with only about 10\\% of the minimal amount of\nlabeled operation data needed for practical learning techniques to barely work.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 01:21:14 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 02:28:32 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Zenan", ""], ["Ma", "Xiaoxing", ""], ["Xu", "Chang", ""], ["Xu", "Jingwei", ""], ["Cao", "Chun", ""], ["L\u00fc", "Jian", ""]]}, {"id": "1910.02366", "submitter": "Dilin Wang", "authors": "Qiang Liu, Lemeng Wu, Dilin Wang", "title": "Splitting Steepest Descent for Growing Neural Architectures", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a progressive training approach for neural networks which\nadaptively grows the network structure by splitting existing neurons to\nmultiple off-springs. By leveraging a functional steepest descent idea, we\nderive a simple criterion for deciding the best subset of neurons to split and\na splitting gradient for optimally updating the off-springs. Theoretically, our\nsplitting strategy is a second-order functional steepest descent for escaping\nsaddle points in an $\\infty$-Wasserstein metric space, on which the standard\nparametric gradient descent is a first-order steepest descent. Our method\nprovides a new computationally efficient approach for optimizing neural network\nstructures, especially for learning lightweight neural architectures in\nresource-constrained settings.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 04:15:23 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 17:17:16 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 22:25:12 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liu", "Qiang", ""], ["Wu", "Lemeng", ""], ["Wang", "Dilin", ""]]}, {"id": "1910.02370", "submitter": "Chenhui Deng", "authors": "Chenhui Deng, Zhiqiang Zhao, Yongyu Wang, Zhiru Zhang, Zhuo Feng", "title": "GraphZoom: A multi-level spectral approach for accurate and scalable\n  graph embedding", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": "International Conference on Learning Representations, ICLR 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 04:43:46 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 18:35:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Deng", "Chenhui", ""], ["Zhao", "Zhiqiang", ""], ["Wang", "Yongyu", ""], ["Zhang", "Zhiru", ""], ["Feng", "Zhuo", ""]]}, {"id": "1910.02373", "submitter": "Sifan Liu", "authors": "Sifan Liu, Edgar Dobriban", "title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following three fundamental problems about ridge regression: (1)\nwhat is the structure of the estimator? (2) how to correctly use\ncross-validation to choose the regularization parameter? and (3) how to\naccelerate computation without losing too much accuracy? We consider the three\nproblems in a unified large-data linear model. We give a precise representation\nof ridge regression as a covariance matrix-dependent linear combination of the\ntrue parameter and the noise. We study the bias of $K$-fold cross-validation\nfor choosing the regularization parameter, and propose a simple\nbias-correction. We analyze the accuracy of primal and dual sketching for ridge\nregression, showing they are surprisingly accurate. Our results are illustrated\nby simulations and by analyzing empirical data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:00:40 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 18:12:43 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 04:14:36 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liu", "Sifan", ""], ["Dobriban", "Edgar", ""]]}, {"id": "1910.02380", "submitter": "Gregory Dobler", "authors": "Gregory Dobler, Jordan Vani, Trang Tran Linh Dam", "title": "Patterns of Urban Foot Traffic Dynamics", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using publicly available traffic camera data in New York City, we quantify\ntime-dependent patterns in aggregate pedestrian foot traffic. These patterns\nexhibit repeatable diurnal behaviors that differ for weekdays and weekends but\nare broadly consistent across neighborhoods in the borough of Manhattan.\nWeekday patterns contain a characteristic 3-peak structure with increased foot\ntraffic around 9:00am, 12:00-1:00pm, and 5:00pm aligned with the \"9-to-5\" work\nday in which pedestrians are on the street during their morning commute, during\nlunch hour, and then during their evening commute. Weekend days do not show a\npeaked structure, but rather increase steadily until sunset. Our study period\nof June 28, 2017 to September 11, 2017 contains two holidays, the 4th of July\nand Labor Day, and their foot traffic patterns are quantitatively similar to\nweekend days despite the fact that they fell on weekdays. Projecting all days\nin our study period onto the weekday/weekend phase space (by regressing against\nthe average weekday and weekend day) we find that Friday foot traffic can be\nrepresented as a mixture of both the 3-peak weekday structure and non-peaked\nweekend structure. We also show that anomalies in the foot traffic patterns can\nbe used for detection of events and network-level disruptions. Finally, we show\nthat clustering of foot traffic time series generates associations between\ncameras that are spatially aligned with Manhattan neighborhood boundaries\nindicating that foot traffic dynamics encode information about neighborhood\ncharacter.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:39:57 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dobler", "Gregory", ""], ["Vani", "Jordan", ""], ["Dam", "Trang Tran Linh", ""]]}, {"id": "1910.02384", "submitter": "Jindong Jiang", "authors": "Jindong Jiang, Sepehr Janghorbani, Gerard de Melo, Sungjin Ahn", "title": "SCALOR: Generative World Models with Scalable Object Representations", "comments": "First two authors contributed equally. Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability in terms of object density in a scene is a primary challenge in\nunsupervised sequential object-oriented representation learning. Most of the\nprevious models have been shown to work only on scenes with a few objects. In\nthis paper, we propose SCALOR, a probabilistic generative world model for\nlearning SCALable Object-oriented Representation of a video. With the proposed\nspatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal\nwith orders of magnitude larger numbers of objects compared to the previous\nstate-of-the-art models. Additionally, we introduce a background module that\nallows SCALOR to model complex dynamic backgrounds as well as many foreground\nobjects in the scene. We demonstrate that SCALOR can deal with crowded scenes\ncontaining up to a hundred objects while jointly modeling complex dynamic\nbackgrounds. Importantly, SCALOR is the first unsupervised object\nrepresentation model shown to work for natural scenes containing several tens\nof moving objects.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 06:26:31 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 16:56:45 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 05:43:36 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 19:09:24 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Jiang", "Jindong", ""], ["Janghorbani", "Sepehr", ""], ["de Melo", "Gerard", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1910.02390", "submitter": "Amber Nigam", "authors": "Amber Nigam, Pragati Jaiswal, Uma Girkar, Teertha Arora, and Leo A.\n  Celi", "title": "Migration through Machine Learning Lens -- Predicting Sexual and\n  Reproductive Health Vulnerability of Young Migrants", "comments": "Accepted for Machine Learning for Health (ML4H) at NeurIPS 2019 -\n  Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have discussed initial findings and results of our\nexperiment to predict sexual and reproductive health vulnerabilities of\nmigrants in a data-constrained environment. Notwithstanding the limited\nresearch and data about migrants and migration cities, we propose a solution\nthat simultaneously focuses on data gathering from migrants, augmenting\nawareness of the migrants to reduce mishaps, and setting up a mechanism to\npresent insights to the key stakeholders in migration to act upon. We have\ndesigned a webapp for the stakeholders involved in migration: migrants, who\nwould participate in data gathering process and can also use the app for\ngetting to know safety and awareness tips based on analysis of the data\nreceived; public health workers, who would have an access to the database of\nmigrants on the app; policy makers, who would have a greater understanding of\nthe ground reality, and of the patterns of migration through machine-learned\nanalysis. Finally, we have experimented with different machine learning models\non an artificially curated dataset. We have shown, through experiments, how\nmachine learning can assist in predicting the migrants at risk and can also\nhelp in identifying the critical factors that make migration dangerous for\nmigrants. The results for identifying vulnerable migrants through machine\nlearning algorithms are statistically significant at an alpha of 0.05.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 07:09:13 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 03:56:45 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 20:14:39 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 10:00:02 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nigam", "Amber", ""], ["Jaiswal", "Pragati", ""], ["Girkar", "Uma", ""], ["Arora", "Teertha", ""], ["Celi", "Leo A.", ""]]}, {"id": "1910.02420", "submitter": "Essam Rashed", "authors": "Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata", "title": "Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": "10.1109/TMI.2020.2969682", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromagnetic stimulation of the human brain is a key tool for the\nneurophysiological characterization and diagnosis of several neurological\ndisorders. Transcranial magnetic stimulation (TMS) is one procedure that is\ncommonly used clinically. However, personalized TMS requires a pipeline for\naccurate head model generation to provide target-specific stimulation. This\nprocess includes intensive segmentation of several head tissues based on\nmagnetic resonance imaging (MRI), which has significant potential for\nsegmentation error, especially for low-contrast tissues. Additionally, a\nuniform electrical conductivity is assigned to each tissue in the model, which\nis an unrealistic assumption based on conventional volume conductor modeling.\nThis paper proposes a novel approach to the automatic estimation of electric\nconductivity in the human head for volume conductor models without anatomical\nsegmentation. A convolutional neural network is designed to estimate\npersonalized electrical conductivity values based on anatomical information\nobtained from T1- and T2-weighted MRI scans. This approach can avoid the\ntime-consuming process of tissue segmentation and maximize the advantages of\nposition-dependent conductivity assignment based on water content values\nestimated from MRI intensity values. The computational results of the proposed\napproach provide similar but smoother electric field results for the brain when\ncompared to conventional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:33:13 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 06:00:57 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Rashed", "Essam A.", ""], ["Gomez-Tames", "Jose", ""], ["Hirata", "Akimasa", ""]]}, {"id": "1910.02421", "submitter": "Nimrod Segol", "authors": "Nimrod Segol, Yaron Lipman", "title": "On Universal Equivariant Set Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep neural networks that are either invariant or equivariant to\npermutations in order to learn functions on unordered sets has become\nprevalent. The most popular, basic models are DeepSets [Zaheer et al. 2017] and\nPointNet [Qi et al. 2017]. While known to be universal for approximating\ninvariant functions, DeepSets and PointNet are not known to be universal when\napproximating \\emph{equivariant} set functions. On the other hand, several\nrecent equivariant set architectures have been proven equivariant universal\n[Sannai et al. 2019], [Keriven et al. 2019], however these models either use\nlayers that are not permutation equivariant (in the standard sense) and/or use\nhigher order tensor variables which are less practical.\n  There is, therefore, a gap in understanding the universality of popular\nequivariant set models versus theoretical ones.\n  In this paper we close this gap by proving that: (i) PointNet is not\nequivariant universal; and (ii) adding a single linear transmission layer makes\nPointNet universal. We call this architecture PointNetST and argue it is the\nsimplest permutation equivariant universal model known to date. Another\nconsequence is that DeepSets is universal, and also PointNetSeg, a popular\npoint cloud segmentation network (used eg, in [Qi et al. 2017]) is universal.\n  The key theoretical tool used to prove the above results is an explicit\ncharacterization of all permutation equivariant polynomial layers. Lastly, we\nprovide numerical experiments validating the theoretical results and comparing\ndifferent permutation equivariant models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:37:56 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 20:15:00 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Segol", "Nimrod", ""], ["Lipman", "Yaron", ""]]}, {"id": "1910.02423", "submitter": "Harikrishnan Nellippallil Balakrishnan", "authors": "Harikrishnan Nellippallil Balakrishnan, Aditi Kathpalia, Snehanshu\n  Saha, Nithin Nagaraj", "title": "ChaosNet: A Chaos based Artificial Neural Network Architecture for\n  Classification", "comments": "27 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by chaotic firing of neurons in the brain, we propose ChaosNet -- a\nnovel chaos based artificial neural network architecture for classification\ntasks. ChaosNet is built using layers of neurons, each of which is a 1D chaotic\nmap known as the Generalized Luroth Series (GLS) which has been shown in\nearlier works to possess very useful properties for compression, cryptography\nand for computing XOR and other logical operations. In this work, we design a\nnovel learning algorithm on ChaosNet that exploits the topological transitivity\nproperty of the chaotic GLS neurons. The proposed learning algorithm gives\nconsistently good performance accuracy in a number of classification tasks on\nwell known publicly available datasets with very limited training samples. Even\nwith as low as 7 (or fewer) training samples/class (which accounts for less\nthan 0.05% of the total available data), ChaosNet yields performance accuracies\nin the range 73.89 % - 98.33 %. We demonstrate the robustness of ChaosNet to\nadditive parameter noise and also provide an example implementation of a\n2-layer ChaosNet for enhancing classification accuracy. We envisage the\ndevelopment of several other novel learning algorithms on ChaosNet in the near\nfuture.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:40:40 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Balakrishnan", "Harikrishnan Nellippallil", ""], ["Kathpalia", "Aditi", ""], ["Saha", "Snehanshu", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1910.02425", "submitter": "Jannik Kossen", "authors": "Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian\n  Kersting", "title": "Structured Object-Aware Physics Prediction for Video Modeling and\n  Planning", "comments": "Published as a conference paper at 2020 International Conference for\n  Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans observe a physical system, they can easily locate objects,\nunderstand their interactions, and anticipate future behavior, even in settings\nwith complicated and previously unseen interactions. For computers, however,\nlearning such models from videos in an unsupervised fashion is an unsolved\nresearch problem. In this paper, we present STOVE, a novel state-space model\nfor videos, which explicitly reasons about objects and their positions,\nvelocities, and interactions. It is constructed by combining an image model and\na dynamics model in compositional manner and improves on previous work by\nreusing the dynamics model for inference, accelerating and regularizing\ntraining. STOVE predicts videos with convincing physical behavior over hundreds\nof timesteps, outperforms previous unsupervised models, and even approaches the\nperformance of supervised baselines. We further demonstrate the strength of our\nmodel as a simulator for sample efficient model-based control in a task with\nheavily interacting objects.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:48:26 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:38:20 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Kossen", "Jannik", ""], ["Stelzner", "Karl", ""], ["Hussing", "Marcel", ""], ["Voelcker", "Claas", ""], ["Kersting", "Kristian", ""]]}, {"id": "1910.02433", "submitter": "Mimi Zhang Dr", "authors": "Mimi Zhang", "title": "Weighted Clustering Ensemble: A Review", "comments": null, "journal-ref": "Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering ensemble has emerged as a powerful tool for improving both the\nrobustness and the stability of results from individual clustering methods.\nWeighted clustering ensemble arises naturally from clustering ensemble. One of\nthe arguments for weighted clustering ensemble is that elements (clusterings or\nclusters) in a clustering ensemble are of different quality, or that objects or\nfeatures are of varying significance. However, it is not possible to directly\napply the weighting mechanisms from classification (supervised) domain to\nclustering (unsupervised) domain, also because clustering is inherently an\nill-posed problem. This paper provides an overview of weighted clustering\nensemble by discussing different types of weights, major approaches to\ndetermining weight values, and applications of weighted clustering ensemble to\ncomplex data. The unifying framework presented in this paper will help\nclustering practitioners select the most appropriate weighting mechanisms for\ntheir own problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:16:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhang", "Mimi", ""]]}, {"id": "1910.02450", "submitter": "Dan Wang", "authors": "Hekai Zhang, Jibing Gong, Zhiyong Teng, Dan Wang, Hongfei Wang,\n  Linfeng Du and Zakirul Alam Bhuiyan", "title": "Mobile APP User Attribute Prediction by Heterogeneous Information\n  Network Modeling", "comments": "10 pages,3 figures,International Conference on Dependability in\n  Sensor, Cloud, and Big Data Systems and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-based attribute information, such as age and gender, is usually\nconsidered as user privacy information. It is difficult for enterprises to\nobtain user-based privacy attribute information. However, user-based privacy\nattribute information has a wide range of applications in personalized\nservices, user behavior analysis and other aspects. this paper advances the\nHetPathMine model and puts forward TPathMine model. With applying the number of\nclicks of attributes under each node to express the user's emotional preference\ninformation, optimizations of the solution of meta-path weight are also\npresented. Based on meta-path in heterogeneous information networks, the new\nmodel integrates all relationships among objects into isomorphic relationships\nof classified objects. Matrix is used to realize the knowledge dissemination of\ncategory knowledge among isomorphic objects. The experimental results show\nthat: (1) the prediction of user attributes based on heterogeneous information\nnetworks can achieve higher accuracy than traditional machine learning\nclassification methods; (2) TPathMine model based on the number of clicks is\nmore accurate in classifying users of different age groups, and the weight of\neach meta-path is consistent with human intuition or the real world situation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 13:41:58 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhang", "Hekai", ""], ["Gong", "Jibing", ""], ["Teng", "Zhiyong", ""], ["Wang", "Dan", ""], ["Wang", "Hongfei", ""], ["Du", "Linfeng", ""], ["Bhuiyan", "Zakirul Alam", ""]]}, {"id": "1910.02483", "submitter": "Daniel Saromo", "authors": "Daniel Saromo, Elizabeth Villota and Edwin Villanueva", "title": "Auto-Rotating Perceptrons", "comments": "LatinX in AI Research Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an improved design of the perceptron unit to mitigate the\nvanishing gradient problem. This nuisance appears when training deep multilayer\nperceptron networks with bounded activation functions. The new neuron design,\nnamed auto-rotating perceptron (ARP), has a mechanism to ensure that the node\nalways operates in the dynamic region of the activation function, by avoiding\nsaturation of the perceptron. The proposed method does not change the inference\nstructure learned at each neuron. We test the effect of using ARP units in some\nnetwork architectures which use the sigmoid activation function. The results\nsupport our hypothesis that neural networks with ARP units can achieve better\nlearning performance than equivalent models with classic perceptrons.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 17:38:47 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 03:24:09 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Saromo", "Daniel", ""], ["Villota", "Elizabeth", ""], ["Villanueva", "Edwin", ""]]}, {"id": "1910.02497", "submitter": "Anirban Chaudhuri", "authors": "Anirban Chaudhuri, Alexandre N. Marques, Karen E. Willcox", "title": "mfEGRA: Multifidelity Efficient Global Reliability Analysis through\n  Active Learning for Failure Boundary Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops mfEGRA, a multifidelity active learning method using\ndata-driven adaptively refined surrogates for failure boundary location in\nreliability analysis. This work addresses the issue of prohibitive cost of\nreliability analysis using Monte Carlo sampling for expensive-to-evaluate\nhigh-fidelity models by using cheaper-to-evaluate approximations of the\nhigh-fidelity model. The method builds on the Efficient Global Reliability\nAnalysis (EGRA) method, which is a surrogate-based method that uses adaptive\nsampling for refining Gaussian process surrogates for failure boundary location\nusing a single-fidelity model. Our method introduces a two-stage adaptive\nsampling criterion that uses a multifidelity Gaussian process surrogate to\nleverage multiple information sources with different fidelities. The method\ncombines expected feasibility criterion from EGRA with one-step lookahead\ninformation gain to refine the surrogate around the failure boundary. The\ncomputational savings from mfEGRA depends on the discrepancy between the\ndifferent models, and the relative cost of evaluating the different models as\ncompared to the high-fidelity model. We show that accurate estimation of\nreliability using mfEGRA leads to computational savings of $\\sim$46% for an\nanalytic multimodal test problem and 24% for a three-dimensional acoustic horn\nproblem, when compared to single-fidelity EGRA. We also show the effect of\nusing a priori drawn Monte Carlo samples in the implementation for the acoustic\nhorn problem, where mfEGRA leads to computational savings of 45% for the\nthree-dimensional case and 48% for a rarer event four-dimensional case as\ncompared to single-fidelity EGRA.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:37:12 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 22:08:27 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 15:32:10 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 19:35:03 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chaudhuri", "Anirban", ""], ["Marques", "Alexandre N.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1910.02505", "submitter": "Philip Versteeg", "authors": "Philip Versteeg, Joris M. Mooij", "title": "Boosting Local Causal Discovery in High-Dimensional Expression Data", "comments": "Accepted at BIBM / CABB 2019", "journal-ref": "2019 IEEE Intl. Conf. Bioinf. and Biomed. (BIBM 2019) pp.\n  2599-2604", "doi": "10.1109/BIBM47256.2019.8983232", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of Local Causal Discovery (LCD), a simple and\nefficient constraint-based method for causal discovery, in predicting causal\neffects in large-scale gene expression data. We construct practical estimators\nspecific to the high-dimensional regime. Inspired by the ICP algorithm, we use\nan optional preselection method and two different statistical tests.\nEmpirically, the resulting LCD estimator is seen to closely approach the\naccuracy of ICP, the state-of-the-art method, while it is algorithmically\nsimpler and computationally more efficient.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:16:23 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 16:19:35 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Versteeg", "Philip", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1910.02519", "submitter": "Donglin Zhan", "authors": "Shiyu Yi, Donglin Zhan, Wenqing Zhang, Denglin Jiang, Hao Wang", "title": "FIS-GAN: GAN with Flow-based Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) training process, in most cases, apply\nUniform or Gaussian sampling methods in the latent space, which probably spends\nmost of the computation on examples that can be properly handled and easy to\ngenerate. Theoretically, importance sampling speeds up stochastic optimization\nin supervised learning by prioritizing training examples. In this paper, we\nexplore the possibility of adapting importance sampling into adversarial\nlearning. We use importance sampling to replace Uniform and Gaussian sampling\nmethods in the latent space and employ normalizing flow to approximate latent\nspace posterior distribution by density estimation. Empirically, results on\nMNIST and Fashion-MNIST demonstrate that our method significantly accelerates\nGAN's optimization while retaining visual fidelity in generated samples.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 20:34:52 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 09:20:36 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yi", "Shiyu", ""], ["Zhan", "Donglin", ""], ["Zhang", "Wenqing", ""], ["Jiang", "Denglin", ""], ["Wang", "Hao", ""]]}, {"id": "1910.02544", "submitter": "Yikuan Li", "authors": "Haotian Liu, Lin Xi, Ying Zhao and Zhixiang Li", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure\n  with Electroencephalography (EEG) Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 22:53:28 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Liu", "Haotian", ""], ["Xi", "Lin", ""], ["Zhao", "Ying", ""], ["Li", "Zhixiang", ""]]}, {"id": "1910.02545", "submitter": "Yikuan Li", "authors": "Zhiheng Li, Xinyue Xing, Bingzhang Lu and Zhixiang Li", "title": "Early Prediction of 30-day ICU Re-admissions Using Natural Language\n  Processing and Machine Learning", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ICU readmission is associated with longer hospitalization, mortality and\nadverse outcomes. An early recognition of ICU re-admission can help prevent\npatients from worse situation and lower treatment cost. As the abundance of\nElectronics Health Records (EHR), it is popular to design clinical decision\ntools with machine learning technique manipulating on healthcare large scale\ndata. We designed data-driven predictive models to estimate the risk of ICU\nreadmission. The discharge summary of each hospital admission was carefully\nrepresented by natural language processing techniques. Unified Medical Language\nSystem (UMLS) was further used to standardize inconsistency of discharge\nsummaries. 5 machine learning classifiers were adopted to construct predictive\nmodels. The best configuration yielded a competitive AUC of 0.748. Our work\nsuggests that natural language processing of discharge summaries is capable to\nsend clinicians warning of unplanned 30-day readmission upon discharge.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 22:54:00 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Zhiheng", ""], ["Xing", "Xinyue", ""], ["Lu", "Bingzhang", ""], ["Li", "Zhixiang", ""]]}, {"id": "1910.02548", "submitter": "Long Zhao", "authors": "Yu Tian, Long Zhao, Xi Peng, Dimitris N. Metaxas", "title": "Rethinking Kernel Methods for Node Representation Learning on Graphs", "comments": "Accepted to NeurIPS 2019. The first two authors contributed equally.\n  The source code is publicly available at\n  https://github.com/bluer555/KernelGCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels are kernel methods measuring graph similarity and serve as a\nstandard tool for graph classification. However, the use of kernel methods for\nnode classification, which is a related problem to graph representation\nlearning, is still ill-posed and the state-of-the-art methods are heavily based\non heuristics. Here, we present a novel theoretical kernel-based framework for\nnode classification that can bridge the gap between these two representation\nlearning problems on graphs. Our approach is motivated by graph kernel\nmethodology but extended to learn the node representations capturing the\nstructural information in a graph. We theoretically show that our formulation\nis as powerful as any positive semidefinite kernels. To efficiently learn the\nkernel, we propose a novel mechanism for node feature aggregation and a\ndata-driven similarity metric employed during the training phase. More\nimportantly, our framework is flexible and complementary to other graph-based\ndeep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically\nevaluate our approach on a number of standard node classification benchmarks,\nand demonstrate that our model sets the new state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 23:06:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Tian", "Yu", ""], ["Zhao", "Long", ""], ["Peng", "Xi", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1910.02551", "submitter": "Ilia Sucholutsky", "authors": "Ilia Sucholutsky, Matthias Schonlau", "title": "Soft-Label Dataset Distillation and Text Dataset Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset distillation is a method for reducing dataset sizes by learning a\nsmall number of synthetic samples containing all the information of a large\ndataset. This has several benefits like speeding up model training, reducing\nenergy consumption, and reducing required storage space. Currently, each\nsynthetic sample is assigned a single `hard' label, and also, dataset\ndistillation can currently only be used with image data.\n  We propose to simultaneously distill both images and their labels, thus\nassigning each synthetic sample a `soft' label (a distribution of labels). Our\nalgorithm increases accuracy by 2-4% over the original algorithm for several\nimage classification tasks. Using `soft' labels also enables distilled datasets\nto consist of fewer samples than there are classes as each sample can encode\ninformation for multiple classes. For example, training a LeNet model with 10\ndistilled images (one per class) results in over 96% accuracy on MNIST, and\nalmost 92% accuracy when trained on just 5 distilled images.\n  We also extend the dataset distillation algorithm to distill sequential\ndatasets including texts. We demonstrate that text distillation outperforms\nother methods across multiple datasets. For example, models attain almost their\noriginal accuracy on the IMDB sentiment analysis task using just 20 distilled\nsentences.\n  Our code can be found at\n$\\href{https://github.com/ilia10000/dataset-distillation}{\\text{https://github.com/ilia10000/dataset-distillation}}$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 23:57:22 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 21:01:12 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 04:09:03 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Sucholutsky", "Ilia", ""], ["Schonlau", "Matthias", ""]]}, {"id": "1910.02558", "submitter": "Urmish Thakker", "authors": "Urmish Thakker, Igor Fedorov, Jesse Beu, Dibakar Gope, Chu Zhou,\n  Ganesh Dasika, Matthew Mattina", "title": "Pushing the limits of RNN Compression", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1906.02876", "journal-ref": "5th edition of Workshop on Energy Efficient Machine Learning and\n  Cognitive Computing at NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) can be difficult to deploy on resource\nconstrained devices due to their size. As a result, there is a need for\ncompression techniques that can significantly compress RNNs without negatively\nimpacting task accuracy. This paper introduces a method to compress RNNs for\nresource constrained environments using Kronecker product (KP). KPs can\ncompress RNN layers by 16-38x with minimal accuracy loss. We show that KP can\nbeat the task accuracy achieved by other state-of-the-art compression\ntechniques (pruning and low-rank matrix factorization) across 4 benchmarks\nspanning 3 different applications, while simultaneously improving inference\nrun-time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 04:00:33 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:29:36 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Thakker", "Urmish", ""], ["Fedorov", "Igor", ""], ["Beu", "Jesse", ""], ["Gope", "Dibakar", ""], ["Zhou", "Chu", ""], ["Dasika", "Ganesh", ""], ["Mattina", "Matthew", ""]]}, {"id": "1910.02566", "submitter": "Purvasha Chakravarti", "authors": "Purvasha Chakravarti, Sivaraman Balakrishnan and Larry Wasserman", "title": "Gaussian Mixture Clustering Using Relative Tests of Fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering based on significance tests for Gaussian Mixture\nModels (GMMs). Our starting point is the SigClust method developed by Liu et\nal. (2008), which introduces a test based on the k-means objective (with k = 2)\nto decide whether the data should be split into two clusters. When applied\nrecursively, this test yields a method for hierarchical clustering that is\nequipped with a significance guarantee. We study the limiting distribution and\npower of this approach in some examples and show that there are large regions\nof the parameter space where the power is low. We then introduce a new test\nbased on the idea of relative fit. Unlike prior work, we test for whether a\nmixture of Gaussians provides a better fit relative to a single Gaussian,\nwithout assuming that either model is correct. The proposed test has a simple\ncritical value and provides provable error control. One version of our test\nprovides exact, finite sample control of the type I error. We show how our\ntests can be used for hierarchical clustering as well as in a sequential manner\nfor model selection. We conclude with an extensive simulation study and a\ncluster analysis of a gene expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 00:51:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chakravarti", "Purvasha", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1910.02574", "submitter": "Tong Wu", "authors": "Tong Wu, Yunlong Wang, Yue Wang, Emily Zhao, Yilian Yuan, Zhi Yang", "title": "Representation Learning of EHR Data via Graph-Based Medical Entity\n  Embedding", "comments": "5 pages, 2 figures, NeurIPS 2019 Graph Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic representation learning of key entities in electronic health record\n(EHR) data is a critical step for healthcare informatics that turns\nheterogeneous medical records into structured and actionable information. Here\nwe propose ME2Vec, an algorithmic framework for learning low-dimensional\nvectors of the most common entities in EHR: medical services, doctors, and\npatients. ME2Vec leverages diverse graph embedding techniques to cater for the\nunique characteristic of each medical entity. Using real-world clinical data,\nwe demonstrate the efficacy of ME2Vec over competitive baselines on disease\ndiagnosis prediction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:01:32 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wu", "Tong", ""], ["Wang", "Yunlong", ""], ["Wang", "Yue", ""], ["Zhao", "Emily", ""], ["Yuan", "Yilian", ""], ["Yang", "Zhi", ""]]}, {"id": "1910.02575", "submitter": "Yuening Li", "authors": "Yuening Li, Daochen Zha, Na Zou, Xia Hu", "title": "PyODDS: An End-to-End Outlier Detection System", "comments": "6 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PyODDS is an end-to end Python system for outlier detection with database\nsupport. PyODDS provides outlier detection algorithms which meet the demands\nfor users in different fields, w/wo data science or machine learning\nbackground. PyODDS gives the ability to execute machine learning algorithms\nin-database without moving data out of the database server or over the network.\nIt also provides access to a wide range of outlier detection algorithms,\nincluding statistical analysis and more recent deep learning based approaches.\nPyODDS is released under the MIT open-source license, and currently available\nat (https://github.com/datamllab/pyodds) with official documentations at\n(https://pyodds.github.io/).\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:01:34 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 04:49:16 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "Yuening", ""], ["Zha", "Daochen", ""], ["Zou", "Na", ""], ["Hu", "Xia", ""]]}, {"id": "1910.02594", "submitter": "Jun Li", "authors": "Hongyu Guo, Khalique Newaz, Scott Emrich, Tijana Milenkovic, Jun Li", "title": "Weighted graphlets and deep neural networks for protein structure\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As proteins with similar structures often have similar functions, analysis of\nprotein structures can help predict protein functions and is thus important. We\nconsider the problem of protein structure classification, which computationally\nclassifies the structures of proteins into pre-defined groups. We develop a\nweighted network that depicts the protein structures, and more importantly, we\npropose the first graphlet-based measure that applies to weighted networks.\nFurther, we develop a deep neural network (DNN) composed of both convolutional\nand recurrent layers to use this measure for classification. Put together, our\napproach shows dramatic improvements in performance over existing\ngraphlet-based approaches on 36 real datasets. Even comparing with the\nstate-of-the-art approach, it almost halves the classification error. In\naddition to protein structure networks, our weighted-graphlet measure and DNN\nclassifier can potentially be applied to classification of other weighted\nnetworks in computational biology as well as in other domains.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:36:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Guo", "Hongyu", ""], ["Newaz", "Khalique", ""], ["Emrich", "Scott", ""], ["Milenkovic", "Tijana", ""], ["Li", "Jun", ""]]}, {"id": "1910.02600", "submitter": "Alexander Amini", "authors": "Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus", "title": "Deep Evidential Regression", "comments": "Code available on: https://github.com/aamini/evidential-deep-learning", "journal-ref": "Advances in Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic neural networks (NNs) are increasingly being deployed in safety\ncritical domains, where calibrated, robust, and efficient measures of\nuncertainty are crucial. In this paper, we propose a novel method for training\nnon-Bayesian NNs to estimate a continuous target as well as its associated\nevidence in order to learn both aleatoric and epistemic uncertainty. We\naccomplish this by placing evidential priors over the original Gaussian\nlikelihood function and training the NN to infer the hyperparameters of the\nevidential distribution. We additionally impose priors during training such\nthat the model is regularized when its predicted evidence is not aligned with\nthe correct output. Our method does not rely on sampling during inference or on\nout-of-distribution (OOD) examples for training, thus enabling efficient and\nscalable uncertainty learning. We demonstrate learning well-calibrated measures\nof uncertainty on various benchmarks, scaling to complex computer vision tasks,\nas well as robustness to adversarial and OOD test samples.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 04:11:34 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 16:37:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Amini", "Alexander", ""], ["Schwarting", "Wilko", ""], ["Soleimany", "Ava", ""], ["Rus", "Daniela", ""]]}, {"id": "1910.02629", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Dongwoo Kim", "title": "Softmax Is Not an Artificial Trick: An Information-Theoretic View of\n  Softmax in Neural Networks", "comments": "Withdrawn due to Zhenyue Qin uploading the manuscript without consent\n  of the other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great popularity of applying softmax to map the non-normalised\noutputs of a neural network to a probability distribution over predicting\nclasses, this normalised exponential transformation still seems to be\nartificial. A theoretic framework that incorporates softmax as an intrinsic\ncomponent is still lacking. In this paper, we view neural networks embedding\nsoftmax from an information-theoretic perspective. Under this view, we can\nnaturally and mathematically derive log-softmax as an inherent component in a\nneural network for evaluating the conditional mutual information between\nnetwork output vectors and labels given an input datum. We show that training\ndeterministic neural networks through maximising log-softmax is equivalent to\nenlarging the conditional mutual information, i.e., feeding label information\ninto network outputs. We also generalise our informative-theoretic perspective\nto neural networks with stochasticity and derive information upper and lower\nbounds of log-softmax. In theory, such an information-theoretic view offers\nrationality support for embedding softmax in neural networks; in practice, we\neventually demonstrate a computer vision application example of how to employ\nour information-theoretic view to filter out targeted objects on images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:46:06 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 00:34:56 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 05:59:37 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Qin", "Zhenyue", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1910.02633", "submitter": "Josh Payne", "authors": "Josh Payne", "title": "Deep Hyperedges: a Framework for Transductive and Inductive Learning on\n  Hypergraphs", "comments": "9 pages, 4 appendices, accepted at NeurIPS '19 workshop on Sets and\n  Partitions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social networks to protein complexes to disease genomes to visual data,\nhypergraphs are everywhere. However, the scope of research studying deep\nlearning on hypergraphs is still quite sparse and nascent, as there has not yet\nexisted an effective, unified framework for using hyperedge and vertex\nembeddings jointly in the hypergraph context, despite a large body of prior\nwork that has shown the utility of deep learning over graphs and sets. Building\nupon these recent advances, we propose \\textit{Deep Hyperedges} (DHE), a\nmodular framework that jointly uses contextual and permutation-invariant vertex\nmembership properties of hyperedges in hypergraphs to perform classification\nand regression in transductive and inductive learning settings. In our\nexperiments, we use a novel random walk procedure and show that our model\nachieves and, in most cases, surpasses state-of-the-art performance on\nbenchmark datasets. Additionally, we study our framework's performance on a\nvariety of diverse, non-standard hypergraph datasets and propose several\navenues of future work to further enhance DHE.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:56:02 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Payne", "Josh", ""]]}, {"id": "1910.02635", "submitter": "D. H. S. Maithripala", "authors": "Pathmanathan Pankayaraj and D. H. S. Maithripala", "title": "A Decentralized Communication Policy for Multi Agent Multi Armed Bandit\n  Problems", "comments": "This is the full version of a preprint that will appear in the\n  proceedings of the 2020 European Control Conference (ECC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel policy for a group of agents to, individually as\nwell as collectively, solve a multi armed bandit (MAB) problem. The policy\nrelies solely on the information that an agent has obtained through sampling of\nthe options on its own and through communication with neighbors. The option\nselection policy is based on an Upper Confidence Based (UCB) strategy while the\ncommunication strategy that is proposed forces agents to communicate with other\nagents who they believe are most likely to be exploring than exploiting. The\noverall strategy is shown to significantly outperform an independent\nErd\\H{o}s-R\\'{e}nyi (ER) graph based random communication policy. The policy is\nshown to be cost effective in terms of communication and thus to be easily\nscalable to a large network of agents.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:05:36 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 06:08:42 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 16:18:33 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Pankayaraj", "Pathmanathan", ""], ["Maithripala", "D. H. S.", ""]]}, {"id": "1910.02653", "submitter": "Ajay Jain", "authors": "Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\n  Abbeel, Kurt Keutzer, Ion Stoica, Joseph E. Gonzalez", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor\n  Rematerialization", "comments": "In Proceedings of 3rd Conference Machine Learning and Systems 2020\n  (MLSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We formalize the problem of trading-off DNN training time and memory\nrequirements as the tensor rematerialization optimization problem, a\ngeneralization of prior checkpointing strategies. We introduce Checkmate, a\nsystem that solves for optimal rematerialization schedules in reasonable times\n(under an hour) using off-the-shelf MILP solvers or near-optimal schedules with\nan approximation algorithm, then uses these schedules to accelerate millions of\ntraining iterations. Our method scales to complex, realistic architectures and\nis hardware-aware through the use of accelerator-specific, profile-based cost\nmodels. In addition to reducing training cost, Checkmate enables real-world\nnetworks to be trained with up to 5.1x larger input sizes. Checkmate is an\nopen-source project, available at https://github.com/parasj/checkmate.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:54:06 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 17:57:45 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 17:46:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Jain", "Paras", ""], ["Jain", "Ajay", ""], ["Nrusimha", "Aniruddha", ""], ["Gholami", "Amir", ""], ["Abbeel", "Pieter", ""], ["Keutzer", "Kurt", ""], ["Stoica", "Ion", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1910.02660", "submitter": "Jiaxuan Xie", "authors": "Jiaxuan Xie, Fanghui Liu, Kaijie Wang, Xiaolin Huang", "title": "Deep Kernel Learning via Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel learning methods are among the most effective learning methods and\nhave been vigorously studied in the past decades. However, when tackling with\ncomplicated tasks, classical kernel methods are not flexible or \"rich\" enough\nto describe the data and hence could not yield satisfactory performance. In\nthis paper, via Random Fourier Features (RFF), we successfully incorporate the\ndeep architecture into kernel learning, which significantly boosts the\nflexibility and richness of kernel machines while keeps kernels' advantage of\npairwise handling small data. With RFF, we could establish a deep structure and\nmake every kernel in RFF layers could be trained end-to-end. Since RFF with\ndifferent distributions could represent different kernels, our model has the\ncapability of finding suitable kernels for each layer, which is much more\nflexible than traditional kernel-based methods where the kernel is\npre-selected. This fact also helps yield a more sophisticated kernel cascade\nconnection in the architecture. On small datasets (less than 1000 samples), for\nwhich deep learning is generally not suitable due to overfitting, our method\nachieves superior performance compared to advanced kernel methods. On\nlarge-scale datasets, including non-image and image classification tasks, our\nmethod also has competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:20:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Xie", "Jiaxuan", ""], ["Liu", "Fanghui", ""], ["Wang", "Kaijie", ""], ["Huang", "Xiaolin", ""]]}, {"id": "1910.02672", "submitter": "Wei Qiu", "authors": "Wei Qiu, Jiaming Guo, Xiang Li, Mengjia Xu, Mo Zhang, Ning Guo and\n  Quanzheng Li", "title": "Multi-label Detection and Classification of Red Blood Cells in\n  Microscopic Images", "comments": "Wei Qiu, Jiaming Guo and Xiang Li contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection and cell type classification from biomedical images play an\nimportant role for high-throughput imaging and various clinical application.\nWhile classification of single cell sample can be performed with standard\ncomputer vision and machine learning methods, analysis of multi-label samples\n(region containing congregating cells) is more challenging, as separation of\nindividual cells can be difficult (e.g. touching cells) or even impossible\n(e.g. overlapping cells). As multi-instance images are common in analyzing Red\nBlood Cell (RBC) for Sickle Cell Disease (SCD) diagnosis, we develop and\nimplement a multi-instance cell detection and classification framework to\naddress this challenge. The framework firstly trains a region proposal model\nbased on Region-based Convolutional Network (RCNN) to obtain bounding-boxes of\nregions potentially containing single or multiple cells from input microscopic\nimages, which are extracted as image patches. High-level image features are\nthen calculated from image patches through a pre-trained Convolutional Neural\nNetwork (CNN) with ResNet-50 structure. Using these image features inputs, six\nnetworks are then trained to make multi-label prediction of whether a given\npatch contains cells belonging to a specific cell type. As the six networks are\ntrained with image patches consisting of both individual cells and\ntouching/overlapping cells, they can effectively recognize cell types that are\npresented in multi-instance image samples. Finally, for the purpose of SCD\ntesting, we train another machine learning classifier to predict whether the\ngiven image patch contains abnormal cell type based on outputs from the six\nnetworks. Testing result of the proposed framework shows that it can achieve\ngood performance in automatic cell detection and classification.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:40:49 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 12:44:04 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Qiu", "Wei", ""], ["Guo", "Jiaming", ""], ["Li", "Xiang", ""], ["Xu", "Mengjia", ""], ["Zhang", "Mo", ""], ["Guo", "Ning", ""], ["Li", "Quanzheng", ""]]}, {"id": "1910.02673", "submitter": "Yulong Wang", "authors": "Yulong Wang, Xiaolin Hu, Hang Su", "title": "Interpretable Disentanglement of Neural Networks by Extracting\n  Class-Specific Subnetwork", "comments": "Accepted to 2019 ICCV Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel perspective to understand deep neural networks in an\ninterpretable disentanglement form. For each semantic class, we extract a\nclass-specific functional subnetwork from the original full model, with\ncompressed structure while maintaining comparable prediction performance. The\nstructure representations of extracted subnetworks display a resemblance to\ntheir corresponding class semantic similarities. We also apply extracted\nsubnetworks in visual explanation and adversarial example detection tasks by\nmerely replacing the original full model with class-specific subnetworks.\nExperiments demonstrate that this intuitive operation can effectively improve\nexplanation saliency accuracy for gradient-based explanation methods, and\nincrease the detection rate for confidence score-based adversarial example\ndetection methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:42:45 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wang", "Yulong", ""], ["Hu", "Xiaolin", ""], ["Su", "Hang", ""]]}, {"id": "1910.02678", "submitter": "Bruno Apolloni", "authors": "Bruno Apolloni", "title": "An Algorithmic Inference Approach to Learn Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new method for estimating the parameter of the bivariate\nClayton copulas within the framework of Algorithmic Inference. The method\nconsists of a variant of the standard boot-strapping procedure for inferring\nrandom parameters, which we expressly devise to bypass the two pitfalls of this\nspecific instance: the non independence of the Kendall statistics, customarily\nat the basis of this inference task, and the absence of a sufficient statistic\nw.r.t. \\alpha. The variant is rooted on a numerical procedure in order to find\nthe \\alpha estimate at a fixed point of an iterative routine. Although paired\nwith the customary complexity of the program which computes them, numerical\nresults show an outperforming accuracy of the estimates.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 09:06:04 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Apolloni", "Bruno", ""]]}, {"id": "1910.02684", "submitter": "Zhou Ziang", "authors": "Ziang Zhou, Jieming Shi, Shengzhong Zhang, Zengfeng Huang, Qing Li", "title": "Effective Semi-Supervised Node Classification on Few-Labeled Graph Data", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are designed for semi-supervised node\nclassification on graphs where only a small subset of nodes have class labels.\nHowever, under extreme cases when very few labels are available (e.g., 1\nlabeled node per class), GNNs suffer from severe result quality degradation.\nSeveral existing studies make an initial effort to ease this situation, but are\nstill far from satisfactory.\n  In this paper, on few-labeled graph data, we propose an effective framework\nABN that is readily applicable to both shallow and deep GNN architectures and\nsignificantly boosts classification accuracy. In particular, on a benchmark\ndataset Cora with only 1 labeled node per class, while the classic graph\nconvolutional network (GCN) only has 44.6% accuracy, an immediate instantiation\nof ABN over GCN achieves 62.5% accuracy; when applied to a deep architecture\nDAGNN, ABN improves accuracy from 59.8% to 66.4%, which is state of the art.\n  ABN obtains superior performance through three main algorithmic designs.\nFirst, it selects high-quality unlabeled nodes via an adaptive pseudo labeling\ntechnique, so as to adaptively enhance the training process of GNNs. Second,\nABN balances the labels of the selected nodes on real-world skewed graph data\nby pseudo label balancing. Finally, a negative sampling regularizer is designed\nfor ABN to further utilize the unlabeled nodes. The effectiveness of the three\ntechniques in ABN is well-validated by both theoretical and empirical analysis.\nExtensive experiments, comparing 12 existing approaches on 4 benchmark\ndatasets, demonstrate that ABN achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 09:21:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 06:40:08 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhou", "Ziang", ""], ["Shi", "Jieming", ""], ["Zhang", "Shengzhong", ""], ["Huang", "Zengfeng", ""], ["Li", "Qing", ""]]}, {"id": "1910.02686", "submitter": "Yuhui Zhang", "authors": "Zhang Yuhui, Greg Gutmann, Konagaya Akihiko", "title": "Irregular Convolutional Auto-Encoder on Point Clouds", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a novel graph convolutional neural network that could construct a\ncoarse, sparse latent point cloud from a dense, raw point cloud. With a novel\nnon-isotropic convolution operation defined on irregular geometries, the model\nthen can reconstruct the original point cloud from this latent cloud with fine\ndetails. Furthermore, we proposed that it is even possible to perform particle\nsimulation using the latent cloud encoded from some simulated particle cloud\n(e.g. fluids), to accelerate the particle simulation process. Our model has\nbeen tested on ShapeNetCore dataset for Auto-Encoding with a limited latent\ndimension and tested on a synthesis dataset for fluids simulation. We also\ncompare the model with other state-of-the-art models, and several\nvisualizations were done to intuitively understand the model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 09:24:08 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yuhui", "Zhang", ""], ["Gutmann", "Greg", ""], ["Akihiko", "Konagaya", ""]]}, {"id": "1910.02717", "submitter": "Daniele Loiacono", "authors": "Edoardo Giacomello, Daniele Loiacono, Luca Mainardi", "title": "Brain MRI Tumor Segmentation with Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207220", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is a promising approach to either automate or simplify several\ntasks in the healthcare domain. In this work, we introduce SegAN-CAT, an\napproach to brain tumor segmentation in Magnetic Resonance Images (MRI), based\non Adversarial Networks. In particular, we extend SegAN, successfully applied\nto the same task in a previous work, in two respects: (i) we used a different\nmodel input and (ii) we employed a modified loss function to train the model.\nWe tested our approach on two large datasets, made available by the Brain Tumor\nImage Segmentation Benchmark (BraTS). First, we trained and tested some\nsegmentation models assuming the availability of all the major MRI contrast\nmodalities, i.e., T1-weighted, T1 weighted contrast-enhanced, T2-weighted, and\nT2-FLAIR. However, as these four modalities are not always all available for\neach patient, we also trained and tested four segmentation models that take as\ninput MRIs acquired only with a single contrast modality. Finally, we proposed\nto apply transfer learning across different contrast modalities to improve the\nperformance of these single-modality models. Our results are promising and show\nthat not SegAN-CAT is able to outperform SegAN when all the four modalities are\navailable, but also that transfer learning can actually lead to better\nperformances when only a single modality is available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:51:56 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 17:23:27 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Giacomello", "Edoardo", ""], ["Loiacono", "Daniele", ""], ["Mainardi", "Luca", ""]]}, {"id": "1910.02718", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi", "title": "Continual Learning in Neural Networks", "comments": "PhD Thesis, Supervisor: Tinne Tuytelaars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have exceeded human-level performance in\naccomplishing several individual tasks (e.g. voice recognition, object\nrecognition, and video games). However, such success remains modest compared to\nhuman intelligence that can learn and perform an unlimited number of tasks.\nHumans' ability of learning and accumulating knowledge over their lifetime is\nan essential aspect of their intelligence. Continual machine learning aims at a\nhigher level of machine intelligence through providing the artificial agents\nwith the ability to learn online from a non-stationary and never-ending stream\nof data. A key component of such a never-ending learning process is to overcome\nthe catastrophic forgetting of previously seen data, a problem that neural\nnetworks are well known to suffer from. The work described in this thesis has\nbeen dedicated to the investigation of continual learning and solutions to\nmitigate the forgetting phenomena in neural networks. To approach the continual\nlearning problem, we first assume a task incremental setting where tasks are\nreceived one at a time and data from previous tasks are not stored. Since the\ntask incremental setting can't be assumed in all continual learning scenarios,\nwe also study the more general online continual setting. We consider an\ninfinite stream of data drawn from a non-stationary distribution with a\nsupervisory or self-supervisory training signal. The proposed methods in this\nthesis have tackled important aspects of continual learning. They were\nevaluated on different benchmarks and over various learning sequences. Advances\nin the state of the art of continual learning have been shown and challenges\nfor bringing continual learning into application were critically identified.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:52:14 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 09:48:14 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Aljundi", "Rahaf", ""]]}, {"id": "1910.02720", "submitter": "Sergey Bartunov", "authors": "Sergey Bartunov, Jack W Rae, Simon Osindero, Timothy P Lillicrap", "title": "Meta-Learning Deep Energy-Based Memory Models", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning associative memory -- a system which is able\nto retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are\nstored as attractors of the network dynamics and associative retrieval is\nperformed by running the dynamics starting from a query pattern until it\nconverges to an attractor. In such models the dynamics are often implemented as\nan optimization procedure that minimizes an energy function, such as in the\nclassical Hopfield network. In general it is difficult to derive a writing rule\nfor a given dynamics and energy that is both compressive and fast. Thus, most\nresearch in energy-based memory has been limited either to tractable energy\nmodels not expressive enough to handle complex high-dimensional objects such as\nnatural images, or to models that do not offer fast writing. We present a novel\nmeta-learning approach to energy-based memory models (EBMM) that allows one to\nuse an arbitrary neural architecture as an energy model and quickly store\npatterns in its weights. We demonstrate experimentally that our EBMM approach\ncan build compressed memories for synthetic and natural data, and is capable of\nassociative retrieval that outperforms existing memory systems in terms of the\nreconstruction error and compression rate.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:58:08 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 08:34:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bartunov", "Sergey", ""], ["Rae", "Jack W", ""], ["Osindero", "Simon", ""], ["Lillicrap", "Timothy P", ""]]}, {"id": "1910.02743", "submitter": "Anton Dereventsov", "authors": "Armenak Petrosyan, Anton Dereventsov, Clayton Webster", "title": "Neural network integral representations with the ReLU activation\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this effort, we derive a formula for the integral representation of a\nshallow neural network with the ReLU activation function. We assume that the\nouter weighs admit a finite $L_1$-norm with respect to Lebesgue measure on the\nsphere. For univariate target functions we further provide a closed-form\nformula for all possible representations. Additionally, in this case our\nformula allows one to explicitly solve the least $L_1$-norm neural network\nrepresentation for a given function.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 12:00:37 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 08:05:43 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 03:13:22 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Petrosyan", "Armenak", ""], ["Dereventsov", "Anton", ""], ["Webster", "Clayton", ""]]}, {"id": "1910.02757", "submitter": "Leonardo Cella", "authors": "Leonardo Cella and Nicol\\`o Cesa-Bianchi", "title": "Stochastic Bandits with Delay-Dependent Payoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recommendation problems in music streaming platforms, we propose\na nonstationary stochastic bandit model in which the expected reward of an arm\ndepends on the number of rounds that have passed since the arm was last pulled.\nAfter proving that finding an optimal policy is NP-hard even when all model\nparameters are known, we introduce a class of ranking policies provably\napproximating, to within a constant factor, the expected reward of the optimal\npolicy. We show an algorithm whose regret with respect to the best ranking\npolicy is bounded by $\\widetilde{\\mathcal{O}}\\big(\\!\\sqrt{kT}\\big)$, where $k$\nis the number of arms and $T$ is time. Our algorithm uses only\n$\\mathcal{O}\\big(k\\ln\\ln T\\big)$ switches, which helps when switching between\npolicies is costly. As constructing the class of learning policies requires\nordering the arms according to their expectations, we also bound the number of\npulls required to do so. Finally, we run experiments to compare our algorithm\nagainst UCB on different problem instances.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 12:48:39 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 18:28:04 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 19:20:10 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2020 10:38:37 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Cella", "Leonardo", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1910.02758", "submitter": "Hector Zenil", "authors": "Santiago Hern\\'andez-Orozco, Hector Zenil, J\\\"urgen Riedel, Adam\n  Uccello, Narsis A. Kiani, Jesper Tegn\\'er", "title": "Algorithmic Probability-guided Supervised Machine Learning on\n  Non-differentiable Spaces", "comments": "33 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how complexity theory can be introduced in machine learning to help\nbring together apparently disparate areas of current research. We show that\nthis new approach requires less training data and is more generalizable as it\nshows greater resilience to random attacks. We investigate the shape of the\ndiscrete algorithmic space when performing regression or classification using a\nloss function parametrized by algorithmic complexity, demonstrating that the\nproperty of differentiation is not necessary to achieve results similar to\nthose obtained using differentiable programming approaches such as deep\nlearning. In doing so we use examples which enable the two approaches to be\ncompared (small, given the computational power required for estimations of\nalgorithmic complexity). We find and report that (i) machine learning can\nsuccessfully be performed on a non-smooth surface using algorithmic complexity;\n(ii) that parameter solutions can be found using an algorithmic-probability\nclassifier, establishing a bridge between a fundamentally discrete theory of\ncomputability and a fundamentally continuous mathematical theory of\noptimization methods; (iii) a formulation of an algorithmically directed search\ntechnique in non-smooth manifolds can be defined and conducted; (iv)\nexploitation techniques and numerical methods for algorithmic search to\nnavigate these discrete non-differentiable spaces can be performed; in\napplication of the (a) identification of generative rules from data\nobservations; (b) solutions to image classification problems more resilient\nagainst pixel attacks compared to neural networks; (c) identification of\nequation parameters from a small data-set in the presence of noise in\ncontinuous ODE system problem, (d) classification of Boolean NK networks by (1)\nnetwork topology, (2) underlying Boolean function, and (3) number of incoming\nedges.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 12:48:58 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 23:41:44 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Hern\u00e1ndez-Orozco", "Santiago", ""], ["Zenil", "Hector", ""], ["Riedel", "J\u00fcrgen", ""], ["Uccello", "Adam", ""], ["Kiani", "Narsis A.", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1910.02760", "submitter": "Adri\\'an Csisz\\'arik", "authors": "Adri\\'an Csisz\\'arik, Beatrix Benk\\H{o}, D\\'aniel Varga", "title": "Negative Sampling in Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose negative sampling as an approach to improve the notoriously bad\nout-of-distribution likelihood estimates of Variational Autoencoder models. Our\nmodel pushes latent images of negative samples away from the prior. When the\nsource of negative samples is an auxiliary dataset, such a model can vastly\nimprove on baselines when evaluated on OOD detection tasks. Perhaps more\nsurprisingly, we present a fully unsupervised version of employing negative\nsampling in VAEs: when the generator is trained in an adversarial manner, using\nthe generator's own outputs as negative samples can also significantly improve\nthe robustness of OOD likelihood estimates.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 12:57:45 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 17:05:27 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Csisz\u00e1rik", "Adri\u00e1n", ""], ["Benk\u0151", "Beatrix", ""], ["Varga", "D\u00e1niel", ""]]}, {"id": "1910.02776", "submitter": "Maciej Wo{\\l}czyk", "authors": "Maciej Wo{\\l}czyk, Jacek Tabor, Marek \\'Smieja, Szymon Maszke", "title": "Biologically-Inspired Spatial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce bio-inspired artificial neural networks consisting of neurons\nthat are additionally characterized by spatial positions. To simulate\nproperties of biological systems we add the costs penalizing long connections\nand the proximity of neurons in a two-dimensional space. Our experiments show\nthat in the case where the network performs two different tasks, the neurons\nnaturally split into clusters, where each cluster is responsible for processing\na different task. This behavior not only corresponds to the biological systems,\nbut also allows for further insight into interpretability or continual\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 13:22:13 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wo\u0142czyk", "Maciej", ""], ["Tabor", "Jacek", ""], ["\u015amieja", "Marek", ""], ["Maszke", "Szymon", ""]]}, {"id": "1910.02785", "submitter": "Kaleel Mahmood", "authors": "Kaleel Mahmood, Phuong Ha Nguyen, Lam M. Nguyen, Thanh Nguyen, Marten\n  van Dijk", "title": "BUZz: BUffer Zones for defending adversarial examples in image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel defense against all existing gradient based adversarial\nattacks on deep neural networks for image classification problems. Our defense\nis based on a combination of deep neural networks and simple image\ntransformations. While straightforward in implementation, this defense yields a\nunique security property which we term buffer zones. We argue that our defense\nbased on buffer zones offers significant improvements over state-of-the-art\ndefenses. We are able to achieve this improvement even when the adversary has\naccess to the {\\em entire} original training data set and unlimited query\naccess to the defense. We verify our claim through experimentation using\nFashion-MNIST and CIFAR-10: We demonstrate $<11\\%$ attack success rate --\nsignificantly lower than what other well-known state-of-the-art defenses offer\n-- at only a price of a $11-18\\%$ drop in clean accuracy. By using a new\nintuitive metric, we explain why this trade-off offers a significant\nimprovement over prior work.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:15:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:15:28 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Mahmood", "Kaleel", ""], ["Nguyen", "Phuong Ha", ""], ["Nguyen", "Lam M.", ""], ["Nguyen", "Thanh", ""], ["van Dijk", "Marten", ""]]}, {"id": "1910.02787", "submitter": "Cristian Bodnar", "authors": "Cristian Bodnar, Adrian Li, Karol Hausman, Peter Pastor, Mrinal\n  Kalakrishnan", "title": "Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping", "comments": "Camera-ready version for RSS 2020. Contains 8 pages, 7 figures", "journal-ref": "Proceedings of Robotics: Science and Systems (2020)", "doi": "10.15607/RSS.2020.XVI.075", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributional perspective on reinforcement learning (RL) has given rise\nto a series of successful Q-learning algorithms, resulting in state-of-the-art\nperformance in arcade game environments. However, it has not yet been analyzed\nhow these findings from a discrete setting translate to complex practical\napplications characterized by noisy, high dimensional and continuous\nstate-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a\ndistributional variant of the recently introduced distributed Q-learning\nalgorithm for continuous domains, and examine its behaviour in a series of\nsimulated and real vision-based robotic grasping tasks. The absence of an actor\nin Q2-Opt allows us to directly draw a parallel to the previous discrete\nexperiments in the literature without the additional complexities induced by an\nactor-critic architecture. We demonstrate that Q2-Opt achieves a superior\nvision-based object grasping success rate, while also being more sample\nefficient. The distributional formulation also allows us to experiment with\nvarious risk distortion metrics that give us an indication of how robots can\nconcretely manage risk in practice using a Deep RL control policy. As an\nadditional contribution, we perform batch RL experiments in our virtual\nenvironment and compare them with the latest findings from discrete settings.\nSurprisingly, we find that the previous batch RL findings from the literature\nobtained on arcade game environments do not generalise to our setup.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:12:00 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 13:48:14 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 12:56:16 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Bodnar", "Cristian", ""], ["Li", "Adrian", ""], ["Hausman", "Karol", ""], ["Pastor", "Peter", ""], ["Kalakrishnan", "Mrinal", ""]]}, {"id": "1910.02804", "submitter": "Shahar Harel", "authors": "Shahar Harel, Meir Maor, Amir Ronen", "title": "Semantic Preserving Generative Adversarial Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce generative adversarial models in which the discriminator is\nreplaced by a calibrated (non-differentiable) classifier repeatedly enhanced by\ndomain relevant features. The role of the classifier is to prove that the\nactual and generated data differ over a controlled semantic space. We\ndemonstrate that such models have the ability to generate objects with strong\nguarantees on their properties in a wide range of domains. They require less\ndata than ordinary GANs, provide natural stopping conditions, uncover important\nproperties of the data, and enhance transfer learning. Our techniques can be\ncombined with standard generative models. We demonstrate the usefulness of our\napproach by applying it to several unrelated domains: generating good locations\nfor cellular antennae, molecule generation preserving key chemical properties,\nand generating and extrapolating lines from very few data points. Intriguing\nopen problems are presented as well.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:06:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Harel", "Shahar", ""], ["Maor", "Meir", ""], ["Ronen", "Amir", ""]]}, {"id": "1910.02806", "submitter": "Hyojin Bahng", "authors": "Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, Seong Joon Oh", "title": "Learning De-biased Representations with Biased Representations", "comments": "Accepted to ICML 2020. Code available at\n  https://github.com/clovaai/rebias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are trained and evaluated by splitting data\nfrom a single source into training and test sets. While such focus on\nin-distribution learning scenarios has led to interesting advancement, it has\nnot been able to tell if models are relying on dataset biases as shortcuts for\nsuccessful prediction (e.g., using snow cues for recognising snowmobiles),\nresulting in biased models that fail to generalise when the bias shifts to a\ndifferent class. The cross-bias generalisation problem has been addressed by\nde-biasing training data through augmentation or re-sampling, which are often\nprohibitive due to the data collection cost (e.g., collecting images of a\nsnowmobile on a desert) and the difficulty of quantifying or expressing biases\nin the first place. In this work, we propose a novel framework to train a\nde-biased representation by encouraging it to be different from a set of\nrepresentations that are biased by design. This tactic is feasible in many\nscenarios where it is much easier to define a set of biased representations\nthan to define and quantify bias. We demonstrate the efficacy of our method\nacross a variety of synthetic and real-world biases; our experiments show that\nthe method discourages models from taking bias shortcuts, resulting in improved\ngeneralisation. Source code is available at https://github.com/clovaai/rebias.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:11:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 16:46:24 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 11:51:02 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bahng", "Hyojin", ""], ["Chun", "Sanghyuk", ""], ["Yun", "Sangdoo", ""], ["Choo", "Jaegul", ""], ["Oh", "Seong Joon", ""]]}, {"id": "1910.02822", "submitter": "Pei Wang", "authors": "Pei Wang, Junqi Wang, Pushpi Paranamana, and Patrick Shafto", "title": "A mathematical theory of cooperative communication", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (NIPS 2030)", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative communication plays a central role in theories of human\ncognition, language, development, culture, and human-robot interaction. Prior\nmodels of cooperative communication are algorithmic in nature and do not shed\nlight on why cooperation may yield effective belief transmission and what\nlimitations may arise due to differences between beliefs of agents. Through a\nconnection to the theory of optimal transport, we establishing a mathematical\nframework for cooperative communication. We derive prior models as special\ncases, statistical interpretations of belief transfer plans, and proofs of\nrobustness and instability. Computational simulations support and elaborate our\ntheoretical results, and demonstrate fit to human behavior. The results show\nthat cooperative communication provably enables effective, robust belief\ntransmission which is required to explain feats of human learning and improve\nhuman-machine interaction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:35:22 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 20:26:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Pei", ""], ["Wang", "Junqi", ""], ["Paranamana", "Pushpi", ""], ["Shafto", "Patrick", ""]]}, {"id": "1910.02826", "submitter": "Pascal Klink", "authors": "Pascal Klink, Hany Abdulsamad, Boris Belousov, Jan Peters", "title": "Self-Paced Contextual Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization and adaptation of learned skills to novel situations is a core\nrequirement for intelligent autonomous robots. Although contextual\nreinforcement learning provides a principled framework for learning and\ngeneralization of behaviors across related tasks, it generally relies on\nuninformed sampling of environments from an unknown, uncontrolled context\ndistribution, thus missing the benefits of structured, sequential learning. We\nintroduce a novel relative entropy reinforcement learning algorithm that gives\nthe agent the freedom to control the intermediate task distribution, allowing\nfor its gradual progression towards the target context distribution. Empirical\nevaluation shows that the proposed curriculum learning scheme drastically\nimproves sample efficiency and enables learning in scenarios with both broad\nand sharp target context distributions in which classical approaches perform\nsub-optimally.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:40:53 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Klink", "Pascal", ""], ["Abdulsamad", "Hany", ""], ["Belousov", "Boris", ""], ["Peters", "Jan", ""]]}, {"id": "1910.02830", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Anitha Kannan, Geoffrey J. Tso, Namit Katariya, Manish\n  Chablani, David Sontag, Xavier Amatriain", "title": "Open Set Medical Diagnosis", "comments": "Abbreviated version to appear at Machine Learning for Healthcare\n  (ML4H) Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learned diagnosis models have shown promise as medical aides but are\ntrained under a closed-set assumption, i.e. that models will only encounter\nconditions on which they have been trained. However, it is practically\ninfeasible to obtain sufficient training data for every human condition, and\nonce deployed such models will invariably face previously unseen conditions. We\nframe machine-learned diagnosis as an open-set learning problem, and study how\nstate-of-the-art approaches compare. Further, we extend our study to a setting\nwhere training data is distributed across several healthcare sites that do not\nallow data pooling, and experiment with different strategies of building\nopen-set diagnostic ensembles. Across both settings, we observe consistent\ngains from explicitly modeling unseen conditions, but find the optimal training\nstrategy to vary across settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:45:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Prabhu", "Viraj", ""], ["Kannan", "Anitha", ""], ["Tso", "Geoffrey J.", ""], ["Katariya", "Namit", ""], ["Chablani", "Manish", ""], ["Sontag", "David", ""], ["Amatriain", "Xavier", ""]]}, {"id": "1910.02835", "submitter": "Steve Heim", "authors": "Steve Heim, Alexander von Rohr, Sebastian Trimpe, and Alexander\n  Badri-Spr\\\"owitz", "title": "A Learnable Safety Measure", "comments": "10 pages, Conference on Robot Learning CoRL 2019, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Failures are challenging for learning to control physical systems since they\nrisk damage, time-consuming resets, and often provide little gradient\ninformation. Adding safety constraints to exploration typically requires a lot\nof prior knowledge and domain expertise. We present a safety measure which\nimplicitly captures how the system dynamics relate to a set of failure states.\nNot only can this measure be used as a safety function, but also to directly\ncompute the set of safe state-action pairs. Further, we show a model-free\napproach to learn this measure by active sampling using Gaussian processes.\nWhile safety can only be guaranteed after learning the safety measure, we show\nthat failures can already be greatly reduced by using the estimated measure\nduring learning.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:53:15 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Heim", "Steve", ""], ["von Rohr", "Alexander", ""], ["Trimpe", "Sebastian", ""], ["Badri-Spr\u00f6witz", "Alexander", ""]]}, {"id": "1910.02845", "submitter": "Joseph Morrone", "authors": "Joseph A. Morrone, Jeffrey K. Weber, Tien Huynh, Heng Luo, Wendy D.\n  Cornell", "title": "Combining docking pose rank and structure with deep learning improves\n  protein-ligand binding mode prediction", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.9b00927", "report-no": null, "categories": "q-bio.BM physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, modular graph-based convolutional neural network that\ntakes structural information from protein-ligand complexes as input to generate\nmodels for activity and binding mode prediction. Complex structures are\ngenerated by a standard docking procedure and fed into a dual-graph\narchitecture that includes separate sub-networks for the ligand bonded topology\nand the ligand-protein contact map. This network division allows contributions\nfrom ligand identity to be distinguished from effects of protein-ligand\ninteractions on classification. We show, in agreement with recent literature,\nthat dataset bias drives many of the promising results on virtual screening\nthat have previously been reported. However, we also show that our neural\nnetwork is capable of learning from protein structural information when, as in\nthe case of binding mode prediction, an unbiased dataset is constructed. We\ndevelop a deep learning model for binding mode prediction that uses docking\nranking as input in combination with docking structures. This strategy mirrors\npast consensus models and outperforms the baseline docking program in a variety\nof tests, including on cross-docking datasets that mimic real-world docking use\ncases. Furthermore, the magnitudes of network predictions serve as reliable\nmeasures of model confidence\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:12:32 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Morrone", "Joseph A.", ""], ["Weber", "Jeffrey K.", ""], ["Huynh", "Tien", ""], ["Luo", "Heng", ""], ["Cornell", "Wendy D.", ""]]}, {"id": "1910.02875", "submitter": "Arthur Jacot", "authors": "Arthur Jacot and Franck Gabriel and Cl\\'ement Hongler", "title": "The asymptotic spectrum of the Hessian of DNN throughout training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of DNNs during gradient descent is described by the so-called\nNeural Tangent Kernel (NTK). In this article, we show that the NTK allows one\nto gain precise insight into the Hessian of the cost of DNNs. When the NTK is\nfixed during training, we obtain a full characterization of the asymptotics of\nthe spectrum of the Hessian, at initialization and during training. In the\nso-called mean-field limit, where the NTK is not fixed during training, we\ndescribe the first two moments of the Hessian at initialization.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:04:14 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 07:57:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jacot", "Arthur", ""], ["Gabriel", "Franck", ""], ["Hongler", "Cl\u00e9ment", ""]]}, {"id": "1910.02876", "submitter": "Ali Shafti", "authors": "Petros Christodoulou, Robert Tjarko Lange, Ali Shafti, A. Aldo Faisal", "title": "Reinforcement Learning with Structured Hierarchical Grammar\n  Representations of Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a young age humans learn to use grammatical principles to hierarchically\ncombine words into sentences. Action grammars is the parallel idea, that there\nis an underlying set of rules (a \"grammar\") that govern how we hierarchically\ncombine actions to form new, more complex actions. We introduce the Action\nGrammar Reinforcement Learning (AG-RL) framework which leverages the concept of\naction grammars to consistently improve the sample efficiency of Reinforcement\nLearning agents. AG-RL works by using a grammar inference algorithm to infer\nthe \"action grammar\" of an agent midway through training. The agent's action\nspace is then augmented with macro-actions identified by the grammar. We apply\nthis framework to Double Deep Q-Learning (AG-DDQN) and a discrete action\nversion of Soft Actor-Critic (AG-SAC) and find that it improves performance in\n8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested\nAtari games (median +96%, maximum +3,756%) respectively without substantive\nhyperparameter tuning. We also show that AG-SAC beats the model-free\nstate-of-the-art for sample efficiency in 17 out of the 20 tested Atari games\n(median +62%, maximum +13,140%), again without substantive hyperparameter\ntuning.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:59:20 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 10:23:19 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Christodoulou", "Petros", ""], ["Lange", "Robert Tjarko", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1910.02910", "submitter": "Siddharth Reddy", "authors": "Gokul Swamy, Siddharth Reddy, Sergey Levine, Anca D. Dragan", "title": "Scaled Autonomy: Enabling Human Operators to Control Robot Fleets", "comments": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots often encounter challenging situations where their control\npolicies fail and an expert human operator must briefly intervene, e.g.,\nthrough teleoperation. In settings where multiple robots act in separate\nenvironments, a single human operator can manage a fleet of robots by\nidentifying and teleoperating one robot at any given time. The key challenge is\nthat users have limited attention: as the number of robots increases, users\nlose the ability to decide which robot requires teleoperation the most. Our\ngoal is to automate this decision, thereby enabling users to supervise more\nrobots than their attention would normally allow for. Our insight is that we\ncan model the user's choice of which robot to control as an approximately\noptimal decision that maximizes the user's utility function. We learn a model\nof the user's preferences from observations of the user's choices in easy\nsettings with a few robots, and use it in challenging settings with more robots\nto automatically identify which robot the user would most likely choose to\ncontrol, if they were able to evaluate the states of all robots at all times.\nWe run simulation experiments and a user study with twelve participants that\nshow our method can be used to assist users in performing a simulated\nnavigation task. We also run a hardware demonstration that illustrates how our\nmethod can be applied to a real-world mobile robot navigation task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:00:49 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 22:36:53 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Swamy", "Gokul", ""], ["Reddy", "Siddharth", ""], ["Levine", "Sergey", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1910.02912", "submitter": "Tim R. Davidson", "authors": "Tim R. Davidson, Jakub M. Tomczak, Efstratios Gavves", "title": "Increasing Expressivity of a Hyperspherical VAE", "comments": "NeurIPS 2019, in Workshop on Bayesian Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning suitable latent representations for observed, high-dimensional data\nis an important research topic underlying many recent advances in machine\nlearning. While traditionally the Gaussian normal distribution has been the\ngo-to latent parameterization, recently a variety of works have successfully\nproposed the use of manifold-valued latents. In one such work (Davidson et al.,\n2018), the authors empirically show the potential benefits of using a\nhyperspherical von Mises-Fisher (vMF) distribution in low dimensionality.\nHowever, due to the unique distributional form of the vMF, expressivity in\nhigher dimensional space is limited as a result of its scalar concentration\nparameter leading to a 'hyperspherical bottleneck'. In this work we propose to\nextend the usability of hyperspherical parameterizations to higher dimensions\nusing a product-space instead, showing improved results on a selection of image\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 16:59:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Davidson", "Tim R.", ""], ["Tomczak", "Jakub M.", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1910.02919", "submitter": "Manan Tomar Mr.", "authors": "Manan Tomar, Yonathan Efroni, Mohammad Ghavamzadeh", "title": "Multi-step Greedy Reinforcement Learning Algorithms", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step greedy policies have been extensively used in model-based\nreinforcement learning (RL), both when a model of the environment is available\n(e.g.,~in the game of Go) and when it is learned. In this paper, we explore\ntheir benefits in model-free RL, when employed using multi-step dynamic\nprogramming algorithms: $\\kappa$-Policy Iteration ($\\kappa$-PI) and\n$\\kappa$-Value Iteration ($\\kappa$-VI). These methods iteratively compute the\nnext policy ($\\kappa$-PI) and value function ($\\kappa$-VI) by solving a\nsurrogate decision problem with a shaped reward and a smaller discount factor.\nWe derive model-free RL algorithms based on $\\kappa$-PI and $\\kappa$-VI in\nwhich the surrogate problem can be solved by any discrete or continuous action\nRL method, such as DQN and TRPO. We identify the importance of a\nhyper-parameter that controls the extent to which the surrogate problem is\nsolved and suggest a way to set this parameter. When evaluated on a range of\nAtari and MuJoCo benchmark tasks, our results indicate that for the right range\nof $\\kappa$, our algorithms outperform DQN and TRPO. This shows that our\nmulti-step greedy algorithms are general enough to be applied over any existing\nRL algorithm and can significantly improve its performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:20:25 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 17:25:19 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 00:00:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tomar", "Manan", ""], ["Efroni", "Yonathan", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1910.02934", "submitter": "Quanquan Gu", "authors": "Spencer Frei and Yuan Cao and Quanquan Gu", "title": "Algorithm-Dependent Generalization Bounds for Overparameterized Deep\n  Residual Networks", "comments": "37 pages. In NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skip-connections used in residual networks have become a standard\narchitecture choice in deep learning due to the increased training stability\nand generalization performance with this architecture, although there has been\nlimited theoretical understanding for this improvement. In this work, we\nanalyze overparameterized deep residual networks trained by gradient descent\nfollowing random initialization, and demonstrate that (i) the class of networks\nlearned by gradient descent constitutes a small subset of the entire neural\nnetwork function class, and (ii) this subclass of networks is sufficiently\nlarge to guarantee small training error. By showing (i) we are able to\ndemonstrate that deep residual networks trained with gradient descent have a\nsmall generalization gap between training and test error, and together with\n(ii) this guarantees that the test error will be small. Our optimization and\ngeneralization guarantees require overparameterization that is only logarithmic\nin the depth of the network, while all known generalization bounds for deep\nnon-residual networks have overparameterization requirements that are at least\npolynomial in the depth. This provides an explanation for why residual networks\nare preferable to non-residual ones.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:52:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Frei", "Spencer", ""], ["Cao", "Yuan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1910.02935", "submitter": "Aydan Gasimova", "authors": "Aydan Gasimova", "title": "Automated Enriched Medical Concept Generation for Chest X-ray Images", "comments": "MICCAI ML-CDS Workshop 2019", "journal-ref": null, "doi": "10.1007/978-3-030-33850-3_10", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision support tools that rely on supervised learning require large amounts\nof expert annotations. Using past radiological reports obtained from hospital\narchiving systems has many advantages as training data above manual\nsingle-class labels: they are expert annotations available in large quantities,\ncovering a population-representative variety of pathologies, and they provide\nadditional context to pathology diagnoses, such as anatomical location and\nseverity. Learning to auto-generate such reports from images present many\nchallenges such as the difficulty in representing and generating long,\nunstructured textual information, accounting for spelling errors and\nrepetition/redundancy, and the inconsistency across different annotators. We\ntherefore propose to first learn visually-informative medical concepts from raw\nreports, and, using the concept predictions as image annotations, learn to\nauto-generate structured reports directly from images. We validate our approach\non the OpenI [2] chest x-ray dataset, which consists of frontal and lateral\nviews of chest x-ray images, their corresponding raw textual reports and manual\nmedical subject heading (MeSH ) annotations made by radiologists.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:52:37 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gasimova", "Aydan", ""]]}, {"id": "1910.02951", "submitter": "Nicolo' Savioli", "authors": "Shihao Jin, Nicol\\`o Savioli, Antonio de Marvao, Timothy JW Dawes,\n  Axel Gandy, Daniel Rueckert, Declan P O'Regan", "title": "Joint analysis of clinical risk factors and 4D cardiac motion for\n  survival prediction using a hybrid deep learning network", "comments": "4 pages, 2 figures", "journal-ref": "NeurIPS 2019, Medical Imaging meets NIPS", "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel approach is proposed for joint analysis of high\ndimensional time-resolved cardiac motion features obtained from segmented\ncardiac MRI and low dimensional clinical risk factors to improve survival\nprediction in heart failure. Different methods are evaluated to find the\noptimal way to insert conventional covariates into deep prediction networks.\nCorrelation analysis between autoencoder latent codes and covariate features is\nused to examine how these predictors interact. We believe that similar\napproaches could also be used to introduce knowledge of genetic variants to\nsuch survival networks to improve outcome prediction by jointly analysing\ncardiac motion traits with inheritable risk factors.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:04:17 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Jin", "Shihao", ""], ["Savioli", "Nicol\u00f2", ""], ["de Marvao", "Antonio", ""], ["Dawes", "Timothy JW", ""], ["Gandy", "Axel", ""], ["Rueckert", "Daniel", ""], ["O'Regan", "Declan P", ""]]}, {"id": "1910.03002", "submitter": "Michael Bohlke-Schneider", "authors": "David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto\n  Medico, Jan Gasthaus", "title": "High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the dependencies between observations from multiple time series is\ncritical for applications such as anomaly detection, financial risk management,\ncausal analysis, or demand forecasting. However, the computational and\nnumerical difficulties of estimating time-varying and high-dimensional\ncovariance matrices often limits existing methods to handling at most a few\nhundred dimensions or requires making strong assumptions on the dependence\nbetween series. We propose to combine an RNN-based time series model with a\nGaussian copula process output model with a low-rank covariance structure to\nreduce the computational complexity and handle non-Gaussian marginal\ndistributions. This permits to drastically reduce the number of parameters and\nconsequently allows the modeling of time-varying correlations of thousands of\ntime series. We show on several real-world datasets that our method provides\nsignificant accuracy improvements over state-of-the-art baselines and perform\nan ablation study analyzing the contributions of the different components of\nour model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:41:00 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 20:16:30 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Salinas", "David", ""], ["Bohlke-Schneider", "Michael", ""], ["Callot", "Laurent", ""], ["Medico", "Roberto", ""], ["Gasthaus", "Jan", ""]]}, {"id": "1910.03003", "submitter": "Joe Watson", "authors": "Joe Watson, Hany Abdulsamad, Jan Peters", "title": "Stochastic Optimal Control as Approximate Input Inference", "comments": "Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal control of stochastic nonlinear dynamical systems is a major\nchallenge in the domain of robot learning. Given the intractability of the\nglobal control problem, state-of-the-art algorithms focus on approximate\nsequential optimization techniques, that heavily rely on heuristics for\nregularization in order to achieve stable convergence. By building upon the\nduality between inference and control, we develop the view of Optimal Control\nas Input Estimation, devising a probabilistic stochastic optimal control\nformulation that iteratively infers the optimal input distributions by\nminimizing an upper bound of the control cost. Inference is performed through\nExpectation Maximization and message passing on a probabilistic graphical model\nof the dynamical system, and time-varying linear Gaussian feedback controllers\nare extracted from the joint state-action distribution. This perspective\nincorporates uncertainty quantification, effective initialization through\npriors, and the principled regularization inherent to the Bayesian treatment.\nMoreover, it can be shown that for deterministic linearized systems, our\nframework derives the maximum entropy linear quadratic optimal control law. We\nprovide a complete and detailed derivation of our probabilistic approach and\nhighlight its advantages in comparison to other deterministic and probabilistic\nsolvers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:41:52 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 11:36:15 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Watson", "Joe", ""], ["Abdulsamad", "Hany", ""], ["Peters", "Jan", ""]]}, {"id": "1910.03016", "submitter": "Ruosong Wang", "authors": "Simon S. Du and Sham M. Kakade and Ruosong Wang and Lin F. Yang", "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement\n  Learning?", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning methods provide effective means to learn good\nrepresentations. However, is a good representation itself sufficient for sample\nefficient reinforcement learning? This question has largely been studied only\nwith respect to (worst-case) approximation error, in the more classical\napproximate dynamic programming literature. With regards to the statistical\nviewpoint, this question is largely unexplored, and the extant body of\nliterature mainly focuses on conditions which permit sample efficient\nreinforcement learning with little understanding of what are necessary\nconditions for efficient reinforcement learning.\n  This work shows that, from the statistical viewpoint, the situation is far\nsubtler than suggested by the more traditional approximation viewpoint, where\nthe requirements on the representation that suffice for sample efficient RL are\neven more stringent. Our main results provide sharp thresholds for\nreinforcement learning methods, showing that there are hard limitations on what\nconstitutes good function approximation (in terms of the dimensionality of the\nrepresentation), where we focus on natural representational conditions relevant\nto value-based, model-based, and policy-based learning. These lower bounds\nhighlight that having a good (value-based, model-based, or policy-based)\nrepresentation in and of itself is insufficient for efficient reinforcement\nlearning, unless the quality of this approximation passes certain hard\nthresholds. Furthermore, our lower bounds also imply exponential separations on\nthe sample complexity between 1) value-based learning with perfect\nrepresentation and value-based learning with a good-but-not-perfect\nrepresentation, 2) value-based learning and policy-based learning, 3)\npolicy-based learning and supervised learning and 4) reinforcement learning and\nimitation learning.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:04:43 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 04:04:19 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 01:00:13 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 03:31:55 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Du", "Simon S.", ""], ["Kakade", "Sham M.", ""], ["Wang", "Ruosong", ""], ["Yang", "Lin F.", ""]]}, {"id": "1910.03019", "submitter": "Atilim Gunes Baydin", "authors": "Gonzalo Mateo-Garcia, Silviu Oprea, Lewis Smith, Josh\n  Veitch-Michaelis, Guy Schumann, Yarin Gal, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin,\n  Dietmar Backes", "title": "Flood Detection On Low Cost Orbital Hardware", "comments": null, "journal-ref": "Artificial Intelligence for Humanitarian Assistance and Disaster\n  Response Workshop, 33rd Conference on Neural Information Processing Systems\n  (NeurIPS 2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite imaging is a critical technology for monitoring and responding to\nnatural disasters such as flooding. Despite the capabilities of modern\nsatellites, there is still much to be desired from the perspective of first\nresponse organisations like UNICEF. Two main challenges are rapid access to\ndata, and the ability to automatically identify flooded regions in images. We\ndescribe a prototypical flood segmentation system, identifying cloud, water and\nland, that could be deployed on a constellation of small satellites, performing\nprocessing on board to reduce downlink bandwidth by 2 orders of magnitude. We\ntarget PhiSat-1, part of the FSSCAT mission, which is planned to be launched by\nthe European Space Agency (ESA) near the start of 2020 as a proof of concept\nfor this new technology.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 13:29:46 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 12:54:40 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 21:03:29 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mateo-Garcia", "Gonzalo", ""], ["Oprea", "Silviu", ""], ["Smith", "Lewis", ""], ["Veitch-Michaelis", "Josh", ""], ["Schumann", "Guy", ""], ["Gal", "Yarin", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Backes", "Dietmar", ""]]}, {"id": "1910.03025", "submitter": "Hyenkyun Woo", "authors": "Hyenkyun Woo", "title": "Bregman-divergence-guided Legendre exponential dispersion model with\n  finite cumulants (K-LED)", "comments": "21pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT cs.LG math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential dispersion model is a useful framework in machine learning and\nstatistics. Primarily, thanks to the additive structure of the model, it can be\nachieved without difficulty to estimate parameters including mean. However,\ntight conditions on cumulant function, such as analyticity, strict convexity,\nand steepness, reduce the class of exponential dispersion model. In this work,\nwe present relaxed exponential dispersion model K-LED (Legendre exponential\ndispersion model with K cumulants). The cumulant function of the proposed model\nis a convex function of Legendre type having continuous partial derivatives of\nK-th order on the interior of a convex domain. Most of the K-LED models are\ndeveloped via Bregman-divergence-guided log-concave density function with\ncoercivity shape constraints. The main advantage of the proposed model is that\nthe first cumulant (or the mean parameter space) of the 1-LED model is easily\ncomputed through the extended global optimum property of Bregman divergence. An\nextended normal distribution is introduced as an example of 1-LED based on\nTweedie distribution. On top of that, we present 2-LED satisfying mean-variance\nrelation of quasi-likelihood function. There is an equivalence between a\nsubclass of quasi-likelihood function and a regular 2-LED model, of which the\ncanonical parameter space is open. A typical example is a regular 2-LED model\nwith power variance function, i.e., a variance is in proportion to the power of\nthe mean of observations. This model is equivalent to a subclass of\nbeta-divergence (or a subclass of quasi-likelihood function with power variance\nfunction). Furthermore, a new parameterized K-LED model, the cumulant function\nof which is the convex extended logistic loss function, is proposed. This model\nincludes Bernoulli distribution and Poisson distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 11:24:31 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Woo", "Hyenkyun", ""]]}, {"id": "1910.03053", "submitter": "Huaxiu Yao", "authors": "Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, Junzhou\n  Huang, Nitesh V. Chawla, Zhenhui Li", "title": "Graph Few-shot Learning via Knowledge Transfer", "comments": "Full paper (with Appendix) of AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards the challenging problem of semi-supervised node classification, there\nhave been extensive studies. As a frontier, Graph Neural Networks (GNNs) have\naroused great interest recently, which update the representation of each node\nby aggregating information of its neighbors. However, most GNNs have shallow\nlayers with a limited receptive field and may not achieve satisfactory\nperformance especially when the number of labeled nodes is quite small. To\naddress this challenge, we innovatively propose a graph few-shot learning (GFL)\nalgorithm that incorporates prior knowledge learned from auxiliary graphs to\nimprove classification accuracy on the target graph. Specifically, a\ntransferable metric space characterized by a node embedding and a\ngraph-specific prototype embedding function is shared between auxiliary graphs\nand the target, facilitating the transfer of structural knowledge. Extensive\nexperiments and ablation studies on four real-world graph datasets demonstrate\nthe effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:52:11 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 14:46:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yao", "Huaxiu", ""], ["Zhang", "Chuxu", ""], ["Wei", "Ying", ""], ["Jiang", "Meng", ""], ["Wang", "Suhang", ""], ["Huang", "Junzhou", ""], ["Chawla", "Nitesh V.", ""], ["Li", "Zhenhui", ""]]}, {"id": "1910.03072", "submitter": "Alexey Zaytsev", "authors": "I. Fursov, A. Zaytsev, R. Khasyanov, M. Spindler, E. Burnaev", "title": "Sequence embeddings help to identify fraudulent cases in healthcare\n  insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud causes substantial costs and losses for companies and clients in the\nfinance and insurance industries. Examples are fraudulent credit card\ntransactions or fraudulent claims. It has been estimated that roughly $10$\npercent of the insurance industry's incurred losses and loss adjustment\nexpenses each year stem from fraudulent claims. The rise and proliferation of\ndigitization in finance and insurance have lead to big data sets, consisting in\nparticular of text data, which can be used for fraud detection. In this paper,\nwe propose architectures for text embeddings via deep learning, which help to\nimprove the detection of fraudulent claims compared to other machine learning\nmethods. We illustrate our methods using a data set from a large international\nhealth insurance company. The empirical results show that our approach\noutperforms other state-of-the-art methods and can help make the claims\nmanagement process more efficient. As (unstructured) text data become\nincreasingly available to economists and econometricians, our proposed methods\nwill be valuable for many similar applications, particularly when variables\nhave a large number of categories as is typical for example of the\nInternational Classification of Disease (ICD) codes in health economics and\nhealth services.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 20:37:02 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fursov", "I.", ""], ["Zaytsev", "A.", ""], ["Khasyanov", "R.", ""], ["Spindler", "M.", ""], ["Burnaev", "E.", ""]]}, {"id": "1910.03081", "submitter": "Keegan Hines E", "authors": "Antonia Gogoglou and C. Bayan Bruss and Keegan E. Hines", "title": "On the Interpretability and Evaluation of Graph Representation Learning", "comments": "NeurIPS 2019 Graph Representation Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising interest in graph representation learning, a variety of\napproaches have been proposed to effectively capture a graph's properties.\nWhile these approaches have improved performance in graph machine learning\ntasks compared to traditional graph techniques, they are still perceived as\ntechniques with limited insight into the information encoded in these\nrepresentations. In this work, we explore methods to interpret node embeddings\nand propose the creation of a robust evaluation framework for comparing graph\nrepresentation learning algorithms and hyperparameters. We test our methods on\ngraphs with different properties and investigate the relationship between\nembedding training parameters and the ability of the produced embedding to\nrecover the structure of the original graph in a downstream task.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:02:13 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Gogoglou", "Antonia", ""], ["Bruss", "C. Bayan", ""], ["Hines", "Keegan E.", ""]]}, {"id": "1910.03084", "submitter": "Kamran Kowsari", "authors": "Rasoul Sali, Lubaina Ehsan, Kamran Kowsari, Marium Khan, Christopher\n  A. Moskaluk, Sana Syed, Donald E. Brown", "title": "CeliacNet: Celiac Disease Severity Diagnosis on Duodenal\n  Histopathological Images Using Deep Residual Networks", "comments": "accepted at IEEE International Conference on Bioinformatics and\n  Biomedicine (IEEE BIBM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celiac Disease (CD) is a chronic autoimmune disease that affects the small\nintestine in genetically predisposed children and adults. Gluten exposure\ntriggers an inflammatory cascade which leads to compromised intestinal barrier\nfunction. If this enteropathy is unrecognized, this can lead to anemia,\ndecreased bone density, and, in longstanding cases, intestinal cancer. The\nprevalence of the disorder is 1% in the United States. An intestinal (duodenal)\nbiopsy is considered the \"gold standard\" for diagnosis. The mild CD might go\nunnoticed due to non-specific clinical symptoms or mild histologic features. In\nour current work, we trained a model based on deep residual networks to\ndiagnose CD severity using a histological scoring system called the modified\nMarsh score. The proposed model was evaluated using an independent set of 120\nwhole slide images from 15 CD patients and achieved an AUC greater than 0.96 in\nall classes. These results demonstrate the diagnostic power of the proposed\nmodel for CD severity classification using histological images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:06:41 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sali", "Rasoul", ""], ["Ehsan", "Lubaina", ""], ["Kowsari", "Kamran", ""], ["Khan", "Marium", ""], ["Moskaluk", "Christopher A.", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "1910.03090", "submitter": "Fatih Cagatay Akyon", "authors": "Fatih Cagatay Akyon, Esat Kalfaoglu", "title": "Instagram Fake and Automated Account Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fake engagement is one of the significant problems in Online Social Networks\n(OSNs) which is used to increase the popularity of an account in an inorganic\nmanner. The detection of fake engagement is crucial because it leads to loss of\nmoney for businesses, wrong audience targeting in advertising, wrong product\npredictions systems, and unhealthy social network environment. This study is\nrelated with the detection of fake and automated accounts which leads to fake\nengagement on Instagram. Prior to this work, there were no publicly available\ndataset for fake and automated accounts. For this purpose, two datasets have\nbeen published for the detection of fake and automated accounts. For the\ndetection of these accounts, machine learning algorithms like Naive Bayes,\nLogistic Regression, Support Vector Machines and Neural Networks are applied.\nAdditionally, for the detection of automated accounts, cost sensitive genetic\nalgorithm is proposed to handle the unnatural bias in the dataset. To deal with\nthe unevenness problem in the fake dataset, Smote-nc algorithm is implemented.\nFor the automated and fake account detection datasets, 86% and 96%\nclassification accuracies are obtained, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:51:01 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 10:08:35 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Akyon", "Fatih Cagatay", ""], ["Kalfaoglu", "Esat", ""]]}, {"id": "1910.03094", "submitter": "Michael V Sullins", "authors": "Ian A. Kash, Michael Sullins, Katja Hofmann", "title": "Combining No-regret and Q-learning", "comments": "Presented as conference paper at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Regret Minimization (CFR) has found success in settings like\npoker which have both terminal states and perfect recall. We seek to understand\nhow to relax these requirements. As a first step, we introduce a simple\nalgorithm, local no-regret learning (LONR), which uses a Q-learning-like update\nrule to allow learning without terminal states or perfect recall. We prove its\nconvergence for the basic case of MDPs (and limited extensions of them) and\npresent empirical results showing that it achieves last iterate convergence in\na number of settings, most notably NoSDE games, a class of Markov games\nspecifically designed to be challenging to learn where no prior algorithm is\nknown to achieve convergence to a stationary equilibrium even on average.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:13:55 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:58:54 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Kash", "Ian A.", ""], ["Sullins", "Michael", ""], ["Hofmann", "Katja", ""]]}, {"id": "1910.03103", "submitter": "Dilin Wang", "authors": "Dilin Wang, Meng Li, Lemeng Wu, Vikas Chandra, Qiang Liu", "title": "Energy-Aware Neural Architecture Optimization with Fast Splitting\n  Steepest Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing energy-efficient networks is of critical importance for enabling\nstate-of-the-art deep learning in mobile and edge settings where the\ncomputation and energy budgets are highly limited. Recently, Liu et al. (2019)\nframed the search of efficient neural architectures into a continuous splitting\nprocess: it iteratively splits existing neurons into multiple off-springs to\nachieve progressive loss minimization, thus finding novel architectures by\ngradually growing the neural network. However, this method was not specifically\ntailored for designing energy-efficient networks, and is computationally\nexpensive on large-scale benchmarks. In this work, we substantially improve Liu\net al. (2019) in two significant ways: 1) we incorporate the energy cost of\nsplitting different neurons to better guide the splitting process, thereby\ndiscovering more energy-efficient network architectures; 2) we substantially\nspeed up the splitting process of Liu et al. (2019), which requires expensive\neigen-decomposition, by proposing a highly scalable Rayleigh-quotient\nstochastic gradient algorithm. Our fast algorithm allows us to reduce the\ncomputational cost of splitting to the same level of typical back-propagation\nupdates and enables efficient implementation on GPU. Extensive empirical\nresults show that our method can train highly accurate and energy-efficient\nnetworks on challenging datasets such as ImageNet, improving a variety of\nbaselines, including the pruning-based methods and expert-designed\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:45:17 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:20:13 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 20:58:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wang", "Dilin", ""], ["Li", "Meng", ""], ["Wu", "Lemeng", ""], ["Chandra", "Vikas", ""], ["Liu", "Qiang", ""]]}, {"id": "1910.03127", "submitter": "Gabriele Scalia", "authors": "Gabriele Scalia, Colin A. Grambow, Barbara Pernici, Yi-Pei Li, William\n  H. Green", "title": "Evaluating Scalable Uncertainty Estimation Methods for DNN-Based\n  Molecular Property Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep neural network (DNN) based molecular property prediction\nhave recently led to the development of models of remarkable accuracy and\ngeneralization ability, with graph convolution neural networks (GCNNs)\nreporting state-of-the-art performance for this task. However, some challenges\nremain and one of the most important that needs to be fully addressed concerns\nuncertainty quantification. DNN performance is affected by the volume and the\nquality of the training samples. Therefore, establishing when and to what\nextent a prediction can be considered reliable is just as important as\noutputting accurate predictions, especially when out-of-domain molecules are\ntargeted. Recently, several methods to account for uncertainty in DNNs have\nbeen proposed, most of which are based on approximate Bayesian inference. Among\nthese, only a few scale to the large datasets required in applications.\nEvaluating and comparing these methods has recently attracted great interest,\nbut results are generally fragmented and absent for molecular property\nprediction. In this paper, we aim to quantitatively compare scalable techniques\nfor uncertainty estimation in GCNNs. We introduce a set of quantitative\ncriteria to capture different uncertainty aspects, and then use these criteria\nto compare MC-Dropout, deep ensembles, and bootstrapping, both theoretically in\na unified framework that separates aleatoric/epistemic uncertainty and\nexperimentally on the QM9 dataset. Our experiments quantify the performance of\nthe different uncertainty estimation methods and their impact on\nuncertainty-related error reduction. Our findings indicate that ensembling and\nbootstrapping consistently outperform MC-Dropout, with different\ncontext-specific pros and cons. Our analysis also leads to a better\nunderstanding of the role of aleatoric/epistemic uncertainty and highlights the\nchallenge posed by out-of-domain uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:04:43 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Scalia", "Gabriele", ""], ["Grambow", "Colin A.", ""], ["Pernici", "Barbara", ""], ["Li", "Yi-Pei", ""], ["Green", "William H.", ""]]}, {"id": "1910.03131", "submitter": "Moritz Hoffmann", "authors": "Moritz Hoffmann, Frank No\\'e", "title": "Generating valid Euclidean distance matrices", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating point clouds, e.g., molecular structures, in arbitrary rotations,\ntranslations, and enumerations remains a challenging task. Meanwhile, neural\nnetworks utilizing symmetry invariant layers have been shown to be able to\noptimize their training objective in a data-efficient way. In this spirit, we\npresent an architecture which allows to produce valid Euclidean distance\nmatrices, which by construction are already invariant under rotation and\ntranslation of the described object. Motivated by the goal to generate\nmolecular structures in Cartesian space, we use this architecture to construct\na Wasserstein GAN utilizing a permutation invariant critic network. This makes\nit possible to generate molecular structures in a one-shot fashion by producing\nEuclidean distance matrices which have a three-dimensional embedding.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:30:05 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 00:59:42 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Hoffmann", "Moritz", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1910.03134", "submitter": "Javier Zapata", "authors": "Javier Zapata, Sang-Yun Oh, Alexander Petersen", "title": "Partial Separability and Functional Graphical Models for Multivariate\n  Gaussian Processes", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The covariance structure of multivariate functional data can be highly\ncomplex, especially if the multivariate dimension is large, making extension of\nstatistical methods for standard multivariate data to the functional data\nsetting quite challenging. For example, Gaussian graphical models have recently\nbeen extended to the setting of multivariate functional data by applying\nmultivariate methods to the coefficients of truncated basis expansions.\nHowever, a key difficulty compared to multivariate data is that the covariance\noperator is compact, and thus not invertible. The methodology in this paper\naddresses the general problem of covariance modeling for multivariate\nfunctional data, and functional Gaussian graphical models in particular. As a\nfirst step, a new notion of separability for multivariate functional data is\nproposed, termed partial separability, leading to a novel Karhunen-Lo\\`eve-type\nexpansion for such data. Next, the partial separability structure is shown to\nbe particularly useful in order to provide a well-defined Gaussian graphical\nmodel that can be identified with a sequence of finite-dimensional graphical\nmodels, each of fixed dimension. This motivates a simple and efficient\nestimation procedure through application of the joint graphical lasso.\nEmpirical performance of the method for graphical model estimation is assessed\nthrough simulation and analysis of functional brain connectivity during a motor\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:42:21 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:58:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 03:16:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zapata", "Javier", ""], ["Oh", "Sang-Yun", ""], ["Petersen", "Alexander", ""]]}, {"id": "1910.03143", "submitter": "Ryan Cory-Wright", "authors": "Dimitris Bertsimas, Ryan Cory-Wright", "title": "On Polyhedral and Second-Order Cone Decompositions of Semidefinite\n  Optimization Problems", "comments": "Submitted minor revision to Operations Research Letters; removed\n  footnotes and corrected some minor typos in previous version", "journal-ref": "Operations Research Letters 48(1):78-85 (2020)", "doi": "10.1016/j.orl.2019.12.003", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a cutting-plane method for semidefinite optimization problems\n(SDOs), and supply a proof of the method's convergence, under a boundedness\nassumption. By relating the method's rate of convergence to an initial outer\napproximation's diameter, we argue that the method performs well when\ninitialized with a second-order-cone approximation, instead of a linear\napproximation. We invoke the method to provide bound gaps of 0.5-6.5% for\nsparse PCA problems with $1000$s of covariates, and solve nuclear norm problems\nover 500x500 matrices.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 00:33:19 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 20:32:51 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Cory-Wright", "Ryan", ""]]}, {"id": "1910.03155", "submitter": "Jiaheng Wei", "authors": "Jiaheng Wei, Zuyue Fu, Yang Liu, Xingyu Li, Zhuoran Yang, Zhaoran Wang", "title": "Sample Elicitation", "comments": "To appear at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to collect credible training samples $(x,y)$ for building\ndata-intensive learning systems (e.g., a deep learning system). Asking people\nto report complex distribution $p(x)$, though theoretically viable, is\nchallenging in practice. This is primarily due to the cognitive loads required\nfor human agents to form the report of this highly complicated information.\nWhile classical elicitation mechanisms apply to eliciting a complex and\ngenerative (and continuous) distribution $p(x)$, we are interested in eliciting\nsamples $x_i \\sim p(x)$ from agents directly. We coin the above problem \"sample\nelicitation\". This paper introduces a deep learning aided method to incentivize\ncredible sample contributions from self-interested and rational agents. We show\nthat with an accurate estimation of a certain $f$-divergence function we can\nachieve approximate incentive compatibility in eliciting truthful samples. We\nthen present an efficient estimator with theoretical guarantees via studying\nthe variational forms of the $f$-divergence function. We also show a connection\nbetween this sample elicitation problem and $f$-GAN, and how this connection\ncan help reconstruct an estimator of the distribution based on collected\nsamples. Experiments on synthetic data, MNIST, and CIFAR-10 datasets\ndemonstrate that our mechanism elicits truthful samples. Our implementation is\navailable at https://github.com/weijiaheng/Credible-sample-elicitation.git.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:23:11 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 20:38:52 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 09:05:19 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wei", "Jiaheng", ""], ["Fu", "Zuyue", ""], ["Liu", "Yang", ""], ["Li", "Xingyu", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1910.03159", "submitter": "Daniel Barry", "authors": "Daniel Barry and Munir Shah and Merel Keijsers and Humayun Khan and\n  Banon Hopman", "title": "xYOLO: A Model For Real-Time Object Detection In Humanoid Soccer On\n  Low-End Hardware", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of onboard vision processing for areas such as the\ninternet of things (IoT), edge computing and autonomous robots, there is\nincreasing demand for computationally efficient convolutional neural network\n(CNN) models to perform real-time object detection on resource constraints\nhardware devices. Tiny-YOLO is generally considered as one of the faster object\ndetectors for low-end devices and is the basis for our work. Our experiments on\nthis network have shown that Tiny-YOLO can achieve 0.14 frames per second(FPS)\non the Raspberry Pi 3 B, which is too slow for soccer playing autonomous\nhumanoid robots detecting goal and ball objects. In this paper we propose an\nadaptation to the YOLO CNN model named xYOLO, that can achieve object detection\nat a speed of 9.66 FPS on the Raspberry Pi 3 B. This is achieved by trading an\nacceptable amount of accuracy, making the network approximately 70 times faster\nthan Tiny-YOLO. Greater inference speed-ups were also achieved on a desktop CPU\nand GPU. Additionally we contribute an annotated Darknet dataset for goal and\nball detection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:33:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Barry", "Daniel", ""], ["Shah", "Munir", ""], ["Keijsers", "Merel", ""], ["Khan", "Humayun", ""], ["Hopman", "Banon", ""]]}, {"id": "1910.03175", "submitter": "Micha Livne", "authors": "Micha Livne, Kevin Swersky, David J. Fleet", "title": "MIM: Mutual Information Machine", "comments": "Pre-print. Project webpage:\n  https://research.seraphlabs.ca/projects/mim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Mutual Information Machine (MIM), a probabilistic\nauto-encoder for learning joint distributions over observations and latent\nvariables. MIM reflects three design principles: 1) low divergence, to\nencourage the encoder and decoder to learn consistent factorizations of the\nsame underlying distribution; 2) high mutual information, to encourage an\ninformative relation between data and latent variables; and 3) low marginal\nentropy, or compression, which tends to encourage clustered latent\nrepresentations. We show that a combination of the Jensen-Shannon divergence\nand the joint entropy of the encoding and decoding distributions satisfies\nthese criteria, and admits a tractable cross-entropy bound that can be\noptimized directly with Monte Carlo and stochastic gradient descent. We\ncontrast MIM learning with maximum likelihood and VAEs. Experiments show that\nMIM learns representations with high mutual information, consistent encoding\nand decoding distributions, effective latent clustering, and data log\nlikelihood comparable to VAE, while avoiding posterior collapse.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 02:31:29 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 16:44:24 GMT"}, {"version": "v3", "created": "Sun, 15 Dec 2019 01:40:54 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 05:09:28 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2020 15:45:19 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Livne", "Micha", ""], ["Swersky", "Kevin", ""], ["Fleet", "David J.", ""]]}, {"id": "1910.03177", "submitter": "Rajeev Bhatt Ambati", "authors": "Rajeev Bhatt Ambati, Saptarashmi Bandyopadhyay and Prasenjit Mitra", "title": "Read, Highlight and Summarize: A Hierarchical Neural Semantic\n  Encoder-based Approach", "comments": "Submitted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional sequence-to-sequence (seq2seq) models and other variations of the\nattention-mechanism such as hierarchical attention have been applied to the\ntext summarization problem. Though there is a hierarchy in the way humans use\nlanguage by forming paragraphs from sentences and sentences from words,\nhierarchical models have usually not worked that much better than their\ntraditional seq2seq counterparts. This effect is mainly because either the\nhierarchical attention mechanisms are too sparse using hard attention or noisy\nusing soft attention. In this paper, we propose a method based on extracting\nthe highlights of a document; a key concept that is conveyed in a few\nsentences. In a typical text summarization dataset consisting of documents that\nare 800 tokens in length (average), capturing long-term dependencies is very\nimportant, e.g., the last sentence can be grouped with the first sentence of a\ndocument to form a summary. LSTMs (Long Short-Term Memory) proved useful for\nmachine translation. However, they often fail to capture long-term dependencies\nwhile modeling long sequences. To address these issues, we have adapted Neural\nSemantic Encoders (NSE) to text summarization, a class of memory-augmented\nneural networks by improving its functionalities and proposed a novel\nhierarchical NSE that outperforms similar previous models significantly. The\nquality of summarization was improved by augmenting linguistic factors, namely\nlemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved\nvocabulary coverage and generalization. The hierarchical NSE model on factored\ndataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further\ndesigned and used the first GPU-based self-critical Reinforcement Learning\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 02:36:02 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 04:54:54 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ambati", "Rajeev Bhatt", ""], ["Bandyopadhyay", "Saptarashmi", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1910.03188", "submitter": "Rahul-Vigneswaran K", "authors": "Rahul-Vigneswaran K, Sachin-Kumar S, Neethu Mohan, Soman KP", "title": "Dynamic Mode Decomposition based feature for Image Classification", "comments": "Selected for Spotlight presentation at TENCON 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Irrespective of the fact that Machine learning has produced groundbreaking\nresults, it demands an enormous amount of data in order to perform so. Even\nthough data production has been in its all-time high, almost all the data is\nunlabelled, hence making them unsuitable for training the algorithms. This\npaper proposes a novel method of extracting the features using Dynamic Mode\nDecomposition (DMD). The experiment is performed using data samples from\nImagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink\napproach (RKS). The results have shown that DMD features with RKS give\ncompeting results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:09:42 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["K", "Rahul-Vigneswaran", ""], ["S", "Sachin-Kumar", ""], ["Mohan", "Neethu", ""], ["KP", "Soman", ""]]}, {"id": "1910.03193", "submitter": "Lu Lu", "authors": "Lu Lu, Pengzhan Jin, George Em Karniadakis", "title": "DeepONet: Learning nonlinear operators for identifying differential\n  equations based on the universal approximation theorem of operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While it is widely known that neural networks are universal approximators of\ncontinuous functions, a less known and perhaps more powerful result is that a\nneural network with a single hidden layer can approximate accurately any\nnonlinear continuous operator. This universal approximation theorem is\nsuggestive of the potential application of neural networks in learning\nnonlinear operators from data. However, the theorem guarantees only a small\napproximation error for a sufficient large network, and does not consider the\nimportant optimization and generalization errors. To realize this theorem in\npractice, we propose deep operator networks (DeepONets) to learn operators\naccurately and efficiently from a relatively small dataset. A DeepONet consists\nof two sub-networks, one for encoding the input function at a fixed number of\nsensors $x_i, i=1,\\dots,m$ (branch net), and another for encoding the locations\nfor the output functions (trunk net). We perform systematic simulations for\nidentifying two types of operators, i.e., dynamic systems and partial\ndifferential equations, and demonstrate that DeepONet significantly reduces the\ngeneralization error compared to the fully-connected networks. We also derive\ntheoretically the dependence of the approximation error in terms of the number\nof sensors (where the input function is defined) as well as the input function\ntype, and we verify the theorem with computational results. More importantly,\nwe observe high-order error convergence in our computational tests, namely\npolynomial rates (from half order to fourth order) and even exponential\nconvergence with respect to the training dataset size.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:21:14 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 02:31:17 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 00:51:54 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Lu", "Lu", ""], ["Jin", "Pengzhan", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1910.03197", "submitter": "Wei Liu", "authors": "Wei Liu, Li Chen, Yunfei Chen and Wenyi Zhang", "title": "Accelerating Federated Learning via Momentum Gradient Descent", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) provides a communication-efficient approach to solve\nmachine learning problems concerning distributed data, without sending raw data\nto a central server. However, existing works on FL only utilize first-order\ngradient descent (GD) and do not consider the preceding iterations to gradient\nupdate which can potentially accelerate convergence. In this paper, we consider\nmomentum term which relates to the last iteration. The proposed momentum\nfederated learning (MFL) uses momentum gradient descent (MGD) in the local\nupdate step of FL system. We establish global convergence properties of MFL and\nderive an upper bound on MFL convergence rate. Comparing the upper bounds on\nMFL and FL convergence rate, we provide conditions in which MFL accelerates the\nconvergence. For different machine learning models, the convergence performance\nof MFL is evaluated based on experiments with MNIST dataset. Simulation results\ncomfirm that MFL is globally convergent and further reveal significant\nconvergence improvement over FL.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:42:58 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:16:34 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Li", ""], ["Chen", "Yunfei", ""], ["Zhang", "Wenyi", ""]]}, {"id": "1910.03201", "submitter": "Yongjin Lee", "authors": "Yognjin Lee", "title": "Differentiable Sparsification for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have relieved a great deal of burden on human experts in\nrelation to feature engineering. However, comparable efforts are instead\nrequired to determine effective architectures. In addition, as the sizes of\nnetworks have grown overly large, a considerable amount of resources is also\ninvested in reducing the sizes. The sparsification of an over-complete model\naddresses these problems as it removes redundant components and connections. In\nthis study, we propose a fully differentiable sparsification method for deep\nneural networks which allows parameters to be zero during training via\nstochastic gradient descent. Thus, the proposed method can learn the sparsified\nstructure and weights of a network in an end-to-end manner. The method is\ndirectly applicable to various modern deep neural networks and imposes minimum\nmodification to existing models. To the best of our knowledge, this is the\nfirst fully [sub-]differentiable sparsification method that zeroes out\nparameters. It provides a foundation for future structure learning and model\ncompression methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:57:04 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 05:29:13 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 02:38:35 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 01:40:03 GMT"}, {"version": "v5", "created": "Thu, 1 Jul 2021 09:42:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lee", "Yognjin", ""]]}, {"id": "1910.03203", "submitter": "Amanda Kowalczyk", "authors": "Zijian Gao and Amanda Kowalczyk", "title": "Random forest model identifies serve strength as a key predictor of\n  tennis match outcome", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tennis is a popular sport worldwide, boasting millions of fans and numerous\nnational and international tournaments. Like many sports, tennis has benefitted\nfrom the popularity of rigorous record-keeping of game and player information,\nas well as the growth of machine learning methods for use in sports analytics.\nOf particular interest to bettors and betting companies alike is potential use\nof sports records to predict tennis match outcomes prior to match start. We\ncompiled, cleaned, and used the largest database of tennis match information to\ndate to predict match outcome using fairly simple machine learning methods.\nUsing such methods allows for rapid fit and prediction times to readily\nincorporate new data and make real-time predictions. We were able to predict\nmatch outcomes with upwards of 80% accuracy, much greater than predictions\nusing betting odds alone, and identify serve strength as a key predictor of\nmatch outcome. By combining prediction accuracies from three models, we were\nable to nearly recreate a probability distribution based on average betting\nodds from betting companies, which indicates that betting companies are using\nsimilar information to assign odds to matches. These results demonstrate the\ncapability of relatively simple machine learning models to quite accurately\npredict tennis match outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 04:05:59 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Gao", "Zijian", ""], ["Kowalczyk", "Amanda", ""]]}, {"id": "1910.03225", "submitter": "Alejandro Schuler", "authors": "Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu,\n  Andrew Y. Ng, Alejandro Schuler", "title": "NGBoost: Natural Gradient Boosting for Probabilistic Prediction", "comments": "Accepted for ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Natural Gradient Boosting (NGBoost), an algorithm for generic\nprobabilistic prediction via gradient boosting. Typical regression models\nreturn a point estimate, conditional on covariates, but probabilistic\nregression models output a full probability distribution over the outcome\nspace, conditional on the covariates. This allows for predictive uncertainty\nestimation -- crucial in applications like healthcare and weather forecasting.\nNGBoost generalizes gradient boosting to probabilistic regression by treating\nthe parameters of the conditional distribution as targets for a multiparameter\nboosting algorithm. Furthermore, we show how the Natural Gradient is required\nto correct the training dynamics of our multiparameter boosting approach.\nNGBoost can be used with any base learner, any family of distributions with\ncontinuous parameters, and any scoring rule. NGBoost matches or exceeds the\nperformance of existing methods for probabilistic prediction while offering\nadditional benefits in flexibility, scalability, and usability. An open-source\nimplementation is available at github.com/stanfordmlgroup/ngboost.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 06:07:13 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 21:56:50 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 20:52:49 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 17:25:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Duan", "Tony", ""], ["Avati", "Anand", ""], ["Ding", "Daisy Yi", ""], ["Thai", "Khanh K.", ""], ["Basu", "Sanjay", ""], ["Ng", "Andrew Y.", ""], ["Schuler", "Alejandro", ""]]}, {"id": "1910.03231", "submitter": "Yang Liu", "authors": "Yang Liu and Hongyi Guo", "title": "Peer Loss Functions: Learning from Noisy Labels without Knowing Noise\n  Rates", "comments": "ICML 2020. Corrected typos in the Appendix", "journal-ref": "International Conference on Machine Learning, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with noisy labels is a common challenge in supervised learning.\nExisting approaches often require practitioners to specify noise rates, i.e., a\nset of parameters controlling the severity of label noises in the problem, and\nthe specifications are either assumed to be given or estimated using additional\nsteps. In this work, we introduce a new family of loss functions that we name\nas peer loss functions, which enables learning from noisy labels and does not\nrequire a priori specification of the noise rates. Peer loss functions work\nwithin the standard empirical risk minimization (ERM) framework. We show that,\nunder mild conditions, performing ERM with peer loss functions on the noisy\ndataset leads to the optimal or a near-optimal classifier as if performing ERM\nover the clean training data, which we do not have access to. We pair our\nresults with an extensive set of experiments. Peer loss provides a way to\nsimplify model development when facing potentially noisy training labels, and\ncan be promoted as a robust candidate loss function in such situations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 06:34:11 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 21:57:23 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 04:32:34 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 08:23:27 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2020 19:05:39 GMT"}, {"version": "v6", "created": "Mon, 29 Jun 2020 20:55:13 GMT"}, {"version": "v7", "created": "Fri, 14 Aug 2020 20:28:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Yang", ""], ["Guo", "Hongyi", ""]]}, {"id": "1910.03276", "submitter": "Michela Moschella", "authors": "Michela Moschella, Mauro Tucci, Emanuele Crisostomi, and Alessandro\n  Betti", "title": "A Machine Learning Model for Long-Term Power Generation Forecasting at\n  Bidding Zone Level", "comments": "Paper presented at IEEE PES ISGT 2019 Conference (29 Sept - 2 Oct,\n  Bucharest, Romania)", "journal-ref": "2019 IEEE PES Innovative Smart Grid Technologies Europe\n  (ISGT-Europe)", "doi": "10.1109/ISGTEurope.2019.8905453", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing penetration level of energy generation from renewable sources\nis demanding for more accurate and reliable forecasting tools to support\nclassic power grid operations (e.g., unit commitment, electricity market\nclearing or maintenance planning). For this purpose, many physical models have\nbeen employed, and more recently many statistical or machine learning\nalgorithms, and data-driven methods in general, are becoming subject of intense\nresearch. While generally the power research community focuses on power\nforecasting at the level of single plants, in a short future horizon of time,\nin this time we are interested in aggregated macro-area power generation (i.e.,\nin a territory of size greater than 100000 km^2) with a future horizon of\ninterest up to 15 days ahead. Real data are used to validate the proposed\nforecasting methodology on a test set of several months.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:45:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:04:20 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Moschella", "Michela", ""], ["Tucci", "Mauro", ""], ["Crisostomi", "Emanuele", ""], ["Betti", "Alessandro", ""]]}, {"id": "1910.03344", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios", "title": "The Universal Approximation Property", "comments": null, "journal-ref": "Annals of Mathematics and Artificial Intelligence, 2020", "doi": "10.1007/s10472-020-09723-1", "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The universal approximation property of various machine learning models is\ncurrently only understood on a case-by-case basis, limiting the rapid\ndevelopment of new theoretically justified neural network architectures and\nblurring our understanding of our current models' potential. This paper works\ntowards overcoming these challenges by presenting a characterization, a\nrepresentation, a construction method, and an existence result, each of which\napplies to any universal approximator on most function spaces of practical\ninterest. Our characterization result is used to describe which activation\nfunctions allow the feed-forward architecture to maintain its universal\napproximation capabilities when multiple constraints are imposed on its final\nlayers and its remaining layers are only sparsely connected. These include a\nrescaled and shifted Leaky ReLU activation function but not the ReLU activation\nfunction. Our construction and representation result is used to exhibit a\nsimple modification of the feed-forward architecture, which can approximate any\ncontinuous function with non-pathological growth, uniformly on the entire\nEuclidean input space. This improves the known capabilities of the feed-forward\narchitecture.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 11:30:33 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 09:20:21 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 20:52:07 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 11:17:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kratsios", "Anastasis", ""]]}, {"id": "1910.03349", "submitter": "Jessica Dafflon", "authors": "Jessica Dafflon, Walter H.L Pinaya, Federico Turkheimer, James H.\n  Cole, Robert Leech, Mathew A. Harris, Simon R. Cox, Heather C. Whalley,\n  Andrew M. McIntosh, Peter J. Hellyer", "title": "Analysis of an Automated Machine Learning Approach in Brain Predictive\n  Modelling: A data-driven approach to Predict Brain Age from Cortical\n  Anatomical Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning (ML) algorithms has significantly increased in\nneuroscience. However, from the vast extent of possible ML algorithms, which\none is the optimal model to predict the target variable? What are the\nhyperparameters for such a model? Given the plethora of possible answers to\nthese questions, in the last years, automated machine learning (autoML) has\nbeen gaining attention. Here, we apply an autoML library called TPOT which uses\na tree-based representation of machine learning pipelines and conducts a\ngenetic-programming based approach to find the model and its hyperparameters\nthat more closely predicts the subject's true age. To explore autoML and\nevaluate its efficacy within neuroimaging datasets, we chose a problem that has\nbeen the focus of previous extensive study: brain age prediction. Without any\nprior knowledge, TPOT was able to scan through the model space and create\npipelines that outperformed the state-of-the-art accuracy for Freesurfer-based\nmodels using only thickness and volume information for anatomical structure. In\nparticular, we compared the performance of TPOT (mean accuracy error (MAE):\n$4.612 \\pm .124$ years) and a Relevance Vector Regression (MAE $5.474 \\pm .140$\nyears). TPOT also suggested interesting combinations of models that do not\nmatch the current most used models for brain prediction but generalise well to\nunseen data. AutoML showed promising results as a data-driven approach to find\noptimal models for neuroimaging applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 11:54:43 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Dafflon", "Jessica", ""], ["Pinaya", "Walter H. L", ""], ["Turkheimer", "Federico", ""], ["Cole", "James H.", ""], ["Leech", "Robert", ""], ["Harris", "Mathew A.", ""], ["Cox", "Simon R.", ""], ["Whalley", "Heather C.", ""], ["McIntosh", "Andrew M.", ""], ["Hellyer", "Peter J.", ""]]}, {"id": "1910.03358", "submitter": "David Hoeller", "authors": "Farbod Farshidian, David Hoeller, Marco Hutter", "title": "Deep Value Model Predictive Control", "comments": "Accepted for publication in the Conference on Robotic Learning (CoRL)\n  2019, Osaka. 10 pages (+5 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an actor-critic algorithm called Deep Value Model\nPredictive Control (DMPC), which combines model-based trajectory optimization\nwith value function estimation. The DMPC actor is a Model Predictive Control\n(MPC) optimizer with an objective function defined in terms of a value function\nestimated by the critic. We show that our MPC actor is an importance sampler,\nwhich minimizes an upper bound of the cross-entropy to the state distribution\nof the optimal sampling policy. In our experiments with a Ballbot system, we\nshow that our algorithm can work with sparse and binary reward signals to\nefficiently solve obstacle avoidance and target reaching tasks. Compared to\nprevious work, we show that including the value function in the running cost of\nthe trajectory optimizer speeds up the convergence. We also discuss the\nnecessary strategies to robustify the algorithm in practice.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 12:19:50 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Farshidian", "Farbod", ""], ["Hoeller", "David", ""], ["Hutter", "Marco", ""]]}, {"id": "1910.03374", "submitter": "Dan Garber", "authors": "Dan Garber, Ben Kretzu", "title": "Improved Regret Bounds for Projection-free Bandit Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the challenge of designing online algorithms for the bandit convex\noptimization problem (BCO) which are also scalable to high dimensional\nproblems. Hence, we consider algorithms that are \\textit{projection-free},\ni.e., based on the conditional gradient method whose only access to the\nfeasible decision set, is through a linear optimization oracle (as opposed to\nother methods which require potentially much more computationally-expensive\nsubprocedures, such as computing Euclidean projections). We present the first\nsuch algorithm that attains $O(T^{3/4})$ expected regret using only $O(T)$\noverall calls to the linear optimization oracle, in expectation, where $T$ is\nthe number of prediction rounds. This improves over the $O(T^{4/5})$ expected\nregret bound recently obtained by \\cite{Karbasi19}, and actually matches the\ncurrent best regret bound for projection-free online learning in the\n\\textit{full information} setting.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 13:04:59 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Garber", "Dan", ""], ["Kretzu", "Ben", ""]]}, {"id": "1910.03434", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Marcus de Carvalho, Renchunzi Xie, Edwin Lughofer\n  and Jie Lu", "title": "ATL: Autonomous Knowledge Transfer from Many Streaming Processes", "comments": "This paper has been accepted for publication in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transferring knowledge across many streaming processes remains an uncharted\nterritory in the existing literature and features unique characteristics: no\nlabelled instance of the target domain, covariate shift of source and target\ndomain, different period of drifts in the source and target domains. Autonomous\ntransfer learning (ATL) is proposed in this paper as a flexible deep learning\napproach for the online unsupervised transfer learning problem across many\nstreaming processes. ATL offers an online domain adaptation strategy via the\ngenerative and discriminative phases coupled with the KL divergence based\noptimization strategy to produce a domain invariant network while putting\nforward an elastic network structure. It automatically evolves its network\nstructure from scratch with/without the presence of ground truth to overcome\nindependent concept drifts in the source and target domain. The rigorous\nnumerical evaluation has been conducted along with a comparison against\nrecently published works. ATL demonstrates improved performance while showing\nsignificantly faster training speed than its counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 14:54:30 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 05:41:29 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Pratama", "Mahardhika", ""], ["de Carvalho", "Marcus", ""], ["Xie", "Renchunzi", ""], ["Lughofer", "Edwin", ""], ["Lu", "Jie", ""]]}, {"id": "1910.03437", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Choiru Za'in, Andri Ashfahani, Yew Soon Ong and\n  Weiping Ding", "title": "Automatic Construction of Multi-layer Perceptron Network from Streaming\n  Examples", "comments": "This paper has been accepted for publication in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous construction of deep neural network (DNNs) is desired for data\nstreams because it potentially offers two advantages: proper model's capacity\nand quick reaction to drift and shift. While the self-organizing mechanism of\nDNNs remains an open issue, this task is even more challenging to be developed\nfor standard multi-layer DNNs than that using the different-depth structures,\nbecause the addition of a new layer results in information loss of previously\ntrained knowledge. A Neural Network with Dynamically Evolved Capacity (NADINE)\nis proposed in this paper. NADINE features a fully open structure where its\nnetwork structure, depth and width, can be automatically evolved from scratch\nin an online manner and without the use of problem-specific thresholds. NADINE\nis structured under a standard MLP architecture and the catastrophic forgetting\nissue during the hidden layer addition phase is resolved using the proposal of\nsoft-forgetting and adaptive memory methods. The advantage of NADINE, namely\nelastic structure and online learning trait, is numerically validated using\nnine data stream classification and regression problems where it demonstrates\nperformance improvement over prominent algorithms in all problems. In addition,\nit is capable of dealing with data stream regression and classification\nproblems equally well.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 14:55:28 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 12:31:01 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Za'in", "Choiru", ""], ["Ashfahani", "Andri", ""], ["Ong", "Yew Soon", ""], ["Ding", "Weiping", ""]]}, {"id": "1910.03466", "submitter": "Paul Kantor", "authors": "Vicki Bier, Paul B. Kantor, Gary Lupyan, Xiaojin Zhu", "title": "Can We Distinguish Machine Learning from Human Learning?", "comments": "14pp. 5 fig. Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a task relatively more or less difficult for a machine compared to\na human? Much AI/ML research has focused on expanding the range of tasks that\nmachines can do, with a focus on whether machines can beat humans. Allowing for\ndifferences in scale, we can seek interesting (anomalous) pairs of tasks T, T'.\nWe define interesting in this way: The \"harder to learn\" relation is reversed\nwhen comparing human intelligence (HI) to AI. While humans seems to be able to\nunderstand problems by formulating rules, ML using neural networks does not\nrely on constructing rules. We discuss a novel approach where the challenge is\nto \"perform well under rules that have been created by human beings.\" We\nsuggest that this provides a rigorous and precise pathway for understanding the\ndifference between the two kinds of learning. Specifically, we suggest a large\nand extensible class of learning tasks, formulated as learning under rules.\nWith these tasks, both the AI and HI will be studied with rigor and precision.\nThe immediate goal is to find interesting groundtruth rule pairs. In the long\nterm, the goal will be to understand, in a generalizable way, what\ndistinguishes interesting pairs from ordinary pairs, and to define saliency\nbehind interesting pairs. This may open new ways of thinking about AI, and\nprovide unexpected insights into human learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:37:03 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Bier", "Vicki", ""], ["Kantor", "Paul B.", ""], ["Lupyan", "Gary", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1910.03467", "submitter": "Ngo Thi-Vinh", "authors": "Thi-Vinh Ngo, Thanh-Le Ha, Phuong-Thai Nguyen, Le-Minh Nguyen", "title": "Overcoming the Rare Word Problem for Low-Resource Language Pairs in\n  Neural Machine Translation", "comments": null, "journal-ref": "Proceedings of the 6th Workshop on Asian Translation, WAT 2019", "doi": "10.18653/v1/D19-5228", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the six challenges of neural machine translation (NMT) coined by (Koehn\nand Knowles, 2017), rare-word problem is considered the most severe one,\nespecially in translation of low-resource languages. In this paper, we propose\nthree solutions to address the rare words in neural machine translation\nsystems. First, we enhance source context to predict the target words by\nconnecting directly the source embeddings to the output of the attention\ncomponent in NMT. Second, we propose an algorithm to learn morphology of\nunknown words for English in supervised way in order to minimize the adverse\neffect of rare-word problem. Finally, we exploit synonymous relation from the\nWordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our\napproaches on two low-resource language pairs: English-Vietnamese and\nJapanese-Vietnamese. In our experiments, we have achieved significant\nimprovements of up to roughly +1.0 BLEU points in both language pairs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:11:13 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:02:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ngo", "Thi-Vinh", ""], ["Ha", "Thanh-Le", ""], ["Nguyen", "Phuong-Thai", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "1910.03468", "submitter": "Matteo Terzi", "authors": "Matteo Terzi, Gian Antonio Susto and Pratik Chaudhari", "title": "Directional Adversarial Training for Cost Sensitive Deep Learning\n  Classification Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications of Machine Learning it is of paramount\nimportance not only to provide accurate predictions, but also to ensure certain\nlevels of robustness. Adversarial Training is a training procedure aiming at\nproviding models that are robust to worst-case perturbations around predefined\npoints. Unfortunately, one of the main issues in adversarial training is that\nrobustness w.r.t. gradient-based attackers is always achieved at the cost of\nprediction accuracy. In this paper, a new algorithm, called Wasserstein\nProjected Gradient Descent (WPGD), for adversarial training is proposed. WPGD\nprovides a simple way to obtain cost-sensitive robustness, resulting in a finer\ncontrol of the robustness-accuracy trade-off. Moreover, WPGD solves an optimal\ntransport problem on the output space of the network and it can efficiently\ndiscover directions where robustness is required, allowing to control the\ndirectional trade-off between accuracy and robustness. The proposed WPGD is\nvalidated in this work on image recognition tasks with different benchmark\ndatasets and architectures. Moreover, real world-like datasets are often\nunbalanced: this paper shows that when dealing with such type of datasets, the\nperformance of adversarial training are mainly affected in term of standard\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:40:09 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""], ["Chaudhari", "Pratik", ""]]}, {"id": "1910.03471", "submitter": "Dominik Schmidt", "authors": "Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher,\n  Daniel Durstewitz", "title": "Identifying nonlinear dynamical systems with multiple time scales and\n  long-range dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main theoretical interest in biology and physics is to identify the\nnonlinear dynamical system (DS) that generated observed time series. Recurrent\nNeural Networks (RNNs) are, in principle, powerful enough to approximate any\nunderlying DS, but in their vanilla form suffer from the exploding vs.\nvanishing gradients problem. Previous attempts to alleviate this problem\nresulted either in more complicated, mathematically less tractable RNN\narchitectures, or strongly limited the dynamical expressiveness of the RNN.\nHere we address this issue by suggesting a simple regularization scheme for\nvanilla RNNs with ReLU activation which enables them to solve long-range\ndependency problems and express slow time scales, while retaining a simple\nmathematical structure which makes their DS properties partly analytically\naccessible. We prove two theorems that establish a tight connection between the\nregularized RNN dynamics and its gradients, illustrate on DS benchmarks that\nour regularization approach strongly eases the reconstruction of DS which\nharbor widely differing time scales, and show that our method is also en par\nwith other long-range architectures like LSTMs on several tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:41:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 08:55:31 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 15:33:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Schmidt", "Dominik", ""], ["Koppe", "Georgia", ""], ["Monfared", "Zahra", ""], ["Beutelspacher", "Max", ""], ["Durstewitz", "Daniel", ""]]}, {"id": "1910.03474", "submitter": "Manish Munikar", "authors": "Manish Munikar, Sushil Shakya, Aakash Shrestha", "title": "Fine-grained Sentiment Classification using BERT", "comments": "Submitted to IEEE International Conference on Artificial Intelligence\n  for Transforming Business and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification is an important process in understanding people's\nperception towards a product, service, or topic. Many natural language\nprocessing models have been proposed to solve the sentiment classification\nproblem. However, most of them have focused on binary sentiment classification.\nIn this paper, we use a promising deep learning model called BERT to solve the\nfine-grained sentiment classification task. Experiments show that our model\noutperforms other popular models for this task without sophisticated\narchitecture. We also demonstrate the effectiveness of transfer learning in\nnatural language processing in the process.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:20:48 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Munikar", "Manish", ""], ["Shakya", "Sushil", ""], ["Shrestha", "Aakash", ""]]}, {"id": "1910.03483", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli, Herwig Wendt", "title": "Learning event representations for temporal segmentation of image\n  sequences by dynamic graph embedding", "comments": "Accepted in IEEE Transactions on Image Processing, 2020. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-supervised learning has proved to be effective to learn\nrepresentations of events suitable for temporal segmentation in image\nsequences, where events are understood as sets of temporally adjacent images\nthat are semantically perceived as a whole. However, although this approach\ndoes not require expensive manual annotations, it is data hungry and suffers\nfrom domain adaptation problems. As an alternative, in this work, we propose a\nnovel approach for learning event representations named Dynamic Graph Embedding\n(DGE). The assumption underlying our model is that a sequence of images can be\nrepresented by a graph that encodes both semantic and temporal similarity. The\nkey novelty of DGE is to learn jointly the graph and its graph embedding. At\nits core, DGE works by iterating over two steps: 1) updating the graph\nrepresenting the semantic and temporal similarity of the data based on the\ncurrent data representation, and 2) updating the data representation to take\ninto account the current data graph structure. The main advantage of DGE over\nstate-of-the-art self-supervised approaches is that it does not require any\ntraining set, but instead learns iteratively from the data itself a\nlow-dimensional embedding that reflects their temporal and semantic similarity.\nExperimental results on two benchmark datasets of real image sequences captured\nat regular time intervals demonstrate that the proposed DGE leads to event\nrepresentations effective for temporal segmentation. In particular, it achieves\nrobust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark\ndatasets, outperforming the state of the art. Additional experiments on two\nHuman Motion Segmentation benchmark datasets demonstrate the generalization\ncapabilities of the proposed DGE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:48:50 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:42:25 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 11:03:37 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Wendt", "Herwig", ""]]}, {"id": "1910.03487", "submitter": "Minmin Shen", "authors": "Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek\n  Sethi, Angeliki Metallinou", "title": "Controlled Text Generation for Data Augmentation in Intelligent\n  Artificial Agents", "comments": "EMNLP WNGT workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data availability is a bottleneck during early stages of development of new\ncapabilities for intelligent artificial agents. We investigate the use of text\ngeneration techniques to augment the training data of a popular commercial\nartificial agent across categories of functionality, with the goal of faster\ndevelopment of new functionality. We explore a variety of encoder-decoder\ngenerative models for synthetic training data generation and propose using\nconditional variational auto-encoders. Our approach requires only direct\noptimization, works well with limited data and significantly outperforms the\nprevious controlled text generation techniques. Further, the generated data are\nused as additional training samples in an extrinsic intent classification task,\nleading to improved performance by up to 5\\% absolute f-score in low-resource\ncases, validating the usefulness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 20:44:21 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Malandrakis", "Nikolaos", ""], ["Shen", "Minmin", ""], ["Goyal", "Anuj", ""], ["Gao", "Shuyang", ""], ["Sethi", "Abhishek", ""], ["Metallinou", "Angeliki", ""]]}, {"id": "1910.03492", "submitter": "Dan Busbridge", "authors": "Joseph Enguehard, Dan Busbridge, Vitalii Zhelezniak, Nils Hammerla", "title": "Neural Language Priors", "comments": "4 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of sentence encoder architecture reflects assumptions about how a\nsentence's meaning is composed from its constituent words. We examine the\ncontribution of these architectures by holding them randomly initialised and\nfixed, effectively treating them as as hand-crafted language priors, and\nevaluating the resulting sentence encoders on downstream language tasks. We\nfind that even when encoders are presented with additional information that can\nbe used to solve tasks, the corresponding priors do not leverage this\ninformation, except in an isolated case. We also find that apparently\nuninformative priors are just as good as seemingly informative priors on almost\nall tasks, indicating that learning is a necessary component to leverage\ninformation provided by architecture choice.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:44:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Enguehard", "Joseph", ""], ["Busbridge", "Dan", ""], ["Zhelezniak", "Vitalii", ""], ["Hammerla", "Nils", ""]]}, {"id": "1910.03497", "submitter": "Seyed Amjad Seyedi", "authors": "Seyed Amjad Seyedi, S.Siamak Ghodsi, Fardin Akhlaghian, Mahdi Jalili,\n  Parham Moradi", "title": "Self-Paced Multi-Label Learning with Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The major challenge of learning from multi-label data has arisen from the\noverwhelming size of label space which makes this problem NP-hard. This problem\ncan be alleviated by gradually involving easy to hard tags into the learning\nprocess. Besides, the utilization of a diversity maintenance approach avoids\noverfitting on a subset of easy labels. In this paper, we propose a self-paced\nmulti-label learning with diversity (SPMLD) which aims to cover diverse labels\nwith respect to its learning pace. In addition, the proposed framework is\napplied to an efficient correlation-based multi-label method. The non-convex\nobjective function is optimized by an extension of the block coordinate descent\nalgorithm. Empirical evaluations on real-world datasets with different\ndimensions of features and labels imply the effectiveness of the proposed\npredictive model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:59:57 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Seyedi", "Seyed Amjad", ""], ["Ghodsi", "S. Siamak", ""], ["Akhlaghian", "Fardin", ""], ["Jalili", "Mahdi", ""], ["Moradi", "Parham", ""]]}, {"id": "1910.03498", "submitter": "Dominique Mercier", "authors": "Dominique Mercier, Akansha Bhardwaj, Andreas Dengel, Sheraz Ahmed", "title": "SentiCite: An Approach for Publication Sentiment Analysis", "comments": "Preprint, 8 pages, 2 figures, 10th International Conference on Agents\n  and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in the number of scientific publications, year after\nyear, it is becoming increasingly difficult to identify quality authoritative\nwork on a single topic. Though there is an availability of scientometric\nmeasures which promise to offer a solution to this problem, these measures are\nmostly quantitative and rely, for instance, only on the number of times an\narticle is cited. With this approach, it becomes irrelevant if an article is\ncited 10 times in a positive, negative or neutral way. In this context, it is\nquite important to study the qualitative aspect of a citation to understand its\nsignificance. This paper presents a novel system for sentiment analysis of\ncitations in scientific documents (SentiCite) and is also capable of detecting\nnature of citations by targeting the motivation behind a citation, e.g.,\nreference to a dataset, reading reference. Furthermore, the paper also presents\ntwo datasets (SentiCiteDB and IntentCiteDB) containing about 2,600 citations\nwith their ground truth for sentiment and nature of citation. SentiCite along\nwith other state-of-the-art methods for sentiment analysis are evaluated on the\npresented datasets. Evaluation results reveal that SentiCite outperforms\nstate-of-the-art methods for sentiment analysis in scientific publications by\nachieving a F1-measure of 0.71.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:49:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mercier", "Dominique", ""], ["Bhardwaj", "Akansha", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1910.03505", "submitter": "Jinghui Lu", "authors": "Jinghui Lu, Maeve Henchion, Brian Mac Namee", "title": "Investigating the Effectiveness of Representations Based on\n  Word-Embeddings in Active Learning for Labelling Text Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually labelling large collections of text data is a time-consuming,\nexpensive, and laborious task, but one that is necessary to support machine\nlearning based on text datasets. Active learning has been shown to be an\neffective way to alleviate some of the effort required in utilising large\ncollections of unlabelled data for machine learning tasks without needing to\nfully label them. The representation mechanism used to represent text documents\nwhen performing active learning, however, has a significant influence on how\neffective the process will be. While simple vector representations such as bag\nof words have been shown to be an effective way to represent documents during\nactive learning, the emergence of representation mechanisms based on the word\nembeddings prevalent in neural network research (e.g. word2vec and\ntransformer-based models like BERT) offer a promising, and as yet not fully\nexplored, alternative. This paper describes a large-scale evaluation of the\neffectiveness of different text representation mechanisms for active learning\nacross 8 datasets from varied domains. This evaluation shows that using\nrepresentations based on modern word embeddings---especially BERT---, which\nhave not yet been widely used in active learning, achieves a significant\nimprovement over more commonly used vector-based methods like bag of words.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 11:00:36 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 11:15:27 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lu", "Jinghui", ""], ["Henchion", "Maeve", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1910.03506", "submitter": "Pan Li", "authors": "Pan Li, Alexander Tuzhilin", "title": "Towards Controllable and Personalized Review Generation", "comments": "Accepted to EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model RevGAN that automatically generates\ncontrollable and personalized user reviews based on the arbitrarily given\nsentimental and stylistic information. RevGAN utilizes the combination of three\nnovel components, including self-attentive recursive autoencoders, conditional\ndiscriminators, and personalized decoders. We test its performance on the\nseveral real-world datasets, where our model significantly outperforms\nstate-of-the-art generation models in terms of sentence quality, coherence,\npersonalization and human evaluations. We also empirically show that the\ngenerated reviews could not be easily distinguished from the organically\nproduced reviews and that they follow the same statistical linguistics laws.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:12:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 15:05:45 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "1910.03524", "submitter": "Stanislav Morozov", "authors": "Denis Mazur, Vage Egiazarian, Stanislav Morozov, Artem Babenko", "title": "Beyond Vector Spaces: Compact Data Representation as Differentiable\n  Weighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning useful representations is a key ingredient to the success of modern\nmachine learning. Currently, representation learning mostly relies on embedding\ndata into Euclidean space. However, recent work has shown that data in some\ndomains is better modeled by non-euclidean metric spaces, and inappropriate\ngeometry can result in inferior performance. In this paper, we aim to eliminate\nthe inductive bias imposed by the embedding space geometry. Namely, we propose\nto map data into more general non-vector metric spaces: a weighted graph with a\nshortest path distance. By design, such graphs can model arbitrary geometry\nwith a proper configuration of edges and weights. Our main contribution is\nPRODIGE: a method that learns a weighted graph representation of data\nend-to-end by gradient descent. Greater generality and fewer model assumptions\nmake PRODIGE more powerful than existing embedding-based approaches. We confirm\nthe superiority of our method via extensive experiments on a wide range of\ntasks, including classification, compression, and collaborative filtering.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:31:11 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 13:08:06 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 13:46:56 GMT"}, {"version": "v4", "created": "Wed, 16 Oct 2019 16:43:20 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Mazur", "Denis", ""], ["Egiazarian", "Vage", ""], ["Morozov", "Stanislav", ""], ["Babenko", "Artem", ""]]}, {"id": "1910.03552", "submitter": "Heinrich K\\\"uttler", "authors": "Heinrich K\\\"uttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici,\n  Viswanath Sivakumar, Tim Rockt\\\"aschel, Edward Grefenstette", "title": "TorchBeast: A PyTorch Platform for Distributed RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TorchBeast is a platform for reinforcement learning (RL) research in PyTorch.\nIt implements a version of the popular IMPALA algorithm for fast, asynchronous,\nparallel training of RL agents. Additionally, TorchBeast has simplicity as an\nexplicit design goal: We provide both a pure-Python implementation\n(\"MonoBeast\") as well as a multi-machine high-performance version\n(\"PolyBeast\"). In the latter, parts of the implementation are written in C++,\nbut all parts pertaining to machine learning are kept in simple Python using\nPyTorch, with the environments provided using the OpenAI Gym interface. This\nenables researchers to conduct scalable RL research using TorchBeast without\nany programming knowledge beyond Python and PyTorch. In this paper, we describe\nthe TorchBeast design principles and implementation and demonstrate that it\nperforms on-par with IMPALA on Atari. TorchBeast is released as an open-source\npackage under the Apache 2.0 license and is available at\n\\url{https://github.com/facebookresearch/torchbeast}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:24:28 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["K\u00fcttler", "Heinrich", ""], ["Nardelli", "Nantas", ""], ["Lavril", "Thibaut", ""], ["Selvatici", "Marco", ""], ["Sivakumar", "Viswanath", ""], ["Rockt\u00e4schel", "Tim", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1910.03553", "submitter": "Ananda Theertha Suresh", "authors": "Ananda Theertha Suresh", "title": "Differentially private anonymized histograms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a dataset of label-count pairs, an anonymized histogram is the multiset\nof counts. Anonymized histograms appear in various potentially sensitive\ncontexts such as password-frequency lists, degree distribution in social\nnetworks, and estimation of symmetric properties of discrete distributions.\nMotivated by these applications, we propose the first differentially private\nmechanism to release anonymized histograms that achieves near-optimal privacy\nutility trade-off both in terms of number of items and the privacy parameter.\nFurther, if the underlying histogram is given in a compact format, the proposed\nalgorithm runs in time sub-linear in the number of items. For anonymized\nhistograms generated from unknown discrete distributions, we show that the\nreleased histogram can be directly used for estimating symmetric properties of\nthe underlying distribution.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:24:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 20:54:42 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Suresh", "Ananda Theertha", ""]]}, {"id": "1910.03561", "submitter": "John Zarka", "authors": "John Zarka, Louis Thiry, Tom\\'as Angles, St\\'ephane Mallat", "title": "Deep Network Classification by Scattering and Homotopy Dictionary\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sparse scattering deep convolutional neural network, which\nprovides a simple model to analyze properties of deep representation learning\nfor classification. Learning a single dictionary matrix with a classifier\nyields a higher classification accuracy than AlexNet over the ImageNet 2012\ndataset. The network first applies a scattering transform that linearizes\nvariabilities due to geometric transformations such as translations and small\ndeformations. A sparse $\\ell^1$ dictionary coding reduces intra-class\nvariability while preserving class separation through projections over unions\nof linear spaces. It is implemented in a deep convolutional network with a\nhomotopy algorithm having an exponential convergence. A convergence proof is\ngiven in a general framework that includes ALISTA. Classification results are\nanalyzed on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:47:44 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 23:16:15 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 17:32:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zarka", "John", ""], ["Thiry", "Louis", ""], ["Angles", "Tom\u00e1s", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1910.03581", "submitter": "Daliang Li Dr.", "authors": "Daliang Li, Junpu Wang", "title": "FedMD: Heterogenous Federated Learning via Model Distillation", "comments": "4 pages, 2 figures, NeurIPS 2019 Workshop on Federated Learning for\n  Data Privacy and Confidentiality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables the creation of a powerful centralized model\nwithout compromising data privacy of multiple participants. While successful,\nit does not incorporate the case where each participant independently designs\nits own model. Due to intellectual property concerns and heterogeneous nature\nof tasks and data, this is a widespread requirement in applications of\nfederated learning to areas such as health care and AI as a service. In this\nwork, we use transfer learning and knowledge distillation to develop a\nuniversal framework that enables federated learning when each agent owns not\nonly their private data, but also uniquely designed models. We test our\nframework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and\nobserve fast improvement across all participating models. With 10 distinct\nparticipants, the final test accuracy of each model on average receives a 20%\ngain on top of what's possible without collaboration and is only a few percent\nlower than the performance each model would have obtained if all private\ndatasets were pooled and made directly available for all participants.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:00:00 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Li", "Daliang", ""], ["Wang", "Junpu", ""]]}, {"id": "1910.03620", "submitter": "Matthias Schultheis", "authors": "Matthias Schultheis, Boris Belousov, Hany Abdulsamad, Jan Peters", "title": "Receding Horizon Curiosity", "comments": "Published at Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sample-efficient exploration is crucial not only for discovering rewarding\nexperiences but also for adapting to environment changes in a task-agnostic\nfashion. A principled treatment of the problem of optimal input synthesis for\nsystem identification is provided within the framework of sequential Bayesian\nexperimental design. In this paper, we present an effective\ntrajectory-optimization-based approximate solution of this otherwise\nintractable problem that models optimal exploration in an unknown Markov\ndecision process (MDP). By interleaving episodic exploration with Bayesian\nnonlinear system identification, our algorithm takes advantage of the inductive\nbias to explore in a directed manner, without assuming prior knowledge of the\nMDP. Empirical evaluations indicate a clear advantage of the proposed algorithm\nin terms of the rate of convergence and the final model fidelity when compared\nto intrinsic-motivation-based algorithms employing exploration bonuses such as\nprediction error and information gain. Moreover, our method maintains a\ncomputational advantage over a recent model-based active exploration (MAX)\nalgorithm, by focusing on the information gain along trajectories instead of\nseeking a global exploration policy. A reference implementation of our\nalgorithm and the conducted experiments is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:11:50 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Schultheis", "Matthias", ""], ["Belousov", "Boris", ""], ["Abdulsamad", "Hany", ""], ["Peters", "Jan", ""]]}, {"id": "1910.03624", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "SmoothFool: An Efficient Framework for Computing Smooth Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are susceptible to adversarial manipulations in the\ninput domain. The extent of vulnerability has been explored intensively in\ncases of $\\ell_p$-bounded and $\\ell_p$-minimal adversarial perturbations.\nHowever, the vulnerability of DNNs to adversarial perturbations with specific\nstatistical properties or frequency-domain characteristics has not been\nsufficiently explored. In this paper, we study the smoothness of perturbations\nand propose SmoothFool, a general and computationally efficient framework for\ncomputing smooth adversarial perturbations. Through extensive experiments, we\nvalidate the efficacy of the proposed method for both the white-box and\nblack-box attack scenarios. In particular, we demonstrate that: (i) there exist\nextremely smooth adversarial perturbations for well-established and widely used\nnetwork architectures, (ii) smoothness significantly enhances the robustness of\nperturbations against state-of-the-art defense mechanisms, (iii) smoothness\nimproves the transferability of adversarial perturbations across both data\npoints and network architectures, and (iv) class categories exhibit a variable\nrange of susceptibility to smooth perturbations. Our results suggest that\nsmooth APs can play a significant role in exploring the vulnerability extent of\nDNNs to adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:22:21 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Taherkhani", "Fariborz", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1910.03632", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Distilling importance sampling", "comments": "This version adds a second application, and fixes some minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main approaches to Bayesian inference are sampling and optimisation\nmethods. However many complicated posteriors are difficult to approximate by\neither. Therefore we propose a novel approach combining features of both. We\nuse a flexible parameterised family of densities, such as a normalising flow.\nGiven a density from this family approximating the posterior, we use importance\nsampling to produce a weighted sample from a more accurate posterior\napproximation. This sample is then used in optimisation to update the\nparameters of the approximate density, which we view as distilling the\nimportance sampling results. We iterate these steps and gradually improve the\nquality of the posterior approximation. We illustrate our method in two\nchallenging examples: a queueing model and a stochastic differential equation\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:38:50 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 14:15:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1910.03643", "submitter": "Denis Belomestny", "authors": "D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov and S. Samsonov", "title": "Variance reduction for Markov chains with application to MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel variance reduction approach for additive\nfunctionals of Markov chains based on minimization of an estimate for the\nasymptotic variance of these functionals over suitable classes of control\nvariates. A distinctive feature of the proposed approach is its ability to\nsignificantly reduce the overall finite sample variance. This feature is\ntheoretically demonstrated by means of a deep non asymptotic analysis of a\nvariance reduced functional as well as by a thorough simulation study. In\nparticular we apply our method to various MCMC Bayesian estimation problems\nwhere it favourably compares to the existing variance reduction approaches.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:05:36 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 08:37:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""]]}, {"id": "1910.03644", "submitter": "Clint D. Lombard Dr", "authors": "Clint D. Lombard, Corn\\'e E. van Daalen", "title": "Stochastic triangular mesh mapping: A terrain mapping technique for\n  autonomous mobile robots", "comments": null, "journal-ref": "Robotics and Autonomous Systems (2020)", "doi": "10.1016/j.robot.2020.103449", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mobile robots to operate autonomously in general environments, perception\nis required in the form of a dense metric map. For this purpose, we present the\nstochastic triangular mesh (STM) mapping technique: a 2.5-D representation of\nthe surface of the environment using a continuous mesh of triangular surface\nelements, where each surface element models the mean plane and roughness of the\nunderlying surface. In contrast to existing mapping techniques, a STM map\nmodels the structure of the environment by ensuring a continuous model, while\nalso being able to be incrementally updated with linear computational cost in\nthe number of measurements. We reduce the effect of uncertainty in the robot\npose (position and orientation) by using landmark-relative submaps. The\nuncertainty in the measurements and robot pose are accounted for by the use of\nBayesian inference techniques during the map update. We demonstrate that a STM\nmap can be used with sensors that generate point measurements, such as light\ndetection and ranging (LiDAR) sensors and stereo cameras. We show that a STM\nmap is a more accurate model than the only comparable online surface mapping\ntechnique$\\unicode{x2014}$a standard elevation map$\\unicode{x2014}$and we also\nprovide qualitative results on practical datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:06:05 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 15:33:35 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lombard", "Clint D.", ""], ["van Daalen", "Corn\u00e9 E.", ""]]}, {"id": "1910.03648", "submitter": "Qianru Sun", "authors": "Qianru Sun, Yaoyao Liu, Zhaozheng Chen, Tat-Seng Chua, and Bernt\n  Schiele", "title": "Meta-Transfer Learning through Hard Tasks", "comments": "An extended version of a paper published in CVPR2019. Under review.\n  arXiv admin note: substantial text overlap with arXiv:1812.02391", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta-learning has been proposed as a framework to address the challenging\nfew-shot learning setting. The key idea is to leverage a large number of\nsimilar few-shot tasks in order to learn how to adapt a base-learner to a new\ntask for which only a few labeled samples are available. As deep neural\nnetworks (DNNs) tend to overfit using a few samples only, typical meta-learning\nmodels use shallow neural networks, thus limiting its effectiveness. In order\nto achieve top performance, some recent works tried to use the DNNs pre-trained\non large-scale datasets but mostly in straight-forward manners, e.g., (1)\ntaking their weights as a warm start of meta-training, and (2) freezing their\nconvolutional layers as the feature extractor of base-learners. In this paper,\nwe propose a novel approach called meta-transfer learning (MTL) which learns to\ntransfer the weights of a deep NN for few-shot learning tasks. Specifically,\nmeta refers to training multiple tasks, and transfer is achieved by learning\nscaling and shifting functions of DNN weights for each task. In addition, we\nintroduce the hard task (HT) meta-batch scheme as an effective learning\ncurriculum that further boosts the learning efficiency of MTL. We conduct\nfew-shot learning experiments and report top performance for five-class\nfew-shot recognition tasks on three challenging benchmarks: miniImageNet,\ntieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related\nworks validate that our MTL approach trained with the proposed HT meta-batch\nscheme achieves top performance. An ablation study also shows that both\ncomponents contribute to fast convergence and high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:05:18 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sun", "Qianru", ""], ["Liu", "Yaoyao", ""], ["Chen", "Zhaozheng", ""], ["Chua", "Tat-Seng", ""], ["Schiele", "Bernt", ""]]}, {"id": "1910.03660", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Mohammad Arashi and Fikri Akdeniz", "title": "Penalized regression via the restricted bridge estimator", "comments": null, "journal-ref": "Soft Computing, 2021", "doi": "10.1007/s00500-021-05763-9", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with the Bridge Regression, which is a special\nfamily in penalized regression with penalty function\n$\\sum_{j=1}^{p}|\\beta_j|^q$ with $q>0$, in a linear model with linear\nrestrictions. The proposed restricted bridge (RBRIDGE) estimator simultaneously\nestimates parameters and selects important variables when a prior information\nabout parameters are available in either low dimensional or high dimensional\ncase. Using local quadratic approximation, the penalty term can be approximated\naround a local initial values vector and the RBRIDGE estimator enjoys a\nclosed-form expression which can be solved when $q>0$. Special cases of our\nproposal are the restricted LASSO ($q=1$), restricted RIDGE ($q=2$), and\nrestricted Elastic Net ($1< q < 2$) estimators. We provide some theoretical\nproperties of the RBRIDGE estimator under for the low dimensional case, whereas\nthe computational aspects are given for both low and high dimensional cases. An\nextensive Monte Carlo simulation study is conducted based on different prior\npieces of information and the performance of the RBRIDGE estiamtor is compared\nwith some competitive penalty estimators as well as the ORACLE. We also\nconsider four real data examples analysis for comparison sake. The numerical\nresults show that the suggested RBRIDGE estimator outperforms outstandingly\nwhen the prior is true or near exact\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:39:16 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Arashi", "Mohammad", ""], ["Akdeniz", "Fikri", ""]]}, {"id": "1910.03678", "submitter": "Muhammad Mahbubur Rahman", "authors": "Muhammad Mahbubur Rahman, Tim Finin", "title": "Unfolding the Structure of a Document using Deep Learning", "comments": "16 pages, 16 figures and 10 tables. arXiv admin note: text overlap\n  with arXiv:1709.00770", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and extracting of information from large documents, such as\nbusiness opportunities, academic articles, medical documents and technical\nreports, poses challenges not present in short documents. Such large documents\nmay be multi-themed, complex, noisy and cover diverse topics. We describe a\nframework that can analyze large documents and help people and computer systems\nlocate desired information in them. We aim to automatically identify and\nclassify different sections of documents and understand their purpose within\nthe document. A key contribution of our research is modeling and extracting the\nlogical and semantic structure of electronic documents using deep learning\ntechniques. We evaluate the effectiveness and robustness of our framework\nthrough extensive experiments on two collections: more than one million\nscholarly articles from arXiv and a collection of requests for proposal\ndocuments from government sources.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 21:33:46 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Finin", "Tim", ""]]}, {"id": "1910.03698", "submitter": "Iddo Drori", "authors": "Iddo Drori, Lu Liu, Yi Nian, Sharath C. Koorathota, Jie S. Li, Antonio\n  Khalil Moretti, Juliana Freire, Madeleine Udell", "title": "AutoML using Metadata Language Embeddings", "comments": null, "journal-ref": "NeurIPS Workshop on Meta-Learning, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a human choosing a supervised learning algorithm, it is natural to begin\nby reading a text description of the dataset and documentation for the\nalgorithms you might use. We demonstrate that the same idea improves the\nperformance of automated machine learning methods. We use language embeddings\nfrom modern NLP to improve state-of-the-art AutoML systems by augmenting their\nrecommendations with vector embeddings of datasets and of algorithms. We use\nthese embeddings in a neural architecture to learn the distance between\nbest-performing pipelines. The resulting (meta-)AutoML framework improves on\nthe performance of existing AutoML frameworks. Our zero-shot AutoML system\nusing dataset metadata embeddings provides good solutions instantaneously,\nrunning in under one second of computation. Performance is competitive with\nAutoML systems OBOE, AutoSklearn, AlphaD3M, and TPOT when each framework is\nallocated a minute of computation. We make our data, models, and code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 21:28:47 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Drori", "Iddo", ""], ["Liu", "Lu", ""], ["Nian", "Yi", ""], ["Koorathota", "Sharath C.", ""], ["Li", "Jie S.", ""], ["Moretti", "Antonio Khalil", ""], ["Freire", "Juliana", ""], ["Udell", "Madeleine", ""]]}, {"id": "1910.03718", "submitter": "Min-Hsiu Hsieh", "authors": "Chao Zhang, Min-Hsiu Hsieh, Dacheng Tao", "title": "On Dimension-free Tail Inequalities for Sums of Random Matrices and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math-ph math.MP math.ST quant-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new framework to obtain tail inequalities for\nsums of random matrices. Compared with existing works, our tail inequalities\nhave the following characteristics: 1) high feasibility--they can be used to\nstudy the tail behavior of various matrix functions, e.g., arbitrary matrix\nnorms, the absolute value of the sum of the sum of the $j$ largest singular\nvalues (resp. eigenvalues) of complex matrices (resp. Hermitian matrices); and\n2) independence of matrix dimension --- they do not have the matrix-dimension\nterm as a product factor, and thus are suitable to the scenario of\nhigh-dimensional or infinite-dimensional random matrices. The price we pay to\nobtain these advantages is that the convergence rate of the resulting\ninequalities will become slow when the number of summand random matrices is\nlarge. We also develop the tail inequalities for matrix random series and\nmatrix martingale difference sequence. We also demonstrate usefulness of our\ntail bounds in several fields. In compressed sensing, we employ the resulted\ntail inequalities to achieve a proof of the restricted isometry property when\nthe measurement matrix is the sum of random matrices without any assumption on\nthe distributions of matrix entries. In probability theory, we derive a new\nupper bound to the supreme of stochastic processes. In machine learning, we\nprove new expectation bounds of sums of random matrices matrix and obtain\nmatrix approximation schemes via random sampling. In quantum information, we\nshow a new analysis relating to the fractional cover number of quantum\nhypergraphs. In theoretical computer science, we obtain randomness-efficient\nsamplers using matrix expander graphs that can be efficiently implemented in\ntime without dependence on matrix dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 23:38:51 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zhang", "Chao", ""], ["Hsieh", "Min-Hsiu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1910.03732", "submitter": "Vibhavari Dasagi", "authors": "Vibhavari Dasagi, Jake Bruce, Thierry Peynot and J\\\"urgen Leitner", "title": "Ctrl-Z: Recovering from Instability in Reinforcement Learning", "comments": "Submitted to ICRA2020, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning behavior, training data is often generated by the learner\nitself; this can result in unstable training dynamics, and this problem has\nparticularly important applications in safety-sensitive real-world control\ntasks such as robotics. In this work, we propose a principled and\nmodel-agnostic approach to mitigate the issue of unstable learning dynamics by\nmaintaining a history of a reinforcement learning agent over the course of\ntraining, and reverting to the parameters of a previous agent whenever\nperformance significantly decreases. We develop techniques for evaluating this\nperformance through statistical hypothesis testing of continued improvement,\nand evaluate them on a standard suite of challenging benchmark tasks involving\ncontinuous control of simulated robots. We show improvements over\nstate-of-the-art reinforcement learning algorithms in performance and\nrobustness to hyperparameters, outperforming DDPG in 5 out of 6 evaluation\nenvironments and showing no decrease in performance with TD3, which is known to\nbe relatively stable. In this way, our approach takes an important step towards\nincreasing data efficiency and stability in training for real-world robotic\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 00:45:53 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Dasagi", "Vibhavari", ""], ["Bruce", "Jake", ""], ["Peynot", "Thierry", ""], ["Leitner", "J\u00fcrgen", ""]]}, {"id": "1910.03742", "submitter": "Tan Nguyen", "authors": "Tan Nguyen, Nan Ye, Peter L. Bartlett", "title": "Greedy Convex Ensemble", "comments": "Replace the previous version with the camera ready version accepted\n  for IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning a convex combination of basis models, and present some\nnew theoretical and empirical results that demonstrate the effectiveness of a\ngreedy approach. Theoretically, we first consider whether we can use linear,\ninstead of convex, combinations, and obtain generalization results similar to\nexisting ones for learning from a convex hull. We obtain a negative result that\neven the linear hull of very simple basis functions can have unbounded\ncapacity, and is thus prone to overfitting; on the other hand, convex hulls are\nstill rich but have bounded capacities. Secondly, we obtain a generalization\nbound for a general class of Lipschitz loss functions. Empirically, we first\ndiscuss how a convex combination can be greedily learned with early stopping,\nand how a convex combination can be non-greedily learned when the number of\nbasis models is known a priori. Our experiments suggest that the greedy scheme\nis competitive with or better than several baselines, including boosting and\nrandom forests. The greedy algorithm requires little effort in hyper-parameter\ntuning, and also seems able to adapt to the underlying complexity of the\nproblem. Our code is available at https://github.com/tan1889/gce.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 01:41:56 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 04:18:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Nguyen", "Tan", ""], ["Ye", "Nan", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1910.03749", "submitter": "Benjam\\'in B\\'ejar", "authors": "Benjam\\'in B\\'ejar, Ivan Dokmani\\'c, Ren\\'e Vidal", "title": "The fastest $\\ell_{1,\\infty}$ prox in the west", "comments": "9 pages, 2 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximal operators are of particular interest in optimization problems\ndealing with non-smooth objectives because in many practical cases they lead to\noptimization algorithms whose updates can be computed in closed form or very\nefficiently. A well-known example is the proximal operator of the vector\n$\\ell_1$ norm, which is given by the soft-thresholding operator. In this paper\nwe study the proximal operator of the mixed $\\ell_{1,\\infty}$ matrix norm and\nshow that it can be computed in closed form by applying the well-known\nsoft-thresholding operator to each column of the matrix. However, unlike the\nvector $\\ell_1$ norm case where the threshold is constant, in the mixed\n$\\ell_{1,\\infty}$ norm case each column of the matrix might require a different\nthreshold and all thresholds depend on the given matrix. We propose a general\niterative algorithm for computing these thresholds, as well as two efficient\nimplementations that further exploit easy to compute lower bounds for the mixed\nnorm of the optimal solution. Experiments on large-scale synthetic and real\ndata indicate that the proposed methods can be orders of magnitude faster than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 01:56:04 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["B\u00e9jar", "Benjam\u00edn", ""], ["Dokmani\u0107", "Ivan", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1910.03751", "submitter": "Behzad Asadi", "authors": "Behzad Asadi, Vijay Varadharajan", "title": "An MDL-Based Classifier for Transactional Datasets with Application in\n  Malware Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a classifier for transactional datasets with application in malware\ndetection. We build the classifier based on the minimum description length\n(MDL) principle. This involves selecting a model that best compresses the\ntraining dataset for each class considering the MDL criterion. To select a\nmodel for a dataset, we first use clustering followed by closed frequent\npattern mining to extract a subset of closed frequent patterns (CFPs). We show\nthat this method acts as a pattern summarization method to avoid pattern\nexplosion; this is done by giving priority to longer CFPs, and without\nrequiring to extract all CFPs. We then use the MDL criterion to further\nsummarize extracted patterns, and construct a code table of patterns. This code\ntable is considered as the selected model for the compression of the dataset.\nWe evaluate our classifier for the problem of static malware detection in\nportable executable (PE) files. We consider API calls of PE files as their\ndistinguishing features. The presence-absence of API calls forms a\ntransactional dataset. Using our proposed method, we construct two code tables,\none for the benign training dataset, and one for the malware training dataset.\nOur dataset consists of 19696 benign, and 19696 malware samples, each a binary\nsequence of size 22761. We compare our classifier with deep neural networks\nproviding us with the state-of-the-art performance. The comparison shows that\nour classifier performs very close to deep neural networks. We also discuss\nthat our classifier is an interpretable classifier. This provides the\nmotivation to use this type of classifiers where some degree of explanation is\nrequired as to why a sample is classified under one class rather than the other\nclass.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 02:08:03 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 23:55:56 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Asadi", "Behzad", ""], ["Varadharajan", "Vijay", ""]]}, {"id": "1910.03783", "submitter": "Alexandre Tartakovsky", "authors": "Tong Ma and Renke Huang and David Barajas-Solano and Ramakrishna\n  Tipireddy and Alexandre M. Tartakovsky", "title": "Electric Load and Power Forecasting Using Ensemble Gaussian Process\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new forecasting method for predicting load demand and generation\nscheduling. Accurate week-long forecasting of load demand and optimal power\ngeneration is critical for efficient operation of power grid systems. In this\nwork, we use a synthetic data set describing a power grid with 700 buses and\n134 generators over a 365-days period with data synthetically generated at an\nhourly rate. The proposed approach for week-long forecasting is based on the\nGaussian process regression (GPR) method, with prior covariance matrices of the\nquantities of interest (QoI) computed from ensembles formed by up to twenty\npreceding weeks of QoI observations. Then, we use these covariances within the\nGPR framework to forecast the QoIs for the following week. We demonstrate that\nthe the proposed ensemble GPR (EGPR) method is capable of accurately\nforecasting weekly total load demand and power generation profiles. The EGPR\nmethod is shown to outperform traditional forecasting methods including the\nstandard GPR and autoregressive integrated moving average (ARIMA) methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:13:51 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ma", "Tong", ""], ["Huang", "Renke", ""], ["Barajas-Solano", "David", ""], ["Tipireddy", "Ramakrishna", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1910.03787", "submitter": "Xueyuan Xu", "authors": "Xia Wu, Xueyuan Xu, Jianhong Liu, Hailing Wang, Bin Hu, Feiping Nie", "title": "Supervised feature selection with orthogonal regression and feature\n  weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective features can improve the performance of a model, which can thus\nhelp us understand the characteristics and underlying structure of complex\ndata. Previous feature selection methods usually cannot keep more local\nstructure information. To address the defects previously mentioned, we propose\na novel supervised orthogonal least square regression model with feature\nweighting for feature selection. The optimization problem of the objection\nfunction can be solved by employing generalized power iteration (GPI) and\naugmented Lagrangian multiplier (ALM) methods. Experimental results show that\nthe proposed method can more effectively reduce the feature dimensionality and\nobtain better classification results than traditional feature selection\nmethods. The convergence of our iterative method is proved as well.\nConsequently, the effectiveness and superiority of the proposed method are\nverified both theoretically and experimentally.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:39:09 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Wu", "Xia", ""], ["Xu", "Xueyuan", ""], ["Liu", "Jianhong", ""], ["Wang", "Hailing", ""], ["Hu", "Bin", ""], ["Nie", "Feiping", ""]]}, {"id": "1910.03802", "submitter": "Hoang Nt", "authors": "Takanori Maehara and Hoang NT", "title": "A Simple Proof of the Universality of Invariant/Equivariant Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple proof for the universality of invariant and equivariant\ntensorized graph neural networks. Our approach considers a restricted\nintermediate hypothetical model named Graph Homomorphism Model to reach the\nuniversality conclusions including an open case for higher-order output. We\nfind that our proposed technique not only leads to simple proofs of the\nuniversality properties but also gives a natural explanation for the\ntensorization of the previously studied models. Finally, we give some remarks\non the connection between our model and the continuous representation of\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 06:07:53 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Maehara", "Takanori", ""], ["NT", "Hoang", ""]]}, {"id": "1910.03810", "submitter": "Marco Schreyer", "authors": "Marco Schreyer, Timur Sattarov, Bernd Reimer, Damian Borth", "title": "Adversarial Learning of Deepfakes in Accounting", "comments": "17 pages, 10 figures, and, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, organizations collect vast quantities of accounting relevant\ntransactions, referred to as 'journal entries', in 'Enterprise Resource\nPlanning' (ERP) systems. The aggregation of those entries ultimately defines an\norganization's financial statement. To detect potential misstatements and\nfraud, international audit standards demand auditors to directly assess journal\nentries using 'Computer Assisted AuditTechniques' (CAATs). At the same time,\ndiscoveries in deep learning research revealed that machine learning models are\nvulnerable to 'adversarial attacks'. It also became evident that such attack\ntechniques can be misused to generate 'Deepfakes' designed to directly attack\nthe perception of humans by creating convincingly altered media content. The\nresearch of such developments and their potential impact on the finance and\naccounting domain is still in its early stage. We believe that it is of vital\nrelevance to investigate how such techniques could be maliciously misused in\nthis sphere. In this work, we show an adversarial attack against CAATs using\ndeep neural networks. We first introduce a real-world 'thread model' designed\nto camouflage accounting anomalies such as fraudulent journal entries. Second,\nwe show that adversarial autoencoder neural networks are capable of learning a\nhuman interpretable model of journal entries that disentangles the entries\nlatent generative factors. Finally, we demonstrate how such a model can be\nmaliciously misused by a perpetrator to generate robust 'adversarial' journal\nentries that mislead CAATs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 06:44:23 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Schreyer", "Marco", ""], ["Sattarov", "Timur", ""], ["Reimer", "Bernd", ""], ["Borth", "Damian", ""]]}, {"id": "1910.03818", "submitter": "Run-Qing Chen", "authors": "Run-Qing Chen and Guang-Hui Shi and Wan-Lei Zhao and Chang-Hui Liang", "title": "A Joint Model for IT Operation Series Prediction and Anomaly Detection", "comments": "This paper has been published in Neurocomputing", "journal-ref": "Volume 448, 11 August 2021, Pages 130-139", "doi": "10.1016/j.neucom.2021.03.062", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Status prediction and anomaly detection are two fundamental tasks in\nautomatic IT systems monitoring. In this paper, a joint model Predictor &\nAnomaly Detector (PAD) is proposed to address these two issues under one\nframework. In our design, the variational auto-encoder (VAE) and long\nshort-term memory (LSTM) are joined together. The prediction block (LSTM) takes\nclean input from the reconstructed time series by VAE, which makes it robust to\nthe anomalies and noise for prediction task. In the meantime, the LSTM block\nmaintains the long-term sequential patterns, which are out of the sight of a\nVAE encoding window. This leads to the better performance of VAE in anomaly\ndetection than it is trained alone. In the whole processing pipeline, the\nspectral residual analysis is integrated with VAE and LSTM to boost the\nperformance of both. The superior performance on two tasks is confirmed with\nthe experiments on two challenging evaluation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 07:25:33 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 07:21:37 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 13:20:57 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 01:54:56 GMT"}, {"version": "v5", "created": "Thu, 22 Apr 2021 03:26:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Run-Qing", ""], ["Shi", "Guang-Hui", ""], ["Zhao", "Wan-Lei", ""], ["Liang", "Chang-Hui", ""]]}, {"id": "1910.03834", "submitter": "Song Liu Dr.", "authors": "Song Liu, Takafumi Kanamori, Daniel J. Williams", "title": "Estimating Density Models with Truncation Boundaries using Score\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated densities are probability density functions defined on truncated\ndomains. They share the same parametric form with their non-truncated\ncounterparts up to a normalizing constant. Since the computation of their\nnormalizing constants is usually infeasible, Maximum Likelihood Estimation\ncannot be easily applied to estimate truncated density models. Score Matching\n(SM) is a powerful tool for fitting parameters using only unnormalized models.\nHowever, it cannot be directly applied here as boundary conditions used to\nderive a tractable SM objective are not satisfied by truncated densities. In\nthis paper, we study parameter estimation for truncated probability densities\nusing SM. The estimator minimizes a weighted Fisher divergence. The weight\nfunction is simply the shortest distance from the data point to the boundary of\nthe domain. We show this choice of weight function naturally arises from\nminimizing the Stein discrepancy as well as upperbounding the finite-sample\nstatistical estimation error. The usefulness of our method is demonstrated by\nnumerical experiments and a Chicago crime dataset. We also show that proposed\ndensity estimation can correct the outlier-trimming bias caused by aggressive\noutlier detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:18:20 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 18:01:45 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 00:22:22 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 15:01:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Song", ""], ["Kanamori", "Takafumi", ""], ["Williams", "Daniel J.", ""]]}, {"id": "1910.03857", "submitter": "Marcin B. Tomczak", "authors": "Marcin B. Tomczak, Dongho Kim, Peter Vrancx and Kee-Eung Kim", "title": "Policy Optimization Through Approximate Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent policy optimization approaches (Schulman et al., 2015a; 2017) have\nachieved substantial empirical successes by constructing new proxy optimization\nobjectives. These proxy objectives allow stable and low variance policy\nlearning, but require small policy updates to ensure that the proxy objective\nremains an accurate approximation of the target policy value. In this paper we\nderive an alternative objective that obtains the value of the target policy by\napplying importance sampling (IS). However, the basic importance sampled\nobjective is not suitable for policy optimization, as it incurs too high\nvariance in policy updates. We therefore introduce an approximation that allows\nus to directly trade-off the bias of approximation with the variance in policy\nupdates. We show that our approximation unifies previously developed approaches\nand allows us to interpolate between them. We develop a practical algorithm by\noptimizing the introduced objective with proximal policy optimization\ntechniques (Schulman et al., 2017). We also provide a theoretical analysis of\nthe introduced policy optimization objective demonstrating bias-variance\ntrade-off. We empirically demonstrate that the resulting algorithm improves\nupon state of the art on-policy policy optimization on continuous control\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:06:35 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:14:43 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tomczak", "Marcin B.", ""], ["Kim", "Dongho", ""], ["Vrancx", "Peter", ""], ["Kim", "Kee-Eung", ""]]}, {"id": "1910.03860", "submitter": "Hicham Janati", "authors": "Hicham Janati, Marco Cuturi, Alexandre Gramfort", "title": "Spatio-Temporal Alignments: Optimal transport through space and time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing data defined over space and time is notoriously hard, because it\ninvolves quantifying both spatial and temporal variability, while at the same\ntime taking into account the chronological structure of data. Dynamic Time\nWarping (DTW) computes an optimal alignment between time series in agreement\nwith the chronological order, but is inherently blind to spatial shifts. In\nthis paper, we propose Spatio-Temporal Alignments (STA), a new differentiable\nformulation of DTW, in which spatial differences between time samples are\naccounted for using regularized optimal transport (OT). Our temporal alignments\nare handled through a smooth variant of DTW called soft-DTW, for which we prove\na new property: soft-DTW increases quadratically with time shifts. The cost\nmatrix within soft-DTW that we use are computed using unbalanced OT, to handle\nthe case in which observations are not normalized probabilities. Experiments on\nhandwritten letters and brain imaging data confirm our theoretical findings and\nillustrate the effectiveness of STA as a dissimilarity for spatio-temporal\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:22:41 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 17:14:23 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 14:00:01 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Janati", "Hicham", ""], ["Cuturi", "Marco", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1910.03861", "submitter": "Aur\\'elien Bellet", "authors": "James Bell and Aur\\'elien Bellet and Adri\\`a Gasc\\'on and Tejas\n  Kulkarni", "title": "Private Protocols for U-Statistics in the Local Model and Beyond", "comments": "Accepted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of computing $U$-statistics of degree\n$2$, i.e., quantities that come in the form of averages over pairs of data\npoints, in the local model of differential privacy (LDP). The class of\n$U$-statistics covers many statistical estimates of interest, including Gini\nmean difference, Kendall's tau coefficient and Area under the ROC Curve (AUC),\nas well as empirical risk measures for machine learning problems such as\nranking, clustering and metric learning. We first introduce an LDP protocol\nbased on quantizing the data into bins and applying randomized response, which\nguarantees an $\\epsilon$-LDP estimate with a Mean Squared Error (MSE) of\n$O(1/\\sqrt{n}\\epsilon)$ under regularity assumptions on the $U$-statistic or\nthe data distribution. We then propose a specialized protocol for AUC based on\na novel use of hierarchical histograms that achieves MSE of\n$O(\\alpha^3/n\\epsilon^2)$ for arbitrary data distribution. We also show that\n2-party secure computation allows to design a protocol with MSE of\n$O(1/n\\epsilon^2)$, without any assumption on the kernel function or data\ndistribution and with total communication linear in the number of users $n$.\nFinally, we evaluate the performance of our protocols through experiments on\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:24:53 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 20:26:02 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Bell", "James", ""], ["Bellet", "Aur\u00e9lien", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Kulkarni", "Tejas", ""]]}, {"id": "1910.03867", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Mikhail Burtsev", "title": "Loss Landscape Sightseeing with Multi-Point Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present multi-point optimization: an optimization technique that allows to\ntrain several models simultaneously without the need to keep the parameters of\neach one individually. The proposed method is used for a thorough empirical\nanalysis of the loss landscape of neural networks. By extensive experiments on\nFashionMNIST and CIFAR10 datasets we demonstrate two things: 1) loss surface is\nsurprisingly diverse and intricate in terms of landscape patterns it contains,\nand 2) adding batch normalization makes it more smooth. Source code to\nreproduce all the reported results is available on GitHub:\nhttps://github.com/universome/loss-patterns.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:44:27 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 11:21:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Burtsev", "Mikhail", ""]]}, {"id": "1910.03875", "submitter": "Anton Mallasto", "authors": "Anton Mallasto, Guido Mont\\'ufar and Augusto Gerolin", "title": "How Well Do WGANs Estimate the Wasserstein Metric?", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modelling is often cast as minimizing a similarity measure between\na data distribution and a model distribution. Recently, a popular choice for\nthe similarity measure has been the Wasserstein metric, which can be expressed\nin the Kantorovich duality formulation as the optimum difference of the\nexpected values of a potential function under the real data distribution and\nthe model hypothesis. In practice, the potential is approximated with a neural\nnetwork and is called the discriminator. Duality constraints on the function\nclass of the discriminator are enforced approximately, and the expectations are\nestimated from samples. This gives at least three sources of errors: the\napproximated discriminator and constraints, the estimation of the expectation\nvalue, and the optimization required to find the optimal potential. In this\nwork, we study how well the methods, that are used in generative adversarial\nnetworks to approximate the Wasserstein metric, perform. We consider, in\nparticular, the $c$-transform formulation, which eliminates the need to enforce\nthe constraints explicitly. We demonstrate that the $c$-transform allows for a\nmore accurate estimation of the true Wasserstein metric from samples, but\nsurprisingly, does not perform the best in the generative setting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 10:00:34 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Mallasto", "Anton", ""], ["Mont\u00fafar", "Guido", ""], ["Gerolin", "Augusto", ""]]}, {"id": "1910.03879", "submitter": "Haakon Robinson", "authors": "Haakon Robinson, Adil Rasheed, Omer San", "title": "Dissecting Deep Neural Networks", "comments": "12 pages, 10 figures (not including bio pics), submitted to IEEE\n  Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exchange for large quantities of data and processing power, deep neural\nnetworks have yielded models that provide state of the art predication\ncapabilities in many fields. However, a lack of strong guarantees on their\nbehaviour have raised concerns over their use in safety-critical applications.\nA first step to understanding these networks is to develop alternate\nrepresentations that allow for further analysis. It has been shown that neural\nnetworks with piecewise affine activation functions are themselves piecewise\naffine, with their domains consisting of a vast number of linear regions. So\nfar, the research on this topic has focused on counting the number of linear\nregions, rather than obtaining explicit piecewise affine representations. This\nwork presents a novel algorithm that can compute the piecewise affine form of\nany fully connected neural network with rectified linear unit activations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 10:05:23 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 12:33:32 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Robinson", "Haakon", ""], ["Rasheed", "Adil", ""], ["San", "Omer", ""]]}, {"id": "1910.03880", "submitter": "Marcin B. Tomczak", "authors": "Marcin B. Tomczak, Sergio Valcarcel Macua, Enrique Munoz de Cote and\n  Peter Vrancx", "title": "Compatible features for Monotonic Policy Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent policy optimization approaches have achieved substantial empirical\nsuccess by constructing surrogate optimization objectives. The Approximate\nPolicy Iteration objective (Schulman et al., 2015a; Kakade and Langford, 2002)\nhas become a standard optimization target for reinforcement learning problems.\nUsing this objective in practice requires an estimator of the advantage\nfunction. Policy optimization methods such as those proposed in Schulman et al.\n(2015b) estimate the advantages using a parametric critic. In this work we\nestablish conditions under which the parametric approximation of the critic\ndoes not introduce bias to the updates of surrogate objective. These results\nhold for a general class of parametric policies, including deep neural\nnetworks. We obtain a result analogous to the compatible features derived for\nthe original Policy Gradient Theorem (Sutton et al., 1999). As a result, we\nalso identify a previously unknown bias that current state-of-the-art policy\noptimization algorithms (Schulman et al., 2015a, 2017) have introduced by not\nemploying these compatible features.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 10:16:19 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 12:49:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Tomczak", "Marcin B.", ""], ["Macua", "Sergio Valcarcel", ""], ["de Cote", "Enrique Munoz", ""], ["Vrancx", "Peter", ""]]}, {"id": "1910.03906", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Gerrit J.J. van den Burg, Theodoros Damoulas,\n  Mark F. J. Steel", "title": "Probabilistic sequential matrix factorization", "comments": "Accepted for publication at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the probabilistic sequential matrix factorization (PSMF) method\nfor factorizing time-varying and non-stationary datasets consisting of\nhigh-dimensional time-series. In particular, we consider nonlinear Gaussian\nstate-space models where sequential approximate inference results in the\nfactorization of a data matrix into a dictionary and time-varying coefficients\nwith potentially nonlinear Markovian dependencies. The assumed Markovian\nstructure on the coefficients enables us to encode temporal dependencies into a\nlow-dimensional feature space. The proposed inference method is solely based on\nan approximate extended Kalman filtering scheme, which makes the resulting\nmethod particularly efficient. PSMF can account for temporal nonlinearities\nand, more importantly, can be used to calibrate and estimate generic\ndifferentiable nonlinear subspace models. We also introduce a robust version of\nPSMF, called rPSMF, which uses Student-t filters to handle model\nmisspecification. We show that PSMF can be used in multiple contexts: modeling\ntime series with a periodic subspace, robustifying changepoint detection\nmethods, and imputing missing data in several high-dimensional time-series,\nsuch as measurements of pollutants across London.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:30:29 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 19:01:46 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 17:08:19 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Burg", "Gerrit J. J. van den", ""], ["Damoulas", "Theodoros", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1910.03916", "submitter": "Giulio Zizzo", "authors": "Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones", "title": "Deep Latent Defence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown state of the art performance in a range of\ntasks from computer vision to natural language processing. However, it is well\nknown that such systems are vulnerable to attackers who craft inputs in order\nto cause misclassification. The level of perturbation an attacker needs to\nintroduce in order to cause such a misclassification can be extremely small,\nand often imperceptible. This is of significant security concern, particularly\nwhere misclassification can cause harm to humans.\n  We thus propose Deep Latent Defence, an architecture which seeks to combine\nadversarial training with a detection system. At its core Deep Latent Defence\nhas a adversarially trained neural network. A series of encoders take the\nintermediate layer representation of data as it passes though the network and\nproject it to a latent space which we use for detecting adversarial samples via\na $k$-nn classifier. We present results using both grey and white box\nattackers, as well as an adaptive $L_{\\infty}$ bounded attack which was\nconstructed specifically to try and evade our defence. We find that even under\nthe strongest attacker model that we have investigated our defence is able to\noffer significant defensive benefits.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 12:00:52 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 11:59:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zizzo", "Giulio", ""], ["Hankin", "Chris", ""], ["Maffeis", "Sergio", ""], ["Jones", "Kevin", ""]]}, {"id": "1910.03937", "submitter": "Mathukumalli Vidyasagar", "authors": "Shantanu Prasad Burnwal, Kaneenika Sinha and Mathukumalli Vidyasagar", "title": "New and Explicit Constructions of Unbalanced Ramanujan Bipartite Graphs", "comments": "This paper is a partial replacement of 1910.03937v1. The phase\n  transition part of 1910.03937v1 will be uploaded as a separate submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objectives of this article are three-fold. Firstly, we present for the\nfirst time explicit constructions of an infinite family of \\textit{unbalanced}\nRamanujan bigraphs. Secondly, we revisit some of the known methods for\nconstructing Ramanujan graphs and discuss the computational work required in\nactually implementing the various construction methods. The third goal of this\narticle is to address the following question: can we construct a bipartite\nRamanujan graph with specified degrees, but with the restriction that the edge\nset of this graph must be distinct from a given set of \"prohibited\" edges? We\nprovide an affirmative answer in many cases, as long as the set of prohibited\nedges is not too large.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:46:31 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 11:32:52 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Burnwal", "Shantanu Prasad", ""], ["Sinha", "Kaneenika", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1910.03943", "submitter": "Ali Sadeghian", "authors": "Ali Sadeghian, Shervin Minaee, Ioannis Partalas, Xinxin Li, Daisy Zhe\n  Wang, Brooke Cowan", "title": "Hotel2vec: Learning Attribute-Aware Hotel Embeddings with\n  Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network architecture for learning vector representations\nof hotels. Unlike previous works, which typically only use user click\ninformation for learning item embeddings, we propose a framework that combines\nseveral sources of data, including user clicks, hotel attributes (e.g.,\nproperty type, star rating, average user rating), amenity information (e.g.,\nthe hotel has free Wi-Fi or free breakfast), and geographic information. During\nmodel training, a joint embedding is learned from all of the above information.\nWe show that including structured attributes about hotels enables us to make\nbetter predictions in a downstream task than when we rely exclusively on click\ndata. We train our embedding model on more than 40 million user click sessions\nfrom a leading online travel platform and learn embeddings for more than one\nmillion hotels. Our final learned embeddings integrate distinct sub-embeddings\nfor user clicks, hotel attributes, and geographic information, providing an\ninterpretable representation that can be used flexibly depending on the\napplication. We show empirically that our model generates high-quality\nrepresentations that boost the performance of a hotel recommendation system in\naddition to other applications. An important advantage of the proposed neural\nmodel is that it addresses the cold-start problem for hotels with insufficient\nhistorical click information by incorporating additional hotel attributes which\nare available for all hotels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:47:55 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sadeghian", "Ali", ""], ["Minaee", "Shervin", ""], ["Partalas", "Ioannis", ""], ["Li", "Xinxin", ""], ["Wang", "Daisy Zhe", ""], ["Cowan", "Brooke", ""]]}, {"id": "1910.03948", "submitter": "ChaeHwan Song", "authors": "Armin Eftekhari, ChaeHwan Song, Volkan Cevher", "title": "Nearly Minimal Over-Parametrization of Shallow Neural Networks", "comments": "This paper is submitted without consent of the co-authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work has shown that an overparametrized neural network can\nperfectly fit the training data, an otherwise often intractable nonconvex\noptimization problem. For (fully-connected) shallow networks, in the best case\nscenario, the existing theory requires quadratic over-parametrization as a\nfunction of the number of training samples. This paper establishes that linear\noverparametrization is sufficient to fit the training data, using a simple\nvariant of the (stochastic) gradient descent. Crucially, unlike several related\nworks, the training considered in this paper is not limited to the lazy regime\nin the sense cautioned against in [1, 2]. Beyond shallow networks, the\nframework developed in this work for over-parametrization is applicable to a\nvariety of learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 12:31:49 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:45:37 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Eftekhari", "Armin", ""], ["Song", "ChaeHwan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1910.03962", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Paul K Rubenstein, Bernhard Sch\\\"olkopf, Adrian\n  Weller", "title": "Optimal experimental design via Bayesian optimization: active causal\n  structure learning for Gaussian process networks", "comments": "Working paper. Accepted as a poster at the NeurIPS 2019 workshop, \"Do\n  the right thing\": machine learning and causal inference for improved decision\n  making. (6 pages + references + appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of causal discovery through targeted interventions.\nStarting from few observational measurements, we follow a Bayesian active\nlearning approach to perform those experiments which, in expectation with\nrespect to the current model, are maximally informative about the underlying\ncausal structure. Unlike previous work, we consider the setting of continuous\nrandom variables with non-linear functional relationships, modelled with\nGaussian process priors. To address the arising problem of choosing from an\nuncountable set of possible interventions, we propose to use Bayesian\noptimisation to efficiently maximise a Monte Carlo estimate of the expected\ninformation gain.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 12:57:35 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Rubenstein", "Paul K", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Weller", "Adrian", ""]]}, {"id": "1910.03976", "submitter": "Lorenzo Nespoli", "authors": "Lorenzo Nespoli, Vasco Medici, Kristijan Lopatichki, Fabrizio Sossan", "title": "Hierarchical Demand Forecasting Benchmark for the Distribution Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparative study of different probabilistic forecasting\ntechniques on the task of predicting the electrical load of secondary\nsubstations and cabinets located in a low voltage distribution grid, as well as\ntheir aggregated power profile. The methods are evaluated using standard KPIs\nfor deterministic and probabilistic forecasts. We also compare the ability of\ndifferent hierarchical techniques in improving the bottom level forecasters'\nperformances. Both the raw and cleaned datasets, including meteorological data,\nare made publicly available to provide a standard benchmark for evaluating\nforecasting algorithms for demand-side management applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:38:56 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 08:11:46 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Nespoli", "Lorenzo", ""], ["Medici", "Vasco", ""], ["Lopatichki", "Kristijan", ""], ["Sossan", "Fabrizio", ""]]}, {"id": "1910.03980", "submitter": "Andrea Giorgetti", "authors": "Andrea Mariani and Andrea Giorgetti and Marco Chiani", "title": "Model Order Selection Based on Information Theoretic Criteria: Design of\n  the Penalty", "comments": "11 pages, 8 figures, journal", "journal-ref": "IEEE Trans. on Signal Processing, vol. 63, no. 11, pp. 2779-2789,\n  June 2015", "doi": "10.1109/TSP.2015.2414900", "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic criteria (ITC) have been widely adopted in engineering\nand statistics for selecting, among an ordered set of candidate models, the one\nthat better fits the observed sample data. The selected model minimizes a\npenalized likelihood metric, where the penalty is determined by the criterion\nadopted. While rules for choosing a penalty that guarantees a consistent\nestimate of the model order are known, theoretical tools for its design with\nfinite samples have never been provided in a general setting. In this paper, we\nstudy model order selection for finite samples under a design perspective,\nfocusing on the generalized information criterion (GIC), which embraces the\nmost common ITC. The theory is general, and as case studies we consider: a) the\nproblem of estimating the number of signals embedded in additive white Gaussian\nnoise (AWGN) by using multiple sensors; b) model selection for the general\nlinear model (GLM), which includes e.g. the problem of estimating the number of\nsinusoids in AWGN. The analysis reveals a trade-off between the probabilities\nof overestimating and underestimating the order of the model. We then propose\nto design the GIC penalty to minimize underestimation while keeping the\noverestimation probability below a specified level. For the considered\nproblems, this method leads to analytical derivation of the optimal penalty for\na given sample size. A performance comparison between the penalty optimized GIC\nand common AIC and BIC is provided, demonstrating the effectiveness of the\nproposed design strategy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:48:16 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Mariani", "Andrea", ""], ["Giorgetti", "Andrea", ""], ["Chiani", "Marco", ""]]}, {"id": "1910.04023", "submitter": "Ignacio Arroyo-Fern\\'andez", "authors": "Ignacio Arroyo-Fern\\'andez and Mauricio Carrasco-Ru\\'iz and J. Anibal\n  Arias-Aguilar", "title": "On the Possibility of Rewarding Structure Learning Agents: Mutual\n  Information on Linguistic Random Sets", "comments": "Paper accepted to the Workshop on Sets & Partitions (NeurIPS 2019,\n  Vancouver, Canada)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a first attempt to elucidate a theoretical and empirical approach\nto design the reward provided by a natural language environment to some\nstructure learning agent. To this end, we revisit the Information Theory of\nunsupervised induction of phrase-structure grammars to characterize the\nbehavior of simulated actions modeled as set-valued random variables (random\nsets of linguistic samples) constituting semantic structures. Our results\nshowed empirical evidence of that simulated semantic structures (Open\nInformation Extraction triplets) can be distinguished from randomly constructed\nones by observing the Mutual Information among their constituents. This\nsuggests the possibility of rewarding structure learning agents without using\npretrained structural analyzers (oracle actors/experts).\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:33:37 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 01:34:52 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 01:48:19 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 16:56:56 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Arroyo-Fern\u00e1ndez", "Ignacio", ""], ["Carrasco-Ru\u00edz", "Mauricio", ""], ["Arias-Aguilar", "J. Anibal", ""]]}, {"id": "1910.04034", "submitter": "Victor Gabillon", "authors": "Victor Gabillon, Rasul Tutunov, Michal Valko, Haitham Bou Ammar", "title": "Derivative-Free & Order-Robust Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formalise order-robust optimisation as an instance of\nonline learning minimising simple regret, and propose Vroom, a zero'th order\noptimisation algorithm capable of achieving vanishing regret in non-stationary\nenvironments, while recovering favorable rates under stochastic\nreward-generating processes. Our results are the first to target simple regret\ndefinitions in adversarial scenarios unveiling a challenge that has been rarely\nconsidered in prior work.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:51:23 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 13:01:50 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 08:49:05 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gabillon", "Victor", ""], ["Tutunov", "Rasul", ""], ["Valko", "Michal", ""], ["Ammar", "Haitham Bou", ""]]}, {"id": "1910.04054", "submitter": "Olivier Delalleau", "authors": "Viswanath Sivakumar, Olivier Delalleau, Tim Rockt\\\"aschel, Alexander\n  H. Miller, Heinrich K\\\"uttler, Nantas Nardelli, Mike Rabbat, Joelle Pineau,\n  Sebastian Riedel", "title": "MVFST-RL: An Asynchronous RL Framework for Congestion Control with\n  Delayed Actions", "comments": "Workshop on ML for Systems at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective network congestion control strategies are key to keeping the\nInternet (or any large computer network) operational. Network congestion\ncontrol has been dominated by hand-crafted heuristics for decades. Recently,\nReinforcementLearning (RL) has emerged as an alternative to automatically\noptimize such control strategies. Research so far has primarily considered RL\ninterfaces which block the sender while an agent considers its next action.\nThis is largely an artifact of building on top of frameworks designed for RL in\ngames (e.g. OpenAI Gym). However, this does not translate to real-world\nnetworking environments, where a network sender waiting on a policy without\nsending data leads to under-utilization of bandwidth. We instead propose to\nformulate congestion control with an asynchronous RL agent that handles delayed\nactions. We present MVFST-RL, a scalable framework for congestion control in\nthe QUIC transport protocol that leverages state-of-the-art in asynchronous RL\ntraining with off-policy correction. We analyze modeling improvements to\nmitigate the deviation from Markovian dynamics, and evaluate our method on\nemulated networks from the Pantheon benchmark platform. The source code is\npublicly available at https://github.com/facebookresearch/mvfst-rl.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:12:30 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 14:49:47 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 17:02:53 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 21:52:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sivakumar", "Viswanath", ""], ["Delalleau", "Olivier", ""], ["Rockt\u00e4schel", "Tim", ""], ["Miller", "Alexander H.", ""], ["K\u00fcttler", "Heinrich", ""], ["Nardelli", "Nantas", ""], ["Rabbat", "Mike", ""], ["Pineau", "Joelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1910.04056", "submitter": "Saida Mahmoud", "authors": "Marco Menardi, Alex Falcon, Saida S.Mohamed, Lorenzo Seidenari,\n  Giuseppe Serra, Alberto Del Bimbo and Carlo Tasso", "title": "Text-to-Image Synthesis Based on Machine Generated Captions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text to Image Synthesis refers to the process of automatic generation of a\nphoto-realistic image starting from a given text and is revolutionizing many\nreal-world applications. In order to perform such process it is necessary to\nexploit datasets containing captioned images, meaning that each image is\nassociated with one (or more) captions describing it. Despite the abundance of\nuncaptioned images datasets, the number of captioned datasets is limited. To\naddress this issue, in this paper we propose an approach capable of generating\nimages starting from a given text using conditional GANs trained on uncaptioned\nimages dataset. In particular, uncaptioned images are fed to an Image\nCaptioning Module to generate the descriptions. Then, the GAN Module is trained\non both the input image and the machine-generated caption. To evaluate the\nresults, the performance of our solution is compared with the results obtained\nby the unconditional GAN. For the experiments, we chose to use the uncaptioned\ndataset LSUN bedroom. The results obtained in our study are preliminary but\nstill promising.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:14:09 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Menardi", "Marco", ""], ["Falcon", "Alex", ""], ["Mohamed", "Saida S.", ""], ["Seidenari", "Lorenzo", ""], ["Serra", "Giuseppe", ""], ["Del Bimbo", "Alberto", ""], ["Tasso", "Carlo", ""]]}, {"id": "1910.04062", "submitter": "Andri Ashfahani", "authors": "Andri Ashfahani, Mahardhika Pratama, Edwin Lughofer and Yew Soon Ong", "title": "DEVDAN: Deep Evolving Denoising Autoencoder", "comments": "This paper has been accepted for publication in Neurocomputing 2019.\n  arXiv admin note: substantial text overlap with arXiv:1809.09081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Denoising Autoencoder (DAE) enhances the flexibility of the data stream\nmethod in exploiting unlabeled samples. Nonetheless, the feasibility of DAE for\ndata stream analytic deserves an in-depth study because it characterizes a\nfixed network capacity that cannot adapt to rapidly changing environments. Deep\nevolving denoising autoencoder (DEVDAN), is proposed in this paper. It features\nan open structure in the generative phase and the discriminative phase where\nthe hidden units can be automatically added and discarded on the fly. The\ngenerative phase refines the predictive performance of the discriminative model\nexploiting unlabeled data. Furthermore, DEVDAN is free of the problem-specific\nthreshold and works fully in the single-pass learning fashion. We show that\nDEVDAN can find competitive network architecture compared with state-of-the-art\nmethods on the classification task using ten prominent datasets simulated under\nthe prequential test-then-train protocol.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:02:19 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 12:22:54 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Ashfahani", "Andri", ""], ["Pratama", "Mahardhika", ""], ["Lughofer", "Edwin", ""], ["Ong", "Yew Soon", ""]]}, {"id": "1910.04067", "submitter": "Weiwei Li", "authors": "Weiwei Li, Jan Hannig, Corbin Jones", "title": "A Note on Optimal Sampling Strategy for Structural Variant Detection\n  Using Optical Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural variants compose the majority of human genetic variation, but are\ndifficult to assess using current genomic sequencing technologies. Optical\nmapping technologies, which measure the size of chromosomal fragments between\nlabeled markers, offer an alternative approach. As these technologies mature\ntowards becoming clinical tools, there is a need to develop an approach for\ndetermining the optimal strategy for sampling biological material in order to\ndetect a variant at some threshold. Here we develop an optimization approach\nusing a simple, yet realistic, model of the genomic mapping process using a\nhyper-geometric distribution and {probabilistic} concentration inequalities.\nOur approach is both computationally and analytically tractable and includes a\nnovel approach to getting tail bounds of hyper-geometric distribution. We show\nthat if a genomic mapping technology can sample most of the chromosomal\nfragments within a sample, comparatively little biological material is needed\nto detect a variant at high confidence.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:18:13 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Li", "Weiwei", ""], ["Hannig", "Jan", ""], ["Jones", "Corbin", ""]]}, {"id": "1910.04069", "submitter": "Emilia Oikarinen", "authors": "Henri Tiittanen, Emilia Oikarinen, Andreas Henelius, Kai Puolam\\\"aki", "title": "Estimating regression errors without ground truth values", "comments": "33 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression analysis is a standard supervised machine learning method used to\nmodel an outcome variable in terms of a set of predictor variables. In most\nreal-world applications we do not know the true value of the outcome variable\nbeing predicted outside the training data, i.e., the ground truth is unknown.\nIt is hence not straightforward to directly observe when the estimate from a\nmodel potentially is wrong, due to phenomena such as overfitting and concept\ndrift. In this paper we present an efficient framework for estimating the\ngeneralization error of regression functions, applicable to any family of\nregression functions when the ground truth is unknown. We present a theoretical\nderivation of the framework and empirically evaluate its strengths and\nlimitations. We find that it performs robustly and is useful for detecting\nconcept drift in datasets in several real-world domains.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:29:37 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Tiittanen", "Henri", ""], ["Oikarinen", "Emilia", ""], ["Henelius", "Andreas", ""], ["Puolam\u00e4ki", "Kai", ""]]}, {"id": "1910.04076", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Sandesh Athni Hiremath, Stefan Milz, Christian Witt,\n  Clement Pinnard, Senthil Yogamani and Patrick Mader", "title": "FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation\n  using Monocular Fisheye Camera for Autonomous Driving", "comments": "Minor fixes added after ICRA 2020 camera ready submission. ICRA 2020\n  presentation video - https://www.youtube.com/watch?v=qAsdpHP5e8c&t", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisheye cameras are commonly used in applications like autonomous driving and\nsurveillance to provide a large field of view ($>180^{\\circ}$). However, they\ncome at the cost of strong non-linear distortions which require more complex\nalgorithms. In this paper, we explore Euclidean distance estimation on fisheye\ncameras for automotive scenes. Obtaining accurate and dense depth supervision\nis difficult in practice, but self-supervised learning approaches show\npromising results and could potentially overcome the problem. We present a\nnovel self-supervised scale-aware framework for learning Euclidean distance and\nego-motion from raw monocular fisheye videos without applying rectification.\nWhile it is possible to perform piece-wise linear approximation of fisheye\nprojection surface and apply standard rectilinear models, it has its own set of\nissues like re-sampling distortion and discontinuities in transition regions.\nTo encourage further research in this area, we will release our dataset as part\nof the WoodScape project \\cite{yogamani2019woodscape}. We further evaluated the\nproposed algorithm on the KITTI dataset and obtained state-of-the-art results\ncomparable to other self-supervised monocular methods. Qualitative results on\nan unseen fisheye video demonstrate impressive performance\nhttps://youtu.be/Sgq1WzoOmXg.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:51:38 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:06:13 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 18:11:34 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 19:29:03 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Hiremath", "Sandesh Athni", ""], ["Milz", "Stefan", ""], ["Witt", "Christian", ""], ["Pinnard", "Clement", ""], ["Yogamani", "Senthil", ""], ["Mader", "Patrick", ""]]}, {"id": "1910.04077", "submitter": "M. Sadegh Talebi", "authors": "Mahsa Asadi, Mohammad Sadegh Talebi, Hippolyte Bourel and\n  Odalric-Ambrym Maillard", "title": "Model-Based Reinforcement Learning Exploiting State-Action Equivalence", "comments": "ACML 2019. Recipient of the Best Student Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging an equivalence property in the state-space of a Markov Decision\nProcess (MDP) has been investigated in several studies. This paper studies\nequivalence structure in the reinforcement learning (RL) setup, where\ntransition distributions are no longer assumed to be known. We present a notion\nof similarity between transition probabilities of various state-action pairs of\nan MDP, which naturally defines an equivalence structure in the state-action\nspace. We present equivalence-aware confidence sets for the case where the\nlearner knows the underlying structure in advance. These sets are provably\nsmaller than their corresponding equivalence-oblivious counterparts. In the\nmore challenging case of an unknown equivalence structure, we present an\nalgorithm called ApproxEquivalence that seeks to find an (approximate)\nequivalence structure, and define confidence sets using the approximate\nequivalence. To illustrate the efficacy of the presented confidence sets, we\npresent C-UCRL, as a natural modification of UCRL2 for RL in undiscounted MDPs.\nIn the case of a known equivalence structure, we show that C-UCRL improves over\nUCRL2 in terms of regret by a factor of $\\sqrt{SA/C}$, in any communicating MDP\nwith $S$ states, $A$ actions, and $C$ classes, which corresponds to a massive\nimprovement when $C \\ll SA$. To the best of our knowledge, this is the first\nwork providing regret bounds for RL when an equivalence structure in the MDP is\nefficiently exploited. In the case of an unknown equivalence structure, we show\nthrough numerical experiments that C-UCRL combined with ApproxEquivalence\noutperforms UCRL2 in ergodic MDPs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:50:05 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Asadi", "Mahsa", ""], ["Talebi", "Mohammad Sadegh", ""], ["Bourel", "Hippolyte", ""], ["Maillard", "Odalric-Ambrym", ""]]}, {"id": "1910.04085", "submitter": "Guillaume Staerman", "authors": "Guillaume Staerman, Pavlo Mozharovskyi and Stephan Cl\\'emen\\c{c}on", "title": "The Area of the Convex Hull of Sampled Curves: a Robust Functional\n  Statistical Depth Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ubiquity of sensors in the IoT era, statistical observations are\nbecoming increasingly available in the form of massive (multivariate)\ntime-series. Formulated as unsupervised anomaly detection tasks, an abundance\nof applications like aviation safety management, the health monitoring of\ncomplex infrastructures or fraud detection can now rely on such functional\ndata, acquired and stored with an ever finer granularity. The concept of\nstatistical depth, which reflects centrality of an arbitrary observation w.r.t.\na statistical population may play a crucial role in this regard, anomalies\ncorresponding to observations with 'small' depth. Supported by sound\ntheoretical and computational developments in the recent decades, it has proven\nto be extremely useful, in particular in functional spaces. However, most\napproaches documented in the literature consist in evaluating independently the\ncentrality of each point forming the time series and consequently exhibit a\ncertain insensitivity to possible shape changes. In this paper, we propose a\nnovel notion of functional depth based on the area of the convex hull of\nsampled curves, capturing gradual departures from centrality, even beyond the\nenvelope of the data, in a natural fashion. We discuss practical relevance of\ncommonly imposed axioms on functional depths and investigate which of them are\nsatisfied by the notion of depth we promote here. Estimation and computational\nissues are also addressed and various numerical experiments provide empirical\nevidence of the relevance of the approach proposed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:13 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:14:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Staerman", "Guillaume", ""], ["Mozharovskyi", "Pavlo", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""]]}, {"id": "1910.04086", "submitter": "David Ginsbourger", "authors": "Poompol Buathong, David Ginsbourger, Tipaluck Krityakierne", "title": "Kernels over Sets of Finite Sets using RKHS Embeddings, with Application\n  to Bayesian (Combinatorial) Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on kernel methods for set-valued inputs and their application to\nBayesian set optimization, notably combinatorial optimization. We investigate\ntwo classes of set kernels that both rely on Reproducing Kernel Hilbert Space\nembeddings, namely the ``Double Sum'' (DS) kernels recently considered in\nBayesian set optimization, and a class introduced here called ``Deep\nEmbedding'' (DE) kernels that essentially consists in applying a radial kernel\non Hilbert space on top of the canonical distance induced by another kernel\nsuch as a DS kernel. We establish in particular that while DS kernels typically\nsuffer from a lack of strict positive definiteness, vast subclasses of DE\nkernels built upon DS kernels do possess this property, enabling in turn\ncombinatorial optimization without requiring to introduce a jitter parameter.\nProofs of theoretical results about considered kernels are complemented by a\nfew practicalities regarding hyperparameter fitting. We furthermore demonstrate\nthe applicability of our approach in prediction and optimization tasks, relying\nboth on toy examples and on two test cases from mechanical engineering and\nhydrogeology, respectively. Experimental results highlight the applicability\nand compared merits of the considered approaches while opening new perspectives\nin prediction and sequential design with set inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:38 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:55:58 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Buathong", "Poompol", ""], ["Ginsbourger", "David", ""], ["Krityakierne", "Tipaluck", ""]]}, {"id": "1910.04091", "submitter": "Kilian Fatras", "authors": "Kilian Fatras, Younes Zine, R\\'emi Flamary, R\\'emi Gribonval, Nicolas\n  Courty", "title": "Learning with minibatch Wasserstein : asymptotic and gradient properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport distances are powerful tools to compare probability\ndistributions and have found many applications in machine learning. Yet their\nalgorithmic complexity prevents their direct use on large scale datasets. To\novercome this challenge, practitioners compute these distances on minibatches\n{\\em i.e.} they average the outcome of several smaller optimal transport\nproblems. We propose in this paper an analysis of this practice, which effects\nare not well understood so far. We notably argue that it is equivalent to an\nimplicit regularization of the original problem, with appealing properties such\nas unbiased estimators, gradients and a concentration bound around the\nexpectation, but also with defects such as loss of distance property. Along\nwith this theoretical analysis, we also conduct empirical experiments on\ngradient flows, GANs or color transfer that highlight the practical interest of\nthis strategy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:08:43 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 08:26:40 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 09:34:48 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Fatras", "Kilian", ""], ["Zine", "Younes", ""], ["Flamary", "R\u00e9mi", ""], ["Gribonval", "R\u00e9mi", ""], ["Courty", "Nicolas", ""]]}, {"id": "1910.04098", "submitter": "Louis Kirsch", "authors": "Louis Kirsch, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Improving Generalization in Meta Reinforcement Learning using Learned\n  Objectives", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological evolution has distilled the experiences of many learners into the\ngeneral learning algorithms of humans. Our novel meta reinforcement learning\nalgorithm MetaGenRL is inspired by this process. MetaGenRL distills the\nexperiences of many complex agents to meta-learn a low-complexity neural\nobjective function that decides how future individuals will learn. Unlike\nrecent meta-RL algorithms, MetaGenRL can generalize to new environments that\nare entirely different from those used for meta-training. In some cases, it\neven outperforms human-engineered RL algorithms. MetaGenRL uses off-policy\nsecond-order gradients during meta-training that greatly increase its sample\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:20:48 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 16:56:33 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Kirsch", "Louis", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1910.04102", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Miko{\\l}aj Kasprzak, Trevor Campbell, Tamara\n  Broderick", "title": "Validated Variational Inference via Practical Posterior Error Bounds", "comments": "A python package for carrying out our validated variational inference\n  workflow -- including doing black-box variational inference and computing the\n  bounds we develop in this paper -- is available at\n  https://github.com/jhuggins/viabel. The same repository also contains code\n  for reproducing all of our experiments", "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational inference has become an increasingly attractive fast alternative\nto Markov chain Monte Carlo methods for approximate Bayesian inference.\nHowever, a major obstacle to the widespread use of variational methods is the\nlack of post-hoc accuracy measures that are both theoretically justified and\ncomputationally efficient. In this paper, we provide rigorous bounds on the\nerror of posterior mean and uncertainty estimates that arise from\nfull-distribution approximations, as in variational inference. Our bounds are\nwidely applicable, as they require only that the approximating and exact\nposteriors have polynomial moments. Our bounds are also computationally\nefficient for variational inference because they require only standard values\nfrom variational objectives, straightforward analytic calculations, and simple\nMonte Carlo estimates. We show that our analysis naturally leads to a new and\nimproved workflow for validated variational inference. Finally, we demonstrate\nthe utility of our proposed workflow and error bounds on a robust regression\nproblem and on a real-data example with a widely used multilevel hierarchical\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:29:21 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 14:44:19 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:04:06 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 04:06:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1910.04109", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Daniel Malinsky, Ilya Shpitser", "title": "Optimal Training of Fair Predictive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been sustained interest in modifying prediction algorithms\nto satisfy fairness constraints. These constraints are typically complex\nnonlinear functionals of the observed data distribution. Focusing on the causal\nconstraints proposed by Nabi and Shpitser (2018), we introduce new theoretical\nresults and optimization techniques to make model training easier and more\naccurate. Specifically, we show how to reparameterize the observed data\nlikelihood such that fairness constraints correspond directly to parameters\nthat appear in the likelihood, transforming a complex constrained optimization\nobjective into a simple optimization problem with box constraints. We also\nexploit methods from empirical likelihood theory in statistics to improve\npredictive performance, without requiring parametric models for\nhigh-dimensional feature vectors.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:44:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:28:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nabi", "Razieh", ""], ["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1910.04112", "submitter": "Honglin Li", "authors": "HongLin Li, Payam Barnaghi, Shirin Enshaeifar, Frieder Ganz", "title": "Continual Learning Using Bayesian Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continual learning models allow to learn and adapt to new changes and tasks\nover time. However, in continual and sequential learning scenarios in which the\nmodels are trained using different data with various distributions, neural\nnetworks tend to forget the previously learned knowledge. This phenomenon is\noften referred to as catastrophic forgetting. The catastrophic forgetting is an\ninevitable problem in continual learning models for dynamic environments. To\naddress this issue, we propose a method, called Continual Bayesian Learning\nNetworks (CBLN), which enables the networks to allocate additional resources to\nadapt to new tasks without forgetting the previously learned tasks. Using a\nBayesian Neural Network, CBLN maintains a mixture of Gaussian posterior\ndistributions that are associated with different tasks. The proposed method\ntries to optimise the number of resources that are needed to learn each task\nand avoids an exponential increase in the number of resources that are involved\nin learning multiple tasks. The proposed method does not need to access the\npast training data and can choose suitable weights to classify the data points\nduring the test time automatically based on an uncertainty criterion. We have\nevaluated our method on the MNIST and UCR time-series datasets. The evaluation\nresults show that our method can address the catastrophic forgetting problem at\na promising rate compared to the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:50:20 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 10:37:37 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Li", "HongLin", ""], ["Barnaghi", "Payam", ""], ["Enshaeifar", "Shirin", ""], ["Ganz", "Frieder", ""]]}, {"id": "1910.04115", "submitter": "Gregory Canal", "authors": "Gregory Canal, Stefano Fenu, Christopher Rozell", "title": "Active Ordinal Querying for Tuplewise Similarity Learning", "comments": "Canal and Fenu contributed equally; correction in metadata title -\n  metadata title now matches manuscript title; updated to camera-ready version\n  to appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks such as clustering, classification, and dataset\nsearch benefit from embedding data points in a space where distances reflect\nnotions of relative similarity as perceived by humans. A common way to\nconstruct such an embedding is to request triplet similarity queries to an\noracle, comparing two objects with respect to a reference. This work\ngeneralizes triplet queries to tuple queries of arbitrary size that ask an\noracle to rank multiple objects against a reference, and introduces an\nefficient and robust adaptive selection method called InfoTuple that uses a\nnovel approach to mutual information maximization. We show that the performance\nof InfoTuple at various tuple sizes exceeds that of the state-of-the-art\nadaptive triplet selection method on synthetic tests and new human response\ndatasets, and empirically demonstrate the significant gains in efficiency and\nquery consistency achieved by querying larger tuples instead of triplets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:55:30 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 00:49:36 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 00:47:12 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Canal", "Gregory", ""], ["Fenu", "Stefano", ""], ["Rozell", "Christopher", ""]]}, {"id": "1910.04153", "submitter": "Micha Livne", "authors": "Micha Livne, Kevin Swersky, David J. Fleet", "title": "High Mutual Information in Representation Learning with Symmetric\n  Variational Inference", "comments": "Bayesian Deep Learning Workshop (NeurIPS 2019). arXiv admin note:\n  substantial text overlap with arXiv:1910.03175", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Mutual Information Machine (MIM), a novel formulation of\nrepresentation learning, using a joint distribution over the observations and\nlatent state in an encoder/decoder framework. Our key principles are symmetry\nand mutual information, where symmetry encourages the encoder and decoder to\nlearn different factorizations of the same underlying distribution, and mutual\ninformation, to encourage the learning of useful representations for downstream\ntasks. Our starting point is the symmetric Jensen-Shannon divergence between\nthe encoding and decoding joint distributions, plus a mutual information\nencouraging regularizer. We show that this can be bounded by a tractable cross\nentropy loss function between the true model and a parameterized approximation,\nand relate this to the maximum likelihood framework. We also relate MIM to\nvariational autoencoders (VAEs) and demonstrate that MIM is capable of learning\nsymmetric factorizations, with high mutual information that avoids posterior\ncollapse.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:37:50 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Livne", "Micha", ""], ["Swersky", "Kevin", ""], ["Fleet", "David J.", ""]]}, {"id": "1910.04183", "submitter": "Yining Wang", "authors": "Xi Chen, Akshay Krishnamurthy, Yining Wang", "title": "Robust Dynamic Assortment Optimization in the Presence of Outlier\n  Customers", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dynamic assortment optimization problem under the multinomial\nlogit model (MNL) with unknown utility parameters. The main question\ninvestigated in this paper is model mis-specification under the\n$\\varepsilon$-contamination model, which is a fundamental model in robust\nstatistics and machine learning. In particular, throughout a selling horizon of\nlength $T$, we assume that customers make purchases according to a well\nspecified underlying multinomial logit choice model in a\n($1-\\varepsilon$)-fraction of the time periods, and make arbitrary purchasing\ndecisions instead in the remaining $\\varepsilon$-fraction of the time periods.\nIn this model, we develop a new robust online assortment optimization policy\nvia an active elimination strategy. We establish both upper and lower bounds on\nthe regret, and show that our policy is optimal up to logarithmic factor in T\nwhen the assortment capacity is constant. Furthermore, we develop a fully\nadaptive policy that does not require any prior knowledge of the contamination\nparameter $\\varepsilon$. Our simulation study shows that our policy outperforms\nthe existing policies based on upper confidence bounds (UCB) and Thompson\nsampling.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 18:02:15 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Chen", "Xi", ""], ["Krishnamurthy", "Akshay", ""], ["Wang", "Yining", ""]]}, {"id": "1910.04192", "submitter": "Clara McCreery", "authors": "Clara McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, Xavier\n  Amatriain", "title": "Domain-Relevant Embeddings for Medical Question Similarity", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rate at which medical questions are asked online far exceeds the capacity\nof qualified people to answer them, and many of these questions are not unique.\nIdentifying same-question pairs could enable questions to be answered more\neffectively. While many research efforts have focused on the problem of general\nquestion similarity for non-medical applications, these approaches do not\ngeneralize well to the medical domain, where medical expertise is often\nrequired to determine semantic similarity. In this paper, we show how a\nsemi-supervised approach of pre-training a neural network on medical\nquestion-answer pairs is a particularly useful intermediate task for the\nultimate goal of determining medical question similarity. While other\npre-training tasks yield an accuracy below 78.7% on this task, our model\nachieves an accuracy of 82.6% with the same number of training examples, and an\naccuracy of 80.0% with a much smaller training set.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 18:19:48 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 01:06:47 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["McCreery", "Clara", ""], ["Katariya", "Namit", ""], ["Kannan", "Anitha", ""], ["Chablani", "Manish", ""], ["Amatriain", "Xavier", ""]]}, {"id": "1910.04209", "submitter": "Jerry Ma", "authors": "Jerry Ma, Denis Yarats", "title": "On the adequacy of untuned warmup for adaptive optimization", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adaptive optimization algorithms such as Adam are widely used in deep\nlearning. The stability of such algorithms is often improved with a warmup\nschedule for the learning rate. Motivated by the difficulty of choosing and\ntuning warmup schedules, recent work proposes automatic variance rectification\nof Adam's adaptive learning rate, claiming that this rectified approach\n(\"RAdam\") surpasses the vanilla Adam algorithm and reduces the need for\nexpensive tuning of Adam with warmup. In this work, we refute this analysis and\nprovide an alternative explanation for the necessity of warmup based on the\nmagnitude of the update term, which is of greater relevance to training\nstability. We then provide some \"rule-of-thumb\" warmup schedules, and we\ndemonstrate that simple untuned warmup of Adam performs more-or-less\nidentically to RAdam in typical practical settings. We conclude by suggesting\nthat practitioners stick to linear warmup with Adam, with a sensible default\nbeing linear warmup over $2 / (1 - \\beta_2)$ training iterations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:25:03 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 01:58:24 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 03:43:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ma", "Jerry", ""], ["Yarats", "Denis", ""]]}, {"id": "1910.04214", "submitter": "Gal Yona", "authors": "Gal Yona and Amirata Ghorbani and James Zou", "title": "Who's responsible? Jointly quantifying the contribution of the learning\n  algorithm and training data", "comments": "To appear in AAAI/ACM Conference on AI, Ethics, and Society (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning algorithm $A$ trained on a dataset $D$ is revealed to have poor\nperformance on some subpopulation at test time. Where should the responsibility\nfor this lay? It can be argued that the data is responsible, if for example\ntraining $A$ on a more representative dataset $D'$ would have improved the\nperformance. But it can similarly be argued that $A$ itself is at fault, if\ntraining a different variant $A'$ on the same dataset $D$ would have improved\nperformance. As ML becomes widespread and such failure cases more common, these\ntypes of questions are proving to be far from hypothetical. With this\nmotivation in mind, in this work we provide a rigorous formulation of the joint\ncredit assignment problem between a learning algorithm $A$ and a dataset $D$.\nWe propose Extended Shapley as a principled framework for this problem, and\nexperiment empirically with how it can be used to address questions of ML\naccountability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:39:08 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 08:28:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yona", "Gal", ""], ["Ghorbani", "Amirata", ""], ["Zou", "James", ""]]}, {"id": "1910.04233", "submitter": "Kevin Liang", "authors": "Kevin J Liang, Guoyin Wang, Yitong Li, Ricardo Henao, Lawrence Carin", "title": "Kernel-Based Approaches for Sequence Modeling: Connections to Neural\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate time-dependent data analysis from the perspective of recurrent\nkernel machines, from which models with hidden units and gated memory cells\narise naturally. By considering dynamic gating of the memory cell, a model\nclosely related to the long short-term memory (LSTM) recurrent neural network\nis derived. Extending this setup to $n$-gram filters, the convolutional neural\nnetwork (CNN), Gated CNN, and recurrent additive network (RAN) are also\nrecovered as special cases. Our analysis provides a new perspective on the\nLSTM, while also extending it to $n$-gram convolutional filters. Experiments\nare performed on natural language processing tasks and on analysis of local\nfield potentials (neuroscience). We demonstrate that the variants we derive\nfrom kernels perform on par or even better than traditional neural methods. For\nthe neuroscience application, the new models demonstrate significant\nimprovements relative to the prior state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 20:15:53 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Liang", "Kevin J", ""], ["Wang", "Guoyin", ""], ["Li", "Yitong", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.04235", "submitter": "Zhouyuan Huo", "authors": "Zhouyuan Huo, Heng Huang", "title": "Straggler-Agnostic and Communication-Efficient Distributed Primal-Dual\n  Algorithm for High-Dimensional Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, reducing communication time between machines becomes the main focus\nof distributed data mining. Previous methods propose to make workers do more\ncomputation locally before aggregating local solutions in the server such that\nfewer communication rounds between server and workers are required. However,\nthese methods do not consider reducing the communication time per round and\nwork very poor under certain conditions, for example, when there are straggler\nproblems or the dataset is of high dimension. In this paper, we target to\nreduce communication time per round as well as the required communication\nrounds. We propose a communication-efficient distributed primal-dual method\nwith straggler-agnostic server and bandwidth-efficient workers. We analyze the\nconvergence property and prove that the proposed method guarantees linear\nconvergence rate to the optimal solution for convex problems. Finally, we\nconduct large-scale experiments in simulated and real distributed systems and\nexperimental results demonstrate that the proposed method is much faster than\ncompared methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 20:23:39 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1910.04241", "submitter": "Sachin Vernekar", "authors": "Sachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Taylor Denouden, Rick\n  Salay, Krzysztof Czarnecki", "title": "Out-of-distribution Detection in Classifiers via Generation", "comments": "NeurIPS 2019, Safety and Robustness in Decision Making Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By design, discriminatively trained neural network classifiers produce\nreliable predictions only for in-distribution samples. For their real-world\ndeployments, detecting out-of-distribution (OOD) samples is essential. Assuming\nOOD to be outside the closed boundary of in-distribution, typical neural\nclassifiers do not contain the knowledge of this boundary for OOD detection\nduring inference. There have been recent approaches to instill this knowledge\nin classifiers by explicitly training the classifier with OOD samples close to\nthe in-distribution boundary. However, these generated samples fail to cover\nthe entire in-distribution boundary effectively, thereby resulting in a\nsub-optimal OOD detector. In this paper, we analyze the feasibility of such\napproaches by investigating the complexity of producing such \"effective\" OOD\nsamples. We also propose a novel algorithm to generate such samples using a\nmanifold learning network (e.g., variational autoencoder) and then train an n+1\nclassifier for OOD detection, where the $n+1^{th}$ class represents the OOD\nsamples. We compare our approach against several recent classifier-based OOD\ndetectors on MNIST and Fashion-MNIST datasets. Overall the proposed approach\nconsistently performs better than the others.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 20:38:56 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Vernekar", "Sachin", ""], ["Gaurav", "Ashish", ""], ["Abdelzad", "Vahdat", ""], ["Denouden", "Taylor", ""], ["Salay", "Rick", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1910.04254", "submitter": "Alexander Preuhs", "authors": "Alexander Preuhs, Michael Manhart, Philipp Roser, Bernhard Stimpel,\n  Christopher Syben, Marios Psychogios, Markus Kowarschik, Andreas Maier", "title": "Image Quality Assessment for Rigid Motion Compensation", "comments": "Accepted at MedNeurips 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic stroke imaging with C-arm cone-beam computed tomography (CBCT)\nenables reduction of time-to-therapy for endovascular procedures. However, the\nprolonged acquisition time compared to helical CT increases the likelihood of\nrigid patient motion. Rigid motion corrupts the geometry alignment assumed\nduring reconstruction, resulting in image blurring or streaking artifacts. To\nreestablish the geometry, we estimate the motion trajectory by an autofocus\nmethod guided by a neural network, which was trained to regress the\nreprojection error, based on the image information of a reconstructed slice.\nThe network was trained with CBCT scans from 19 patients and evaluated using an\nadditional test patient. It adapts well to unseen motion amplitudes and\nachieves superior results in a motion estimation benchmark compared to the\ncommonly used entropy-based method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:03:45 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 16:01:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Preuhs", "Alexander", ""], ["Manhart", "Michael", ""], ["Roser", "Philipp", ""], ["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["Psychogios", "Marios", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1910.04256", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Anh Nguyen", "title": "Explaining image classifiers by removing input features using generative\n  models", "comments": "Accepted to Asian Conference on Computer Vision (ACCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturbation-based explanation methods often measure the contribution of an\ninput feature to an image classifier's outputs by heuristically removing it via\ne.g. blurring, adding noise, or graying out, which often produce unrealistic,\nout-of-samples. Instead, we propose to integrate a generative inpainter into\nthree representative attribution methods to remove an input feature. Our\nproposed change improved all three methods in (1) generating more plausible\ncounterfactual samples under the true data distribution; (2) being more\naccurate according to three metrics: object localization, deletion, and\nsaliency metrics; and (3) being more robust to hyperparameter changes. Our\nfindings were consistent across both ImageNet and Places365 datasets and two\ndifferent pairs of classifiers and inpainters.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:08:25 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 03:36:07 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 17:52:27 GMT"}, {"version": "v4", "created": "Sat, 25 Jul 2020 09:14:35 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 01:27:39 GMT"}, {"version": "v6", "created": "Sun, 4 Oct 2020 19:27:15 GMT"}, {"version": "v7", "created": "Tue, 6 Oct 2020 16:08:42 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Agarwal", "Chirag", ""], ["Nguyen", "Anh", ""]]}, {"id": "1910.04257", "submitter": "Samyadeep Basu", "authors": "Samyadeep Basu, Rauf Izmailov and Chris Mesterharm", "title": "Membership Model Inversion Attacks for Deep Networks", "comments": "NeurIPS 2019, Workshop on Privacy in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of AI, inherent security and privacy\nvulnerabilities formachine learning systems are being discovered. One such\nvulnerability makes itpossible for an adversary to obtain private information\nabout the types of instancesused to train the targeted machine learning model.\nThis so-called model inversionattack is based on sequential leveraging of\nclassification scores towards obtaininghigh confidence representations for\nvarious classes. However, for deep networks,such procedures usually lead to\nunrecognizable representations that are uselessfor the adversary. In this\npaper, we introduce a more realistic definition of modelinversion, where the\nadversary is aware of the general purpose of the attackedmodel (for instance,\nwhether it is an OCR system or a facial recognition system),and the goal is to\nfind realistic class representations within the corresponding lower-dimensional\nmanifold (of, respectively, general symbols or general faces). To thatend, we\nleverage properties of generative adversarial networks for constructinga\nconnected lower-dimensional manifold, and demonstrate the efficiency of\nourmodel inversion attack that is carried out within that manifold.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:18:53 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Basu", "Samyadeep", ""], ["Izmailov", "Rauf", ""], ["Mesterharm", "Chris", ""]]}, {"id": "1910.04267", "submitter": "Changxiao Cai", "authors": "Changxiao Cai, Gen Li, Yuejie Chi, H. Vincent Poor, Yuxin Chen", "title": "Subspace Estimation from Unbalanced and Incomplete Data Matrices:\n  $\\ell_{2,\\infty}$ Statistical Guarantees", "comments": "Accepted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with estimating the column space of an unknown\nlow-rank matrix $\\boldsymbol{A}^{\\star}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$,\ngiven noisy and partial observations of its entries. There is no shortage of\nscenarios where the observations -- while being too noisy to support faithful\nrecovery of the entire matrix -- still convey sufficient information to enable\nreliable estimation of the column space of interest. This is particularly\nevident and crucial for the highly unbalanced case where the column dimension\n$d_{2}$ far exceeds the row dimension $d_{1}$, which is the focal point of the\ncurrent paper. We investigate an efficient spectral method, which operates upon\nthe sample Gram matrix with diagonal deletion. While this algorithmic idea has\nbeen studied before, we establish new statistical guarantees for this method in\nterms of both $\\ell_{2}$ and $\\ell_{2,\\infty}$ estimation accuracy, which\nimprove upon prior results if $d_{2}$ is substantially larger than $d_{1}$. To\nillustrate the effectiveness of our findings, we derive matching minimax lower\nbounds with respect to the noise levels, and develop consequences of our\ngeneral theory for three applications of practical importance: (1) tensor\ncompletion from noisy data, (2) covariance estimation / principal component\nanalysis with missing data, and (3) community recovery in bipartite graphs. Our\ntheory leads to improved performance guarantees for all three cases.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:39:04 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 22:16:18 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 14:01:08 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 00:25:35 GMT"}, {"version": "v5", "created": "Sun, 15 Nov 2020 21:04:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cai", "Changxiao", ""], ["Li", "Gen", ""], ["Chi", "Yuejie", ""], ["Poor", "H. Vincent", ""], ["Chen", "Yuxin", ""]]}, {"id": "1910.04279", "submitter": "Shixian Wen", "authors": "Shixian Wen, Laurent Itti", "title": "Adversarial Training: embedding adversarial perturbations into the\n  parameter space of a neural network to build a robust system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training, in which a network is trained on both adversarial and\nclean examples, is one of the most trusted defense methods against adversarial\nattacks. However, there are three major practical difficulties in implementing\nand deploying this method - expensive in terms of extra memory and computation\ncosts; accuracy trade-off between clean and adversarial examples; and lack of\ndiversity of adversarial perturbations. Classical adversarial training uses\nfixed, precomputed perturbations in adversarial examples (input space). In\ncontrast, we introduce dynamic adversarial perturbations into the parameter\nspace of the network, by adding perturbation biases to the fully connected\nlayers of deep convolutional neural network. During training, using only clean\nimages, the perturbation biases are updated in the Fast Gradient Sign Direction\nto automatically create and store adversarial perturbations by recycling the\ngradient information computed. The network learns and adjusts itself\nautomatically to these learned adversarial perturbations. Thus, we can achieve\nadversarial training with negligible cost compared to requiring a training set\nof adversarial example images. In addition, if combined with classical\nadversarial training, our perturbation biases can alleviate accuracy trade-off\ndifficulties, and diversify adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:16:09 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Wen", "Shixian", ""], ["Itti", "Laurent", ""]]}, {"id": "1910.04281", "submitter": "Nicholas Waytowich", "authors": "Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John\n  Valasek, Nicholas R. Waytowich", "title": "Integrating Behavior Cloning and Reinforcement Learning for Improved\n  Performance in Dense and Sparse Reward Environments", "comments": "9 pages, 5 Figures. AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to efficiently transition and update policies,\ntrained initially with demonstrations, using off-policy actor-critic\nreinforcement learning. It is well-known that techniques based on Learning from\nDemonstrations, for example behavior cloning, can lead to proficient policies\ngiven limited data. However, it is currently unclear how to efficiently update\nthat policy using reinforcement learning as these approaches are inherently\noptimizing different objective functions. Previous works have used loss\nfunctions, which combine behavior cloning losses with reinforcement learning\nlosses to enable this update. However, the components of these loss functions\nare often set anecdotally, and their individual contributions are not well\nunderstood. In this work, we propose the Cycle-of-Learning (CoL) framework that\nuses an actor-critic architecture with a loss function that combines behavior\ncloning and 1-step Q-learning losses with an off-policy pre-training step from\nhuman demonstrations. This enables transition from behavior cloning to\nreinforcement learning without performance degradation and improves\nreinforcement learning in terms of overall performance and training time.\nAdditionally, we carefully study the composition of these combined losses and\ntheir impact on overall policy learning. We show that our approach outperforms\nstate-of-the-art techniques for combining behavior cloning and reinforcement\nlearning for both dense and sparse reward scenarios. Our results also suggest\nthat directly including the behavior cloning loss on demonstration data helps\nto ensure stable learning and ground future policy updates.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:32:23 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 19:08:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Goecks", "Vinicius G.", ""], ["Gremillion", "Gregory M.", ""], ["Lawhern", "Vernon J.", ""], ["Valasek", "John", ""], ["Waytowich", "Nicholas R.", ""]]}, {"id": "1910.04284", "submitter": "Colin Wei", "authors": "Colin Wei, Tengyu Ma", "title": "Improved Sample Complexities for Deep Networks and Robust Classification\n  via an All-Layer Margin", "comments": "Code for all-layer margin optimization is available at the following\n  link: https://github.com/cwein3/all-layer-margin-opt. Version 4: Re-organized\n  proofs for more clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For linear classifiers, the relationship between (normalized) output margin\nand generalization is captured in a clear and simple bound -- a large output\nmargin implies good generalization. Unfortunately, for deep models, this\nrelationship is less clear: existing analyses of the output margin give\ncomplicated bounds which sometimes depend exponentially on depth. In this work,\nwe propose to instead analyze a new notion of margin, which we call the\n\"all-layer margin.\" Our analysis reveals that the all-layer margin has a clear\nand direct relationship with generalization for deep models. This enables the\nfollowing concrete applications of the all-layer margin: 1) by analyzing the\nall-layer margin, we obtain tighter generalization bounds for neural nets which\ndepend on Jacobian and hidden layer norms and remove the exponential dependency\non depth 2) our neural net results easily translate to the adversarially robust\nsetting, giving the first direct analysis of robust test error for deep\nnetworks, and 3) we present a theoretically inspired training algorithm for\nincreasing the all-layer margin. Our algorithm improves both clean and\nadversarially robust test performance over strong baselines in practice.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:45:45 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 08:52:51 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 06:24:54 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 08:30:15 GMT"}, {"version": "v5", "created": "Wed, 16 Jun 2021 05:12:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wei", "Colin", ""], ["Ma", "Tengyu", ""]]}, {"id": "1910.04301", "submitter": "Yueming Lyu", "authors": "Yueming Lyu and Ivor W. Tsang", "title": "Black-box Optimizer with Implicit Natural Gradient", "comments": "Black-box Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box optimization is primarily important for many compute-intensive\napplications, including reinforcement learning (RL), robot control, etc. This\npaper presents a novel theoretical framework for black-box optimization, in\nwhich our method performs stochastic update with the implicit natural gradient\nof an exponential-family distribution. Theoretically, we prove the convergence\nrate of our framework with full matrix update for convex functions. Our\ntheoretical results also hold for continuous non-differentiable black-box\nfunctions. Our methods are very simple and contain less hyper-parameters than\nCMA-ES \\cite{hansen2006cma}. Empirically, our method with full matrix update\nachieves competitive performance compared with one of the state-of-the-art\nmethod CMA-ES on benchmark test problems. Moreover, our methods can achieve\nhigh optimization precision on some challenging test functions (e.g.,\n$l_1$-norm ellipsoid test problem and Levy test problem), while methods with\nexplicit natural gradient, i.e., IGO \\cite{ollivier2017information} with full\nmatrix update can not. This shows the efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 23:34:36 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 23:22:51 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 10:46:10 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Lyu", "Yueming", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "1910.04302", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei, Michalis K.\n  Titsias", "title": "Prescribed Generative Adversarial Networks", "comments": "Code for this paper can be found at\n  https://github.com/adjidieng/PresGANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a powerful approach to\nunsupervised learning. They have achieved state-of-the-art performance in the\nimage domain. However, GANs are limited in two ways. They often learn\ndistributions with low support---a phenomenon known as mode collapse---and they\ndo not guarantee the existence of a probability density, which makes evaluating\ngeneralization using predictive log-likelihood impossible. In this paper, we\ndevelop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs\nadd noise to the output of a density network and optimize an\nentropy-regularized adversarial loss. The added noise renders tractable\napproximations of the predictive log-likelihood and stabilizes the training\nprocedure. The entropy regularizer encourages PresGANs to capture all the modes\nof the data distribution. Fitting PresGANs involves computing the intractable\ngradients of the entropy regularization term; PresGANs sidestep this\nintractability using unbiased stochastic estimates. We evaluate PresGANs on\nseveral datasets and found they mitigate mode collapse and generate samples\nwith high perceptual quality. We further found that PresGANs reduce the gap in\nperformance in terms of predictive log-likelihood between traditional GANs and\nvariational autoencoders (VAEs).\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 23:40:05 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Dieng", "Adji B.", ""], ["Ruiz", "Francisco J. R.", ""], ["Blei", "David M.", ""], ["Titsias", "Michalis K.", ""]]}, {"id": "1910.04322", "submitter": "Mingrui Zhang", "authors": "Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, Amin\n  Karbasi", "title": "One Sample Stochastic Frank-Wolfe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the beauties of the projected gradient descent method lies in its\nrather simple mechanism and yet stable behavior with inexact, stochastic\ngradients, which has led to its wide-spread use in many machine learning\napplications. However, once we replace the projection operator with a simpler\nlinear program, as is done in the Frank-Wolfe method, both simplicity and\nstability take a serious hit. The aim of this paper is to bring them back\nwithout sacrificing the efficiency. In this paper, we propose the first\none-sample stochastic Frank-Wolfe algorithm, called 1-SFW, that avoids the need\nto carefully tune the batch size, step size, learning rate, and other\ncomplicated hyper parameters. In particular, 1-SFW achieves the optimal\nconvergence rate of $\\mathcal{O}(1/\\epsilon^2)$ for reaching an\n$\\epsilon$-suboptimal solution in the stochastic convex setting, and a\n$(1-1/e)-\\epsilon$ approximate solution for a stochastic monotone DR-submodular\nmaximization problem. Moreover, in a general non-convex setting, 1-SFW finds an\n$\\epsilon$-first-order stationary point after at most\n$\\mathcal{O}(1/\\epsilon^3)$ iterations, achieving the current best known\nconvergence rate. All of this is possible by designing a novel unbiased\nmomentum estimator that governs the stability of the optimization process while\nusing a single sample at each iteration.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 01:35:31 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zhang", "Mingrui", ""], ["Shen", "Zebang", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1910.04329", "submitter": "Keizo Kato", "authors": "Keizo Kato, Jing Zhou, Tomotake Sasaki, and Akira Nakagawa", "title": "Rate-Distortion Optimization Guided Autoencoder for Isometric Embedding\n  in Euclidean Latent Space", "comments": "Accepted to the International Conference on Machine Learning (ICML)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze high-dimensional and complex data in the real world, deep\ngenerative models, such as variational autoencoder (VAE) embed data in a\nlow-dimensional space (latent space) and learn a probabilistic model in the\nlatent space. However, they struggle to accurately reproduce the probability\ndistribution function (PDF) in the input space from that in the latent space.\nIf the embedding were isometric, this issue can be solved, because the relation\nof PDFs can become tractable. To achieve isometric property, we propose Rate-\nDistortion Optimization guided autoencoder inspired by orthonormal transform\ncoding. We show our method has the following properties: (i) the Jacobian\nmatrix between the input space and a Euclidean latent space forms a\nconstantlyscaled orthonormal system and enables isometric data embedding; (ii)\nthe relation of PDFs in both spaces can become tractable one such as\nproportional relation. Furthermore, our method outperforms state-of-the-art\nmethods in unsupervised anomaly detection with four public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:03:22 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 10:33:15 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 08:49:29 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 03:32:25 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kato", "Keizo", ""], ["Zhou", "Jing", ""], ["Sasaki", "Tomotake", ""], ["Nakagawa", "Akira", ""]]}, {"id": "1910.04332", "submitter": "Zachary Sunberg", "authors": "Michael H. Lim, Claire J. Tomlin, Zachary N. Sunberg", "title": "Sparse tree search optimality guarantees in POMDPs with continuous\n  observation spaces", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2020/572", "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) with continuous state\nand observation spaces have powerful flexibility for representing real-world\ndecision and control problems but are notoriously difficult to solve. Recent\nonline sampling-based algorithms that use observation likelihood weighting have\nshown unprecedented effectiveness in domains with continuous observation\nspaces. However there has been no formal theoretical justification for this\ntechnique. This work offers such a justification, proving that a simplified\nalgorithm, partially observable weighted sparse sampling (POWSS), will estimate\nQ-values accurately with high probability and can be made to perform\narbitrarily near the optimal solution by increasing computational power.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:22:55 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 07:17:55 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 23:16:44 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lim", "Michael H.", ""], ["Tomlin", "Claire J.", ""], ["Sunberg", "Zachary N.", ""]]}, {"id": "1910.04341", "submitter": "Chang Wei Tan", "authors": "Chang Wei Tan, Francois Petitjean, Eamonn Keogh, Geoffrey I. Webb", "title": "Time series classification for varying length series", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into time series classification has tended to focus on the case of\nseries of uniform length. However, it is common for real-world time series data\nto have unequal lengths. Differing time series lengths may arise from a number\nof fundamentally different mechanisms. In this work, we identify and evaluate\ntwo classes of such mechanisms -- variations in sampling rate relative to the\nrelevant signal and variations between the start and end points of one time\nseries relative to one another. We investigate how time series generated by\neach of these classes of mechanism are best addressed for time series\nclassification. We perform extensive experiments and provide practical\nrecommendations on how variations in length should be handled in time series\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:53:28 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Tan", "Chang Wei", ""], ["Petitjean", "Francois", ""], ["Keogh", "Eamonn", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1910.04357", "submitter": "Wei Xu", "authors": "Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin\n  Yager, Wei Xu", "title": "Visual Understanding of Multiple Attributes Learning Model of X-Ray\n  Scattering Images", "comments": "5 pages, 2 figures, ICCV conference co-held XAIC workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract presents a visualization system, which is designed for\ndomain scientists to visually understand their deep learning model of\nextracting multiple attributes in x-ray scattering images. The system focuses\non studying the model behaviors related to multiple structural attributes. It\nallows users to explore the images in the feature space, the classification\noutput of different attributes, with respect to the actual attributes labelled\nby domain scientists. Abundant interactions allow users to flexibly select\ninstance images, their clusters, and compare them visually in details. Two\npreliminary case studies demonstrate its functionalities and usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 03:51:58 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Huang", "Xinyi", ""], ["Jamonnak", "Suphanut", ""], ["Zhao", "Ye", ""], ["Wang", "Boyu", ""], ["Hoai", "Minh", ""], ["Yager", "Kevin", ""], ["Xu", "Wei", ""]]}, {"id": "1910.04371", "submitter": "Vishnu Suresh Lokhande", "authors": "Muni Sreenivas Pydi and Vishnu Suresh Lokhande", "title": "Active Learning with Importance Sampling", "comments": "NeurIPS 2019 Workshop on Machine Learning with Guarantees, Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an active learning setting where the algorithm has access to a\nlarge pool of unlabeled data and a small pool of labeled data. In each\niteration, the algorithm chooses few unlabeled data points and obtains their\nlabels from an oracle. In this paper, we consider a probabilistic querying\nprocedure to choose the points to be labeled. We propose an algorithm for\nActive Learning with Importance Sampling (ALIS), and derive upper bounds on the\ntrue loss incurred by the algorithm for any arbitrary probabilistic sampling\nprocedure. Further, we propose an optimal sampling distribution that minimizes\nthe upper bound on the true loss.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 05:11:08 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Pydi", "Muni Sreenivas", ""], ["Lokhande", "Vishnu Suresh", ""]]}, {"id": "1910.04375", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Estimating Transfer Entropy via Copula Entropy", "comments": "17 pages, 5 figures. with new experiments, discussion, and section on\n  related research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery is a fundamental problem in statistics and has wide\napplications in different fields. Transfer Entropy (TE) is a important notion\ndefined for measuring causality, which is essentially conditional Mutual\nInformation (MI). Copula Entropy (CE) is a theory on measurement of statistical\nindependence and is equivalent to MI. In this paper, we prove that TE can be\nrepresented with only CE and then propose a non-parametric method for\nestimating TE via CE. The proposed method was applied to analyze the Beijing\nPM2.5 data in the experiments. Experimental results show that the proposed\nmethod can infer causality relationships from data effectively and hence help\nto understand the data better.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 05:49:03 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 00:39:33 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 09:39:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "1910.04382", "submitter": "Yang Liu", "authors": "Yang Liu and David P. Helmbold", "title": "Online Learning Using Only Peer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a variant of the classical online learning problem with\nexpert predictions. Our model's differences and challenges are due to lacking\nany direct feedback on the loss each expert incurs at each time step $t$. We\npropose an approach that uses peer prediction and identify conditions where it\nsucceeds. Our techniques revolve around a carefully designed peer score\nfunction $s()$ that scores experts' predictions based on the peer consensus. We\nshow a sufficient condition, that we call \\emph{peer calibration}, under which\nstandard online learning algorithms using loss feedback computed by the\ncarefully crafted $s()$ have bounded regret with respect to the unrevealed\nground truth values. We then demonstrate how suitable $s()$ functions can be\nderived for different assumptions and models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:19:30 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 06:48:27 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Yang", ""], ["Helmbold", "David P.", ""]]}, {"id": "1910.04385", "submitter": "Nontawat Charoenphakdee", "authors": "Nontawat Charoenphakdee, Jongyeong Lee, Yiping Jin, Dittaya Wanvarie,\n  Masashi Sugiyama", "title": "Learning Only from Relevant Keywords and Unlabeled Documents", "comments": "EMNLP-IJCNLP2019, fix typos in Theorem 1: change $\\pi$ and $\\pi'$ to\n  $\\theta$ and $\\theta'$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a document classification problem where document labels are\nabsent but only relevant keywords of a target class and unlabeled documents are\ngiven. Although heuristic methods based on pseudo-labeling have been\nconsidered, theoretical understanding of this problem has still been limited.\nMoreover, previous methods cannot easily incorporate well-developed techniques\nin supervised text classification. In this paper, we propose a theoretically\nguaranteed learning framework that is simple to implement and has flexible\nchoices of models, e.g., linear models or neural networks. We demonstrate how\nto optimize the area under the receiver operating characteristic curve (AUC)\neffectively and also discuss how to adjust it to optimize other well-known\nevaluation metrics such as the accuracy and F1-measure. Finally, we show the\neffectiveness of our framework using benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:29:22 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 03:40:51 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Charoenphakdee", "Nontawat", ""], ["Lee", "Jongyeong", ""], ["Jin", "Yiping", ""], ["Wanvarie", "Dittaya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.04388", "submitter": "Yuma Koizumi", "authors": "Luca Mazzon, Yuma Koizumi, Masahiro Yasuda, Noboru Harada", "title": "First Order Ambisonics Domain Spatial Augmentation for DNN-based\n  Direction of Arrival Estimation", "comments": "5 pages, to appear in DCASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data augmentation method for training\nneural networks for Direction of Arrival (DOA) estimation. This method focuses\non expanding the representation of the DOA subspace of a dataset. Given some\ninput data, it applies a transformation to it in order to change its DOA\ninformation and simulate new potentially unseen one. Such transformation, in\ngeneral, is a combination of a rotation and a reflection. It is possible to\napply such transformation due to a well-known property of First Order\nAmbisonics (FOA). The same transformation is applied also to the labels, in\norder to maintain consistency between input data and target labels. Three\nmethods with different level of generality are proposed for applying this\naugmentation principle. Experiments are conducted on two different DOA\nnetworks. Results of both experiments demonstrate the effectiveness of the\nnovel augmentation strategy by improving the DOA error by around 40%.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:38:57 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Mazzon", "Luca", ""], ["Koizumi", "Yuma", ""], ["Yasuda", "Masahiro", ""], ["Harada", "Noboru", ""]]}, {"id": "1910.04394", "submitter": "Yivan Zhang", "authors": "Yivan Zhang, Nontawat Charoenphakdee, Masashi Sugiyama", "title": "Learning from Indirect Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised learning is a paradigm for alleviating the scarcity of\nlabeled data by leveraging lower-quality but larger-scale supervision signals.\nWhile existing work mainly focuses on utilizing a certain type of weak\nsupervision, we present a probabilistic framework, learning from indirect\nobservations, for learning from a wide range of weak supervision in real-world\nproblems, e.g., noisy labels, complementary labels and coarse-grained labels.\nWe propose a general method based on the maximum likelihood principle, which\nhas desirable theoretical properties and can be straightforwardly implemented\nfor deep neural networks. Concretely, a discriminative model for the true\ntarget is used for modeling the indirect observation, which is a random\nvariable entirely depending on the true target stochastically or\ndeterministically. Then, maximizing the likelihood given indirect observations\nleads to an estimator of the true target implicitly. Comprehensive experiments\nfor two novel problem settings --- learning from multiclass label proportions\nand learning from coarse-grained labels, illustrate practical usefulness of our\nmethod and demonstrate how to integrate various sources of weak supervision.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:15:02 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zhang", "Yivan", ""], ["Charoenphakdee", "Nontawat", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.04415", "submitter": "Yuma Koizumi", "authors": "Masahiro Yasuda, Yuma Koizumi, Luca Mazzon, Shoichiro Saito, Hisashi\n  Uematsu", "title": "DOA Estimation by DNN-based Denoising and Dereverberation from Sound\n  Intensity Vector", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a direction of arrival (DOA) estimation method that combines\nsound-intensity vector (IV)-based DOA estimation and DNN-based denoising and\ndereverberation. Since the accuracy of IV-based DOA estimation degrades due to\nenvironmental noise and reverberation, two DNNs are used to remove such effects\nfrom the observed IVs. DOA is then estimated from the refined IVs based on the\nphysics of wave propagation. Experiments on an open dataset showed that the\naverage DOA error of the proposed method was 0.528 degrees, and it outperformed\na conventional IV-based and DNN-based DOA estimation method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:57:31 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Yasuda", "Masahiro", ""], ["Koizumi", "Yuma", ""], ["Mazzon", "Luca", ""], ["Saito", "Shoichiro", ""], ["Uematsu", "Hisashi", ""]]}, {"id": "1910.04417", "submitter": "Xiaojian Ma", "authors": "Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu,\n  Junzhou Huang, Chuang Gan", "title": "Imitation Learning from Observations by Minimizing Inverse Dynamics\n  Disagreement", "comments": "Accepted to NeurIPS 2019 as a spotlight. Chao Yang and Xiaojian Ma\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Learning from Observations (LfO) for imitation learning\nwith access to state-only demonstrations. In contrast to Learning from\nDemonstration (LfD) that involves both action and state supervision, LfO is\nmore practical in leveraging previously inapplicable resources (e.g. videos),\nyet more challenging due to the incomplete expert guidance. In this paper, we\ninvestigate LfO and its difference with LfD in both theoretical and practical\nperspectives. We first prove that the gap between LfD and LfO actually lies in\nthe disagreement of inverse dynamics models between the imitator and the\nexpert, if following the modeling approach of GAIL. More importantly, the upper\nbound of this gap is revealed by a negative causal entropy which can be\nminimized in a model-free way. We term our method as\nInverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the\nconventional LfO method through further bridging the gap to LfD. Considerable\nempirical results on challenging benchmarks indicate that our method attains\nconsistent improvements over other LfO counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:07:17 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 20:10:53 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2019 19:21:27 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 00:17:12 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yang", "Chao", ""], ["Ma", "Xiaojian", ""], ["Huang", "Wenbing", ""], ["Sun", "Fuchun", ""], ["Liu", "Huaping", ""], ["Huang", "Junzhou", ""], ["Gan", "Chuang", ""]]}, {"id": "1910.04420", "submitter": "Changying Du", "authors": "Changying Du, Fuzhen Zhuang, Jia He, Qing He and Guoping Long", "title": "Learning beyond Predefined Label Space via Bayesian Nonparametric Topic\n  Modelling", "comments": "Learning beyond predefined labels; Generalized zero-shot learning;\n  Semi-supervised learning; Generative model; Nonparametric Bayesian learning;\n  Hierarchical Dirichlet process; Topic modelling; Collapsed Gibbs sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world machine learning applications, testing data may contain some\nmeaningful new categories that have not been seen in labeled training data. To\nsimultaneously recognize new data categories and assign most appropriate\ncategory labels to the data actually from known categories, existing models\nassume the number of unknown new categories is pre-specified, though it is\ndifficult to determine in advance. In this paper, we propose a Bayesian\nnonparametric topic model to automatically infer this number, based on the\nhierarchical Dirichlet process and the notion of latent Dirichlet allocation.\nExact inference in our model is intractable, so we provide an efficient\ncollapsed Gibbs sampling algorithm for approximate posterior inference.\nExtensive experiments on various text data sets show that: (a) compared with\nparametric approaches that use pre-specified true number of new categories, the\nproposed nonparametric approach can yield comparable performance; and (b) when\nthe exact number of new categories is unavailable, i.e. the parametric\napproaches only have a rough idea about the new categories, our approach has\nevident performance advantages.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:15:08 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Du", "Changying", ""], ["Zhuang", "Fuzhen", ""], ["He", "Jia", ""], ["He", "Qing", ""], ["Long", "Guoping", ""]]}, {"id": "1910.04426", "submitter": "Ying-Cheng Lai", "authors": "Junjie Jiang, Ying-Cheng Lai", "title": "Model-free prediction of spatiotemporal dynamical systems with recurrent\n  neural networks: Role of network spectral radius", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG nlin.CD physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common difficulty in applications of machine learning is the lack of any\ngeneral principle for guiding the choices of key parameters of the underlying\nneural network. Focusing on a class of recurrent neural networks - reservoir\ncomputing systems that have recently been exploited for model-free prediction\nof nonlinear dynamical systems, we uncover a surprising phenomenon: the\nemergence of an interval in the spectral radius of the neural network in which\nthe prediction error is minimized. In a three-dimensional representation of the\nerror versus time and spectral radius, the interval corresponds to the bottom\nregion of a \"valley.\" Such a valley arises for a variety of spatiotemporal\ndynamical systems described by nonlinear partial differential equations,\nregardless of the structure and the edge-weight distribution of the underlying\nreservoir network. We also find that, while the particular location and size of\nthe valley would depend on the details of the target system to be predicted,\nthe interval tends to be larger for undirected than for directed networks. The\nvalley phenomenon can be beneficial to the design of optimal reservoir\ncomputing, representing a small step forward in understanding these\nmachine-learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:27:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Jiang", "Junjie", ""], ["Lai", "Ying-Cheng", ""]]}, {"id": "1910.04439", "submitter": "Haohao Li", "authors": "Haohao Li, Huibing Wang", "title": "A Multi-view Dimensionality Reduction Algorithm Based on Smooth\n  Representation Model", "comments": "Revise some experimental results and formulates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, we have witnessed a large family of algorithms\nthat have been designed to provide different solutions to the problem of\ndimensionality reduction (DR). The DR is an essential tool to excavate the\nimportant information from the high-dimensional data by mapping the data to a\nlow-dimensional subspace. Furthermore, for the diversity of varied\nhigh-dimensional data, the multi-view features can be utilized for improving\nthe learning performance. However, many DR methods fail to integrating multiple\nviews. Although the features from different views are extracted by different\nmanners, they are utilized to describe the same sample, which implies that they\nare highly related. Therefore, how to learn the subspace for high-dimensional\nfeatures by utilizing the consistency and complementary properties of\nmulti-view features is important in the present. In this paper, we propose an\neffective multi-view dimensionality reduction algorithm named Multi-view Smooth\nPreserve Projection. Firstly, we construct a single view DR method named Smooth\nPreserve Projection based on the Smooth Representation model. The proposed\nmethod aims to find a subspace for the high-dimensional data, in which the\nsmooth reconstructive weights are preserved as much as possible. Then, we\nextend it to a multi-view version in which we exploits Hilbert-Schmidt\nIndependence Criterion to jointly learn one common subspace for all views. A\nplenty of experiments on multi-view datasets show the excellent performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 09:02:46 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 09:56:12 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 09:37:48 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Li", "Haohao", ""], ["Wang", "Huibing", ""]]}, {"id": "1910.04460", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Louis Pujol", "title": "Still no free lunches: the price to pay for tighter PAC-Bayes bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \"No free lunch\" results state the impossibility of obtaining meaningful\nbounds on the error of a learning algorithm without prior assumptions and\nmodelling. Some models are expensive (strong assumptions, such as as\nsubgaussian tails), others are cheap (simply finite variance). As it is well\nknown, the more you pay, the more you get: in other words, the most expensive\nmodels yield the more interesting bounds. Recent advances in robust statistics\nhave investigated procedures to obtain tight bounds while keeping the cost\nminimal. The present paper explores and exhibits what the limits are for\nobtaining tight PAC-Bayes bounds in a robust setting for cheap models,\naddressing the question: is PAC-Bayes good value for money?\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:01:02 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Guedj", "Benjamin", ""], ["Pujol", "Louis", ""]]}, {"id": "1910.04462", "submitter": "Tam Le", "authors": "Tam Le, Nhat Ho, Makoto Yamada", "title": "Flow-based Alignment Approaches for Probability Measures in Different\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gromov-Wasserstein (GW) is a powerful tool to compare probability measures\nwhose supports are in different metric spaces. GW suffers however from a\ncomputational drawback since it requires to solve a complex non-convex\nquadratic program. We consider in this work a specific family of cost metrics,\nnamely \\textit{tree metrics} for a space of supports of each probability\nmeasure, and aim for developing efficient and scalable discrepancies between\nthe probability measures. By leveraging a tree structure, we propose to align\n\\textit{flows} from a root to each support instead of pair-wise tree metrics of\nsupports, i.e., flows from a support to another, in GW. Consequently, we\npropose a novel discrepancy, named Flow-based Alignment (\\FlowAlign), by\nmatching the flows of the probability measures. We show that \\FlowAlign~shares\na similar structure as a univariate optimal transport distance. Therefore,\n\\FlowAlign~is fast for computation and scalable for large-scale applications.\nBy further exploring tree structures, we propose a variant of \\FlowAlign, named\nDepth-based Alignment (\\DepthAlign), by aligning the flows hierarchically along\neach depth level of the tree structures. Theoretically, we prove that both\n\\FlowAlign~and \\DepthAlign~are pseudo-distances. Moreover, we also derive\ntree-sliced variants, computed by averaging the corresponding \\FlowAlign~/\n\\DepthAlign~using random tree metrics, built adaptively in spaces of supports.\nEmpirically, we test our proposed discrepancies against other baselines on some\nbenchmark tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:04:13 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 05:59:07 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 08:00:25 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2020 10:30:26 GMT"}, {"version": "v5", "created": "Wed, 17 Jun 2020 07:40:50 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Le", "Tam", ""], ["Ho", "Nhat", ""], ["Yamada", "Makoto", ""]]}, {"id": "1910.04464", "submitter": "Benjamin Guedj", "authors": "Kento Nozawa and Pascal Germain and Benjamin Guedj", "title": "PAC-Bayesian Contrastive Unsupervised Representation Learning", "comments": "Published in the proceedings of the Conference on Uncertainty in\n  Artificial Intelligence 2020 (UAI)", "journal-ref": "PMLR, volume 124 (UAI 2020), 2020", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contrastive unsupervised representation learning (CURL) is the\nstate-of-the-art technique to learn representations (as a set of features) from\nunlabelled data. While CURL has collected several empirical successes recently,\ntheoretical understanding of its performance was still missing. In a recent\nwork, Arora et al. (2019) provide the first generalisation bounds for CURL,\nrelying on a Rademacher complexity. We extend their framework to the flexible\nPAC-Bayes setting, allowing us to deal with the non-iid setting. We present\nPAC-Bayesian generalisation bounds for CURL, which are then used to derive a\nnew representation learning algorithm. Numerical experiments on real-life\ndatasets illustrate that our algorithm achieves competitive accuracy, and\nyields non-vacuous generalisation bounds.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:13:01 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 05:21:33 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 18:30:17 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 13:41:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nozawa", "Kento", ""], ["Germain", "Pascal", ""], ["Guedj", "Benjamin", ""]]}, {"id": "1910.04483", "submitter": "Tam Le", "authors": "Tam Le, Viet Huynh, Nhat Ho, Dinh Phung, Makoto Yamada", "title": "Tree-Wasserstein Barycenter for Large-Scale Multilevel Clustering and\n  Scalable Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper a variant of Wasserstein barycenter problem, which we\nrefer to as tree-Wasserstein barycenter, by leveraging a specific class of\nground metrics, namely tree metrics, for Wasserstein distance. Drawing on the\ntree structure, we propose an efficient algorithmic approach to solve the\ntree-Wasserstein barycenter and its variants. The proposed approach is not only\nfast for computation but also efficient for memory usage. Exploiting the\ntree-Wasserstein barycenter and its variants, we scale up multi-level\nclustering and scalable Bayes, especially for large-scale applications where\nthe number of supports in probability measures is large. Empirically, we test\nour proposed approach against other baselines on large-scale synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 11:09:26 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 10:51:54 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 03:20:03 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Le", "Tam", ""], ["Huynh", "Viet", ""], ["Ho", "Nhat", ""], ["Phung", "Dinh", ""], ["Yamada", "Makoto", ""]]}, {"id": "1910.04499", "submitter": "Nezihe Merve G\\\"urel", "authors": "Xupeng Miao, Nezihe Merve G\\\"urel, Wentao Zhang, Zhichao Han, Bo Li,\n  Wei Min, Xi Rao, Hansheng Ren, Yinan Shan, Yingxia Shao, Yujie Wang, Fan Wu,\n  Hui Xue, Yaming Yang, Zitao Zhang, Yang Zhao, Shuai Zhang, Yujing Wang, Bin\n  Cui, Ce Zhang", "title": "DeGNN: Characterizing and Improving Graph Neural Networks with Graph\n  Decomposition", "comments": "20 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the wide application of Graph Convolutional Network (GCN), one major\nlimitation is that it does not benefit from the increasing depth and suffers\nfrom the oversmoothing problem. In this work, we first characterize this\nphenomenon from the information-theoretic perspective and show that under\ncertain conditions, the mutual information between the output after $l$ layers\nand the input of GCN converges to 0 exponentially with respect to $l$. We also\nshow that, on the other hand, graph decomposition can potentially weaken the\ncondition of such convergence rate, which enabled our analysis for GraphCNN.\nWhile different graph structures can only benefit from the corresponding\ndecomposition, in practice, we propose an automatic connectivity-aware graph\ndecomposition algorithm, DeGNN, to improve the performance of general graph\nneural networks. Extensive experiments on widely adopted benchmark datasets\ndemonstrate that DeGNN can not only significantly boost the performance of\ncorresponding GNNs, but also achieves the state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 11:52:36 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 13:50:26 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 12:06:41 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 08:21:25 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Miao", "Xupeng", ""], ["G\u00fcrel", "Nezihe Merve", ""], ["Zhang", "Wentao", ""], ["Han", "Zhichao", ""], ["Li", "Bo", ""], ["Min", "Wei", ""], ["Rao", "Xi", ""], ["Ren", "Hansheng", ""], ["Shan", "Yinan", ""], ["Shao", "Yingxia", ""], ["Wang", "Yujie", ""], ["Wu", "Fan", ""], ["Xue", "Hui", ""], ["Yang", "Yaming", ""], ["Zhang", "Zitao", ""], ["Zhao", "Yang", ""], ["Zhang", "Shuai", ""], ["Wang", "Yujing", ""], ["Cui", "Bin", ""], ["Zhang", "Ce", ""]]}, {"id": "1910.04500", "submitter": "Mingu Lee", "authors": "Mingu Lee, Jinkyu Lee, Hye Jin Jang, Byeonggeun Kim, Wonil Chang and\n  Kyuwoong Hwang", "title": "Orthogonality Constrained Multi-Head Attention For Keyword Spotting", "comments": "Accepted to ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attention mechanism is capable of learning various representations\nfrom sequential data while paying attention to different subsequences, e.g.,\nword-pieces or syllables in a spoken word. From the subsequences, it retrieves\nricher information than a single-head attention which only summarizes the whole\nsequence into one context vector. However, a naive use of the multi-head\nattention does not guarantee such richness as the attention heads may have\npositional and representational redundancy. In this paper, we propose a\nregularization technique for multi-head attention mechanism in an end-to-end\nneural keyword spotting system. Augmenting regularization terms which penalize\npositional and contextual non-orthogonality between the attention heads\nencourages to output different representations from separate subsequences,\nwhich in turn enables leveraging structured information without explicit\nsequence models such as hidden Markov models. In addition, intra-head\ncontextual non-orthogonality regularization encourages each attention head to\nhave similar representations across keyword examples, which helps\nclassification by reducing feature variability. The experimental results\ndemonstrate that the proposed regularization technique significantly improves\nthe keyword spotting performance for the keyword \"Hey Snapdragon\".\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 12:00:33 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lee", "Mingu", ""], ["Lee", "Jinkyu", ""], ["Jang", "Hye Jin", ""], ["Kim", "Byeonggeun", ""], ["Chang", "Wonil", ""], ["Hwang", "Kyuwoong", ""]]}, {"id": "1910.04514", "submitter": "Sebastian Szyller", "authors": "Samuel Marchal and Sebastian Szyller", "title": "Detecting organized eCommerce fraud using scalable categorical\n  clustering", "comments": "14 pages, 6 figures, Annual Computer Security Applications Conference\n  (ACSAC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online retail, eCommerce, frequently falls victim to fraud conducted by\nmalicious customers (fraudsters) who obtain goods or services through\ndeception. Fraud coordinated by groups of professional fraudsters that place\nseveral fraudulent orders to maximize their gain is referred to as organized\nfraud. Existing approaches to fraud detection typically analyze orders in\nisolation and they are not effective at identifying groups of fraudulent orders\nlinked to organized fraud. These also wrongly identify many legitimate orders\nas fraud, which hinders their usage for automated fraud cancellation. We\nintroduce a novel solution to detect organized fraud by analyzing orders in\nbulk. Our approach is based on clustering and aims to group together fraudulent\norders placed by the same group of fraudsters. It selectively uses two existing\ntechniques, agglomerative clustering and sampling to recursively group orders\ninto small clusters in a reasonable amount of time. We assess our clustering\ntechnique on real-world orders placed on the Zalando website, the largest\nonline apparel retailer in Europe1. Our clustering processes 100,000s of orders\nin a few hours and groups 35-45% of fraudulent orders together. We propose a\nsimple technique built on top of our clustering that detects 26.2% of fraud\nwhile raising false alarms for only 0.1% of legitimate orders.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 12:34:02 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Marchal", "Samuel", ""], ["Szyller", "Sebastian", ""]]}, {"id": "1910.04522", "submitter": "Matilde Gargiani", "authors": "Matilde Gargiani and Aaron Klein and Stefan Falkner and Frank Hutter", "title": "Probabilistic Rollouts for Learning Curve Extrapolation Across\n  Hyperparameter Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose probabilistic models that can extrapolate learning curves of\niterative machine learning algorithms, such as stochastic gradient descent for\ntraining deep networks, based on training data with variable-length learning\ncurves. We study instantiations of this framework based on random forests and\nBayesian recurrent neural networks. Our experiments show that these models\nyield better predictions than state-of-the-art models from the hyperparameter\noptimization literature when extrapolating the performance of neural networks\ntrained with different hyperparameter settings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 12:49:22 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gargiani", "Matilde", ""], ["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""]]}, {"id": "1910.04536", "submitter": "Martin Trapp", "authors": "Martin Trapp, Robert Peharz, Franz Pernkopf, Carl E. Rasmussen", "title": "Deep Structured Mixtures of Gaussian Processes", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are powerful non-parametric Bayesian regression\nmodels that allow exact posterior inference, but exhibit high computational and\nmemory costs. In order to improve scalability of GPs, approximate posterior\ninference is frequently employed, where a prominent class of approximation\ntechniques is based on local GP experts. However, local-expert techniques\nproposed so far are either not well-principled, come with limited approximation\nguarantees, or lead to intractable models. In this paper, we introduce deep\nstructured mixtures of GP experts, a stochastic process model which i) allows\nexact posterior inference, ii) has attractive computational and memory costs,\nand iii) when used as GP approximation, captures predictive uncertainties\nconsistently better than previous expert-based approximations. In a variety of\nexperiments, we show that deep structured mixtures have a low approximation\nerror and often perform competitive or outperform prior work.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 13:16:13 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:08:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Trapp", "Martin", ""], ["Peharz", "Robert", ""], ["Pernkopf", "Franz", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1910.04537", "submitter": "Arpit Garg", "authors": "Arpit Garg, Yazied A. Hasan, Adam Ya\\~nez and Lydia Tapia", "title": "Defensive Escort Teams via Multi-Agent Deep Reinforcement Learning", "comments": "IEEE Robotics and Automation Letters with International Conference on\n  Robotics and Automation (ICRA) option, 2020, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinated defensive escorts can aid a navigating payload by positioning\nthemselves in order to maintain the safety of the payload from obstacles. In\nthis paper, we present a novel, end-to-end solution for coordinating an escort\nteam for protecting high-value payloads. Our solution employs deep\nreinforcement learning (RL) in order to train a team of escorts to maintain\npayload safety while navigating alongside the payload. This is done in a\ndistributed fashion, relying only on limited range positional information of\nother escorts, the payload, and the obstacles. When compared to a state-of-art\nalgorithm for obstacle avoidance, our solution with a single escort increases\nnavigation success up to 31%. Additionally, escort teams increase success rate\nby up to 75% percent over escorts in static formations. We also show that this\nlearned solution is general to several adaptations in the scenario including: a\nchanging number of escorts in the team, changing obstacle density, and changes\nin payload conformation. Video: https://youtu.be/SoYesKti4VA.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:57:49 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Garg", "Arpit", ""], ["Hasan", "Yazied A.", ""], ["Ya\u00f1ez", "Adam", ""], ["Tapia", "Lydia", ""]]}, {"id": "1910.04540", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Zhiqiu Lin, Guandao Yang, Christopher De Sa", "title": "QPyTorch: A Low-Precision Arithmetic Simulation Framework", "comments": "NeurIPS 2019 EMC^2 Workshop on Energy Efficient Machine Learning and\n  Cognitive Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision training reduces computational cost and produces efficient\nmodels. Recent research in developing new low-precision training algorithms\noften relies on simulation to empirically evaluate the statistical effects of\nquantization while avoiding the substantial overhead of building specific\nhardware. To support this empirical research, we introduce QPyTorch, a\nlow-precision arithmetic simulation framework. Built natively in PyTorch,\nQPyTorch provides a convenient interface that minimizes the efforts needed to\nreliably convert existing codes to study low-precision training. QPyTorch is\ngeneral, and supports a variety of combinations of precisions, number formats,\nand rounding options. Additionally, it leverages an efficient fused-kernel\napproach to reduce simulator overhead, which enables simulation of large-scale,\nrealistic problems. QPyTorch is publicly available at\nhttps://github.com/Tiiiger/QPyTorch.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:15:08 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zhang", "Tianyi", ""], ["Lin", "Zhiqiu", ""], ["Yang", "Guandao", ""], ["De Sa", "Christopher", ""]]}, {"id": "1910.04615", "submitter": "Pei Wang", "authors": "Pei Wang, Arash Givchi, and Patrick Shafto", "title": "Learning a manifold from a teacher's demonstrations", "comments": "NeurIPS 2020 TDA workshop version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a manifold from a teacher's\ndemonstration. Extending existing approaches of learning from randomly sampled\ndata points, we consider contexts where data may be chosen by a teacher. We\nanalyze learning from teachers who can provide structured data such as\nindividual examples (isolated data points) and demonstrations (sequences of\npoints). Our analysis shows that for the purpose of teaching the topology of a\nmanifold, demonstrations can yield remarkable decreases in the amount of data\npoints required in comparison to teaching with randomly sampled points. We also\ndiscuss the implications of our analysis for learning in humans and machines.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:45:49 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 21:27:56 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 22:14:01 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wang", "Pei", ""], ["Givchi", "Arash", ""], ["Shafto", "Patrick", ""]]}, {"id": "1910.04618", "submitter": "Hang Gao", "authors": "Hang Gao and Tim Oates", "title": "Universal Adversarial Perturbation for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a state-of-the-art deep neural network text classifier, we show the\nexistence of a universal and very small perturbation vector (in the embedding\nspace) that causes natural text to be misclassified with high probability.\nUnlike images on which a single fixed-size adversarial perturbation can be\nfound, text is of variable length, so we define the \"universality\" as\n\"token-agnostic\", where a single perturbation is applied to each token,\nresulting in different perturbations of flexible sizes at the sequence level.\nWe propose an algorithm to compute universal adversarial perturbations, and\nshow that the state-of-the-art deep neural networks are highly vulnerable to\nthem, even though they keep the neighborhood of tokens mostly preserved. We\nalso show how to use these adversarial perturbations to generate adversarial\ntext samples. The surprising existence of universal \"token-agnostic\"\nadversarial perturbations may reveal important properties of a text classifier.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:48:22 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gao", "Hang", ""], ["Oates", "Tim", ""]]}, {"id": "1910.04621", "submitter": "Pierre Laforgue", "authors": "Pierre Laforgue, Alex Lambert, Luc Brogat-Motte, Florence\n  d'Alch\\'e-Buc", "title": "Duality in RKHSs with Infinite Dimensional Outputs: Application to\n  Robust Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing\nKernel Hilbert Spaces provide an elegant way to extend scalar kernel methods\nwhen the output space is a Hilbert space. Although primarily used in finite\ndimension for problems like multi-task regression, the ability of this\nframework to deal with infinite dimensional output spaces unlocks many more\napplications, such as functional regression, structured output prediction, and\nstructured data representation. However, these sophisticated schemes crucially\nrely on the kernel trick in the output space, so that most of previous works\nhave focused on the square norm loss function, completely neglecting robustness\nissues that may arise in such surrogate problems. To overcome this limitation,\nthis paper develops a duality approach that allows to solve OVK machines for a\nwide range of loss functions. The infinite dimensional Lagrange multipliers are\nhandled through a Double Representer Theorem, and algorithms for\n$\\epsilon$-insensitive losses and the Huber loss are thoroughly detailed.\nRobustness benefits are emphasized by a theoretical stability analysis, as well\nas empirical improvements on structured data applications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:59:28 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 15:45:37 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 09:06:10 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Laforgue", "Pierre", ""], ["Lambert", "Alex", ""], ["Brogat-Motte", "Luc", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "1910.04636", "submitter": "Yicheng Hong", "authors": "Yicheng (Katherine) Hong", "title": "Comparison of Generative Adversarial Networks Architectures Which Reduce\n  Mode Collapse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks are known for their high quality outputs and\nversatility. However, they also suffer the mode collapse in their output data\ndistribution. There have been many efforts to revamp GANs model and reduce mode\ncollapse. This paper focuses on two of these models, PacGAN and VEEGAN. This\npaper explains the mathematical theory behind aforementioned models, and\ncompare their degree of mode collapse with vanilla GAN using MNIST digits as\ninput data. The result indicates that PacGAN performs slightly better than\nvanilla GAN in terms of mode collapse, and VEEGAN performs worse than both\nPacGAN and vanilla GAN. VEEGAN's poor performance may be attributed to average\nautoencoder loss in its objective function and small penalty for blurry\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:17:12 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Yicheng", "", "", "Katherine"], ["Hong", "", ""]]}, {"id": "1910.04650", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Mengye Ren, Raquel Urtasun", "title": "Learning to Remember from a Multi-Task Teacher", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on catastrophic forgetting during sequential learning\ntypically focus on fixing the accuracy of the predictions for a previously\nlearned task. In this paper we argue that the outputs of neural networks are\nsubject to rapid changes when learning a new data distribution, and networks\nthat appear to \"forget\" everything still contain useful representation towards\nprevious tasks. Instead of enforcing the output accuracy to stay the same, we\npropose to reduce the effect of catastrophic forgetting on the representation\nlevel, as the output layer can be quickly recovered later with a small number\nof examples. Towards this goal, we propose an experimental setup that measures\nthe amount of representational forgetting, and develop a novel meta-learning\nalgorithm to overcome this issue. The proposed meta-learner produces weight\nupdates of a sequential learning network, mimicking a multi-task teacher\nnetwork's representation. We show that our meta-learner can improve its learned\nrepresentations on new tasks, while maintaining a good representation for old\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:33:19 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:27:07 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Xiong", "Yuwen", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1910.04665", "submitter": "Clayton Scott", "authors": "Clayton Scott and Jianxin Zhang", "title": "Learning from Multiple Corrupted Sources, with Application to Learning\n  from Label Proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study binary classification in the setting where the learner is presented\nwith multiple corrupted training samples, with possibly different sample sizes\nand degrees of corruption, and introduce an approach based on minimizing a\nweighted combination of corruption-corrected empirical risks. We establish a\ngeneralization error bound, and further show that the bound is optimized when\nthe weights are certain interpretable and intuitive functions of the sample\nsizes and degrees of corruptions. We then apply this setting to the problem of\nlearning with label proportions (LLP), and propose an algorithm that enjoys the\nmost general statistical performance guarantees known for LLP. Experiments\ndemonstrate the utility of our theory.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:01:42 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Scott", "Clayton", ""], ["Zhang", "Jianxin", ""]]}, {"id": "1910.04672", "submitter": "Alexander Buchholz", "authors": "Alexander Buchholz, Daniel Ahfock, Sylvia Richardson", "title": "Distributed Computation for Marginal Likelihood based Model Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general method for distributed Bayesian model choice, using the\nmarginal likelihood, where each worker has access only to non-overlapping\nsubsets of the data. Our approach approximates the model evidence for the full\ndata set through Monte Carlo sampling from the posterior on every subset\ngenerating a model evidence per subset. The model evidences per worker are then\nconsistently combined using a novel approach which corrects for the splitting\nusing summary statistics of the generated samples. This divide-and-conquer\napproach allows Bayesian model choice in the large data setting, exploiting all\navailable information but limiting communication between workers. Our work\nthereby complements the work on consensus Monte Carlo (Scott et al., 2016) by\nexplicitly enabling model choice. In addition, we show how the suggested\napproach can be extended to model choice within a reversible jump setting that\nexplores multiple feature combinations within one run.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:23:34 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:58:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Buchholz", "Alexander", ""], ["Ahfock", "Daniel", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1910.04689", "submitter": "Paul Bendich", "authors": "Lihan Yao and Paul Bendich", "title": "Graph Spectral Embedding for Parsimonious Transmission of Multivariate\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graph spectral representation of time series data that 1) is\nparsimoniously encoded to user-demanded resolution; 2) is unsupervised and\nperformant in data-constrained scenarios; 3) captures event and\nevent-transition structure within the time series; and 4) has near-linear\ncomputational complexity in both signal length and ambient dimension. This\nrepresentation, which we call Laplacian Events Signal Segmentation (LESS), can\nbe computed on time series of arbitrary dimension and originating from sensors\nof arbitrary type. Hence, time series originating from sensors of heterogeneous\ntype can be compressed to levels demanded by constrained-communication\nenvironments, before being fused at a common center.\n  Temporal dynamics of the data is summarized without explicit partitioning or\nprobabilistic modeling. As a proof-of-principle, we apply this technique on\nhigh dimensional wavelet coefficients computed from the Free Spoken Digit\nDataset to generate a memory efficient representation that is interpretable.\nDue to its unsupervised and non-parametric nature, LESS representations remain\nperformant in the digit classification task despite the absence of labels and\nlimited data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:49:18 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Yao", "Lihan", ""], ["Bendich", "Paul", ""]]}, {"id": "1910.04701", "submitter": "Jordan J. Bird", "authors": "Jordan J. Bird, Anik\\'o Ek\\'art, Diego R. Faria", "title": "On the Effects of Pseudo and Quantum Random Number Generators in Soft\n  Computing", "comments": null, "journal-ref": "Soft Computing 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.ET stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we argue that the implications of Pseudo and Quantum Random\nNumber Generators (PRNG and QRNG) inexplicably affect the performances and\nbehaviours of various machine learning models that require a random input.\nThese implications are yet to be explored in Soft Computing until this work. We\nuse a CPU and a QPU to generate random numbers for multiple Machine Learning\ntechniques. Random numbers are employed in the random initial weight\ndistributions of Dense and Convolutional Neural Networks, in which results show\na profound difference in learning patterns for the two. In 50 Dense Neural\nNetworks (25 PRNG/25 QRNG), QRNG increases over PRNG for accent classification\nat +0.1%, and QRNG exceeded PRNG for mental state EEG classification by +2.82%.\nIn 50 Convolutional Neural Networks (25 PRNG/25 QRNG), the MNIST and CIFAR-10\nproblems are benchmarked, in MNIST the QRNG experiences a higher starting\naccuracy than the PRNG but ultimately only exceeds it by 0.02%. In CIFAR-10,\nthe QRNG outperforms PRNG by +0.92%. The n-random split of a Random Tree is\nenhanced towards and new Quantum Random Tree (QRT) model, which has differing\nclassification abilities to its classical counterpart, 200 trees are trained\nand compared (100 PRNG/100 QRNG). Using the accent and EEG classification\ndatasets, a QRT seemed inferior to a RT as it performed on average worse by\n-0.12%. This pattern is also seen in the EEG classification problem, where a\nQRT performs worse than a RT by -0.28%. Finally, the QRT is ensembled into a\nQuantum Random Forest (QRF), which also has a noticeable effect when compared\nto the standard Random Forest (RF)... ABSTRACT SHORTENED DUE TO ARXIV LIMIT\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:57:17 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Bird", "Jordan J.", ""], ["Ek\u00e1rt", "Anik\u00f3", ""], ["Faria", "Diego R.", ""]]}, {"id": "1910.04721", "submitter": "David Wood", "authors": "David Wood, James Cole, Thomas Booth", "title": "NEURO-DRAM: a 3D recurrent visual attention model for interpretable\n  neuroimaging classification", "comments": "Improved network figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is attracting significant interest in the neuroimaging\ncommunity as a means to diagnose psychiatric and neurological disorders from\nstructural magnetic resonance images. However, there is a tendency amongst\nresearchers to adopt architectures optimized for traditional computer vision\ntasks, rather than design networks customized for neuroimaging data. We address\nthis by introducing NEURO-DRAM, a 3D recurrent visual attention model tailored\nfor neuroimaging classification. The model comprises an agent which, trained by\nreinforcement learning, learns to navigate through volumetric images,\nselectively attending to the most informative regions for a given task. When\napplied to Alzheimer's disease prediction, NEURODRAM achieves state-of-the-art\nclassification accuracy on an out-of-sample dataset, significantly\noutperforming a baseline convolutional neural network. When further applied to\nthe task of predicting which patients with mild cognitive impairment will be\ndiagnosed with Alzheimer's disease within two years, the model achieves\nstate-of-the-art accuracy with no additional training. Encouragingly, the agent\nlearns, without explicit instruction, a search policy in agreement with\nstandardized radiological hallmarks of Alzheimer's disease, suggesting a route\nto automated biomarker discovery for more poorly understood disorders.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:28:35 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 14:44:50 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 18:55:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wood", "David", ""], ["Cole", "James", ""], ["Booth", "Thomas", ""]]}, {"id": "1910.04724", "submitter": "Hang Gao", "authors": "Karan K. Budhraja and Hang Gao and Tim Oates", "title": "Using Neural Networks for Programming by Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based modeling is a paradigm of modeling dynamic systems of interacting\nagents that are individually governed by specified behavioral rules. Training a\nmodel of such agents to produce an emergent behavior by specification of the\nemergent (as opposed to agent) behavior is easier from a demonstration\nperspective. Without the involvement of manual behavior specification via code\nor reliance on a defined taxonomy of possible behaviors, the demonstrator\nspecifies the desired emergent behavior of the system over time, and retrieves\nagent-level parameters required to execute that motion. A low time-complexity\nand data requirement favoring framework for reproducing emergent behavior,\ngiven an abstract demonstration, is discussed in [1], [2]. The existing\nframework does, however, observe an inherent limitation in scalability because\nof an exponentially growing search space (with the number of agent-level\nparameters). Our work addresses this limitation by pursuing a more scalable\narchitecture with the use of neural networks. While the (proof-of-concept)\narchitecture is not suitable for many evaluated domains because of its lack of\nrepresentational capacity for that domain, it is more suitable than existing\nwork for larger datasets for the Civil Violence agent-based model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:32:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Budhraja", "Karan K.", ""], ["Gao", "Hang", ""], ["Oates", "Tim", ""]]}, {"id": "1910.04729", "submitter": "Muhammad Burhan Hafez", "authors": "Muhammad Burhan Hafez, Cornelius Weber, Matthias Kerzel and Stefan\n  Wermter", "title": "Efficient Intrinsically Motivated Robotic Grasping with\n  Learning-Adaptive Imagination in Latent Space", "comments": "In: Proceedings of the Joint IEEE International Conference on\n  Development and Learning and on Epigenetic Robotics (ICDL-EpiRob), Oslo,\n  Norway, Aug. 19-22, 2019", "journal-ref": null, "doi": "10.1109/DEVLRN.2019.8850723", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining model-based and model-free deep reinforcement learning has shown\ngreat promise for improving sample efficiency on complex control tasks while\nstill retaining high performance. Incorporating imagination is a recent effort\nin this direction inspired by human mental simulation of motor behavior. We\npropose a learning-adaptive imagination approach which, unlike previous\napproaches, takes into account the reliability of the learned dynamics model\nused for imagining the future. Our approach learns an ensemble of disjoint\nlocal dynamics models in latent space and derives an intrinsic reward based on\nlearning progress, motivating the controller to take actions leading to data\nthat improves the models. The learned models are used to generate imagined\nexperiences, augmenting the training set of real experiences. We evaluate our\napproach on learning vision-based robotic grasping and show that it\nsignificantly improves sample efficiency and achieves near-optimal performance\nin a sparse reward environment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:43:05 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Hafez", "Muhammad Burhan", ""], ["Weber", "Cornelius", ""], ["Kerzel", "Matthias", ""], ["Wermter", "Stefan", ""]]}, {"id": "1910.04732", "submitter": "Jeremy Wohlwend", "authors": "Ziheng Wang, Jeremy Wohlwend, Tao Lei", "title": "Structured Pruning of Large Language Models", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.496", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have recently achieved state of the art performance\nacross a wide variety of natural language tasks. Meanwhile, the size of these\nmodels and their latency have significantly increased, which makes their usage\ncostly, and raises an interesting question: do language models need to be\nlarge? We study this question through the lens of model compression. We present\na generic, structured pruning approach by parameterizing each weight matrix\nusing its low-rank factorization, and adaptively removing rank-1 components\nduring training. On language modeling tasks, our structured approach\noutperforms other unstructured and block-structured pruning baselines at\nvarious compression levels, while achieving significant speedups during both\ntraining and inference. We also demonstrate that our method can be applied to\npruning adaptive word embeddings in large language models, and to pruning the\nBERT model on several downstream fine-tuning classification benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:44:18 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 19:04:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Ziheng", ""], ["Wohlwend", "Jeremy", ""], ["Lei", "Tao", ""]]}, {"id": "1910.04743", "submitter": "Daniel LeJeune", "authors": "Daniel LeJeune, Hamid Javadi, Richard G. Baraniuk", "title": "The Implicit Regularization of Ordinary Least Squares Ensembles", "comments": "18 pages, 4 figures. To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensemble methods that average over a collection of independent predictors\nthat are each limited to a subsampling of both the examples and features of the\ntraining data command a significant presence in machine learning, such as the\never-popular random forest, yet the nature of the subsampling effect,\nparticularly of the features, is not well understood. We study the case of an\nensemble of linear predictors, where each individual predictor is fit using\nordinary least squares on a random submatrix of the data matrix. We show that,\nunder standard Gaussianity assumptions, when the number of features selected\nfor each predictor is optimally tuned, the asymptotic risk of a large ensemble\nis equal to the asymptotic ridge regression risk, which is known to be optimal\namong linear predictors in this setting. In addition to eliciting this implicit\nregularization that results from subsampling, we also connect this ensemble to\nthe dropout technique used in training deep (neural) networks, another strategy\nthat has been shown to have a ridge-like regularizing effect.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:52:07 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 18:46:30 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["LeJeune", "Daniel", ""], ["Javadi", "Hamid", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1910.04749", "submitter": "Ximing Qiao", "authors": "Ximing Qiao, Yukun Yang, Hai Li", "title": "Defending Neural Backdoors via Generative Distribution Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural backdoor attack is emerging as a severe security threat to deep\nlearning, while the capability of existing defense methods is limited,\nespecially for complex backdoor triggers. In the work, we explore the space\nformed by the pixel values of all possible backdoor triggers. An original\ntrigger used by an attacker to build the backdoored model represents only a\npoint in the space. It then will be generalized into a distribution of valid\ntriggers, all of which can influence the backdoored model. Thus, previous\nmethods that model only one point of the trigger distribution is not\nsufficient. Getting the entire trigger distribution, e.g., via generative\nmodeling, is a key to effective defense. However, existing generative modeling\ntechniques for image generation are not applicable to the backdoor scenario as\nthe trigger distribution is completely unknown. In this work, we propose\nmax-entropy staircase approximator (MESA), an algorithm for high-dimensional\nsampling-free generative modeling and use it to recover the trigger\ndistribution. We also develop a defense technique to remove the triggers from\nthe backdoored model. Our experiments on Cifar10/100 dataset demonstrate the\neffectiveness of MESA in modeling the trigger distribution and the robustness\nof the proposed defense method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:55:49 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:38:01 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Qiao", "Ximing", ""], ["Yang", "Yukun", ""], ["Li", "Hai", ""]]}, {"id": "1910.04751", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Maxwell D. Collins and Yukun Zhu and Ting Liu and\n  Thomas S. Huang and Hartwig Adam and Liang-Chieh Chen", "title": "Panoptic-DeepLab", "comments": "This work is presented at ICCV 2019 Joint COCO and Mapillary\n  Recognition Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Panoptic-DeepLab, a bottom-up and single-shot approach for\npanoptic segmentation. Our Panoptic-DeepLab is conceptually simple and delivers\nstate-of-the-art results. In particular, we adopt the dual-ASPP and\ndual-decoder structures specific to semantic, and instance segmentation,\nrespectively. The semantic segmentation branch is the same as the typical\ndesign of any semantic segmentation model (e.g., DeepLab), while the instance\nsegmentation branch is class-agnostic, involving a simple instance center\nregression. Our single Panoptic-DeepLab sets the new state-of-art at all three\nCityscapes benchmarks, reaching 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set,\nand advances results on the other challenging Mapillary Vistas.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:57:19 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 15:27:18 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 01:13:12 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Cheng", "Bowen", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Liu", "Ting", ""], ["Huang", "Thomas S.", ""], ["Adam", "Hartwig", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1910.04760", "submitter": "Qi Li", "authors": "Qi Li, Long Mai, Michael A. Alcorn, Anh Nguyen", "title": "A cost-effective method for improving and re-purposing large,\n  pre-trained GANs by fine-tuning their class-embeddings", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, pre-trained generative models have been increasingly popular and\nuseful to both the research and wider communities. Specifically, BigGANs a\nclass-conditional Generative Adversarial Networks trained on\nImageNet---achieved excellent, state-of-the-art capability in generating\nrealistic photos. However, fine-tuning or training BigGANs from scratch is\npractically impossible for most researchers and engineers because (1) GAN\ntraining is often unstable and suffering from mode-collapse; and (2) the\ntraining requires a significant amount of computation, 256 Google TPUs for 2\ndays or 8xV100 GPUs for 15 days. Importantly, many pre-trained generative\nmodels both in NLP and image domains were found to contain biases that are\nharmful to society. Thus, we need computationally-feasible methods for\nmodifying and re-purposing these huge, pre-trained models for downstream tasks.\nIn this paper, we propose a cost-effective optimization method for improving\nand re-purposing BigGANs by fine-tuning only the class-embedding layer. We show\nthe effectiveness of our model-editing approach in three tasks: (1)\nsignificantly improving the realism and diversity of samples of complete\nmode-collapse classes; (2) re-purposing ImageNet BigGANs for generating images\nfor Places365; and (3) de-biasing or improving the sample diversity for\nselected ImageNet classes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:18:28 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 18:34:35 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 06:35:26 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 21:46:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Qi", ""], ["Mai", "Long", ""], ["Alcorn", "Michael A.", ""], ["Nguyen", "Anh", ""]]}, {"id": "1910.04778", "submitter": "Hengrui Luo", "authors": "Hengrui Luo, Justin Strait", "title": "Combining Geometric and Topological Information for Boundary Estimation", "comments": "38 pages with appendices, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in computer vision is boundary estimation, where the\ngoal is to delineate the boundary of objects in an image. In this paper, we\npropose a method which jointly incorporates geometric and topological\ninformation within an image to simultaneously estimate boundaries for objects\nwithin images with more complex topologies. We use a topological\nclustering-based method to assist initialization of the Bayesian active contour\nmodel. This combines pixel clustering, boundary smoothness, and potential prior\nshape information to produce an estimated object boundary. Active contour\nmethods are knownto be extremely sensitive to algorithm initialization, relying\non the user to provide a reasonable starting curve to the algorithm. In the\npresence of images featuring objects with complex topological structures, such\nas objects with holes or multiple objects, the user must initialize separate\ncurves for each boundary of interest. Our proposed topologically-guided method\ncan provide an interpretable, smart initialization in these settings, freeing\nup the user from potential pitfalls associated with objects of complex\ntopological structure. We provide a detailed simulation study comparing our\ninitialization to boundary estimates obtained from standard segmentation\nalgorithms. The method is demonstrated on artificial image datasets from\ncomputer vision, as well as real-world applications to skin lesion and neural\ncellular images, for which multiple topological features can be identified.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:00:10 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:36:03 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 23:27:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Luo", "Hengrui", ""], ["Strait", "Justin", ""]]}, {"id": "1910.04817", "submitter": "Maggie Makar", "authors": "Maggie Makar, Fredrik D. Johansson, John Guttag, David Sontag", "title": "Estimation of Bounds on Potential Outcomes For Decision Making", "comments": null, "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of individual treatment effects is commonly used as the basis for\ncontextual decision making in fields such as healthcare, education, and\neconomics. However, it is often sufficient for the decision maker to have\nestimates of upper and lower bounds on the potential outcomes of decision\nalternatives to assess risks and benefits. We show that, in such cases, we can\nimprove sample efficiency by estimating simple functions that bound these\noutcomes instead of estimating their conditional expectations, which may be\ncomplex and hard to estimate. Our analysis highlights a trade-off between the\ncomplexity of the learning task and the confidence with which the learned\nbounds hold. Guided by these findings, we develop an algorithm for learning\nupper and lower bounds on potential outcomes which optimize an objective\nfunction defined by the decision maker, subject to the probability that bounds\nare violated being small. Using a clinical dataset and a well-known causality\nbenchmark, we demonstrate that our algorithm outperforms baselines, providing\ntighter, more reliable bounds.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 19:07:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:40:17 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 21:34:35 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 04:26:34 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Makar", "Maggie", ""], ["Johansson", "Fredrik D.", ""], ["Guttag", "John", ""], ["Sontag", "David", ""]]}, {"id": "1910.04819", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis", "title": "Information Aware Max-Norm Dirichlet Networks for Predictive Uncertainty\n  Estimation", "comments": "To appear in Neural Networks.\n  https://doi.org/10.1016/j.neunet.2020.12.011", "journal-ref": "Neural Networks, Volume 135, March 2021, Pages 105-114", "doi": "10.1016/j.neunet.2020.12.011", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise estimation of uncertainty in predictions for AI systems is a critical\nfactor in ensuring trust and safety. Deep neural networks trained with a\nconventional method are prone to over-confident predictions. In contrast to\nBayesian neural networks that learn approximate distributions on weights to\ninfer prediction confidence, we propose a novel method, Information Aware\nDirichlet networks, that learn an explicit Dirichlet prior distribution on\npredictive distributions by minimizing a bound on the expected max norm of the\nprediction error and penalizing information associated with incorrect outcomes.\nProperties of the new cost function are derived to indicate how improved\nuncertainty estimation is achieved. Experiments using real datasets show that\nour technique outperforms, by a large margin, state-of-the-art neural networks\nfor estimating within-distribution and out-of-distribution uncertainty, and\ndetecting adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 19:10:42 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 18:44:49 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 00:32:23 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 16:13:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""]]}, {"id": "1910.04832", "submitter": "Sui Tang", "authors": "Fei Lu, Mauro Maggioni, Sui Tang", "title": "Learning interaction kernels in heterogeneous systems of agents from\n  multiple trajectories", "comments": "63 pages, revised various places", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems of interacting particles or agents have wide applications in many\ndisciplines such as Physics, Chemistry, Biology and Economics. These systems\nare governed by interaction laws, which are often unknown: estimating them from\nobservation data is a fundamental task that can provide meaningful insights and\naccurate predictions of the behaviour of the agents. In this paper, we consider\nthe inverse problem of learning interaction laws given data from multiple\ntrajectories, in a nonparametric fashion, when the interaction kernels depend\non pairwise distances. We establish a condition for learnability of interaction\nkernels, and construct estimators that are guaranteed to converge in a suitable\n$L^2$ space, at the optimal min-max rate for 1-dimensional nonparametric\nregression. We propose an efficient learning algorithm based on least squares,\nwhich can be implemented in parallel for multiple trajectories and is therefore\nwell-suited for the high dimensional, big data regime. Numerical simulations on\na variety examples, including opinion dynamics, predator-swarm dynamics and\nheterogeneous particle dynamics, suggest that the learnability condition is\nsatisfied in models used in practice, and the rate of convergence of our\nestimator is consistent with the theory. These simulations also suggest that\nour estimators are robust to noise in the observations, and produce accurate\npredictions of dynamics in relative large time intervals, even when they are\nlearned from data collected in short time intervals.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 19:54:04 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:44:42 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 00:48:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lu", "Fei", ""], ["Maggioni", "Mauro", ""], ["Tang", "Sui", ""]]}, {"id": "1910.04849", "submitter": "Lu Wang", "authors": "Xinyun Chen, Lu Wang, Yizhe Hang, Heng Ge, Hongyuan Zha", "title": "Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior\n  Policies", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider off-policy policy evaluation when the trajectory data are\ngenerated by multiple behavior policies. Recent work has shown the key role\nplayed by the state or state-action stationary distribution corrections in the\ninfinite horizon context for off-policy policy evaluation. We propose estimated\nmixture policy (EMP), a novel class of partially policy-agnostic methods to\naccurately estimate those quantities. With careful analysis, we show that EMP\ngives rise to estimates with reduced variance for estimating the state\nstationary distribution correction while it also offers a useful induction bias\nfor estimating the state-action stationary distribution correction. In\nextensive experiments with both continuous and discrete environments, we\ndemonstrate that our algorithm offers significantly improved accuracy compared\nto the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 21:01:07 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chen", "Xinyun", ""], ["Wang", "Lu", ""], ["Hang", "Yizhe", ""], ["Ge", "Heng", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1910.04851", "submitter": "Charles Corbi\\`ere", "authors": "Charles Corbi\\`ere, Nicolas Thome, Avner Bar-Hen, Matthieu Cord,\n  Patrick P\\'erez", "title": "Addressing Failure Prediction by Learning Model Confidence", "comments": "NeurIPS 2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing reliably the confidence of a deep neural network and predicting its\nfailures is of primary importance for the practical deployment of these models.\nIn this paper, we propose a new target criterion for model confidence,\ncorresponding to the True Class Probability (TCP). We show how using the TCP is\nmore suited than relying on the classic Maximum Class Probability (MCP). We\nprovide in addition theoretical guarantees for TCP in the context of failure\nprediction. Since the true class is by essence unknown at test time, we propose\nto learn TCP criterion on the training set, introducing a specific learning\nscheme adapted to this context. Extensive experiments are conducted for\nvalidating the relevance of the proposed approach. We study various network\narchitectures, small and large scale datasets for image classification and\nsemantic segmentation. We show that our approach consistently outperforms\nseveral strong methods, from MCP to Bayesian uncertainty, as well as recent\napproaches specifically designed for failure prediction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:23:45 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 15:09:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Corbi\u00e8re", "Charles", ""], ["Thome", "Nicolas", ""], ["Bar-Hen", "Avner", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1910.04856", "submitter": "Giuseppe Lancioni", "authors": "Marco Zamprogno, Marco Passon, Niki Martinel, Giuseppe Serra, Giuseppe\n  Lancioni, Christian Micheloni, Carlo Tasso, Gian Luca Foresti", "title": "Video-Based Convolutional Attention for Person Re-Identification", "comments": "11 pages, 2 figures. Accepted by ICIAP2019, 20th International\n  Conference on IMAGE ANALYSIS AND PROCESSING, Trento, Italy, 9-13 September,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of video-based person\nre-identification, which is the task of associating videos of the same person\ncaptured by different and non-overlapping cameras. We propose a Siamese\nframework in which video frames of the person to re-identify and of the\ncandidate one are processed by two identical networks which produce a\nsimilarity score. We introduce an attention mechanisms to capture the relevant\ninformation both at frame level (spatial information) and at video level\n(temporal information given by the importance of a specific frame within the\nsequence). One of the novelties of our approach is given by a joint concurrent\nprocessing of both frame and video levels, providing in such a way a very\nsimple architecture. Despite this fact, our approach achieves better\nperformance than the state-of-the-art on the challenging iLIDS-VID dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:23:43 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zamprogno", "Marco", ""], ["Passon", "Marco", ""], ["Martinel", "Niki", ""], ["Serra", "Giuseppe", ""], ["Lancioni", "Giuseppe", ""], ["Micheloni", "Christian", ""], ["Tasso", "Carlo", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "1910.04857", "submitter": "Suryabhan Singh Hada", "authors": "Suryabhan Singh Hada and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Sampling the \"Inverse Set\" of a Neuron: An Approach to Understanding\n  Neural Nets", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of deep neural networks in computer vision, it is\nimportant to understand the internal working of these networks. What does a\ngiven neuron represent? The concepts captured by a neuron may be hard to\nunderstand or express in simple terms. The approach we propose in this paper is\nto characterize the region of input space that excites a given neuron to a\ncertain level; we call this the inverse set. This inverse set is a complicated\nhigh dimensional object that we explore by an optimization-based sampling\napproach. Inspection of samples of this set by a human can reveal regularities\nthat help to understand the neuron. This goes beyond approaches which were\nlimited to finding an image which maximally activates the neuron or using\nMarkov chain Monte Carlo to sample images, but this is very slow, generates\nsamples with little diversity and lacks control over the activation value of\nthe generated samples. Our approach also allows us to explore the intersection\nof inverse sets of several neurons and other variations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:22:43 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 00:49:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hada", "Suryabhan Singh", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1910.04864", "submitter": "Lichao Chen", "authors": "Lichao Chen, Sudhir Singh, Thomas Kailath, and Vwani Roychowdhury", "title": "Brain-inspired automated visual object discovery and detection", "comments": null, "journal-ref": "PNAS January 2, 2019 116 (1) 96-105", "doi": "10.1073/pnas.1802103115", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent progress, machine vision systems lag considerably\nbehind their biological counterparts in performance, scalability, and\nrobustness. A distinctive hallmark of the brain is its ability to automatically\ndiscover and model objects, at multiscale resolutions, from repeated exposures\nto unlabeled contextual data and then to be able to robustly detect the learned\nobjects under various nonideal circumstances, such as partial occlusion and\ndifferent view angles. Replication of such capabilities in a machine would\nrequire three key ingredients: (i) access to large-scale perceptual data of the\nkind that humans experience, (ii) flexible representations of objects, and\n(iii) an efficient unsupervised learning algorithm. The Internet fortunately\nprovides unprecedented access to vast amounts of visual data. This paper\nleverages the availability of such data to develop a scalable framework for\nunsupervised learning of object prototypes--brain-inspired flexible, scale, and\nshift invariant representations of deformable objects (e.g., humans,\nmotorcycles, cars, airplanes) comprised of parts, their different\nconfigurations and views, and their spatial relationships. Computationally, the\nobject prototypes are represented as geometric associative networks using\nprobabilistic constructs such as Markov random fields. We apply our framework\nto various datasets and show that our approach is computationally scalable and\ncan construct accurate and operational part-aware object models much more\nefficiently than in much of the recent computer vision literature. We also\npresent efficient algorithms for detection and localization in new scenes of\nobjects and their partial views.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 01:55:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chen", "Lichao", ""], ["Singh", "Sudhir", ""], ["Kailath", "Thomas", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "1910.04867", "submitter": "Neil Houlsby", "authors": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen,\n  Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim\n  Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen,\n  Marcin Michalski, Olivier Bousquet, Sylvain Gelly, Neil Houlsby", "title": "A Large-scale Study of Representation Learning with the Visual Task\n  Adaptation Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning promises to unlock deep learning for the long tail of\nvision tasks without expensive labelled datasets. Yet, the absence of a unified\nevaluation for general visual representations hinders progress. Popular\nprotocols are often too constrained (linear classification), limited in\ndiversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to\nrepresentation quality (ELBO, reconstruction error). We present the Visual Task\nAdaptation Benchmark (VTAB), which defines good representations as those that\nadapt to diverse, unseen tasks with few examples. With VTAB, we conduct a\nlarge-scale study of many popular publicly-available representation learning\nalgorithms. We carefully control confounders such as architecture and tuning\nbudget. We address questions like: How effective are ImageNet representations\nbeyond standard natural datasets? How do representations trained via generative\nand discriminative models compare? To what extent can self-supervision replace\nlabels? And, how close are we to general visual representations?\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:06:29 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 13:36:15 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhai", "Xiaohua", ""], ["Puigcerver", "Joan", ""], ["Kolesnikov", "Alexander", ""], ["Ruyssen", "Pierre", ""], ["Riquelme", "Carlos", ""], ["Lucic", "Mario", ""], ["Djolonga", "Josip", ""], ["Pinto", "Andre Susano", ""], ["Neumann", "Maxim", ""], ["Dosovitskiy", "Alexey", ""], ["Beyer", "Lucas", ""], ["Bachem", "Olivier", ""], ["Tschannen", "Michael", ""], ["Michalski", "Marcin", ""], ["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Houlsby", "Neil", ""]]}, {"id": "1910.04868", "submitter": "Haraldur Hallgr\\'imsson", "authors": "Haraldur T. Hallgrimsson, Richika Sharan, Scott T. Grafton, Ambuj K.\n  Singh", "title": "Estimating localized complexity of white-matter wiring with GANs", "comments": "Three page extended abstract, accepted to Medical Imaging meets\n  NeurIPS 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-vivo examination of the physical connectivity of axonal projections\nthrough the white matter of the human brain is made possible by diffusion\nweighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers\nderived scalar metrics such as fractional anisotrophy as proxies for \"white\nmatter integrity,\" and differences of such measures have been observed as\nsignificantly correlating with various neurological diagnosis and clinical\nmeasures such as executive function, presence of multiple sclerosis, and\ngenetic similarity. The analysis of such voxel measures is confounded in areas\nof more complicated fiber wiring due to crossing, kissing, and dispersing\nfibers. Recently, Volz et al. introduced a simple probabilistic measure of the\ncount of distinct fiber populations within a voxel, which was shown to reduce\nvariance in group comparisons. We propose a complementary measure that\nconsiders the complexity of a voxel in context of its local region, with an aim\nto quantify the localized wiring complexity of every part of white matter. This\nallows, for example, identification of particularly ambiguous regions of the\nbrain for tractographic approaches of modeling global wiring connectivity. Our\nmethod builds on recent advances in image inpainting, in which the task is to\nplausibly fill in a missing region of an image. Our proposed method builds on a\nBayesian estimate of heteroscedastic aleatoric uncertainty of a region of white\nmatter by inpainting it from its context. We define the localized wiring\ncomplexity of white matter as how accurately and confidently a well-trained\nmodel can predict the missing patch. In our results, we observe low aleatoric\nuncertainty along major neuronal pathways which increases at junctions and\ntowards cortex boundaries. This directly quantifies the difficulty of lesion\ninpainting of dMRI images at all parts of white matter.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:45:32 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 02:48:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hallgrimsson", "Haraldur T.", ""], ["Sharan", "Richika", ""], ["Grafton", "Scott T.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1910.04870", "submitter": "Rachel Blin", "authors": "Rachel Blin, Samia Ainouz, St\\'ephane Canu and Fabrice Meriaudeau", "title": "Road scenes analysis in adverse weather conditions by\n  polarization-encoded images and adapted deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in road scenes is necessary to develop both autonomous\nvehicles and driving assistance systems. Even if deep neural networks for\nrecognition task have shown great performances using conventional images, they\nfail to detect objects in road scenes in complex acquisition situations. In\ncontrast, polarization images, characterizing the light wave, can robustly\ndescribe important physical properties of the object even under poor\nillumination or strong reflections. This paper shows how non-conventional\npolarimetric imaging modality overcomes the classical methods for object\ndetection especially in adverse weather conditions. The efficiency of the\nproposed method is mostly due to the high power of the polarimetry to\ndiscriminate any object by its reflective properties and on the use of deep\nneural networks for object detection. Our goal by this work, is to prove that\npolarimetry brings a real added value compared with RGB images for object\ndetection. Experimental results on our own dataset composed of road scene\nimages taken during adverse weather conditions show that polarimetry together\nwith deep learning can improve the state-of-the-art by about 20% to 50% on\ndifferent detection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:47:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Blin", "Rachel", ""], ["Ainouz", "Samia", ""], ["Canu", "St\u00e9phane", ""], ["Meriaudeau", "Fabrice", ""]]}, {"id": "1910.04895", "submitter": "Hamda Ajmal", "authors": "Hamda Ajmal, Michael Madden, Catherine Enright", "title": "PROFET: Construction and Inference of DBNs Based on Mathematical Models", "comments": "9 pages. In Proceedings of the 14th UAI Bayesian Modelling\n  Applications Workshop (BMAW 2017), co-located with the 33rd Conference on\n  Uncertainty in Artificial Intelligence (UAI 2017) Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents, evaluates, and discusses a new software tool to\nautomatically build Dynamic Bayesian Networks (DBNs) from ordinary differential\nequations (ODEs) entered by the user. The DBNs generated from ODE models can\nhandle both data uncertainty and model uncertainty in a principled manner. The\napplication, named PROFET, can be used for temporal data mining with noisy or\nmissing variables. It enables automatic re-estimation of model parameters using\ntemporal evidence in the form of data streams. For temporal inference, PROFET\nincludes both standard fixed time step particle filtering and its extension,\nadaptive-time particle filtering algorithms. Adaptive-time particle filtering\nenables the DBN to automatically adapt its time step length to match the\ndynamics of the model. We demonstrate PROFET's functionality by using it to\ninfer the model variables by estimating the model parameters of four benchmark\nODE systems. From the generation of the DBN model to temporal inference, the\nentire process is automated and is delivered as an open-source\nplatform-independent software application with a comprehensive user interface.\nPROFET is released under the Apache License 2.0. Its source code, executable\nand documentation are available at http:://profet.it.nuigalway.ie.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 22:17:12 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:36:45 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ajmal", "Hamda", ""], ["Madden", "Michael", ""], ["Enright", "Catherine", ""]]}, {"id": "1910.04900", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Aaditya Ramdas", "title": "Online control of the familywise error rate", "comments": "Submitted to Biostatistics; added the real data example; renewed some\n  of the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological research often involves testing a growing number of null\nhypotheses as new data is accumulated over time. We study the problem of online\ncontrol of the familywise error rate (FWER), that is testing an apriori\nunbounded sequence of hypotheses (p-values) one by one over time without\nknowing the future, such that with high probability there are no false\ndiscoveries in the entire sequence. This paper unifies algorithmic concepts\ndeveloped for offline (single batch) FWER control and online false discovery\nrate control to develop novel online FWER control methods. Though many offline\nFWER methods (e.g. Bonferroni, fallback procedures and Sidak's method) can\ntrivially be extended to the online setting, our main contribution is the\ndesign of new, powerful, adaptive online algorithms that control the FWER when\nthe p-values are independent or locally dependent in time. Our experiments\ndemonstrate substantial gains in power, that are also formally proved in a\nGaussian sequence model. Multiple testing, FWER control, online setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 22:50:03 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:21:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tian", "Jinjin", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1910.04909", "submitter": "Hamda Ajmal", "authors": "Hamda Ajmal, Michael Madden, Catherine Enright", "title": "Dealing with Stochasticity in Biological ODE Models", "comments": "5 pages. In Workshop on Computational Biology (WCB) 2017, co-located\n  with the 34th International Conference on Machine Learning (ICML). arXiv\n  admin note: text overlap with arXiv:1910.04895", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical modeling with Ordinary Differential Equations (ODEs) has proven\nto be extremely successful in a variety of fields, including biology. However,\nthese models are completely deterministic given a certain set of initial\nconditions. We convert mathematical ODE models of three benchmark biological\nsystems to Dynamic Bayesian Networks (DBNs). The DBN model can handle model\nuncertainty and data uncertainty in a principled manner. They can be used for\ntemporal data mining for noisy and missing variables. We apply Particle\nFiltering algorithm to infer the model variables by re-estimating the models\nparameters of various biological ODE models. The model parameters are\nautomatically re-estimated using temporal evidence in the form of data streams.\nThe results show that DBNs are capable of inferring the model variables of the\nODE model with high accuracy in situations where data is missing, incomplete,\nsparse and irregular and true values of model parameters are not known.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:20:19 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:57:03 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ajmal", "Hamda", ""], ["Madden", "Michael", ""], ["Enright", "Catherine", ""]]}, {"id": "1910.04915", "submitter": "Krzysztof Maziarz", "authors": "Krzysztof Maziarz, Efi Kokiopoulou, Andrea Gesmundo, Luciano Sbaiz,\n  Gabor Bartok, Jesse Berent", "title": "Flexible Multi-task Networks by Learning Parameter Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel learning method for multi-task applications.\nMulti-task neural networks can learn to transfer knowledge across different\ntasks by using parameter sharing. However, sharing parameters between unrelated\ntasks can hurt performance. To address this issue, we propose a framework to\nlearn fine-grained patterns of parameter sharing. Assuming that the network is\ncomposed of several components across layers, our framework uses learned binary\nvariables to allocate components to tasks in order to encourage more parameter\nsharing between related tasks, and discourage parameter sharing otherwise. The\nbinary allocation variables are learned jointly with the model parameters by\nstandard back-propagation thanks to the Gumbel-Softmax reparametrization\nmethod. When applied to the Omniglot benchmark, the proposed method achieves a\n17% relative reduction of the error rate compared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:46:08 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 15:21:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Maziarz", "Krzysztof", ""], ["Kokiopoulou", "Efi", ""], ["Gesmundo", "Andrea", ""], ["Sbaiz", "Luciano", ""], ["Bartok", "Gabor", ""], ["Berent", "Jesse", ""]]}, {"id": "1910.04920", "submitter": "Sharan Vaswani", "authors": "Si Yi Meng, Sharan Vaswani, Issam Laradji, Mark Schmidt, Simon\n  Lacoste-Julien", "title": "Fast and Furious Convergence: Stochastic Second Order Methods under\n  Interpolation", "comments": "AISTATS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic second-order methods for minimizing smooth and\nstrongly-convex functions under an interpolation condition satisfied by\nover-parameterized models. Under this condition, we show that the regularized\nsubsampled Newton method (R-SSN) achieves global linear convergence with an\nadaptive step-size and a constant batch-size. By growing the batch size for\nboth the subsampled gradient and Hessian, we show that R-SSN can converge at a\nquadratic rate in a local neighbourhood of the solution. We also show that\nR-SSN attains local linear convergence for the family of self-concordant\nfunctions. Furthermore, we analyze stochastic BFGS algorithms in the\ninterpolation setting and prove their global linear convergence. We empirically\nevaluate stochastic L-BFGS and a \"Hessian-free\" implementation of R-SSN for\nbinary classification on synthetic, linearly-separable datasets and real\ndatasets under a kernel mapping. Our experimental results demonstrate the fast\nconvergence of these methods, both in terms of the number of iterations and\nwall-clock time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 00:24:19 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 21:47:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Meng", "Si Yi", ""], ["Vaswani", "Sharan", ""], ["Laradji", "Issam", ""], ["Schmidt", "Mark", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1910.04928", "submitter": "Sharan Vaswani", "authors": "Sharan Vaswani, Abbas Mehrabian, Audrey Durand, Branislav Kveton", "title": "Old Dog Learns New Tricks: Randomized UCB for Bandit Problems", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose $\\tt RandUCB$, a bandit strategy that builds on theoretically\nderived confidence intervals similar to upper confidence bound (UCB)\nalgorithms, but akin to Thompson sampling (TS), it uses randomization to trade\noff exploration and exploitation. In the $K$-armed bandit setting, we show that\nthere are infinitely many variants of $\\tt RandUCB$, all of which achieve the\nminimax-optimal $\\widetilde{O}(\\sqrt{K T})$ regret after $T$ rounds. Moreover,\nfor a specific multi-armed bandit setting, we show that both UCB and TS can be\nrecovered as special cases of $\\tt RandUCB$. For structured bandits, where each\narm is associated with a $d$-dimensional feature vector and rewards are\ndistributed according to a linear or generalized linear model, we prove that\n$\\tt RandUCB$ achieves the minimax-optimal $\\widetilde{O}(d \\sqrt{T})$ regret\neven in the case of infinitely many arms. Through experiments in both the\nmulti-armed and structured bandit settings, we demonstrate that $\\tt RandUCB$\nmatches or outperforms TS and other randomized exploration strategies. Our\ntheoretical and empirical results together imply that $\\tt RandUCB$ achieves\nthe best of both worlds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:15:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 00:11:07 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Vaswani", "Sharan", ""], ["Mehrabian", "Abbas", ""], ["Durand", "Audrey", ""], ["Kveton", "Branislav", ""]]}, {"id": "1910.04930", "submitter": "Qilong Gu", "authors": "Arindam Banerjee, Qilong Gu, Vidyashankar Sivakumar, and Zhiwei Steven\n  Wu", "title": "Random Quadratic Forms with Dependence: Applications to Restricted\n  Isometry and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important families of computational and statistical results in\nmachine learning and randomized algorithms rely on uniform bounds on quadratic\nforms of random vectors or matrices. Such results include the\nJohnson-Lindenstrauss (J-L) Lemma, the Restricted Isometry Property (RIP),\nrandomized sketching algorithms, and approximate linear algebra. The existing\nresults critically depend on statistical independence, e.g., independent\nentries for random vectors, independent rows for random matrices, etc., which\nprevent their usage in dependent or adaptive modeling settings. In this paper,\nwe show that such independence is in fact not needed for such results which\ncontinue to hold under fairly general dependence structures. In particular, we\npresent uniform bounds on random quadratic forms of stochastic processes which\nare conditionally independent and sub-Gaussian given another (latent) process.\nOur setup allows general dependencies of the stochastic process on the history\nof the latent process and the latent process to be influenced by realizations\nof the stochastic process. The results are thus applicable to adaptive modeling\nsettings and also allows for sequential design of random vectors and matrices.\nWe also discuss stochastic process based forms of J-L, RIP, and sketching, to\nillustrate the generality of the results.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:30:46 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 05:32:02 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Banerjee", "Arindam", ""], ["Gu", "Qilong", ""], ["Sivakumar", "Vidyashankar", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1910.04938", "submitter": "Yangyi Lu", "authors": "Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, Zhenyu Yan", "title": "Regret Analysis of Bandit Problems with Causal Background Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to learn optimal interventions sequentially given causal\ninformation represented as a causal graph along with associated conditional\ndistributions. Causal modeling is useful in real world problems like online\nadvertisement where complex causal mechanisms underlie the relationship between\ninterventions and outcomes. We propose two algorithms, causal upper confidence\nbound (C-UCB) and causal Thompson Sampling (C-TS), that enjoy improved\ncumulative regret bounds compared with algorithms that do not use causal\ninformation. We thus resolve an open problem posed by\n\\cite{lattimore2016causal}. Further, we extend C-UCB and C-TS to the linear\nbandit setting and propose causal linear UCB (CL-UCB) and causal linear TS\n(CL-TS) algorithms. These algorithms enjoy a cumulative regret bound that only\nscales with the feature dimension. Our experiments show the benefit of using\ncausal information. For example, we observe that even with a few hundreds of\niterations, the regret of causal algorithms is less than that of standard\nalgorithms by a factor of three. We also show that under certain causal\nstructures, our algorithms scale better than the standard bandit algorithms as\nthe number of interventions increases.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:00:32 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 18:28:46 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 18:31:45 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Lu", "Yangyi", ""], ["Meisami", "Amirhossein", ""], ["Tewari", "Ambuj", ""], ["Yan", "Zhenyu", ""]]}, {"id": "1910.04939", "submitter": "Hung Ngo", "authors": "Ryan Curtin, Ben Moseley, Hung Q. Ngo, XuanLong Nguyen, Dan Olteanu,\n  Maximilian Schleich", "title": "Rk-means: Fast Clustering for Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional machine learning algorithms cannot be applied until a data\nmatrix is available to process. When the data matrix needs to be obtained from\na relational database via a feature extraction query, the computation cost can\nbe prohibitive, as the data matrix may be (much) larger than the total input\nrelation size. This paper introduces Rk-means, or relational k -means\nalgorithm, for clustering relational data tuples without having to access the\nfull data matrix. As such, we avoid having to run the expensive feature\nextraction query and storing its output. Our algorithm leverages the underlying\nstructures in relational data. It involves construction of a small {\\it grid\ncoreset} of the data matrix for subsequent cluster construction. This gives a\nconstant approximation for the k -means objective, while having asymptotic\nruntime improvements over standard approaches of first running the database\nquery and then clustering. Empirical results show orders-of-magnitude speedup,\nand Rk-means can run faster on the database than even just computing the data\nmatrix.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:03:08 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Curtin", "Ryan", ""], ["Moseley", "Ben", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1910.04952", "submitter": "Anastasios Kyrillidis", "authors": "John Chen, Cameron Wolfe, Zhao Li, Anastasios Kyrillidis", "title": "Demon: Improved Neural Network Training with Momentum Decay", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Momentum is a widely used technique for gradient-based optimizers in deep\nlearning. In this paper, we propose a decaying momentum (\\textsc{Demon}) rule.\nWe conduct the first large-scale empirical analysis of momentum decay methods\nfor modern neural network optimization, in addition to the most popular\nlearning rate decay schedules. Across 28 relevant combinations of models,\nepochs, datasets, and optimizers, \\textsc{Demon} achieves the highest number of\nTop-1 and Top-3 finishes at 39\\% and 85\\% respectively, almost doubling the\nsecond-placed learning rate cosine schedule at 17\\% and 60\\%, respectively.\n\\textsc{Demon} also outperforms other widely used schedulers including, but not\nlimited to, the learning rate step schedule, linear schedule, OneCycle\nschedule, and exponential schedule. Compared with the widely used learning rate\nstep schedule, \\textsc{Demon} is observed to be less sensitive to parameter\ntuning, which is critical to training neural networks in practice. Results are\ndemonstrated across a variety of settings and architectures, including image\nclassification, generative models, and language models. \\textsc{Demon} is easy\nto implement, requires no additional tuning, and incurs almost no extra\ncomputational overhead compared to the vanilla counterparts. Code is readily\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:12:21 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:04:16 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 22:35:49 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 12:50:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chen", "John", ""], ["Wolfe", "Cameron", ""], ["Li", "Zhao", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1910.04956", "submitter": "Chaoyang He", "authors": "Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, Ji Liu", "title": "Central Server Free Federated Learning over Single-sided Trust Social\n  Networks", "comments": "decentralized federated learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning has become increasingly important for modern machine\nlearning, especially for data privacy-sensitive scenarios. Existing federated\nlearning mostly adopts the central server-based architecture or centralized\narchitecture. However, in many social network scenarios, centralized federated\nlearning is not applicable (e.g., a central agent or server connecting all\nusers may not exist, or the communication cost to the central server is not\naffordable). In this paper, we consider a generic setting: 1) the central\nserver may not exist, and 2) the social network is unidirectional or of\nsingle-sided trust (i.e., user A trusts user B but user B may not trust user\nA). We propose a central server free federated learning algorithm, named Online\nPush-Sum (OPS) method, to handle this challenging but generic scenario. A\nrigorous regret analysis is also provided, which shows very interesting results\non how users can benefit from communication with trusted users in the federated\nlearning scenario. This work builds upon the fundamental algorithm framework\nand theoretical guarantees for federated learning in the generic social network\nscenario.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:36:53 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 17:20:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["He", "Chaoyang", ""], ["Tan", "Conghui", ""], ["Tang", "Hanlin", ""], ["Qiu", "Shuang", ""], ["Liu", "Ji", ""]]}, {"id": "1910.04968", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Daniel L. Jiang, Aaditya Ramdas, Michael I. Jordan", "title": "The Power of Batching in Multiple Hypothesis Testing", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important partition of algorithms for controlling the false discovery\nrate (FDR) in multiple testing is into offline and online algorithms. The first\ngenerally achieve significantly higher power of discovery, while the latter\nallow making decisions sequentially as well as adaptively formulating\nhypotheses based on past observations. Using existing methodology, it is\nunclear how one could trade off the benefits of these two broad families of\nalgorithms, all the while preserving their formal FDR guarantees. To this end,\nwe introduce $\\text{Batch}_{\\text{BH}}$ and $\\text{Batch}_{\\text{St-BH}}$,\nalgorithms for controlling the FDR when a possibly infinite sequence of batches\nof hypotheses is tested by repeated application of one of the most widely used\noffline algorithms, the Benjamini-Hochberg (BH) method or Storey's improvement\nof the BH method. We show that our algorithms interpolate between existing\nonline and offline methodology, thus trading off the best of both worlds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:35:20 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 05:38:58 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 00:11:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zrnic", "Tijana", ""], ["Jiang", "Daniel L.", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1910.04970", "submitter": "Gege Zhang", "authors": "Gege Zhang, Gangwei Li, Ningwei Shen and Weidong Zhang", "title": "The Expressivity and Training of Deep Neural Networks: toward the Edge\n  of Chaos?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressivity is one of the most significant issues in assessing neural\nnetworks. In this paper, we provide a quantitative analysis of the expressivity\nfor the deep neural network (DNN) from its dynamic model, where the Hilbert\nspace is employed to analyze the convergence and criticality. We study the\nfeature mapping of several widely used activation functions obtained by Hermite\npolynomials, and find sharp declines or even saddle points in the feature\nspace, which stagnate the information transfer in DNNs. We then present a new\nactivation function design based on the Hermite polynomials for better\nutilization of spatial representation. Moreover, we analyze the information\ntransfer of DNNs, emphasizing the convergence problem caused by the mismatch\nbetween input and topological structure. We also study the effects of input\nperturbations and regularization operators on critical expressivity. Our\ntheoretical analysis reveals that DNNs use spatial domains for information\nrepresentation and evolve to the edge of chaos as depth increases. In actual\ntraining, whether a particular network can ultimately arrive the edge of chaos\ndepends on its ability to overcome convergence and pass information to the\nrequired network depth. Finally, we demonstrate the empirical performance of\nthe proposed hypothesis via multivariate time series prediction and image\nclassification examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:44:10 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 13:33:24 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhang", "Gege", ""], ["Li", "Gangwei", ""], ["Shen", "Ningwei", ""], ["Zhang", "Weidong", ""]]}, {"id": "1910.04979", "submitter": "Nicholas Andrews", "authors": "Nicholas Andrews and Marcus Bishop", "title": "Learning Invariant Representations of Social Media Users", "comments": "12 pages, 3 figures; to be published in EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of social media users' behavior over time complicates\nuser-level comparison tasks such as verification, classification, clustering,\nand ranking. As a result, na\\\"ive approaches may fail to generalize to new\nusers or even to future observations of previously known users. In this paper,\nwe propose a novel procedure to learn a mapping from short episodes of user\nactivity on social media to a vector space in which the distance between points\ncaptures the similarity of the corresponding users' invariant features. We fit\nthe model by optimizing a surrogate metric learning objective over a large\ncorpus of unlabeled social media content. Once learned, the mapping may be\napplied to users not seen at training time and enables efficient comparisons of\nusers in the resulting vector space. We present a comprehensive evaluation to\nvalidate the benefits of the proposed approach using data from Reddit, Twitter,\nand Wikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 05:37:11 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Andrews", "Nicholas", ""], ["Bishop", "Marcus", ""]]}, {"id": "1910.05006", "submitter": "Zvika Ben-Haim", "authors": "Zvika Ben-Haim, Vladimir Anisimov, Aaron Yonas, Varun Gulshan, Yusef\n  Shafi, Stephan Hoyer, and Sella Nevo", "title": "Inundation Modeling in Data Scarce Regions", "comments": "To appear in the Artificial Intelligence for Humanitarian Assistance\n  and Disaster Response Workshop (AI+HADR) @ NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flood forecasts are crucial for effective individual and governmental\nprotective action. The vast majority of flood-related casualties occur in\ndeveloping countries, where providing spatially accurate forecasts is a\nchallenge due to scarcity of data and lack of funding. This paper describes an\noperational system providing flood extent forecast maps covering several\nflood-prone regions in India, with the goal of being sufficiently scalable and\ncost-efficient to facilitate the establishment of effective flood forecasting\nsystems globally.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 07:40:39 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:54:48 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Ben-Haim", "Zvika", ""], ["Anisimov", "Vladimir", ""], ["Yonas", "Aaron", ""], ["Gulshan", "Varun", ""], ["Shafi", "Yusef", ""], ["Hoyer", "Stephan", ""], ["Nevo", "Sella", ""]]}, {"id": "1910.05018", "submitter": "Nathana\\\"el Fijalkow", "authors": "Nathana\\\"el Fijalkow, Mohit Kumar Gupta", "title": "Verification of Neural Networks: Specifying Global Robustness using\n  Generative Models", "comments": "A preliminary version was presented at the VNN Symposium\n  (Verification of Neural Networks) in Stanford, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of neural networks across most machine learning tasks and the\npersistence of adversarial examples have made the verification of such models\nan important quest. Several techniques have been successfully developed to\nverify robustness, and are now able to evaluate neural networks with thousands\nof nodes. The main weakness of this approach is in the specification:\nrobustness is asserted on a validation set consisting of a finite set of\nexamples, i.e. locally.\n  We propose a notion of global robustness based on generative models, which\nasserts the robustness on a very large and representative set of examples. We\nshow how this can be used for verifying neural networks. In this paper we\nexperimentally explore the merits of this approach, and show how it can be used\nto construct realistic adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 08:05:54 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Fijalkow", "Nathana\u00ebl", ""], ["Gupta", "Mohit Kumar", ""]]}, {"id": "1910.05026", "submitter": "Alex Bird", "authors": "Alex Bird, Christopher K. I. Williams", "title": "Customizing Sequence Generation with Multi-Task Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical system models (including RNNs) often lack the ability to adapt the\nsequence generation or prediction to a given context, limiting their real-world\napplication. In this paper we show that hierarchical multi-task dynamical\nsystems (MTDSs) provide direct user control over sequence generation, via use\nof a latent code $\\mathbf{z}$ that specifies the customization to the\nindividual data sequence. This enables style transfer, interpolation and\nmorphing within generated sequences. We show the MTDS can improve predictions\nvia latent code interpolation, and avoid the long-term performance degradation\nof standard RNN approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 08:32:13 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Bird", "Alex", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1910.05030", "submitter": "Maximilian Idahl", "authors": "Maximilian Idahl, Megha Khosla and Avishek Anand", "title": "Finding Interpretable Concept Spaces in Node Embeddings using Knowledge\n  Bases", "comments": "Accepted for poster presentation at ECML PKDD AIMLAI-XKDD workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study the novel problem of explaining node\nembeddings by finding embedded human interpretable subspaces in already trained\nunsupervised node representation embeddings. We use an external knowledge base\nthat is organized as a taxonomy of human-understandable concepts over entities\nas a guide to identify subspaces in node embeddings learned from an entity\ngraph derived from Wikipedia. We propose a method that given a concept finds a\nlinear transformation to a subspace where the structure of the concept is\nretained. Our initial experiments show that we obtain low error in finding\nfine-grained concepts.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 08:40:50 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Idahl", "Maximilian", ""], ["Khosla", "Megha", ""], ["Anand", "Avishek", ""]]}, {"id": "1910.05057", "submitter": "Elahe Arani", "authors": "Elahe Arani, Fahad Sarfraz, Bahram Zonooz", "title": "Noise as a Resource for Learning in Knowledge Distillation", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV, 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While noise is commonly considered a nuisance in computing systems, a number\nof studies in neuroscience have shown several benefits of noise in the nervous\nsystem from enabling the brain to carry out computations such as probabilistic\ninference as well as carrying additional information about the stimuli.\nSimilarly, noise has been shown to improve the performance of deep neural\nnetworks. In this study, we further investigate the effect of adding noise in\nthe knowledge distillation framework because of its resemblance to\ncollaborative subnetworks in the brain regions. We empirically show that\ninjecting constructive noise at different levels in the collaborative learning\nframework enables us to train the model effectively and distill desirable\ncharacteristics in the student model. In doing so, we propose three different\nmethods that target the common challenges in deep neural networks: minimizing\nthe performance gap between a compact model and large model (Fickle Teacher),\ntraining high performance compact adversarially robust models (Soft\nRandomization), and training models efficiently under label noise (Messy\nCollaboration). Our findings motivate further study in the role of noise as a\nresource for learning in a collaborative learning framework.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 09:58:50 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:52:30 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Arani", "Elahe", ""], ["Sarfraz", "Fahad", ""], ["Zonooz", "Bahram", ""]]}, {"id": "1910.05083", "submitter": "Kohei Yoshikawa", "authors": "Kohei Yoshikawa, Shuichi Kawano", "title": "Sparse Reduced-Rank Regression for Simultaneous Rank and Variable\n  Selection via Manifold Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a reduced-rank regression model whose\ncoefficient parameter is represented as a singular value decomposition with\nsparse singular vectors. The traditional estimation procedure for the\ncoefficient parameter often fails when the true rank of the parameter is high.\nTo overcome this issue, we develop an estimation algorithm with rank and\nvariable selection via sparse regularization and manifold optimization, which\nenables us to obtain an accurate estimation of the coefficient parameter even\nif the true rank of the coefficient parameter is high. Using sparse\nregularization, we can also select an optimal value of the rank. We conduct\nMonte Carlo experiments and real data analysis to illustrate the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:22:21 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 05:38:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Yoshikawa", "Kohei", ""], ["Kawano", "Shuichi", ""]]}, {"id": "1910.05093", "submitter": "Tao Sun", "authors": "Tao Sun, Yuejiao Sun, Dongsheng Li, Qing Liao", "title": "General Proximal Incremental Aggregated Gradient Algorithms: Better and\n  Novel Results under General Scheme", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incremental aggregated gradient algorithm is popular in network\noptimization and machine learning research. However, the current convergence\nresults require the objective function to be strongly convex. And the existing\nconvergence rates are also limited to linear convergence. Due to the\nmathematical techniques, the stepsize in the algorithm is restricted by the\nstrongly convex constant, which may make the stepsize be very small (the\nstrongly convex constant may be small).\n  In this paper, we propose a general proximal incremental aggregated gradient\nalgorithm, which contains various existing algorithms including the basic\nincremental aggregated gradient method. Better and new convergence results are\nproved even with the general scheme. The novel results presented in this paper,\nwhich have not appeared in previous literature, include: a general scheme,\nnonconvex analysis, the sublinear convergence rates of the function values,\nmuch larger stepsizes that guarantee the convergence, the convergence when\nnoise exists, the line search strategy of the proximal incremental aggregated\ngradient algorithm and its convergence.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:41:37 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Sun", "Tao", ""], ["Sun", "Yuejiao", ""], ["Li", "Dongsheng", ""], ["Liao", "Qing", ""]]}, {"id": "1910.05098", "submitter": "Elahe Ghalebi", "authors": "Elahe Ghalebi, Hamidreza Mahyar, Radu Grosu, Graham W. Taylor, Sinead\n  A. Williamson", "title": "A Nonparametric Bayesian Model for Sparse Dynamic Multigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the availability and importance of temporal interaction data--such as\nemail communication--increases, it becomes increasingly important to understand\nthe underlying structure that underpins these interactions. Often these\ninteractions form a multigraph, where we might have multiple interactions\nbetween two entities. Such multigraphs tend to be sparse yet structured, and\ntheir distribution often evolves over time. Existing statistical models with\ninterpretable parameters can capture some, but not all, of these properties. We\npropose a dynamic nonparametric model for interaction multigraphs that combines\nthe sparsity of edge-exchangeable multigraphs with dynamic clustering patterns\nthat tend to reinforce recent behavioral patterns. We show that our method\nyields improved held-out likelihood over stationary variants, and impressive\npredictive performance against a range of state-of-the-art dynamic graph\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:07:54 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 23:01:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ghalebi", "Elahe", ""], ["Mahyar", "Hamidreza", ""], ["Grosu", "Radu", ""], ["Taylor", "Graham W.", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1910.05103", "submitter": "Wittawat Jitkrittum", "authors": "Mijung Park, Margarita Vinaroz, Wittawat Jitkrittum", "title": "ABCDP: Approximate Bayesian Computation with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approximate Bayesian computation (ABC) framework, ABCDP,\nthat produces differentially private (DP) and approximate posterior samples.\nOur framework takes advantage of the Sparse Vector Technique (SVT), widely\nstudied in the differential privacy literature. SVT incurs the privacy cost\nonly when a condition (whether a quantity of interest is above/below a\nthreshold) is met. If the condition is met sparsely during the repeated\nqueries, SVT can drastically reduces the cumulative privacy loss, unlike the\nusual case where every query incurs the privacy loss. In ABC, the quantity of\ninterest is the distance between observed and simulated data, and only when the\ndistance is below a threshold, we take the corresponding prior sample as a\nposterior sample. Hence, applying SVT to ABC is an organic way to transform an\nABC algorithm to a privacy-preserving variant with minimal modification, but\nyields the posterior samples with a high privacy level. We theoretically\nanalyze the interplay between the noise added for privacy and the accuracy of\nthe posterior samples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:14:22 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:36:42 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 20:55:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Park", "Mijung", ""], ["Vinaroz", "Margarita", ""], ["Jitkrittum", "Wittawat", ""]]}, {"id": "1910.05104", "submitter": "Igor Colin", "authors": "Igor Colin, Ludovic Dos Santos, Kevin Scaman", "title": "Theoretical Limits of Pipeline Parallel Optimization and Application to\n  Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the theoretical limits of pipeline parallel learning of deep\nlearning architectures, a distributed setup in which the computation is\ndistributed per layer instead of per example. For smooth convex and non-convex\nobjective functions, we provide matching lower and upper complexity bounds and\nshow that a naive pipeline parallelization of Nesterov's accelerated gradient\ndescent is optimal. For non-smooth convex functions, we provide a novel\nalgorithm coined Pipeline Parallel Random Smoothing (PPRS) that is within a\n$d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is\nthe underlying dimension. While the convergence rate still obeys a slow\n$\\varepsilon^{-2}$ convergence rate, the depth-dependent part is accelerated,\nresulting in a near-linear speed-up and convergence time that only slightly\ndepends on the depth of the deep learning architecture. Finally, we perform an\nempirical analysis of the non-smooth non-convex case and show that, for\ndifficult and highly non-smooth problems, PPRS outperforms more traditional\noptimization algorithms such as gradient descent and Nesterov's accelerated\ngradient descent for problems where the sample size is limited, such as\nfew-shot or adversarial learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:18:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Colin", "Igor", ""], ["Santos", "Ludovic Dos", ""], ["Scaman", "Kevin", ""]]}, {"id": "1910.05108", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "Differentially Private Survival Function Estimation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival function estimation is used in many disciplines, but it is most\ncommon in medical analytics in the form of the Kaplan-Meier estimator.\nSensitive data (patient records) is used in the estimation without any explicit\ncontrol on the information leakage, which is a significant privacy concern. We\npropose a first differentially private estimator of the survival function and\nshow that it can be easily extended to provide differentially private\nconfidence intervals and test statistics without spending any extra privacy\nbudget. We further provide extensions for differentially private estimation of\nthe competing risk cumulative incidence function, Nelson-Aalen's estimator for\nthe hazard function, etc. Using eleven real-life clinical datasets, we provide\nempirical evidence that our proposed method provides good utility while\nsimultaneously providing strong privacy guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:15:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 21:02:49 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1910.05110", "submitter": "Edgar Liberis", "authors": "Edgar Liberis, Nicholas D. Lane", "title": "Neural networks on microcontrollers: saving memory at inference via\n  operator reordering", "comments": "The tool is available at https://github.com/oxmlsys/tflite-tools", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing deep learning models for highly-constrained hardware would allow\nimbuing many edge devices with intelligence. Microcontrollers (MCUs) are an\nattractive platform for building smart devices due to their low cost, wide\navailability, and modest power usage. However, they lack the computational\nresources to run neural networks as straightforwardly as mobile or server\nplatforms, which necessitates changes to the network architecture and the\ninference software. In this work, we discuss the deployment and memory concerns\nof neural networks on MCUs and present a way of saving memory by changing the\nexecution order of the network's operators, which is orthogonal to other\ncompression methods. We publish a tool for reordering operators of TensorFlow\nLite models and demonstrate its utility by sufficiently reducing the memory\nfootprint of a CNN to deploy it on an MCU with 512KB SRAM.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:17:14 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 17:31:30 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Liberis", "Edgar", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "1910.05113", "submitter": "Deepak P", "authors": "Savitha Sam Abraham, Deepak P, Sowmya S Sundaram", "title": "Fairness in Clustering with Multiple Sensitive Attributes", "comments": "Proceedings of the 23rd International Conference on Extending\n  Database Technology (EDBT 2020), 30th March-2nd April, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clustering may be considered as fair on pre-specified sensitive attributes\nif the proportions of sensitive attribute groups in each cluster reflect that\nin the dataset. In this paper, we consider the task of fair clustering for\nscenarios involving multiple multi-valued or numeric sensitive attributes. We\npropose a fair clustering method, \\textit{FairKM} (Fair K-Means), that is\ninspired by the popular K-Means clustering formulation. We outline a\ncomputational notion of fairness which is used along with a cluster coherence\nobjective, to yield the FairKM clustering method. We empirically evaluate our\napproach, wherein we quantify both the quality and fairness of clusters, over\nreal-world datasets. Our experimental evaluation illustrates that the clusters\ngenerated by FairKM fare significantly better on both clustering quality and\nfair representation of sensitive attribute groups compared to the clusters from\na state-of-the-art baseline fair clustering method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:28:52 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 16:52:25 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Abraham", "Savitha Sam", ""], ["P", "Deepak", ""], ["Sundaram", "Sowmya S", ""]]}, {"id": "1910.05117", "submitter": "Steven Atkinson", "authors": "Steven Atkinson and Waad Subber and Liping Wang and Genghis Khan and\n  Philippe Hawi and Roger Ghanem", "title": "Data-driven discovery of free-form governing differential equations", "comments": "Approved for public release; distribution is unlimited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of discovering governing differential equations from data\nwithout the need to specify a priori the terms to appear in the equation. The\ninput to our method is a dataset (or ensemble of datasets) corresponding to a\nparticular solution (or ensemble of particular solutions) of a differential\nequation. The output is a human-readable differential equation with parameters\ncalibrated to the individual particular solutions provided. The key to our\nmethod is to learn differentiable models of the data that subsequently serve as\ninputs to a genetic programming algorithm in which graphs specify computation\nover arbitrary compositions of functions, parameters, and (potentially\ndifferential) operators on functions. Differential operators are composed and\nevaluated using recursive application of automatic differentiation, allowing\nour algorithm to explore arbitrary compositions of operators without the need\nfor human intervention. We also demonstrate an active learning process to\nidentify and remedy deficiencies in the proposed governing equations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:12:19 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 17:17:48 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Atkinson", "Steven", ""], ["Subber", "Waad", ""], ["Wang", "Liping", ""], ["Khan", "Genghis", ""], ["Hawi", "Philippe", ""], ["Ghanem", "Roger", ""]]}, {"id": "1910.05124", "submitter": "Christopher De Sa", "authors": "Bowen Yang, Jian Zhang, Jonathan Li, Christopher R\\'e, Christopher R.\n  Aberger and Christopher De Sa", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipeline parallelism (PP) when training neural networks enables larger models\nto be partitioned spatially, leading to both lower network communication and\noverall higher hardware utilization. Unfortunately, to preserve the statistical\nefficiency of sequential training, existing PP techniques sacrifice hardware\nefficiency by decreasing pipeline utilization or incurring extra memory costs.\nIn this paper, we investigate to what extent these sacrifices are necessary. We\ndevise PipeMare, a simple yet robust training method that tolerates\nasynchronous updates during PP execution without sacrificing utilization or\nmemory, which allows efficient use of fine-grained pipeline parallelism.\nConcretely, when tested on ResNet and Transformer networks, asynchrony enables\nPipeMare to use up to $2.7\\times$ less memory or get $4.3\\times$ higher\npipeline utilization, with similar model quality, when compared to\nstate-of-the-art synchronous PP training techniques.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:20:24 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 01:34:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yang", "Bowen", ""], ["Zhang", "Jian", ""], ["Li", "Jonathan", ""], ["R\u00e9", "Christopher", ""], ["Aberger", "Christopher R.", ""], ["De Sa", "Christopher", ""]]}, {"id": "1910.05132", "submitter": "Pramod Kaushik Mudrakarta", "authors": "Pramod Kaushik Mudrakarta, Shubhendu Trivedi, Risi Kondor", "title": "Asymmetric Multiresolution Matrix Factorization", "comments": "preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Matrix Factorization (MMF) was recently introduced as an\nalternative to the dominant low-rank paradigm in order to capture structure in\nmatrices at multiple different scales. Using ideas from multiresolution\nanalysis (MRA), MMF teased out hierarchical structure in symmetric matrices by\nconstructing a sequence of wavelet bases. While effective for such matrices,\nthere is plenty of data that is more naturally represented as nonsymmetric\nmatrices (e.g. directed graphs), but nevertheless has similar hierarchical\nstructure. In this paper, we explore techniques for extending MMF to any square\nmatrix. We validate our approach on numerous matrix compression tasks,\ndemonstrating its efficacy compared to low-rank methods. Moreover, we also show\nthat a combined low-rank and MMF approach, which amounts to removing a small\nglobal-scale component of the matrix and then extracting hierarchical structure\nfrom the residual, is even more effective than each of the two complementary\nmethods for matrix compression.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:24:03 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mudrakarta", "Pramod Kaushik", ""], ["Trivedi", "Shubhendu", ""], ["Kondor", "Risi", ""]]}, {"id": "1910.05150", "submitter": "Qichen Li", "authors": "Qichen Li, Jiaxin Pei, Jianding Zhang, Bo Han", "title": "SUM: Suboptimal Unitary Multi-task Learning Framework for Spatiotemporal\n  Data Prediction", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical multi-task learning methods for spatio-temporal data prediction\ninvolve low-rank tensor computation. However, such a method have relatively\nweak performance when the task number is small, and we cannot integrate it into\nnon-linear models. In this paper, we propose a two-step suboptimal unitary\nmethod (SUM) to combine a meta-learning strategy into multi-task models. In the\nfirst step, it searches for a global pattern by optimising the general\nparameters with gradient descents under constraints, which is a geological\nregularizer to enable model learning with less training data. In the second\nstep, we derive an optimised model on each specific task from the global\npattern with only a few local training data. Compared with traditional\nmulti-task learning methods, SUM shows advantages of generalisation ability on\ndistant tasks. It can be applied on any multi-task models with the gradient\ndescent as its optimiser regardless if the prediction function is linear or\nnot. Moreover, we can harness the model to enable traditional prediction model\nto make coKriging. The experiments on public datasets have suggested that our\nframework, when combined with current multi-task models, has a conspicuously\nbetter prediction result when the task number is small compared to low-rank\ntensor learning, and our model has a quite satisfying outcome when adjusting\nthe current prediction models for coKriging.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:02:30 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "Qichen", ""], ["Pei", "Jiaxin", ""], ["Zhang", "Jianding", ""], ["Han", "Bo", ""]]}, {"id": "1910.05171", "submitter": "Byeonggeun Kim", "authors": "Byeonggeun Kim, Mingu Lee, Jinkyu Lee, Yeonseok Kim, and Kyuwoong\n  Hwang", "title": "Query-by-example on-device keyword spotting", "comments": "IEEE ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A keyword spotting (KWS) system determines the existence of, usually\npredefined, keyword in a continuous speech stream. This paper presents a\nquery-by-example on-device KWS system which is user-specific. The proposed\nsystem consists of two main steps: query enrollment and testing. In query\nenrollment step, phonetic posteriors are output by a small-footprint automatic\nspeech recognition model based on connectionist temporal classification. Using\nthe phonetic-level posteriorgram, hypothesis graph of finite-state transducer\n(FST) is built, thus can enroll any keywords thus avoiding an out-of-vocabulary\nproblem. In testing, a log-likelihood is scored for input audio using the FST.\nWe propose a threshold prediction method while using the user-specific keyword\nhypothesis only. The system generates query-specific negatives by rearranging\neach query utterance in waveform. The threshold is decided based on the\nenrollment queries and generated negatives. We tested two keywords in English,\nand the proposed work shows promising performance while preserving simplicity.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:28:03 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:37:05 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 01:55:51 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kim", "Byeonggeun", ""], ["Lee", "Mingu", ""], ["Lee", "Jinkyu", ""], ["Kim", "Yeonseok", ""], ["Hwang", "Kyuwoong", ""]]}, {"id": "1910.05173", "submitter": "Ibai Roman", "authors": "Ibai Roman, Roberto Santana, Alexander Mendiburu, Jose A. Lozano", "title": "Evolving Gaussian Process kernels from elementary mathematical\n  expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Choosing the most adequate kernel is crucial in many Machine Learning\napplications. Gaussian Process is a state-of-the-art technique for regression\nand classification that heavily relies on a kernel function. However, in the\nGaussian Process literature, kernels have usually been either ad hoc designed,\nselected from a predefined set, or searched for in a space of compositions of\nkernels which have been defined a priori. In this paper, we propose a\nGenetic-Programming algorithm that represents a kernel function as a tree of\nelementary mathematical expressions. By means of this representation, a wider\nset of kernels can be modeled, where potentially better solutions can be found,\nalthough new challenges also arise. The proposed algorithm is able to overcome\nthese difficulties and find kernels that accurately model the characteristics\nof the data. This method has been tested in several real-world time-series\nextrapolation problems, improving the state-of-the-art results while reducing\nthe complexity of the kernels.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:29:42 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:20:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Roman", "Ibai", ""], ["Santana", "Roberto", ""], ["Mendiburu", "Alexander", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1910.05177", "submitter": "Michael Pradel", "authors": "Yaza Wainakh and Moiz Rauf and Michael Pradel", "title": "IdBench: Evaluating Semantic Representations of Identifier Names in\n  Source Code", "comments": "Accepted as full research paper at International Conference on\n  Software Engineering (ICSE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifier names convey useful information about the intended semantics of\ncode. Name-based program analyses use this information, e.g., to detect bugs,\nto predict types, and to improve the readability of code. At the core of\nname-based analyses are semantic representations of identifiers, e.g., in the\nform of learned embeddings. The high-level goal of such a representation is to\nencode whether two identifiers, e.g., len and size, are semantically similar.\nUnfortunately, it is currently unclear to what extent semantic representations\nmatch the semantic relatedness and similarity perceived by developers. This\npaper presents IdBench, the first benchmark for evaluating semantic\nrepresentations against a ground truth created from thousands of ratings by 500\nsoftware developers. We use IdBench to study state-of-the-art embedding\ntechniques proposed for natural language, an embedding technique specifically\ndesigned for source code, and lexical string distance functions. Our results\nshow that the effectiveness of semantic representations varies significantly\nand that the best available embeddings successfully represent semantic\nrelatedness. On the downside, no existing technique provides a satisfactory\nrepresentation of semantic similarities, among other reasons because\nidentifiers with opposing meanings are incorrectly considered to be similar,\nwhich may lead to fatal mistakes, e.g., in a refactoring tool. Studying the\nstrengths and weaknesses of the different techniques shows that they complement\neach other. As a first step toward exploiting this complementarity, we present\nan ensemble model that combines existing techniques and that clearly\noutperforms the best available semantic representation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:34:30 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:07:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wainakh", "Yaza", ""], ["Rauf", "Moiz", ""], ["Pradel", "Michael", ""]]}, {"id": "1910.05199", "submitter": "Massimiliano Patacchiola PhD", "authors": "Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels", "comments": "Advances in Neural Information Processing Systems (NeurIPS 2020,\n  Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, different machine learning methods have been introduced to tackle\nthe challenging few-shot learning scenario that is, learning from a small\nlabeled dataset related to a specific task. Common approaches have taken the\nform of meta-learning: learning to learn on the new problem given the old.\nFollowing the recognition that meta-learning is implementing learning in a\nmulti-level model, we present a Bayesian treatment for the meta-learning inner\nloop through the use of deep kernels. As a result we can learn a kernel that\ntransfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach\nhas many advantages: is straightforward to implement as a single optimizer,\nprovides uncertainty quantification, and does not require estimation of\ntask-specific parameters. We empirically demonstrate that DKT outperforms\nseveral state-of-the-art algorithms in few-shot classification, and is the\nstate of the art for cross-domain adaptation and regression. We conclude that\ncomplex meta-learning routines can be replaced by a simpler Bayesian model\nwithout loss of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:06:39 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 14:33:51 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 16:03:19 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 14:51:41 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Patacchiola", "Massimiliano", ""], ["Turner", "Jack", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "1910.05206", "submitter": "Victor Coscrato", "authors": "Victor Coscrato, Marco Henrique de Almeida In\\'acio, Tiago Botari,\n  Rafael Izbicki", "title": "NLS: an accurate and yet easy-to-interpret regression method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important feature of successful supervised machine learning applications\nis to be able to explain the predictions given by the regression or\nclassification model being used. However, most state-of-the-art models that\nhave good predictive power lead to predictions that are hard to interpret.\nThus, several model-agnostic interpreters have been developed recently as a way\nof explaining black-box classifiers. In practice, using these methods is a slow\nprocess because a novel fitting is required for each new testing instance, and\nseveral non-trivial choices must be made. We develop NLS (neural local\nsmoother), a method that is complex enough to give good predictions, and yet\ngives solutions that are easy to be interpreted without the need of using a\nseparate interpreter. The key idea is to use a neural network that imposes a\nlocal linear shape to the output layer. We show that NLS leads to predictive\npower that is comparable to state-of-the-art machine learning models, and yet\nis easier to interpret.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:15:52 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Coscrato", "Victor", ""], ["In\u00e1cio", "Marco Henrique de Almeida", ""], ["Botari", "Tiago", ""], ["Izbicki", "Rafael", ""]]}, {"id": "1910.05231", "submitter": "Aleksandar Stanic", "authors": "Aleksandar Stani\\'c and J\\\"urgen Schmidhuber", "title": "R-SQAIR: Relational Sequential Attend, Infer, Repeat", "comments": "4 page workshop paper accepted at the NeurIPS 2019 Workshop on\n  Perception as Generative Reasoning: Structure, Causality, Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional sequential multi-object attention models rely on a recurrent\nmechanism to infer object relations. We propose a relational extension\n(R-SQAIR) of one such attention model (SQAIR) by endowing it with a module with\nstrong relational inductive bias that computes in parallel pairwise\ninteractions between inferred objects. Two recently proposed relational modules\nare studied on tasks of unsupervised learning from videos. We demonstrate gains\nover sequential relational mechanisms, also in terms of combinatorial\ngeneralization.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:02:34 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Stani\u0107", "Aleksandar", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1910.05245", "submitter": "Asier Mujika", "authors": "Asier Mujika and Felix Weissenberger and Angelika Steger", "title": "Decoupling Hierarchical Recurrent Neural Networks With Locally\n  Computable Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long-term dependencies is a key long-standing challenge of recurrent\nneural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have\nbeen considered a promising approach as long-term dependencies are resolved\nthrough shortcuts up and down the hierarchy. Yet, the memory requirements of\nTruncated Backpropagation Through Time (TBPTT) still prevent training them on\nvery long sequences. In this paper, we empirically show that in (deep) HRNNs,\npropagating gradients back from higher to lower levels can be replaced by\nlocally computable losses, without harming the learning capability of the\nnetwork, over a wide range of tasks. This decoupling by local losses reduces\nthe memory requirements of training by a factor exponential in the depth of the\nhierarchy in comparison to standard TBPTT.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:25:28 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mujika", "Asier", ""], ["Weissenberger", "Felix", ""], ["Steger", "Angelika", ""]]}, {"id": "1910.05250", "submitter": "Changying Du", "authors": "Changying Du, Jia He, Changde Du, Fuzhen Zhuang, Qing He and Guoping\n  Long", "title": "Efficient and Adaptive Kernelization for Nonlinear Max-margin Multi-view\n  Learning", "comments": "Multi-view learning, Adaptive kernel, Maximum margin learning, Linear\n  scalability, Dirichlet process Gaussian mixtures, Bayesian inference, Data\n  augmentation, Hamiltonian Monte Carlo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-view learning methods based on kernel function either require\nthe user to select and tune a single predefined kernel or have to compute and\nstore many Gram matrices to perform multiple kernel learning. Apart from the\nhuge consumption of manpower, computation and memory resources, most of these\nmodels seek point estimation of their parameters, and are prone to overfitting\nto small training data. This paper presents an adaptive kernel nonlinear\nmax-margin multi-view learning model under the Bayesian framework.\nSpecifically, we regularize the posterior of an efficient multi-view latent\nvariable model by explicitly mapping the latent representations extracted from\nmultiple data views to a random Fourier feature space where max-margin\nclassification constraints are imposed. Assuming these random features are\ndrawn from Dirichlet process Gaussian mixtures, we can adaptively learn\nshift-invariant kernels from data according to Bochners theorem. For inference,\nwe employ the data augmentation idea for hinge loss, and design an efficient\ngradient-based MCMC sampler in the augmented space. Having no need to compute\nthe Gram matrix, our algorithm scales linearly with the size of training set.\nExtensive experiments on real-world datasets demonstrate that our method has\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:32:20 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Du", "Changying", ""], ["He", "Jia", ""], ["Du", "Changde", ""], ["Zhuang", "Fuzhen", ""], ["He", "Qing", ""], ["Long", "Guoping", ""]]}, {"id": "1910.05270", "submitter": "Aryeh Kontorovich", "authors": "Klim Efremenko, Aryeh Kontorovich, Moshe Noivirt", "title": "Fast and Bayes-consistent nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on nearest-neighbor methods tends to focus somewhat dichotomously\neither on the statistical or the computational aspects -- either on, say, Bayes\nconsistency and rates of convergence or on techniques for speeding up the\nproximity search. This paper aims at bridging these realms: to reap the\nadvantages of fast evaluation time while maintaining Bayes consistency, and\nfurther without sacrificing too much in the risk decay rate. We combine the\nlocality-sensitive hashing (LSH) technique with a novel missing-mass argument\nto obtain a fast and Bayes-consistent classifier. Our algorithm's prediction\nruntime compares favorably against state of the art approximate NN methods,\nwhile maintaining Bayes-consistency and attaining rates comparable to minimax.\nOn samples of size $n$ in $\\R^d$, our pre-processing phase has runtime $O(d n\n\\log n)$, while the evaluation phase has runtime $O(d\\log n)$ per query point.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:46:37 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 21:15:47 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 21:46:51 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Efremenko", "Klim", ""], ["Kontorovich", "Aryeh", ""], ["Noivirt", "Moshe", ""]]}, {"id": "1910.05271", "submitter": "Elena Kalinina", "authors": "Elena Kalinina, Fabian Pedregosa, Vittorio Iacovella, Emanuele\n  Olivetti, Paolo Avesani", "title": "A Test for Shared Patterns in Cross-modal Brain Activation Analysis", "comments": "5 figures, tables after References (as required by SciRep template)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the extent to which different cognitive modalities (understood\nhere as the set of cognitive processes underlying the elaboration of a stimulus\nby the brain) rely on overlapping neural representations is a fundamental issue\nin cognitive neuroscience. In the last decade, the identification of shared\nactivity patterns has been mostly framed as a supervised learning problem. For\ninstance, a classifier is trained to discriminate categories (e.g. faces vs.\nhouses) in modality I (e.g. perception) and tested on the same categories in\nmodality II (e.g. imagery). This type of analysis is often referred to as\ncross-modal decoding. In this paper we take a different approach and instead\nformulate the problem of assessing shared patterns across modalities within the\nframework of statistical hypothesis testing. We propose both an appropriate\ntest statistic and a scheme based on permutation testing to compute the\nsignificance of this test while making only minimal distributional assumption.\nWe denote this test cross-modal permutation test (CMPT). We also provide\nempirical evidence on synthetic datasets that our approach has greater\nstatistical power than the cross-modal decoding method while maintaining low\nType I errors (rejecting a true null hypothesis). We compare both approaches on\nan fMRI dataset with three different cognitive modalities (perception, imagery,\nvisual search). Finally, we show how CMPT can be combined with Searchlight\nanalysis to explore spatial distribution of shared activity patterns.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:33:49 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kalinina", "Elena", ""], ["Pedregosa", "Fabian", ""], ["Iacovella", "Vittorio", ""], ["Olivetti", "Emanuele", ""], ["Avesani", "Paolo", ""]]}, {"id": "1910.05299", "submitter": "Awni Hannun", "authors": "Awni Hannun, Brian Knott, Shubho Sengupta, Laurens van der Maaten", "title": "Privacy-Preserving Multi-Party Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits are online learners that, given an input, select an arm\nand receive a reward for that arm. They use the reward as a learning signal and\naim to maximize the total reward over the inputs. Contextual bandits are\ncommonly used to solve recommendation or ranking problems. This paper considers\na learning setting in which multiple parties aim to train a contextual bandit\ntogether in a private way: the parties aim to maximize the total reward but do\nnot want to share any of the relevant information they possess with the other\nparties. Specifically, multiple parties have access to (different) features\nthat may benefit the learner but that cannot be shared with other parties. One\nof the parties pulls the arm but other parties may not learn which arm was\npulled. One party receives the reward but the other parties may not learn the\nreward value. This paper develops a privacy-preserving multi-party contextual\nbandit for this learning setting by combining secure multi-party computation\nwith a differentially private mechanism based on epsilon-greedy exploration.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:48:17 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 09:59:47 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 15:30:00 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Hannun", "Awni", ""], ["Knott", "Brian", ""], ["Sengupta", "Shubho", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1910.05303", "submitter": "Yulun Jiang", "authors": "Yulun Jiang, Lei Yu, Haijian Zhang, Zhou Liu", "title": "Learning Cluster Structured Sparsity by Reweighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the paradigm of unfolding iterative algorithms into finite-length\nfeed-forward neural networks has achieved a great success in the area of sparse\nrecovery. Benefit from available training data, the learned networks have\nachieved state-of-the-art performance in respect of both speed and accuracy.\nHowever, the structure behind sparsity, imposing constraint on the support of\nsparse signals, is often an essential prior knowledge but seldom considered in\nthe existing networks. In this paper, we aim at bridging this gap.\nSpecifically, exploiting the iterative reweighted $\\ell_1$ minimization (IRL1)\nalgorithm, we propose to learn the cluster structured sparsity (CSS) by\nrewegihting adaptively. In particular, we first unfold the Reweighted Iterative\nShrinkage Algorithm (RwISTA) into an end-to-end trainable deep architecture\ntermed as RW-LISTA. Then instead of the element-wise reweighting, the global\nand local reweighting manner are proposed for the cluster structured sparse\nlearning. Numerical experiments further show the superiority of our algorithm\nagainst both classical algorithms and learning-based networks on different\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:56:36 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Jiang", "Yulun", ""], ["Yu", "Lei", ""], ["Zhang", "Haijian", ""], ["Liu", "Zhou", ""]]}, {"id": "1910.05305", "submitter": "Faris B Mismar", "authors": "Faris B. Mismar, Ahmad AlAmmouri, Ahmed Alkhateeb, Jeffrey G. Andrews,\n  Brian L. Evans", "title": "Deep Learning Predictive Band Switching in Wireless Networks", "comments": "31 pages, 15 figures, revised and resubmitted to IEEE Transactions on\n  Wireless Communications on October 2, 2019, March 9, 2020, July 2, 2020, and\n  September 1, 2020", "journal-ref": "2020 IEEE Transactions on Wireless Communications", "doi": "10.1109/TWC.2020.3023397", "report-no": null, "categories": "cs.NI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cellular systems, the user equipment (UE) can request a change in the\nfrequency band when its rate drops below a threshold on the current band. The\nUE is then instructed by the base station (BS) to measure the quality of\ncandidate bands, which requires a measurement gap in the data transmission,\nthus lowering the data rate. We propose an online-learning based band switching\napproach that does not require any measurement gap. Our proposed\nclassifier-based band switching policy instead exploits spatial and spectral\ncorrelation between radio frequency signals in different bands based on\nknowledge of the UE location. We focus on switching between a lower (e.g., 3.5\nGHz) band and a millimeter wave band (e.g., 28 GHz), and design and evaluate\ntwo classification models that are trained on a ray-tracing dataset. A key\ninsight is that measurement gaps are overkill, in that only the relative order\nof the bands is necessary for band selection, rather than a full channel\nestimate. Our proposed machine learning based policies achieve roughly 30%\nimprovement in mean effective rates over those of the industry standard policy,\nwhile achieving misclassification errors well below 0.5% and maintaining\nresilience against blockage uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:44:50 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 16:47:07 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 19:08:43 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 14:48:19 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mismar", "Faris B.", ""], ["AlAmmouri", "Ahmad", ""], ["Alkhateeb", "Ahmed", ""], ["Andrews", "Jeffrey G.", ""], ["Evans", "Brian L.", ""]]}, {"id": "1910.05308", "submitter": "Mahadesh Panju", "authors": "Ramkumar Raghu, Pratheek Upadhyaya, Mahadesh Panju, Vaneet Aggarwal,\n  Vinod Sharma", "title": "Deep Reinforcement Learning Based Power control for Wireless Multicast\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multicast scheme recently proposed for a wireless downlink in\n[1]. It was shown earlier that power control can significantly improve its\nperformance. However for this system, obtaining optimal power control is\nintractable because of a very large state space. Therefore in this paper we use\ndeep reinforcement learning where we use function approximation of the\nQ-function via a deep neural network. We show that optimal power control can be\nlearnt for reasonably large systems via this approach. The average power\nconstraint is ensured via a Lagrange multiplier, which is also learnt. Finally,\nwe demonstrate that a slight modification of the learning algorithm allows the\noptimal control to track the time varying system statistics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 08:08:09 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 01:12:09 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Raghu", "Ramkumar", ""], ["Upadhyaya", "Pratheek", ""], ["Panju", "Mahadesh", ""], ["Aggarwal", "Vaneet", ""], ["Sharma", "Vinod", ""]]}, {"id": "1910.05318", "submitter": "Dimitrios Kollias", "authors": "Mengyao Liu and Dimitrios Kollias", "title": "Aff-Wild Database and AffWildNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of HCI, building an automatic system to recognize affect of\nhuman facial expression in real-world condition is very crucial to make machine\ninteract naturallisticaly with a man. However, existing facial emotion\ndatabases usually contain expression in the limited scenario under\nwell-controlled condition. Aff-Wild is currently the largest database\nconsisting of spontaneous facial expression in the wild annotated with valence\nand arousal. The first contribution of this project is the completion of\nextending Aff-Wild database which is fulfilled by collecting videos from\nYouTube on which the videos have spontaneous facial expressions in the wild,\nannotating videos with valence and arousal ranging in [-1,1], detecting faces\nin frames using FFLD2 detector and partitioning the whole data set into train,\nvalidate and test set, with 527056, 94223 and 135145 frames. The diversity is\nguaranteed regarding age, ethnicity and values of valence and arousal. The\nratio of male to female is close to 1. Regarding the techniques used to build\nthe automatic system, deep learning is outstanding since almost all winning\nmethods in emotion challenges adopt DNN techniques. The second contribution of\nthis project is that an end-to-end DNN is constructed to have joint CNN and RNN\nblock and gives the estimation on valence and arousal for each frame in\nsequential data. VGGFace, ResNet, DenseNet with the corresponding pre-trained\nmodel for CNN block and LSTM, GRU, IndRNN, Attention mechanism for RNN block\nare experimented aiming to find the best combination. Fine tuning and transfer\nlearning techniques are also tried out. By comparing the CCC evaluation value\non test data, the best model is found to be pre-trained VGGFace connected with\n2 layers GRU with attention mechanism. The models test performance is 0.555 CCC\nfor valence with sequence length 80 and 0.499 CCC for arousal with sequence\nlength 70.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 17:24:45 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:42:52 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Mengyao", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05321", "submitter": "Jack Goetz", "authors": "Jack Goetz, Ambuj Tewari", "title": "Not All are Made Equal: Consistency of Weighted Averaging Estimators\n  Under Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning seeks to build the best possible model with a budget of\nlabelled data by sequentially selecting the next point to label. However the\ntraining set is no longer \\textit{iid}, violating the conditions required by\nexisting consistency results. Inspired by the success of Stone's Theorem we aim\nto regain consistency for weighted averaging estimators under active learning.\nBased on ideas in \\citet{dasgupta2012consistency}, our approach is to enforce a\nsmall amount of random sampling by running an augmented version of the\nunderlying active learning algorithm. We generalize Stone's Theorem in the\nnoise free setting, proving consistency for well known classifiers such as\n$k$-NN, histogram and kernel estimators under conditions which mirror classical\nresults. However in the presence of noise we can no longer deal with these\nestimators in a unified manner; for some satisfying this condition also\nguarantees sufficiency in the noisy case, while for others we can achieve near\nperfect inconsistency while this condition holds. Finally we provide conditions\nfor consistency in the presence of noise, which give insight into why these\nestimators can behave so differently under the combination of noise and active\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 17:38:00 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Goetz", "Jack", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1910.05338", "submitter": "Minh Vu", "authors": "Minh H. Vu, Tufve Nyholm, Tommy L\\\"ofstedt", "title": "TuNet: End-to-end Hierarchical Brain Tumor Segmentation using Cascaded\n  Networks", "comments": "Accepted at MICCAI BrainLes 2019", "journal-ref": null, "doi": "10.1007/978-3-030-46640-4_17", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Glioma is one of the most common types of brain tumors; it arises in the\nglial cells in the human brain and in the spinal cord. In addition to having a\nhigh mortality rate, glioma treatment is also very expensive. Hence, automatic\nand accurate segmentation and measurement from the early stages are critical in\norder to prolong the survival rates of the patients and to reduce the costs of\nthe treatment. In the present work, we propose a novel end-to-end cascaded\nnetwork for semantic segmentation that utilizes the hierarchical structure of\nthe tumor sub-regions with ResNet-like blocks and Squeeze-and-Excitation\nmodules after each convolution and concatenation block. By utilizing\ncross-validation, an average ensemble technique, and a simple post-processing\ntechnique, we obtained dice scores of 88.06, 80.84, and 80.29, and Hausdorff\nDistances (95th percentile) of 6.10, 5.17, and 2.21 for the whole tumor, tumor\ncore, and enhancing tumor, respectively, on the online test set.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:02:58 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:52:11 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 06:43:53 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Vu", "Minh H.", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "1910.05366", "submitter": "Tonghan Wang", "authors": "Tonghan Wang, Jianhao Wang, Chongyi Zheng, Chongjie Zhang", "title": "Learning Nearly Decomposable Value Functions Via Communication\n  Minimization", "comments": "8th International Conference on Learning Representations (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning encounters major challenges in multi-agent settings,\nsuch as scalability and non-stationarity. Recently, value function\nfactorization learning emerges as a promising way to address these challenges\nin collaborative multi-agent systems. However, existing methods have been\nfocusing on learning fully decentralized value functions, which are not\nefficient for tasks requiring communication. To address this limitation, this\npaper presents a novel framework for learning nearly decomposable Q-functions\n(NDQ) via communication minimization, with which agents act on their own most\nof the time but occasionally send messages to other agents in order for\neffective coordination. This framework hybridizes value function factorization\nlearning and communication learning by introducing two information-theoretic\nregularizers. These regularizers are maximizing mutual information between\nagents' action selection and communication messages while minimizing the\nentropy of messages between agents. We show how to optimize these regularizers\nin a way that is easily integrated with existing value function factorization\nmethods such as QMIX. Finally, we demonstrate that, on the StarCraft unit\nmicromanagement benchmark, our framework significantly outperforms baseline\nmethods and allows us to cut off more than $80\\%$ of communication without\nsacrificing the performance. The videos of our experiments are available at\nhttps://sites.google.com/view/ndq.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:31:15 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 01:30:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Tonghan", ""], ["Wang", "Jianhao", ""], ["Zheng", "Chongyi", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1910.05376", "submitter": "Dimitrios Kollias", "authors": "Alvertos Benroumpi and Dimitrios Kollias", "title": "AffWild Net and Aff-Wild Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions recognition is the task of recognizing people's emotions. Usually it\nis achieved by analyzing expression of peoples faces. There are two ways for\nrepresenting emotions: The categorical approach and the dimensional approach by\nusing valence and arousal values. Valence shows how negative or positive an\nemotion is and arousal shows how much it is activated. Recent deep learning\nmodels, that have to do with emotions recognition, are using the second\napproach, valence and arousal. Moreover, a more interesting concept, which is\nuseful in real life is the \"in the wild\" emotions recognition. \"In the wild\"\nmeans that the images analyzed for the recognition task, come from from real\nlife sources(online videos, online photos, etc.) and not from staged\nexperiments. So, they introduce unpredictable situations in the images, that\nhave to be modeled. The purpose of this project is to study the previous work\nthat was done for the \"in the wild\" emotions recognition concept, design a new\ndataset which has as a standard the \"Aff-wild\" database, implement new deep\nlearning models and evaluate the results. First, already existing databases and\ndeep learning models are presented. Then, inspired by them a new database is\ncreated which includes 507.208 frames in total from 106 videos, which were\ngathered from online sources. Then, the data are tested in a CNN model based on\nCNN-M architecture, in order to be sure about their usability. Next, the main\nmodel of this project is implemented. That is a Regression GAN which can\nexecute unsupervised and supervised learning at the same time. More\nspecifically, it keeps the main functionality of GANs, which is to produce fake\nimages that look as good as the real ones, while it can also predict valence\nand arousal values for both real and fake images. Finally, the database created\nearlier is applied to this model and the results are presented and evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:58:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Benroumpi", "Alvertos", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05384", "submitter": "Yinsong Wang", "authors": "Yinsong Wang and Shahin Shahrampour", "title": "A General Scoring Rule for Randomized Kernel Approximation with\n  Application to Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random features has been widely used for kernel approximation in large-scale\nmachine learning. A number of recent studies have explored data-dependent\nsampling of features, modifying the stochastic oracle from which random\nfeatures are sampled. While proposed techniques in this realm improve the\napproximation, their application is limited to a specific learning task. In\nthis paper, we propose a general scoring rule for sampling random features,\nwhich can be employed for various applications with some adjustments. We first\nobserve that our method can recover a number of data-dependent sampling methods\n(e.g., leverage scores and energy-based sampling). Then, we restrict our\nattention to a ubiquitous problem in statistics and machine learning, namely\nCanonical Correlation Analysis (CCA). We provide a principled guide for finding\nthe distribution maximizing the canonical correlations, resulting in a novel\ndata-dependent method for sampling features. Numerical experiments verify that\nour algorithm consistently outperforms other sampling techniques in the CCA\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:50:00 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 05:24:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Yinsong", ""], ["Shahrampour", "Shahin", ""]]}, {"id": "1910.05387", "submitter": "Amanda Gentzel", "authors": "Amanda Gentzel, Dan Garant, and David Jensen", "title": "The Case for Evaluating Causal Models Using Interventional Measures and\n  Empirical Data", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference is central to many areas of artificial intelligence,\nincluding complex reasoning, planning, knowledge-base construction, robotics,\nexplanation, and fairness. An active community of researchers develops and\nenhances algorithms that learn causal models from data, and this work has\nproduced a series of impressive technical advances. However, evaluation\ntechniques for causal modeling algorithms have remained somewhat primitive,\nlimiting what we can learn from experimental studies of algorithm performance,\nconstraining the types of algorithms and model representations that researchers\nconsider, and creating a gap between theory and practice. We argue for more\nfrequent use of evaluation techniques that examine interventional measures\nrather than structural or observational measures, and that evaluate those\nmeasures on empirical data rather than synthetic data. We survey the current\npractice in evaluation and show that the techniques we recommend are rarely\nused in practice. We show that such techniques are feasible and that data sets\nare available to conduct such evaluations. We also show that these techniques\nproduce substantially different results than using structural measures and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:54:30 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 21:22:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Gentzel", "Amanda", ""], ["Garant", "Dan", ""], ["Jensen", "David", ""]]}, {"id": "1910.05396", "submitter": "Kimin Lee", "authors": "Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee", "title": "Network Randomization: A Simple Technique for Generalization in Deep\n  Reinforcement Learning", "comments": "Accepted in ICLR 2020 and NeurIPS Workshop on Deep RL 2019 / First\n  two authors are equally contributed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) agents often fail to generalize to unseen\nenvironments (yet semantically similar to trained agents), particularly when\nthey are trained on high-dimensional state spaces, such as images. In this\npaper, we propose a simple technique to improve a generalization ability of\ndeep RL agents by introducing a randomized (convolutional) neural network that\nrandomly perturbs input observations. It enables trained agents to adapt to new\ndomains by learning robust features invariant across varied and randomized\nenvironments. Furthermore, we consider an inference method based on the Monte\nCarlo approximation to reduce the variance induced by this randomization. We\ndemonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab\nexploration and 3D robotics control tasks: it significantly outperforms various\nregularization and data augmentation methods for the same purpose.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:12:52 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 07:44:13 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 08:29:25 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lee", "Kimin", ""], ["Lee", "Kibok", ""], ["Shin", "Jinwoo", ""], ["Lee", "Honglak", ""]]}, {"id": "1910.05399", "submitter": "Konstantinos Slavakis", "authors": "Konstantinos Slavakis and Sinjini Banerjee", "title": "Robust Hierarchical-Optimization RLS Against Sparse Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper fortifies the recently introduced hierarchical-optimization\nrecursive least squares (HO-RLS) against outliers which contaminate\ninfrequently linear-regression models. Outliers are modeled as nuisance\nvariables and are estimated together with the linear filter/system variables\nvia a sparsity-inducing (non-)convexly regularized least-squares task. The\nproposed outlier-robust HO-RLS builds on steepest-descent directions with a\nconstant step size (learning rate), needs no matrix inversion (lemma),\naccommodates colored nominal noise of known correlation matrix, exhibits small\ncomputational footprint, and offers theoretical guarantees, in a probabilistic\nsense, for the convergence of the system estimates to the solutions of a\nhierarchical-optimization problem: Minimize a convex loss, which models\na-priori knowledge about the unknown system, over the minimizers of the\nclassical ensemble LS loss. Extensive numerical tests on synthetically\ngenerated data in both stationary and non-stationary scenarios showcase notable\nimprovements of the proposed scheme over state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:27:07 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Slavakis", "Konstantinos", ""], ["Banerjee", "Sinjini", ""]]}, {"id": "1910.05405", "submitter": "Shuhang Chen", "authors": "Shuhang Chen, Adithya M. Devraj, Fan Lu, Ana Bu\\v{s}i\\'c, Sean P. Meyn", "title": "Zap Q-Learning With Nonlinear Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zap Q-learning is a recent class of reinforcement learning algorithms,\nmotivated primarily as a means to accelerate convergence. Stability theory has\nbeen absent outside of two restrictive classes: the tabular setting, and\noptimal stopping. This paper introduces a new framework for analysis of a more\ngeneral class of recursive algorithms known as stochastic approximation. Based\non this general theory, it is shown that Zap Q-learning is consistent under a\nnon-degeneracy assumption, even when the function approximation architecture is\nnonlinear. Zap Q-learning with neural network function approximation emerges as\na special case, and is tested on examples from OpenAI Gym. Based on multiple\nexperiments with a range of neural network sizes, it is found that the new\nalgorithms converge quickly and are robust to choice of function approximation\narchitecture.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:48:09 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 20:02:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Shuhang", ""], ["Devraj", "Adithya M.", ""], ["Lu", "Fan", ""], ["Bu\u0161i\u0107", "Ana", ""], ["Meyn", "Sean P.", ""]]}, {"id": "1910.05411", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Jakob Grahn, Markus Eckerstorfer, Eirik Malnes,\n  Hannah Vickers", "title": "Snow avalanche segmentation in SAR images with Fully Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about frequency and location of snow avalanche activity is\nessential for forecasting and mapping of snow avalanche hazard. Traditional\nfield monitoring of avalanche activity has limitations, especially when\nsurveying large and remote areas. In recent years, avalanche detection in\nSentinel-1 radar satellite imagery has been developed to improve monitoring.\nHowever, the current state-of-the-art detection algorithms, based on radar\nsignal processing techniques, are still much less accurate than human experts.\nTo reduce this gap, we propose a deep learning architecture for detecting\navalanches in Sentinel-1 radar images. We trained a neural network on 6,345\nmanually labelled avalanches from 117 Sentinel-1 images, each one consisting of\nsix channels that include backscatter and topographical information. Then, we\ntested our trained model on a new SAR image. Comparing to the manual labelling\n(the gold standard), we achieved an F1 score above 66\\%, while the\nstate-of-the-art detection algorithm sits at an F1 score of only 38\\%. A visual\ninspection of the results generated by our deep learning model shows that only\nsmall avalanches are undetected, while some avalanches that were originally not\nlabelled by the human expert are discovered.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:02:57 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 13:00:56 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Grahn", "Jakob", ""], ["Eckerstorfer", "Markus", ""], ["Malnes", "Eirik", ""], ["Vickers", "Hannah", ""]]}, {"id": "1910.05421", "submitter": "Amine Remita", "authors": "Amine M. Remita and Abdoulaye Banir\\'e Diallo", "title": "Statistical Linear Models in Virus Genomic Alignment-free\n  Classification: Application to Hepatitis C Viruses", "comments": "Accepted as a regular paper for publication in IEEE BIBM 2019\n  (Camera-ready version + Supplemental material)", "journal-ref": "2019 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM), San Diego, CA, USA, 2019, pp. 474-481", "doi": "10.1109/BIBM47256.2019.8983375", "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viral sequence classification is an important task in pathogen detection,\nepidemiological surveys and evolutionary studies. Statistical learning methods\nare widely used to classify and identify viral sequences in samples from\nenvironments. These methods face several challenges associated with the nature\nand properties of viral genomes such as recombination, mutation rate and\ndiversity. Also, new generations of sequencing technologies rise other\ndifficulties by generating massive amounts of fragmented sequences. While\nlinear classifiers are often used to classify viruses, there is a lack of\nexploration of the accuracy space of existing models in the context of\nalignment free approaches. In this study, we present an exhaustive assessment\nprocedure exploring the power of linear classifiers in genotyping and subtyping\npartial and complete genomes. It is applied to the Hepatitis C viruses (HCV).\nSeveral variables are considered in this investigation such as classifier types\n(generative and discriminative) and their hyper-parameters (smoothing value and\nregularization penalty function), the classification task (genotyping and\nsubtyping), the length of the tested sequences (partial and complete) and the\nlength of k-mer words. Overall, several classifiers perform well given a set of\nprecise combination of the experimental variables mentioned above. Finally, we\nprovide the procedure and benchmark data to allow for more robust assessment of\nclassification from virus genomes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:40:24 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 18:23:53 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Remita", "Amine M.", ""], ["Diallo", "Abdoulaye Banir\u00e9", ""]]}, {"id": "1910.05422", "submitter": "Cenk Baykal", "authors": "Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman,\n  Daniela Rus", "title": "SiPPing Neural Networks: Sensitivity-informed Provable Pruning of Neural\n  Networks", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a pruning algorithm that provably sparsifies the parameters of a\ntrained model in a way that approximately preserves the model's predictive\naccuracy. Our algorithm uses a small batch of input points to construct a\ndata-informed importance sampling distribution over the network's parameters,\nand adaptively mixes a sampling-based and deterministic pruning procedure to\ndiscard redundant weights. Our pruning method is simultaneously computationally\nefficient, provably accurate, and broadly applicable to various network\narchitectures and data distributions. Our empirical comparisons show that our\nalgorithm reliably generates highly compressed networks that incur minimal loss\nin performance relative to that of the original network. We present\nexperimental results that demonstrate our algorithm's potential to unearth\nessential network connections that can be trained successfully in isolation,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:40:59 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 01:03:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baykal", "Cenk", ""], ["Liebenwein", "Lucas", ""], ["Gilitschenski", "Igor", ""], ["Feldman", "Dan", ""], ["Rus", "Daniela", ""]]}, {"id": "1910.05425", "submitter": "Mostafa Karimi", "authors": "Mostafa Karimi, Gopalkrishna Veni, Yen-Yun Yu", "title": "Illegible Text to Readable Text: An Image-to-Image Transformation using\n  Conditional Sliced Wasserstein Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text recognition from ancient handwritten record images is an\nimportant problem in the genealogy domain. However, critical challenges such as\nvarying noise conditions, vanishing texts, and variations in handwriting make\nthe recognition task difficult. We tackle this problem by developing a\nhandwritten-to-machine-print conditional Generative Adversarial network\n(HW2MP-GAN) model that formulates handwritten recognition as a\ntext-Image-to-text-Image translation problem where a given image, typically in\nan illegible form, is converted into another image, close to its machine-print\nform. The proposed model consists of three-components including a generator,\nand word-level and character-level discriminators. The model incorporates\nSliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for\nbetter quality image-to-image transformation. Our experiments reveal that\nHW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in\nFrechet Handwritten Distance (FHD), 0.6 on average Levenshtein distance and 39%\nin word accuracy for image-to-image translation on IAM database. Further,\nHW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to\nbaseline handwritten recognition models on the IAM database.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 22:01:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Karimi", "Mostafa", ""], ["Veni", "Gopalkrishna", ""], ["Yu", "Yen-Yun", ""]]}, {"id": "1910.05429", "submitter": "Buse Gul Atli", "authors": "Buse Gul Atli, Sebastian Szyller, Mika Juuti, Samuel Marchal, N.\n  Asokan", "title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?", "comments": "16 pages, 1 figure, Accepted for publication in AAAI-20 Workshop on\n  Engineering Dependable and Secure Machine Learning Systems (AAAI-EDSMLS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning (ML) has introduced advanced solutions to many\ndomains. Since ML models provide business advantage to model owners, protecting\nintellectual property of ML models has emerged as an important consideration.\nConfidentiality of ML models can be protected by exposing them to clients only\nvia prediction APIs. However, model extraction attacks can steal the\nfunctionality of ML models using the information leaked to clients through the\nresults returned via the API. In this work, we question whether model\nextraction is a serious threat to complex, real-life ML models. We evaluate the\ncurrent state-of-the-art model extraction attack (Knockoff nets) against\ncomplex models. We reproduce and confirm the results in the original paper. But\nwe also show that the performance of this attack can be limited by several\nfactors, including ML model architecture and the granularity of API response.\nFurthermore, we introduce a defense based on distinguishing queries used for\nKnockoff nets from benign queries. Despite the limitations of the Knockoff\nnets, we show that a more realistic adversary can effectively steal complex ML\nmodels and evade known defenses.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 22:25:11 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 21:53:39 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 09:19:08 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Atli", "Buse Gul", ""], ["Szyller", "Sebastian", ""], ["Juuti", "Mika", ""], ["Marchal", "Samuel", ""], ["Asokan", "N.", ""]]}, {"id": "1910.05437", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Roweis Discriminant Analysis: A Generalized Subspace Learning Method", "comments": "This is the paper for the methods Roweis Discriminant Analysis (RDA),\n  dual RDA, kernel RDA, and Roweisfaces. This is in memory of Sam Roweis (rest\n  in peace) to whom subspace and manifold learning owes significantly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method which generalizes subspace learning based on\neigenvalue and generalized eigenvalue problems. This method, Roweis\nDiscriminant Analysis (RDA), is named after Sam Roweis to whom the field of\nsubspace learning owes significantly. RDA is a family of infinite number of\nalgorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and\nFisher Discriminant Analysis (FDA) are special cases. One of the extreme\nspecial cases, which we name Double Supervised Discriminant Analysis (DSDA),\nuses the labels twice; it is novel and has not appeared elsewhere. We propose a\ndual for RDA for some special cases. We also propose kernel RDA, generalizing\nkernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation\ntheory. Our theoretical analysis explains previously known facts such as why\nSPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA\ndoes not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does\nnot, and why PCA is the best linear method for reconstruction. Roweisfaces and\nkernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces,\nsupervised eigenfaces, and their kernel variants. We also report experiments\nshowing the effectiveness of RDA and kernel RDA on some benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:14:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "1910.05438", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn, Ilya Shpitser, and Eric J. Tchetgen Tchetgen", "title": "Comment on \"Blessings of Multiple Causes\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (This comment has been updated to respond to Wang and Blei's rejoinder\n[arXiv:1910.07320].)\n  The premise of the deconfounder method proposed in \"Blessings of Multiple\nCauses\" by Wang and Blei [arXiv:1805.06826], namely that a variable that\nrenders multiple causes conditionally independent also controls for unmeasured\nmulti-cause confounding, is incorrect. This can be seen by noting that no fact\nabout the observed data alone can be informative about ignorability, since\nignorability is compatible with any observed data distribution. Methods to\ncontrol for unmeasured confounding may be valid with additional assumptions in\nspecific settings, but they cannot, in general, provide a checkable approach to\ncausal inference, and they do not, in general, require weaker assumptions than\nthe assumptions that are commonly used for causal inference. While this is\noutside the scope of this comment, we note that much recent work on applying\nideas from latent variable modeling to causal inference problems suffers from\nsimilar issues.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:19:45 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:08:22 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 16:20:09 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1910.05446", "submitter": "Dami Choi", "authors": "Dami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J.\n  Maddison, George E. Dahl", "title": "On Empirical Comparisons of Optimizers for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an optimizer is a central step in the contemporary deep learning\npipeline. In this paper, we demonstrate the sensitivity of optimizer\ncomparisons to the hyperparameter tuning protocol. Our findings suggest that\nthe hyperparameter search space may be the single most important factor\nexplaining the rankings obtained by recent empirical comparisons in the\nliterature. In fact, we show that these results can be contradicted when\nhyperparameter search spaces are changed. As tuning effort grows without bound,\nmore general optimizers should never underperform the ones they can approximate\n(i.e., Adam should never perform worse than momentum), but recent attempts to\ncompare optimizers either assume these inclusion relationships are not\npractically relevant or restrict the hyperparameters in ways that break the\ninclusions. In our experiments, we find that inclusion relationships between\noptimizers matter in practice and always predict optimizer comparisons. In\nparticular, we find that the popular adaptive gradient methods never\nunderperform momentum or gradient descent. We also report practical tips around\ntuning often ignored hyperparameters of adaptive gradient methods and raise\nconcerns about fairly benchmarking optimizers for neural network training.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:51:09 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 05:28:34 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 00:58:12 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Choi", "Dami", ""], ["Shallue", "Christopher J.", ""], ["Nado", "Zachary", ""], ["Lee", "Jaehoon", ""], ["Maddison", "Chris J.", ""], ["Dahl", "George E.", ""]]}, {"id": "1910.05448", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, David Ahmedt-Aristizabal, Sridha\n  Sridharan, Kristin Laurens, Patrick Johnston, and Clinton Fookes", "title": "Neural Memory Plasticity for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of machine learning, Neural Memory Networks (NMNs) have\nrecently achieved impressive results in a variety of application areas\nincluding visual question answering, trajectory prediction, object tracking,\nand language modelling. However, we observe that the attention based knowledge\nretrieval mechanisms used in current NMNs restricts them from achieving their\nfull potential as the attention process retrieves information based on a set of\nstatic connection weights. This is suboptimal in a setting where there are vast\ndifferences among samples in the data domain; such as anomaly detection where\nthere is no consistent criteria for what constitutes an anomaly. In this paper,\nwe propose a plastic neural memory access mechanism which exploits both static\nand dynamic connection weights in the memory read, write and output generation\nprocedures. We demonstrate the effectiveness and flexibility of the proposed\nmemory model in three challenging anomaly detection tasks in the medical\ndomain: abnormal EEG identification, MRI tumour type classification and\nschizophrenia risk detection in children. In all settings, the proposed\napproach outperforms the current state-of-the-art. Furthermore, we perform an\nin-depth analysis demonstrating the utility of neural plasticity for the\nknowledge retrieval process and provide evidence on how the proposed memory\nmodel generates sparse yet informative memory outputs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:32:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Ahmedt-Aristizabal", "David", ""], ["Sridharan", "Sridha", ""], ["Laurens", "Kristin", ""], ["Johnston", "Patrick", ""], ["Fookes", "Clinton", ""]]}, {"id": "1910.05449", "submitter": "Yuning Chai", "authors": "Yuning Chai and Benjamin Sapp and Mayank Bansal and Dragomir Anguelov", "title": "MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for\n  Behavior Prediction", "comments": "Appears in CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human behavior is a difficult and crucial task required for motion\nplanning. It is challenging in large part due to the highly uncertain and\nmulti-modal set of possible outcomes in real-world domains such as autonomous\ndriving. Beyond single MAP trajectory prediction, obtaining an accurate\nprobability distribution of the future is an area of active interest. We\npresent MultiPath, which leverages a fixed set of future state-sequence anchors\nthat correspond to modes of the trajectory distribution. At inference, our\nmodel predicts a discrete distribution over the anchors and, for each anchor,\nregresses offsets from anchor waypoints along with uncertainties, yielding a\nGaussian mixture at each time step. Our model is efficient, requiring only one\nforward inference pass to obtain multi-modal future distributions, and the\noutput is parametric, allowing compact communication and analytical\nprobabilistic queries. We show on several datasets that our model achieves more\naccurate predictions, and compared to sampling baselines, does so with an order\nof magnitude fewer trajectories.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:34:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chai", "Yuning", ""], ["Sapp", "Benjamin", ""], ["Bansal", "Mayank", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1910.05471", "submitter": "Yi Zhu", "authors": "YI Zhu, Jing Dong, Henry Lam", "title": "Efficient Inference and Exploration for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite an ever growing literature on reinforcement learning algorithms and\napplications, much less is known about their statistical inference. In this\npaper, we investigate the large sample behaviors of the Q-value estimates with\nclosed-form characterizations of the asymptotic variances. This allows us to\nefficiently construct confidence regions for Q-value and optimal value\nfunctions, and to develop policies to minimize their estimation errors. This\nalso leads to a policy exploration strategy that relies on estimating the\nrelative discrepancies among the Q estimates. Numerical experiments show\nsuperior performances of our exploration strategy than other benchmark\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 03:02:14 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 20:25:09 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Zhu", "YI", ""], ["Dong", "Jing", ""], ["Lam", "Henry", ""]]}, {"id": "1910.05480", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec, Arun K Kuchibhotla", "title": "First order expansion of convex regularized estimators", "comments": "Accepted at NeurIPS 2019 and published at\n  https://papers.nips.cc/paper/8606-first-order-expansion-of-convex-regularized-estimators\n  . The version here includes the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider first order expansions of convex penalized estimators in\nhigh-dimensional regression problems with random designs. Our setting includes\nlinear regression and logistic regression as special cases. For a given penalty\nfunction $h$ and the corresponding penalized estimator $\\hat\\beta$, we\nconstruct a quantity $\\eta$, the first order expansion of $\\hat\\beta$, such\nthat the distance between $\\hat\\beta$ and $\\eta$ is an order of magnitude\nsmaller than the estimation error $\\|\\hat{\\beta} - \\beta^*\\|$. In this sense,\nthe first order expansion $\\eta$ can be thought of as a generalization of\ninfluence functions from the mathematical statistics literature to regularized\nestimators in high-dimensions. Such first order expansion implies that the risk\nof $\\hat{\\beta}$ is asymptotically the same as the risk of $\\eta$ which leads\nto a precise characterization of the MSE of $\\hat\\beta$; this characterization\ntakes a particularly simple form for isotropic design. Such first order\nexpansion also leads to inference results based on $\\hat{\\beta}$. We provide\nsufficient conditions for the existence of such first order expansion for three\nregularizers: the Lasso in its constrained form, the lasso in its penalized\nform, and the Group-Lasso. The results apply to general loss functions under\nsome conditions and those conditions are satisfied for the squared loss in\nlinear regression and for the logistic loss in the logistic model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 03:56:44 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 17:20:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Bellec", "Pierre C", ""], ["Kuchibhotla", "Arun K", ""]]}, {"id": "1910.05484", "submitter": "Chao Qian", "authors": "Chao Qian and Hang Xiong and Ke Xue", "title": "Bayesian Optimization using Pseudo-Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a popular approach for expensive black-box\noptimization, with applications including parameter tuning, experimental\ndesign, robotics. BO usually models the objective function by a Gaussian\nprocess (GP), and iteratively samples the next data point by maximizing an\nacquisition function. In this paper, we propose a new general framework for BO\nby generating pseudo-points (i.e., data points whose objective values are not\nevaluated) to improve the GP model. With the classic acquisition function,\ni.e., upper confidence bound (UCB), we prove that the cumulative regret can be\ngenerally upper bounded. Experiments using UCB and other acquisition functions,\ni.e., probability of improvement (PI) and expectation of improvement (EI), on\nsynthetic as well as real-world problems clearly show the advantage of\ngenerating pseudo-points.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:17:26 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 06:41:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Qian", "Chao", ""], ["Xiong", "Hang", ""], ["Xue", "Ke", ""]]}, {"id": "1910.05485", "submitter": "Jiapeng Liu", "authors": "Jiapeng Liu, Milosz Kadzinski, Xiuwu Liao, Xiaoxin Mao, Yao Wang", "title": "A preference learning framework for multiple criteria sorting with\n  diverse additive value models and valued assignment examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a preference learning framework for multiple criteria sorting. We\nconsider sorting procedures applying an additive value model with diverse types\nof marginal value functions (including linear, piecewise-linear, splined, and\ngeneral monotone ones) under a unified analytical framework. Differently from\nthe existing sorting methods that infer a preference model from crisp decision\nexamples, where each reference alternative is assigned to a unique class, our\nframework allows to consider valued assignment examples in which a reference\nalternative can be classified into multiple classes with respective credibility\ndegrees. We propose an optimization model for constructing a preference model\nfrom such valued examples by maximizing the credible consistency among\nreference alternatives. To improve the predictive ability of the constructed\nmodel on new instances, we employ the regularization techniques. Moreover, to\nenhance the capability of addressing large-scale datasets, we introduce a\nstate-of-the-art algorithm that is widely used in the machine learning\ncommunity to solve the proposed optimization model in a computationally\nefficient way. Using the constructed additive value model, we determine both\ncrisp and valued assignments for non-reference alternatives. Moreover, we allow\nthe Decision Maker to prioritize importance of classes and give the method a\nflexibility to adjust classification performance across classes according to\nthe specified priorities. The practical usefulness of the analytical framework\nis demonstrated on a real-world dataset by comparing it to several existing\nsorting methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:18:46 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Liu", "Jiapeng", ""], ["Kadzinski", "Milosz", ""], ["Liao", "Xiuwu", ""], ["Mao", "Xiaoxin", ""], ["Wang", "Yao", ""]]}, {"id": "1910.05493", "submitter": "Yasir Hussain", "authors": "Yasir Hussain, Zhiqiu Huang, Yu Zhou and Senzhang Wang", "title": "Deep Transfer Learning for Source Code Modeling", "comments": null, "journal-ref": "International Journal of Software Engineering and Knowledge\n  Engineering. Vol. 30, No. 05, pp. 649-668 (2020)", "doi": "10.1142/S0218194020500230", "report-no": null, "categories": "cs.LG cs.NE cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning models have shown great potential in source\ncode modeling and analysis. Generally, deep learning-based approaches are\nproblem-specific and data-hungry. A challenging issue of these approaches is\nthat they require training from starch for a different related problem. In this\nwork, we propose a transfer learning-based approach that significantly improves\nthe performance of deep learning-based source code models. In contrast to\ntraditional learning paradigms, transfer learning can transfer the knowledge\nlearned in solving one problem into another related problem. First, we present\ntwo recurrent neural network-based models RNN and GRU for the purpose of\ntransfer learning in the domain of source code modeling. Next, via transfer\nlearning, these pre-trained (RNN and GRU) models are used as feature\nextractors. Then, these extracted features are combined into attention learner\nfor different downstream tasks. The attention learner leverages from the\nlearned knowledge of pre-trained models and fine-tunes them for a specific\ndownstream task. We evaluate the performance of the proposed approach with\nextensive experiments with the source code suggestion task. The results\nindicate that the proposed approach outperforms the state-of-the-art models in\nterms of accuracy, precision, recall, and F-measure without training the models\nfrom scratch.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:59:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 12:08:39 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hussain", "Yasir", ""], ["Huang", "Zhiqiu", ""], ["Zhou", "Yu", ""], ["Wang", "Senzhang", ""]]}, {"id": "1910.05495", "submitter": "Jason Ren", "authors": "Jason Ren, Russell Kunes, Finale Doshi-Velez", "title": "Prediction Focused Topic Models via Feature Selection", "comments": "AISTATS 2020. arXiv admin note: substantial text overlap with\n  arXiv:1911.08551", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models are often sought to balance prediction quality and\ninterpretability. However, when models are (inevitably) misspecified, standard\napproaches rarely deliver on both. We introduce a novel approach, the\nprediction-focused topic model, that uses the supervisory signal to retain only\nvocabulary terms that improve, or at least do not hinder, prediction\nperformance. By removing terms with irrelevant signal, the topic model is able\nto learn task-relevant, coherent topics. We demonstrate on several data sets\nthat compared to existing approaches, prediction-focused topic models learn\nmuch more coherent topics while maintaining competitive predictions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 05:08:43 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 06:20:26 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Ren", "Jason", ""], ["Kunes", "Russell", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1910.05510", "submitter": "Ahmed Ahmed", "authors": "Ahmed B.Zaky, Joshua Zhexue Huang, KaishunWu and Basem M.ElHalawany", "title": "Generative Neural Network based Spectrum Sharing using Linear Sum\n  Assignment Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectrum management and resource allocation (RA) problems are challenging and\ncritical in a vast number of research areas such as wireless communications and\ncomputer networks. The traditional approaches for solving such problems usually\nconsume time and memory, especially for large size problems. Recently different\nmachine learning approaches have been considered as potential promising\ntechniques for combinatorial optimization problems, especially the generative\nmodel of the deep neural networks. In this work, we propose a resource\nallocation deep autoencoder network, as one of the promising generative models,\nfor enabling spectrum sharing in underlay device-to-device (D2D) communication\nby solving linear sum assignment problems (LSAPs). Specifically, we investigate\nthe performance of three different architectures for the conditional\nvariational autoencoders (CVAE). The three proposed architecture are the\nconvolutional neural network (CVAE-CNN) autoencoder, the feed-forward neural\nnetwork (CVAE-FNN) autoencoder, and the hybrid (H-CVAE) autoencoder. The\nsimulation results show that the proposed approach could be used as a\nreplacement of the conventional RA techniques, such as the Hungarian algorithm,\ndue to its ability to find solutions of LASPs of different sizes with high\naccuracy and very fast execution time. Moreover, the simulation results reveal\nthat the accuracy of the proposed hybrid autoencoder architecture outperforms\nthe other proposed architectures and the state-of-the-art DNN techniques.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:05:07 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zaky", "Ahmed B.", ""], ["Huang", "Joshua Zhexue", ""], ["KaishunWu", "", ""], ["ElHalawany", "Basem M.", ""]]}, {"id": "1910.05512", "submitter": "Tonghan Wang", "authors": "Tonghan Wang, Jianhao Wang, Yi Wu, Chongjie Zhang", "title": "Influence-Based Multi-Agent Exploration", "comments": null, "journal-ref": "International Conference on Learning Representations, 2020,\n  spotlight", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically motivated reinforcement learning aims to address the\nexploration challenge for sparse-reward tasks. However, the study of\nexploration methods in transition-dependent multi-agent settings is largely\nabsent from the literature. We aim to take a step towards solving this problem.\nWe present two exploration methods: exploration via information-theoretic\ninfluence (EITI) and exploration via decision-theoretic influence (EDTI), by\nexploiting the role of interaction in coordinated behaviors of agents. EITI\nuses mutual information to capture influence transition dynamics. EDTI uses a\nnovel intrinsic reward, called Value of Interaction (VoI), to characterize and\nquantify the influence of one agent's behavior on expected returns of other\nagents. By optimizing EITI or EDTI objective as a regularizer, agents are\nencouraged to coordinate their exploration and learn policies to optimize team\nperformance. We show how to optimize these regularizers so that they can be\neasily integrated with policy gradient reinforcement learning. The resulting\nupdate rule draws a connection between coordinated exploration and intrinsic\nreward distribution. Finally, we empirically demonstrate the significant\nstrength of our method in a variety of multi-agent scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:14:33 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Tonghan", ""], ["Wang", "Jianhao", ""], ["Wu", "Yi", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1910.05513", "submitter": "Hanshu Yan", "authors": "Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, Jiashi Feng", "title": "On Robustness of Neural Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ordinary differential equations (ODEs) have been attracting increasing\nattention in various research domains recently. There have been some works\nstudying optimization issues and approximation capabilities of neural ODEs, but\ntheir robustness is still yet unclear. In this work, we fill this important gap\nby exploring robustness properties of neural ODEs both empirically and\ntheoretically. We first present an empirical study on the robustness of the\nneural ODE-based networks (ODENets) by exposing them to inputs with various\ntypes of perturbations and subsequently investigating the changes of the\ncorresponding outputs. In contrast to conventional convolutional neural\nnetworks (CNNs), we find that the ODENets are more robust against both random\nGaussian perturbations and adversarial attack examples. We then provide an\ninsightful understanding of this phenomenon by exploiting a certain desirable\nproperty of the flow of a continuous-time ODE, namely that integral curves are\nnon-intersecting. Our work suggests that, due to their intrinsic robustness, it\nis promising to use neural ODEs as a basic block for building robust deep\nnetwork models. To further enhance the robustness of vanilla neural ODEs, we\npropose the time-invariant steady neural ODE (TisODE), which regularizes the\nflow on perturbed data via the time-invariant property and the imposition of a\nsteady-state constraint. We show that the TisODE method outperforms vanilla\nneural ODEs and also can work in conjunction with other state-of-the-art\narchitectural methods to build more robust deep networks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:15:44 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 03:33:56 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Yan", "Hanshu", ""], ["Du", "Jiawei", ""], ["Tan", "Vincent Y. F.", ""], ["Feng", "Jiashi", ""]]}, {"id": "1910.05527", "submitter": "Rinu Boney", "authors": "Rinu Boney, Juho Kannala, Alexander Ilin", "title": "Regularizing Model-Based Planning with Energy-Based Models", "comments": "Conference on Robot Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning could enable sample-efficient learning by\nquickly acquiring rich knowledge about the world and using it to improve\nbehaviour without additional data. Learned dynamics models can be directly used\nfor planning actions but this has been challenging because of inaccuracies in\nthe learned models. In this paper, we focus on planning with learned dynamics\nmodels and propose to regularize it using energy estimates of state transitions\nin the environment. We visually demonstrate the effectiveness of the proposed\nmethod and show that off-policy training of an energy estimator can be\neffectively used to regularize planning with pre-trained dynamics models.\nFurther, we demonstrate that the proposed method enables sample-efficient\nlearning to achieve competitive performance in challenging continuous control\ntasks such as Half-cheetah and Ant in just a few minutes of experience.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 08:29:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Boney", "Rinu", ""], ["Kannala", "Juho", ""], ["Ilin", "Alexander", ""]]}, {"id": "1910.05534", "submitter": "Ian Gallagher", "authors": "Ian Gallagher, Andrew Jones, Anna Bertiger, Carey Priebe, and Patrick\n  Rubin-Delanchy", "title": "Spectral embedding of weighted graphs", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the statistical analysis of a weighted graph through\nspectral embedding. Under a latent position model in which the expected\nadjacency matrix has low rank, we prove uniform consistency and a central limit\ntheorem for the embedded nodes, treated as latent position estimates. In the\nspecial case of a weighted stochastic block model, this result implies that the\nembedding follows a Gaussian mixture model with each component representing a\ncommunity. We exploit this to formally evaluate different weight\nrepresentations of the graph using Chernoff information. For example, in a\nnetwork anomaly detection problem where we observe a p-value on each edge, we\nrecommend against directly embedding the matrix of p-values, and instead using\nthreshold or log p-values, depending on network sparsity and signal strength.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 09:13:26 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:12:22 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 12:56:48 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Gallagher", "Ian", ""], ["Jones", "Andrew", ""], ["Bertiger", "Anna", ""], ["Priebe", "Carey", ""], ["Rubin-Delanchy", "Patrick", ""]]}, {"id": "1910.05547", "submitter": "Malik Aqeel Anwar", "authors": "Aqeel Anwar, Arijit Raychowdhury", "title": "Autonomous Navigation via Deep Reinforcement Learning for Resource\n  Constraint Edge Nodes using Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart and agile drones are fast becoming ubiquitous at the edge of the cloud.\nThe usage of these drones are constrained by their limited power and compute\ncapability. In this paper, we present a Transfer Learning (TL) based approach\nto reduce on-board computation required to train a deep neural network for\nautonomous navigation via Deep Reinforcement Learning for a target algorithmic\nperformance. A library of 3D realistic meta-environments is manually designed\nusing Unreal Gaming Engine and the network is trained end-to-end. These trained\nmeta-weights are then used as initializers to the network in a test environment\nand fine-tuned for the last few fully connected layers. Variation in drone\ndynamics and environmental characteristics is carried out to show robustness of\nthe approach. Using NVIDIA GPU profiler it was shown that the energy\nconsumption and training latency is reduced by 3.7x and 1.8x respectively\nwithout significant degradation in the performance in terms of average distance\ntraveled before crash i.e. Mean Safe Flight (MSF). The approach is also tested\non a real environment using DJI Tello drone and similar results were reported.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 10:14:11 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Anwar", "Aqeel", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1910.05563", "submitter": "Arnu Pretorius", "authors": "Arnu Pretorius, Herman Kamper, Steve Kroon", "title": "On the expected behaviour of noise regularised deep neural networks as\n  Gaussian processes", "comments": "8 pages, 6 figures, preliminary work", "journal-ref": "Pattern Recognition Letters 138 (2020) 75-81", "doi": "10.1016/j.patrec.2020.06.027", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has established the equivalence between deep neural networks and\nGaussian processes (GPs), resulting in so-called neural network Gaussian\nprocesses (NNGPs). The behaviour of these models depends on the initialisation\nof the corresponding network. In this work, we consider the impact of noise\nregularisation (e.g. dropout) on NNGPs, and relate their behaviour to signal\npropagation theory in noise regularised deep neural networks. For ReLU\nactivations, we find that the best performing NNGPs have kernel parameters that\ncorrespond to a recently proposed initialisation scheme for noise regularised\nReLU networks. In addition, we show how the noise influences the covariance\nmatrix of the NNGP, producing a stronger prior towards simple functions away\nfrom the training points. We verify our theoretical findings with experiments\non MNIST and CIFAR-10 as well as on synthetic data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 13:23:58 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Pretorius", "Arnu", ""], ["Kamper", "Herman", ""], ["Kroon", "Steve", ""]]}, {"id": "1910.05565", "submitter": "Melanie Weber", "authors": "Melanie Weber", "title": "Neighborhood Growth Determines Geometric Priors for Relational\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying geometric structure in heterogeneous,\nhigh-dimensional data is a cornerstone of representation learning. While there\nexists a large body of literature on the embeddability of canonical graphs,\nsuch as lattices or trees, the heterogeneity of the relational data typically\nencountered in practice limits the applicability of these classical methods. In\nthis paper, we propose a combinatorial approach to evaluating embeddability,\ni.e., to decide whether a data set is best represented in Euclidean, Hyperbolic\nor Spherical space. Our method analyzes nearest-neighbor structures and local\nneighborhood growth rates to identify the geometric priors of suitable\nembedding spaces. For canonical graphs, the algorithm's prediction provably\nmatches classical results. As for large, heterogeneous graphs, we introduce an\nefficiently computable statistic that approximates the algorithm's decision\nrule. We validate our method over a range of benchmark data sets and compare\nwith recently published optimization-based embeddability methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 14:04:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Weber", "Melanie", ""]]}, {"id": "1910.05569", "submitter": "Shuai Yang", "authors": "Shuai Yang, Wenqi Zhu, Yuesheng Zhu", "title": "Residual Encoder-Decoder Network for Deep Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering aims to cluster unlabeled data that lies in a union of\nlow-dimensional linear subspaces. Deep subspace clustering approaches based on\nauto-encoders have become very popular to solve subspace clustering problems.\nHowever, the training of current deep methods converges slowly, which is much\nless efficient than traditional approaches. We propose a Residual\nEncoder-Decoder network for deep Subspace Clustering (RED-SC), which\nsymmetrically links convolutional and deconvolutional layers with skip-layer\nconnections, with which the training converges much faster. We use a\nself-expressive layer to generate more accurate linear representation\ncoefficients through different latent representations from multiple latent\nspaces. Experiments show the superiority of RED-SC in training efficiency and\nclustering accuracy. Moreover, we are the first one to apply residual\nencoder-decoder on unsupervised learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 14:35:54 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yang", "Shuai", ""], ["Zhu", "Wenqi", ""], ["Zhu", "Yuesheng", ""]]}, {"id": "1910.05570", "submitter": "Yuan Jin", "authors": "Yuan Jin, Ming Liu, Yunfeng Li, Ruohua Xu, Lan Du, Longxiang Gao, Yong\n  Xiang", "title": "Variational Auto-encoder Based Bayesian Poisson Tensor Factorization for\n  Sparse and Imbalanced Count Data", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-020-00723-7", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-negative tensor factorization models enable predictive analysis on count\ndata. Among them, Bayesian Poisson-Gamma models can derive full posterior\ndistributions of latent factors and are less sensitive to sparse count data.\nHowever, current inference methods for these Bayesian models adopt restricted\nupdate rules for the posterior parameters. They also fail to share the update\ninformation to better cope with the data sparsity. Moreover, these models are\nnot endowed with a component that handles the imbalance in count data values.\nIn this paper, we propose a novel variational auto-encoder framework called\nVAE-BPTF which addresses the above issues. It uses multi-layer perceptron\nnetworks to encode and share complex update information. The encoded\ninformation is then reweighted per data instance to penalize common data values\nbefore aggregated to compute the posterior parameters for the latent factors.\nUnder synthetic data evaluation, VAE-BPTF tended to recover the right number of\nlatent factors and posterior parameter values. It also outperformed current\nmodels in both reconstruction errors and latent factor (semantic) coherence\nacross five real-world datasets. Furthermore, the latent factors inferred by\nVAE-BPTF are perceived to be meaningful and coherent under a qualitative\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 14:36:45 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 14:16:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jin", "Yuan", ""], ["Liu", "Ming", ""], ["Li", "Yunfeng", ""], ["Xu", "Ruohua", ""], ["Du", "Lan", ""], ["Gao", "Longxiang", ""], ["Xiang", "Yong", ""]]}, {"id": "1910.05575", "submitter": "Rafael Izbicki", "authors": "Rafael Izbicki, Gilson T. Shimizu, Rafael B. Stern", "title": "Flexible distribution-free conditional predictive bands using density\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal methods create prediction bands that control average coverage under\nno assumptions besides i.i.d. data. Besides average coverage, one might also\ndesire to control conditional coverage, that is, coverage for every new testing\npoint. However, without strong assumptions, conditional coverage is\nunachievable. Given this limitation, the literature has focused on methods with\nasymptotical conditional coverage. In order to obtain this property, these\nmethods require strong conditions on the dependence between the target variable\nand the features. We introduce two conformal methods based on conditional\ndensity estimators that do not depend on this type of assumption to obtain\nasymptotic conditional coverage: Dist-split and CD-split. While Dist-split\nasymptotically obtains optimal intervals, which are easier to interpret than\ngeneral regions, CD-split obtains optimal size regions, which are smaller than\nintervals. CD-split also obtains local coverage by creating a data-driven\npartition of the feature space that scales to high-dimensional settings and by\ngenerating prediction bands locally on the partition elements. In a wide\nvariety of simulated scenarios, our methods have a better control of\nconditional coverage and have smaller length than previously proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 15:23:13 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 15:28:21 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Izbicki", "Rafael", ""], ["Shimizu", "Gilson T.", ""], ["Stern", "Rafael B.", ""]]}, {"id": "1910.05587", "submitter": "Anna Sepliarskaia", "authors": "Anna Sepliarskaia, Julia Kiseleva and Maarten de Rijke", "title": "How to Not Measure Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate disentangled representations several metrics have been proposed.\nHowever, theoretical guarantees for conventional metrics of disentanglement are\nmissing. Moreover, conventional metrics do not have a consistent correlation\nwith the outcomes of qualitative studies. In this paper we analyze metrics of\ndisentanglement and their properties. We conclude that existing metrics of\ndisentanglement were created to reflect different characteristics of\ndisentanglement and do not satisfy two basic desirable properties: (1) assign a\nhigh score to representations that are disentangled according to the\ndefinition; and (2) assign a low score to representations that are entangled\naccording to the definition. In addition, we propose a new metric of\ndisentanglement and prove that it satisfies both of the properties.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:09:16 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 10:13:06 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 10:43:28 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sepliarskaia", "Anna", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1910.05591", "submitter": "Juliana Cesaro", "authors": "Juliana Cesaro and Fabio G. Cozman", "title": "Measuring Unfairness through Game-Theoretic Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One often finds in the literature connections between measures of fairness\nand measures of feature importance employed to interpret trained classifiers.\nHowever, there seems to be no study that compares fairness measures and feature\nimportance measures. In this paper we propose ways to evaluate and compare such\nmeasures. We focus in particular on SHAP, a game-theoretic measure of feature\nimportance; we present results for a number of unfairness-prone datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:23:04 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Cesaro", "Juliana", ""], ["Cozman", "Fabio G.", ""]]}, {"id": "1910.05594", "submitter": "Ayman Wagdy", "authors": "Ayman Wagdy, Veronica Garcia-Hansen, Mohammed Elhenawy, Gillian\n  Isoardi, Robin Drogemuller, Fatma Fathy", "title": "Open-plan Glare Evaluator (OGE): A Demonstration of a New Glare\n  Prediction Approach Using Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting discomfort glare in open-plan offices is a challenging problem.\nAlthough glare research has existed for more than 50 years, all current glare\nmetrics have accuracy limitations, especially in open-plan offices with low\nlighting levels. Thus, it is crucial to develop a new method to predict glare\nmore accurately. This paper is the first to adopt Machine Learning (ML)\napproaches in the prediction of glare. This research aims to demonstrate the\nvalidity of this approach by comparing the accuracy of the new ML model for\nopen-plan offices (OGE) to the accuracy of the existing glare metrics using\nlocal dataset. To utilize and test this approach, Post-Occupancy Evaluation\n(POE) and High Dynamic Range (HDR) images were collected from 80 occupants\n(n=80) in four different open-plan offices in Brisbane, Australia.\nConsequently, various multi-region luminance values, luminance, and glare\nindices were calculated and examined as input features to train ML models. The\naccuracy of the ML model was compared to the accuracy of 24 indices which were\nalso evaluated using a Receiver Operating Characteristic (ROC) analysis to\nidentify the best cutoff values (thresholds) for each index in open-plan\nconfigurations. Results showed that the ML approach could predict glare with an\naccuracy of 83.8% (0.80 true positive rate and 0.86 true negative rate), which\noutperformed the accuracy of the previously developed glare metrics. OGE is\napplicable for open-plan office situations with low vertical illuminance (200\nto 600 lux). However, ML models can be trained with more substantial datasets\nto achieve global model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:33:02 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 14:13:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wagdy", "Ayman", ""], ["Garcia-Hansen", "Veronica", ""], ["Elhenawy", "Mohammed", ""], ["Isoardi", "Gillian", ""], ["Drogemuller", "Robin", ""], ["Fathy", "Fatma", ""]]}, {"id": "1910.05625", "submitter": "Laura Niss", "authors": "Laura Niss and Ambuj Tewari", "title": "What You See May Not Be What You Get: UCB Bandit Algorithms Robust to\n  {\\epsilon}-Contamination", "comments": "Changes to reflect UAI2020 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications of bandit algorithms in education, we consider a\nstochastic multi-armed bandit problem with $\\varepsilon$-contaminated rewards.\nWe allow an adversary to give arbitrary unbounded contaminated rewards with\nfull knowledge of the past and future. We impose the constraint that for each\ntime $t$ the proportion of contaminated rewards for any action is less than or\nequal to $\\varepsilon$. We derive concentration inequalities for two robust\nmean estimators for sub-Gaussian distributions in the\n$\\varepsilon$-contamination context. We define the $\\varepsilon$-contaminated\nstochastic bandit problem and use our robust mean estimators to give two\nvariants of a robust Upper Confidence Bound (UCB) algorithm, crUCB. Using\nregret derived from only the underlying stochastic rewards, both variants of\ncrUCB achieve $\\mathcal{O} (\\sqrt{KT\\log T})$ regret for small enough\ncontamination proportions. Our simulations assume small horizons, reflecting\nthe newly explored setting of bandits in education. We show that in certain\nadversarial regimes crUCB not only outperforms algorithms designed for\nstochastic (UCB1) and adversarial (EXP3) bandits but also those that have \"best\nof both worlds\" guarantees (EXP3++ and TsallisInf) even when our constraint on\nthe proportion of contaminated rewards is broken.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 19:00:17 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 17:27:49 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 18:16:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Niss", "Laura", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1910.05626", "submitter": "Daniel Jung", "authors": "Daniel Jung", "title": "Isolation and Localization of Unknown Faults Using Neural Network-Based\n  Residuals", "comments": "8 pages, 7 figures, If citing this paper please use: In: Proceedings\n  of the Annual Conference of the PHM Society, Scottsdale, Arizona, USA (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of unknown faults in industrial systems is a difficult task for\ndata-driven diagnosis methods. The classification performance of many machine\nlearning methods relies on the quality of training data. Unknown faults, for\nexample faults not represented in training data, can be detected using, for\nexample, anomaly classifiers. However, mapping these unknown faults to an\nactual location in the real system is a non-trivial problem. In model-based\ndiagnosis, physical-based models are used to create residuals that isolate\nfaults by mapping model equations to faulty system components. Developing\nsufficiently accurate physical-based models can be a time-consuming process.\nHybrid modeling methods combining physical-based methods and machine learning\nis one solution to design data-driven residuals for fault isolation. In this\nwork, a set of neural network-based residuals are designed by incorporating\nphysical insights about the system behavior in the residual model structure.\nThe residuals are trained using only fault-free data and a simulation case\nstudy shows that they can be used to perform fault isolation and localization\nof unknown faults in the system.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 19:00:21 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Jung", "Daniel", ""]]}, {"id": "1910.05639", "submitter": "Niklas Stoehr", "authors": "Niklas Stoehr, Emine Yilmaz, Marc Brockschmidt, Jan Stuehmer", "title": "Disentangling Interpretable Generative Parameters of Random and\n  Real-World Graphs", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Workshop on Graph Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a wide range of interpretable generative procedures for graphs exist,\nmatching observed graph topologies with such procedures and choices for its\nparameters remains an open problem. Devising generative models that closely\nreproduce real-world graphs requires domain knowledge and time-consuming\nsimulation. While existing deep learning approaches rely on less manual\nmodelling, they offer little interpretability. This work approaches graph\ngeneration (decoding) as the inverse of graph compression (encoding). We show\nthat in a disentanglement-focused deep autoencoding framework, specifically\nBeta-Variational Autoencoders (Beta-VAE), choices of generative procedures and\ntheir parameters arise naturally in the latent space. Our model is capable of\nlearning disentangled, interpretable latent variables that represent the\ngenerative parameters of procedurally generated random graphs and real-world\ngraphs. The degree of disentanglement is quantitatively measured using the\nMutual Information Gap (MIG). When training our Beta-VAE model on ER random\ngraphs, its latent variables have a near one-to-one mapping to the ER random\ngraph parameters n and p. We deploy the model to analyse the correlation\nbetween graph topology and node attributes measuring their mutual dependence\nwithout handpicking topological properties.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 19:57:55 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 19:40:41 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Stoehr", "Niklas", ""], ["Yilmaz", "Emine", ""], ["Brockschmidt", "Marc", ""], ["Stuehmer", "Jan", ""]]}, {"id": "1910.05640", "submitter": "Xujiang Zhao", "authors": "Xujiang Zhao, Feng Chen, Jin-Hee Cho", "title": "Deep Learning for Predicting Dynamic Uncertain Opinions in Network Data", "comments": "IEEE Bigdata 2018", "journal-ref": "2018 IEEE International Conference on Big Data (Big Data)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective Logic (SL) is one of well-known belief models that can explicitly\ndeal with uncertain opinions and infer unknown opinions based on a rich set of\noperators of fusing multiple opinions. Due to high simplicity and\napplicability, SL has been substantially applied in a variety of decision\nmaking in the area of cybersecurity, opinion models, trust models, and/or\nsocial network analysis. However, SL and its variants have exposed limitations\nin predicting uncertain opinions in real-world dynamic network data mainly in\nthree-fold: (1) a lack of scalability to deal with a large-scale network; (2)\nlimited capability to handle heterogeneous topological and temporal\ndependencies among node-level opinions; and (3) a high sensitivity with\nconflicting evidence that may generate counterintuitive opinions derived from\nthe evidence. In this work, we proposed a novel deep learning (DL)-based\ndynamic opinion inference model while node-level opinions are still formalized\nbased on SL meaning that an opinion has a dimension of uncertainty in addition\nto belief and disbelief in a binomial opinion (i.e., agree or disagree). The\nproposed DL-based dynamic opinion inference model overcomes the above three\nlimitations by integrating the following techniques: (1) state-of-the-art DL\ntechniques, such as the Graph Convolutional Network (GCN) and the Gated\nRecurrent Units (GRU) for modeling the topological and temporal heterogeneous\ndependency information of a given dynamic network; (2) modeling conflicting\nopinions based on robust statistics; and (3) a highly scalable inference\nalgorithm to predict dynamic, uncertain opinions in a linear computation time.\nWe validated the outperformance of our proposed DL-based algorithm (i.e.,\nGCN-GRU-opinion model) via extensive comparative performance analysis based on\nfour real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 20:10:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhao", "Xujiang", ""], ["Chen", "Feng", ""], ["Cho", "Jin-Hee", ""]]}, {"id": "1910.05651", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash", "title": "Interventional Experiment Design for Causal Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that from purely observational data, a causal DAG is identifiable\nonly up to its Markov equivalence class, and for many ground truth DAGs, the\ndirection of a large portion of the edges will be remained unidentified. The\ngolden standard for learning the causal DAG beyond Markov equivalence is to\nperform a sequence of interventions in the system and use the data gathered\nfrom the interventional distributions. We consider a setup in which given a\nbudget $k$, we design $k$ interventions non-adaptively. We cast the problem of\nfinding the best intervention target set as an optimization problem which aims\nto maximize the number of edges whose directions are identified due to the\nperformed interventions. First, we consider the case that the underlying causal\nstructure is a tree. For this case, we propose an efficient exact algorithm for\nthe worst-case gain setup, as well as an approximate algorithm for the average\ngain setup. We then show that the proposed approach for the average gain setup\ncan be extended to the case of general causal structures. In this case, besides\nthe design of interventions, calculating the objective function is also\nchallenging. We propose an efficient exact calculator as well as two estimators\nfor this task. We evaluate the proposed methods using synthetic as well as real\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 21:48:22 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1910.05653", "submitter": "Sidak Pal Singh", "authors": "Sidak Pal Singh and Martin Jaggi", "title": "Model Fusion via Optimal Transport", "comments": "NeurIPS 2020 conference proceedings (early version featured in the\n  Optimal Transport & Machine Learning workshop, NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combining different models is a widely used paradigm in machine learning\napplications. While the most common approach is to form an ensemble of models\nand average their individual predictions, this approach is often rendered\ninfeasible by given resource constraints in terms of memory and computation,\nwhich grow linearly with the number of models. We present a layer-wise model\nfusion algorithm for neural networks that utilizes optimal transport to (soft-)\nalign neurons across the models before averaging their associated parameters.\n  We show that this can successfully yield \"one-shot\" knowledge transfer (i.e,\nwithout requiring any retraining) between neural networks trained on\nheterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we\nillustrate that our approach significantly outperforms vanilla averaging, as\nwell as how it can serve as an efficient replacement for the ensemble with\nmoderate fine-tuning, for standard convolutional networks (like VGG11),\nresidual networks (like ResNet18), and multi-layer perceptrons on CIFAR10,\nCIFAR100, and MNIST. Finally, our approach also provides a principled way to\ncombine the parameters of neural networks with different widths, and we explore\nits application for model compression. The code is available at the following\nlink, https://github.com/sidak/otfusion.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 22:07:15 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 13:05:55 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 15:16:06 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 16:42:43 GMT"}, {"version": "v5", "created": "Sun, 14 Feb 2021 21:50:57 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Singh", "Sidak Pal", ""], ["Jaggi", "Martin", ""]]}, {"id": "1910.05654", "submitter": "Young Hun Jung", "authors": "Young Hun Jung, Marc Abeille, Ambuj Tewari", "title": "Thompson Sampling in Non-Episodic Restless Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restless bandit problems assume time-varying reward distributions of the\narms, which adds flexibility to the model but makes the analysis more\nchallenging. We study learning algorithms over the unknown reward distributions\nand prove a sub-linear, $O(\\sqrt{T}\\log T)$, regret bound for a variant of\nThompson sampling. Our analysis applies in the infinite time horizon setting,\nresolving the open question raised by Jung and Tewari (2019) whose analysis is\nlimited to the episodic case. We adopt their policy mapping framework, which\nallows our algorithm to be efficient and simultaneously keeps the regret\nmeaningful. Our algorithm adapts the TSDE algorithm of Ouyang et al. (2017) in\na non-trivial manner to account for the special structure of restless bandits.\nWe test our algorithm on a simulated dynamic channel access problem with\nseveral policy mappings, and the empirical regrets agree with the theoretical\nbound regardless of the choice of the policy mapping.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 22:30:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Jung", "Young Hun", ""], ["Abeille", "Marc", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1910.05664", "submitter": "Yonadav Shavit", "authors": "Yonadav Shavit, William S. Moses", "title": "Extracting Incentives from Black-Box Decisions", "comments": "Accepted to the NeurIPS 2019 Workshop on Robust AI in Financial\n  Services: Data, Fairness, Explainability, Trustworthiness, and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithmic decision-maker incentivizes people to act in certain ways to\nreceive better decisions. These incentives can dramatically influence subjects'\nbehaviors and lives, and it is important that both decision-makers and\ndecision-recipients have clarity on which actions are incentivized by the\nchosen model. While for linear functions, the changes a subject is incentivized\nto make may be clear, we prove that for many non-linear functions (e.g. neural\nnetworks, random forests), classical methods for interpreting the behavior of\nmodels (e.g. input gradients) provide poor advice to individuals on which\nactions they should take. In this work, we propose a mathematical framework for\nunderstanding algorithmic incentives as the challenge of solving a Markov\nDecision Process, where the state includes the set of input features, and the\nreward is a function of the model's output. We can then leverage the many\ntoolkits for solving MDPs (e.g. tree-based planning, reinforcement learning) to\nidentify the optimal actions each individual is incentivized to take to improve\ntheir decision under a given model. We demonstrate the utility of our method by\nestimating the maximally-incentivized actions in two real-world settings: a\nrecidivism risk predictor we train using ProPublica's COMPAS dataset, and an\nonline credit scoring tool published by the Fair Isaac Corporation (FICO).\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 01:17:29 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shavit", "Yonadav", ""], ["Moses", "William S.", ""]]}, {"id": "1910.05683", "submitter": "Brosnan Yuen", "authors": "Brosnan Yuen", "title": "Hardware/Software Codesign for Training/Testing Multiple Neural Networks\n  on Multiple FPGAs", "comments": "It was submitted without proper permission granted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most neural network designs for FPGAs are inflexible. In this paper, we\npropose a flexible VHDL structure that would allow any neural network to be\nimplemented on multiple FPGAs. Moreover, the VHDL structure allows for testing\nas well as training multiple neural networks. The VHDL design consists of\nmultiple processor groups. There are two types of processor groups: Mini Vector\nMachine Processor Group and Activation Processor Group. Each processor group\nconsists of individual Mini Vector Machines and Activation Processor. The Mini\nVector Machines apply vector operations to the data, while the Activation\nProcessors apply activation functions to the data. A ring buffer was\nimplemented to connect the various processor groups.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 04:12:32 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 22:26:16 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yuen", "Brosnan", ""]]}, {"id": "1910.05691", "submitter": "Dhrubasish Sarkar", "authors": "Dhrubasish Sarkar and Premananda Jana", "title": "Analyzing User Activities Using Vector Space Model in Online Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of internet, wireless technologies and mobile\ndevices has led to the birth of mass connectivity and online interaction\nthrough Online Social Networks (OSNs) and similar environments. OSN reflects a\nsocial structure consist of a set of individuals and different types of ties\nlike connections, relationships, interactions etc among them and helps its\nusers to connect with their friends and common interest groups, share views and\nto pass information. Now days the users choose OSN sites as a most preferred\nplace for sharing their updates, different views, posting photographs and would\nlike to make it available for others for viewing, rating and making comments.\nThe current paper aims to explore and analyze the association between the\nobjects (like photographs, posts etc) and its viewers (friends, acquaintances\netc) for a given user and to find activity relationship among them by using the\nTF-IDF scheme of Vector Space Model. After vectorization the vector data has\nbeen presented through a weighted graph with various properties.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 05:57:16 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sarkar", "Dhrubasish", ""], ["Jana", "Premananda", ""]]}, {"id": "1910.05695", "submitter": "Babak Shahbaba", "authors": "Tian Chen, Lingge Li, Gabriel Elias, Norbert Fortin, and Babak\n  Shahbaba", "title": "Bayesian Neural Decoding Using A Diversity-Encouraging Latent\n  Representation Learning Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well established that temporal organization is critical to memory, and\nthat the ability to temporally organize information is fundamental to many\nperceptual, cognitive, and motor processes. While our understanding of how the\nbrain processes the spatial context of memories has advanced considerably, our\nunderstanding of their temporal organization lags far behind. In this paper, we\npropose a new approach for elucidating the neural basis of complex behaviors\nand temporal organization of memories. More specifically, we focus on neural\ndecoding - the prediction of behavioral or experimental conditions based on\nobserved neural data. In general, this is a challenging classification problem,\nwhich is of immense interest in neuroscience. Our goal is to develop a new\nframework that not only improves the overall accuracy of decoding, but also\nprovides a clear latent representation of the decoding process. To accomplish\nthis, our approach uses a Variational Auto-encoder (VAE) model with a\ndiversity-encouraging prior based on determinantal point processes (DPP) to\nimprove latent representation learning by avoiding redundancy in the latent\nspace. We apply our method to data collected from a novel rat experiment that\ninvolves presenting repeated sequences of odors at a single port and testing\nthe rats' ability to identify each odor. We show that our method leads to\nsubstantially higher accuracy rate for neural decoding and allows to discover\nnovel biological phenomena by providing a clear latent representation of the\ndecoding process.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:03:39 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chen", "Tian", ""], ["Li", "Lingge", ""], ["Elias", "Gabriel", ""], ["Fortin", "Norbert", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1910.05697", "submitter": "Amit Daniely", "authors": "Amit Daniely and Elad Granot", "title": "Generalization Bounds for Neural Networks via Approximate Description\n  Length", "comments": "To appear in NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sample complexity of networks with bounds on the magnitude\nof its weights. In particular, we consider the class \\[\nH=\\left\\{W_t\\circ\\rho\\circ \\ldots\\circ\\rho\\circ W_{1} :W_1,\\ldots,W_{t-1}\\in\nM_{d, d}, W_t\\in M_{1,d}\\right\\} \\] where the spectral norm of each $W_i$ is\nbounded by $O(1)$, the Frobenius norm is bounded by $R$, and $\\rho$ is the\nsigmoid function $\\frac{e^x}{1+e^x}$ or the smoothened ReLU function $ \\ln\n(1+e^x)$. We show that for any depth $t$, if the inputs are in $[-1,1]^d$, the\nsample complexity of $H$ is $\\tilde O\\left(\\frac{dR^2}{\\epsilon^2}\\right)$.\nThis bound is optimal up to log-factors, and substantially improves over the\nprevious state of the art of $\\tilde O\\left(\\frac{d^2R^2}{\\epsilon^2}\\right)$.\n  We furthermore show that this bound remains valid if instead of considering\nthe magnitude of the $W_i$'s, we consider the magnitude of $W_i - W_i^0$, where\n$W_i^0$ are some reference matrices, with spectral norm of $O(1)$. By taking\nthe $W_i^0$ to be the matrices at the onset of the training process, we get\nsample complexity bounds that are sub-linear in the number of parameters, in\nmany typical regimes of parameters.\n  To establish our results we develop a new technique to analyze the sample\ncomplexity of families $H$ of predictors. We start by defining a new notion of\na randomized approximate description of functions $f:X\\to\\mathbb{R}^d$. We then\nshow that if there is a way to approximately describe functions in a class $H$\nusing $d$ bits, then $d/\\epsilon^2$ examples suffices to guarantee uniform\nconvergence. Namely, that the empirical loss of all the functions in the class\nis $\\epsilon$-close to the true loss. Finally, we develop a set of tools for\ncalculating the approximate description length of classes of functions that can\nbe presented as a composition of linear function classes and non-linear\nfunctions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:21:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Daniely", "Amit", ""], ["Granot", "Elad", ""]]}, {"id": "1910.05725", "submitter": "Herman Kamper", "authors": "Arnu Pretorius, Elan van Biljon, Benjamin van Niekerk, Ryan Eloff,\n  Matthew Reynard, Steve James, Benjamin Rosman, Herman Kamper, Steve Kroon", "title": "If dropout limits trainable depth, does critical initialisation still\n  matter? A large-scale statistical analysis on ReLU networks", "comments": "8 pages, 6 figures, under consideration at Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in signal propagation theory has shown that dropout limits the\ndepth to which information can propagate through a neural network. In this\npaper, we investigate the effect of initialisation on training speed and\ngeneralisation for ReLU networks within this depth limit. We ask the following\nresearch question: given that critical initialisation is crucial for training\nat large depth, if dropout limits the depth at which networks are trainable,\ndoes initialising critically still matter? We conduct a large-scale controlled\nexperiment, and perform a statistical analysis of over $12000$ trained\nnetworks. We find that (1) trainable networks show no statistically significant\ndifference in performance over a wide range of non-critical initialisations;\n(2) for initialisations that show a statistically significant difference, the\nnet effect on performance is small; (3) only extreme initialisations (very\nsmall or very large) perform worse than criticality. These findings also apply\nto standard ReLU networks of moderate depth as a special case of zero dropout.\nOur results therefore suggest that, in the shallow-to-moderate depth setting,\ncritical initialisation provides zero performance gains when compared to\noff-critical initialisations and that searching for off-critical\ninitialisations that might improve training speed or generalisation, is likely\nto be a fruitless endeavour.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 10:39:32 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 10:34:44 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pretorius", "Arnu", ""], ["van Biljon", "Elan", ""], ["van Niekerk", "Benjamin", ""], ["Eloff", "Ryan", ""], ["Reynard", "Matthew", ""], ["James", "Steve", ""], ["Rosman", "Benjamin", ""], ["Kamper", "Herman", ""], ["Kroon", "Steve", ""]]}, {"id": "1910.05744", "submitter": "Dong Liu", "authors": "Dong Liu and Antoine Honor\\'e and Saikat Chatterjee and Lars K.\n  Rasmussen", "title": "Powering Hidden Markov Model by Neural Network based Generative Models", "comments": null, "journal-ref": "ECAI 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov model (HMM) has been successfully used for sequential data\nmodeling problems. In this work, we propose to power the modeling capacity of\nHMM by bringing in neural network based generative models. The proposed model\nis termed as GenHMM. In the proposed GenHMM, each HMM hidden state is\nassociated with a neural network based generative model that has tractability\nof exact likelihood and provides efficient likelihood computation. A generative\nmodel in GenHMM consists of mixture of generators that are realized by flow\nmodels. A learning algorithm for GenHMM is proposed in expectation-maximization\nframework. The convergence of the learning GenHMM is analyzed. We demonstrate\nthe efficiency of GenHMM by classification tasks on practical sequential data.\nCode available at https://github.com/FirstHandScientist/genhmm.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 13:06:43 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 15:41:27 GMT"}, {"version": "v3", "created": "Sun, 24 May 2020 20:07:01 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liu", "Dong", ""], ["Honor\u00e9", "Antoine", ""], ["Chatterjee", "Saikat", ""], ["Rasmussen", "Lars K.", ""]]}, {"id": "1910.05751", "submitter": "Jia Liu", "authors": "Wenhua Zhang, Licheng Jiao, Jia Liu", "title": "Hierarchical Feature-Aware Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a hierarchical feature-aware tracking framework for\nefficient visual tracking. Recent years, ensembled trackers which combine\nmultiple component trackers have achieved impressive performance. In ensembled\ntrackers, the decision of results is usually a post-event process, i.e.,\ntracking result for each tracker is first obtained and then the suitable one is\nselected according to result ensemble. In this paper, we propose a pre-event\nmethod. We construct an expert pool with each expert being one set of features.\nFor each frame, several experts are first selected in the pool according to\ntheir past performance and then they are used to predict the object. The\nselection rate of each expert in the pool is then updated and tracking result\nis obtained according to result ensemble. We propose a novel pre-known\nexpert-adaptive selection strategy. Since the process is more efficient, more\nexperts can be constructed by fusing more types of features which leads to more\nrobustness. Moreover, with the novel expert selection strategy, overfitting\ncaused by fixed experts for each frame can be mitigated. Experiments on several\npublic available datasets demonstrate the superiority of the proposed method\nand its state-of-the-art performance among ensembled trackers.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 13:51:44 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 14:09:12 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Wenhua", ""], ["Jiao", "Licheng", ""], ["Liu", "Jia", ""]]}, {"id": "1910.05769", "submitter": "Bo Li", "authors": "Bo Li and David Saad", "title": "Large Deviation Analysis of Function Sensitivity in Random Deep Neural\n  Networks", "comments": null, "journal-ref": "J. Phys. A: Math. Theor. 53. 104002 (2020)", "doi": "10.1088/1751-8121/ab6a6f", "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field theory has been successfully used to analyze deep neural networks\n(DNN) in the infinite size limit. Given the finite size of realistic DNN, we\nutilize the large deviation theory and path integral analysis to study the\ndeviation of functions represented by DNN from their typical mean field\nsolutions. The parameter perturbations investigated include weight\nsparsification (dilution) and binarization, which are commonly used in model\nsimplification, for both ReLU and sign activation functions. We find that\nrandom networks with ReLU activation are more robust to parameter perturbations\nwith respect to their counterparts with sign activation, which arguably is\nreflected in the simplicity of the functions they generate.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 15:20:18 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 17:24:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Li", "Bo", ""], ["Saad", "David", ""]]}, {"id": "1910.05774", "submitter": "Dimitrios Kollias", "authors": "Hanne Carlsson and Dimitrios Kollias", "title": "Image Generation and Recognition (Emotions)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) were proposed in 2014 by Goodfellow et\nal., and have since been extended into multiple computer vision applications.\nThis report provides a thorough survey of recent GAN research, outlining the\nvarious architectures and applications, as well as methods for training GANs\nand dealing with latent space. This is followed by a discussion of potential\nareas for future GAN research, including: evaluating GANs, better understanding\nGANs, and techniques for training GANs. The second part of this report outlines\nthe compilation of a dataset of images `in the wild' representing each of the 7\nbasic human emotions, and analyses experiments done when training a StarGAN on\nthis dataset combined with the FER2013 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 16:00:06 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:10:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Carlsson", "Hanne", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05784", "submitter": "Dimitrios Kollias", "authors": "Xia Yicheng and Dimitrios Kollias", "title": "Interpretable Deep Neural Networks for Dimensional and Categorical\n  Emotion Recognition in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play an important role in people's life. Understanding and\nrecognising is not only important for interpersonal communication, but also has\npromising applications in Human-Computer Interaction, automobile safety and\nmedical research. This project focuses on extending the emotion recognition\ndatabase, and training the CNN + RNN emotion recognition neural networks with\nemotion category representation and valence \\& arousal representation. The\ncombined models are constructed by training the two representations\nsimultaneously. The comparison and analysis between the three types of model\nare discussed. The inner-relationship between two emotion representations and\nthe interpretability of the neural networks are investigated. The findings\nsuggest that categorical emotion recognition performance can benefit from\ntraining with a combined model. And the mapping of emotion category and valence\n\\& arousal values can explain this phenomenon.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 16:33:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:25:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yicheng", "Xia", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05787", "submitter": "Chao-Tsung Huang", "authors": "Chao-Tsung Huang", "title": "ERNet Family: Hardware-Oriented CNN Models for Computational Imaging\n  Using Block-Based Inference", "comments": "5 pages; appearing in IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) demand huge DRAM bandwidth for\ncomputational imaging tasks, and block-based processing has recently been\napplied to greatly reduce the bandwidth. However, the induced additional\ncomputation for feature recomputing or the large SRAM for feature reusing will\ndegrade the performance or even forbid the usage of state-of-the-art models. In\nthis paper, we address these issues by considering the overheads and hardware\nconstraints in advance when constructing CNNs. We investigate a novel model\nfamily---ERNet---which includes temporary layer expansion as another means for\nincreasing model capacity. We analyze three ERNet variants in terms of hardware\nrequirement and introduce a hardware-aware model optimization procedure.\nEvaluations on Full HD and 4K UHD applications will be given to show the\neffectiveness in terms of image quality, pixel throughput, and SRAM usage. The\nresults also show that, for block-based inference, ERNet can outperform the\nstate-of-the-art FFDNet and EDSR-baseline models for image denoising and\nsuper-resolution respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:02:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 08:13:33 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Huang", "Chao-Tsung", ""]]}, {"id": "1910.05789", "submitter": "Micah Carroll", "authors": "Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A.\n  Seshia, Pieter Abbeel, Anca Dragan", "title": "On the Utility of Learning about Humans for Human-AI Coordination", "comments": "Published at NeurIPS 2019\n  (http://papers.nips.cc/paper/8760-on-the-utility-of-learning-about-humans-for-human-ai-coordination)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While we would like agents that can coordinate with humans, current\nalgorithms such as self-play and population-based training create agents that\ncan coordinate with themselves. Agents that assume their partner to be optimal\nor similar to them can converge to coordination protocols that fail to\nunderstand and be understood by humans. To demonstrate this, we introduce a\nsimple environment that requires challenging coordination, based on the popular\ngame Overcooked, and learn a simple model that mimics human play. We evaluate\nthe performance of agents trained via self-play and population-based training.\nThese agents perform very well when paired with themselves, but when paired\nwith our human model, they are significantly worse than agents designed to play\nwith the human model. An experiment with a planning algorithm yields the same\nconclusion, though only when the human-aware planner is given the exact human\nmodel that it is playing with. A user study with real humans shows this pattern\nas well, though less strongly. Qualitatively, we find that the gains come from\nhaving the agent adapt to the human's gameplay. Given this result, we suggest\nseveral approaches for designing agents that learn about humans in order to\nbetter coordinate with them. Code is available at\nhttps://github.com/HumanCompatibleAI/overcooked_ai.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:17:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 00:51:44 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Carroll", "Micah", ""], ["Shah", "Rohin", ""], ["Ho", "Mark K.", ""], ["Griffiths", "Thomas L.", ""], ["Seshia", "Sanjit A.", ""], ["Abbeel", "Pieter", ""], ["Dragan", "Anca", ""]]}, {"id": "1910.05804", "submitter": "Ching-Yao Chuang", "authors": "Ching-Yao Chuang, Antonio Torralba, Stefanie Jegelka", "title": "The Role of Embedding Complexity in Domain-invariant Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to generalize the hypothesis trained in a\nsource domain to an unlabeled target domain. One popular approach to this\nproblem is to learn domain-invariant embeddings for both domains. In this work,\nwe study, theoretically and empirically, the effect of the embedding complexity\non generalization to the target domain. In particular, this complexity affects\nan upper bound on the target risk; this is reflected in experiments, too. Next,\nwe specify our theoretical framework to multilayer neural networks. As a\nresult, we develop a strategy that mitigates sensitivity to the embedding\ncomplexity, and empirically achieves performance on par with or better than the\nbest layer-dependent complexity tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 18:20:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chuang", "Ching-Yao", ""], ["Torralba", "Antonio", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1910.05811", "submitter": "Fan Ding", "authors": "Fan Ding, Hanjing Wang, Ashish Sabharwal, Yexiang Xue", "title": "Towards Efficient Discrete Integration via Adaptive Quantile Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete integration in a high dimensional space of n variables poses\nfundamental challenges. The WISH algorithm reduces the intractable discrete\nintegration problem into n optimization queries subject to randomized\nconstraints, obtaining a constant approximation guarantee. The optimization\nqueries are expensive, which limits the applicability of WISH. We propose\nAdaWISH, which is able to obtain the same guarantee but accesses only a small\nsubset of queries of WISH. For example, when the number of function values is\nbounded by a constant, AdaWISH issues only O(log n) queries. The key idea is to\nquery adaptively, taking advantage of the shape of the weight function being\nintegrated. In general, we prove that AdaWISH has a regret of only O(log n)\nrelative to an idealistic oracle that issues queries at data-dependent optimal\npoints. Experimentally, AdaWISH gives precise estimates for discrete\nintegration problems, of the same quality as that of WISH and better than\nseveral competing approaches, on a variety of probabilistic inference\nbenchmarks. At the same time, it saves substantially on the number of\noptimization queries compared to WISH. On a suite of UAI inference challenge\nbenchmarks, it saves 81.5% of WISH queries while retaining the quality of\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 18:45:10 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 23:24:40 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ding", "Fan", ""], ["Wang", "Hanjing", ""], ["Sabharwal", "Ashish", ""], ["Xue", "Yexiang", ""]]}, {"id": "1910.05814", "submitter": "Samuel Melton", "authors": "Samuel Melton, Sharad Ramanathan", "title": "Discovering a sparse set of pairwise discriminating features in high\n  dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting an understanding of the underlying system from high dimensional\ndata is a growing problem in science. Discovering informative and meaningful\nfeatures is crucial for clustering, classification, and low dimensional data\nembedding. Here we propose to construct features based on their ability to\ndiscriminate between clusters of the data points. We define a class of problems\nin which linear separability of clusters is hidden in a low dimensional space.\nWe propose an unsupervised method to identify the subset of features that\ndefine a low dimensional subspace in which clustering can be conducted. This is\nachieved by averaging over discriminators trained on an ensemble of proposed\ncluster configurations. We then apply our method to single cell RNA-seq data\nfrom mouse gastrulation, and identify 27 key transcription factors (out of 409\ntotal), 18 of which are known to define cell states through their expression\nlevels. In this inferred subspace, we find clear signatures of known cell types\nthat eluded classification prior to discovery of the correct low dimensional\nsubspace.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 19:19:32 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 16:47:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Melton", "Samuel", ""], ["Ramanathan", "Sharad", ""]]}, {"id": "1910.05821", "submitter": "Yuzhe Ma", "authors": "Yuzhe Ma, Xuezhou Zhang, Wen Sun, Xiaojin Zhu", "title": "Policy Poisoning in Batch Reinforcement Learning and Control", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a security threat to batch reinforcement learning and control where\nthe attacker aims to poison the learned policy. The victim is a reinforcement\nlearner / controller which first estimates the dynamics and the rewards from a\nbatch data set, and then solves for the optimal policy with respect to the\nestimates. The attacker can modify the data set slightly before learning\nhappens, and wants to force the learner into learning a target policy chosen by\nthe attacker. We present a unified framework for solving batch policy poisoning\nattacks, and instantiate the attack on two standard victims: tabular certainty\nequivalence learner in reinforcement learning and linear quadratic regulator in\ncontrol. We show that both instantiation result in a convex optimization\nproblem on which global optimality is guaranteed, and provide analysis on\nattack feasibility and attack cost. Experiments show the effectiveness of\npolicy poisoning attacks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 19:53:31 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:58:12 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ma", "Yuzhe", ""], ["Zhang", "Xuezhou", ""], ["Sun", "Wen", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1910.05843", "submitter": "Rui Meng", "authors": "Rui Meng, Herbert Lee, Soper Braden, Priyadip Ray", "title": "Regularized Sparse Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are a flexible Bayesian nonparametric modelling approach\nthat has been widely applied but poses computational challenges. To address the\npoor scaling of exact inference methods, approximation methods based on sparse\nGaussian processes (SGP) are attractive. An issue faced by SGP, especially in\nlatent variable models, is the inefficient learning of the inducing inputs,\nwhich leads to poor model prediction. We propose a regularization approach by\nbalancing the reconstruction performance of data and the approximation\nperformance of the model itself. This regularization improves both inference\nand prediction performance. We extend this regularization approach into latent\nvariable models with SGPs and show that performing variational inference (VI)\non those models is equivalent to performing VI on a related empirical Bayes\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 21:53:44 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:09:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Meng", "Rui", ""], ["Lee", "Herbert", ""], ["Braden", "Soper", ""], ["Ray", "Priyadip", ""]]}, {"id": "1910.05847", "submitter": "Rui Meng", "authors": "Rui Meng, Soper Braden, Jan Nygard, Mari Nygrad, Herbert Lee", "title": "Hierarchical Hidden Markov Jump Processes for Cancer Screening Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov jump processes are an attractive approach for modeling clinical\ndisease progression data because they are explainable and capable of handling\nboth irregularly sampled and noisy data. Most applications in this context\nconsider time-homogeneous models due to their relative computational\nsimplicity. However, the time homogeneous assumption is too strong to\naccurately model the natural history of many diseases. Moreover, the population\nat risk is not homogeneous either, since disease exposure and susceptibility\ncan vary considerably. In this paper, we propose a piece-wise stationary\ntransition matrix to explain the heterogeneity in time. We propose a\nhierarchical structure for the heterogeneity in population, where prior\ninformation is considered to deal with unbalanced data. Moreover, an efficient,\nscalable EM algorithm is proposed for inference. We demonstrate the feasibility\nand superiority of our model on a cervical cancer screening dataset from the\nCancer Registry of Norway. Experiments show that our model outperforms\nstate-of-the-art recurrent neural network models in terms of prediction\naccuracy and significantly outperforms a standard hidden Markov jump process in\ngenerating Kaplan-Meier estimators.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:23:34 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Braden", "Soper", ""], ["Nygard", "Jan", ""], ["Nygrad", "Mari", ""], ["Lee", "Herbert", ""]]}, {"id": "1910.05851", "submitter": "Rui Meng", "authors": "Rui Meng, Braden Soper, Herbert Lee, Vincent X. Liu, John D. Greene,\n  Priyadip Ray", "title": "Nonstationary Multivariate Gaussian Processes for Electronic Health\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multivariate nonstationary Gaussian processes for jointly modeling\nmultiple clinical variables, where the key parameters, length-scales, standard\ndeviations and the correlations between the observed output, are all time\ndependent. We perform posterior inference via Hamiltonian Monte Carlo (HMC). We\nalso provide methods for obtaining computationally efficient gradient-based\nmaximum a posteriori (MAP) estimates. We validate our model on synthetic data\nas well as on electronic health records (EHR) data from Kaiser Permanente (KP).\nWe show that the proposed model provides better predictive performance over a\nstationary model as well as uncovers interesting latent correlation processes\nacross vitals which are potentially predictive of patient risk.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:37:08 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Soper", "Braden", ""], ["Lee", "Herbert", ""], ["Liu", "Vincent X.", ""], ["Greene", "John D.", ""], ["Ray", "Priyadip", ""]]}, {"id": "1910.05852", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer, Hongkai Zheng, Anima Anandkumar", "title": "Implicit competitive regularization in GANs", "comments": "The code used to produce the numerical experiments can be found under\n  http://github.com/devzhk/ICR . A high-level overview of this work can be\n  found under https://f-t-s.github.io/projects/icr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the stability of GAN training we need to understand why they can\nproduce realistic samples. Presently, this is attributed to properties of the\ndivergence obtained under an optimal discriminator. This argument has a\nfundamental flaw: If we do not impose regularity of the discriminator, it can\nexploit visually imperceptible errors of the generator to always achieve the\nmaximal generator loss. In practice, gradient penalties are used to regularize\nthe discriminator. However, this needs a metric on the space of images that\ncaptures visual similarity. Such a metric is not known, which explains the\nlimited success of gradient penalties in stabilizing GANs. We argue that the\nperformance of GANs is instead due to the implicit competitive regularization\n(ICR) arising from the simultaneous optimization of generator and\ndiscriminator. ICR promotes solutions that look real to the discriminator and\nthus leverages its inductive biases to generate realistic images. We show that\nopponent-aware modelling of generator and discriminator, as present in\ncompetitive gradient descent (CGD), can significantly strengthen ICR and thus\nstabilize GAN training without explicit regularization. In our experiments, we\nuse an existing implementation of WGAN-GP and show that by training it with CGD\nwe can improve the inception score (IS) on CIFAR10 for a wide range of\nscenarios, without any hyperparameter tuning. The highest IS is obtained by\ncombining CGD with the WGAN-loss, without any explicit regularization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:39:23 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:57:42 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 22:25:31 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 19:08:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Zheng", "Hongkai", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1910.05857", "submitter": "Haoran Sun", "authors": "Haoran Sun, Songtao Lu, Mingyi Hong", "title": "Improving the Sample and Communication Complexity for Decentralized\n  Non-Convex Optimization: A Joint Gradient Estimation and Tracking Approach", "comments": null, "journal-ref": "Published at the International Conference on Machine Learning\n  (ICML 2020)", "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern large-scale machine learning problems benefit from decentralized\nand stochastic optimization. Recent works have shown that utilizing both\ndecentralized computing and local stochastic gradient estimates can outperform\nstate-of-the-art centralized algorithms, in applications involving highly\nnon-convex problems, such as training deep neural networks.\n  In this work, we propose a decentralized stochastic algorithm to deal with\ncertain smooth non-convex problems where there are $m$ nodes in the system, and\neach node has a large number of samples (denoted as $n$). Differently from the\nmajority of the existing decentralized learning algorithms for either\nstochastic or finite-sum problems, our focus is given to both reducing the\ntotal communication rounds among the nodes, while accessing the minimum number\nof local data samples. In particular, we propose an algorithm named D-GET\n(decentralized gradient estimation and tracking), which jointly performs\ndecentralized gradient estimation (which estimates the local gradient using a\nsubset of local samples) and gradient tracking (which tracks the global full\ngradient using local estimates). We show that, to achieve certain $\\epsilon$\nstationary solution of the deterministic finite sum problem, the proposed\nalgorithm achieves an $\\mathcal{O}(mn^{1/2}\\epsilon^{-1})$ sample complexity\nand an $\\mathcal{O}(\\epsilon^{-1})$ communication complexity. These bounds\nsignificantly improve upon the best existing bounds of\n$\\mathcal{O}(mn\\epsilon^{-1})$ and $\\mathcal{O}(\\epsilon^{-1})$, respectively.\nSimilarly, for online problems, the proposed method achieves an $\\mathcal{O}(m\n\\epsilon^{-3/2})$ sample complexity and an $\\mathcal{O}(\\epsilon^{-1})$\ncommunication complexity, while the best existing bounds are\n$\\mathcal{O}(m\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-2})$, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:57:33 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Sun", "Haoran", ""], ["Lu", "Songtao", ""], ["Hong", "Mingyi", ""]]}, {"id": "1910.05858", "submitter": "Ankur Mallick", "authors": "Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, T.\n  Yong-Jin Han", "title": "Deep Probabilistic Kernels for Sample-Efficient Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) with an appropriate kernel are known to provide\naccurate predictions and uncertainty estimates even with very small amounts of\nlabeled data. However, GPs are generally unable to learn a good representation\nthat can encode intricate structures in high dimensional data. The\nrepresentation power of GPs depends heavily on kernel functions used to\nquantify the similarity between data points. Traditional GP kernels are not\nvery effective at capturing similarity between high dimensional data points,\nwhile methods that use deep neural networks to learn a kernel are not\nsample-efficient. To overcome these drawbacks, we propose deep probabilistic\nkernels which use a probabilistic neural network to map high-dimensional data\nto a probability distribution in a low dimensional subspace, and leverage the\nrich work on kernels between distributions to capture the similarity between\nthese distributions. Experiments on a variety of datasets show that building a\nGP using this covariance kernel solves the conflicting problems of\nrepresentation learning and sample efficiency. Our model can be extended beyond\nGPs to other small-data paradigms such as few-shot classification where we show\ncompetitive performance with state-of-the-art models on the mini-Imagenet\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 23:10:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mallick", "Ankur", ""], ["Dwivedi", "Chaitanya", ""], ["Kailkhura", "Bhavya", ""], ["Joshi", "Gauri", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "1910.05862", "submitter": "Yuwei Wang", "authors": "Yuwei Wang, Yan Zheng, Yanqing Peng, Wei Zhang, Feifei Li", "title": "Feature Detection and Attenuation in Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding is one of the fundamental building blocks for data analysis tasks.\nAlthough most embedding schemes are designed to be domain-specific, they have\nbeen recently extended to represent various other research domains. However,\nthere are relatively few discussions on analyzing these generated embeddings,\nand removing undesired features from the embedding. In this paper, we first\npropose an innovative embedding analyzing method that quantitatively measures\nthe features in the embedding data. We then propose an unsupervised method to\nremove or alleviate undesired features in the embedding by applying Domain\nAdversarial Network (DAN). Our empirical results demonstrate that the proposed\nalgorithm has good performance on both industry and natural language processing\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 23:19:18 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wang", "Yuwei", ""], ["Zheng", "Yan", ""], ["Peng", "Yanqing", ""], ["Zhang", "Wei", ""], ["Li", "Feifei", ""]]}, {"id": "1910.05863", "submitter": "Wei Xie", "authors": "Wei Xie, Yuan Yi, Hua Zheng", "title": "Global-Local Metamodel Assisted Two-Stage Optimization via Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To integrate strategic, tactical and operational decisions, the two-stage\noptimization has been widely used to guide dynamic decision making. In this\npaper, we study the two-stage stochastic programming for complex systems with\nunknown response estimated by simulation. We introduce the global-local\nmetamodel assisted two-stage optimization via simulation that can efficiently\nemploy the simulation resource to iteratively solve for the optimal first- and\nsecond-stage decisions. Specifically, at each visited first-stage decision, we\ndevelop a local metamodel to simultaneously solve a set of scenario-based\nsecond-stage optimization problems, which also allows us to estimate the\noptimality gap. Then, we construct a global metamodel accounting for the errors\ninduced by: (1) using a finite number of scenarios to approximate the expected\nfuture cost occurring in the planning horizon, (2) second-stage optimality gap,\nand (3) finite visited first-stage decisions. Assisted by the global-local\nmetamodel, we propose a new simulation optimization approach that can\nefficiently and iteratively search for the optimal first- and second-stage\ndecisions. Our framework can guarantee the convergence of optimal solution for\nthe discrete two-stage optimization with unknown objective, and the empirical\nstudy indicates that it achieves substantial efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 23:29:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xie", "Wei", ""], ["Yi", "Yuan", ""], ["Zheng", "Hua", ""]]}, {"id": "1910.05872", "submitter": "Hankook Lee", "authors": "Hankook Lee, Sung Ju Hwang, Jinwoo Shin", "title": "Self-supervised Label Augmentation via Input Transformations", "comments": "Accepted to ICML 2020. Code available at\n  https://github.com/hankook/SLA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning, which learns by constructing artificial labels\ngiven only the input signals, has recently gained considerable attention for\nlearning representations with unlabeled datasets, i.e., learning without any\nhuman-annotated supervision. In this paper, we show that such a technique can\nbe used to significantly improve the model accuracy even under fully-labeled\ndatasets. Our scheme trains the model to learn both original and\nself-supervised tasks, but is different from conventional multi-task learning\nframeworks that optimize the summation of their corresponding losses. Our main\nidea is to learn a single unified task with respect to the joint distribution\nof the original and self-supervised labels, i.e., we augment original labels\nvia self-supervision of input transformation. This simple, yet effective\napproach allows to train models easier by relaxing a certain invariant\nconstraint during learning the original and self-supervised tasks\nsimultaneously. It also enables an aggregated inference which combines the\npredictions from different augmentations to improve the prediction accuracy.\nFurthermore, we propose a novel knowledge transfer technique, which we refer to\nas self-distillation, that has the effect of the aggregated inference in a\nsingle (faster) inference. We demonstrate the large accuracy improvement and\nwide applicability of our framework on various fully-supervised settings, e.g.,\nthe few-shot and imbalanced classification scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 00:37:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 12:10:28 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "Hankook", ""], ["Hwang", "Sung Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1910.05874", "submitter": "Yeonjong Shin", "authors": "Yeonjong Shin", "title": "Effects of Depth, Width, and Initialization: A Convergence Analysis of\n  Layer-wise Training for Deep Linear Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks have been used in various machine learning applications\nand achieved tremendous empirical successes. However, training deep neural\nnetworks is a challenging task. Many alternatives have been proposed in place\nof end-to-end back-propagation. Layer-wise training is one of them, which\ntrains a single layer at a time, rather than trains the whole layers\nsimultaneously. In this paper, we study a layer-wise training using a block\ncoordinate gradient descent (BCGD) for deep linear networks. We establish a\ngeneral convergence analysis of BCGD and found the optimal learning rate, which\nresults in the fastest decrease in the loss. More importantly, the optimal\nlearning rate can directly be applied in practice, as it does not require any\nprior knowledge. Thus, tuning the learning rate is not needed at all. Also, we\nidentify the effects of depth, width, and initialization in the training\nprocess. We show that when the orthogonal-like initialization is employed, the\nwidth of intermediate layers plays no role in gradient-based training, as long\nas the width is greater than or equal to both the input and output dimensions.\nWe show that under some conditions, the deeper the network is, the faster the\nconvergence is guaranteed. This implies that in an extreme case, the global\noptimum is achieved after updating each weight matrix only once. Besides, we\nfound that the use of deep networks could drastically accelerate convergence\nwhen it is compared to those of a depth 1 network, even when the computational\ncost is considered. Numerical examples are provided to justify our theoretical\nfindings and demonstrate the performance of layer-wise training by BCGD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 00:50:55 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 21:21:37 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Shin", "Yeonjong", ""]]}, {"id": "1910.05876", "submitter": "Jonathan Lebensold", "authors": "Jonathan Lebensold, William Hamilton, Borja Balle, Doina Precup", "title": "Actor Critic with Differentially Private Critic", "comments": "6 Pages, Presented at the Privacy in Machine Learning Workshop,\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning algorithms are known to be sample inefficient, and\noften performance on one task can be substantially improved by leveraging\ninformation (e.g., via pre-training) on other related tasks. In this work, we\npropose a technique to achieve such knowledge transfer in cases where agent\ntrajectories contain sensitive or private information, such as in the\nhealthcare domain. Our approach leverages a differentially private policy\nevaluation algorithm to initialize an actor-critic model and improve the\neffectiveness of learning in downstream tasks. We empirically show this\ntechnique increases sample efficiency in resource-constrained control problems\nwhile preserving the privacy of trajectories collected in an upstream task.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:19:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Lebensold", "Jonathan", ""], ["Hamilton", "William", ""], ["Balle", "Borja", ""], ["Precup", "Doina", ""]]}, {"id": "1910.05877", "submitter": "Dimitrios Kollias", "authors": "Valentin Richer and Dimitrios Kollias", "title": "Interpretable Deep Neural Networks for Facial Expression and Dimensional\n  Emotion Recognition in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we created a database with two types of annotations used in\nthe emotion recognition domain : Action Units and Valence Arousal to try to\nachieve better results than with only one model. The originality of the\napproach is also based on the type of architecture used to perform the\nprediction of the emotions : a categorical Generative Adversarial Network. This\nkind of dual network can generate images based on the pictures from the new\ndataset thanks to its generative network and decide if an image is fake or real\nthanks to its discriminative network as well as help to predict the annotations\nfor Action Units and Valence Arousal due to its categorical nature. GANs were\ntrained on the Action Units model only, then the Valence Arousal model only and\nthen on both the Action Units model and Valence Arousal model in order to test\ndifferent parameters and understand their influence. The generative and\ndiscriminative aspects of the GANs have performed interesting results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:33:06 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:17:07 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Richer", "Valentin", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05885", "submitter": "Lucas A. Wilson", "authors": "Pei Yang, Srinivas Varadharajan, Lucas A. Wilson, Don D. Smith II,\n  John A Lockman III, Vineet Gundecha, Quy Ta", "title": "Parallelized Training of Restricted Boltzmann Machines using\n  Markov-Chain Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a generative stochastic neural network\nthat can be applied to collaborative filtering technique used by recommendation\nsystems. Prediction accuracy of the RBM model is usually better than that of\nother models for recommendation systems. However, training the RBM model\ninvolves Markov-Chain Monte Carlo (MCMC) method, which is computationally\nexpensive. In this paper, we have successfully applied distributed parallel\ntraining using Horovod framework to improve the training time of the RBM model.\nOur tests show that the distributed training approach of the RBM model has a\ngood scaling efficiency. We also show that this approach effectively reduces\nthe training time to little over 12 minutes on 64 CPU nodes compared to 5 hours\non a single CPU node. This will make RBM models more practically applicable in\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:50:57 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Yang", "Pei", ""], ["Varadharajan", "Srinivas", ""], ["Wilson", "Lucas A.", ""], ["Smith", "Don D.", "II"], ["Lockman", "John A", "III"], ["Gundecha", "Vineet", ""], ["Ta", "Quy", ""]]}, {"id": "1910.05895", "submitter": "Julian Salazar", "authors": "Toan Q. Nguyen and Julian Salazar", "title": "Transformers without Tears: Improving the Normalization of\n  Self-Attention", "comments": "Accepted to IWSLT 2019 (oral); code is available at\n  https://github.com/tnq177/transformers_without_tears", "journal-ref": null, "doi": "10.5281/zenodo.3525484", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate three simple, normalization-centric changes to improve\nTransformer training. First, we show that pre-norm residual connections\n(PreNorm) and smaller initializations enable warmup-free, validation-based\ntraining with large learning rates. Second, we propose $\\ell_2$ normalization\nwith a single scale parameter (ScaleNorm) for faster training and better\nperformance. Finally, we reaffirm the effectiveness of normalizing word\nembeddings to a fixed length (FixNorm). On five low-resource translation pairs\nfrom TED Talks-based corpora, these changes always converge, giving an average\n+1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on\nIWSLT'15 English-Vietnamese. We observe sharper performance curves, more\nconsistent gradient norms, and a linear relationship between activation scaling\nand decoder depth. Surprisingly, in the high-resource setting (WMT'14\nEnglish-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 02:23:43 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 03:53:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nguyen", "Toan Q.", ""], ["Salazar", "Julian", ""]]}, {"id": "1910.05897", "submitter": "Haichuan Yang", "authors": "Haichuan Yang, Shupeng Gui, Yuhao Zhu, Ji Liu", "title": "Automatic Neural Network Compression by Sparsity-Quantization Joint\n  Learning: A Constrained Optimization-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are applied in a wide range of usecases. There is\nan increased demand for deploying DNNs on devices that do not have abundant\nresources such as memory and computation units. Recently, network compression\nthrough a variety of techniques such as pruning and quantization have been\nproposed to reduce the resource requirement. A key parameter that all existing\ncompression techniques are sensitive to is the compression ratio (e.g., pruning\nsparsity, quantization bitwidth) of each layer. Traditional solutions treat the\ncompression ratios of each layer as hyper-parameters, and tune them using human\nheuristic. Recent researchers start using black-box hyper-parameter\noptimizations, but they will introduce new hyper-parameters and have efficiency\nissue. In this paper, we propose a framework to jointly prune and quantize the\nDNNs automatically according to a target model size without using any\nhyper-parameters to manually set the compression ratio for each layer. In the\nexperiments, we show that our framework can compress the weights data of\nResNet-50 to be 836$\\times$ smaller without accuracy loss on CIFAR-10, and\ncompress AlexNet to be 205$\\times$ smaller without accuracy loss on ImageNet\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:00:58 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 15:29:24 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 17:14:44 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 07:07:09 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yang", "Haichuan", ""], ["Gui", "Shupeng", ""], ["Zhu", "Yuhao", ""], ["Liu", "Ji", ""]]}, {"id": "1910.05898", "submitter": "Junxiang Chen", "authors": "Junxiang Chen, Kayhan Batmanghelich", "title": "Robust Ordinal VAE: Employing Noisy Pairwise Comparisons for\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Locatello et al. (2018) has shown that an inductive bias is\nrequired to disentangle factors of interest in Variational Autoencoder (VAE).\nMotivated by a real-world problem, we propose a setting where such bias is\nintroduced by providing pairwise ordinal comparisons between instances, based\non the desired factor to be disentangled. For example, a doctor compares pairs\nof patients based on the level of severity of their illnesses, and the desired\nfactor is a quantitive level of the disease severity. In a real-world\napplication, the pairwise comparisons are usually noisy. Our method, Robust\nOrdinal VAE (ROVAE), incorporates the noisy pairwise ordinal comparisons in the\ndisentanglement task. We introduce non-negative random variables in ROVAE, such\nthat it can automatically determine whether each pairwise ordinal comparison is\ntrustworthy and ignore the noisy comparisons. Experimental results demonstrate\nthat ROVAE outperforms existing methods and is more robust to noisy pairwise\ncomparisons in both benchmark datasets and a real-world application.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:01:46 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chen", "Junxiang", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "1910.05905", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Xinwei Sun, Zhiyong Yang, Xiaochun Cao, Qingming Huang,\n  Yuan Yao", "title": "iSplit LBI: Individualized Partial Ranking with Ties via Split LBI", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the inherent uncertainty of data, the problem of predicting partial\nranking from pairwise comparison data with ties has attracted increasing\ninterest in recent years. However, in real-world scenarios, different\nindividuals often hold distinct preferences. It might be misleading to merely\nlook at a global partial ranking while ignoring personal diversity. In this\npaper, instead of learning a global ranking which is agreed with the consensus,\nwe pursue the tie-aware partial ranking from an individualized perspective.\nParticularly, we formulate a unified framework which not only can be used for\nindividualized partial ranking prediction, but also be helpful for abnormal\nuser selection. This is realized by a variable splitting-based algorithm called\n\\ilbi. Specifically, our algorithm generates a sequence of estimations with a\nregularization path, where both the hyperparameters and model parameters are\nupdated. At each step of the path, the parameters can be decomposed into three\northogonal parts, namely, abnormal signals, personalized signals and random\nnoise. The abnormal signals can serve the purpose of abnormal user selection,\nwhile the abnormal signals and personalized signals together are mainly\nresponsible for individual partial ranking prediction. Extensive experiments on\nsimulated and real-world datasets demonstrate that our new approach\nsignificantly outperforms state-of-the-art alternatives. The code is now\navailiable at https://github.com/qianqianxu010/NeurIPS2019-iSplitLBI.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:25:51 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xu", "Qianqian", ""], ["Sun", "Xinwei", ""], ["Yang", "Zhiyong", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1910.05927", "submitter": "Yuping Luo", "authors": "Kefan Dong, Yuping Luo, Tengyu Ma", "title": "On the Expressivity of Neural Networks for Deep Reinforcement Learning", "comments": "Accepted in ICML 2020. Title of previous version was \"Bootstrapping\n  the Expressivity with Model-based Planning\". Code is available at\n  https://github.com/roosephu/boots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the model-free reinforcement learning with the model-based\napproaches through the lens of the expressive power of neural networks for\npolicies, $Q$-functions, and dynamics. We show, theoretically and empirically,\nthat even for one-dimensional continuous state space, there are many MDPs whose\noptimal $Q$-functions and policies are much more complex than the dynamics. We\nhypothesize many real-world MDPs also have a similar property. For these MDPs,\nmodel-based planning is a favorable algorithm, because the resulting policies\ncan approximate the optimal policy significantly better than a neural network\nparameterization can, and model-free or model-based policy optimization rely on\npolicy parameterization. Motivated by the theory, we apply a simple multi-step\nmodel-based bootstrapping planner (BOOTS) to bootstrap a weak $Q$-function into\na stronger policy. Empirical results show that applying BOOTS on top of\nmodel-based or model-free policy optimization algorithms at the test time\nimproves the performance on MuJoCo benchmark tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 06:17:49 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 01:07:35 GMT"}, {"version": "v3", "created": "Sun, 6 Sep 2020 12:15:54 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dong", "Kefan", ""], ["Luo", "Yuping", ""], ["Ma", "Tengyu", ""]]}, {"id": "1910.05929", "submitter": "Stanislav Fort", "authors": "Stanislav Fort, Surya Ganguli", "title": "Emergent properties of the local geometry of neural loss landscapes", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local geometry of high dimensional neural network loss landscapes can\nboth challenge our cherished theoretical intuitions as well as dramatically\nimpact the practical success of neural network training. Indeed recent works\nhave observed 4 striking local properties of neural loss landscapes on\nclassification tasks: (1) the landscape exhibits exactly $C$ directions of high\npositive curvature, where $C$ is the number of classes; (2) gradient directions\nare largely confined to this extremely low dimensional subspace of positive\nHessian curvature, leaving the vast majority of directions in weight space\nunexplored; (3) gradient descent transiently explores intermediate regions of\nhigher positive curvature before eventually finding flatter minima; (4)\ntraining can be successful even when confined to low dimensional {\\it random}\naffine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of\nhigher than average curvature. We develop a simple theoretical model of\ngradients and Hessians, justified by numerical experiments on architectures and\ndatasets used in practice, that {\\it simultaneously} accounts for all $4$ of\nthese surprising and seemingly unrelated properties. Our unified model provides\nconceptual insights into the emergence of these properties and makes\nconnections with diverse topics in neural networks, random matrix theory, and\nspin glasses, including the neural tangent kernel, BBP phase transitions, and\nDerrida's random energy model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 06:23:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Fort", "Stanislav", ""], ["Ganguli", "Surya", ""]]}, {"id": "1910.05933", "submitter": "Ali Hassani", "authors": "Ali Hassani, Amir Iranmanesh, Mahdi Eftekhari, Abbas Salemi", "title": "DISCERN: Diversity-based Selection of Centroids for k-Estimation and\n  Rapid Non-stochastic Clustering", "comments": "Int. J. Mach. Learn. & Cyber. (2020)", "journal-ref": null, "doi": "10.1007/s13042-020-01193-5", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the applications of center-based clustering algorithms such as K-Means\nis partitioning data points into K clusters. In some examples, the feature\nspace relates to the underlying problem we are trying to solve, and sometimes\nwe can obtain a suitable feature space. Nevertheless, while K-Means is one of\nthe most efficient offline clustering algorithms, it is not equipped to\nestimate the number of clusters, which is useful in some practical cases. Other\npractical methods which do are simply too complex, as they require at least one\nrun of K-Means for each possible K. In order to address this issue, we propose\na K-Means initialization similar to K-Means++, which would be able to estimate\nK based on the feature space while finding suitable initial centroids for\nK-Means in a deterministic manner. Then we compare the proposed method,\nDISCERN, with a few of the most practical K estimation methods, while also\ncomparing clustering results of K-Means when initialized randomly, using\nK-Means++ and using DISCERN. The results show improvement in both the\nestimation and final clustering performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 06:45:41 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 19:58:57 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 07:13:15 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 04:41:16 GMT"}, {"version": "v5", "created": "Tue, 22 Sep 2020 06:32:51 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Hassani", "Ali", ""], ["Iranmanesh", "Amir", ""], ["Eftekhari", "Mahdi", ""], ["Salemi", "Abbas", ""]]}, {"id": "1910.05954", "submitter": "Quentin M\\'erigot", "authors": "Quentin M\\'erigot, Alex Delalande, Fr\\'ed\\'eric Chazal", "title": "Quantitative stability of optimal transport maps and linearization of\n  the 2-Wasserstein space", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.MG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies an explicit embedding of the set of probability measures\ninto a Hilbert space, defined using optimal transport maps from a reference\nprobability density. This embedding linearizes to some extent the 2-Wasserstein\nspace, and enables the direct use of generic supervised and unsupervised\nlearning algorithms on measure data. Our main result is that the embedding is\n(bi-)H\\\"older continuous, when the reference density is uniform over a convex\nset, and can be equivalently phrased as a dimension-independent\nH\\\"older-stability results for optimal transport maps.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:24:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["M\u00e9rigot", "Quentin", ""], ["Delalande", "Alex", ""], ["Chazal", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1910.05964", "submitter": "Riikka Huusari", "authors": "Riikka Huusari, C\\'ecile Capponi, Paul Villoutreix, Hachem Kadri", "title": "Kernel transfer over multiple views for missing data completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the kernel completion problem with the presence of multiple views\nin the data. In this context the data samples can be fully missing in some\nviews, creating missing columns and rows to the kernel matrices that are\ncalculated individually for each view. We propose to solve the problem of\ncompleting the kernel matrices by transferring the features of the other views\nto represent the view under consideration. We align the known part of the\nkernel matrix with a new kernel built from the features of the other views. We\nare thus able to find generalizable structures in the kernel under completion,\nand represent it accurately. Its missing values can be predicted with the data\navailable in other views. We illustrate the benefits of our approach with\nsimulated data and multivariate digits dataset, as well as with real biological\ndatasets from studies of pattern formation in early \\textit{Drosophila\nmelanogaster} embryogenesis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:45:12 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Huusari", "Riikka", ""], ["Capponi", "C\u00e9cile", ""], ["Villoutreix", "Paul", ""], ["Kadri", "Hachem", ""]]}, {"id": "1910.05983", "submitter": "Mohammed Sabry", "authors": "Mohammed Sabry and Amr M. A. Khalifa", "title": "On the Reduction of Variance and Overestimation of Deep Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The breakthrough of deep Q-Learning on different types of environments\nrevolutionized the algorithmic design of Reinforcement Learning to introduce\nmore stable and robust algorithms, to that end many extensions to deep\nQ-Learning algorithm have been proposed to reduce the variance of the target\nvalues and the overestimation phenomena. In this paper, we examine new\nmethodology to solve these issues, we propose using Dropout techniques on deep\nQ-Learning algorithm as a way to reduce variance and overestimation. We further\npresent experiments on some of the benchmark environments that demonstrate\nsignificant improvement of the stability of the performance and a reduction in\nvariance and overestimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:43:06 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sabry", "Mohammed", ""], ["Khalifa", "Amr M. A.", ""]]}, {"id": "1910.05992", "submitter": "Ryo Karakida", "authors": "Ryo Karakida, Shotaro Akaho, Shun-ichi Amari", "title": "Pathological spectra of the Fisher information metric and its variants\n  in deep neural networks", "comments": "23 pages, 7 figures; v2: minor improvements, Section 3.4 added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) plays an essential role in statistics and\nmachine learning as a Riemannian metric tensor or a component of the Hessian\nmatrix of loss functions. Focusing on the FIM and its variants in deep neural\nnetworks (DNNs), we reveal their characteristic scale dependence on the network\nwidth, depth and sample size when the network has random weights and is\nsufficiently wide. This study covers two widely-used FIMs for regression with\nlinear output and for classification with softmax output. Both FIMs\nasymptotically show pathological eigenvalue spectra in the sense that a small\nnumber of eigenvalues become large outliers depending the width or sample size\nwhile the others are much smaller. It implies that the local shape of the\nparameter space or loss landscape is very sharp in a few specific directions\nwhile almost flat in the other directions. In particular, the softmax output\ndisperses the outliers and makes a tail of the eigenvalue density spread from\nthe bulk. We also show that pathological spectra appear in other variants of\nFIMs: one is the neural tangent kernel; another is a metric for the input\nsignal and feature space that arises from feedforward signal propagation. Thus,\nwe provide a unified perspective on the FIM and its variants that will lead to\nmore quantitative understanding of learning in large-scale DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:54:56 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 08:31:59 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Karakida", "Ryo", ""], ["Akaho", "Shotaro", ""], ["Amari", "Shun-ichi", ""]]}, {"id": "1910.06000", "submitter": "Lifu Wang", "authors": "Lifu Wang, Bo Shen, Ning Zhao", "title": "Second-Order Convergence of Asynchronous Parallel Stochastic Gradient\n  Descent: When Is the Linear Speedup Achieved?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, asynchronous parallel stochastic gradient descent\n(APSGD) is broadly used to speed up the training process through multi-workers.\nMeanwhile, the time delay of stale gradients in asynchronous algorithms is\ngenerally proportional to the total number of workers, which brings additional\ndeviation from the accurate gradient due to using delayed gradients. This may\nhave a negative influence on the convergence of the algorithm. One may ask: How\nmany workers can we use at most to achieve a good convergence and the linear\nspeedup?\n  In this paper, we consider the second-order convergence of asynchronous\nalgorithms in non-convex optimization. We investigate the behaviors of APSGD\nwith consistent read near strictly saddle points and provide a theoretical\nguarantee that if the total number of workers is bounded by\n$\\widetilde{O}(K^{1/3}M^{-1/3})$ ($K$ is the total steps and $M$ is the\nmini-batch size), APSGD will converge to good stationary points ($||\\nabla\nf(x)||\\leq \\epsilon, \\nabla^2 f(x)\\succeq -\\sqrt{\\epsilon}\\bm{I},\n\\epsilon^2\\leq O(\\sqrt{\\frac{1}{MK}}) $) and the linear speedup is achieved.\nOur works give the first theoretical guarantee on the second-order convergence\nfor asynchronous algorithms. The technique we provide can be generalized to\nanalyze other types of asynchronous algorithms to understand the behaviors of\nasynchronous algorithms in distributed asynchronous parallel training.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:14:55 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 07:52:21 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 13:42:14 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 06:15:42 GMT"}, {"version": "v5", "created": "Sun, 31 May 2020 12:37:39 GMT"}, {"version": "v6", "created": "Sun, 7 Jun 2020 17:23:20 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Lifu", ""], ["Shen", "Bo", ""], ["Zhao", "Ning", ""]]}, {"id": "1910.06002", "submitter": "Kaito Ariu", "authors": "Kaito Ariu, Jungseul Ok, Alexandre Proutiere, Se-Young Yun", "title": "Optimal Clustering from Noisy Binary Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving large-scale labeling tasks with minimal\neffort put on the users. Examples of such tasks include those in some of the\nrecent CAPTCHA systems, where users clicks (binary answers) constitute the only\ndata available to label images. Specifically, we study the generic problem of\nclustering a set of items from binary user feedback. Items are grouped into\ninitially unknown non-overlapping clusters. To recover these clusters, the\nlearner sequentially presents to users a finite list of items together with a\nquestion with a binary answer selected from a fixed finite set. For each of\nthese items, the user provides a noisy answer whose expectation is determined\nby the item cluster and the question and by an item-specific parameter\ncharacterizing the {\\it hardness} of classifying the item. The objective is to\ndevise an algorithm with a minimal cluster recovery error rate. We derive\nproblem-specific information-theoretical lower bounds on the error rate\nsatisfied by any algorithm, for both uniform and adaptive (list, question)\nselection strategies. For uniform selection, we present a simple algorithm\nbuilt upon the K-means algorithm and whose performance almost matches the\nfundamental limits. For adaptive selection, we develop an adaptive algorithm\nthat is inspired by the derivation of the information-theoretical error lower\nbounds, and in turn allocates the budget in an efficient way. The algorithm\nlearns to select items hard to cluster and relevant questions more often. We\ncompare the performance of our algorithms with or without the adaptive\nselection strategy numerically and illustrate the gain achieved by being\nadaptive.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:18:26 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 03:01:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ariu", "Kaito", ""], ["Ok", "Jungseul", ""], ["Proutiere", "Alexandre", ""], ["Yun", "Se-Young", ""]]}, {"id": "1910.06031", "submitter": "Ali Ghadirzadeh", "authors": "Judith B\\\"utepage, Ali Ghadirzadeh, \\\"Ozge \\\"Oztimur Karada\\~g,\n  M{\\aa}rten Bj\\\"orkman and Danica Kragic", "title": "Imitating by generating: deep generative models for imitation of\n  interactive tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To coordinate actions with an interaction partner requires a constant\nexchange of sensorimotor signals. Humans acquire these skills in infancy and\nearly childhood mostly by imitation learning and active engagement with a\nskilled partner. They require the ability to predict and adapt to one's partner\nduring an interaction. In this work we want to explore these ideas in a\nhuman-robot interaction setting in which a robot is required to learn\ninteractive tasks from a combination of observational and kinesthetic learning.\nTo this end, we propose a deep learning framework consisting of a number of\ncomponents for (1) human and robot motion embedding, (2) motion prediction of\nthe human partner and (3) generation of robot joint trajectories matching the\nhuman motion. To test these ideas, we collect human-human interaction data and\nhuman-robot interaction data of four interactive tasks \"hand-shake\",\n\"hand-wave\", \"parachute fist-bump\" and \"rocket fist-bump\". We demonstrate\nexperimentally the importance of predictive and adaptive components as well as\nlow-level abstractions to successfully learn to imitate human behavior in\ninteractive social tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 10:42:50 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["B\u00fctepage", "Judith", ""], ["Ghadirzadeh", "Ali", ""], ["Karadag", "\u00d6zge \u00d6ztimur", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""], ["Kragic", "Danica", ""]]}, {"id": "1910.06044", "submitter": "Lixu Wang", "authors": "Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu", "title": "Eavesdrop the Composition Proportion of Training Labels in Federated\n  Learning", "comments": "15 pages, 10 figures, security conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as a new form of collaborative\nmachine learning, where a common model can be learned while keeping all the\ntraining data on local devices. Although it is designed for enhancing the data\nprivacy, we demonstrated in this paper a new direction in inference attacks in\nthe context of FL, where valuable information about training data can be\nobtained by adversaries with very limited power. In particular, we proposed\nthree new types of attacks to exploit this vulnerability. The first type of\nattack, Class Sniffing, can detect whether a certain label appears in training.\nThe other two types of attacks can determine the quantity of each label, i.e.,\nQuantity Inference attack determines the composition proportion of the training\nlabel owned by the selected clients in a single round, while Whole\nDetermination attack determines that of the whole training process. We\nevaluated our attacks on a variety of tasks and datasets with different\nsettings, and the corresponding results showed that our attacks work well\ngenerally. Finally, we analyzed the impact of major hyper-parameters to our\nattacks and discussed possible defenses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:26:10 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 03:21:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Lixu", ""], ["Xu", "Shichao", ""], ["Wang", "Xiao", ""], ["Zhu", "Qi", ""]]}, {"id": "1910.06054", "submitter": "Julian Zimmert", "authors": "Julian Zimmert and Yevgeny Seldin", "title": "An Optimal Algorithm for Adversarial Bandits with Arbitrary Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for adversarial multi-armed bandits with\nunrestricted delays. The algorithm is based on a novel hybrid regularizer\napplied in the Follow the Regularized Leader (FTRL) framework. It achieves\n$\\mathcal{O}(\\sqrt{kn}+\\sqrt{D\\log(k)})$ regret guarantee, where $k$ is the\nnumber of arms, $n$ is the number of rounds, and $D$ is the total delay. The\nresult matches the lower bound within constants and requires no prior knowledge\nof $n$ or $D$. Additionally, we propose a refined tuning of the algorithm,\nwhich achieves $\\mathcal{O}(\\sqrt{kn}+\\min_{S}|S|+\\sqrt{D_{\\bar S}\\log(k)})$\nregret guarantee, where $S$ is a set of rounds excluded from delay counting,\n$\\bar S = [n]\\setminus S$ are the counted rounds, and $D_{\\bar S}$ is the total\ndelay in the counted rounds. If the delays are highly unbalanced, the latter\nregret guarantee can be significantly tighter than the former. The result\nrequires no advance knowledge of the delays and resolves an open problem of\nThune et al. (2019). The new FTRL algorithm and its refined tuning are anytime\nand require no doubling, which resolves another open problem of Thune et al.\n(2019).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:48:21 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 09:20:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zimmert", "Julian", ""], ["Seldin", "Yevgeny", ""]]}, {"id": "1910.06078", "submitter": "Fangli Xu", "authors": "Fangli Xu, Lingfei Wu, KP Thai, Carol Hsu, Wei Wang, Richard Tong", "title": "MUTLA: A Large-Scale Dataset for Multimodal Teaching and Learning\n  Analytics", "comments": "3 pages, 1 figure, 2 tables workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of teacher and student interactions could be very\nimportant to improve the quality of teaching and student engagement. However,\ndespite some recent progress in utilizing multimodal data for teaching and\nlearning analytics, a thorough analysis of a rich multimodal dataset coming for\na complex real learning environment has yet to be done. To bridge this gap, we\npresent a large-scale MUlti-modal Teaching and Learning Analytics (MUTLA)\ndataset. This dataset includes time-synchronized multimodal data records of\nstudents (learning logs, videos, EEG brainwaves) as they work in various\nsubjects from Squirrel AI Learning System (SAIL) to solve problems of varying\ndifficulty levels. The dataset resources include user records from the learner\nrecords store of SAIL, brainwave data collected by EEG headset devices, and\nvideo data captured by web cameras while students worked in the SAIL products.\nOur hope is that by analyzing real-world student learning activities, facial\nexpressions, and brainwave patterns, researchers can better predict engagement,\nwhich can then be used to improve adaptive learning selection and student\nlearning outcomes. An additional goal is to provide a dataset gathered from\nreal-world educational activities versus those from controlled lab environments\nto benefit the educational learning community.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 03:53:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xu", "Fangli", ""], ["Wu", "Lingfei", ""], ["Thai", "KP", ""], ["Hsu", "Carol", ""], ["Wang", "Wei", ""], ["Tong", "Richard", ""]]}, {"id": "1910.06100", "submitter": "Irfan Al-Hussaini", "authors": "Irfan Al-Hussaini, Cao Xiao, M. Brandon Westover, Jimeng Sun", "title": "SLEEPER: interpretable Sleep staging via Prototypes from Expert Rules", "comments": "Machine Learning for Healthcare Conference (MLHC) 2019. Proceedings\n  of Machine Learning Research 106", "journal-ref": "PMLR 106:721-739, 2019", "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep staging is a crucial task for diagnosing sleep disorders. It is tedious\nand complex as it can take a trained expert several hours to annotate just one\npatient's polysomnogram (PSG) from a single night. Although deep learning\nmodels have demonstrated state-of-the-art performance in automating sleep\nstaging, interpretability which defines other desiderata, has largely remained\nunexplored. In this study, we propose Sleep staging via Prototypes from Expert\nRules (SLEEPER), which combines deep learning models with expert defined rules\nusing a prototype learning framework to generate simple interpretable models.\nIn particular, SLEEPER utilizes sleep scoring rules and expert defined features\nto derive prototypes which are embeddings of PSG data fragments via\nconvolutional neural networks. The final models are simple interpretable models\nlike a shallow decision tree defined over those phenotypes. We evaluated\nSLEEPER using two PSG datasets collected from sleep studies and demonstrated\nthat SLEEPER could provide accurate sleep stage classification comparable to\nhuman experts and deep neural networks with about 85% ROC-AUC and .7 kappa.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:37:38 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Al-Hussaini", "Irfan", ""], ["Xiao", "Cao", ""], ["Westover", "M. Brandon", ""], ["Sun", "Jimeng", ""]]}, {"id": "1910.06121", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Aki Vehtari, Pekka Marttinen", "title": "Batch simulations and uncertainty quantification in Gaussian process\n  surrogate approximate Bayesian computation", "comments": "Minor improvements and clarifications to the text over the previous\n  version. 20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational efficiency of approximate Bayesian computation (ABC) has\nbeen improved by using surrogate models such as Gaussian processes (GP). In one\nsuch promising framework the discrepancy between the simulated and observed\ndata is modelled with a GP which is further used to form a model-based\nestimator for the intractable posterior. In this article we improve this\napproach in several ways. We develop batch-sequential Bayesian experimental\ndesign strategies to parallellise the expensive simulations. In earlier work\nonly sequential strategies have been used. Current surrogate-based ABC methods\nalso do not fully account the uncertainty due to the limited budget of\nsimulations as they output only a point estimate of the ABC posterior. We\npropose a numerical method to fully quantify the uncertainty in, for example,\nABC posterior moments. We also provide some new analysis on the GP modelling\nassumptions in the resulting improved framework called Bayesian ABC and discuss\nits connection to Bayesian quadrature (BQ) and Bayesian optimisation (BO).\nExperiments with toy and real-world simulation models demonstrate advantages of\nthe proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:01:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 12:35:02 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 12:46:52 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1910.06134", "submitter": "Jen Ning Lim", "authors": "Jen Ning Lim, Makoto Yamada, Wittawat Jitkrittum, Yoshikazu Terada,\n  Shigeyuki Matsui, Hidetoshi Shimodaira", "title": "More Powerful Selective Kernel Tests for Feature Selection", "comments": "Accepted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refining one's hypotheses in the light of data is a common scientific\npractice; however, the dependency on the data introduces selection bias and can\nlead to specious statistical analysis. An approach for addressing this is via\nconditioning on the selection procedure to account for how we have used the\ndata to generate our hypotheses, and prevent information to be used again after\nselection. Many selective inference (a.k.a. post-selection inference)\nalgorithms typically take this approach but will \"over-condition\" for sake of\ntractability. While this practice yields well calibrated statistic tests with\ncontrolled false positive rates (FPR), it can incur a major loss in power. In\nour work, we extend two recent proposals for selecting features using the\nMaximum Mean Discrepancy and Hilbert Schmidt Independence Criterion to\ncondition on the minimal conditioning event. We show how recent advances in\nmultiscale bootstrap makes conditioning on the minimal selection event possible\nand demonstrate our proposal over a range of synthetic and real world\nexperiments. Our results show that our proposed test is indeed more powerful in\nmost scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:28:22 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 12:05:58 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lim", "Jen Ning", ""], ["Yamada", "Makoto", ""], ["Jitkrittum", "Wittawat", ""], ["Terada", "Yoshikazu", ""], ["Matsui", "Shigeyuki", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1910.06149", "submitter": "Yujia Ding", "authors": "Yujia Ding and Weiqing Gu", "title": "Accelerometer-Based Gait Segmentation: Simultaneously User and Adversary\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new gait segmentation method based on\naccelerometer data and develop a new distance function between two time series,\nshowing novel and effectiveness in simultaneously identifying user and\nadversary. Comparing with the normally used Neural Network methods, our\napproaches use geometric features to extract walking cycles more precisely and\nemploy a new similarity metric to conduct user-adversary identification. This\nnew technology for simultaneously identify user and adversary contributes to\ncybersecurity beyond user-only identification. In particular, the new\ntechnology is being applied to cell phone recorded walking data and performs an\naccuracy of $98.79\\%$ for 6 classes classification (user-adversary\nidentification) and $99.06\\%$ for binary classification (user only\nidentification). In addition to walking signal, our approach works on walking\nup, walking down and mixed walking signals. This technology is feasible for\nboth large and small data set, overcoming the current challenges facing to\nNeural Networks such as tuning large number of hyper-parameters for large data\nsets and lacking of training data for small data sets. In addition, the new\ndistance function developed here can be applied in any signal analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:45:30 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ding", "Yujia", ""], ["Gu", "Weiqing", ""]]}, {"id": "1910.06153", "submitter": "Ravinath Kausik", "authors": "Augustin Prado, Ravinath Kausik, Lalitha Venkataramanan", "title": "Dual Neural Network Architecture for Determining Epistemic and Aleatoric\n  Uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been shown to be extremely effective for\nvarious classification and regression problems, but quantifying the uncertainty\nof their predictions and separating them into the epistemic and aleatoric\nfractions is still considered challenging. In oil and gas exploration projects,\ntools consisting of seismic, sonic, magnetic resonance, resistivity, dielectric\nand/or nuclear sensors are sent downhole through boreholes to probe the earth's\nrock and fluid properties. The measurements from these tools are used to build\nreservoir models that are subsequently used for estimation and optimization of\nhydrocarbon production. Machine learning algorithms are often used to estimate\nthe rock and fluid properties from the measured downhole data. Quantifying\nuncertainties of these properties is crucial for rock and fluid evaluation and\nsubsequent reservoir optimization and production decisions. These machine\nlearning algorithms are often trained on a \"ground-truth\" or core database.\nDuring the inference phase which involves application of these algorithms to\nfield data, it is critical that the machine learning algorithm flag data as out\nof distribution from new geologies that the model was not trained upon. It is\nalso highly important to be sensitive to heteroscedastic aleatoric noise in the\nfeature space arising from the combination of tool and geological conditions.\nUnderstanding the source of the uncertainty and reducing them is key to\ndesigning intelligent tools and applications such as automated log\ninterpretation answer products for exploration and field development. In this\npaper we describe a methodology consisting of a system of dual networks\ncomprising of the combination of a Bayesian Neural Network (BNN) and an\nArtificial Neural Network (ANN) addressing this challenge for geophysical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:00:29 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Prado", "Augustin", ""], ["Kausik", "Ravinath", ""], ["Venkataramanan", "Lalitha", ""]]}, {"id": "1910.06205", "submitter": "Maximilian Soelch", "authors": "Adnan Akhundov, Maximilian Soelch, Justin Bayer, Patrick van der Smagt", "title": "Variational Tracking and Prediction with Generative Disentangled\n  State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address tracking and prediction of multiple moving objects in visual data\nstreams as inference and sampling in a disentangled latent state-space model.\nBy encoding objects separately and including explicit position information in\nthe latent state space, we perform tracking via amortized variational Bayesian\ninference of the respective latent positions. Inference is implemented in a\nmodular neural framework tailored towards our disentangled latent space.\nGenerative and inference model are jointly learned from observations only.\nComparing to related prior work, we empirically show that our Markovian\nstate-space assumption enables faithful and much improved long-term prediction\nwell beyond the training horizon. Further, our inference model correctly\ndecomposes frames into objects, even in the presence of occlusions. Tracking\nperformance is increased significantly over prior art.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:20:30 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Akhundov", "Adnan", ""], ["Soelch", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1910.06222", "submitter": "Jiaming Song", "authors": "Jiaming Song and Stefano Ermon", "title": "Understanding the Limitations of Variational Mutual Information\n  Estimators", "comments": "Fixed some typos, credit to Yilun Xu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approaches based on neural networks are showing promise for\nestimating mutual information (MI) between high dimensional variables. However,\nthey can be difficult to use in practice due to poorly understood bias/variance\ntradeoffs. We theoretically show that, under some conditions, estimators such\nas MINE exhibit variance that could grow exponentially with the true amount of\nunderlying MI. We also empirically demonstrate that existing estimators fail to\nsatisfy basic self-consistency properties of MI, such as data processing and\nadditivity under independence. Based on a unified perspective of variational\napproaches, we develop a new estimator that focuses on variance reduction.\nEmpirical results on standard benchmark tasks demonstrate that our proposed\nestimator exhibits improved bias-variance trade-offs on standard benchmark\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:45:21 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 04:40:12 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.06239", "submitter": "Matthias Kirchler", "authors": "Matthias Kirchler, Shahryar Khorasani, Marius Kloft, Christoph Lippert", "title": "Two-sample Testing Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample testing procedure based on learned deep neural\nnetwork representations. To this end, we define two test statistics that\nperform an asymptotic location test on data samples mapped onto a hidden layer.\nThe tests are consistent and asymptotically control the type-1 error rate.\nTheir test statistics can be evaluated in linear time (in the sample size).\nSuitable data representations are obtained in a data-driven way, by solving a\nsupervised or unsupervised transfer-learning task on an auxiliary (potentially\ndistinct) data set. If no auxiliary data is available, we split the data into\ntwo chunks: one for learning representations and one for computing the test\nstatistic. In experiments on audio samples, natural images and\nthree-dimensional neuroimaging data our tests yield significant decreases in\ntype-2 error rate (up to 35 percentage points) compared to state-of-the-art\ntwo-sample tests such as kernel-methods and classifier two-sample tests.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:16:58 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:01:53 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Kirchler", "Matthias", ""], ["Khorasani", "Shahryar", ""], ["Kloft", "Marius", ""], ["Lippert", "Christoph", ""]]}, {"id": "1910.06243", "submitter": "Adam Derek Cobb", "authors": "Adam D. Cobb, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Andrew Markham, Stephen\n  J. Roberts", "title": "Introducing an Explicit Symplectic Integration Scheme for Riemannian\n  Manifold Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recent symplectic integration scheme derived for solving\nphysically motivated systems with non-separable Hamiltonians. We show its\nrelevance to Riemannian manifold Hamiltonian Monte Carlo (RMHMC) and provide an\nalternative to the currently used generalised leapfrog symplectic integrator,\nwhich relies on solving multiple fixed point iterations to convergence. Via\nthis approach, we are able to reduce the number of higher-order derivative\ncalculations per leapfrog step. We explore the implications of this integrator\nand demonstrate its efficacy in reducing the computational burden of RMHMC. Our\ncode is provided in a new open-source Python package, hamiltorch.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:20:06 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Cobb", "Adam D.", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Markham", "Andrew", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1910.06259", "submitter": "David Stutz", "authors": "David Stutz, Matthias Hein, Bernt Schiele", "title": "Confidence-Calibrated Adversarial Training: Generalizing to Unseen\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training yields robust models against a specific threat model,\ne.g., $L_\\infty$ adversarial examples. Typically robustness does not generalize\nto previously unseen threat models, e.g., other $L_p$ norms, or larger\nperturbations. Our confidence-calibrated adversarial training (CCAT) tackles\nthis problem by biasing the model towards low confidence predictions on\nadversarial examples. By allowing to reject examples with low confidence,\nrobustness generalizes beyond the threat model employed during training. CCAT,\ntrained only on $L_\\infty$ adversarial examples, increases robustness against\nlarger $L_\\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal\nadversarial examples and corrupted examples and yields better clean accuracy\ncompared to adversarial training. For thorough evaluation we developed novel\nwhite- and black-box attacks directly attacking CCAT by maximizing confidence.\nFor each threat model, we use $7$ attacks with up to $50$ restarts and $5000$\niterations and report worst-case robust test error, extended to our\nconfidence-thresholded setting, across all attacks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:38:03 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:34:42 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 16:15:44 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 12:03:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Stutz", "David", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1910.06324", "submitter": "Fengpei Li", "authors": "Henry Lam, Fengpei Li, Siddharth Prusty", "title": "Robust Importance Weighting for Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning problems, the training and testing data follow different\ndistributions and a particularly common situation is the \\textit{covariate\nshift}. To correct for sampling biases, most approaches, including the popular\nkernel mean matching (KMM), focus on estimating the importance weights between\nthe two distributions. Reweighting-based methods, however, are exposed to high\nvariance when the distributional discrepancy is large and the weights are\npoorly estimated. On the other hand, the alternate approach of using\nnonparametric regression (NR) incurs high bias when the training size is\nlimited. In this paper, we propose and analyze a new estimator that\nsystematically integrates the residuals of NR with KMM reweighting, based on a\ncontrol-variate perspective. The proposed estimator can be shown to either\nstrictly outperform or match the best-known existing rates for both KMM and NR,\nand thus is a robust combination of both estimators. The experiments shows the\nestimator works well in practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:20:04 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 22:23:46 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Lam", "Henry", ""], ["Li", "Fengpei", ""], ["Prusty", "Siddharth", ""]]}, {"id": "1910.06358", "submitter": "Christopher Frye", "authors": "Christopher Frye, Colin Rowat, Ilya Feige", "title": "Asymmetric Shapley values: incorporating causal knowledge into\n  model-agnostic explainability", "comments": "To appear in NeurIPS 2020; 9 pages, 2 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining AI systems is fundamental both to the development of high\nperforming models and to the trust placed in them by their users. The Shapley\nframework for explainability has strength in its general applicability combined\nwith its precise, rigorous foundation: it provides a common, model-agnostic\nlanguage for AI explainability and uniquely satisfies a set of intuitive\nmathematical axioms. However, Shapley values are too restrictive in one\nsignificant regard: they ignore all causal structure in the data. We introduce\na less restrictive framework, Asymmetric Shapley values (ASVs), which are\nrigorously founded on a set of axioms, applicable to any AI system, and\nflexible enough to incorporate any causal structure known to be respected by\nthe data. We demonstrate that ASVs can (i) improve model explanations by\nincorporating causal information, (ii) provide an unambiguous test for unfair\ndiscrimination in model predictions, (iii) enable sequentially incremental\nexplanations in time-series models, and (iv) support feature-selection studies\nwithout the need for model retraining.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:08:32 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 19:15:21 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Frye", "Christopher", ""], ["Rowat", "Colin", ""], ["Feige", "Ilya", ""]]}, {"id": "1910.06366", "submitter": "Lijun Sun Mr", "authors": "Xinyu Chen and Lijun Sun", "title": "Bayesian Temporal Factorization for Multidimensional Time Series\n  Prediction", "comments": "15 pages, 9 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3066551", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale and multidimensional spatiotemporal data sets are becoming\nubiquitous in many real-world applications such as monitoring urban traffic and\nair quality. Making predictions on these time series has become a critical\nchallenge due to not only the large-scale and high-dimensional nature but also\nthe considerable amount of missing data. In this paper, we propose a Bayesian\ntemporal factorization (BTF) framework for modeling multidimensional time\nseries -- in particular spatiotemporal data -- in the presence of missing\nvalues. By integrating low-rank matrix/tensor factorization and vector\nautoregressive (VAR) process into a single probabilistic graphical model, this\nframework can characterize both global and local consistencies in large-scale\ntime series data. The graphical model allows us to effectively perform\nprobabilistic predictions and produce uncertainty estimates without imputing\nthose missing values. We develop efficient Gibbs sampling algorithms for model\ninference and model updating for real-time prediction and test the proposed BTF\nframework on several real-world spatiotemporal data sets for both missing data\nimputation and multi-step rolling prediction tasks. The numerical experiments\ndemonstrate the superiority of the proposed BTF approaches over existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:21:33 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:24:39 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Xinyu", ""], ["Sun", "Lijun", ""]]}, {"id": "1910.06368", "submitter": "Yichong Xu", "authors": "Yichong Xu, Xi Chen, Aarti Singh and Artur Dubrawski", "title": "Thresholding Bandit Problem with Both Duels and Pulls", "comments": "15 pages, 8 figures; The 23rd International Conference on Artificial\n  Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Thresholding Bandit Problem (TBP) aims to find the set of arms with mean\nrewards greater than a given threshold. We consider a new setting of TBP, where\nin addition to pulling arms, one can also \\emph{duel} two arms and get the arm\nwith a greater mean. In our motivating application from crowdsourcing, dueling\ntwo arms can be more cost-effective and time-efficient than direct pulls. We\nrefer to this problem as TBP with Dueling Choices (TBP-DC). This paper provides\nan algorithm called Rank-Search (RS) for solving TBP-DC by alternating between\nranking and binary search. We prove theoretical guarantees for RS, and also\ngive lower bounds to show the optimality of it. Experiments show that RS\noutperforms previous baseline algorithms that only use pulls or duels.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:24:21 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 21:25:04 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Xu", "Yichong", ""], ["Chen", "Xi", ""], ["Singh", "Aarti", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1910.06378", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J.\n  Reddi, Sebastian U. Stich, Ananda Theertha Suresh", "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "comments": "v2 contains analysis of FedAvg, non-convex rates of Scaffold, and\n  experimental evaluation. v3 fixes typos, ICML version. v4 slightly improves\n  rate of SCAFFOLD for general convex functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Averaging (FedAvg) has emerged as the algorithm of choice for\nfederated learning due to its simplicity and low communication cost. However,\nin spite of recent research efforts, its performance is not fully understood.\nWe obtain tight convergence rates for FedAvg and prove that it suffers from\n`client-drift' when the data is heterogeneous (non-iid), resulting in unstable\nand slow convergence.\n  As a solution, we propose a new algorithm (SCAFFOLD) which uses control\nvariates (variance reduction) to correct for the `client-drift' in its local\nupdates. We prove that SCAFFOLD requires significantly fewer communication\nrounds and is not affected by data heterogeneity or client sampling. Further,\nwe show that (for quadratics) SCAFFOLD can take advantage of similarity in the\nclient's data yielding even faster convergence. The latter is the first result\nto quantify the usefulness of local-steps in distributed optimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:49:20 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 15:57:57 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 23:37:28 GMT"}, {"version": "v4", "created": "Fri, 9 Apr 2021 16:21:59 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["Kale", "Satyen", ""], ["Mohri", "Mehryar", ""], ["Reddi", "Sashank J.", ""], ["Stich", "Sebastian U.", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1910.06392", "submitter": "Muhammad Usman", "authors": "Muhammad Usman and Jeong A Lee", "title": "AFP-CKSAAP: Prediction of Antifreeze Proteins Using Composition of\n  k-Spaced Amino Acid Pairs with Deep Neural Network", "comments": "Accepted for oral presentation at 19th 2019 IEEE International\n  Conference on Bioinformatics and Bioengineering (IC-BIBE 2019) Copyright (c)\n  2019 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antifreeze proteins (AFPs) are the sub-set of ice binding proteins\nindispensable for the species living in extreme cold weather. These proteins\nbind to the ice crystals, hindering their growth into large ice lattice that\ncould cause physical damage. There are variety of AFPs found in numerous\norganisms and due to the heterogeneous sequence characteristics, AFPs are found\nto demonstrate a high degree of diversity, which makes their prediction a\nchallenging task. Herein, we propose a machine learning framework to deal with\nthis vigorous and diverse prediction problem using the manifolding learning\nthrough composition of k-spaced amino acid pairs. We propose to use the deep\nneural network with skipped connection and ReLU non-linearity to learn the\nnon-linear mapping of protein sequence descriptor and class label. The proposed\nantifreeze protein prediction method called AFP-CKSAAP has shown to outperform\nthe contemporary methods, achieving excellent prediction scores on standard\ndataset. The main evaluater for the performance of the proposed method in this\nstudy is Youden's index whose high value is dependent on both sensitivity and\nspecificity. In particular, AFP-CKSAAP yields a Youden's index value of 0.82 on\nthe independent dataset, which is better than previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:13:14 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Usman", "Muhammad", ""], ["Lee", "Jeong A", ""]]}, {"id": "1910.06403", "submitter": "Maximilian Balandat", "authors": "Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton,\n  Benjamin Letham, Andrew Gordon Wilson, Eytan Bakshy", "title": "BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization provides sample-efficient global optimization for a\nbroad range of applications, including automatic machine learning, engineering,\nphysics, and experimental design. We introduce BoTorch, a modern programming\nframework for Bayesian optimization that combines Monte-Carlo (MC) acquisition\nfunctions, a novel sample average approximation optimization approach,\nauto-differentiation, and variance reduction techniques. BoTorch's modular\ndesign facilitates flexible specification and optimization of probabilistic\nmodels written in PyTorch, simplifying implementation of new acquisition\nfunctions. Our approach is backed by novel theoretical convergence results and\nmade practical by a distinctive algorithmic foundation that leverages fast\npredictive distributions, hardware acceleration, and deterministic\noptimization. We also propose a novel \"one-shot\" formulation of the Knowledge\nGradient, enabled by a combination of our theoretical and software\ncontributions. In experiments, we demonstrate the improved sample efficiency of\nBoTorch relative to other popular libraries.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:11:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:28:05 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 19:31:38 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Balandat", "Maximilian", ""], ["Karrer", "Brian", ""], ["Jiang", "Daniel R.", ""], ["Daulton", "Samuel", ""], ["Letham", "Benjamin", ""], ["Wilson", "Andrew Gordon", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1910.06419", "submitter": "Paavo Parmas", "authors": "Paavo Parmas and Masashi Sugiyama", "title": "A unified view of likelihood ratio and reparameterization gradients and\n  an optimal importance sampling scheme", "comments": "8 pages + 19 pages appendix. Preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reparameterization (RP) and likelihood ratio (LR) gradient estimators are\nused throughout machine and reinforcement learning; however, they are usually\nexplained as simple mathematical tricks without providing any insight into\ntheir nature. We use a first principles approach to explain LR and RP, and show\na connection between the two via the divergence theorem. The theory motivated\nus to derive optimal importance sampling schemes to reduce LR gradient\nvariance. Our newly derived distributions have analytic probability densities\nand can be directly sampled from. The improvement for Gaussian target\ndistributions was modest, but for other distributions such as a Beta\ndistribution, our method could lead to arbitrarily large improvements, and was\ncrucial to obtain competitive performance in evolution strategies experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:59:13 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Parmas", "Paavo", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.06444", "submitter": "Joseph Xu", "authors": "Joseph Z. Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, Valeriya Zaytseva", "title": "Building Damage Detection in Satellite Imagery Using Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In all types of disasters, from earthquakes to armed conflicts, aid workers\nneed accurate and timely data such as damage to buildings and population\ndisplacement to mount an effective response. Remote sensing provides this data\nat an unprecedented scale, but extracting operationalizable information from\nsatellite images is slow and labor-intensive. In this work, we use machine\nlearning to automate the detection of building damage in satellite imagery. We\ncompare the performance of four different convolutional neural network models\nin detecting damaged buildings in the 2010 Haiti earthquake. We also quantify\nhow well the models will generalize to future disasters by training and testing\nmodels on different disaster events.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:03:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Xu", "Joseph Z.", ""], ["Lu", "Wenhan", ""], ["Li", "Zebo", ""], ["Khaitan", "Pranav", ""], ["Zaytseva", "Valeriya", ""]]}, {"id": "1910.06456", "submitter": "Shaika Chowdhury", "authors": "Shaika Chowdhury, Chenwei Zhang, Philip S.Yu and Yuan Luo", "title": "Mixed Pooling Multi-View Attention Autoencoder for Representation\n  Learning in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations have been used to support downstream tasks in\nhealthcare recently. Healthcare data (e.g., electronic health records) contain\nmultiple modalities of data from heterogeneous sources that can provide\ncomplementary information, alongside an added dimension to learning\npersonalized patient representations. To this end, in this paper we propose a\nnovel unsupervised encoder-decoder model, namely Mixed Pooling Multi-View\nAttention Autoencoder (MPVAA), that generates patient representations\nencapsulating a holistic view of their medical profile. Specifically, by first\nlearning personalized graph embeddings pertaining to each patient's\nheterogeneous healthcare data, it then integrates the non-linear relationships\namong them into a unified representation through multi-view attention\nmechanism. Additionally, a mixed pooling strategy is incorporated in the\nencoding step to learn diverse information specific to each data modality.\nExperiments conducted for multiple tasks demonstrate the effectiveness of the\nproposed model over the state-of-the-art representation learning methods in\nhealthcare.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:59:51 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Chowdhury", "Shaika", ""], ["Zhang", "Chenwei", ""], ["Yu", "Philip S.", ""], ["Luo", "Yuan", ""]]}, {"id": "1910.06464", "submitter": "Cristina Garbacea", "authors": "Cristina G\\^arbacea, A\\\"aron van den Oord, Yazhe Li, Felicia S C Lim,\n  Alejandro Luebs, Oriol Vinyals, Thomas C Walters", "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "comments": "ICASSP 2019", "journal-ref": "ICASSP 2019-2019 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), pp. 735-739. IEEE, 2019", "doi": "10.1109/ICASSP.2019.8683277", "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to efficiently transmit and store speech signals, speech codecs\ncreate a minimally redundant representation of the input signal which is then\ndecoded at the receiver with the best possible perceptual quality. In this work\nwe demonstrate that a neural network architecture based on VQ-VAE with a\nWaveNet decoder can be used to perform very low bit-rate speech coding with\nhigh reconstruction quality. A prosody-transparent and speaker-independent\nmodel trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits\nperceptual quality which is around halfway between the MELP codec at 2.4 kbps\nand AMR-WB codec at 23.05 kbps. In addition, when training on high-quality\nrecorded speech with the test speaker included in the training set, a model\ncoding speech at 1.6 kbps produces output of similar perceptual quality to that\ngenerated by AMR-WB at 23.05 kbps.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 23:54:08 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["G\u00e2rbacea", "Cristina", ""], ["Oord", "A\u00e4ron van den", ""], ["Li", "Yazhe", ""], ["Lim", "Felicia S C", ""], ["Luebs", "Alejandro", ""], ["Vinyals", "Oriol", ""], ["Walters", "Thomas C", ""]]}, {"id": "1910.06489", "submitter": "Sneha Aenugu", "authors": "Sneha Aenugu, Abhishek Sharma, Sasikiran Yelamarthi, Hananel Hazan,\n  Philip S. Thomas, Robert Kozma", "title": "Reinforcement learning with a network of spiking agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientific theory suggests that dopaminergic neurons broadcast global\nreward prediction errors to large areas of the brain influencing the synaptic\nplasticity of the neurons in those regions. We build on this theory to propose\na multi-agent learning framework with spiking neurons in the generalized linear\nmodel (GLM) formulation as agents, to solve reinforcement learning (RL) tasks.\nWe show that a network of GLM spiking agents connected in a hierarchical\nfashion, where each spiking agent modulates its firing policy based on local\ninformation and a global prediction error, can learn complex action\nrepresentations to solve RL tasks. We further show how leveraging principles of\nmodularity and population coding inspired from the brain can help reduce\nvariance in the learning updates making it a viable optimization technique.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:27:18 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 15:12:39 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 22:19:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Aenugu", "Sneha", ""], ["Sharma", "Abhishek", ""], ["Yelamarthi", "Sasikiran", ""], ["Hazan", "Hananel", ""], ["Thomas", "Philip S.", ""], ["Kozma", "Robert", ""]]}, {"id": "1910.06508", "submitter": "Yao Liu", "authors": "Yao Liu, Pierre-Luc Bacon, Emma Brunskill", "title": "Understanding the Curse of Horizon in Off-Policy Evaluation via\n  Conditional Importance Sampling", "comments": "Accepted by ICML 2020, 21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy policy estimators that use importance sampling (IS) can suffer\nfrom high variance in long-horizon domains, and there has been particular\nexcitement over new IS methods that leverage the structure of Markov decision\nprocesses. We analyze the variance of the most popular approaches through the\nviewpoint of conditional Monte Carlo. Surprisingly, we find that in finite\nhorizon MDPs there is no strict variance reduction of per-decision importance\nsampling or stationary importance sampling, comparing with vanilla importance\nsampling. We then provide sufficient conditions under which the per-decision or\nstationary estimators will provably reduce the variance over importance\nsampling with finite horizons. For the asymptotic (in terms of horizon $T$)\ncase, we develop upper and lower bounds on the variance of those estimators\nwhich yields sufficient conditions under which there exists an exponential v.s.\npolynomial gap between the variance of importance sampling and that of the\nper-decision or stationary estimators. These results help advance our\nunderstanding of if and when new types of IS estimators will improve the\naccuracy of off-policy estimation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:35:30 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 01:09:35 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Yao", ""], ["Bacon", "Pierre-Luc", ""], ["Brunskill", "Emma", ""]]}, {"id": "1910.06509", "submitter": "Kaixuan Zhang", "authors": "Kaixuan Zhang, Qinglong Wang, Xue Liu, C. Lee Giles", "title": "Shapley Homology: Topological Analysis of Sample Influence for Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data samples collected for training machine learning models are typically\nassumed to be independent and identically distributed (iid). Recent research\nhas demonstrated that this assumption can be problematic as it simplifies the\nmanifold of structured data. This has motivated different research areas such\nas data poisoning, model improvement, and explanation of machine learning\nmodels. In this work, we study the influence of a sample on determining the\nintrinsic topological features of its underlying manifold. We propose the\nShapley Homology framework, which provides a quantitative metric for the\ninfluence of a sample of the homology of a simplicial complex. By interpreting\nthe influence as a probability measure, we further define an entropy which\nreflects the complexity of the data manifold. Our empirical studies show that\nwhen using the 0-dimensional homology, on neighboring graphs, samples with\nhigher influence scores have more impact on the accuracy of neural networks for\ndetermining the graph connectivity and on several regular grammars whose higher\nentropy values imply more difficulty in being learned.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:40:45 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhang", "Kaixuan", ""], ["Wang", "Qinglong", ""], ["Liu", "Xue", ""], ["Giles", "C. Lee", ""]]}, {"id": "1910.06513", "submitter": "Xiangyi Chen", "authors": "Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong,\n  David Cox", "title": "ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive momentum method (AdaMM), which uses past gradients to update\ndescent directions and learning rates simultaneously, has become one of the\nmost popular first-order optimization methods for solving machine learning\nproblems. However, AdaMM is not suited for solving black-box optimization\nproblems, where explicit gradient forms are difficult or infeasible to obtain.\nIn this paper, we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm, that\ngeneralizes AdaMM to the gradient-free regime. We show that the convergence\nrate of ZO-AdaMM for both convex and nonconvex optimization is roughly a factor\nof $O(\\sqrt{d})$ worse than that of the first-order AdaMM algorithm, where $d$\nis problem size. In particular, we provide a deep understanding on why\nMahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type\nmethods. As a byproduct, our analysis makes the first step toward understanding\nadaptive learning rate methods for nonconvex constrained optimization.\nFurthermore, we demonstrate two applications, designing per-image and universal\nadversarial attacks from black-box neural networks, respectively. We perform\nextensive experiments on ImageNet and empirically show that ZO-AdaMM converges\nmuch faster to a solution of high accuracy compared with $6$ state-of-the-art\nZO optimization methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:54:11 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 02:27:28 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chen", "Xiangyi", ""], ["Liu", "Sijia", ""], ["Xu", "Kaidi", ""], ["Li", "Xingguo", ""], ["Lin", "Xue", ""], ["Hong", "Mingyi", ""], ["Cox", "David", ""]]}, {"id": "1910.06521", "submitter": "Chelsea Sidrane", "authors": "Chelsea Sidrane, Dylan J Fitzpatrick, Andrew Annex, Diane O'Donoghue,\n  Yarin Gal, Piotr Bili\\'nski", "title": "Machine Learning for Generalizable Prediction of Flood Susceptibility", "comments": "Will be presented at hadri.ai 2019, a workshop at NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flooding is a destructive and dangerous hazard and climate change appears to\nbe increasing the frequency of catastrophic flooding events around the world.\nPhysics-based flood models are costly to calibrate and are rarely generalizable\nacross different river basins, as model outputs are sensitive to site-specific\nparameters and human-regulated infrastructure. In contrast, statistical models\nimplicitly account for such factors through the data on which they are trained.\nSuch models trained primarily from remotely-sensed Earth observation data could\nreduce the need for extensive in-situ measurements. In this work, we develop\ngeneralizable, multi-basin models of river flooding susceptibility using\ngeographically-distributed data from the USGS stream gauge network. Machine\nlearning models are trained in a supervised framework to predict two measures\nof flood susceptibility from a mix of river basin attributes, impervious\nsurface cover information derived from satellite imagery, and historical\nrecords of rainfall and stream height. We report prediction performance of\nmultiple models using precision-recall curves, and compare with performance of\nnaive baselines. This work on multi-basin flood prediction represents a step in\nthe direction of making flood prediction accessible to all at-risk communities.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 04:31:39 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Sidrane", "Chelsea", ""], ["Fitzpatrick", "Dylan J", ""], ["Annex", "Andrew", ""], ["O'Donoghue", "Diane", ""], ["Gal", "Yarin", ""], ["Bili\u0144ski", "Piotr", ""]]}, {"id": "1910.06532", "submitter": "Bingcong Li", "authors": "Bingcong Li, Georgios B. Giannakis", "title": "Adaptive Step Sizes in Variance Reduction via Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this work is equipping convex and nonconvex problems with\nBarzilai-Borwein (BB) step size. With the adaptivity of BB step sizes granted,\nthey can fail when the objective function is not strongly convex. To overcome\nthis challenge, the key idea here is to bridge (non)convex problems and\nstrongly convex ones via regularization. The proposed regularization schemes\nare \\textit{simple} yet effective. Wedding the BB step size with a variance\nreduction method, known as SARAH, offers a free lunch compared with vanilla\nSARAH in convex problems. The convergence of BB step sizes in nonconvex\nproblems is also established and its complexity is no worse than other adaptive\nstep sizes such as AdaGrad. As a byproduct, our regularized SARAH methods for\nconvex functions ensure that the complexity to find $\\mathbb{E}[\\| \\nabla\nf(\\mathbf{x}) \\|^2]\\leq \\epsilon$ is ${\\cal O}\\big(\n(n+\\frac{1}{\\sqrt{\\epsilon}})\\ln{\\frac{1}{\\epsilon}}\\big)$, improving\n$\\epsilon$ dependence over existing results. Numerical tests further validate\nthe merits of proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:24:32 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Li", "Bingcong", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1910.06535", "submitter": "Tianyu Li", "authors": "Tianyu Li, Chien-Chih Wang, Yukun Ma, Patricia Ortal, Qifang Zhao,\n  Bjorn Stenger, Yu Hirate", "title": "Learning Classifiers on Positive and Unlabeled Data with Policy Gradient", "comments": "Proceedings of the IEEE International Conference on Data Mining, pp.\n  309-408, Beijing, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing algorithms aiming to learn a binary classifier from positive (P) and\nunlabeled (U) data generally require estimating the class prior or label noises\nahead of building a classification model. However, the estimation and\nclassifier learning are normally conducted in a pipeline instead of being\njointly optimized. In this paper, we propose to alternatively train the two\nsteps using reinforcement learning. Our proposal adopts a policy network to\nadaptively make assumptions on the labels of unlabeled data, while a classifier\nis built upon the output of the policy network and provides rewards to learn a\nbetter strategy. The dynamic and interactive training between the policy maker\nand the classifier can exploit the unlabeled data in a more effective manner\nand yield a significant improvement on the classification performance.\nFurthermore, we present two different approaches to represent the actions\nsampled from the policy. The first approach considers continuous actions as\nsoft labels, while the other uses discrete actions as hard assignment of labels\nfor unlabeled examples.We validate the effectiveness of the proposed method on\ntwo benchmark datasets as well as one e-commerce dataset. The result shows the\nproposed method is able to consistently outperform state-of-the-art methods in\nvarious settings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:34:23 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 11:04:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Tianyu", ""], ["Wang", "Chien-Chih", ""], ["Ma", "Yukun", ""], ["Ortal", "Patricia", ""], ["Zhao", "Qifang", ""], ["Stenger", "Bjorn", ""], ["Hirate", "Yu", ""]]}, {"id": "1910.06539", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou and Jacob Hinkle and M. Todd Young and David\n  Womble", "title": "Challenges in Markov chain Monte Carlo for Bayesian neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in\nBayesian neural networks (BNNs). This paper initially reviews the main\nchallenges in sampling from the parameter posterior of a neural network via\nMCMC. Such challenges culminate to lack of convergence to the parameter\nposterior. Nevertheless, this paper shows that a non-converged Markov chain,\ngenerated via MCMC sampling from the parameter space of a neural network, can\nyield via Bayesian marginalization a valuable predictive posterior of the\noutput of the neural network. Classification examples based on multilayer\nperceptrons showcase highly accurate predictive posteriors. The postulate of\nlimited scope for MCMC developments in BNNs is partially valid; an\nasymptotically exact parameter posterior seems less plausible, yet an accurate\npredictive posterior is a tenable research avenue.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:43:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 18:02:21 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 03:37:55 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2021 10:39:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Hinkle", "Jacob", ""], ["Young", "M. Todd", ""], ["Womble", "David", ""]]}, {"id": "1910.06540", "submitter": "Jasper Sebastiaan Wijnands", "authors": "Jasper S. Wijnands, Jason Thompson, Kerry A. Nice, Gideon D. P. A.\n  Aschwanden, Mark Stevenson", "title": "Real-time monitoring of driver drowsiness on mobile platforms using 3D\n  neural networks", "comments": "13 pages, 2 figures, 'Online First' version. For associated mp4\n  files, see journal website", "journal-ref": "Neural Computing and Applications (2019)", "doi": "10.1007/s00521-019-04506-0", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driver drowsiness increases crash risk, leading to substantial road trauma\neach year. Drowsiness detection methods have received considerable attention,\nbut few studies have investigated the implementation of a detection approach on\na mobile phone. Phone applications reduce the need for specialised hardware and\nhence, enable a cost-effective roll-out of the technology across the driving\npopulation. While it has been shown that three-dimensional (3D) operations are\nmore suitable for spatiotemporal feature learning, current methods for\ndrowsiness detection commonly use frame-based, multi-step approaches. However,\ncomputationally expensive techniques that achieve superior results on action\nrecognition benchmarks (e.g. 3D convolutions, optical flow extraction) create\nbottlenecks for real-time, safety-critical applications on mobile devices.\nHere, we show how depthwise separable 3D convolutions, combined with an early\nfusion of spatial and temporal information, can achieve a balance between high\nprediction accuracy and real-time inference requirements. In particular,\nincreased accuracy is achieved when assessment requires motion information, for\nexample, when sunglasses conceal the eyes. Further, a custom TensorFlow-based\nsmartphone application shows the true impact of various approaches on inference\ntimes and demonstrates the effectiveness of real-time monitoring based on\nout-of-sample data to alert a drowsy driver. Our model is pre-trained on\nImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness\nDetection dataset. Fine-tuning on large naturalistic driving datasets could\nfurther improve accuracy to obtain robust in-vehicle performance. Overall, our\nresearch is a step towards practical deep learning applications, potentially\npreventing micro-sleeps and reducing road trauma.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:44:28 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wijnands", "Jasper S.", ""], ["Thompson", "Jason", ""], ["Nice", "Kerry A.", ""], ["Aschwanden", "Gideon D. P. A.", ""], ["Stevenson", "Mark", ""]]}, {"id": "1910.06548", "submitter": "Zissis Poulos", "authors": "Zissis Poulos, Ali Nouri, Andreas Moshovos", "title": "Training CNNs faster with Dynamic Input and Kernel Downsampling", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce training time in convolutional networks (CNNs) with a method that,\nfor some of the mini-batches: a) scales down the resolution of input images via\ndownsampling, and b) reduces the forward pass operations via pooling on the\nconvolution filters. Training is performed in an interleaved fashion; some\nbatches undergo the regular forward and backpropagation passes with original\nnetwork parameters, whereas others undergo a forward pass with pooled filters\nand downsampled inputs. Since pooling is differentiable, the gradients of the\npooled filters propagate to the original network parameters for a standard\nparameter update. The latter phase requires fewer floating point operations and\nless storage due to the reduced spatial dimensions in feature maps and filters.\nThe key idea is that this phase leads to smaller and approximate updates and\nthus slower learning, but at significantly reduced cost, followed by passes\nthat use the original network parameters as a refinement stage. Deciding how\noften and for which batches the downsmapling occurs can be done either\nstochastically or deterministically, and can be defined as a training\nhyperparameter itself. Experiments on residual architectures show that we can\nachieve up to 23% reduction in training time with minimal loss in validation\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 06:18:29 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Poulos", "Zissis", ""], ["Nouri", "Ali", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1910.06552", "submitter": "Masaaki Imaizumi", "authors": "Akiyoshi Sannai, Masaaki Imaizumi, Makoto Kawano", "title": "Improved Generalization Bounds of Group Invariant / Equivariant Deep\n  Networks via Quotient Feature Spaces", "comments": "Old title: \"Improved Generalization Bound of Permutation Invariant\n  Deep Neural Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous invariant (or equivariant) neural networks have succeeded in\nhandling invariant data such as point clouds and graphs. However, a\ngeneralization theory for the neural networks has not been well developed,\nbecause several essential factors for the theory, such as network size and\nmargin distribution, are not deeply connected to the invariance and\nequivariance. In this study, we develop a novel generalization error bound for\ninvariant and equivariant deep neural networks. To describe the effect of\ninvariance and equivariance on generalization, we develop a notion of a\n\\textit{quotient feature space}, which measures the effect of group actions for\nthe properties. Our main result proves that the volume of quotient feature\nspaces can describe the generalization error. Furthermore, the bound shows that\nthe invariance and equivariance significantly improve the leading term of the\nbound. We apply our result to specific invariant and equivariant networks, such\nas DeepSets (Zaheer et al. (2017)), and show that their generalization bound is\nconsiderably improved by $\\sqrt{n!}$, where $n!$ is the number of permutations.\nWe also discuss the expressive power of invariant DNNs and show that they can\nachieve an optimal approximation rate. Our experimental result supports our\ntheoretical claims.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 06:24:53 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 12:56:51 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 15:53:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sannai", "Akiyoshi", ""], ["Imaizumi", "Masaaki", ""], ["Kawano", "Makoto", ""]]}, {"id": "1910.06562", "submitter": "Cheng-En Wu", "authors": "Steven C.Y. Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming\n  Chan, Chu-Song Chen", "title": "Compacting, Picking and Growing for Unforgetting Continual Learning", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual lifelong learning is essential to many applications. In this paper,\nwe propose a simple but effective approach to continual deep learning. Our\napproach leverages the principles of deep model compression, critical weights\nselection, and progressive networks expansion. By enforcing their integration\nin an iterative manner, we introduce an incremental learning method that is\nscalable to the number of sequential tasks in a continual learning process. Our\napproach is easy to implement and owns several favorable characteristics.\nFirst, it can avoid forgetting (i.e., learn new tasks while remembering all\nprevious tasks). Second, it allows model expansion but can maintain the model\ncompactness when handling sequential tasks. Besides, through our compaction and\nselection/expansion mechanism, we show that the knowledge accumulated through\nlearning previous tasks is helpful to build a better model for the new tasks\ncompared to training the models independently with tasks. Experimental results\nshow that our approach can incrementally learn a deep model tackling multiple\ntasks without forgetting, while the model compactness is maintained with the\nperformance more satisfiable than individual task training.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:02:01 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 03:44:22 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 06:29:48 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Hung", "Steven C. Y.", ""], ["Tu", "Cheng-Hao", ""], ["Wu", "Cheng-En", ""], ["Chen", "Chien-Hung", ""], ["Chan", "Yi-Ming", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1910.06569", "submitter": "Pablo Martinez Olmos", "authors": "Fernando Perez-Cruz, Pablo M. Olmos, Michael Minyi Zhang, and Howard\n  Huang", "title": "Probabilistic Time of Arrival Localization", "comments": "IEEE Signal Processing Letters, 2019", "journal-ref": null, "doi": "10.1109/LSP.2019.2944005", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a new approach for time of arrival geo-localization.\nWe show that the main sources of error in metropolitan areas are due to\nenvironmental imperfections that bias our solutions, and that we can rely on a\nprobabilistic model to learn and compensate for them. The resulting\nlocalization error is validated using measurements from a live LTE cellular\nnetwork to be less than 10 meters, representing an order-of-magnitude\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:43:53 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Perez-Cruz", "Fernando", ""], ["Olmos", "Pablo M.", ""], ["Zhang", "Michael Minyi", ""], ["Huang", "Howard", ""]]}, {"id": "1910.06588", "submitter": "Yuanyuan Wei", "authors": "Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh", "title": "MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and\n  Local Outliers", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a technique in data mining that aims to detect unusual\nor unexpected records in the dataset. Existing outlier detection algorithms\nhave different pros and cons and exhibit different sensitivity to noisy data\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\ndetection algorithm named MSD-Kmeans that combines the statistical method of\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\nK-means to detect outliers more accurately with the better control of extreme\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\napplying MSD algorithm to eliminate as many noisy data to minimize the\ninterference on clusters, and (2) applying K-means algorithm to obtain local\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\nin the context of detecting possible overcharging of taxi fares, as greedy\ndishonest drivers may attempt to charge high fares by detouring. We compare the\nperformance indicators of MSD-Kmeans with those of other outlier detection\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\nand efficient outlier detection on data of varying quality on IoT devices.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:24:16 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wei", "Yuanyuan", ""], ["Jang-Jaccard", "Julian", ""], ["Sabrina", "Fariza", ""], ["McIntosh", "Timothy", ""]]}, {"id": "1910.06591", "submitter": "Rapha\\\"el Marinier", "authors": "Lasse Espeholt, Rapha\\\"el Marinier, Piotr Stanczyk, Ke Wang, Marcin\n  Michalski", "title": "SEED RL: Scalable and Efficient Deep-RL with Accelerated Central\n  Inference", "comments": "New version that includes changes made during the ICLR 2020 review\n  process (https://openreview.net/forum?id=rkgvXlrKwH)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a modern scalable reinforcement learning agent called SEED\n(Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we\nshow that it is not only possible to train on millions of frames per second but\nalso to lower the cost of experiments compared to current methods. We achieve\nthis with a simple architecture that features centralized inference and an\noptimized communication layer. SEED adopts two state of the art distributed\nalgorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is\nevaluated on Atari-57, DeepMind Lab and Google Research Football. We improve\nthe state of the art on Football and are able to reach state of the art on\nAtari-57 three times faster in wall-time. For the scenarios we consider, a 40%\nto 80% cost reduction for running experiments is achieved. The implementation\nalong with experiments is open-sourced so results can be reproduced and novel\nideas tried out.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:32:45 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 16:08:46 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Espeholt", "Lasse", ""], ["Marinier", "Rapha\u00ebl", ""], ["Stanczyk", "Piotr", ""], ["Wang", "Ke", ""], ["Michalski", "Marcin", ""]]}, {"id": "1910.06611", "submitter": "Imanol Schlag", "authors": "Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic,\n  J\\\"urgen Schmidhuber, Jianfeng Gao", "title": "Enhancing the Transformer with Explicit Relational Encoding for Math\n  Problem Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We incorporate Tensor-Product Representations within the Transformer in order\nto better support the explicit representation of relation structure. Our\nTensor-Product Transformer (TP-Transformer) sets a new state of the art on the\nrecently-introduced Mathematics Dataset containing 56 categories of free-form\nmath word-problems. The essential component of the model is a novel attention\nmechanism, called TP-Attention, which explicitly encodes the relations between\neach Transformer cell and the other cells from which values have been retrieved\nby attention. TP-Attention goes beyond linear combination of retrieved values,\nstrengthening representation-building and resolving ambiguities introduced by\nmultiple layers of standard attention. The TP-Transformer's attention maps give\nbetter insights into how it is capable of solving the Mathematics Dataset's\nchallenging problems. Pretrained models and code will be made available after\npublication.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:19:55 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:28:24 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Schlag", "Imanol", ""], ["Smolensky", "Paul", ""], ["Fernandez", "Roland", ""], ["Jojic", "Nebojsa", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1910.06640", "submitter": "Carlos Ruiz", "authors": "Andr\\'es M. Alonso, F. Javier Nogales and Carlos Ruiz", "title": "A Single Scalable LSTM Model for Short-Term Forecasting of Disaggregated\n  Electricity Loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most electricity systems worldwide are deploying advanced metering\ninfrastructures to collect relevant operational data. In particular, smart\nmeters allow tracking electricity load consumption at a very disaggregated\nlevel and at high frequency rates. This data opens the possibility of\ndeveloping new forecasting models with a potential positive impact in\nelectricity systems. We present a general methodology that is able to process\nand forecast a large number of smart meter time series. Instead of using\ntraditional and univariate approaches, we develop a single but complex\nrecurrent neural-network model with long short-term memory that can capture\nindividual consumption patterns and also consumptions from different\nhouseholds. The resulting model can accurately predict future loads\n(short-term) of individual consumers, even if these were not included in the\noriginal training set. This entails a great potential for large scale\napplications as once the single network is trained, accurate individual\nforecast for new consumers can be obtained at almost no computational cost. The\nproposed model is tested under a large set of numerical experiments by using a\nreal-world dataset with thousands of disaggregated electricity consumption time\nseries. Furthermore, we explore how geo-demographic segmentation of consumers\nmay impact the forecasting accuracy of the model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 10:33:34 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:42:47 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Alonso", "Andr\u00e9s M.", ""], ["Nogales", "F. Javier", ""], ["Ruiz", "Carlos", ""]]}, {"id": "1910.06673", "submitter": "Tessa Van Der Heiden", "authors": "Tessa van der Heiden, Naveen Shankar Nagaraja, Christian Weiss,\n  Efstratios Gavves", "title": "SafeCritic: Collision-Aware Trajectory Prediction", "comments": "To Appear as workshop paper for the British Machine Vision Conference\n  (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Navigating complex urban environments safely is a key to realize fully\nautonomous systems. Predicting future locations of vulnerable road users, such\nas pedestrians and cyclists, thus, has received a lot of attention in the\nrecent years. While previous works have addressed modeling interactions with\nthe static (obstacles) and dynamic (humans) environment agents, we address an\nimportant gap in trajectory prediction. We propose SafeCritic, a model that\nsynergizes generative adversarial networks for generating multiple \"real\"\ntrajectories with reinforcement learning to generate \"safe\" trajectories. The\nDiscriminator evaluates the generated candidates on whether they are consistent\nwith the observed inputs. The Critic network is environmentally aware to prune\ntrajectories that are in collision or are in violation with the environment.\nThe auto-encoding loss stabilizes training and prevents mode-collapse. We\ndemonstrate results on two large scale data sets with a considerable\nimprovement over state-of-the-art. We also show that the Critic is able to\nclassify the safety of trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:15:19 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["van der Heiden", "Tessa", ""], ["Nagaraja", "Naveen Shankar", ""], ["Weiss", "Christian", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1910.06705", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Sanghyuk Chun, Sangdoo Yun, Jung-Woo Ha, Jaejun Yoo", "title": "Neural Approximation of an Auto-Regressive Process through Confidence\n  Guided Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic confidence-based approximation that can be plugged in\nand simplify the auto-regressive generation process with a proved convergence.\nWe first assume that the priors of future samples can be generated in an\nindependently and identically distributed (i.i.d.) manner using an efficient\npredictor. Given the past samples and future priors, the mother AR model can\npost-process the priors while the accompanied confidence predictor decides\nwhether the current sample needs a resampling or not. Thanks to the i.i.d.\nassumption, the post-processing can update each sample in a parallel way, which\nremarkably accelerates the mother model. Our experiments on different data\ndomains including sequences and images show that the proposed method can\nsuccessfully capture the complex structures of the data and generate the\nmeaningful future samples with lower computational cost while preserving the\nsequential relationship of the data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:11:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Chun", "Sanghyuk", ""], ["Yun", "Sangdoo", ""], ["Ha", "Jung-Woo", ""], ["Yoo", "Jaejun", ""]]}, {"id": "1910.06717", "submitter": "Kenton Murray", "authors": "Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer,\n  David Chiang", "title": "Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and\n  Performance for Low-Resource Machine Translation", "comments": "The 3rd Workshop on Neural Generation and Translation (WNGT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence models, particularly the Transformer, are the\nstate of the art in machine translation. Yet these neural networks are very\nsensitive to architecture and hyperparameter settings. Optimizing these\nsettings by grid or random search is computationally expensive because it\nrequires many training runs. In this paper, we incorporate architecture search\ninto a single training run through auto-sizing, which uses regularization to\ndelete neurons in a network over the course of training. On very low-resource\nlanguage pairs, we show that auto-sizing can improve BLEU scores by up to 3.9\npoints while removing one-third of the parameters from the model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 19:21:26 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Murray", "Kenton", ""], ["Kinnison", "Jeffery", ""], ["Nguyen", "Toan Q.", ""], ["Scheirer", "Walter", ""], ["Chiang", "David", ""]]}, {"id": "1910.06724", "submitter": "H{\\aa}vard Kvamme", "authors": "H{\\aa}vard Kvamme and {\\O}rnulf Borgan", "title": "Continuous and Discrete-Time Survival Prediction with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of discrete-time survival methods for continuous-time survival\nprediction is considered. For this purpose, a scheme for discretization of\ncontinuous-time data is proposed by considering the quantiles of the estimated\nevent-time distribution, and, for smaller data sets, it is found to be\npreferable over the commonly used equidistant scheme. Furthermore, two\ninterpolation schemes for continuous-time survival estimates are explored, both\nof which are shown to yield improved performance compared to the discrete-time\nestimates. The survival methods considered are based on the likelihood for\nright-censored survival data, and parameterize either the probability mass\nfunction (PMF) or the discrete-time hazard rate, both with neural networks.\nThrough simulations and study of real-world data, the hazard rate\nparametrization is found to perform slightly better than the parametrization of\nthe PMF. Inspired by these investigations, a continuous-time method is proposed\nby assuming that the continuous-time hazard rate is piecewise constant. The\nmethod, named PC-Hazard, is found to be highly competitive with the\naforementioned methods in addition to other methods for survival prediction\nfound in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:23:19 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kvamme", "H\u00e5vard", ""], ["Borgan", "\u00d8rnulf", ""]]}, {"id": "1910.06741", "submitter": "Luis Polanco", "authors": "Luis Polanco and Jose A. Perea", "title": "Adaptive template systems: Data-driven feature selection for learning\n  with persistence diagrams", "comments": "To appear in proceedings of IEEE ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG eess.IV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction from persistence diagrams, as a tool to enrich machine\nlearning techniques, has received increasing attention in recent years. In this\npaper we explore an adaptive methodology to localize features in persistent\ndiagrams, which are then used in learning tasks. Specifically, we investigate\nthree algorithms, CDER, GMM and HDBSCAN, to obtain adaptive template\nfunctions/features. Said features are evaluated in three classification\nexperiments with persistence diagrams. Namely, manifold, human shapes and\nprotein classification. The main conclusion of our analysis is that adaptive\ntemplate systems, as a feature extraction technique, yield competitive and\noften superior results in the studied examples. Moreover, from the adaptive\nalgorithms here studied, CDER consistently provides the most reliable and\nrobust adaptive featurization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 04:15:31 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Polanco", "Luis", ""], ["Perea", "Jose A.", ""]]}, {"id": "1910.06742", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Generic Bounds on the Maximum Deviations in Sequential Prediction: An\n  Information-Theoretic Analysis", "comments": "arXiv admin note: text overlap with arXiv:1904.04765. text overlap\n  with arXiv:2001.03813", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive generic bounds on the maximum deviations in\nprediction errors for sequential prediction via an information-theoretic\napproach. The fundamental bounds are shown to depend only on the conditional\nentropy of the data point to be predicted given the previous data points. In\nthe asymptotic case, the bounds are achieved if and only if the prediction\nerror is white and uniformly distributed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:31:17 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:54:54 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 15:01:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1910.06749", "submitter": "Shanshan Wang", "authors": "Yu Gong, Hongming Shan, Yueyang Teng, Ning Tu, Ming Li, Guodong Liang,\n  Ge Wang and Shanshan Wang", "title": "Parameter-Transferred Wasserstein Generative Adversarial Network\n  (PT-WGAN) for Low-Dose PET Image Denoising", "comments": "10 pages and 12 figures", "journal-ref": "IEEE Transactions on Radiation and Plasma Medical Sciences, 2021", "doi": "10.1109/TRPMS.2020.3025071", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the widespread use of positron emission tomography (PET) in clinical\npractice, the potential risk of PET-associated radiation dose to patients needs\nto be minimized. However, with the reduction in the radiation dose, the\nresultant images may suffer from noise and artifacts that compromise diagnostic\nperformance. In this paper, we propose a parameter-transferred Wasserstein\ngenerative adversarial network (PT-WGAN) for low-dose PET image denoising. The\ncontributions of this paper are twofold: i) a PT-WGAN framework is designed to\ndenoise low-dose PET images without compromising structural details, and ii) a\ntask-specific initialization based on transfer learning is developed to train\nPT-WGAN using trainable parameters transferred from a pretrained model, which\nsignificantly improves the training efficiency of PT-WGAN. The experimental\nresults on clinical data show that the proposed network can suppress image\nnoise more effectively while preserving better image fidelity than recently\npublished state-of-the-art methods. We make our code available at\nhttps://github.com/90n9-yu/PT-WGAN.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 02:41:04 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 08:49:23 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 06:09:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Gong", "Yu", ""], ["Shan", "Hongming", ""], ["Teng", "Yueyang", ""], ["Tu", "Ning", ""], ["Li", "Ming", ""], ["Liang", "Guodong", ""], ["Wang", "Ge", ""], ["Wang", "Shanshan", ""]]}, {"id": "1910.06750", "submitter": "Marija Jegorova", "authors": "Marija Jegorova, Antti Ilari Karjalainen, Jose Vazquez, Timothy\n  Hospedales", "title": "Full-Scale Continuous Synthetic Sonar Data Generation with Markov\n  Conditional Generative Adversarial Networks", "comments": "6 pages, 6 figures. Accepted to ICRA2020. 2020 IEEE International\n  Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployment and operation of autonomous underwater vehicles is expensive and\ntime-consuming. High-quality realistic sonar data simulation could be of\nbenefit to multiple applications, including training of human operators for\npost-mission analysis, as well as tuning and validation of autonomous target\nrecognition (ATR) systems for underwater vehicles. Producing realistic\nsynthetic sonar imagery is a challenging problem as the model has to account\nfor specific artefacts of real acoustic sensors, vehicle altitude, and a\nvariety of environmental factors. We propose a novel method for generating\nrealistic-looking sonar side-scans of full-length missions, called Markov\nConditional pix2pix (MC-pix2pix). Quantitative assessment results confirm that\nthe quality of the produced data is almost indistinguishable from real.\nFurthermore, we show that bootstrapping ATR systems with MC-pix2pix data can\nimprove the performance. Synthetic data is generated 18 times faster than real\nacquisition speed, with full user control over the topography of the generated\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:53:03 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 16:58:24 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Jegorova", "Marija", ""], ["Karjalainen", "Antti Ilari", ""], ["Vazquez", "Jose", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1910.06764", "submitter": "Emilio Parisotto", "authors": "Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar\n  Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan\n  Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, Raia Hadsell", "title": "Stabilizing Transformers for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to their ability to both effectively integrate information over long\ntime horizons and scale to massive amounts of data, self-attention\narchitectures have recently shown breakthrough success in natural language\nprocessing (NLP), achieving state-of-the-art results in domains such as\nlanguage modeling and machine translation. Harnessing the transformer's ability\nto process long time horizons of information could provide a similar\nperformance boost in partially observable reinforcement learning (RL) domains,\nbut the large-scale transformers used in NLP have yet to be successfully\napplied to the RL setting. In this work we demonstrate that the standard\ntransformer architecture is difficult to optimize, which was previously\nobserved in the supervised learning setting but becomes especially pronounced\nwith RL objectives. We propose architectural modifications that substantially\nimprove the stability and learning speed of the original Transformer and XL\nvariant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses\nLSTMs on challenging memory environments and achieves state-of-the-art results\non the multi-task DMLab-30 benchmark suite, exceeding the performance of an\nexternal memory architecture. We show that the GTrXL, trained using the same\nlosses, has stability and performance that consistently matches or exceeds a\ncompetitive LSTM baseline, including on more reactive tasks where memory is\nless critical. GTrXL offers an easy-to-train, simple-to-implement but\nsubstantially more expressive architectural alternative to the standard\nmulti-layer LSTM ubiquitously used for RL agents in partially observable\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 20:02:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Parisotto", "Emilio", ""], ["Song", "H. Francis", ""], ["Rae", "Jack W.", ""], ["Pascanu", "Razvan", ""], ["Gulcehre", "Caglar", ""], ["Jayakumar", "Siddhant M.", ""], ["Jaderberg", "Max", ""], ["Kaufman", "Raphael Lopez", ""], ["Clark", "Aidan", ""], ["Noury", "Seb", ""], ["Botvinick", "Matthew M.", ""], ["Heess", "Nicolas", ""], ["Hadsell", "Raia", ""]]}, {"id": "1910.06772", "submitter": "Jonathan Richens", "authors": "Jonathan G. Richens, Ciaran M. Lee, Saurabh Johri", "title": "Counterfactual diagnosis", "comments": "Restructured and new subsections. Improved figures. Introduction\n  rewritten", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning promises to revolutionize clinical decision making and\ndiagnosis. In medical diagnosis a doctor aims to explain a patient's symptoms\nby determining the diseases \\emph{causing} them. However, existing diagnostic\nalgorithms are purely associative, identifying diseases that are strongly\ncorrelated with a patients symptoms and medical history. We show that this\ninability to disentangle correlation from causation can result in sub-optimal\nor dangerous diagnoses. To overcome this, we reformulate diagnosis as a\ncounterfactual inference task and derive new counterfactual diagnostic\nalgorithms. We show that this approach is closer to the diagnostic reasoning of\nclinicians and significantly improves the accuracy and safety of the resulting\ndiagnoses. We compare our counterfactual algorithm to the standard Bayesian\ndiagnostic algorithm and a cohort of 44 doctors using a test set of clinical\nvignettes. While the Bayesian algorithm achieves an accuracy comparable to the\naverage doctor, placing in the top 48% of doctors in our cohort, our\ncounterfactual algorithm places in the top 25% of doctors, achieving expert\nclinical accuracy. This improvement is achieved simply by changing how we query\nour model, without requiring any additional model improvements. Our results\nshow that counterfactual reasoning is a vital missing ingredient for applying\nmachine learning to medical diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:07:43 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 23:55:42 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 18:14:43 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Richens", "Jonathan G.", ""], ["Lee", "Ciaran M.", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.06784", "submitter": "Hyoungwoo Park", "authors": "Janghoon Cho, Sungrack Yun, Hyoungwoo Park, Jungyun Eum and Kyuwoong\n  Hwang", "title": "Acoustic Scene Classification Based on a Large-margin Factorized CNN", "comments": "5 pages, DCASE 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an acoustic scene classification framework based on\na large-margin factorized convolutional neural network (CNN). We adopt the\nfactorized CNN to learn the patterns in the time-frequency domain by\nfactorizing the 2D kernel into two separate 1D kernels. The factorized kernel\nleads to learn the main component of two patterns: the long-term ambient and\nshort-term event sounds which are the key patterns of the audio scene\nclassification. In training our model, we consider the loss function based on\nthe triplet sampling such that the same audio scene samples from different\nenvironments are minimized, and simultaneously the different audio scene\nsamples are maximized. With this loss function, the samples from the same audio\nscene are clustered independently of the environment, and thus we can get the\nclassifier with better generalization ability in an unseen environment. We\nevaluated our audio scene classification framework using the dataset of the\nDCASE challenge 2019 task1A. Experimental results show that the proposed\nalgorithm improves the performance of the baseline network and reduces the\nnumber of parameters to one third. Furthermore, the performance gain is higher\non unseen data, and it shows that the proposed algorithm has better\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:47:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Cho", "Janghoon", ""], ["Yun", "Sungrack", ""], ["Park", "Hyoungwoo", ""], ["Eum", "Jungyun", ""], ["Hwang", "Kyuwoong", ""]]}, {"id": "1910.06789", "submitter": "Surya Karthik Mukkavilli", "authors": "Caleb Hoyne, S. Karthik Mukkavilli and David Meger", "title": "Deep learning for Aerosol Forecasting", "comments": "Machine Learning and the Physical Sciences Workshop at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.ao-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reanalysis datasets combining numerical physics models and limited\nobservations to generate a synthesised estimate of variables in an Earth\nsystem, are prone to biases against ground truth. Biases identified with the\nNASA Modern-Era Retrospective Analysis for Research and Applications, Version 2\n(MERRA-2) aerosol optical depth (AOD) dataset, against the Aerosol Robotic\nNetwork (AERONET) ground measurements in previous studies, motivated the\ndevelopment of a deep learning based AOD prediction model globally. This study\ncombines a convolutional neural network (CNN) with MERRA-2, tested against all\nAERONET sites. The new hybrid CNN-based model provides better estimates\nvalidated versus AERONET ground truth, than only using MERRA-2 reanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:35:08 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Hoyne", "Caleb", ""], ["Mukkavilli", "S. Karthik", ""], ["Meger", "David", ""]]}, {"id": "1910.06790", "submitter": "Hyoungwoo Park", "authors": "Hyoungwoo Park, Sungrack Yun, Jungyun Eum, Janghoon Cho and Kyuwoong\n  Hwang", "title": "Weakly Labeled Sound Event Detection Using Tri-training and Adversarial\n  Learning", "comments": "5 pages, DCASE 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a semi-supervised learning framework for weakly labeled\npolyphonic sound event detection problems for the DCASE 2019 challenge's task4\nby combining both the tri-training and adversarial learning. The goal of the\ntask4 is to detect onsets and offsets of multiple sound events in a single\naudio clip. The entire dataset consists of the synthetic data with a strong\nlabel (sound event labels with boundaries) and real data with weakly labeled\n(sound event labels) and unlabeled dataset. Given this dataset, we apply the\ntri-training where two different classifiers are used to obtain pseudo labels\non the weakly labeled and unlabeled dataset, and the final classifier is\ntrained using the strongly labeled dataset and weakly/unlabeled dataset with\npseudo labels. Also, we apply the adversarial learning to reduce the domain gap\nbetween the real and synthetic dataset. We evaluated our learning framework\nusing the validation set of the task4 dataset, and in the experiments, our\nlearning framework shows a considerable performance improvement over the\nbaseline model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:47:55 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Park", "Hyoungwoo", ""], ["Yun", "Sungrack", ""], ["Eum", "Jungyun", ""], ["Cho", "Janghoon", ""], ["Hwang", "Kyuwoong", ""]]}, {"id": "1910.06792", "submitter": "Luchen Liu", "authors": "Luchen Liu, Haoxian Wu, Zichang Wang, Zequn Liu, Ming Zhang", "title": "Early Prediction of Sepsis From Clinical Datavia Heterogeneous Event\n  Aggregation", "comments": "4 pages, 2 figures, Accept by CINC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a life-threatening condition that seriously endangers millions of\npeople over the world. Hopefully, with the widespread availability of\nelectronic health records (EHR), predictive models that can effectively deal\nwith clinical sequential data increase the possibility to predict sepsis and\ntake early preventive treatment. However, the early prediction is challenging\nbecause patients' sequential data in EHR contains temporal interactions of\nmultiple clinical events. And capturing temporal interactions in the long event\nsequence is hard for traditional LSTM. Rather than directly applying the LSTM\nmodel to the event sequences, our proposed model firstly aggregates\nheterogeneous clinical events in a short period and then captures temporal\ninteractions of the aggregated representations with LSTM. Our proposed\nHeterogeneous Event Aggregation can not only shorten the length of clinical\nevent sequence but also help to retain temporal interactions of both\ncategorical and numerical features of clinical events in the multiple heads of\nthe aggregation representations. In the PhysioNet/Computing in Cardiology\nChallenge 2019, with the team named PKU_DLIB, our proposed model, in high\nefficiency, achieved utility score (0.321) in the full test set.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:05:48 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Liu", "Luchen", ""], ["Wu", "Haoxian", ""], ["Wang", "Zichang", ""], ["Liu", "Zequn", ""], ["Zhang", "Ming", ""]]}, {"id": "1910.06813", "submitter": "Anindya Sarkar", "authors": "Anindya Sarkar, Anirudh Sunder Raj, Raghu Sesha Iyengar", "title": "ODE guided Neural Data Augmentation Techniques for Time Series Data and\n  its Benefits on Robustness", "comments": "8 pages, 5 figures, International Conference on Machine Learning and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring adversarial attack vectors and studying their effects on machine\nlearning algorithms has been of interest to researchers. Deep neural networks\nworking with time series data have received lesser interest compared to their\nimage counterparts in this context. In a recent finding, it has been revealed\nthat current state-of-the-art deep learning time series classifiers are\nvulnerable to adversarial attacks. In this paper, we introduce two local\ngradient based and one spectral density based time series data augmentation\ntechniques. We show that a model trained with data obtained using our\ntechniques obtains state-of-the-art classification accuracy on various time\nseries benchmarks. In addition, it improves the robustness of the model against\nsome of the most common corruption techniques,such as Fast Gradient Sign Method\n(FGSM) and Basic Iterative Method (BIM).\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:37:18 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 10:31:48 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 17:53:53 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sarkar", "Anindya", ""], ["Raj", "Anirudh Sunder", ""], ["Iyengar", "Raghu Sesha", ""]]}, {"id": "1910.06816", "submitter": "Antoine Saporta", "authors": "Antoine Saporta, Yifu Chen, Michael Blot, Matthieu Cord", "title": "REVE: Regularizing Deep Learning with Variational Entropy Bound", "comments": "Published in 2019 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2019.8804396", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on generalization performance of machine learning algorithms under\nthe scope of information theory suggest that compressed representations can\nguarantee good generalization, inspiring many compression-based regularization\nmethods. In this paper, we introduce REVE, a new regularization scheme. Noting\nthat compressing the representation can be sub-optimal, our first contribution\nis to identify a variable that is directly responsible for the final\nprediction. Our method aims at compressing the class conditioned entropy of\nthis latter variable. Second, we introduce a variational upper bound on this\nconditional entropy term. Finally, we propose a scheme to instantiate a\ntractable loss that is integrated within the training procedure of the neural\nnetwork and demonstrate its efficiency on different neural networks and\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:38:30 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Saporta", "Antoine", ""], ["Chen", "Yifu", ""], ["Blot", "Michael", ""], ["Cord", "Matthieu", ""]]}, {"id": "1910.06832", "submitter": "Akinori Tanaka", "authors": "Akinori Tanaka", "title": "Discriminator optimal transport", "comments": "github link added, NeurIPS2019", "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-19", "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a broad class of generative adversarial networks, we show that\ndiscriminator optimization process increases a lower bound of the dual cost\nfunction for the Wasserstein distance between the target distribution $p$ and\nthe generator distribution $p_G$. It implies that the trained discriminator can\napproximate optimal transport (OT) from $p_G$ to $p$.Based on some experiments\nand a bit of OT theory, we propose a discriminator optimal transport (DOT)\nscheme to improve generated images. We show that it improves inception score\nand FID calculated by un-conditional GAN trained by CIFAR-10, STL-10 and a\npublic pre-trained model of conditional GAN by ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:47:37 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 17:18:07 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Tanaka", "Akinori", ""]]}, {"id": "1910.06853", "submitter": "Nikolas Ioannou", "authors": "Andreea Anghel, Nikolas Ioannou, Thomas Parnell, Nikolaos Papandreou,\n  Celestine Mendler-D\\\"unner, Haris Pozidis", "title": "Breadth-first, Depth-next Training of Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze, evaluate, and improve the performance of training\nRandom Forest (RF) models on modern CPU architectures. An exact,\nstate-of-the-art binary decision tree building algorithm is used as the basis\nof this study. Firstly, we investigate the trade-offs between using different\ntree building algorithms, namely breadth-first-search (BFS) and\ndepth-search-first (DFS). We design a novel, dynamic, hybrid BFS-DFS algorithm\nand demonstrate that it performs better than both BFS and DFS, and is more\nrobust in the presence of workloads with different characteristics. Secondly,\nwe identify CPU performance bottlenecks when generating trees using this\napproach, and propose optimizations to alleviate them. The proposed hybrid tree\nbuilding algorithm for RF is implemented in the Snap Machine Learning\nframework, and speeds up the training of RFs by 7.8x on average when compared\nto state-of-the-art RF solvers (sklearn, H2O, and xgboost) on a range of\ndatasets, RF configurations, and multi-core CPU architectures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:14:35 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Anghel", "Andreea", ""], ["Ioannou", "Nikolas", ""], ["Parnell", "Thomas", ""], ["Papandreou", "Nikolaos", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Pozidis", "Haris", ""]]}, {"id": "1910.06878", "submitter": "Sin Yong Tan", "authors": "Zhanhong Jiang, Aditya Balu, Sin Yong Tan, Young M Lee, Chinmay Hegde,\n  Soumik Sarkar", "title": "On Higher-order Moments in Adam", "comments": "Accepted in Beyond First Order Methods in Machine Learning workshop\n  in 33rd Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the popular deep learning optimization routine,\nAdam, from the perspective of statistical moments. While Adam is an adaptive\nlower-order moment based (of the stochastic gradient) method, we propose an\nextension namely, HAdam, which uses higher order moments of the stochastic\ngradient. Our analysis and experiments reveal that certain higher-order moments\nof the stochastic gradient are able to achieve better performance compared to\nthe vanilla Adam algorithm. We also provide some analysis of HAdam related to\nodd and even moments to explain some intriguing and seemingly non-intuitive\nempirical results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:45:38 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Jiang", "Zhanhong", ""], ["Balu", "Aditya", ""], ["Tan", "Sin Yong", ""], ["Lee", "Young M", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1910.06893", "submitter": "Ankit Pensia", "authors": "Ankit Pensia, Varun Jog, Po-Ling Loh", "title": "Extracting robust and accurate features via a robust information\n  bottleneck", "comments": "A version of this paper was submitted to IEEE Journal on Selected\n  Areas in Information Theory (JSAIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel strategy for extracting features in supervised learning\nthat can be used to construct a classifier which is more robust to small\nperturbations in the input space. Our method builds upon the idea of the\ninformation bottleneck by introducing an additional penalty term that\nencourages the Fisher information of the extracted features to be small, when\nparametrized by the inputs. By tuning the regularization parameter, we can\nexplicitly trade off the opposing desiderata of robustness and accuracy when\nconstructing a classifier. We derive the optimal solution to the robust\ninformation bottleneck when the inputs and outputs are jointly Gaussian,\nproving that the optimally robust features are also jointly Gaussian in that\nsetting. Furthermore, we propose a method for optimizing a variational bound on\nthe robust information bottleneck objective in general settings using\nstochastic gradient descent, which may be implemented efficiently in neural\nnetworks. Our experimental results for synthetic and real data sets show that\nthe proposed feature extraction method indeed produces classifiers with\nincreased robustness to perturbations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:07:34 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Pensia", "Ankit", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1910.06899", "submitter": "Maxim Vaysburd", "authors": "Maxim Vaysburd", "title": "Identifying Epigenetic Signature of Breast Cancer with Machine Learning", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research reported in this paper identifies the epigenetic biomarker\n(methylation beta pattern) of breast cancer. Many cancers are triggered by\nabnormal gene expression levels caused by aberrant methylation of CpG sites in\nthe DNA. In order to develop early diagnostics of cancer-causing methylations\nand to develop a treatment, it is necessary to identify a few dozen key\ncancer-related CpG methylation sites out of the millions of locations in the\nDNA. This research used public TCGA dataset to train a TensorFlow machine\nlearning model to classify breast cancer versus non-breast-cancer tissue\nsamples, based on over 300,000 methylation beta values in each sample. L1\nregularization was applied to identify the CpG methylation sites most important\nfor accurate classification. It was hypothesized that CpG sites with the\nhighest learned model weights correspond to DNA locations most relevant to\nbreast cancer. A reduced model trained on methylation betas of just the 25 CpG\nsites having the highest weights in the full model (trained on methylation\nbetas at over 300,000 CpG sites) has achieved over 94% accuracy on evaluation\ndata, confirming that the identified 25 CpG sites are indeed a biomarker of\nbreast cancer.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 19:46:17 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Vaysburd", "Maxim", ""]]}, {"id": "1910.06909", "submitter": "Jordan Dotzel", "authors": "Ritchie Zhao, Jordan Dotzel, Zhanqiu Hu, Preslav Ivanov, Christopher\n  De Sa, Zhiru Zhang", "title": "OverQ: Opportunistic Outlier Quantization for Neural Network\n  Accelerators", "comments": "Preprint, work in progress. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outliers in weights and activations pose a key challenge for fixed-point\nquantization of neural networks. While they can be addressed by fine-tuning,\nthis is not practical for ML service providers (e.g., Google or Microsoft) who\noften receive customer models without training data. Specialized hardware for\nhandling activation outliers can enable low-precision neural networks, but at\nthe cost of nontrivial area overhead. We instead propose overwrite quantization\n(OverQ), a lightweight hardware technique that opportunistically increases\nbitwidth for activation outliers by overwriting nearby zeros. It has two major\nmodes of operation: range overwrite and precision overwrite. Range overwrite\nreallocates bits to increase the range of outliers, while precision overwrite\nreuses zeros to increase the precision of non-outlier values. Combining range\noverwrite with a simple cascading logic, we handle the vast majority of\noutliers to significantly improve model accuracy at low bitwidth. Our\nexperiments show that with modest cascading, we can consistently handle over\n90% of outliers and achieve +5% ImageNet Top-1 accuracy on a quantized\nResNet-50 at 4 bits. Our ASIC prototype shows OverQ can be implemented\nefficiently on top of existing weight-stationary systolic arrays with small\narea increases per processing element. We imagine this technique can complement\nmodern DNN accelerator designs to provide small increases in accuracy with\ninsignificant area overhead.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 23:21:59 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 07:01:47 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhao", "Ritchie", ""], ["Dotzel", "Jordan", ""], ["Hu", "Zhanqiu", ""], ["Ivanov", "Preslav", ""], ["De Sa", "Christopher", ""], ["Zhang", "Zhiru", ""]]}, {"id": "1910.06922", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, Ioannis Mitliagkas", "title": "Gradient penalty from a maximum margin perspective", "comments": "Code at https://github.com/AlexiaJM/MaximumMarginGANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular heuristic for improved performance in Generative adversarial\nnetworks (GANs) is to use some form of gradient penalty on the discriminator.\nThis gradient penalty was originally motivated by a Wasserstein distance\nformulation. However, the use of gradient penalty in other GAN formulations is\nnot well motivated. We present a unifying framework of expected margin\nmaximization and show that a wide range of gradient-penalized GANs (e.g.,\nWasserstein, Standard, Least-Squares, and Hinge GANs) can be derived from this\nframework. Our results imply that employing gradient penalties induces a\nlarge-margin classifier (thus, a large-margin discriminator in GANs). We\ndescribe how expected margin maximization helps reduce vanishing gradients at\nfake (generated) samples, a known problem in GANs. From this framework, we\nderive a new $L^\\infty$ gradient norm penalty with Hinge loss which generally\nproduces equally good (or better) generated output in GANs than $L^2$-norm\npenalties (based on the Fr\\'echet Inception Distance).\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:48:43 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:41:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "1910.06924", "submitter": "Frederik Harder", "authors": "Frederik Harder, Jonas K\\\"ohler, Max Welling, Mijung Park", "title": "DP-MAC: The Differentially Private Method of Auxiliary Coordinates for\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a differentially private deep learning algorithm is challenging,\ndue to the difficulty in analyzing the sensitivity of objective functions that\nare typically used to train deep neural networks. Many existing methods resort\nto the stochastic gradient descent algorithm and apply a pre-defined\nsensitivity to the gradients for privatizing weights. However, their slow\nconvergence typically yields a high cumulative privacy loss. Here, we take a\ndifferent route by employing the method of auxiliary coordinates, which allows\nus to independently update the weights per layer by optimizing a per-layer\nobjective function. This objective function can be well approximated by a\nlow-order Taylor's expansion, in which sensitivity analysis becomes tractable.\nWe perturb the coefficients of the expansion for privacy, which we optimize\nusing more advanced optimization routines than SGD for faster convergence. We\nempirically show that our algorithm provides a decent trained model quality\nunder a modest privacy budget.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:51:36 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Harder", "Frederik", ""], ["K\u00f6hler", "Jonas", ""], ["Welling", "Max", ""], ["Park", "Mijung", ""]]}, {"id": "1910.06939", "submitter": "Benjamin Lengerich", "authors": "Benjamin Lengerich, Bryon Aragam, Eric P. Xing", "title": "Learning Sample-Specific Models with Low-Rank Personalized Regression", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications of machine learning (ML) deal with increasingly\nheterogeneous datasets comprised of data collected from overlapping latent\nsubpopulations. As a result, traditional models trained over large datasets may\nfail to recognize highly predictive localized effects in favour of weakly\npredictive global patterns. This is a problem because localized effects are\ncritical to developing individualized policies and treatment plans in\napplications ranging from precision medicine to advertising. To address this\nchallenge, we propose to estimate sample-specific models that tailor inference\nand prediction at the individual level. In contrast to classical ML models that\nestimate a single, complex model (or only a few complex models), our approach\nproduces a model personalized to each sample. These sample-specific models can\nbe studied to understand subgroup dynamics that go beyond coarse-grained class\nlabels. Crucially, our approach does not assume that relationships between\nsamples (e.g. a similarity network) are known a priori. Instead, we use\nunmodeled covariates to learn a latent distance metric over the samples. We\napply this approach to financial, biomedical, and electoral data as well as\nsimulated data and show that sample-specific models provide fine-grained\ninterpretations of complicated phenomena without sacrificing predictive\naccuracy compared to state-of-the-art models such as deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:24:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lengerich", "Benjamin", ""], ["Aragam", "Bryon", ""], ["Xing", "Eric P.", ""]]}, {"id": "1910.06943", "submitter": "Weijie J. Su", "authors": "Hangfeng He and Weijie J. Su", "title": "The Local Elasticity of Neural Networks", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a phenomenon in neural networks that we refer to as\n\\textit{local elasticity}. Roughly speaking, a classifier is said to be locally\nelastic if its prediction at a feature vector $\\bx'$ is \\textit{not}\nsignificantly perturbed, after the classifier is updated via stochastic\ngradient descent at a (labeled) feature vector $\\bx$ that is\n\\textit{dissimilar} to $\\bx'$ in a certain sense. This phenomenon is shown to\npersist for neural networks with nonlinear activation functions through\nextensive simulations on real-life and synthetic datasets, whereas this is not\nobserved in linear classifiers. In addition, we offer a geometric\ninterpretation of local elasticity using the neural tangent kernel\n\\citep{jacot2018neural}. Building on top of local elasticity, we obtain\npairwise similarity measures between feature vectors, which can be used for\nclustering in conjunction with $K$-means. The effectiveness of the clustering\nalgorithm on the MNIST and CIFAR-10 datasets in turn corroborates the\nhypothesis of local elasticity of neural networks on real-life data. Finally,\nwe discuss some implications of local elasticity to shed light on several\nintriguing aspects of deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:35:30 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 02:04:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["He", "Hangfeng", ""], ["Su", "Weijie J.", ""]]}, {"id": "1910.06948", "submitter": "Kailiang Wu", "authors": "Kailiang Wu, Dongbin Xiu", "title": "Data-Driven Deep Learning of Partial Differential Equations in Modal\n  Space", "comments": "Minor notational changes", "journal-ref": "Journal of Computational Physics, 408: 109307, 2020", "doi": "10.1016/j.jcp.2020.109307", "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for recovering/approximating unknown time-dependent\npartial differential equation (PDE) using its solution data. Instead of\nidentifying the terms in the underlying PDE, we seek to approximate the\nevolution operator of the underlying PDE numerically. The evolution operator of\nthe PDE, defined in infinite-dimensional space, maps the solution from a\ncurrent time to a future time and completely characterizes the solution\nevolution of the underlying unknown PDE. Our recovery strategy relies on\napproximation of the evolution operator in a properly defined modal space,\ni.e., generalized Fourier space, in order to reduce the problem to finite\ndimensions. The finite dimensional approximation is then accomplished by\ntraining a deep neural network structure, which is based on residual network\n(ResNet), using the given data. Error analysis is provided to illustrate the\npredictive accuracy of the proposed method. A set of examples of different\ntypes of PDEs, including inviscid Burgers' equation that develops discontinuity\nin its solution, are presented to demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:43:32 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 23:59:10 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1910.06950", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, James S. Duncan", "title": "Jointly Discriminative and Generative Recurrent Neural Networks for\n  Learning from fMRI", "comments": "10th International Workshop on Machine Learning in Medical Imaging\n  (MLMI 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-32692-0_44", "report-no": null, "categories": "eess.IV cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) were designed for dealing with time-series\ndata and have recently been used for creating predictive models from functional\nmagnetic resonance imaging (fMRI) data. However, gathering large fMRI datasets\nfor learning is a difficult task. Furthermore, network interpretability is\nunclear. To address these issues, we utilize multitask learning and design a\nnovel RNN-based model that learns to discriminate between classes while\nsimultaneously learning to generate the fMRI time-series data. Employing the\nlong short-term memory (LSTM) structure, we develop a discriminative model\nbased on the hidden state and a generative model based on the cell state. The\naddition of the generative model constrains the network to learn functional\ncommunities represented by the LSTM nodes that are both consistent with the\ndata generation as well as useful for the classification task. We apply our\napproach to the classification of subjects with autism vs. healthy controls\nusing several datasets from the Autism Brain Imaging Data Exchange. Experiments\nshow that our jointly discriminative and generative model improves\nclassification learning while also producing robust and meaningful functional\ncommunities for better model understanding.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:43:45 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Zhuang", "Juntang", ""], ["Duncan", "James S.", ""]]}, {"id": "1910.06956", "submitter": "Matus Telgarsky", "authors": "Ziwei Ji and Matus Telgarsky and Ruicheng Xian", "title": "Neural tangent kernels, transportation mappings, and universal\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes rates of universal approximation for the shallow\nneural tangent kernel (NTK): network weights are only allowed microscopic\nchanges from random initialization, which entails that activations are mostly\nunchanged, and the network is nearly equivalent to its linearization.\nConcretely, the paper has two main contributions: a generic scheme to\napproximate functions with the NTK by sampling from transport mappings between\nthe initial weights and their desired values, and the construction of transport\nmappings via Fourier transforms. Regarding the first contribution, the proof\nscheme provides another perspective on how the NTK regime arises from\nrescaling: redundancy in the weights due to resampling allows individual\nweights to be scaled down. Regarding the second contribution, the most notable\ntransport mapping asserts that roughly $1 / \\delta^{10d}$ nodes are sufficient\nto approximate continuous functions, where $\\delta$ depends on the continuity\nproperties of the target function. By contrast, nearly the same proof yields a\nbound of $1 / \\delta^{2d}$ for shallow ReLU networks; this gap suggests a\ntantalizing direction for future work, separating shallow ReLU networks and\ntheir linearization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:52:49 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 00:45:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ji", "Ziwei", ""], ["Telgarsky", "Matus", ""], ["Xian", "Ruicheng", ""]]}, {"id": "1910.06985", "submitter": "Katja Ried", "authors": "Katja Ried and Benjamin Eva and Thomas M\\\"uller and Hans J. Briegel", "title": "How a minimal learning agent can infer the existence of unobserved\n  variables in a complex environment", "comments": "28 pages plus references, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to a mainstream position in contemporary cognitive science and\nphilosophy, the use of abstract compositional concepts is both a necessary and\na sufficient condition for the presence of genuine thought. In this article, we\nshow how the ability to develop and utilise abstract conceptual structures can\nbe achieved by a particular kind of learning agents. More specifically, we\nprovide and motivate a concrete operational definition of what it means for\nthese agents to be in possession of abstract concepts, before presenting an\nexplicit example of a minimal architecture that supports this capability. We\nthen proceed to demonstrate how the existence of abstract conceptual structures\ncan be operationally useful in the process of employing previously acquired\nknowledge in the face of new experiences, thereby vindicating the natural\nconjecture that the cognitive functions of abstraction and generalisation are\nclosely related.\n  Keywords: concept formation, projective simulation, reinforcement learning,\ntransparent artificial intelligence, theory formation, explainable artificial\nintelligence (XAI)\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:08:08 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ried", "Katja", ""], ["Eva", "Benjamin", ""], ["M\u00fcller", "Thomas", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1910.06990", "submitter": "Raed Al Kontar", "authors": "Xubo Yue, Raed Kontar", "title": "The Renyi Gaussian Process: Towards Improved Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an alternative closed form lower bound on the Gaussian process\n($\\mathcal{GP}$) likelihood based on the R\\'enyi $\\alpha$-divergence. This new\nlower bound can be viewed as a convex combination of the Nystr\\\"om\napproximation and the exact $\\mathcal{GP}$. The key advantage of this bound, is\nits capability to control and tune the enforced regularization on the model and\nthus is a generalization of the traditional variational $\\mathcal{GP}$\nregression. From a theoretical perspective, we provide the convergence rate and\nrisk bound for inference using our proposed approach. Experiments on real data\nshow that the proposed algorithm may be able to deliver improvement over\nseveral $\\mathcal{GP}$ inference methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:21:06 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 01:25:20 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 16:32:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yue", "Xubo", ""], ["Kontar", "Raed", ""]]}, {"id": "1910.06991", "submitter": "Zhichao Jiang", "authors": "Kosuke Imai and Zhichao Jiang", "title": "Discussion of \"The Blessings of Multiple Causes\" by Wang and Blei", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This commentary has two goals. We first critically review the deconfounder\nmethod and point out its advantages and limitations. We then briefly consider\nthree possible ways to address some of the limitations of the deconfounder\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:21:45 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Imai", "Kosuke", ""], ["Jiang", "Zhichao", ""]]}, {"id": "1910.06995", "submitter": "Talgat Daulbaev", "authors": "Julia Gusak, Talgat Daulbaev, Evgeny Ponomarev, Andrzej Cichocki, Ivan\n  Oseledets", "title": "Reduced-Order Modeling of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for speeding up the inference of deep neural\nnetworks. It is somewhat inspired by the reduced-order modeling techniques for\ndynamical systems.The cornerstone of the proposed method is the maximum volume\nalgorithm. We demonstrate efficiency on neural networks pre-trained on\ndifferent datasets. We show that in many practical cases it is possible to\nreplace convolutional layers with much smaller fully-connected layers with a\nrelatively small drop in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:25:26 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 08:03:10 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 14:09:00 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 08:55:41 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 09:45:47 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Gusak", "Julia", ""], ["Daulbaev", "Talgat", ""], ["Ponomarev", "Evgeny", ""], ["Cichocki", "Andrzej", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1910.06996", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Csaba Szepesvari", "title": "Adaptive Exploration in Linear Contextual Bandit", "comments": "Accepted at AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits serve as a fundamental model for many sequential decision\nmaking tasks. The most popular theoretically justified approaches are based on\nthe optimism principle. While these algorithms can be practical, they are known\nto be suboptimal asymptotically. On the other hand, existing asymptotically\noptimal algorithms for this problem do not exploit the linear structure in an\noptimal way and suffer from lower-order terms that dominate the regret in all\npractically interesting regimes. We start to bridge the gap by designing an\nalgorithm that is asymptotically optimal and has good finite-time empirical\nperformance. At the same time, we make connections to the recent literature on\nwhen exploration-free methods are effective. Indeed, if the distribution of\ncontexts is well behaved, then our algorithm acts mostly greedily and enjoys\nsub-logarithmic regret. Furthermore, our approach is adaptive in the sense that\nit automatically detects the nice case. Numerical results demonstrate\nsignificant regret reductions by our method relative to several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:26:52 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 01:01:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1910.07003", "submitter": "Valerio Perrone", "authors": "Valerio Perrone, Iaroslav Shcherbatyi, Rodolphe Jenatton, Cedric\n  Archambeau, Matthias Seeger", "title": "Constrained Bayesian Optimization with Max-Value Entropy Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a model-based approach to sequentially optimize\nexpensive black-box functions, such as the validation error of a deep neural\nnetwork with respect to its hyperparameters. In many real-world scenarios, the\noptimization is further subject to a priori unknown constraints. For example,\ntraining a deep network configuration may fail with an out-of-memory error when\nthe model is too large. In this work, we focus on a general formulation of\nGaussian process-based BO with continuous or binary constraints. We propose\nconstrained Max-value Entropy Search (cMES), a novel information\ntheoretic-based acquisition function implementing this formulation. We also\nrevisit the validity of the factorized approximation adopted for rapid\ncomputation of the MES acquisition function, showing empirically that this\nleads to inaccurate results. On an extensive set of real-world constrained\nhyperparameter optimization problems we show that cMES compares favourably to\nprior work, while being simpler to implement and faster than other constrained\nextensions of Entropy Search.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:56:04 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Perrone", "Valerio", ""], ["Shcherbatyi", "Iaroslav", ""], ["Jenatton", "Rodolphe", ""], ["Archambeau", "Cedric", ""], ["Seeger", "Matthias", ""]]}, {"id": "1910.07012", "submitter": "Gean Pereira", "authors": "Gean Trindade Pereira, Mois\\'es dos Santos, Edesio Alcoba\\c{c}a,\n  Rafael Mantovani and Andr\\'e Carvalho", "title": "Transfer Learning for Algorithm Recommendation", "comments": "Short-paper accepted in LXAI Research Workshop co-located with\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-Learning is a subarea of Machine Learning that aims to take advantage of\nprior knowledge to learn faster and with fewer data [1]. There are different\nscenarios where meta-learning can be applied, and one of the most common is\nalgorithm recommendation, where previous experience on applying machine\nlearning algorithms for several datasets can be used to learn which algorithm,\nfrom a set of options, would be more suitable for a new dataset [2]. Perhaps\nthe most popular form of meta-learning is transfer learning, which consists of\ntransferring knowledge acquired by a machine learning algorithm in a previous\nlearning task to increase its performance faster in another and similar task\n[3]. Transfer Learning has been widely applied in a variety of complex tasks\nsuch as image classification, machine translation and, speech recognition,\nachieving remarkable results [4,5,6,7,8]. Although transfer learning is very\nused in traditional or base-learning, it is still unknown if it is useful in a\nmeta-learning setup. For that purpose, in this paper, we investigate the\neffects of transferring knowledge in the meta-level instead of base-level.\nThus, we train a neural network on meta-datasets related to algorithm\nrecommendation, and then using transfer learning, we reuse the knowledge\nlearned by the neural network in other similar datasets from the same domain,\nto verify how transferable is the acquired meta-knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 19:26:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Pereira", "Gean Trindade", ""], ["Santos", "Mois\u00e9s dos", ""], ["Alcoba\u00e7a", "Edesio", ""], ["Mantovani", "Rafael", ""], ["Carvalho", "Andr\u00e9", ""]]}, {"id": "1910.07030", "submitter": "Qi Lei", "authors": "Qi Lei, Jason D. Lee, Alexandros G. Dimakis, Constantinos Daskalakis", "title": "SGD Learns One-Layer Networks in WGANs", "comments": "24 pages, 4 figures, ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a widely used framework for\nlearning generative models. Wasserstein GANs (WGANs), one of the most\nsuccessful variants of GANs, require solving a minmax optimization problem to\nglobal optimality, but are in practice successfully trained using stochastic\ngradient descent-ascent. In this paper, we show that, when the generator is a\none-layer network, stochastic gradient descent-ascent converges to a global\nsolution with polynomial time and sample complexity.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:01:27 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 02:35:27 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Lei", "Qi", ""], ["Lee", "Jason D.", ""], ["Dimakis", "Alexandros G.", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1910.07042", "submitter": "Mayoore Jaiswal", "authors": "Mayoore S. Jaiswal, Bumsoo Kang, Jinho Lee, Minsik Cho", "title": "MUTE: Data-Similarity Driven Multi-hot Target Encoding for Neural\n  Network Design", "comments": "NeurIPS Workshop 2019 - Learning with Rich Experience: Integration of\n  Learning Paradigms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target encoding is an effective technique to deliver better performance for\nconventional machine learning methods, and recently, for deep neural networks\nas well. However, the existing target encoding approaches require significant\nincrease in the learning capacity, thus demand higher computation power and\nmore training data. In this paper, we present a novel and efficient target\nencoding scheme, MUTE to improve both generalizability and robustness of a\ntarget model by understanding the inter-class characteristics of a target\ndataset. By extracting the confusion level between the target classes in a\ndataset, MUTE strategically optimizes the Hamming distances among target\nencoding. Such optimized target encoding offers higher classification strength\nfor neural network models with negligible computation overhead and without\nincreasing the model size. When MUTE is applied to the popular image\nclassification networks and datasets, our experimental results show that MUTE\noffers better generalization and defense against the noises and adversarial\nattacks over the existing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:23:06 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jaiswal", "Mayoore S.", ""], ["Kang", "Bumsoo", ""], ["Lee", "Jinho", ""], ["Cho", "Minsik", ""]]}, {"id": "1910.07047", "submitter": "Salar Jafarlou", "authors": "Salar Jafarlou, Soheil Khorram, Vinay Kothapally, John H.L. Hansen", "title": "Analyzing Large Receptive Field Convolutional Networks for Distant\n  Speech Recognition", "comments": "ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant efforts over the last few years to build a robust\nautomatic speech recognition (ASR) system for different acoustic settings, the\nperformance of the current state-of-the-art technologies significantly degrades\nin noisy reverberant environments.\n  Convolutional Neural Networks (CNNs) have been successfully used to achieve\nsubstantial improvements in many speech processing applications including\ndistant speech recognition (DSR). However, standard CNN architectures were not\nefficient in capturing long-term speech dynamics, which are essential in the\ndesign of a robust DSR system. In the present study, we address this issue by\ninvestigating variants of large receptive field CNNs (LRF-CNNs) which include\ndeeply recursive networks, dilated convolutional neural networks, and stacked\nhourglass networks. To compare the efficacy of the aforementioned architectures\nwith the standard CNN for Wall Street Journal (WSJ) corpus, we use a hybrid\nDNN-HMM based speech recognition system. We extend the study to evaluate the\nsystem performances for distant speech simulated using realistic room impulse\nresponses (RIRs). Our experiments show that with fixed number of parameters\nacross all architectures, the large receptive field networks show consistent\nimprovements over the standard CNNs for distant speech. Amongst the explored\nLRF-CNNs, stacked hourglass network has shown improvements with a 8.9% relative\nreduction in word error rate (WER) and 10.7% relative improvement in frame\naccuracy compared to the standard CNNs for distant simulated speech signals.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:47:29 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Jafarlou", "Salar", ""], ["Khorram", "Soheil", ""], ["Kothapally", "Vinay", ""], ["Hansen", "John H. L.", ""]]}, {"id": "1910.07056", "submitter": "Youngsuk Park", "authors": "Youngsuk Park, Sauptik Dhar, Stephen Boyd, Mohak Shah", "title": "Variable Metric Proximal Gradient Method with Diagonal Barzilai-Borwein\n  Stepsize", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable metric proximal gradient (VM-PG) is a widely used class of convex\noptimization method. Lately, there has been a lot of research on the\ntheoretical guarantees of VM-PG with different metric selections. However, most\nsuch metric selections are dependent on (an expensive) Hessian, or limited to\nscalar stepsizes like the Barzilai-Borwein (BB) stepsize with lots of\nsafeguarding. Instead, in this paper we propose an adaptive metric selection\nstrategy called the diagonal Barzilai-Borwein (BB) stepsize. The proposed\ndiagonal selection better captures the local geometry of the problem while\nkeeping per-step computation cost similar to the scalar BB stepsize i.e.\n$O(n)$. Under this metric selection for VM-PG, the theoretical convergence is\nanalyzed. Our empirical studies illustrate the improved convergence results\nunder the proposed diagonal BB stepsize, specifically for ill-conditioned\nmachine learning problems for both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:04:11 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Park", "Youngsuk", ""], ["Dhar", "Sauptik", ""], ["Boyd", "Stephen", ""], ["Shah", "Mohak", ""]]}, {"id": "1910.07072", "submitter": "Chen-Yu Wei", "authors": "Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma,\n  Rahul Jain", "title": "Model-free Reinforcement Learning in Infinite-horizon Average-reward\n  Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning is known to be memory and computation\nefficient and more amendable to large scale problems. In this paper, two\nmodel-free algorithms are introduced for learning infinite-horizon\naverage-reward Markov Decision Processes (MDPs). The first algorithm reduces\nthe problem to the discounted-reward version and achieves\n$\\mathcal{O}(T^{2/3})$ regret after $T$ steps, under the minimal assumption of\nweakly communicating MDPs. To our knowledge, this is the first model-free\nalgorithm for general MDPs in this setting. The second algorithm makes use of\nrecent advances in adaptive algorithms for adversarial multi-armed bandits and\nimproves the regret to $\\mathcal{O}(\\sqrt{T})$, albeit with a stronger ergodic\nassumption. This result significantly improves over the $\\mathcal{O}(T^{3/4})$\nregret achieved by the only existing model-free algorithm by Abbasi-Yadkori et\nal. (2019a) for ergodic MDPs in the infinite-horizon average-reward setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 22:01:31 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 15:23:05 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wei", "Chen-Yu", ""], ["Jafarnia-Jahromi", "Mehdi", ""], ["Luo", "Haipeng", ""], ["Sharma", "Hiteshi", ""], ["Jain", "Rahul", ""]]}, {"id": "1910.07099", "submitter": "Jing Zhang", "authors": "Hong Wen and Jing Zhang and Yuan Wang and Fuyu Lv and Wentian Bao and\n  Quan Lin and Keping Yang", "title": "Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition\n  for Conversion Rate Prediction", "comments": "10page, 7 figures. Accepted by SIGIR 2020. The source code will be\n  released at https://github.com/chaimi2013/ESM2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system, as an essential part of modern e-commerce, consists of\ntwo fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate\n(CVR) prediction. While CVR has a direct impact on the purchasing volume, its\nprediction is well-known challenging due to the Sample Selection Bias (SSB) and\nData Sparsity (DS) issues. Although existing methods, typically built on the\nuser sequential behavior path ``impression$\\to$click$\\to$purchase'', is\neffective for dealing with SSB issue, they still struggle to address the DS\nissue due to rare purchase training samples. Observing that users always take\nseveral purchase-related actions after clicking, we propose a novel idea of\npost-click behavior decomposition. Specifically, disjoint purchase-related\nDeterministic Action (DAction) and Other Action (OAction) are inserted between\nclick and purchase in parallel, forming a novel user sequential behavior graph\n``impression$\\to$click$\\to$D(O)Action$\\to$purchase''. Defining model on this\ngraph enables to leverage all the impression samples over the entire space and\nextra abundant supervised signals from D(O)Action, which will effectively\naddress the SSB and DS issues together. To this end, we devise a novel deep\nrecommendation model named Elaborated Entire Space Supervised Multi-task Model\n($ESM^{2}$). According to the conditional probability rule defined on the\ngraph, it employs multi-task learning to predict some decomposed sub-targets in\nparallel and compose them sequentially to formulate the final CVR. Extensive\nexperiments on both offline and online environments demonstrate the superiority\nof $ESM^{2}$ over state-of-the-art models. The source code and dataset will be\nreleased.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 23:15:42 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 00:44:54 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wen", "Hong", ""], ["Zhang", "Jing", ""], ["Wang", "Yuan", ""], ["Lv", "Fuyu", ""], ["Bao", "Wentian", ""], ["Lin", "Quan", ""], ["Yang", "Keping", ""]]}, {"id": "1910.07104", "submitter": "Mehrdad Farajtabar", "authors": "Mehrdad Farajtabar, Navid Azizan, Alex Mott, Ang Li", "title": "Orthogonal Gradient Descent for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are achieving state of the art and sometimes super-human\nperformance on learning tasks across a variety of domains. Whenever these\nproblems require learning in a continual or sequential manner, however, neural\nnetworks suffer from the problem of catastrophic forgetting; they forget how to\nsolve previous tasks after being trained on a new task, despite having the\nessential capacity to solve both tasks if they were trained on both\nsimultaneously. In this paper, we propose to address this issue from a\nparameter space perspective and study an approach to restrict the direction of\nthe gradient updates to avoid forgetting previously-learned data. We present\nthe Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by\nprojecting the gradients from new tasks onto a subspace in which the neural\nnetwork output on previous task does not change and the projected gradient is\nstill in a useful direction for learning the new task. Our approach utilizes\nthe high capacity of a neural network more efficiently and does not require\nstoring the previously learned data that might raise privacy concerns.\nExperiments on common benchmarks reveal the effectiveness of the proposed OGD\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 23:31:26 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Farajtabar", "Mehrdad", ""], ["Azizan", "Navid", ""], ["Mott", "Alex", ""], ["Li", "Ang", ""]]}, {"id": "1910.07113", "submitter": "Matthias Plappert", "authors": "OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz\n  Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn\n  Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter\n  Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang", "title": "Solving Rubik's Cube with a Robot Hand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that models trained only in simulation can be used to solve a\nmanipulation problem of unprecedented complexity on a real robot. This is made\npossible by two key components: a novel algorithm, which we call automatic\ndomain randomization (ADR) and a robot platform built for machine learning. ADR\nautomatically generates a distribution over randomized environments of\never-increasing difficulty. Control policies and vision state estimators\ntrained with ADR exhibit vastly improved sim2real transfer. For control\npolicies, memory-augmented models trained on an ADR-generated distribution of\nenvironments show clear signs of emergent meta-learning at test time. The\ncombination of ADR with our custom robot platform allows us to solve a Rubik's\ncube with a humanoid robot hand, which involves both control and state\nestimation problems. Videos summarizing our results are available:\nhttps://openai.com/blog/solving-rubiks-cube/\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 00:59:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["OpenAI", "", ""], ["Akkaya", "Ilge", ""], ["Andrychowicz", "Marcin", ""], ["Chociej", "Maciek", ""], ["Litwin", "Mateusz", ""], ["McGrew", "Bob", ""], ["Petron", "Arthur", ""], ["Paino", "Alex", ""], ["Plappert", "Matthias", ""], ["Powell", "Glenn", ""], ["Ribas", "Raphael", ""], ["Schneider", "Jonas", ""], ["Tezak", "Nikolas", ""], ["Tworek", "Jerry", ""], ["Welinder", "Peter", ""], ["Weng", "Lilian", ""], ["Yuan", "Qiming", ""], ["Zaremba", "Wojciech", ""], ["Zhang", "Lei", ""]]}, {"id": "1910.07115", "submitter": "Yu Zhang", "authors": "Yu Zhang, Frank F. Xu, Sha Li, Yu Meng, Xuan Wang, Qi Li, Jiawei Han", "title": "HiGitClass: Keyword-Driven Hierarchical Classification of GitHub\n  Repositories", "comments": "10 pages; Accepted to ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GitHub has become an important platform for code sharing and scientific\nexchange. With the massive number of repositories available, there is a\npressing need for topic-based search. Even though the topic label functionality\nhas been introduced, the majority of GitHub repositories do not have any\nlabels, impeding the utility of search and topic-based analysis. This work\ntargets the automatic repository classification problem as\n\\textit{keyword-driven hierarchical classification}. Specifically, users only\nneed to provide a label hierarchy with keywords to supply as supervision. This\nsetting is flexible, adaptive to the users' needs, accounts for the different\ngranularity of topic labels and requires minimal human effort. We identify\nthree key challenges of this problem, namely (1) the presence of multi-modal\nsignals; (2) supervision scarcity and bias; (3) supervision format mismatch. In\nrecognition of these challenges, we propose the \\textsc{HiGitClass} framework,\ncomprising of three modules: heterogeneous information network embedding;\nkeyword enrichment; topic modeling and pseudo document generation. Experimental\nresults on two GitHub repository collections confirm that \\textsc{HiGitClass}\nis superior to existing weakly-supervised and dataless hierarchical\nclassification methods, especially in its ability to integrate both structured\nand unstructured data for repository classification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:05:26 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zhang", "Yu", ""], ["Xu", "Frank F.", ""], ["Li", "Sha", ""], ["Meng", "Yu", ""], ["Wang", "Xuan", ""], ["Li", "Qi", ""], ["Han", "Jiawei", ""]]}, {"id": "1910.07123", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak, Geoff Pleiss, Jacob R. Gardner", "title": "Parametric Gaussian Process Regressors", "comments": "17 pages, 10 figures; as appeared in ICML 2020", "journal-ref": "International Conference on Machine Learning, pp. 4702-4712. PMLR,\n  2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of inducing point methods with stochastic variational\ninference has enabled approximate Gaussian Process (GP) inference on large\ndatasets. Unfortunately, the resulting predictive distributions often exhibit\nsubstantially underestimated uncertainties. Notably, in the regression case the\npredictive variance is typically dominated by observation noise, yielding\nuncertainty estimates that make little use of the input-dependent function\nuncertainty that makes GP priors attractive. In this work we propose two simple\nmethods for scalable GP regression that address this issue and thus yield\nsubstantially improved predictive uncertainties. The first applies variational\ninference to FITC (Fully Independent Training Conditional; Snelson\net.~al.~2006). The second bypasses posterior approximations and instead\ndirectly targets the posterior predictive distribution. In an extensive\nempirical comparison with a number of alternative methods for scalable GP\nregression, we find that the resulting predictive distributions exhibit\nsignificantly better calibrated uncertainties and higher log likelihoods--often\nby as much as half a nat per datapoint.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:27:15 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 16:36:01 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2020 17:16:16 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Jankowiak", "Martin", ""], ["Pleiss", "Geoff", ""], ["Gardner", "Jacob R.", ""]]}, {"id": "1910.07162", "submitter": "Han Zhao", "authors": "Han Zhao, Amanda Coston, Tameem Adel, Geoffrey J. Gordon", "title": "Conditional Learning of Fair Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for learning fair representations that can\nsimultaneously mitigate two notions of disparity among different demographic\nsubgroups in the classification setting. Two key components underpinning the\ndesign of our algorithm are balanced error rate and conditional alignment of\nrepresentations. We show how these two components contribute to ensuring\naccuracy parity and equalized false-positive and false-negative rates across\ngroups without impacting demographic parity. Furthermore, we also demonstrate\nboth in theory and on two real-world experiments that the proposed algorithm\nleads to a better utility-fairness trade-off on balanced datasets compared with\nexisting algorithms on learning fair representations for classification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 04:12:50 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 18:10:34 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 00:10:35 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Han", ""], ["Coston", "Amanda", ""], ["Adel", "Tameem", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1910.07169", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Michael Muelly, Jia Deng, Tomas Pfister, Li-Jia Li", "title": "Generative Modeling for Small-Data Object Detection", "comments": "Published in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores object detection in the small data regime, where only a\nlimited number of annotated bounding boxes are available due to data rarity and\nannotation expense. This is a common challenge today with machine learning\nbeing applied to many new tasks where obtaining training data is more\nchallenging, e.g. in medical images with rare diseases that doctors sometimes\nonly see once in their life-time. In this work we explore this problem from a\ngenerative modeling perspective by learning to generate new images with\nassociated bounding boxes, and using these for training an object detector. We\nshow that simply training previously proposed generative models does not yield\nsatisfactory performance due to them optimizing for image realism rather than\nobject detection accuracy. To this end we develop a new model with a novel\nunrolling mechanism that jointly optimizes the generative model and a detector\nsuch that the generated images improve the performance of the detector. We show\nthis method outperforms the state of the art on two challenging datasets,\ndisease detection and small data pedestrian detection, improving the average\nprecision on NIH Chest X-ray by a relative 20% and localization accuracy by a\nrelative 50%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 04:57:25 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Liu", "Lanlan", ""], ["Muelly", "Michael", ""], ["Deng", "Jia", ""], ["Pfister", "Tomas", ""], ["Li", "Li-Jia", ""]]}, {"id": "1910.07174", "submitter": "Momo Matsuda", "authors": "Momo Matsuda, Keiichi Morikuni, Akira Imakura, Xiucai Ye, Tetsuya\n  Sakurai", "title": "Multiclass spectral feature scaling method for dimensionality reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular features disrupt the desired classification. In this paper, we\nconsider aggressively modifying scales of features in the original space\naccording to the label information to form well-separated clusters in\nlow-dimensional space. The proposed method exploits spectral clustering to\nderive scaling factors that are used to modify the features. Specifically, we\nreformulate the Laplacian eigenproblem of the spectral clustering as an\neigenproblem of a linear matrix pencil whose eigenvector has the scaling\nfactors. Numerical experiments show that the proposed method outperforms\nwell-established supervised dimensionality reduction methods for toy problems\nwith more samples than features and real-world problems with more features than\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 05:33:12 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Matsuda", "Momo", ""], ["Morikuni", "Keiichi", ""], ["Imakura", "Akira", ""], ["Ye", "Xiucai", ""], ["Sakurai", "Tetsuya", ""]]}, {"id": "1910.07178", "submitter": "Chirag Modi", "authors": "Chirag Modi, Uros Seljak", "title": "Generative Learning of Counterfactual for Synthetic Control Applications\n  in Econometrics", "comments": "6 pages, 3 figures. Accepted at NeurIPS 2019 Workshop on Causal\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common statistical problem in econometrics is to estimate the impact of a\ntreatment on a treated unit given a control sample with untreated outcomes.\nHere we develop a generative learning approach to this problem, learning the\nprobability distribution of the data, which can be used for downstream tasks\nsuch as post-treatment counterfactual prediction and hypothesis testing. We use\ncontrol samples to transform the data to a Gaussian and homoschedastic form and\nthen perform Gaussian process analysis in Fourier space, evaluating the optimal\nGaussian kernel via non-parametric power spectrum estimation. We combine this\nGaussian prior with the data likelihood given by the pre-treatment data of the\nsingle unit, to obtain the synthetic prediction of the unit post-treatment,\nwhich minimizes the error variance of synthetic prediction. Given the\ngenerative model the minimum variance counterfactual is unique, and comes with\nan associated error covariance matrix. We extend this basic formalism to\ninclude correlations of primary variable with other covariates of interest.\nGiven the probabilistic description of generative model we can compare\nsynthetic data prediction with real data to address the question of whether the\ntreatment had a statistically significant impact. For this purpose we develop a\nhypothesis testing approach and evaluate the Bayes factor. We apply the method\nto the well studied example of California (CA) tobacco sales tax of 1988. We\nalso perform a placebo analysis using control states to validate our\nmethodology. Our hypothesis testing method suggests 5.8:1 odds in favor of CA\ntobacco sales tax having an impact on the tobacco sales, a value that is at\nleast three times higher than any of the 38 control states.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 05:50:14 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Modi", "Chirag", ""], ["Seljak", "Uros", ""]]}, {"id": "1910.07186", "submitter": "Yihao Feng", "authors": "Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, Qiang Liu", "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite horizon off-policy policy evaluation is a highly challenging task\ndue to the excessively large variance of typical importance sampling (IS)\nestimators. Recently, Liu et al. (2018a) proposed an approach that\nsignificantly reduces the variance of infinite-horizon off-policy evaluation by\nestimating the stationary density ratio, but at the cost of introducing\npotentially high biases due to the error in density ratio estimation. In this\npaper, we develop a bias-reduced augmentation of their method, which can take\nadvantage of a learned value function to obtain higher accuracy. Our method is\ndoubly robust in that the bias vanishes when either the density ratio or the\nvalue function estimation is perfect. In general, when either of them is\naccurate, the bias can also be reduced. Both theoretical and empirical results\nshow that our method yields significant advantages over previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 06:33:17 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Tang", "Ziyang", ""], ["Feng", "Yihao", ""], ["Li", "Lihong", ""], ["Zhou", "Dengyong", ""], ["Liu", "Qiang", ""]]}, {"id": "1910.07207", "submitter": "Petros Christodoulou Mr", "authors": "Petros Christodoulou", "title": "Soft Actor-Critic for Discrete Action Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft Actor-Critic is a state-of-the-art reinforcement learning algorithm for\ncontinuous action settings that is not applicable to discrete action settings.\nMany important settings involve discrete actions, however, and so here we\nderive an alternative version of the Soft Actor-Critic algorithm that is\napplicable to discrete action settings. We then show that, even without any\nhyperparameter tuning, it is competitive with the tuned model-free\nstate-of-the-art on a selection of games from the Atari suite.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:11:08 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 09:17:44 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Christodoulou", "Petros", ""]]}, {"id": "1910.07210", "submitter": "Chaitanya K. Joshi", "authors": "Chaitanya K. Joshi, Thomas Laurent, Xavier Bresson", "title": "On Learning Paradigms for the Travelling Salesman Problem", "comments": "Presented at the NeurIPS 2019 Graph Representation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the impact of learning paradigms on training deep neural networks\nfor the Travelling Salesman Problem. We design controlled experiments to train\nsupervised learning (SL) and reinforcement learning (RL) models on fixed graph\nsizes up to 100 nodes, and evaluate them on variable sized graphs up to 500\nnodes. Beyond not needing labelled data, our results reveal favorable\nproperties of RL over SL: RL training leads to better emergent generalization\nto variable graph sizes and is a key component for learning scale-invariant\nsolvers for novel combinatorial problems.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:17:37 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 08:53:08 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Joshi", "Chaitanya K.", ""], ["Laurent", "Thomas", ""], ["Bresson", "Xavier", ""]]}, {"id": "1910.07224", "submitter": "R\\'emy Portelas", "authors": "R\\'emy Portelas, C\\'edric Colas, Katja Hofmann, Pierre-Yves Oudeyer", "title": "Teacher algorithms for curriculum learning of Deep RL in continuously\n  parameterized environments", "comments": "Accepted at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how a teacher algorithm can enable an unknown Deep\nReinforcement Learning (DRL) student to become good at a skill over a wide\nrange of diverse environments. To do so, we study how a teacher algorithm can\nlearn to generate a learning curriculum, whereby it sequentially samples\nparameters controlling a stochastic procedural generation of environments.\nBecause it does not initially know the capacities of its student, a key\nchallenge for the teacher is to discover which environments are easy, difficult\nor unlearnable, and in what order to propose them to maximize the efficiency of\nlearning over the learnable ones. To achieve this, this problem is transformed\ninto a surrogate continuous bandit problem where the teacher samples\nenvironments in order to maximize absolute learning progress of its student. We\npresent a new algorithm modeling absolute learning progress with Gaussian\nmixture models (ALP-GMM). We also adapt existing algorithms and provide a\ncomplete study in the context of DRL. Using parameterized variants of the\nBipedalWalker environment, we study their efficiency to personalize a learning\ncurriculum for different learners (embodiments), their robustness to the ratio\nof learnable/unlearnable environments, and their scalability to non-linear and\nhigh-dimensional parameter spaces. Videos and code are available at\nhttps://github.com/flowersteam/teachDeepRL.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:07:43 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Portelas", "R\u00e9my", ""], ["Colas", "C\u00e9dric", ""], ["Hofmann", "Katja", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1910.07236", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, G\\\"okhan Yildirim", "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided\n  Image Collages", "comments": "To be presented at the NeurIPS 2019 workshop on Creativity and AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting and pasting image segments feels intuitive: the choice of source\ntemplates gives artists flexibility in recombining existing source material.\nFormally, this process takes an image set as input and outputs a collage of the\nset elements. Such selection from sets of source templates does not fit easily\nin classical convolutional neural models requiring inputs of fixed size.\nInspired by advances in attention and set-input machine learning, we present a\nnovel architecture that can generate in one forward pass image collages of\nsource templates using set-structured representations. This paper has the\nfollowing contributions: (i) a novel framework for image generation called\nMemory Attentive Generation of Image Collages (MAGIC) which gives artists new\nways to create digital collages; (ii) from the machine-learning perspective, we\nshow a novel Generative Adversarial Networks (GAN) architecture that uses\nSet-Transformer layers and set-pooling to blend sets of random image samples -\na hybrid non-parametric approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:28:40 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:57:00 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Yildirim", "G\u00f6khan", ""]]}, {"id": "1910.07247", "submitter": "Gard Spreemann", "authors": "Stefania Ebli, Gard Spreemann", "title": "A Notion of Harmonic Clustering in Simplicial Complexes", "comments": null, "journal-ref": "2019 18th IEEE International Conference On Machine Learning And\n  Applications (ICMLA)", "doi": "10.1109/ICMLA.2019.00182", "report-no": null, "categories": "cs.LG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a novel clustering scheme for simplicial complexes that produces\nclusters of simplices in a way that is sensitive to the homology of the\ncomplex. The method is inspired by, and can be seen as a higher-dimensional\nversion of, graph spectral clustering. The algorithm involves only sparse\neigenproblems, and is therefore computationally efficient. We believe that it\nhas broad application as a way to extract features from simplicial complexes\nthat often arise in topological data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:40:20 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ebli", "Stefania", ""], ["Spreemann", "Gard", ""]]}, {"id": "1910.07254", "submitter": "Florian Henkel", "authors": "Florian Henkel, Rainer Kelz, Gerhard Widmer", "title": "Audio-Conditioned U-Net for Position Estimation in Full Sheet Images", "comments": "Accepted at International Workshop on Reading Music Systems 2019\n  (WoRMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of score following is to track a musical performance, usually in the\nform of audio, in a corresponding score representation. Established methods\nmainly rely on computer-readable scores in the form of MIDI or MusicXML and\nachieve robust and reliable tracking results. Recently, multimodal deep\nlearning methods have been used to follow along musical performances in raw\nsheet images. Among the current limits of these systems is that they require a\nnon trivial amount of preprocessing steps that unravel the raw sheet image into\na single long system of staves. The current work is an attempt at removing this\nparticular limitation. We propose an architecture capable of estimating\nmatching score positions directly within entire unprocessed sheet images. We\nargue that this is a necessary first step towards a fully integrated score\nfollowing system that does not rely on any preprocessing steps such as optical\nmusic recognition.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:58:27 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Henkel", "Florian", ""], ["Kelz", "Rainer", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1910.07265", "submitter": "Jeroen Berrevoets", "authors": "Jeroen Berrevoets, Sam Verboven, Wouter Verbeke", "title": "Optimising Individual-Treatment-Effect Using Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying causal inference models in areas such as economics, healthcare and\nmarketing receives great interest from the machine learning community. In\nparticular, estimating the individual-treatment-effect (ITE) in settings such\nas precision medicine and targeted advertising has peaked in application.\nOptimising this ITE under the strong-ignorability-assumption -- meaning all\nconfounders expressing influence on the outcome of a treatment are registered\nin the data -- is often referred to as uplift modeling (UM). While these\ntechniques have proven useful in many settings, they suffer vividly in a\ndynamic environment due to concept drift. Take for example the negative\ninfluence on a marketing campaign when a competitor product is released. To\ncounter this, we propose the uplifted contextual multi-armed bandit (U-CMAB), a\nnovel approach to optimise the ITE by drawing upon bandit literature.\nExperiments on real and simulated data indicate that our proposed approach\ncompares favourably against the state-of-the-art. All our code can be found\nonline at https://github.com/vub-dl/u-cmab.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 10:33:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Berrevoets", "Jeroen", ""], ["Verboven", "Sam", ""], ["Verbeke", "Wouter", ""]]}, {"id": "1910.07283", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico", "title": "FISHDBC: Flexible, Incremental, Scalable, Hierarchical Density-Based\n  Clustering for Arbitrary Data and Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FISHDBC is a flexible, incremental, scalable, and hierarchical density-based\nclustering algorithm. It is flexible because it empowers users to work on\narbitrary data, skipping the feature extraction step that usually transforms\nraw data in numeric arrays letting users define an arbitrary distance function\ninstead. It is incremental and scalable: it avoids the $\\mathcal O(n^2)$\nperformance of other approaches in non-metric spaces and requires only\nlightweight computation to update the clustering when few items are added. It\nis hierarchical: it produces a \"flat\" clustering which can be expanded to a\ntree structure, so that users can group and/or divide clusters in sub- or\nsuper-clusters when data exploration requires so. It is density-based and\napproximates HDBSCAN*, an evolution of DBSCAN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:06:23 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Dell'Amico", "Matteo", ""]]}, {"id": "1910.07294", "submitter": "Ozsel Kilinc", "authors": "Ozsel Kilinc, Yang Hu, Giovanni Montana", "title": "Reinforcement Learning for Robotic Manipulation using Simulated\n  Locomotion Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robotic manipulation through reinforcement learning (RL) using only\nsparse reward signals is still considered a largely unsolved problem.\nLeveraging human demonstrations can make the learning process more sample\nefficient, but obtaining high-quality demonstrations can be costly or\nunfeasible. In this paper we propose a novel approach that introduces\nobject-level demonstrations, i.e. examples of where the objects should be at\nany state. These demonstrations are generated automatically through RL hence\nrequire no expert knowledge. We observe that, during a manipulation task, an\nobject is moved from an initial to a final position. When seen from the point\nof view of the object being manipulated, this induces a locomotion task that\ncan be decoupled from the manipulation task and learnt through a\nphysically-realistic simulator. The resulting object-level trajectories, called\nsimulated locomotion demonstrations (SLDs), are then leveraged to define\nauxiliary rewards that are used to learn the manipulation policy. The proposed\napproach has been evaluated on 13 tasks of increasing complexity, and has been\ndemonstrated to achieve higher success rate and faster learning rates compared\nto alternative algorithms. SLDs are especially beneficial for tasks like\nmulti-object stacking and non-rigid object manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:38:43 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 10:19:13 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 21:58:30 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kilinc", "Ozsel", ""], ["Hu", "Yang", ""], ["Montana", "Giovanni", ""]]}, {"id": "1910.07295", "submitter": "Yuta Saito", "authors": "Yuta Saito", "title": "Offline Recommender Learning Meets Unsupervised Domain Adaptation", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the offline recommender learning problem in the presence of\nselection bias in rating feedback. A current promising solution to address the\nbias is to use the propensity score. However, the performance of the existing\npropensity-based methods can significantly suffer from propensity estimation\nbias. To solve the problem, we formulate the recommendation with selection bias\nas unsupervised domain adaptation and derive a propensity-independent\ngeneralization error bound. We further propose a novel algorithm that minimizes\nthe bound via adversarial learning. Our theory and algorithm do not depend on\npropensity scores, and thus can result in a well-performing rating predictor\nwithout requiring the true propensity information. Empirical evaluation\ndemonstrates the effectiveness and real-world applicability of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:41:17 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 16:36:09 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 21:04:32 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 13:07:20 GMT"}, {"version": "v5", "created": "Thu, 18 Jun 2020 07:56:23 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Saito", "Yuta", ""]]}, {"id": "1910.07320", "submitter": "David Blei", "authors": "Yixin Wang and David M. Blei", "title": "The Blessings of Multiple Causes: A Reply to Ogburn et al. (2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ogburn et al. (2019, arXiv:1910.05438) discuss \"The Blessings of Multiple\nCauses\" (Wang and Blei, 2018, arXiv:1805.06826). Many of their remarks are\ninteresting. But they also claim that the paper has \"foundational errors\" and\nthat its \"premise is...incorrect.\" These claims are not substantiated. There\nare no foundational errors; the premise is correct.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:43:53 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:20:25 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 19:34:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1910.07325", "submitter": "Florian Ziel", "authors": "Florian Ziel, Kevin Berk", "title": "Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper\n  Scoring Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, probabilistic forecasting is an emerging topic, which is why\nthere is a growing need of suitable methods for the evaluation of multivariate\npredictions. We analyze the sensitivity of the most common scoring rules,\nespecially regarding quality of the forecasted dependency structures.\nAdditionally, we propose scoring rules based on the copula, which uniquely\ndescribes the dependency structure for every probability distribution with\ncontinuous marginal distributions. Efficient estimation of the considered\nscoring rules and evaluation methods such as the Diebold-Mariano test are\ndiscussed. In detailed simulation studies, we compare the performance of the\nrenowned scoring rules and the ones we propose. Besides extended synthetic\nstudies based on recently published results we also consider a real data\nexample. We find that the energy score, which is probably the most widely used\nmultivariate scoring rule, performs comparably well in detecting forecast\nerrors, also regarding dependencies. This contradicts other studies. The\nresults also show that a proposed copula score provides very strong distinction\nbetween models with correct and incorrect dependency structure. We close with a\ncomprehensive discussion on the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 12:57:00 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""], ["Berk", "Kevin", ""]]}, {"id": "1910.07344", "submitter": "Micha{\\l} Stypu{\\l}kowski", "authors": "Micha{\\l} Stypu{\\l}kowski, Maciej Zamorski, Maciej Zi\\k{e}ba, Jan\n  Chorowski", "title": "Conditional Invertible Flow for Point Cloud Generation", "comments": "Published in Sets & Partitions Workshop at NeurIPS 2019\n  (https://www.sets.parts/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a novel generative approach for 3D point clouds that\nmakes use of invertible flow-based models. The main idea of the method is to\ntreat a point cloud as a probability density in 3D space that is modeled using\na cloud-specific neural network. To capture the similarity between point clouds\nwe rely on parameter sharing among networks, with each cloud having only a\nsmall embedding vector that defines it. We use invertible flows networks to\ngenerate the individual point clouds, and to regularize the embedding vectors.\nWe evaluate the generative capabilities of the model both in qualitative and\nquantitative manner.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:47:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Stypu\u0142kowski", "Micha\u0142", ""], ["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Chorowski", "Jan", ""]]}, {"id": "1910.07368", "submitter": "Jiwoong Im", "authors": "Daniel Jiwoong Im, Yibo Jiang, Nakul Verma", "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning has emerged as an important framework for learning new tasks\nfrom just a few examples. The success of any meta-learning model depends on (i)\nits fast adaptation to new tasks, as well as (ii) having a shared\nrepresentation across similar tasks. Here we extend the model-agnostic\nmeta-learning (MAML) framework introduced by Finn et al. (2017) to achieve\nimproved performance by analyzing the temporal dynamics of the optimization\nprocedure via the Runge-Kutta method. This method enables us to gain\nfine-grained control over the optimization and helps us achieve both the\nadaptation and representation goals across tasks. By leveraging this refined\ncontrol, we demonstrate that there are multiple principled ways to update MAML\nand show that the classic MAML optimization is simply a special case of\nsecond-order Runge-Kutta method that mainly focuses on fast-adaptation.\nExperiments on benchmark classification, regression and reinforcement learning\ntasks show that this refined control helps attain improved results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:22:35 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 21:28:07 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Jiang", "Yibo", ""], ["Verma", "Nakul", ""]]}, {"id": "1910.07410", "submitter": "Jennifer Hobbs", "authors": "Matthew Holbrook, Jennifer Hobbs, Patrick Lucey", "title": "Rugby-Bot: Utilizing Multi-Task Learning & Fine-Grained Features for\n  Rugby League Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sporting events are extremely complex and require a multitude of metrics to\naccurate describe the event. When making multiple predictions, one should make\nthem from a single source to keep consistency across the predictions. We\npresent a multi-task learning method of generating multiple predictions for\nanalysis via a single prediction source. To enable this approach, we utilize a\nfine-grain representation using fine-grain spatial data using a wide-and-deep\nlearning approach. Additionally, our approach can predict distributions rather\nthan single point values. We highlighted the utility of our approach on the\nsport of Rugby League and call our prediction engine \"Rugby-Bot\".\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:29:33 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Holbrook", "Matthew", ""], ["Hobbs", "Jennifer", ""], ["Lucey", "Patrick", ""]]}, {"id": "1910.07423", "submitter": "Vishnu Naresh Boddeti", "authors": "Bashir Sadeghi, Runyi Yu, Vishnu Naresh Boddeti", "title": "On the Global Optima of Kernelized Adversarial Representation Learning", "comments": "Accepted for publication at ICCV 2019. This version includes\n  additional theoretical and experimental analysis. Minor update to the GMM\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial representation learning is a promising paradigm for obtaining\ndata representations that are invariant to certain sensitive attributes while\nretaining the information necessary for predicting target attributes. Existing\napproaches solve this problem through iterative adversarial minimax\noptimization and lack theoretical guarantees. In this paper, we first study the\n\"linear\" form of this problem i.e., the setting where all the players are\nlinear functions. We show that the resulting optimization problem is both\nnon-convex and non-differentiable. We obtain an exact closed-form expression\nfor its global optima through spectral learning and provide performance\nguarantees in terms of analytical bounds on the achievable utility and\ninvariance. We then extend this solution and analysis to non-linear functions\nthrough kernel representation. Numerical experiments on UCI, Extended Yale B\nand CIFAR-100 datasets indicate that, (a) practically, our solution is ideal\nfor \"imparting\" provable invariance to any biased pre-trained data\nrepresentation, and (b) empirically, the trade-off between utility and\ninvariance provided by our solution is comparable to iterative minimax\noptimization of existing deep neural network based approaches. Code is\navailable at https://github.com/human-analysis/Kernel-ARL\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:37:14 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 16:41:09 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sadeghi", "Bashir", ""], ["Yu", "Runyi", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1910.07425", "submitter": "Tai-Danae Bradley", "authors": "Tai-Danae Bradley, E. Miles Stoudenmire, John Terilla", "title": "Modeling Sequences with Quantum States: A Look Under the Hood", "comments": "27 pages", "journal-ref": "2020 Mach. Learn.: Sci. Technol. 1 035008", "doi": "10.1088/2632-2153/ab8731", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical probability distributions on sets of sequences can be modeled using\nquantum states. Here, we do so with a quantum state that is pure and entangled.\nBecause it is entangled, the reduced densities that describe subsystems also\ncarry information about the complementary subsystem. This is in contrast to the\nclassical marginal distributions on a subsystem in which information about the\ncomplementary system has been integrated out and lost. A training algorithm\nbased on the density matrix renormalization group (DMRG) procedure uses the\nextra information contained in the reduced densities and organizes it into a\ntensor network model. An understanding of the extra information contained in\nthe reduced densities allow us to examine the mechanics of this DMRG algorithm\nand study the generalization error of the resulting model. As an illustration,\nwe work with the even-parity dataset and produce an estimate for the\ngeneralization error as a function of the fraction of the dataset used in\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:39:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Bradley", "Tai-Danae", ""], ["Stoudenmire", "E. Miles", ""], ["Terilla", "John", ""]]}, {"id": "1910.07440", "submitter": "Thomas Booth", "authors": "Thomas Booth, Matthew Williams, Aysha Luis, Jorge Cardoso, Ashkan\n  Keyoumars, Haris Shuaib", "title": "Machine learning and glioma imaging biomarkers", "comments": null, "journal-ref": null, "doi": "10.1016/j.crad.2019.07.001", "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aim: To review how machine learning (ML) is applied to imaging biomarkers in\nneuro-oncology, in particular for diagnosis, prognosis, and treatment response\nmonitoring.\n  Materials and Methods: The PubMed and MEDLINE databases were searched for\narticles published before September 2018 using relevant search terms. The\nsearch strategy focused on articles applying ML to high-grade glioma biomarkers\nfor treatment response monitoring, prognosis, and prediction.\n  Results: Magnetic resonance imaging (MRI) is typically used throughout the\npatient pathway because routine structural imaging provides detailed anatomical\nand pathological information and advanced techniques provide additional\nphysiological detail. Using carefully chosen image features, ML is frequently\nused to allow accurate classification in a variety of scenarios. Rather than\nbeing chosen by human selection, ML also enables image features to be\nidentified by an algorithm. Much research is applied to determining molecular\nprofiles, histological tumour grade, and prognosis using MRI images acquired at\nthe time that patients first present with a brain tumour. Differentiating a\ntreatment response from a post-treatment-related effect using imaging is\nclinically important and also an area of active study (described here in one of\ntwo Special Issue publications dedicated to the application of ML in glioma\nimaging).\n  Conclusion: Although pioneering, most of the evidence is of a low level,\nhaving been obtained retrospectively and in single centres. Studies applying ML\nto build neuro-oncology monitoring biomarker models have yet to show an overall\nadvantage over those using traditional statistical methods. Development and\nvalidation of ML models applied to neuro-oncology require large, well-annotated\ndatasets, and therefore multidisciplinary and multi-centre collaborations are\nnecessary.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 12:44:30 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Booth", "Thomas", ""], ["Williams", "Matthew", ""], ["Luis", "Aysha", ""], ["Cardoso", "Jorge", ""], ["Keyoumars", "Ashkan", ""], ["Shuaib", "Haris", ""]]}, {"id": "1910.07454", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li, Sanjeev Arora", "title": "An Exponential Learning Rate Schedule for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intriguing empirical evidence exists that deep learning can work well with\nexoticschedules for varying the learning rate. This paper suggests that the\nphenomenon may be due to Batch Normalization or BN, which is ubiquitous and\nprovides benefits in optimization and generalization across all standard\narchitectures. The following new results are shown about BN with weight decay\nand momentum (in other words, the typical use case which was not considered in\nearlier theoretical analyses of stand-alone BN.\n  1. Training can be done using SGD with momentum and an exponentially\nincreasing learning rate schedule, i.e., learning rate increases by some $(1\n+\\alpha)$ factor in every epoch for some $\\alpha >0$. (Precise statement in the\npaper.) To the best of our knowledge this is the first time such a rate\nschedule has been successfully used, let alone for highly successful\narchitectures. As expected, such training rapidly blows up network weights, but\nthe net stays well-behaved due to normalization.\n  2. Mathematical explanation of the success of the above rate schedule: a\nrigorous proof that it is equivalent to the standard setting of BN + SGD +\nStandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other\nnormalization layers as well, Group Normalization, LayerNormalization, Instance\nNorm, etc.\n  3. A worked-out toy example illustrating the above linkage of\nhyper-parameters. Using either weight decay or BN alone reaches global minimum,\nbut convergence fails when both are used.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:22:58 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 17:56:03 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 11:01:34 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Li", "Zhiyuan", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1910.07459", "submitter": "Malhar Bhoite", "authors": "Juan Carlos Vargas, Malhar Bhoite, Amir Barati Farimani", "title": "Creativity in Robot Manipulation with Deep Reinforcement Learning", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) has emerged as a powerful control technique\nin robotic science. In contrast to control theory, DRL is more robust in the\nthorough exploration of the environment. This capability of DRL generates more\nhuman-like behaviour and intelligence when applied to the robots. To explore\nthis capability, we designed challenging manipulation tasks to observe robots\nstrategy to handle complex scenarios. We observed that robots not only perform\ntasks successfully, but also transpire a creative and non intuitive solution.\nWe also observed robot's persistence in tasks that are close to success and its\nstriking ability in discerning to continue or give up.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:33:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Vargas", "Juan Carlos", ""], ["Bhoite", "Malhar", ""], ["Farimani", "Amir Barati", ""]]}, {"id": "1910.07467", "submitter": "Biao Zhang", "authors": "Biao Zhang, Rico Sennrich", "title": "Root Mean Square Layer Normalization", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer normalization (LayerNorm) has been successfully applied to various deep\nneural networks to help stabilize training and boost model convergence because\nof its capability in handling re-centering and re-scaling of both inputs and\nweight matrix. However, the computational overhead introduced by LayerNorm\nmakes these improvements expensive and significantly slows the underlying\nnetwork, e.g. RNN in particular. In this paper, we hypothesize that\nre-centering invariance in LayerNorm is dispensable and propose root mean\nsquare layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs\nto a neuron in one layer according to root mean square (RMS), giving the model\nre-scaling invariance property and implicit learning rate adaptation ability.\nRMSNorm is computationally simpler and thus more efficient than LayerNorm. We\nalso present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of\nthe summed inputs without breaking the above properties. Extensive experiments\non several tasks using diverse network architectures show that RMSNorm achieves\ncomparable performance against LayerNorm but reduces the running time by 7%~64%\non different models. Source code is available at\nhttps://github.com/bzhangGo/rmsnorm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:44:22 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zhang", "Biao", ""], ["Sennrich", "Rico", ""]]}, {"id": "1910.07474", "submitter": "Robert Walecki Mr", "authors": "Robert Walecki, Kostis Gourgoulias, Adam Baker, Chris Hart, Chris\n  Lucas, Max Zwiessele, Albert Buchard, Maria Lomeli, Yura Perov, Saurabh Johri", "title": "Universal Marginaliser for Deep Amortised Inference for Probabilistic\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages (PPLs) are powerful modelling tools which\nallow to formalise our knowledge about the world and reason about its inherent\nuncertainty. Inference methods used in PPL can be computationally costly due to\nsignificant time burden and/or storage requirements; or they can lack\ntheoretical guarantees of convergence and accuracy when applied to large scale\ngraphical models. To this end, we present the Universal Marginaliser (UM), a\nnovel method for amortised inference, in PPL. We show how combining samples\ndrawn from the original probabilistic program prior with an appropriate\naugmentation method allows us to train one neural network to approximate any of\nthe corresponding conditional marginal distributions, with any separation into\nlatent and observed variables, and thus amortise the cost of inference.\nFinally, we benchmark the method on multiple probabilistic programs, in Pyro,\nwith different model structure.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:01:02 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Walecki", "Robert", ""], ["Gourgoulias", "Kostis", ""], ["Baker", "Adam", ""], ["Hart", "Chris", ""], ["Lucas", "Chris", ""], ["Zwiessele", "Max", ""], ["Buchard", "Albert", ""], ["Lomeli", "Maria", ""], ["Perov", "Yura", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.07476", "submitter": "Michael Biehl", "authors": "Elisa Oostwal and Michiel Straat and Michael Biehl", "title": "Hidden Unit Specialization in Layered Neural Networks: ReLU vs.\n  Sigmoidal Activation", "comments": "Main changes compared to first version: Added a section on supporting\n  Monte Carlo simulations, results and additional figures are presented and\n  discussed. Some references added. Layout changed to single column layout for\n  better readability. Minor textual changes and typos corrected", "journal-ref": "Physica A: Statistical Mechanics and its Applications 564: 125517,\n  2020", "doi": "10.1016/j.physa.2020.125517", "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study layered neural networks of rectified linear units (ReLU) in a\nmodelling framework for stochastic training processes. The comparison with\nsigmoidal activation functions is in the center of interest. We compute typical\nlearning curves for shallow networks with K hidden units in matching student\nteacher scenarios. The systems exhibit sudden changes of the generalization\nperformance via the process of hidden unit specialization at critical sizes of\nthe training set. Surprisingly, our results show that the training behavior of\nReLU networks is qualitatively different from that of networks with sigmoidal\nactivations. In networks with K >= 3 sigmoidal hidden units, the transition is\ndiscontinuous: Specialized network configurations co-exist and compete with\nstates of poor performance even for very large training sets. On the contrary,\nthe use of ReLU activations results in continuous transitions for all K: For\nlarge enough training sets, two competing, differently specialized states\ndisplay similar generalization abilities, which coincide exactly for large\nnetworks in the limit K to infinity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:06:00 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 05:19:14 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Oostwal", "Elisa", ""], ["Straat", "Michiel", ""], ["Biehl", "Michael", ""]]}, {"id": "1910.07478", "submitter": "Mark Rowland", "authors": "Mark Rowland, Will Dabney, R\\'emi Munos", "title": "Adaptive Trade-Offs in Off-Policy Learning", "comments": "AISTATS 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great variety of off-policy learning algorithms exist in the literature,\nand new breakthroughs in this area continue to be made, improving theoretical\nunderstanding and yielding state-of-the-art reinforcement learning algorithms.\nIn this paper, we take a unifying view of this space of algorithms, and\nconsider their trade-offs of three fundamental quantities: update variance,\nfixed-point bias, and contraction rate. This leads to new perspectives of\nexisting methods, and also naturally yields novel algorithms for off-policy\nevaluation and control. We develop one such algorithm, C-trace, demonstrating\nthat it is able to more efficiently make these trade-offs than existing methods\nin use, and that it can be scaled to yield state-of-the-art performance in\nlarge-scale environments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:09:19 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 11:24:06 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Rowland", "Mark", ""], ["Dabney", "Will", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1910.07479", "submitter": "Mark Rowland", "authors": "Mark Rowland, Anna Harutyunyan, Hado van Hasselt, Diana Borsa, Tom\n  Schaul, R\\'emi Munos, Will Dabney", "title": "Conditional Importance Sampling for Off-Policy Learning", "comments": "AISTATS 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principal contribution of this paper is a conceptual framework for\noff-policy reinforcement learning, based on conditional expectations of\nimportance sampling ratios. This framework yields new perspectives and\nunderstanding of existing off-policy algorithms, and reveals a broad space of\nunexplored algorithms. We theoretically analyse this space, and concretely\ninvestigate several algorithms that arise from this framework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:09:33 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 10:24:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Rowland", "Mark", ""], ["Harutyunyan", "Anna", ""], ["van Hasselt", "Hado", ""], ["Borsa", "Diana", ""], ["Schaul", "Tom", ""], ["Munos", "R\u00e9mi", ""], ["Dabney", "Will", ""]]}, {"id": "1910.07483", "submitter": "Anuj Mahajan", "authors": "Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, Shimon Whiteson", "title": "MAVEN: Multi-Agent Variational Exploration", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 32, 2019,\n  7611-7622", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centralised training with decentralised execution is an important setting for\ncooperative deep multi-agent reinforcement learning due to communication\nconstraints during execution and computational tractability in training. In\nthis paper, we analyse value-based methods that are known to have superior\nperformance in complex environments [43]. We specifically focus on QMIX [40],\nthe current state-of-the-art in this domain. We show that the representational\nconstraints on the joint action-values introduced by QMIX and similar methods\nlead to provably poor exploration and suboptimality. Furthermore, we propose a\nnovel approach called MAVEN that hybridises value and policy-based methods by\nintroducing a latent space for hierarchical control. The value-based agents\ncondition their behaviour on the shared latent variable controlled by a\nhierarchical policy. This allows MAVEN to achieve committed, temporally\nextended exploration, which is key to solving complex multi-agent tasks. Our\nexperimental results show that MAVEN achieves significant performance\nimprovements on the challenging SMAC domain [43].\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:29:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 18:25:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mahajan", "Anuj", ""], ["Rashid", "Tabish", ""], ["Samvelyan", "Mikayel", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1910.07485", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker and Timoth\\'ee Mathieu", "title": "Excess risk bounds in robust empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates robust versions of the general empirical risk\nminimization algorithm, one of the core techniques underlying modern\nstatistical methods. Success of the empirical risk minimization is based on the\nfact that for a \"well-behaved\" stochastic process $\\left\\{ f(X), \\ f\\in\n\\mathcal F\\right\\}$ indexed by a class of functions $f\\in \\mathcal F$, averages\n$\\frac{1}{N}\\sum_{j=1}^N f(X_j)$ evaluated over a sample $X_1,\\ldots,X_N$ of\ni.i.d. copies of $X$ provide good approximation to the expectations $\\mathbb E\nf(X)$ uniformly over large classes $f\\in \\mathcal F$. However, this might no\nlonger be true if the marginal distributions of the process are heavy-tailed or\nif the sample contains outliers. We propose a version of empirical risk\nminimization based on the idea of replacing sample averages by robust proxies\nof the expectation, and obtain high-confidence bounds for the excess risk of\nresulting estimators. In particular, we show that the excess risk of robust\nestimators can converge to $0$ at fast rates with respect to the sample size.\nWe discuss implications of the main results to the linear and logistic\nregression problems, and evaluate the numerical performance of proposed methods\non simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:33:14 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Minsker", "Stanislav", ""], ["Mathieu", "Timoth\u00e9e", ""]]}, {"id": "1910.07487", "submitter": "Joshua Powers", "authors": "Joshua Powers, Ryan Grindle, Sam Kriegman, Lapo Frati, Nick Cheney,\n  Josh Bongard", "title": "Embodiment dictates learnability in neural controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting continues to severely restrict the learnability of\ncontrollers suitable for multiple task environments. Efforts to combat\ncatastrophic forgetting reported in the literature to date have focused on how\ncontrol systems can be updated more rapidly, hastening their adjustment from\ngood initial settings to new environments, or more circumspectly, suppressing\ntheir ability to overfit to any one environment. When using robots, the\nenvironment includes the robot's own body, its shape and material properties,\nand how its actuators and sensors are distributed along its mechanical\nstructure. Here we demonstrate for the first time how one such design decision\n(sensor placement) can alter the landscape of the loss function itself, either\nexpanding or shrinking the weight manifolds containing suitable controllers for\neach individual task, thus increasing or decreasing their probability of\noverlap across tasks, and thus reducing or inducing the potential for\ncatastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:38:20 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 15:06:25 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Powers", "Joshua", ""], ["Grindle", "Ryan", ""], ["Kriegman", "Sam", ""], ["Frati", "Lapo", ""], ["Cheney", "Nick", ""], ["Bongard", "Josh", ""]]}, {"id": "1910.07498", "submitter": "Zuyue Fu", "authors": "Zuyue Fu, Zhuoran Yang, Yongxin Chen, Zhaoran Wang", "title": "Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic\n  Mean-Field Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete-time mean-field Markov games with infinite numbers of\nagents where each agent aims to minimize its ergodic cost. We consider the\nsetting where the agents have identical linear state transitions and quadratic\ncost functions, while the aggregated effect of the agents is captured by the\npopulation mean of their states, namely, the mean-field state. For such a game,\nbased on the Nash certainty equivalence principle, we provide sufficient\nconditions for the existence and uniqueness of its Nash equilibrium. Moreover,\nto find the Nash equilibrium, we propose a mean-field actor-critic algorithm\nwith linear function approximation, which does not require knowing the model of\ndynamics. Specifically, at each iteration of our algorithm, we use the\nsingle-agent actor-critic algorithm to approximately obtain the optimal policy\nof the each agent given the current mean-field state, and then update the\nmean-field state. In particular, we prove that our algorithm converges to the\nNash equilibrium at a linear rate. To the best of our knowledge, this is the\nfirst success of applying model-free reinforcement learning with function\napproximation to discrete-time mean-field Markov games with provable\nnon-asymptotic global convergence guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:59:20 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Fu", "Zuyue", ""], ["Yang", "Zhuoran", ""], ["Chen", "Yongxin", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1910.07512", "submitter": "Guodong Zhang", "authors": "Yuanhao Wang, Guodong Zhang, Jimmy Ba", "title": "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in modern machine learning can be formulated as finding equilibria\nin \\emph{sequential} games. In particular, two-player zero-sum sequential\ngames, also known as minimax optimization, have received growing interest. It\nis tempting to apply gradient descent to solve minimax optimization given its\npopularity and success in supervised learning. However, it has been noted that\nnaive application of gradient descent fails to find some local minimax and can\nconverge to non-local-minimax points. In this paper, we propose\n\\emph{Follow-the-Ridge} (FR), a novel algorithm that provably converges to and\nonly converges to local minimax. We show theoretically that the algorithm\naddresses the notorious rotational behaviour of gradient dynamics, and is\ncompatible with preconditioning and \\emph{positive} momentum. Empirically, FR\nsolves toy minimax problems and improves the convergence of GAN training\ncompared to the recent minimax optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:57:08 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 15:54:54 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wang", "Yuanhao", ""], ["Zhang", "Guodong", ""], ["Ba", "Jimmy", ""]]}, {"id": "1910.07521", "submitter": "Minh Vu", "authors": "Minh H. Vu and Guus Grimbergen and Attila Simk\\'o and Tufve Nyholm and\n  Tommy L\\\"ofstedt", "title": "End-to-End Cascaded U-Nets with a Localization Network for Kidney Tumor\n  Segmentation", "comments": "2019 Kidney Tumor Segmentation Challenge", "journal-ref": null, "doi": "10.24926/548719.073", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kidney tumor segmentation emerges as a new frontier of computer vision in\nmedical imaging. This is partly due to its challenging manual annotation and\ngreat medical impact. Within the scope of the Kidney Tumor Segmentation\nChallenge 2019, that is aiming at combined kidney and tumor segmentation, this\nwork proposes a novel combination of 3D U-Nets---collectively denoted\nTuNet---utilizing the resulting kidney masks for the consecutive tumor\nsegmentation. The proposed method achieves a S{\\o}rensen-Dice coefficient score\nof 0.902 for the kidney, and 0.408 for the tumor segmentation, computed from a\nfive-fold cross-validation on the 210 patients available in the data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:25:23 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Vu", "Minh H.", ""], ["Grimbergen", "Guus", ""], ["Simk\u00f3", "Attila", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "1910.07561", "submitter": "Xiaorui Liu", "authors": "Xiaorui Liu, Yao Li, Jiliang Tang and Ming Yan", "title": "A Double Residual Compression Algorithm for Efficient Distributed\n  Learning", "comments": null, "journal-ref": "PMLR 108:133-143, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning models are often trained by parallel stochastic\ngradient descent algorithms. However, the communication cost of gradient\naggregation and model synchronization between the master and worker nodes\nbecomes the major obstacle for efficient learning as the number of workers and\nthe dimension of the model increase. In this paper, we propose DORE, a DOuble\nREsidual compression stochastic gradient descent algorithm, to reduce over\n$95\\%$ of the overall communication such that the obstacle can be immensely\nmitigated. Our theoretical analyses demonstrate that the proposed strategy has\nsuperior convergence properties for both strongly convex and nonconvex\nobjective functions. The experimental results validate that DORE achieves the\nbest communication efficiency while maintaining similar model accuracy and\nconvergence speed in comparison with start-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:19:19 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Xiaorui", ""], ["Li", "Yao", ""], ["Tang", "Jiliang", ""], ["Yan", "Ming", ""]]}, {"id": "1910.07567", "submitter": "Yichong Xu", "authors": "Yuexin Wu, Yichong Xu, Aarti Singh, Yiming Yang, Artur Dubrawski", "title": "Active Learning for Graph Neural Networks via Node Feature Propagation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) for prediction tasks like node classification or\nedge prediction have received increasing attention in recent machine learning\nfrom graphically structured data. However, a large quantity of labeled graphs\nis difficult to obtain, which significantly limits the true success of GNNs.\nAlthough active learning has been widely studied for addressing label-sparse\nissues with other data types like text, images, etc., how to make it effective\nover graphs is an open question for research. In this paper, we present an\ninvestigation on active learning with GNNs for node classification tasks.\nSpecifically, we propose a new method, which uses node feature propagation\nfollowed by K-Medoids clustering of the nodes for instance selection in active\nlearning. With a theoretical bound analysis we justify the design choice of our\napproach. In our experiments on four benchmark datasets, the proposed method\noutperforms other representative baseline methods consistently and\nsignificantly.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:44:30 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wu", "Yuexin", ""], ["Xu", "Yichong", ""], ["Singh", "Aarti", ""], ["Yang", "Yiming", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1910.07600", "submitter": "Defeng Liu", "authors": "Defeng Liu, Andrea Lodi, Mathieu Tanneau", "title": "Learning chordal extensions", "comments": "Submitted to Journal of Global Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly influential ingredient of many techniques designed to exploit\nsparsity in numerical optimization is the so-called chordal extension of a\ngraph representation of the optimization problem. The definitive relation\nbetween chordal extension and the performance of the optimization algorithm\nthat uses the extension is not a mathematically understood task. For this\nreason, we follow the current research trend of looking at Combinatorial\nOptimization tasks by using a Machine Learning lens, and we devise a framework\nfor learning elimination rules yielding high-quality chordal extensions. As a\nfirst building block of the learning framework, we propose an on-policy\nimitation learning scheme that mimics the elimination ordering provided by the\n(classical) minimum degree rule. The results show that our on-policy imitation\nlearning approach is effective in learning the minimum degree policy and,\nconsequently, produces graphs with desirable fill-in characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 20:25:45 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Liu", "Defeng", ""], ["Lodi", "Andrea", ""], ["Tanneau", "Mathieu", ""]]}, {"id": "1910.07617", "submitter": "Samir Chowdhury", "authors": "Samir Chowdhury, Thomas Gebhart, Steve Huntsman, Matvey Yutin", "title": "Path homologies of deep feedforward networks", "comments": "To appear in the proceedings of IEEE ICMLA 2019", "journal-ref": null, "doi": "10.1109/ICMLA.2019.00181", "report-no": null, "categories": "math.AT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a characterization of two types of directed homology for\nfully-connected, feedforward neural network architectures. These exact\ncharacterizations of the directed homology structure of a neural network\narchitecture are the first of their kind. We show that the directed flag\nhomology of deep networks reduces to computing the simplicial homology of the\nunderlying undirected graph, which is explicitly given by Euler characteristic\ncomputations. We also show that the path homology of these networks is\nnon-trivial in higher dimensions and depends on the number and size of the\nlayers within the network. These results provide a foundation for investigating\nhomological differences between neural network architectures and their realized\nstructure as implied by their parameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:14:55 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chowdhury", "Samir", ""], ["Gebhart", "Thomas", ""], ["Huntsman", "Steve", ""], ["Yutin", "Matvey", ""]]}, {"id": "1910.07623", "submitter": "Azade Nazi", "authors": "Azade Nazi, Will Hang, Anna Goldie, Sujith Ravi, Azalia Mirhoseini", "title": "Generalized Clustering by Learning to Optimize Expected Normalized Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel end-to-end approach for learning to cluster in the\nabsence of labeled examples. Our clustering objective is based on optimizing\nnormalized cuts, a criterion which measures both intra-cluster similarity as\nwell as inter-cluster dissimilarity. We define a differentiable loss function\nequivalent to the expected normalized cuts. Unlike much of the work in\nunsupervised deep learning, our trained model directly outputs final cluster\nassignments, rather than embeddings that need further processing to be usable.\nOur approach generalizes to unseen datasets across a wide variety of domains,\nincluding text, and image. Specifically, we achieve state-of-the-art results on\npopular unsupervised clustering benchmarks (e.g., MNIST, Reuters, CIFAR-10, and\nCIFAR-100), outperforming the strongest baselines by up to 10.9%. Our\ngeneralization results are superior (by up to 21.9%) to the recent\ntop-performing clustering approach with the ability to generalize.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:28:51 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Nazi", "Azade", ""], ["Hang", "Will", ""], ["Goldie", "Anna", ""], ["Ravi", "Sujith", ""], ["Mirhoseini", "Azalia", ""]]}, {"id": "1910.07629", "submitter": "Tao Yu", "authors": "Tao Yu, Shengyuan Hu, Chuan Guo, Wei-Lun Chao, Kilian Q. Weinberger", "title": "A New Defense Against Adversarial Images: Turning a Weakness into a\n  Strength", "comments": "NeurIPS 2019, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images are virtually surrounded by low-density misclassified regions\nthat can be efficiently discovered by gradient-guided search --- enabling the\ngeneration of adversarial images. While many techniques for detecting these\nattacks have been proposed, they are easily bypassed when the adversary has\nfull knowledge of the detection mechanism and adapts the attack strategy\naccordingly. In this paper, we adopt a novel perspective and regard the\nomnipresence of adversarial perturbations as a strength rather than a weakness.\nWe postulate that if an image has been tampered with, these adversarial\ndirections either become harder to find with gradient methods or have\nsubstantially higher density than for natural images. We develop a practical\ntest for this signature characteristic to successfully detect adversarial\nattacks, achieving unprecedented accuracy under the white-box setting where the\nadversary is given full knowledge of our detection mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:48:59 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 04:01:23 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Yu", "Tao", ""], ["Hu", "Shengyuan", ""], ["Guo", "Chuan", ""], ["Chao", "Wei-Lun", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1910.07632", "submitter": "Donglin Zhan", "authors": "Donglin Zhan, Shiyu Yi, Dongli Xu, Xiao Yu, Denglin Jiang, Siqi Yu,\n  Haoting Zhang, Wenfang Shangguan and Weihua Zhang", "title": "Adaptive Transfer Learning of Multi-View Time Series Classification", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Series Classification (TSC) has been an important and challenging task\nin data mining, especially on multivariate time series and multi-view time\nseries data sets. Meanwhile, transfer learning has been widely applied in\ncomputer vision and natural language processing applications to improve deep\nneural network's generalization capabilities. However, very few previous works\napplied transfer learning framework to time series mining problems.\nParticularly, the technique of measuring similarities between source domain and\ntarget domain based on dynamic representation such as density estimation with\nimportance sampling has never been combined with transfer learning framework.\nIn this paper, we first proposed a general adaptive transfer learning framework\nfor multi-view time series data, which shows strong ability in storing\ninter-view importance value in the process of knowledge transfer. Next, we\nrepresented inter-view importance through some time series similarity\nmeasurements and approximated the posterior distribution in latent space for\nthe importance sampling via density estimation techniques. We then computed the\nmatrix norm of sampled importance value, which controls the degree of knowledge\ntransfer in pre-training process. We further evaluated our work, applied it to\nmany other time series classification tasks, and observed that our architecture\nmaintained desirable generalization ability. Finally, we concluded that our\nframework could be adapted with deep learning techniques to receive significant\nmodel performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:46:03 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhan", "Donglin", ""], ["Yi", "Shiyu", ""], ["Xu", "Dongli", ""], ["Yu", "Xiao", ""], ["Jiang", "Denglin", ""], ["Yu", "Siqi", ""], ["Zhang", "Haoting", ""], ["Shangguan", "Wenfang", ""], ["Zhang", "Weihua", ""]]}, {"id": "1910.07633", "submitter": "Xiaoyang Xu", "authors": "Xiaoyang Xu, Yiqun Liu, Hanqing Chao, Youcheng Luo, Hai Chu, Lei Chen,\n  Junping Zhang, Leiming Ma", "title": "Towards a Precipitation Bias Corrector against Noise and Maldistribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With broad applications in various public services like aviation management\nand urban disaster warning, numerical precipitation prediction plays a crucial\nrole in weather forecast. However, constrained by the limitation of observation\nand conventional meteorological models, the numerical precipitation predictions\nare often highly biased. To correct this bias, classical correction methods\nheavily depend on profound experts who have knowledge in aerodynamics,\nthermodynamics and meteorology. As precipitation can be influenced by countless\nfactors, however, the performances of these expert-driven methods can drop\ndrastically when some un-modeled factors change. To address this issue, this\npaper presents a data-driven deep learning model which mainly includes two\nblocks, i.e. a Denoising Autoencoder Block and an Ordinal Regression Block. To\nthe best of our knowledge, it is the first expert-free models for bias\ncorrection. The proposed model can effectively correct the numerical\nprecipitation prediction based on 37 basic meteorological data from European\nCentre for Medium-Range Weather Forecasts (ECMWF). Experiments indicate that\ncompared with several classical machine learning algorithms and deep learning\nmodels, our method achieves the best correcting performance and meteorological\nindex, namely the threat scores (TS), obtaining satisfactory visualization\neffect.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:25:58 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Xu", "Xiaoyang", ""], ["Liu", "Yiqun", ""], ["Chao", "Hanqing", ""], ["Luo", "Youcheng", ""], ["Chu", "Hai", ""], ["Chen", "Lei", ""], ["Zhang", "Junping", ""], ["Ma", "Leiming", ""]]}, {"id": "1910.07643", "submitter": "Osman Asif Malik", "authors": "Osman Asif Malik, Shashanka Ubaru, Lior Horesh, Misha E. Kilmer, Haim\n  Avron", "title": "Dynamic Graph Convolutional Networks Using the Tensor M-Product", "comments": "Accepted to SIAM International Conference on Data Mining (SDM) 2021", "journal-ref": null, "doi": "10.1137/1.9781611976700.82", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many irregular domains such as social networks, financial transactions,\nneuron connections, and natural language constructs are represented using graph\nstructures. In recent years, a variety of graph neural networks (GNNs) have\nbeen successfully applied for representation learning and prediction on such\ngraphs. In many of the real-world applications, the underlying graph changes\nover time, however, most of the existing GNNs are inadequate for handling such\ndynamic graphs. In this paper we propose a novel technique for learning\nembeddings of dynamic graphs using a tensor algebra framework. Our method\nextends the popular graph convolutional network (GCN) for learning\nrepresentations of dynamic graphs using the recently proposed tensor M-product\ntechnique. Theoretical results presented establish a connection between the\nproposed tensor approach and spectral convolution of tensors. The proposed\nmethod TM-GCN is consistent with the Message Passing Neural Network (MPNN)\nframework, accounting for both spatial and temporal message passing. Numerical\nexperiments on real-world datasets demonstrate the performance of the proposed\nmethod for edge classification and link prediction tasks on dynamic graphs. We\nalso consider an application related to the COVID-19 pandemic, and show how our\nmethod can be used for early detection of infected individuals from contact\ntracing data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 23:06:34 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 02:28:32 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 02:46:40 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Malik", "Osman Asif", ""], ["Ubaru", "Shashanka", ""], ["Horesh", "Lior", ""], ["Kilmer", "Misha E.", ""], ["Avron", "Haim", ""]]}, {"id": "1910.07663", "submitter": "James P. Crutchfield", "authors": "S. E. Marzen and J. P. Crutchfield", "title": "Probabilistic Deterministic Finite Automata and Recurrent Networks,\n  Revisited", "comments": "15 pages, 4 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/pdfarnr.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computers (RCs) and recurrent neural networks (RNNs) can mimic any\nfinite-state automaton in theory, and some workers demonstrated that this can\nhold in practice. We test the capability of generalized linear models, RCs, and\nLong Short-Term Memory (LSTM) RNN architectures to predict the stochastic\nprocesses generated by a large suite of probabilistic deterministic\nfinite-state automata (PDFA). PDFAs provide an excellent performance benchmark\nin that they can be systematically enumerated, the randomness and correlation\nstructure of their generated processes are exactly known, and their optimal\nmemory-limited predictors are easily computed. Unsurprisingly, LSTMs outperform\nRCs, which outperform generalized linear models. Surprisingly, each of these\nmethods can fall short of the maximal predictive accuracy by as much as 50%\nafter training and, when optimized, tend to fall short of the maximal\npredictive accuracy by ~5%, even though previously available methods achieve\nmaximal predictive accuracy with orders-of-magnitude less data. Thus, despite\nthe representational universality of RCs and RNNs, using them can engender a\nsurprising predictive gap for simple stimuli. One concludes that there is an\nimportant and underappreciated role for methods that infer \"causal states\" or\n\"predictive state representations\".\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 00:32:12 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Marzen", "S. E.", ""], ["Crutchfield", "J. P.", ""]]}, {"id": "1910.07703", "submitter": "Jiacheng Zhuo", "authors": "Jiacheng Zhuo, Qi Lei, Alexandros G. Dimakis, Constantine Caramanis", "title": "Communication-Efficient Asynchronous Stochastic Frank-Wolfe over\n  Nuclear-norm Balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning training suffers from two prior challenges,\nspecifically for nuclear-norm constrained problems with distributed systems:\nthe synchronization slowdown due to the straggling workers, and high\ncommunication costs. In this work, we propose an asynchronous Stochastic Frank\nWolfe (SFW-asyn) method, which, for the first time, solves the two problems\nsimultaneously, while successfully maintaining the same convergence rate as the\nvanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon\nEC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number\nof machines compared to the vanilla SFW.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 04:22:21 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhuo", "Jiacheng", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1910.07719", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Brenden Petersen, Hongyuan Zha, Daniel Faissol", "title": "Single Episode Policy Transfer in Reinforcement Learning", "comments": "Published at International Conference on Learning Representations\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer and adaptation to new unknown environmental dynamics is a key\nchallenge for reinforcement learning (RL). An even greater challenge is\nperforming near-optimally in a single attempt at test time, possibly without\naccess to dense rewards, which is not addressed by current methods that require\nmultiple experience rollouts for adaptation. To achieve single episode transfer\nin a family of environments with related dynamics, we propose a general\nalgorithm that optimizes a probe and an inference model to rapidly estimate\nunderlying latent variables of test dynamics, which are then immediately used\nas input to a universal control policy. This modular approach enables\nintegration of state-of-the-art algorithms for variational inference or RL.\nMoreover, our approach does not require access to rewards at test time,\nallowing it to perform in settings where existing adaptive approaches cannot.\nIn diverse experimental domains with a single episode test constraint, our\nmethod significantly outperforms existing adaptive approaches and shows\nfavorable performance against baselines for robust transfer.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:37:50 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 17:24:49 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 19:54:41 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Yang", "Jiachen", ""], ["Petersen", "Brenden", ""], ["Zha", "Hongyuan", ""], ["Faissol", "Daniel", ""]]}, {"id": "1910.07737", "submitter": "Alexander Li", "authors": "Murtaza Dalal, Alexander C. Li, Rohan Taori", "title": "Autoregressive Models: What Are They Good For?", "comments": "Accepted for the Information Theory and Machine Learning workshop at\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive (AR) models have become a popular tool for unsupervised\nlearning, achieving state-of-the-art log likelihood estimates. We investigate\nthe use of AR models as density estimators in two settings -- as a learning\nsignal for image translation, and as an outlier detector -- and find that these\ndensity estimates are much less reliable than previously thought. We examine\nthe underlying optimization issues from both an empirical and theoretical\nperspective, and provide a toy example that illustrates the problem.\nOverwhelmingly, we find that density estimates do not correlate with perceptual\nquality and are unhelpful for downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:04:13 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Dalal", "Murtaza", ""], ["Li", "Alexander C.", ""], ["Taori", "Rohan", ""]]}, {"id": "1910.07747", "submitter": "Eunjin Jeon", "authors": "Eunjin Jeon, Wonjun Ko, Jee Seok Yoon, Heung-Il Suk", "title": "Mutual Information-driven Subject-invariant and Class-relevant Deep\n  Representation Learning in BCI", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based feature representation methods have\nshown a promising impact in electroencephalography (EEG)-based brain-computer\ninterface (BCI). Nonetheless, owing to high intra- and inter-subject\nvariabilities, many studies on decoding EEG were designed in a subject-specific\nmanner by using calibration samples, with no concern of its practical use,\nhampered by time-consuming steps and a large data requirement. To this end,\nrecent studies adopted a transfer learning strategy, especially domain\nadaptation techniques. Among those, to our knowledge, an adversarial learning\nhas shown its potential in BCIs. In the meantime, it is known that adversarial\nlearning-based domain adaptation methods are prone to negative transfer that\ndisrupts learning generalized feature representations, applicable to diverse\ndomains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel\nframework that learns class-relevant and subject-invariant feature\nrepresentations in an information-theoretic manner, without using adversarial\nlearning. To be specific, we devise two operational components in a deep\nnetwork that explicitly estimate mutual information between feature\nrepresentations; (1) to decompose features in an intermediate layer into\nclass-relevant and class-irrelevant ones, (2) to enrich class-discriminative\nfeature representation. On two large EEG datasets, we validated the\neffectiveness of our proposed framework by comparing with several comparative\nmethods in performance. Further, we conducted rigorous analyses by performing\nan ablation study in regard to the components in our network, explaining our\nmodel's decision on input EEG signals via layer-wise relevance propagation, and\nvisualizing the distribution of learned features via t-SNE.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:32:40 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 09:24:45 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 01:27:47 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 06:32:26 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Jeon", "Eunjin", ""], ["Ko", "Wonjun", ""], ["Yoon", "Jee Seok", ""], ["Suk", "Heung-Il", ""]]}, {"id": "1910.07755", "submitter": "Hufei Zhu", "authors": "Hufei Zhu, Chenghao Wei", "title": "Reducing the Computational Complexity of Pseudoinverse for the\n  Incremental Broad Learning System on Added Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief, we improve the Broad Learning System (BLS) [7] by reducing the\ncomputational complexity of the incremental learning for added inputs. We\nutilize the inverse of a sum of matrices in [8] to improve a step in the\npseudoinverse of a row-partitioned matrix. Accordingly we propose two fast\nalgorithms for the cases of q > k and q < k, respectively, where q and k denote\nthe number of additional training samples and the total number of nodes,\nrespectively. Specifically, when q > k, the proposed algorithm computes only a\nk * k matrix inverse, instead of a q * q matrix inverse in the existing\nalgorithm. Accordingly it can reduce the complexity dramatically. Our\nsimulations, which follow those for Table V in [7], show that the proposed\nalgorithm and the existing algorithm achieve the same testing accuracy, while\nthe speedups in BLS training time of the proposed algorithm over the existing\nalgorithm are 1.24 - 1.30.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:58:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhu", "Hufei", ""], ["Wei", "Chenghao", ""]]}, {"id": "1910.07762", "submitter": "Zengyi Li", "authors": "Zengyi Li, Yubei Chen, Friedrich T. Sommer", "title": "Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale\n  Denoising Score Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-Based Models (EBMs) assign unnormalized log-probability to data\nsamples. This functionality has a variety of applications, such as sample\nsynthesis, data denoising, sample restoration, outlier detection, Bayesian\nreasoning, and many more. But training of EBMs using standard maximum\nlikelihood is extremely slow because it requires sampling from the model\ndistribution. Score matching potentially alleviates this problem. In\nparticular, denoising score matching \\citep{vincent2011connection} has been\nsuccessfully used to train EBMs. Using noisy data samples with one fixed noise\nlevel, these models learn fast and yield good results in data denoising\n\\citep{saremi2019neural}. However, demonstrations of such models in high\nquality sample synthesis of high dimensional data were lacking. Recently,\n\\citet{song2019generative} have shown that a generative model trained by\ndenoising score matching accomplishes excellent sample synthesis, when trained\nwith data samples corrupted with multiple levels of noise. Here we provide\nanalysis and empirical evidence showing that training with multiple noise\nlevels is necessary when the data dimension is high. Leveraging this insight,\nwe propose a novel EBM trained with multi-scale denoising score matching. Our\nmodel exhibits data generation performance comparable to state-of-the-art\ntechniques such as GANs, and sets a new baseline for EBMs. The proposed model\nalso provides density information and performs well in an image inpainting\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:21:17 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 04:58:04 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Zengyi", ""], ["Chen", "Yubei", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1910.07763", "submitter": "Andreas Kopf", "authors": "Andreas Kopf, Vincent Fortuin, Vignesh Ram Somnath, Manfred Claassen", "title": "Mixture-of-Experts Variational Autoencoder for Clustering and Generating\n  from Similarity-Based Representations on Single Cell Data", "comments": "Submitted to PLOS Computational Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering high-dimensional data, such as images or biological measurements,\nis a long-standingproblem and has been studied extensively. Recently, Deep\nClustering has gained popularity due toits flexibility in fitting the specific\npeculiarities of complex data. Here we introduce the Mixture-of-Experts\nSimilarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering\nmodel.The model can learn multi-modal distributions of high-dimensional data\nand use these to generaterealistic data with high efficacy and efficiency.\nMoE-Sim-VAE is based on a Variational Autoencoder(VAE), where the decoder\nconsists of a Mixture-of-Experts (MoE) architecture. This specific architecture\nallows for various modes of the data to be automatically learned by means of\nthe experts.Additionally, we encourage the lower dimensional latent\nrepresentation of our model to follow aGaussian mixture distribution and to\naccurately represent the similarities between the data points. Weassess the\nperformance of our model on the MNIST benchmark data set and challenging\nreal-worldtasks of clustering mouse organs from single-cell RNA-sequencing\nmeasurements and defining cellsubpopulations from mass cytometry (CyTOF)\nmeasurements on hundreds of different datasets.MoE-Sim-VAE exhibits superior\nclustering performance on all these tasks in comparison to thebaselines as well\nas competitor methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:21:28 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:05:05 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 14:23:42 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kopf", "Andreas", ""], ["Fortuin", "Vincent", ""], ["Somnath", "Vignesh Ram", ""], ["Claassen", "Manfred", ""]]}, {"id": "1910.07772", "submitter": "Florian Wirthm\\\"uller", "authors": "Florian Wirthm\\\"uller, Julian Schlechtriemen, Jochen Hipp and Manfred\n  Reichert", "title": "Teaching Vehicles to Anticipate: A Systematic Study on Probabilistic\n  Behavior Prediction Using Large Data Sets", "comments": "the paper has been accepted for publication in IEEE Transcations on\n  Intelligent Transportation Systems (T-ITS) 16 pages 13 figures 12 tables", "journal-ref": null, "doi": "10.1109/TITS.2020.3002070", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By observing their environment as well as other traffic participants, humans\nare enabled to drive road vehicles safely. Vehicle passengers, however,\nperceive a notable difference between non-experienced and experienced drivers.\nIn particular, they may get the impression that the latter ones anticipate what\nwill happen in the next few moments and consider these foresights in their\ndriving behavior. To make the driving style of automated vehicles comparable to\nthe one of human drivers with respect to comfort and perceived safety, the\naforementioned anticipation skills need to become a built-in feature of\nself-driving vehicles. This article provides a systematic comparison of methods\nand strategies to generate this intention for self-driving cars using machine\nlearning techniques. To implement and test these algorithms we use a large data\nset collected over more than 30000 km of highway driving and containing\napproximately 40000 real-world driving situations. We further show that it is\npossible to classify driving maneuvers upcoming within the next 5 s with an\nArea Under the ROC Curve (AUC) above 0.92 for all defined maneuver classes.\nThis enables us to predict the lateral position with a prediction horizon of 5\ns with a median lateral error of less than 0.21 m.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:42:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 13:46:33 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 15:14:48 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 15:43:01 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wirthm\u00fcller", "Florian", ""], ["Schlechtriemen", "Julian", ""], ["Hipp", "Jochen", ""], ["Reichert", "Manfred", ""]]}, {"id": "1910.07779", "submitter": "Ryan-Rhys Griffiths", "authors": "Ryan-Rhys Griffiths, Alexander A. Aldrick, Miguel Garcia-Ortegon,\n  Vidhi R. Lalchand, Alpha A. Lee", "title": "Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic\n  Bayesian Optimisation", "comments": "Accepted to the 2019 NeurIPS Workshop on Safety and Robustness in\n  Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation is a sample-efficient search methodology that holds\ngreat promise for accelerating drug and materials discovery programs. A\nfrequently-overlooked modelling consideration in Bayesian optimisation\nstrategies however, is the representation of heteroscedastic aleatoric\nuncertainty. In many practical applications it is desirable to identify inputs\nwith low aleatoric noise, an example of which might be a material composition\nwhich consistently displays robust properties in response to a noisy\nfabrication process. In this paper, we propose a heteroscedastic Bayesian\noptimisation scheme capable of representing and minimising aleatoric noise\nacross the input space. Our scheme employs a heteroscedastic Gaussian process\n(GP) surrogate model in conjunction with two straightforward adaptations of\nexisting acquisition functions. First, we extend the augmented expected\nimprovement (AEI) heuristic to the heteroscedastic setting and second, we\nintroduce the aleatoric noise-penalised expected improvement (ANPEI) heuristic.\nBoth methodologies are capable of penalising aleatoric noise in the suggestions\nand yield improved performance relative to homoscedastic Bayesian optimisation\nand random sampling on toy problems as well as on two real-world scientific\ndatasets. Code is available at:\n\\url{https://github.com/Ryan-Rhys/Heteroscedastic-BO}\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:15:46 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 22:28:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Griffiths", "Ryan-Rhys", ""], ["Aldrick", "Alexander A.", ""], ["Garcia-Ortegon", "Miguel", ""], ["Lalchand", "Vidhi R.", ""], ["Lee", "Alpha A.", ""]]}, {"id": "1910.07796", "submitter": "Neta Shoham", "authors": "Neta Shoham (Edgify), Tomer Avidor (Edgify), Aviv Keren (Edgify),\n  Nadav Israel (Edgify), Daniel Benditkis (Edgify), Liron Mor-Yosef (Edgify),\n  Itai Zeitak (Edgify)", "title": "Overcoming Forgetting in Federated Learning on Non-IID Data", "comments": "Accepted to NeurIPS 2019 Workshop on Federated Learning for Data\n  Privacy and Confidentiality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of Federated Learning in the non i.i.d. case, in which\nlocal models drift apart, inhibiting learning. Building on an analogy with\nLifelong Learning, we adapt a solution for catastrophic forgetting to Federated\nLearning. We add a penalty term to the loss function, compelling all local\nmodels to converge to a shared optimum. We show that this can be done\nefficiently for communication (adding no further privacy risks), scaling with\nthe number of nodes in the distributed setting. Our experiments show that this\nmethod is superior to competing ones for image recognition on the MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:53:16 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Shoham", "Neta", "", "Edgify"], ["Avidor", "Tomer", "", "Edgify"], ["Keren", "Aviv", "", "Edgify"], ["Israel", "Nadav", "", "Edgify"], ["Benditkis", "Daniel", "", "Edgify"], ["Mor-Yosef", "Liron", "", "Edgify"], ["Zeitak", "Itai", "", "Edgify"]]}, {"id": "1910.07817", "submitter": "Soroosh Shafieezadeh-Abadeh", "authors": "Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Man-Chung Yue, Daniel\n  Kuhn, Wolfram Wiesemann", "title": "Calculating Optimistic Likelihoods Using (Geodesically) Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem arising in many areas of machine learning is the\nevaluation of the likelihood of a given observation under different nominal\ndistributions. Frequently, these nominal distributions are themselves estimated\nfrom data, which makes them susceptible to estimation errors. We thus propose\nto replace each nominal distribution with an ambiguity set containing all\ndistributions in its vicinity and to evaluate an \\emph{optimistic likelihood},\nthat is, the maximum of the likelihood over all distributions in the ambiguity\nset. When the proximity of distributions is quantified by the Fisher-Rao\ndistance or the Kullback-Leibler divergence, the emerging optimistic\nlikelihoods can be computed efficiently using either geodesic or standard\nconvex optimization techniques. We showcase the advantages of working with\noptimistic likelihoods on a classification problem using synthetic as well as\nempirical data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 10:40:02 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Shafieezadeh-Abadeh", "Soroosh", ""], ["Yue", "Man-Chung", ""], ["Kuhn", "Daniel", ""], ["Wiesemann", "Wolfram", ""]]}, {"id": "1910.07833", "submitter": "Nikita Zhivotovskiy", "authors": "Olivier Bousquet, Yegor Klochkov, Nikita Zhivotovskiy", "title": "Sharper bounds for uniformly stable algorithms", "comments": "17 pages, minor improvements, to appear in COLT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving generalization bounds for stable algorithms is a classical question\nin learning theory taking its roots in the early works by Vapnik and\nChervonenkis (1974) and Rogers and Wagner (1978). In a series of recent\nbreakthrough papers by Feldman and Vondrak (2018, 2019), it was shown that the\nbest known high probability upper bounds for uniformly stable learning\nalgorithms due to Bousquet and Elisseef (2002) are sub-optimal in some natural\nregimes. To do so, they proved two generalization bounds that significantly\noutperform the simple generalization bound of Bousquet and Elisseef (2002).\nFeldman and Vondrak also asked if it is possible to provide sharper bounds and\nprove corresponding high probability lower bounds. This paper is devoted to\nthese questions: firstly, inspired by the original arguments of Feldman and\nVondrak (2019), we provide a short proof of the moment bound that implies the\ngeneralization bound stronger than both recent results (Feldman and Vondrak,\n2018, 2019). Secondly, we prove general lower bounds, showing that our moment\nbound is sharp (up to a logarithmic factor) unless some additional properties\nof the corresponding random variables are used. Our main probabilistic result\nis a general concentration inequality for weakly correlated random variables,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 11:42:37 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 08:08:48 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bousquet", "Olivier", ""], ["Klochkov", "Yegor", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "1910.07842", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov", "title": "Kernel density estimation based sampling for imbalanced class\n  distribution", "comments": "This is the version of the article that was accepted in Information\n  Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2019.10.017", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalanced response variable distribution is a common occurrence in data\nscience. In fields such as fraud detection, medical diagnostics, system\nintrusion detection and many others where abnormal behavior is rarely observed\nthe data under study often features disproportionate target class distribution.\nOne common way to combat class imbalance is through resampling the minority\nclass to achieve a more balanced distribution. In this paper, we investigate\nthe performance of the sampling method based on kernel density estimation\n(KDE). We believe that KDE offers a more natural way of generating new\ninstances of minority class that is less prone to overfitting than other\nstandard sampling techniques. It is based on a well established theory of\nnonparametric statistical estimation. Numerical experiments show that KDE can\noutperform other sampling techniques on a range of real life datasets as\nmeasured by F1-score and G-mean. The results remain consistent across a number\nof classification algorithms used in the experiments. Furthermore, the proposed\nmethod outperforms the benchmark methods irregardless of the class distribution\nratio. We conclude, based on the solid theoretical foundation and strong\nexperimental results, that the proposed method would be a valuable tool in\nproblems involving imbalanced class distribution.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 11:52:30 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 09:51:06 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kamalov", "Firuz", ""]]}, {"id": "1910.07856", "submitter": "Johannes Rabold", "authors": "Ludwig Schallner, Johannes Rabold, Oliver Scholz, Ute Schmid", "title": "Effect of Superpixel Aggregation on Explanations in LIME -- A Case Study\n  with Biological Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning with deep neural networks, such as convolutional neural\nnetworks (CNNs), has been demonstrated to be very successful for different\ntasks of image classification. To make decisions of black-box approaches\ntransparent, different solutions have been proposed. LIME is an approach to\nexplainable AI relying on segmenting images into superpixels based on the\nQuick-Shift algorithm. In this paper, we present an explorative study of how\ndifferent superpixel methods, namely Felzenszwalb, SLIC and Compact-Watershed,\nimpact the generated visual explanations. We compare the resulting relevance\nareas with the image parts marked by a human reference. Results show that image\nparts selected as relevant strongly vary depending on the applied method.\nQuick-Shift resulted in the least and Compact-Watershed in the highest\ncorrespondence with the reference relevance areas.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:30:05 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Schallner", "Ludwig", ""], ["Rabold", "Johannes", ""], ["Scholz", "Oliver", ""], ["Schmid", "Ute", ""]]}, {"id": "1910.07870", "submitter": "Kush Varshney", "authors": "Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu,\n  Kush R. Varshney", "title": "Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using\n  Mismatched Hypothesis Testing", "comments": "This paper appears in the Proceedings of the 37th International\n  Conference on Machine Learning, pp. 2803--2813, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trade-off between accuracy and fairness is almost taken as a given in the\nexisting literature on fairness in machine learning. Yet, it is not preordained\nthat accuracy should decrease with increased fairness. Novel to this work, we\nexamine fair classification through the lens of mismatched hypothesis testing:\ntrying to find a classifier that distinguishes between two ideal distributions\nwhen given two mismatched distributions that are biased. Using Chernoff\ninformation, a tool in information theory, we theoretically demonstrate that,\ncontrary to popular belief, there always exist ideal distributions such that\noptimal fairness and accuracy (with respect to the ideal distributions) are\nachieved simultaneously: there is no trade-off. Moreover, the same classifier\nyields the lack of a trade-off with respect to ideal distributions while\nyielding a trade-off when accuracy is measured with respect to the given\n(possibly biased) dataset. To complement our main result, we formulate an\noptimization to find ideal distributions and derive fundamental limits to\nexplain why a trade-off exists on the given biased dataset. We also derive\nconditions under which active data collection can alleviate the\nfairness-accuracy trade-off in the real world. Our results lead us to contend\nthat it is problematic to measure accuracy with respect to data that reflects\nbias, and instead, we should be considering accuracy with respect to ideal,\nunbiased data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:59:25 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 20:03:48 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Wei", "Dennis", ""], ["Yueksel", "Hazar", ""], ["Chen", "Pin-Yu", ""], ["Liu", "Sijia", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1910.07892", "submitter": "Wenhao Zhang", "authors": "Wenhao Zhang, Ramin Ramezani, Arash Naeim", "title": "WOTBoost: Weighted Oversampling Technique in Boosting for imbalanced\n  learning", "comments": "10 pages, 5 figures, 3 tables; Accepted by 5th Special Session on\n  Intelligent Data Mining in IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers often stumble over imbalanced datasets where\nclasses are not equally represented. This inherent bias towards the majority\nclass may result in low accuracy in labeling minority class. Imbalanced\nlearning is prevalent in many real-world applications, such as medical\nresearch, network intrusion detection, and fraud detection in credit card\ntransactions, etc. A good number of research works have been reported to tackle\nthis challenging problem. For example, Synthetic Minority Over-sampling\nTEchnique (SMOTE) and ADAptive SYNthetic sampling approach (ADASYN) use\noversampling techniques to balance the skewed datasets. In this paper, we\npropose a novel method that combines a Weighted Oversampling Technique and\nensemble Boosting method (WOTBoost) to improve the classification accuracy of\nminority data without sacrificing the accuracy of the majority class. WOTBoost\nadjusts its oversampling strategy at each round of boosting to synthesize more\ntargeted minority data samples. The adjustment is enforced using a weighted\ndistribution. We compare WOTBoost with other four classification models (i.e.,\ndecision tree, SMOTE + decision tree, ADASYN + decision tree, SMOTEBoost)\nextensively on 18 public accessible imbalanced datasets. WOTBoost achieves the\nbest G mean on 6 datasets and highest AUC score on 7 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:27:04 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 20:10:50 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 19:36:59 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhang", "Wenhao", ""], ["Ramezani", "Ramin", ""], ["Naeim", "Arash", ""]]}, {"id": "1910.07899", "submitter": "Hari Prasanna Das", "authors": "Ioannis C. Konstantakopoulos, Hari Prasanna Das, Andrew R. Barkan,\n  Shiying He, Tanya Veeravalli, Huihan Liu, Aummul Baneen Manasawala, Yu-Wen\n  Lin and Costas J. Spanos", "title": "Design, Benchmarking and Explainability Analysis of a Game-Theoretic\n  Framework towards Energy Efficiency in Smart Infrastructure", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.05142,\n  arXiv:1810.10533", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a gamification approach as a novel framework for\nsmart building infrastructure with the goal of motivating human occupants to\nreconsider personal energy usage and to have positive effects on their\nenvironment. Human interaction in the context of cyber-physical systems is a\ncore component and consideration in the implementation of any smart building\ntechnology. Research has shown that the adoption of human-centric building\nservices and amenities leads to improvements in the operational efficiency of\nthese cyber-physical systems directed towards controlling building energy\nusage. We introduce a strategy in form of a game-theoretic framework that\nincorporates humans-in-the-loop modeling by creating an interface to allow\nbuilding managers to interact with occupants and potentially incentivize energy\nefficient behavior. Prior works on game theoretic analysis typically rely on\nthe assumption that the utility function of each individual agent is known a\npriori. Instead, we propose novel utility learning framework for benchmarking\nthat employs robust estimations of occupant actions towards energy efficiency.\nTo improve forecasting performance, we extend the utility learning scheme by\nleveraging deep bi-directional recurrent neural networks. Using the proposed\nmethods on data gathered from occupant actions for resources such as room\nlighting, we forecast patterns of energy resource usage to demonstrate the\nprediction performance of the methods. The results of our study show that we\ncan achieve a highly accurate representation of the ground truth for occupant\nenergy resource usage. We also demonstrate the explainable nature on human\ndecision making towards energy usage inherent in the dataset using graphical\nlasso and granger causality algorithms. Finally, we open source the\nde-identified, high-dimensional data pertaining to the energy game-theoretic\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:36:26 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Konstantakopoulos", "Ioannis C.", ""], ["Das", "Hari Prasanna", ""], ["Barkan", "Andrew R.", ""], ["He", "Shiying", ""], ["Veeravalli", "Tanya", ""], ["Liu", "Huihan", ""], ["Manasawala", "Aummul Baneen", ""], ["Lin", "Yu-Wen", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1910.07918", "submitter": "Jun Sun", "authors": "Jun Sun, Steffen Staab, J\\'er\\^ome Kunegis", "title": "Understanding Social Networks using Transfer Learning", "comments": "11 pages, 4 figures, IEEE Computer. arXiv admin note: text overlap\n  with arXiv:1611.02941", "journal-ref": "IEEE Computer (Volume: 51, Issue: 6, June 2018)", "doi": "10.1109/MC.2018.2701640", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A detailed understanding of users contributes to the understanding of the\nWeb's evolution, and to the development of Web applications. Although for new\nWeb platforms such a study is especially important, it is often jeopardized by\nthe lack of knowledge about novel phenomena due to the sparsity of data. Akin\nto human transfer of experiences from one domain to the next, transfer learning\nas a subfield of machine learning adapts knowledge acquired in one domain to a\nnew domain. We systematically investigate how the concept of transfer learning\nmay be applied to the study of users on newly created (emerging) Web platforms,\nand propose our transfer learning-based approach, TraNet. We show two use cases\nwhere TraNet is applied to tasks involving the identification of user trust and\nroles on different Web platforms. We compare the performance of TraNet with\nother approaches and find that our approach can best transfer knowledge on\nusers across platforms in the given tasks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:51:13 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Sun", "Jun", ""], ["Staab", "Steffen", ""], ["Kunegis", "J\u00e9r\u00f4me", ""]]}, {"id": "1910.07927", "submitter": "Sergul Aydore", "authors": "Sergul Aydore, Tianhao Zhu, Dean Foster", "title": "Dynamic Local Regret for Non-convex Online Forecasting", "comments": "NeurIPS2019. arXiv admin note: substantial text overlap with\n  arXiv:1905.08850", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online forecasting problems for non-convex machine learning\nmodels. Forecasting introduces several challenges such as (i) frequent updates\nare necessary to deal with concept drift issues since the dynamics of the\nenvironment change over time, and (ii) the state of the art models are\nnon-convex models. We address these challenges with a novel regret framework.\nStandard regret measures commonly do not consider both dynamic environment and\nnon-convex models. We introduce a local regret for non-convex models in a\ndynamic environment. We present an update rule incurring a cost, according to\nour proposed local regret, which is sublinear in time T. Our update uses\ntime-smoothed gradients. Using a real-world dataset we show that our\ntime-smoothed approach yields several benefits when compared with\nstate-of-the-art competitors: results are more stable against new data;\ntraining is more robust to hyperparameter selection; and our approach is more\ncomputationally efficient than the alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:01:08 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 14:34:31 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Aydore", "Sergul", ""], ["Zhu", "Tianhao", ""], ["Foster", "Dean", ""]]}, {"id": "1910.07939", "submitter": "S Indrapriyadarsini", "authors": "Sota Yasuda, Shahrzad Mahboubi, S. Indrapriyadarsini, Hiroshi Ninomiya\n  and Hideki Asai", "title": "A Stochastic Variance Reduced Nesterov's Accelerated Quasi-Newton Method", "comments": "Accepted in ICMLA 2019", "journal-ref": null, "doi": "10.1109/ICMLA.2019.00301", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently algorithms incorporating second order curvature information have\nbecome popular in training neural networks. The Nesterov's Accelerated\nQuasi-Newton (NAQ) method has shown to effectively accelerate the BFGS\nquasi-Newton method by incorporating the momentum term and Nesterov's\naccelerated gradient vector. A stochastic version of NAQ method was proposed\nfor training of large-scale problems. However, this method incurs high\nstochastic variance noise. This paper proposes a stochastic variance reduced\nNesterov's Accelerated Quasi-Newton method in full (SVR-NAQ) and limited\n(SVRLNAQ) memory forms. The performance of the proposed method is evaluated in\nTensorflow on four benchmark problems - two regression and two classification\nproblems respectively. The results show improved performance compared to\nconventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:31:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Yasuda", "Sota", ""], ["Mahboubi", "Shahrzad", ""], ["Indrapriyadarsini", "S.", ""], ["Ninomiya", "Hiroshi", ""], ["Asai", "Hideki", ""]]}, {"id": "1910.07942", "submitter": "Topi Paananen", "authors": "Topi Paananen, Michael Riis Andersen, Aki Vehtari", "title": "Uncertainty-aware Sensitivity Analysis Using R\\'enyi Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nonlinear supervised learning models, assessing the importance of\npredictor variables or their interactions is not straightforward because it can\nvary in the domain of the variables. Importance can be assessed locally with\nsensitivity analysis using general methods that rely on the model's predictions\nor their derivatives. In this work, we extend derivative based sensitivity\nanalysis to a Bayesian setting by differentiating the R\\'enyi divergence of a\nmodel's predictive distribution. By utilising the predictive distribution\ninstead of a point prediction, the model uncertainty is taken into account in a\nprincipled way. Our empirical results on simulated and real data sets\ndemonstrate accurate and reliable identification of important variables and\ninteraction effects compared to alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:33:49 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 09:48:17 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Paananen", "Topi", ""], ["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""]]}, {"id": "1910.07969", "submitter": "Chih-Kuan Yeh", "authors": "Chih-Kuan Yeh, Been Kim, Sercan O. Arik, Chun-Liang Li, Tomas Pfister,\n  and Pradeep Ravikumar", "title": "On Completeness-aware Concept-Based Explanations in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human explanations of high-level decisions are often expressed in terms of\nkey concepts the decisions are based on. In this paper, we study such\nconcept-based explainability for Deep Neural Networks (DNNs). First, we define\nthe notion of completeness, which quantifies how sufficient a particular set of\nconcepts is in explaining a model's prediction behavior based on the assumption\nthat complete concept scores are sufficient statistics of the model prediction.\nNext, we propose a concept discovery method that aims to infer a complete set\nof concepts that are additionally encouraged to be interpretable, which\naddresses the limitations of existing methods on concept explanations. To\ndefine an importance score for each discovered concept, we adapt game-theoretic\nnotions to aggregate over sets and propose ConceptSHAP. Via proposed metrics\nand user studies, on a synthetic dataset with apriori-known concept\nexplanations, as well as on real-world image and language datasets, we validate\nthe effectiveness of our method in finding concepts that are both complete in\nexplaining the decisions and interpretable. (The code is released at\nhttps://github.com/chihkuanyeh/concept_exp)\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 15:27:37 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 04:15:19 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 18:38:56 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 10:57:12 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 00:57:19 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Kim", "Been", ""], ["Arik", "Sercan O.", ""], ["Li", "Chun-Liang", ""], ["Pfister", "Tomas", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1910.07988", "submitter": "Yue Zhao", "authors": "Yue Zhao, Xuejian Wang, Cheng Cheng, Xueying Ding", "title": "Combining Machine Learning Models using combo Library", "comments": "In Proceedings of Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI 2020)", "journal-ref": null, "doi": "10.1609/aaai.v34i09.7111", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model combination, often regarded as a key sub-field of ensemble learning,\nhas been widely used in both academic research and industry applications. To\nfacilitate this process, we propose and implement an easy-to-use Python\ntoolkit, combo, to aggregate models and scores under various scenarios,\nincluding classification, clustering, and anomaly detection. In a nutshell,\ncombo provides a unified and consistent way to combine both raw and pretrained\nmodels from popular machine learning libraries, e.g., scikit-learn, XGBoost,\nand LightGBM. With accessibility and robustness in mind, combo is designed with\ndetailed documentation, interactive examples, continuous integration, code\ncoverage, and maintainability check; it can be installed easily through Python\nPackage Index (PyPI) or https://github.com/yzhao062/combo.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 15:00:13 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 21:00:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yue", ""], ["Wang", "Xuejian", ""], ["Cheng", "Cheng", ""], ["Ding", "Xueying", ""]]}, {"id": "1910.08007", "submitter": "Thu Nguyen Ms.", "authors": "Thu Nguyen", "title": "Faster feature selection with a Dropping Forward-Backward algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this era of big data, feature selection techniques, which have long been\nproven to simplify the model, makes the model more comprehensible, speed up the\nprocess of learning, have become more and more important. Among many developed\nmethods, forward and stepwise feature selection regression remained widely used\ndue to their simplicity and efficiency. However, they all involving rescanning\nall the un-selected features again and again. Moreover, many times, the\nbackward steps in stepwise deem unnecessary, as we will illustrate in our\nexample. These remarks motivate us to introduce a novel algorithm that may\nboost the speed up to 65.77% compared to the stepwise procedure while\nmaintaining good performance in terms of the number of selected features and\nerror rates. Also, our experiments illustrate that feature selection procedures\nmay be a better choice for high-dimensional problems where the number of\nfeatures highly exceeds the number of samples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:25:47 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 02:47:55 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 17:50:26 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nguyen", "Thu", ""]]}, {"id": "1910.08013", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison", "title": "Why bigger is not always better: on finite and infinite neural networks", "comments": null, "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has argued that neural networks can be understood theoretically\nby taking the number of channels to infinity, at which point the outputs become\nGaussian process (GP) distributed. However, we note that infinite Bayesian\nneural networks lack a key facet of the behaviour of real neural networks: the\nfixed kernel, determined only by network hyperparameters, implies that they\ncannot do any form of representation learning. The lack of representation or\nequivalently kernel learning leads to less flexibility and hence worse\nperformance, giving a potential explanation for the inferior performance of\ninfinite networks observed in the literature (e.g. Novak et al. 2019). We give\nanalytic results characterising the prior over representations and\nrepresentation learning in finite deep linear networks. We show empirically\nthat the representations in SOTA architectures such as ResNets trained with SGD\nare much closer to those suggested by our deep linear results than by the\ncorresponding infinite network. This motivates the introduction of a new class\nof network: infinite networks with bottlenecks, which inherit the theoretical\ntractability of infinite networks while at the same time allowing\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:33:34 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 11:51:36 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 08:53:07 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Aitchison", "Laurence", ""]]}, {"id": "1910.08018", "submitter": "Xinjie Fan", "authors": "Xinjie Fan, Yuguang Yue, Purnamrita Sarkar, Y. X. Rachel Wang", "title": "A Unified Framework for Tuning Hyperparameters in Clustering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting hyperparameters for unsupervised learning problems is challenging\nin general due to the lack of ground truth for validation. Despite the\nprevalence of this issue in statistics and machine learning, especially in\nclustering problems, there are not many methods for tuning these\nhyperparameters with theoretical guarantees. In this paper, we provide a\nframework with provable guarantees for selecting hyperparameters in a number of\ndistinct models. We consider both the subgaussian mixture model and network\nmodels to serve as examples of i.i.d. and non-i.i.d. data. We demonstrate that\nthe same framework can be used to choose the Lagrange multipliers of penalty\nterms in semi-definite programming (SDP) relaxations for community detection,\nand the bandwidth parameter for constructing kernel similarity matrices for\nspectral clustering. By incorporating a cross-validation procedure, we show the\nframework can also do consistent model selection for network models. Using a\nvariety of simulated and real data examples, we show that our framework\noutperforms other widely used tuning procedures in a broad range of parameter\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:40:42 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 02:12:38 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Fan", "Xinjie", ""], ["Yue", "Yuguang", ""], ["Sarkar", "Purnamrita", ""], ["Wang", "Y. X. Rachel", ""]]}, {"id": "1910.08031", "submitter": "Boyan Gao", "authors": "Boyan Gao, Yongxin Yang, Henry Gouk, Timothy M. Hospedales", "title": "Deep clustering with concrete k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of simultaneously learning a k-means clustering and\ndeep feature representation from unlabelled data, which is of interest due to\nthe potential of deep k-means to outperform traditional two-step feature\nextraction and shallow-clustering strategies. We achieve this by developing a\ngradient-estimator for the non-differentiable k-means objective via the\nGumbel-Softmax reparameterisation trick. In contrast to previous attempts at\ndeep clustering, our concrete k-means model can be optimised with respect to\nthe canonical k-means objective and is easily trained end-to-end without\nresorting to alternating optimisation. We demonstrate the efficacy of our\nmethod on standard clustering benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:57:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Gao", "Boyan", ""], ["Yang", "Yongxin", ""], ["Gouk", "Henry", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1910.08032", "submitter": "George Kesidis", "authors": "George Kesidis and David J. Miller and Zhen Xiang", "title": "Notes on Margin Training and Margin p-Values for Deep Neural Network\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new local class-purity theorem for Lipschitz continuous DNN\nclassifiers. In addition, we discuss how to achieve classification margin for\ntraining samples. Finally, we describe how to compute margin p-values for test\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:26:32 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 16:31:08 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kesidis", "George", ""], ["Miller", "David J.", ""], ["Xiang", "Zhen", ""]]}, {"id": "1910.08036", "submitter": "Philippe Schwaller", "authors": "Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair,\n  Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and\n  Teodoro Laino", "title": "Predicting retrosynthetic pathways using a combined linguistic model and\n  hyper-graph exploration strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of our Molecular Transformer architecture combined\nwith a hyper-graph exploration strategy for automatic retrosynthesis route\nplanning without human intervention. The single-step retrosynthetic model sets\na new state of the art for predicting reactants as well as reagents, solvents\nand catalysts for each retrosynthetic step. We introduce new metrics (coverage,\nclass diversity, round-trip accuracy and Jensen-Shannon divergence) to evaluate\nthe single-step retrosynthetic models, using the forward prediction and a\nreaction classification model always based on the transformer architecture. The\nhypergraph is constructed on the fly, and the nodes are filtered and further\nexpanded based on a Bayesian-like probability. We critically assessed the\nend-to-end framework with several retrosynthesis examples from literature and\nacademic exams. Overall, the frameworks has a very good performance with few\nweaknesses due to the bias induced during the training process. The use of the\nnewly introduced metrics opens up the possibility to optimize entire\nretrosynthetic frameworks through focusing on the performance of the\nsingle-step model only.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:02:41 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Schwaller", "Philippe", ""], ["Petraglia", "Riccardo", ""], ["Zullo", "Valerio", ""], ["Nair", "Vishnu H", ""], ["Haeuselmann", "Rico Andreas", ""], ["Pisoni", "Riccardo", ""], ["Bekas", "Costas", ""], ["Iuliano", "Anna", ""], ["Laino", "Teodoro", ""]]}, {"id": "1910.08037", "submitter": "Han Bao", "authors": "Han Bao, Jinyong Feng, Nam Dinh, Hongbin Zhang", "title": "Computationally Efficient CFD Prediction of Bubbly Flow using\n  Physics-Guided Deep Learning", "comments": "This paper is under review of International Journal of Multi-phase\n  Flow", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.data-an physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To realize efficient computational fluid dynamics (CFD) prediction of\ntwo-phase flow, a multi-scale framework was proposed in this paper by applying\na physics-guided data-driven approach. Instrumental to this framework, Feature\nSimilarity Measurement (FSM) technique was developed for error estimation in\ntwo-phase flow simulation using coarse-mesh CFD, to achieve a comparable\naccuracy as fine-mesh simulations with fast-running feature. By defining\nphysics-guided parameters and variable gradients as physical features, FSM has\nthe capability to capture the underlying local patterns in the coarse-mesh CFD\nsimulation. Massive low-fidelity data and respective high-fidelity data are\nused to explore the underlying information relevant to the main simulation\nerrors and the effects of phenomenological scaling. By learning from previous\nsimulation data, a surrogate model using deep feedforward neural network (DFNN)\ncan be developed and trained to estimate the simulation error of coarse-mesh\nCFD. The research documented supports the feasibility of the physics-guided\ndeep learning methods for coarse mesh CFD simulations which has a potential for\nthe efficient industrial design.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:03:38 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Bao", "Han", ""], ["Feng", "Jinyong", ""], ["Dinh", "Nam", ""], ["Zhang", "Hongbin", ""]]}, {"id": "1910.08042", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour", "title": "Comment: Reflections on the Deconfounder", "comments": "Comment to appear in JASA discussion of \"The Blessings of Multiple\n  Causes.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this comment (set to appear in a formal discussion in JASA) is to\ndraw out some conclusions from an extended back-and-forth I have had with Wang\nand Blei regarding the deconfounder method proposed in \"The Blessings of\nMultiple Causes\" [arXiv:1805.06826]. I will make three points here. First, in\nmy role as the critic in this conversation, I will summarize some arguments\nabout the lack of causal identification in the bulk of settings where the\n\"informal\" message of the paper suggests that the deconfounder could be used.\nThis is a point that is discussed at length in D'Amour 2019 [arXiv:1902.10286],\nwhich motivated the results concerning causal identification in Theorems 6--8\nof \"Blessings\". Second, I will argue that adding parametric assumptions to the\nworking model in order to obtain identification of causal parameters (a\nstrategy followed in Theorem 6 and in the experimental examples) is a risky\nstrategy, and should only be done when extremely strong prior information is\navailable. Finally, I will consider the implications of the nonparametric\nidentification results provided for a narrow, but non-trivial, set of causal\nestimands in Theorems 7 and 8. I will highlight that these results may be even\nmore interesting from the perspective of detecting causal identification from\nobserved data, under relatively weak assumptions about confounders.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:10:30 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["D'Amour", "Alexander", ""]]}, {"id": "1910.08051", "submitter": "Yogesh Balaji", "authors": "Yogesh Balaji, Tom Goldstein, Judy Hoffman", "title": "Instance adaptive adversarial training: Improved accuracy tradeoffs in\n  neural nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is by far the most successful strategy for improving\nrobustness of neural networks to adversarial attacks. Despite its success as a\ndefense mechanism, adversarial training fails to generalize well to unperturbed\ntest set. We hypothesize that this poor generalization is a consequence of\nadversarial training with uniform perturbation radius around every training\nsample. Samples close to decision boundary can be morphed into a different\nclass under a small perturbation budget, and enforcing large margins around\nthese samples produce poor decision boundaries that generalize poorly.\nMotivated by this hypothesis, we propose instance adaptive adversarial training\n-- a technique that enforces sample-specific perturbation margins around every\ntraining sample. We show that using our approach, test accuracy on unperturbed\nsamples improve with a marginal drop in robustness. Extensive experiments on\nCIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:24:22 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Balaji", "Yogesh", ""], ["Goldstein", "Tom", ""], ["Hoffman", "Judy", ""]]}, {"id": "1910.08057", "submitter": "Tony Duan", "authors": "Tony Duan and Juho Lee", "title": "Graph Embedding VAE: A Permutation Invariant Model of Graph Structure", "comments": "Presented at the NeurIPS 2019 Workshop on Graph Representation\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative models of graph structure have applications in biology and social\nsciences. The state of the art is GraphRNN, which decomposes the graph\ngeneration process into a series of sequential steps. While effective for\nmodest sizes, it loses its permutation invariance for larger graphs. Instead,\nwe present a permutation invariant latent-variable generative model relying on\ngraph embeddings to encode structure. Using tools from the random graph\nliterature, our model is highly scalable to large graphs with likelihood\nevaluation and generation in $O(|V | + |E|)$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:38:43 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Duan", "Tony", ""], ["Lee", "Juho", ""]]}, {"id": "1910.08074", "submitter": "Shen Wang", "authors": "Shen Wang, Zhengzhang Chen, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang,\n  Jiaping Gui, Zhichun Li, Haifeng Chen, Philip S. Yu", "title": "Heterogeneous Graph Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information systems have widely been the target of malware attacks.\nTraditional signature-based malicious program detection algorithms can only\ndetect known malware and are prone to evasion techniques such as binary\nobfuscation, while behavior-based approaches highly rely on the malware\ntraining samples and incur prohibitively high training cost. To address the\nlimitations of existing techniques, we propose MatchGNet, a heterogeneous Graph\nMatching Network model to learn the graph representation and similarity metric\nsimultaneously based on the invariant graph modeling of the program's execution\nbehaviors. We conduct a systematic evaluation of our model and show that it is\naccurate in detecting malicious program behavior and can help detect malware\nattacks with less false positives. MatchGNet outperforms the state-of-the-art\nalgorithms in malware detection by generating 50% less false positives while\nkeeping zero false negatives.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:40:29 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Wang", "Shen", ""], ["Chen", "Zhengzhang", ""], ["Yu", "Xiao", ""], ["Li", "Ding", ""], ["Ni", "Jingchao", ""], ["Tang", "Lu-An", ""], ["Gui", "Jiaping", ""], ["Li", "Zhichun", ""], ["Chen", "Haifeng", ""], ["Yu", "Philip S.", ""]]}, {"id": "1910.08091", "submitter": "Yura Perov N", "authors": "Yura Perov, Logan Graham, Kostis Gourgoulias, Jonathan G. Richens,\n  Ciar\\'an M. Lee, Adam Baker, Saurabh Johri", "title": "MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic\n  Programming", "comments": "Logan and Yura have made equal contributions to the paper. Accepted\n  to the 2nd Symposium on Advances in Approximate Bayesian Inference\n  (Vancouver, Canada, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We elaborate on using importance sampling for causal reasoning, in particular\nfor counterfactual inference. We show how this can be implemented natively in\nprobabilistic programming. By considering the structure of the counterfactual\nquery, one can significantly optimise the inference process. We also consider\ndesign choices to enable further optimisations. We introduce MultiVerse, a\nprobabilistic programming prototype engine for approximate causal reasoning. We\nprovide experimental results and compare with Pyro, an existing probabilistic\nprogramming framework with some of causal reasoning tools.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:00:24 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:05:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Perov", "Yura", ""], ["Graham", "Logan", ""], ["Gourgoulias", "Kostis", ""], ["Richens", "Jonathan G.", ""], ["Lee", "Ciar\u00e1n M.", ""], ["Baker", "Adam", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.08103", "submitter": "Alex Georges", "authors": "Jacek Cyranka, Alexander Georges, David Meyer", "title": "Mapper Based Classifier", "comments": "12 pages, accepted to IEEE ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis aims to extract topological quantities from data,\nwhich tend to focus on the broader global structure of the data rather than\nlocal information. The Mapper method, specifically, generalizes clustering\nmethods to identify significant global mathematical structures, which are out\nof reach of many other approaches. We propose a classifier based on applying\nthe Mapper algorithm to data projected onto a latent space. We obtain the\nlatent space by using PCA or autoencoders. Notably, a classifier based on the\nMapper method is immune to any gradient based attack, and improves robustness\nover traditional CNNs (convolutional neural networks). We report theoretical\njustification and some numerical experiments that confirm our claims.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:28:01 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 15:32:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cyranka", "Jacek", ""], ["Georges", "Alexander", ""], ["Meyer", "David", ""]]}, {"id": "1910.08105", "submitter": "Matteo Fontana", "authors": "Ilia Nouretdinov, James Gammerman, Matteo Fontana, Daljit Rehal", "title": "Multi-level conformal clustering: A distribution-free technique for\n  clustering and anomaly detection", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.07.114", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a clustering technique called \\textit{multi-level\nconformal clustering (MLCC)}. The technique is hierarchical in nature because\nit can be performed at multiple significance levels which yields greater\ninsight into the data than performing it at just one level. We describe the\ntheoretical underpinnings of MLCC, compare and contrast it with the\nhierarchical clustering algorithm, and then apply it to real world datasets to\nassess its performance. There are several advantages to using MLCC over more\nclassical clustering techniques: Once a significance level has been set, MLCC\nis able to automatically select the number of clusters. Furthermore, thanks to\nthe conformal prediction framework the resulting clustering model has a clear\nstatistical meaning without any assumptions about the distribution of the data.\nThis statistical robustness also allows us to perform clustering and anomaly\ndetection simultaneously. Moreover, due to the flexibility of the conformal\nprediction framework, our algorithm can be used on top of many other machine\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:28:56 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:03:28 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nouretdinov", "Ilia", ""], ["Gammerman", "James", ""], ["Fontana", "Matteo", ""], ["Rehal", "Daljit", ""]]}, {"id": "1910.08108", "submitter": "Anindya Sarkar", "authors": "Anindya Sarkar, Nikhil Kumar Gupta and Raghu Iyengar", "title": "Enforcing Linearity in DNN succours Robustness and Adversarial Image\n  Generation", "comments": "Adversarial Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on the adversarial vulnerability of neural networks have shown\nthat models trained with the objective of minimizing an upper bound on the\nworst-case loss over all possible adversarial perturbations improve robustness\nagainst adversarial attacks. Beside exploiting adversarial training framework,\nwe show that by enforcing a Deep Neural Network (DNN) to be linear in\ntransformed input and feature space improves robustness significantly. We also\ndemonstrate that by augmenting the objective function with Local Lipschitz\nregularizer boost robustness of the model further. Our method outperforms most\nsophisticated adversarial training methods and achieves state of the art\nadversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also\npropose a novel adversarial image generation method by leveraging Inverse\nRepresentation Learning and Linearity aspect of an adversarially trained deep\nneural network classifier.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:38:40 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 17:00:50 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sarkar", "Anindya", ""], ["Gupta", "Nikhil Kumar", ""], ["Iyengar", "Raghu", ""]]}, {"id": "1910.08109", "submitter": "Hsiang Hsu", "authors": "Hsiang Hsu and Shahab Asoodeh and Flavio du Pin Calmon", "title": "Obfuscation via Information Density Estimation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying features that leak information about sensitive attributes is a\nkey challenge in the design of information obfuscation mechanisms. In this\npaper, we propose a framework to identify information-leaking features via\ninformation density estimation. Here, features whose information densities\nexceed a pre-defined threshold are deemed information-leaking features. Once\nthese features are identified, we sequentially pass them through a targeted\nobfuscation mechanism with a provable leakage guarantee in terms of\n$\\mathsf{E}_\\gamma$-divergence. The core of this mechanism relies on a\ndata-driven estimate of the trimmed information density for which we propose a\nnovel estimator, named the trimmed information density estimator (TIDE). We\nthen use TIDE to implement our mechanism on three real-world datasets. Our\napproach can be used as a data-driven pipeline for designing obfuscation\nmechanisms targeting specific features.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:42:57 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hsu", "Hsiang", ""], ["Asoodeh", "Shahab", ""], ["Calmon", "Flavio du Pin", ""]]}, {"id": "1910.08112", "submitter": "Kevin Nguyen", "authors": "Kevin P. Nguyen, Cherise Chin Fatt, Alex Treacher, Cooper Mellema,\n  Madhukar H. Trivedi, Albert Montillo", "title": "Anatomically-Informed Data Augmentation for functional MRI with\n  Applications to Deep Learning", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning to build accurate predictive models from\nfunctional neuroimaging data is often hindered by limited dataset sizes. Though\ndata augmentation can help mitigate such training obstacles, most data\naugmentation methods have been developed for natural images as in computer\nvision tasks such as CIFAR, not for medical images. This work helps to fills in\nthis gap by proposing a method for generating new functional Magnetic Resonance\nImages (fMRI) with realistic brain morphology. This method is tested on a\nchallenging task of predicting antidepressant treatment response from\npre-treatment task-based fMRI and demonstrates a 26% improvement in performance\nin predicting response using augmented images. This improvement compares\nfavorably to state-of-the-art augmentation methods for natural images. Through\nan ablative test, augmentation is also shown to substantively improve\nperformance when applied before hyperparameter optimization. These results\nsuggest the optimal order of operations and support the role of data\naugmentation method for improving predictive performance in tasks using fMRI.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:55:10 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Nguyen", "Kevin P.", ""], ["Fatt", "Cherise Chin", ""], ["Treacher", "Alex", ""], ["Mellema", "Cooper", ""], ["Trivedi", "Madhukar H.", ""], ["Montillo", "Albert", ""]]}, {"id": "1910.08143", "submitter": "Huazhe Xu", "authors": "Huazhe Xu, Boyuan Chen, Yang Gao, Trevor Darrell", "title": "Zero-shot Policy Learning with Spatial Temporal RewardDecomposition on\n  Contingency-aware Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-standing challenge to enable an intelligent agent to learn in\none environment and generalize to an unseen environment without further data\ncollection and finetuning. In this paper, we consider a zero shot\ngeneralization problem setup that complies with biological intelligent agents'\nlearning and generalization processes. The agent is first presented with\nprevious experiences in the training environment, along with task description\nin the form of trajectory-level sparse rewards. Later when it is placed in the\nnew testing environment, it is asked to perform the task without any\ninteraction with the testing environment. We find this setting natural for\nbiological creatures and at the same time, challenging for previous methods.\nBehavior cloning, state-of-art RL along with other zero-shot learning methods\nperform poorly on this benchmark. Given a set of experiences in the training\nenvironment, our method learns a neural function that decomposes the sparse\nreward into particular regions in a contingency-aware observation as a per step\nreward. Based on such decomposed rewards, we further learn a dynamics model and\nuse Model Predictive Control (MPC) to obtain a policy. Since the rewards are\ndecomposed to finer-granularity observations, they are naturally generalizable\nto new environments that are composed of similar basic elements. We demonstrate\nour method on a wide range of environments, including a classic video game --\nSuper Mario Bros, as well as a robotic continuous control task. Please refer to\nthe project page for more visualized results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 20:15:36 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 05:06:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xu", "Huazhe", ""], ["Chen", "Boyuan", ""], ["Gao", "Yang", ""], ["Darrell", "Trevor", ""]]}, {"id": "1910.08145", "submitter": "Pouria Golshanrad", "authors": "Pouria Golshanrad, Hamid Mahini, Behnam Bahrak", "title": "Predicting passenger origin-destination in online taxi-hailing systems", "comments": "25 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of transportation planning, traffic management, and dispatch\noptimization importance, passenger origin-destination prediction has become one\nof the most important requirements for intelligent transportation systems\nmanagement. In this paper, we propose a model to predict the next specified\ntime window travels' origin and destination. To extract meaningful travel\nflows, we use K-means clustering in four-dimensional space with maximum cluster\nsize limitation for origin and destination zones. Because of the large number\nof clusters, we use non-negative matrix factorization to decrease the number of\ntravel clusters. Also, we use a stacked recurrent neural network model to\npredict travel count in each cluster. Comparing our results with other existing\nmodels shows that our proposed model has 5-7% lower mean absolute percentage\nerror (MAPE) for 1-hour time windows, and 14% lower MAPE for 30-minute time\nwindows.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 20:19:23 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 12:36:24 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 10:43:15 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Golshanrad", "Pouria", ""], ["Mahini", "Hamid", ""], ["Bahrak", "Behnam", ""]]}, {"id": "1910.08149", "submitter": "Sagar Verma", "authors": "Sagar Verma and Shikha Singh and Angshul Majumdar", "title": "Multi Label Restricted Boltzmann Machine for Non-Intrusive Load\n  Monitoring", "comments": "5 pages, ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing population indicates that energy demands need to be managed in the\nresidential sector. Prior studies have reflected that the customers tend to\nreduce a significant amount of energy consumption if they are provided with\nappliance-level feedback. This observation has increased the relevance of load\nmonitoring in today's tech-savvy world. Most of the previously proposed\nsolutions claim to perform load monitoring without intrusion, but they are not\ncompletely non-intrusive. These methods require historical appliance-level data\nfor training the model for each of the devices. This data is gathered by\nputting a sensor on each of the appliances present in the home which causes\nintrusion in the building. Some recent studies have proposed that if we frame\nNon-Intrusive Load Monitoring (NILM) as a multi-label classification problem,\nthe need for appliance-level data can be avoided. In this paper, we propose\nMulti-label Restricted Boltzmann Machine(ML-RBM) for NILM and report an\nexperimental evaluation of proposed and state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 20:31:15 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Verma", "Sagar", ""], ["Singh", "Shikha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1910.08151", "submitter": "Sean Sinclair", "authors": "Sean R. Sinclair, Siddhartha Banerjee, Christina Lee Yu", "title": "Adaptive Discretization for Episodic Reinforcement Learning in Metric\n  Spaces", "comments": "46 pages, 15 figures", "journal-ref": null, "doi": "10.1145/3366703", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for model-free episodic reinforcement\nlearning on large (potentially continuous) state-action spaces. Our algorithm\nis based on a novel $Q$-learning policy with adaptive data-driven\ndiscretization. The central idea is to maintain a finer partition of the\nstate-action space in regions which are frequently visited in historical\ntrajectories, and have higher payoff estimates. We demonstrate how our adaptive\npartitions take advantage of the shape of the optimal $Q$-function and the\njoint space, without sacrificing the worst-case performance. In particular, we\nrecover the regret guarantees of prior algorithms for continuous state-action\nspaces, which additionally require either an optimal discretization as input,\nand/or access to a simulation oracle. Moreover, experiments demonstrate how our\nalgorithm automatically adapts to the underlying structure of the problem,\nresulting in much better performance compared both to heuristics and\n$Q$-learning with uniform discretization.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 20:40:37 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 12:59:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Sinclair", "Sean R.", ""], ["Banerjee", "Siddhartha", ""], ["Yu", "Christina Lee", ""]]}, {"id": "1910.08168", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Deep Sub-Ensembles for Fast Uncertainty Estimation in Image\n  Classification", "comments": "7 pages, 8 figures, Bayesian Deep Learning Workshop 2019 @ NeurIPS\n  2019, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast estimates of model uncertainty are required for many robust robotics\napplications. Deep Ensembles provides state of the art uncertainty without\nrequiring Bayesian methods, but still it is computationally expensive. In this\npaper we propose deep sub-ensembles, an approximation to deep ensembles where\nthe core idea is to ensemble only the layers close to the output, and not the\nwhole model. With ResNet-20 on the CIFAR10 dataset, we obtain 1.5-2.5 speedup\nover a Deep Ensemble, with a small increase in error and NLL, and similarly up\nto 5-15 speedup with a VGG-like network on the SVHN dataset. Our results show\nthat this idea enables a trade-off between error and uncertainty quality versus\ncomputational performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:07:40 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 12:23:45 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1910.08211", "submitter": "Tomasz Arodz", "authors": "Xi Gao, Han Zhang, Aliakbar Panahi, Tom Arodz", "title": "Differentiable Combinatorial Losses through Generalized Gradients of\n  Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When samples have internal structure, we often see a mismatch between the\nobjective optimized during training and the model's goal during inference. For\nexample, in sequence-to-sequence modeling we are interested in high-quality\ntranslated sentences, but training typically uses maximum likelihood at the\nword level. The natural training-time loss would involve a combinatorial\nproblem -- dynamic programming-based global sequence alignment -- but solutions\nto combinatorial problems are not differentiable with respect to their input\nparameters, so surrogate, differentiable losses are used instead. Here, we show\nhow to perform gradient descent over combinatorial optimization algorithms that\ninvolve continuous parameters, for example edge weights, and can be efficiently\nexpressed as linear programs. We demonstrate usefulness of gradient descent\nover combinatorial optimization in sequence-to-sequence modeling using\ndifferentiable encoder-decoder architecture with softmax or Gumbel-softmax, and\nin image classification in a weakly supervised setting where instead of the\ncorrect class for each photo, only groups of photos labeled with correct but\nunordered set of classes are available during training.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 00:53:55 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 14:39:27 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:31:22 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 16:44:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gao", "Xi", ""], ["Zhang", "Han", ""], ["Panahi", "Aliakbar", ""], ["Arodz", "Tom", ""]]}, {"id": "1910.08215", "submitter": "Alireza Rezvanifar", "authors": "Alireza Rezvanifar, Tunai Porto Marques, Melissa Cote, Alexandra\n  Branzan Albu, Alex Slonimer, Thomas Tolhurst, Kaan Ersahin, Todd Mudge,\n  Stephane Gauthier", "title": "A Deep Learning-based Framework for the Detection of Schools of Herring\n  in Echograms", "comments": "Accepted to NeurIPS 2019 workshop on Tackling Climate Change with\n  Machine Learning, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tracking the abundance of underwater species is crucial for understanding the\neffects of climate change on marine ecosystems. Biologists typically monitor\nunderwater sites with echosounders and visualize data as 2D images (echograms);\nthey interpret these data manually or semi-automatically, which is\ntime-consuming and prone to inconsistencies. This paper proposes a deep\nlearning framework for the automatic detection of schools of herring from\nechograms. Experiments demonstrated that our approach outperforms a traditional\nmachine learning algorithm using hand-crafted features. Our framework could\neasily be expanded to detect more species of interest to sustainable fisheries.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:12:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Rezvanifar", "Alireza", ""], ["Marques", "Tunai Porto", ""], ["Cote", "Melissa", ""], ["Albu", "Alexandra Branzan", ""], ["Slonimer", "Alex", ""], ["Tolhurst", "Thomas", ""], ["Ersahin", "Kaan", ""], ["Mudge", "Todd", ""], ["Gauthier", "Stephane", ""]]}, {"id": "1910.08216", "submitter": "Emma Frejinger", "authors": "Emma Frejinger, Eric Larsen", "title": "A language processing algorithm for predicting tactical solutions to an\n  operational planning problem under uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the prediction of solutions to a stochastic discrete\noptimization problem. Through an application, we illustrate how we can use a\nstate-of-the-art neural machine translation (NMT) algorithm to predict the\nsolutions by defining appropriate vocabularies, syntaxes and constraints. We\nattend to applications where the predictions need to be computed in very short\ncomputing time -- in the order of milliseconds or less. The results show that\nwith minimal adaptations to the model architecture and hyperparameter tuning,\nthe NMT algorithm can produce accurate solutions within the computing time\nbudget. While these predictions are slightly less accurate than approximate\nstochastic programming solutions (sample average approximation), they can be\ncomputed faster and with less variability.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:16:30 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Frejinger", "Emma", ""], ["Larsen", "Eric", ""]]}, {"id": "1910.08219", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Lei Zheng, Jiawei Zhang, Jiayu Han, Philip S. Yu", "title": "JSCN: Joint Spectral Convolutional Network for Cross Domain\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation can alleviate the data sparsity problem in\nrecommender systems. To transfer the knowledge from one domain to another, one\ncan either utilize the neighborhood information or learn a direct mapping\nfunction. However, all existing methods ignore the high-order connectivity\ninformation in cross-domain recommendation area and suffer from the\ndomain-incompatibility problem. In this paper, we propose a \\textbf{J}oint\n\\textbf{S}pectral \\textbf{C}onvolutional \\textbf{N}etwork (JSCN) for\ncross-domain recommendation. JSCN will simultaneously operate multi-layer\nspectral convolutions on different graphs, and jointly learn a domain-invariant\nuser representation with a domain adaptive user mapping module. As a result,\nthe high-order comprehensive connectivity information can be extracted by the\nspectral convolutions and the information can be transferred across domains\nwith the domain-invariant user mapping. The domain adaptive user mapping module\ncan help the incompatible domains to transfer the knowledge across each other.\nExtensive experiments on $24$ Amazon rating datasets show the effectiveness of\nJSCN in the cross-domain recommendation, with $9.2\\%$ improvement on recall and\n$36.4\\%$ improvement on MAP compared with state-of-the-art methods. Our code is\navailable online ~\\footnote{https://github.com/JimLiu96/JSCN}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:32:23 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Liu", "Zhiwei", ""], ["Zheng", "Lei", ""], ["Zhang", "Jiawei", ""], ["Han", "Jiayu", ""], ["Yu", "Philip S.", ""]]}, {"id": "1910.08222", "submitter": "Scott Sievert", "authors": "Scott Sievert", "title": "Improving the convergence of SGD through adaptive batch sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch stochastic gradient descent (SGD) and variants thereof approximate\nthe objective function's gradient with a small number of training examples, aka\nthe batch size. Small batch sizes require little computation for each model\nupdate but can yield high-variance gradient estimates, which poses some\nchallenges for optimization. Conversely, large batches require more computation\nbut can yield higher precision gradient estimates. This work presents a method\nto adapt the batch size to the model's training loss. For various function\nclasses, we show that our method requires the same order of model updates as\ngradient descent while requiring the same order of gradient computations as\nSGD. This method requires evaluating the model's loss on the entire dataset\nevery model update. However, the required computation is greatly reduced with a\npassive approximation of the adaptive method. We provide extensive experiments\nillustrating that our methods require fewer model updates without increasing\nthe total amount of computation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:45:03 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:09:16 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 22:13:52 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sievert", "Scott", ""]]}, {"id": "1910.08225", "submitter": "NIcholas O'Dell", "authors": "Nicholas O'Dell and Christopher Renton and Adrian Wills", "title": "Learning Continuous Occupancy Maps with the Ising Process Model", "comments": "Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of learning a continuous occupancy field for use in\nrobot navigation. Occupancy grid maps, or variants of, are possibly the most\nwidely used and accepted method of building a map of a robot's environment.\nVarious methods have been developed to learn continuous occupancy maps and have\nsuccessfully resolved many of the shortcomings of grid mapping, namely, priori\ndiscretisation and spatial correlation. However, most methods for producing a\ncontinuous occupancy field remain computationally expensive or heuristic in\nnature. Our method explores a generalisation of the so-called Ising model as a\nsuitable candidate for modelling an occupancy field. We also present a unique\nkernel for use within our method that models range measurements. The method is\nquite attractive as it requires only a small number of hyperparameters to be\ntrained, and is computationally efficient. The small number of hyperparameters\ncan be quickly learned by maximising a pseudo likelihood. The technique is\ndemonstrated on both a small simulated indoor environment with known ground\ntruth as well as large indoor and outdoor areas, using two common real data\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:58:02 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["O'Dell", "Nicholas", ""], ["Renton", "Christopher", ""], ["Wills", "Adrian", ""]]}, {"id": "1910.08234", "submitter": "Xin Yao", "authors": "Xin Yao, Tianchi Huang, Rui-Xiao Zhang, Ruiyu Li, Lifeng Sun", "title": "Federated Learning with Unbiased Gradient Aggregation and Controllable\n  Meta Updating", "comments": "This manuscript has been accepted to the Workshop on Federated\n  Learning for Data Privacy and Confidentiality (FL - NeurIPS 2019, in\n  Conjunction with NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) aims to train machine learning models in the\ndecentralized system consisting of an enormous amount of smart edge devices.\nFederated averaging (FedAvg), the fundamental algorithm in FL settings,\nproposes on-device training and model aggregation to avoid the potential heavy\ncommunication costs and privacy concerns brought by transmitting raw data.\nHowever, through theoretical analysis we argue that 1) the multiple steps of\nlocal updating will result in gradient biases and 2) there is an inconsistency\nbetween the expected target distribution and the optimization objectives\nfollowing the training paradigm in FedAvg. To tackle these problems, we first\npropose an unbiased gradient aggregation algorithm with the keep-trace gradient\ndescent and the gradient evaluation strategy. Then we introduce an additional\ncontrollable meta updating procedure with a small set of data samples,\nindicating the expected target distribution, to provide a clear and consistent\noptimization objective. Both the two improvements are model- and task-agnostic\nand can be applied individually or together. Experimental results demonstrate\nthat the proposed methods are faster in convergence and achieve higher accuracy\nwith different network architectures in various FL settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:17:22 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 09:17:12 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 06:26:36 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Yao", "Xin", ""], ["Huang", "Tianchi", ""], ["Zhang", "Rui-Xiao", ""], ["Li", "Ruiyu", ""], ["Sun", "Lifeng", ""]]}, {"id": "1910.08237", "submitter": "Kartik Gupta", "authors": "Thalaiyasingam Ajanthan, Kartik Gupta, Philip H. S. Torr, Richard\n  Hartley, Puneet K. Dokania", "title": "Mirror Descent View for Neural Network Quantization", "comments": "This paper was accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantizing large Neural Networks (NN) while maintaining the performance is\nhighly desirable for resource-limited devices due to reduced memory and time\ncomplexity. It is usually formulated as a constrained optimization problem and\noptimized via a modified version of gradient descent. In this work, by\ninterpreting the continuous parameters (unconstrained) as the dual of the\nquantized ones, we introduce a Mirror Descent (MD) framework for NN\nquantization. Specifically, we provide conditions on the projections (i.e.,\nmapping from continuous to quantized ones) which would enable us to derive\nvalid mirror maps and in turn the respective MD updates. Furthermore, we\npresent a numerically stable implementation of MD that requires storing an\nadditional set of auxiliary variables (unconstrained), and show that it is\nstrikingly analogous to the Straight Through Estimator (STE) based method which\nis typically viewed as a \"trick\" to avoid vanishing gradients issue. Our\nexperiments on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets\nwith VGG-16, ResNet-18, and MobileNetV2 architectures show that our MD variants\nobtain quantized networks with state-of-the-art performance. Code is available\nat https://github.com/kartikgupta-at-anu/md-bnn.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:19:21 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 07:20:30 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 05:13:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Gupta", "Kartik", ""], ["Torr", "Philip H. S.", ""], ["Hartley", "Richard", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1910.08264", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, Antonio Torralba", "title": "Learning Compositional Koopman Operators for Model-Based Control", "comments": "The first two authors contributed equally to this paper. Project\n  Page: http://koopman.csail.mit.edu/ Video: https://youtu.be/MnXo_hjh1Q4 Code:\n  https://github.com/YunzhuLi/CompositionalKoopmanOperators", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an embedding space for a linear approximation of a nonlinear\ndynamical system enables efficient system identification and control synthesis.\nThe Koopman operator theory lays the foundation for identifying the\nnonlinear-to-linear coordinate transformations with data-driven methods.\nRecently, researchers have proposed to use deep neural networks as a more\nexpressive class of basis functions for calculating the Koopman operators.\nThese approaches, however, assume a fixed dimensional state space; they are\ntherefore not applicable to scenarios with a variable number of objects. In\nthis paper, we propose to learn compositional Koopman operators, using graph\nneural networks to encode the state into object-centric embeddings and using a\nblock-wise linear transition matrix to regularize the shared structure across\nobjects. The learned dynamics can quickly adapt to new environments of unknown\nphysical parameters and produce control signals to achieve a specified goal.\nOur experiments on manipulating ropes and controlling soft robots show that the\nproposed method has better efficiency and generalization ability than existing\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 05:11:16 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 17:09:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Li", "Yunzhu", ""], ["He", "Hao", ""], ["Wu", "Jiajun", ""], ["Katabi", "Dina", ""], ["Torralba", "Antonio", ""]]}, {"id": "1910.08278", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda, Yasuo Tabei", "title": "$b$-Bit Sketch Trie: Scalable Similarity Search on Integer Sketches", "comments": "To be appeared in the Proceedings of IEEE BigData'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, randomly mapping vectorial data to strings of discrete symbols\n(i.e., sketches) for fast and space-efficient similarity searches has become\npopular. Such random mapping is called similarity-preserving hashing and\napproximates a similarity metric by using the Hamming distance. Although many\nefficient similarity searches have been proposed, most of them are designed for\nbinary sketches. Similarity searches on integer sketches are in their infancy.\nIn this paper, we present a novel space-efficient trie named $b$-bit sketch\ntrie on integer sketches for scalable similarity searches by leveraging the\nidea behind succinct data structures (i.e., space-efficient data structures\nwhile supporting various data operations in the compressed format) and a\nfavorable property of integer sketches as fixed-length strings. Our\nexperimental results obtained using real-world datasets show that a trie-based\nindex is built from integer sketches and efficiently performs similarity\nsearches on the index by pruning useless portions of the search space, which\ngreatly improves the search time and space-efficiency of the similarity search.\nThe experimental results show that our similarity search is at most one order\nof magnitude faster than state-of-the-art similarity searches. Besides, our\nmethod needs only 10 GiB of memory on a billion-scale database, while\nstate-of-the-art similarity searches need 29 GiB of memory.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:23:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kanda", "Shunsuke", ""], ["Tabei", "Yasuo", ""]]}, {"id": "1910.08280", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki, Tomoya Sakai, Takafumi Kanamori", "title": "Robust modal regression with direct log-density derivative estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal regression is aimed at estimating the global mode (i.e., global\nmaximum) of the conditional density function of the output variable given input\nvariables, and has led to regression methods robust against heavy-tailed or\nskewed noises. The conditional mode is often estimated through maximization of\nthe modal regression risk (MRR). In order to apply a gradient method for the\nmaximization, the fundamental challenge is accurate approximation of the\ngradient of MRR, not MRR itself. To overcome this challenge, in this paper, we\ntake a novel approach of directly approximating the gradient of MRR. To\napproximate the gradient, we develop kernelized and neural-network-based\nversions of the least-squares log-density derivative estimator, which directly\napproximates the derivative of the log-density without density estimation. With\ndirect approximation of the MRR gradient, we first propose a modal regression\nmethod with kernels, and derive a new parameter update rule based on a\nfixed-point method. Then, the derived update rule is theoretically proved to\nhave a monotonic hill-climbing property towards the conditional mode.\nFurthermore, we indicate that our approach of directly approximating the\ngradient is compatible with recent sophisticated stochastic gradient methods\n(e.g., Adam), and then propose another modal regression method based on neural\nnetworks. Finally, the superior performance of the proposed methods is\ndemonstrated on various artificial and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:46:01 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Sakai", "Tomoya", ""], ["Kanamori", "Takafumi", ""]]}, {"id": "1910.08281", "submitter": "Nazanin Mehrasa", "authors": "Nazanin Mehrasa, Ruizhi Deng, Mohamed Osama Ahmed, Bo Chang, Jiawei\n  He, Thibaut Durand, Marcus Brubaker, Greg Mori", "title": "Point Process Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequences can be modeled by temporal point processes (TPPs) to capture\ntheir asynchronous and probabilistic nature. We propose an intensity-free\nframework that directly models the point process distribution by utilizing\nnormalizing flows. This approach is capable of capturing highly complex\ntemporal distributions and does not rely on restrictive parametric forms.\nComparisons with state-of-the-art baseline models on both synthetic and\nchallenging real-life datasets show that the proposed framework is effective at\nmodeling the stochasticity of discrete event sequences.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:48:26 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 02:34:27 GMT"}, {"version": "v3", "created": "Sun, 22 Dec 2019 21:31:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mehrasa", "Nazanin", ""], ["Deng", "Ruizhi", ""], ["Ahmed", "Mohamed Osama", ""], ["Chang", "Bo", ""], ["He", "Jiawei", ""], ["Durand", "Thibaut", ""], ["Brubaker", "Marcus", ""], ["Mori", "Greg", ""]]}, {"id": "1910.08285", "submitter": "Minne Li", "authors": "Minne Li, Lisheng Wu, Haitham Bou Ammar, Jun Wang", "title": "Multi-View Reinforcement Learning", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with multi-view reinforcement learning (MVRL), which\nallows for decision making when agents share common dynamics but adhere to\ndifferent observation models. We define the MVRL framework by extending\npartially observable Markov decision processes (POMDPs) to support more than\none observation model and propose two solution methods through observation\naugmentation and cross-view policy transfer. We empirically evaluate our method\nand demonstrate its effectiveness in a variety of environments. Specifically,\nwe show reductions in sample complexities and computational time for acquiring\npolicies that handle multi-view environments.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 07:14:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Li", "Minne", ""], ["Wu", "Lisheng", ""], ["Ammar", "Haitham Bou", ""], ["Wang", "Jun", ""]]}, {"id": "1910.08322", "submitter": "Ville Hyv\\\"onen", "authors": "Ville Hyv\\\"onen, Elias J\\\"a\\\"asaari, Teemu Roos", "title": "Approximate Nearest Neighbor Search as a Multi-Label Classification\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate approximate nearest neighbor (ANN) search as a multi-label\nclassification task. The implications are twofold. First, tree-based indexes\ncan be searched more efficiently by interpreting them as models to solve this\ntask. Second, in addition to index structures designed specifically for ANN\nsearch, any type of classifier can be used as an index.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 09:37:19 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 14:40:33 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 12:43:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Hyv\u00f6nen", "Ville", ""], ["J\u00e4\u00e4saari", "Elias", ""], ["Roos", "Teemu", ""]]}, {"id": "1910.08345", "submitter": "Ad\\'elie Garin", "authors": "Ad\\'elie Garin and Guillaume Tauzin", "title": "A Topological \"Reading\" Lesson: Classification of MNIST using TDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a way to use Topological Data Analysis (TDA) for machine learning\ntasks on grayscale images. We apply persistent homology to generate a wide\nrange of topological features using a point cloud obtained from an image, its\nnatural grayscale filtration, and different filtrations defined on the\nbinarized image. We show that this topological machine learning pipeline can be\nused as a highly relevant dimensionality reduction by applying it to the MNIST\ndigits dataset. We conduct a feature selection and study their correlations\nwhile providing an intuitive interpretation of their importance, which is\nrelevant in both machine learning and TDA. Finally, we show that we can\nclassify digit images while reducing the size of the feature set by a factor 5\ncompared to the grayscale pixel value features and maintain similar accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 11:37:44 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 13:31:06 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Garin", "Ad\u00e9lie", ""], ["Tauzin", "Guillaume", ""]]}, {"id": "1910.08348", "submitter": "Luisa Zintgraf", "authors": "Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze,\n  Yarin Gal, Katja Hofmann, Shimon Whiteson", "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trading off exploration and exploitation in an unknown environment is key to\nmaximising expected return during learning. A Bayes-optimal policy, which does\nso optimally, conditions its actions not only on the environment state but on\nthe agent's uncertainty about the environment. Computing a Bayes-optimal policy\nis however intractable for all but the smallest tasks. In this paper, we\nintroduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to\nperform approximate inference in an unknown environment, and incorporate task\nuncertainty directly during action selection. In a grid-world domain, we\nillustrate how variBAD performs structured online exploration as a function of\ntask uncertainty. We further evaluate variBAD on MuJoCo domains widely used in\nmeta-RL and show that it achieves higher online return than existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 11:44:59 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 19:40:21 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zintgraf", "Luisa", ""], ["Shiarlis", "Kyriacos", ""], ["Igl", "Maximilian", ""], ["Schulze", "Sebastian", ""], ["Gal", "Yarin", ""], ["Hofmann", "Katja", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1910.08371", "submitter": "Taras Khakhulin", "authors": "Taras Khakhulin, Roman Schutski and Ivan Oseledets", "title": "Graph Convolutional Policy for Solving Tree Decomposition via\n  Reinforcement Learning Heuristics", "comments": "8 pages, 7 figures", "journal-ref": "NeurIPS 2020 Learning Meets Combinatorial Algorithms Workshop", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Reinforcement Learning based approach to approximately solve the\nTree Decomposition (TD) problem. TD is a combinatorial problem, which is\ncentral to the analysis of graph minor structure and computational complexity,\nas well as in the algorithms of probabilistic inference, register allocation,\nand other practical tasks. Recently, it has been shown that combinatorial\nproblems can be successively solved by learned heuristics. However, the\nmajority of existing works do not address the question of the generalization of\nlearning-based solutions. Our model is based on the graph convolution neural\nnetwork (GCN) for learning graph representations. We show that the agent\nbuilton GCN and trained on a single graph using an Actor-Critic method can\nefficiently generalize to real-world TD problem instances. We establish that\nour method successfully generalizes from small graphs, where TD can be found by\nexact algorithms, to large instances of practical interest, while still having\nvery low time-to-solution. On the other hand, the agent-based approach\nsurpasses all greedy heuristics by the quality of the solution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:27:38 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 11:26:50 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Khakhulin", "Taras", ""], ["Schutski", "Roman", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1910.08385", "submitter": "Aleksei Triastcyn", "authors": "Aleksei Triastcyn, Boi Faltings", "title": "Federated Generative Privacy", "comments": "IJCAI Workshop on Federated Machine Learning for User Privacy and\n  Data Confidentiality (FL-IJCAI 2019). 7 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose FedGP, a framework for privacy-preserving data\nrelease in the federated learning setting. We use generative adversarial\nnetworks, generator components of which are trained by FedAvg algorithm, to\ndraw privacy-preserving artificial data samples and empirically assess the risk\nof information disclosure. Our experiments show that FedGP is able to generate\nlabelled data of high quality to successfully train and validate supervised\nmodels. Finally, we demonstrate that our approach significantly reduces\nvulnerability of such models to model inversion attacks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:41:15 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Triastcyn", "Aleksei", ""], ["Faltings", "Boi", ""]]}, {"id": "1910.08406", "submitter": "Marie-Liesse Cauwet", "authors": "M.-L. Cauwet, C. Couprie, J. Dehos, P. Luc, J. Rapin, M. Riviere, F.\n  Teytaud, O. Teytaud", "title": "Fully Parallel Hyperparameter Search: Reshaped Space-Filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling designs such as scrambled-Hammersley, Latin Hypercube Sampling\nand Jittered Sampling have been proposed for fully parallel hyperparameter\nsearch, and were shown to be more effective than random or grid search. In this\npaper, we show that these designs only improve over random search by a constant\nfactor. In contrast, we introduce a new approach based on reshaping the search\ndistribution, which leads to substantial gains over random search, both\ntheoretically and empirically. We propose two flavors of reshaping. First, when\nthe distribution of the optimum is some known $P_0$, we propose Recentering,\nwhich uses as search distribution a modified version of $P_0$ tightened closer\nto the center of the domain, in a dimension-dependent and budget-dependent\nmanner. Second, we show that in a wide range of experiments with $P_0$ unknown,\nusing a proposed Cauchy transformation, which simultaneously has a heavier tail\n(for unbounded hyperparameters) and is closer to the boundaries (for bounded\nhyperparameters), leads to improved performances. Besides artificial\nexperiments and simple real world tests on clustering or Salmon mappings, we\ncheck our proposed methods on expensive artificial intelligence tasks such as\nattend/infer/repeat, video next frame segmentation forecasting and progressive\ngenerative adversarial networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:16:57 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 09:11:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Cauwet", "M. -L.", ""], ["Couprie", "C.", ""], ["Dehos", "J.", ""], ["Luc", "P.", ""], ["Rapin", "J.", ""], ["Riviere", "M.", ""], ["Teytaud", "F.", ""], ["Teytaud", "O.", ""]]}, {"id": "1910.08408", "submitter": "Alexander Matei", "authors": "Tristan Gally, Peter Groche, Florian Hoppe, Anja Kuttich, Alexander\n  Matei, Marc E. Pfetsch, Martin Rakowitsch, Stefan Ulbrich", "title": "Identification of Model Uncertainty via Optimal Design of Experiments\n  Applied to a Mechanical Press", "comments": null, "journal-ref": null, "doi": "10.1007/s11081-021-09600-8", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering applications almost all processes are described with the help\nof models. Especially forming machines heavily rely on mathematical models for\ncontrol and condition monitoring. Inaccuracies during the modeling,\nmanufacturing and assembly of these machines induce model uncertainty which\nimpairs the controller's performance. In this paper we propose an approach to\nidentify model uncertainty using parameter identification, optimal design of\nexperiments and hypothesis testing. The experimental setup is characterized by\noptimal sensor positions such that specific model parameters can be determined\nwith minimal variance. This allows for the computation of confidence regions in\nwhich the real parameters or the parameter estimates from different test sets\nhave to lie. We claim that inconsistencies in the estimated parameter values,\nconsidering their approximated confidence ellipsoids as well, cannot be\nexplained by data uncertainty but are indicators of model uncertainty. The\nproposed method is demonstrated using a component of the 3D Servo Press, a\nmulti-technology forming machine that combines spindles with eccentric servo\ndrives.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:27:53 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 14:21:35 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 13:45:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gally", "Tristan", ""], ["Groche", "Peter", ""], ["Hoppe", "Florian", ""], ["Kuttich", "Anja", ""], ["Matei", "Alexander", ""], ["Pfetsch", "Marc E.", ""], ["Rakowitsch", "Martin", ""], ["Ulbrich", "Stefan", ""]]}, {"id": "1910.08412", "submitter": "Harshat Kumar", "authors": "Harshat Kumar and Alec Koppel and Alejandro Ribeiro", "title": "On the Sample Complexity of Actor-Critic Method for Reinforcement\n  Learning with Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning, mathematically described by Markov Decision Problems,\nmay be approached either through dynamic programming or policy search.\nActor-critic algorithms combine the merits of both approaches by alternating\nbetween steps to estimate the value function and policy gradient updates. Due\nto the fact that the updates exhibit correlated noise and biased gradient\nupdates, only the asymptotic behavior of actor-critic is known by connecting\nits behavior to dynamical systems. This work puts forth a new variant of\nactor-critic that employs Monte Carlo rollouts during the policy search\nupdates, which results in controllable bias that depends on the number of\ncritic evaluations. As a result, we are able to provide for the first time the\nconvergence rate of actor-critic algorithms when the policy search step employs\npolicy gradient, agnostic to the choice of policy evaluation technique. In\nparticular, we establish conditions under which the sample complexity is\ncomparable to stochastic gradient method for non-convex problems or slower as a\nresult of the critic estimation error, which is the main complexity bottleneck.\nThese results hold for in continuous state and action spaces with linear\nfunction approximation for the value function. We then specialize these\nconceptual results to the case where the critic is estimated by Temporal\nDifference, Gradient Temporal Difference, and Accelerated Gradient Temporal\nDifference. These learning rates are then corroborated on a navigation problem\ninvolving an obstacle, which suggests that learning more slowly may lead to\nimproved limit points, providing insight into the interplay between\noptimization and generalization in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:33:17 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kumar", "Harshat", ""], ["Koppel", "Alec", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1910.08413", "submitter": "Alexander Ra{\\ss}", "authors": "Faramarz Khosravi, Alexander Ra{\\ss}, J\\\"urgen Teich", "title": "Efficient Computation of Probabilistic Dominance in Robust\n  Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world problems typically require the simultaneous optimization of\nseveral, often conflicting objectives. Many of these multi-objective\noptimization problems are characterized by wide ranges of uncertainties in\ntheir decision variables or objective functions, which further increases the\ncomplexity of optimization. To cope with such uncertainties, robust\noptimization is widely studied aiming to distinguish candidate solutions with\nuncertain objectives specified by confidence intervals, probability\ndistributions or sampled data. However, existing techniques mostly either fail\nto consider the actual distributions or assume uncertainty as instances of\nuniform or Gaussian distributions. This paper introduces an empirical approach\nthat enables an efficient comparison of candidate solutions with uncertain\nobjectives that can follow arbitrary distributions. Given two candidate\nsolutions under comparison, this operator calculates the probability that one\nsolution dominates the other in terms of each uncertain objective. It can\nsubstitute for the standard comparison operator of existing optimization\ntechniques such as evolutionary algorithms to enable discovering robust\nsolutions to problems with multiple uncertain objectives. This paper also\nproposes to incorporate various uncertainties in well-known multi-objective\nproblems to provide a benchmark for evaluating uncertainty-aware optimization\ntechniques. The proposed comparison operator and benchmark suite are integrated\ninto an existing optimization tool that features a selection of multi-objective\noptimization problems and algorithms. Experiments show that in comparison with\nexisting techniques, the proposed approach achieves higher optimization quality\nat lower overheads.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:33:55 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Khosravi", "Faramarz", ""], ["Ra\u00df", "Alexander", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1910.08438", "submitter": "Kin Gwn Lore", "authors": "Kin Gwn Lore, Kishore K. Reddy", "title": "Implicit Context-aware Learning and Discovery for Streaming Data\n  Analytics", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of machine learning model can be further improved if\ncontextual cues are provided as input along with base features that are\ndirectly related to an inference task. In offline learning, one can inspect\nhistorical training data to identify contextual clusters either through feature\nclustering, or hand-crafting additional features to describe a context. While\noffline training enjoys the privilege of learning reliable models based on\nalready-defined contextual features, online training for streaming data may be\nmore challenging -- the data is streamed through time, and the underlying\ncontext during a data generation process may change. Furthermore, the problem\nis exacerbated when the number of possible context is not known. In this study,\nwe propose an online-learning algorithm involving the use of a neural\nnetwork-based autoencoder to identify contextual changes during training, then\ncompares the currently-inferred context to a knowledge base of learned contexts\nas training advances. Results show that classifier-training benefits from the\nautomatically discovered contexts which demonstrates quicker learning\nconvergence during contextual changes compared to current methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 14:30:27 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Lore", "Kin Gwn", ""], ["Reddy", "Kishore K.", ""]]}, {"id": "1910.08446", "submitter": "Pratik Gajane", "authors": "Pratik Gajane, Ronald Ortner, Peter Auer and Csaba Szepesvari", "title": "Autonomous exploration for navigating in non-stationary CMPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting in which the objective is to learn to navigate in a\ncontrolled Markov process (CMP) where transition probabilities may abruptly\nchange. For this setting, we propose a performance measure called exploration\nsteps which counts the time steps at which the learner lacks sufficient\nknowledge to navigate its environment efficiently. We devise a learning\nmeta-algorithm, MNM and prove an upper bound on the exploration steps in terms\nof the number of changes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 14:40:26 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Gajane", "Pratik", ""], ["Ortner", "Ronald", ""], ["Auer", "Peter", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1910.08461", "submitter": "Theodore Moskovitz", "authors": "Ted Moskovitz, Rui Wang, Janice Lan, Sanyam Kapoor, Thomas Miconi,\n  Jason Yosinski, Aditya Rawal", "title": "First-Order Preconditioning via Hypergradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard gradient descent methods are susceptible to a range of issues that\ncan impede training, such as high correlations and different scaling in\nparameter space.These difficulties can be addressed by second-order approaches\nthat apply a pre-conditioning matrix to the gradient to improve convergence.\nUnfortunately, such algorithms typically struggle to scale to high-dimensional\nproblems, in part because the calculation of specific preconditioners such as\nthe inverse Hessian or Fisher information matrix is highly expensive. We\nintroduce first-order preconditioning (FOP), a fast, scalable approach that\ngeneralizes previous work on hypergradient descent (Almeida et al., 1998;\nMaclaurin et al., 2015; Baydin et al.,2017) to learn a preconditioning matrix\nthat only makes use of first-order information. Experiments show that FOP is\nable to improve the performance of standard deep learning optimizers on visual\nclassification and reinforcement learning tasks with minimal computational\noverhead. We also investigate the properties of the learned preconditioning\nmatrices and perform a preliminary theoretical analysis of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:12:36 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 23:12:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Moskovitz", "Ted", ""], ["Wang", "Rui", ""], ["Lan", "Janice", ""], ["Kapoor", "Sanyam", ""], ["Miconi", "Thomas", ""], ["Yosinski", "Jason", ""], ["Rawal", "Aditya", ""]]}, {"id": "1910.08475", "submitter": "Jordan Ash", "authors": "Jordan T. Ash and Ryan P. Adams", "title": "On Warm-Starting Neural Network Training", "comments": null, "journal-ref": "2020 Advances in Neural Information Processing Systems", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world deployments of machine learning systems, data arrive\npiecemeal. These learning scenarios may be passive, where data arrive\nincrementally due to structural properties of the problem (e.g., daily\nfinancial data) or active, where samples are selected according to a measure of\ntheir quality (e.g., experimental design). In both of these cases, we are\nbuilding a sequence of models that incorporate an increasing amount of data. We\nwould like each of these models in the sequence to be performant and take\nadvantage of all the data that are available to that point. Conventional\nintuition suggests that when solving a sequence of related optimization\nproblems of this form, it should be possible to initialize using the solution\nof the previous iterate -- to \"warm start\" the optimization rather than\ninitialize from scratch -- and see reductions in wall-clock time. However, in\npractice this warm-starting seems to yield poorer generalization performance\nthan models that have fresh random initializations, even though the final\ntraining losses are similar. While it appears that some hyperparameter settings\nallow a practitioner to close this generalization gap, they seem to only do so\nin regimes that damage the wall-clock gains of the warm start. Nevertheless, it\nis highly desirable to be able to warm-start neural network training, as it\nwould dramatically reduce the resource usage associated with the construction\nof performant deep learning systems. In this work, we take a closer look at\nthis empirical phenomenon and try to understand when and how it occurs. We also\nprovide a surprisingly simple trick that overcomes this pathology in several\nimportant situations, and present experiments that elucidate some of its\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:35:59 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 18:09:19 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 07:58:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ash", "Jordan T.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1910.08476", "submitter": "Nino Vieillard", "authors": "Nino Vieillard, Olivier Pietquin, Matthieu Geist", "title": "On Connections between Constrained Optimization and Reinforcement\n  Learning", "comments": "Optimization Foundations of Reinforcement Learning Workshop at\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic Programming (DP) provides standard algorithms to solve Markov\nDecision Processes. However, these algorithms generally do not optimize a\nscalar objective function. In this paper, we draw connections between DP and\n(constrained) convex optimization. Specifically, we show clear links in the\nalgorithmic structure between three DP schemes and optimization algorithms. We\nlink Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified\nPolicy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert\nPrediction) to Dual Averaging. These abstract DP schemes are representative of\na number of (deep) Reinforcement Learning (RL) algorithms. By highlighting\nthese connections (most of which have been noticed earlier, but in a scattered\nway), we would like to encourage further studies linking RL and convex\noptimization, that could lead to the design of new, more efficient, and better\nunderstood RL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:42:35 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 13:38:59 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Vieillard", "Nino", ""], ["Pietquin", "Olivier", ""], ["Geist", "Matthieu", ""]]}, {"id": "1910.08483", "submitter": "Agni Orfanoudaki", "authors": "Dimitris Bertsimas, Agni Orfanoudaki, Rory B. Weiner", "title": "Personalized Treatment for Coronary Artery Disease Patients: A Machine\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current clinical practice guidelines for managing Coronary Artery Disease\n(CAD) account for general cardiovascular risk factors. However, they do not\npresent a framework that considers personalized patient-specific\ncharacteristics. Using the electronic health records of 21,460 patients, we\ncreated data-driven models for personalized CAD management that significantly\nimprove health outcomes relative to the standard of care. We develop binary\nclassifiers to detect whether a patient will experience an adverse event due to\nCAD within a 10-year time frame. Combining the patients' medical history and\nclinical examination results, we achieve 81.5% AUC. For each treatment, we also\ncreate a series of regression models that are based on different supervised\nmachine learning algorithms. We are able to estimate with average R squared =\n0.801 the time from diagnosis to a potential adverse event (TAE) and gain\naccurate approximations of the counterfactual treatment effects. Leveraging\ncombinations of these models, we present ML4CAD, a novel personalized\nprescriptive algorithm. Considering the recommendations of multiple predictive\nmodels at once, ML4CAD identifies for every patient the therapy with the best\nexpected outcome using a voting mechanism. We evaluate its performance by\nmeasuring the prescription effectiveness and robustness under alternative\nground truths. We show that our methodology improves the expected TAE upon the\ncurrent baseline by 24.11%, increasing it from 4.56 to 5.66 years. The\nalgorithm performs particularly well for the male (24.3% improvement) and\nHispanic (58.41% improvement) subpopulations. Finally, we create an interactive\ninterface, providing physicians with an intuitive, accurate, readily\nimplementable, and effective tool.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:57:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Orfanoudaki", "Agni", ""], ["Weiner", "Rory B.", ""]]}, {"id": "1910.08485", "submitter": "Ruth Fong", "authors": "Ruth Fong, Mandela Patrick, Andrea Vedaldi", "title": "Understanding Deep Networks via Extremal Perturbations and Smooth Masks", "comments": "Accepted at ICCV 2019 as oral; supp mat at\n  http://ruthcfong.github.io/files/fong19_extremal_supps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of attribution is concerned with identifying the parts of an\ninput that are responsible for a model's output. An important family of\nattribution methods is based on measuring the effect of perturbations applied\nto the input. In this paper, we discuss some of the shortcomings of existing\napproaches to perturbation analysis and address them by introducing the concept\nof extremal perturbations, which are theoretically grounded and interpretable.\nWe also introduce a number of technical innovations to compute extremal\nperturbations, including a new area constraint and a parametric family of\nsmooth perturbations, which allow us to remove all tunable hyper-parameters\nfrom the optimization problem. We analyze the effect of perturbations as a\nfunction of their area, demonstrating excellent sensitivity to the spatial\nproperties of the deep neural network under stimulation. We also extend\nperturbation analysis to the intermediate layers of a network. This application\nallows us to identify the salient channels necessary for classification, which,\nwhen visualized using feature inversion, can be used to elucidate model\nbehavior. Lastly, we introduce TorchRay, an interpretability library built on\nPyTorch.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:02:01 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Fong", "Ruth", ""], ["Patrick", "Mandela", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1910.08489", "submitter": "Seok-Ju Hahn", "authors": "Seok-Ju Hahn, Junghye Lee", "title": "Privacy-preserving Federated Bayesian Learning of a Generative Model for\n  Imbalanced Classification of Clinical Data", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical research, the lack of events of interest often necessitates\nimbalanced learning. One approach to resolve this obstacle is data integration\nor sharing, but due to privacy concerns neither is practical. Therefore, there\nis an increasing demand for a platform on which an analysis can be performed in\na federated environment while maintaining privacy. However, it is quite\nchallenging to develop a federated learning algorithm that can address both\nprivacy-preserving and class imbalanced issues. In this study, we introduce a\nfederated generative model learning platform for generating samples in a\ndata-distributed environment while preserving privacy. We specifically propose\napproximate Bayesian computation-based Gaussian Mixture Model called 'Federated\nABC-GMM', which can oversample data in a minor class by estimating the\nposterior distribution of model parameters across institutions in a\nprivacy-preserving manner. PhysioNet2012, a dataset for prediction of mortality\nof patients in an Intensive Care Unit (ICU), was used to verify the performance\nof the proposed method. Experimental results show that our method boosts\nclassification performance in terms of F1 score up to nearly an ideal\nsituation. It is believed that the proposed method can be a novel alternative\nto solving class imbalance problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:15:09 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 08:09:58 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 07:15:32 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hahn", "Seok-Ju", ""], ["Lee", "Junghye", ""]]}, {"id": "1910.08501", "submitter": "Mariia Dmitrieva", "authors": "Mariia Dmitrieva, Keith E. Brown, Gary J. Heald, David M. Lane", "title": "Classification of spherical objects based on the form function of\n  acoustic echoes", "comments": null, "journal-ref": "Proc. of the 4th Underwater Acoustics Conference and Exhibition\n  (UACE), Skiathos, Greece, Sept 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to recognise an object is to study how the echo has been shaped\nduring the interaction with the target. Wideband sonar allows the study of the\nenergy distribution for a large range of frequencies. The frequency\ndistribution contains information about an object, including its inner\nstructure. This information is a key for automatic recognition. The scattering\nby a target can be quantitatively described by its Form Function. The Form\nFunction can be calculated based on the data of the initial pulse, reflected\npulse and parameters of a medium where the pulse is propagating. In this work\nspherical objects are classified based on their filler material - water or air.\nWe limit the study to spherical 2 layered targets immersed in water. The Form\nFunction is used as a descriptor and fed into a Neural Network classifier,\nMultilayer Perceptron (MLP). The performance of the classifier is compared with\nSupport Vector Machine (SVM) and the Form Function descriptor is examined in\ncontrast to the Time and Frequency Representation of the echo.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:51:30 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Dmitrieva", "Mariia", ""], ["Brown", "Keith E.", ""], ["Heald", "Gary J.", ""], ["Lane", "David M.", ""]]}, {"id": "1910.08512", "submitter": "Batiste Le Bars", "authors": "Batiste Le Bars and Pierre Humbert and Argyris Kalogeratos and Nicolas\n  Vayatis", "title": "Learning the piece-wise constant graph structure of a varying Ising\n  model", "comments": "18 pages (9 pages for Appendix), 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the estimation of multiple change-points in a\ntime-varying Ising model that evolves piece-wise constantly. The aim is to\nidentify both the moments at which significant changes occur in the Ising\nmodel, as well as the underlying graph structures. For this purpose, we propose\nto estimate the neighborhood of each node by maximizing a penalized version of\nits conditional log-likelihood. The objective of the penalization is twofold:\nit imposes sparsity in the learned graphs and, thanks to a fused-type penalty,\nit also enforces them to evolve piece-wise constantly. Using few assumptions,\nwe provide two change-points consistency theorems. Those are the first in the\ncontext of unknown number of change-points detection in time-varying Ising\nmodel. Finally, experimental results on several synthetic datasets and a\nreal-world dataset demonstrate the performance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:20:37 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 12:51:45 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bars", "Batiste Le", ""], ["Humbert", "Pierre", ""], ["Kalogeratos", "Argyris", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1910.08519", "submitter": "Sam Ringer", "authors": "Sam Ringer, Will Williams, Tom Ash, Remi Francis, David MacLeod", "title": "Texture Bias Of CNNs Limits Few-Shot Classification Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate image classification given small amounts of labelled data (few-shot\nclassification) remains an open problem in computer vision. In this work we\nexamine how the known texture bias of Convolutional Neural Networks (CNNs)\naffects few-shot classification performance. Although texture bias can help in\nstandard image classification, in this work we show it significantly harms\nfew-shot classification performance. After correcting this bias we demonstrate\nstate-of-the-art performance on the competitive miniImageNet task using a\nmethod far simpler than the current best performing few-shot learning\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:30:11 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ringer", "Sam", ""], ["Williams", "Will", ""], ["Ash", "Tom", ""], ["Francis", "Remi", ""], ["MacLeod", "David", ""]]}, {"id": "1910.08525", "submitter": "Luca Franceschi", "authors": "Michele Donini, Luca Franceschi, Massimiliano Pontil, Orchid Majumder,\n  Paolo Frasconi", "title": "MARTHE: Scheduling the Learning Rate Via Online Hypergradients", "comments": "IJCAI 2020. Larger images. Code available at\n  https://github.com/awslabs/adatune", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of fitting task-specific learning rate schedules from\nthe perspective of hyperparameter optimization, aiming at good generalization.\nWe describe the structure of the gradient of a validation error w.r.t. the\nlearning rate schedule -- the hypergradient. Based on this, we introduce\nMARTHE, a novel online algorithm guided by cheap approximations of the\nhypergradient that uses past information from the optimization trajectory to\nsimulate future behaviour. It interpolates between two recent techniques, RTHO\n(Franceschi et al., 2017) and HD (Baydin et al. 2018), and is able to produce\nlearning rate schedules that are more stable leading to models that generalize\nbetter.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:42:09 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 16:04:50 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 16:43:15 GMT"}, {"version": "v4", "created": "Sun, 17 May 2020 10:32:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Donini", "Michele", ""], ["Franceschi", "Luca", ""], ["Pontil", "Massimiliano", ""], ["Majumder", "Orchid", ""], ["Frasconi", "Paolo", ""]]}, {"id": "1910.08527", "submitter": "Ignavier Ng", "authors": "Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, Jun Wang", "title": "Masked Gradient-Based Causal Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning causal structures from\nobservational data. We reformulate the Structural Equation Model (SEM) in an\naugmented form with a binary graph adjacency matrix and show that, if the\noriginal SEM is identifiable, then this augmented form can be identified up to\nsuper-graphs of the true causal graph under mild conditions. Three methods are\nfurther provided to remove spurious edges to recover the true graph. We next\nutilize the augmented form to develop a masked structure learning method that\ncan be efficiently trained using gradient-based optimization methods, by\nleveraging a smooth characterization on acyclicity and the Gumbel-Softmax\napproach to approximate the binary adjacency matrix. It is found that the\nobtained entries are typically near zero or one, and can be easily thresholded\nto identify the edges. We conduct experiments on synthetic and real datasets to\nvalidate the effectiveness of the proposed method and show that the method can\nreadily include different smooth functions to model causal relationships.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:46:44 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 05:47:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ng", "Ignavier", ""], ["Fang", "Zhuangyan", ""], ["Zhu", "Shengyu", ""], ["Chen", "Zhitang", ""], ["Wang", "Jun", ""]]}, {"id": "1910.08540", "submitter": "Wenyuan Li", "authors": "Wenyuan Li, Zichen Wang, Yuguang Yue, Jiayun Li, William Speier,\n  Mingyuan Zhou, Corey W. Arnold", "title": "Semi-supervised Learning using Adversarial Training with Good and Bad\n  Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate semi-supervised learning (SSL) for image\nclassification using adversarial training. Previous results have illustrated\nthat generative adversarial networks (GANs) can be used for multiple purposes.\nTriple-GAN, which aims to jointly optimize model components by incorporating\nthree players, generates suitable image-label pairs to compensate for the lack\nof labeled data in SSL with improved benchmark performance. Conversely, Bad (or\ncomplementary) GAN, optimizes generation to produce complementary data-label\npairs and force a classifier's decision boundary to lie between data manifolds.\nAlthough it generally outperforms Triple-GAN, Bad GAN is highly sensitive to\nthe amount of labeled data used for training. Unifying these two approaches, we\npresent unified-GAN (UGAN), a novel framework that enables a classifier to\nsimultaneously learn from both good and bad samples through adversarial\ntraining. We perform extensive experiments on various datasets and demonstrate\nthat UGAN: 1) achieves state-of-the-art performance among other deep generative\nmodels, and 2) is robust to variations in the amount of labeled data used for\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 05:47:08 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Wenyuan", ""], ["Wang", "Zichen", ""], ["Yue", "Yuguang", ""], ["Li", "Jiayun", ""], ["Speier", "William", ""], ["Zhou", "Mingyuan", ""], ["Arnold", "Corey W.", ""]]}, {"id": "1910.08573", "submitter": "Gautier Cosne", "authors": "Gautier Cosne, Guillaume Maze, Pierre Tandeo", "title": "Coupling Oceanic Observation Systems to Study Mesoscale Ocean Dynamics", "comments": "Accepted as a workshop paper to NeurIPS 2019 Workshop 'Tackling\n  Climate Change with Machine Learning'", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding local currents in the North Atlantic region of the ocean is a\nkey part of modelling heat transfer and global climate patterns. Satellites\nprovide a surface signature of the temperature of the ocean with a high\nhorizontal resolution while in situ autonomous probes supply high vertical\nresolution, but horizontally sparse, knowledge of the ocean interior thermal\nstructure. The objective of this paper is to develop a methodology to combine\nthese complementary ocean observing systems measurements to obtain a\nthree-dimensional time series of ocean temperatures with high horizontal and\nvertical resolution. Within an observation-driven framework, we investigate the\nextent to which mesoscale ocean dynamics in the North Atlantic region may be\ndecomposed into a mixture of dynamical modes, characterized by different local\nregressions between Sea Surface Temperature (SST), Sea Level Anomalies (SLA)\nand Vertical Temperature fields. Ultimately we propose a Latent-class\nregression method to improve prediction of vertical ocean temperature.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 18:10:47 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cosne", "Gautier", ""], ["Maze", "Guillaume", ""], ["Tandeo", "Pierre", ""]]}, {"id": "1910.08581", "submitter": "Shaeke Salman", "authors": "Shaeke Salman, Canlin Zhang, Xiuwen Liu, Washington Mio", "title": "Towards Quantifying Intrinsic Generalization of Deep ReLU Networks", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the underlying mechanisms that enable the empirical successes\nof deep neural networks is essential for further improving their performance\nand explaining such networks. Towards this goal, a specific question is how to\nexplain the \"surprising\" behavior of the same over-parametrized deep neural\nnetworks that can generalize well on real datasets and at the same time\n\"memorize\" training samples when the labels are randomized. In this paper, we\ndemonstrate that deep ReLU networks generalize from training samples to new\npoints via piece-wise linear interpolation. We provide a quantified analysis on\nthe generalization ability of a deep ReLU network: Given a fixed point\n$\\mathbf{x}$ and a fixed direction in the input space $\\mathcal{S}$, there is\nalways a segment such that any point on the segment will be classified the same\nas the fixed point $\\mathbf{x}$. We call this segment the $generalization \\\ninterval$. We show that the generalization intervals of a ReLU network behave\nsimilarly along pairwise directions between samples of the same label in both\nreal and random cases on the MNIST and CIFAR-10 datasets. This result suggests\nthat the same interpolation mechanism is used in both cases. Additionally, for\ndatasets using real labels, such networks provide a good approximation of the\nunderlying manifold in the data, where the changes are much smaller along\ntangent directions than along normal directions. On the other hand, however,\nfor datasets with random labels, generalization intervals along mid-lines of\ntriangles with the same label are much smaller than those on the datasets with\nreal labels, suggesting different behaviors along other directions. Our\nsystematic experiments demonstrate for the first time that such deep neural\nnetworks generalize through the same interpolation and explain the differences\nbetween their performance on datasets with real and random labels.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 18:38:22 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Salman", "Shaeke", ""], ["Zhang", "Canlin", ""], ["Liu", "Xiuwen", ""], ["Mio", "Washington", ""]]}, {"id": "1910.08589", "submitter": "Paul Scherer", "authors": "Paul Scherer, Helena Andres-Terre, Pietro Lio, Mateja Jamnik", "title": "Decoupling feature propagation from the design of graph auto-encoders", "comments": "4 pages (considering single anonymous naming during original\n  submission, now a few lines over 4). Originally submitted to NeurIPS 2019\n  Graph Representation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two instances, L-GAE and L-VGAE, of the variational graph\nauto-encoding family (VGAE) based on separating feature propagation operations\nfrom graph convolution layers typically found in graph learning methods to a\nsingle linear matrix computation made prior to input in standard auto-encoder\narchitectures. This decoupling enables the independent and fixed design of the\nauto-encoder without requiring additional GCN layers for every desired increase\nin the size of a node's local receptive field. Fixing the auto-encoder enables\na fairer assessment on the size of a nodes receptive field in building\nrepresentations. Furthermore a by-product of fixing the auto-encoder design\noften results in substantially smaller networks than their VGAE counterparts\nespecially as we increase the number of feature propagations. A comparative\ndownstream evaluation on link prediction tasks show comparable state of the art\nperformance to similar VGAE arrangements despite considerable simplification.\nWe also show the simple application of our methodology to more challenging\nrepresentation learning scenarios such as spatio-temporal graph representation\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:01:41 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Scherer", "Paul", ""], ["Andres-Terre", "Helena", ""], ["Lio", "Pietro", ""], ["Jamnik", "Mateja", ""]]}, {"id": "1910.08595", "submitter": "Brett Mullins", "authors": "Brett Mullins", "title": "Identifying the Most Explainable Classifier", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of pointwise coverage to measure the explainability\nproperties of machine learning classifiers. An explanation for a prediction is\na definably simple region of the feature space sharing the same label as the\nprediction, and the coverage of an explanation measures its size or\ngeneralizability. With this notion of explanation, we investigate whether or\nnot there is a natural characterization of the most explainable classifier.\nAccording with our intuitions, we prove that the binary linear classifier is\nuniquely the most explainable classifier up to negligible sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:24:38 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:05:02 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Mullins", "Brett", ""]]}, {"id": "1910.08597", "submitter": "Matteo Sordello", "authors": "Matteo Sordello, Hangfeng He and Weijie Su", "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting\n  Diagnostic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes SplitSGD, a new dynamic learning rate schedule for\nstochastic optimization. This method decreases the learning rate for better\nadaptation to the local geometry of the objective function whenever a\nstationary phase is detected, that is, the iterates are likely to bounce at\naround a vicinity of a local minimum. The detection is performed by splitting\nthe single thread into two and using the inner product of the gradients from\nthe two threads as a measure of stationarity. Owing to this simple yet provably\nvalid stationarity detection, SplitSGD is easy-to-implement and essentially\ndoes not incur additional computational cost than standard SGD. Through a\nseries of extensive experiments, we show that this method is appropriate for\nboth convex problems and training (non-convex) neural networks, with\nperformance compared favorably to other stochastic optimization methods.\nImportantly, this method is observed to be very robust with a set of default\nparameters for a wide range of problems and, moreover, yields better\ngeneralization performance than other adaptive gradient methods such as Adam.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:38:53 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 13:59:57 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 17:30:27 GMT"}, {"version": "v4", "created": "Sun, 7 Jun 2020 16:38:13 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sordello", "Matteo", ""], ["He", "Hangfeng", ""], ["Su", "Weijie", ""]]}, {"id": "1910.08605", "submitter": "Anthony Ashmore", "authors": "Anthony Ashmore, Yang-Hui He, Burt Ovrut", "title": "Machine learning Calabi-Yau metrics", "comments": "56 pages, 14 figures, references added", "journal-ref": null, "doi": "10.1002/prop.202000068", "report-no": null, "categories": "hep-th math.AG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply machine learning to the problem of finding numerical Calabi-Yau\nmetrics. Building on Donaldson's algorithm for calculating balanced metrics on\nK\\\"ahler manifolds, we combine conventional curve fitting and machine-learning\ntechniques to numerically approximate Ricci-flat metrics. We show that machine\nlearning is able to predict the Calabi-Yau metric and quantities associated\nwith it, such as its determinant, having seen only a small sample of training\ndata. Using this in conjunction with a straightforward curve fitting routine,\nwe demonstrate that it is possible to find highly accurate numerical metrics\nmuch more quickly than by using Donaldson's algorithm alone, with our new\nmachine-learning algorithm decreasing the time required by between one and two\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:53:34 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 16:25:01 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ashmore", "Anthony", ""], ["He", "Yang-Hui", ""], ["Ovrut", "Burt", ""]]}, {"id": "1910.08606", "submitter": "Chase Shimmin", "authors": "Benjamin Nachman and Chase Shimmin", "title": "AI Safety for High Energy Physics", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of high-energy physics (HEP), along with many scientific\ndisciplines, is currently experiencing a dramatic influx of new methodologies\npowered by modern machine learning techniques. Over the last few years, a\ngrowing body of HEP literature has focused on identifying promising\napplications of deep learning in particular, and more recently these techniques\nare starting to be realized in an increasing number of experimental\nmeasurements. The overall conclusion from this impressive and extensive set of\nstudies is that rarer and more complex physics signatures can be identified\nwith the new set of powerful tools from deep learning. However, there is an\nunstudied systematic risk associated with combining the traditional HEP\nworkflow and deep learning with high-dimensional data. In particular,\ncalibrating and validating the response of deep neural networks is in general\nnot experimentally feasible, and therefore current methods may be biased in\nways that are not covered by current uncertainty estimates. By borrowing ideas\nfrom AI safety, we illustrate these potential issues and propose a method to\nbound the size of unaccounted for uncertainty. In addition to providing a\npragmatic diagnostic, this work will hopefully begin a dialogue within the\ncommunity about the robust application of deep learning to experimental\nanalyses.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:54:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Nachman", "Benjamin", ""], ["Shimmin", "Chase", ""]]}, {"id": "1910.08613", "submitter": "Ali Girayhan Ozbay", "authors": "Ali Girayhan \\\"Ozbay, Arash Hamzehloo, Sylvain Laizet, Panagiotis\n  Tzirakis, Georgios Rizos, Bj\\\"orn Schuller", "title": "Poisson CNN: Convolutional neural networks for the solution of the\n  Poisson equation on a Cartesian mesh", "comments": "34 pages, 18 figures. Publ{\\i}shed in Data Centric Engineering. Code\n  available at https://github.com/aligirayhanozbay/poisson_CNN", "journal-ref": "Data-Centric Engineering. [Online] Cambridge University Press;\n  2021;2: e6", "doi": "10.1017/dce.2021.7", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Poisson equation is commonly encountered in engineering, for instance in\ncomputational fluid dynamics (CFD) where it is needed to compute corrections to\nthe pressure field to ensure the incompressibility of the velocity field. In\nthe present work, we propose a novel fully convolutional neural network (CNN)\narchitecture to infer the solution of the Poisson equation on a 2D Cartesian\ngrid with different resolutions given the right hand side term, arbitrary\nboundary conditions and grid parameters. It provides unprecedented versatility\nfor a CNN approach dealing with partial differential equations. The boundary\nconditions are handled using a novel approach by decomposing the original\nPoisson problem into a homogeneous Poisson problem plus four inhomogeneous\nLaplace sub-problems. The model is trained using a novel loss function\napproximating the continuous $L^p$ norm between the prediction and the target.\nEven when predicting on grids denser than previously encountered, our model\ndemonstrates encouraging capacity to reproduce the correct solution profile.\nThe proposed model, which outperforms well-known neural network models, can be\nincluded in a CFD solver to help with solving the Poisson equation. Analytical\ntest cases indicate that our CNN architecture is capable of predicting the\ncorrect solution of a Poisson problem with mean percentage errors below 10%, an\nimprovement by comparison to the first step of conventional iterative methods.\nPredictions from our model, used as the initial guess to iterative algorithms\nlike Multigrid, can reduce the RMS error after a single iteration by more than\n90% compared to a zero initial guess.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 20:29:20 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:33:36 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 11:00:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["\u00d6zbay", "Ali Girayhan", ""], ["Hamzehloo", "Arash", ""], ["Laizet", "Sylvain", ""], ["Tzirakis", "Panagiotis", ""], ["Rizos", "Georgios", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1910.08623", "submitter": "Yasaman Esfandiari", "authors": "Yasaman Esfandiari, Aditya Balu, Keivan Ebrahimi, Umesh Vaidya, Nicola\n  Elia, Soumik Sarkar", "title": "A Fast Saddle-Point Dynamical System Approach to Robust Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent focus on robustness to adversarial attacks for deep neural networks\nproduced a large variety of algorithms for training robust models. Most of the\neffective algorithms involve solving the min-max optimization problem for\ntraining robust models (min step) under worst-case attacks (max step). However,\nthey often suffer from high computational cost from running several inner\nmaximization iterations (to find an optimal attack) inside every outer\nminimization iteration. Therefore, it becomes difficult to readily apply such\nalgorithms for moderate to large size real world data sets. To alleviate this,\nwe explore the effectiveness of iterative descent-ascent algorithms where the\nmaximization and minimization steps are executed in an alternate fashion to\nsimultaneously obtain the worst-case attack and the corresponding robust model.\nSpecifically, we propose a novel discrete-time dynamical system-based algorithm\nthat aims to find the saddle point of a min-max optimization problem in the\npresence of uncertainties. Under the assumptions that the cost function is\nconvex and uncertainties enter concavely in the robust learning problem, we\nanalytically show that our algorithm converges asymptotically to the robust\noptimal solution under a general adversarial budget constraints as induced by\n$\\ell_p$ norm, for $1\\leq p\\leq \\infty$. Based on our proposed analysis, we\ndevise a fast robust training algorithm for deep neural networks. Although such\ntraining involves highly non-convex robust optimization problems, empirical\nresults show that the algorithm can achieve significant robustness compared to\nother state-of-the-art robust models on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 20:55:39 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 21:24:47 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 20:38:24 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Esfandiari", "Yasaman", ""], ["Balu", "Aditya", ""], ["Ebrahimi", "Keivan", ""], ["Vaidya", "Umesh", ""], ["Elia", "Nicola", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1910.08629", "submitter": "Yongfeng Zhang", "authors": "Shaoyun Shi, Hanxiong Chen, Min Zhang, Yongfeng Zhang", "title": "Neural Logic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the great success of deep neural networks in many\nresearch areas. The fundamental idea behind the design of most neural networks\nis to learn similarity patterns from data for prediction and inference, which\nlacks the ability of logical reasoning. However, the concrete ability of\nlogical reasoning is critical to many theoretical and practical problems. In\nthis paper, we propose Neural Logic Network (NLN), which is a dynamic neural\narchitecture that builds the computational graph according to input logical\nexpressions. It learns basic logical operations as neural modules, and conducts\npropositional logical reasoning through the network for inference. Experiments\non simulated data show that NLN achieves significant performance on solving\nlogical equations. Further experiments on real-world data show that NLN\nsignificantly outperforms state-of-the-art models on collaborative filtering\nand personalized recommendation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 01:53:37 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shi", "Shaoyun", ""], ["Chen", "Hanxiong", ""], ["Zhang", "Min", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "1910.08635", "submitter": "Li Yang", "authors": "Li Yang, Abdallah Moubayed, Ismail Hamieh, Abdallah Shami", "title": "Tree-based Intelligent Intrusion Detection System in Internet of\n  Vehicles", "comments": "Accepted in IEEE Global Communications Conference (GLOBECOM) 2019", "journal-ref": null, "doi": "10.1109/GLOBECOM38437.2019.9013892", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of autonomous vehicles (AVs) is a promising technology in Intelligent\nTransportation Systems (ITSs) to improve safety and driving efficiency.\nVehicle-to-everything (V2X) technology enables communication among vehicles and\nother infrastructures. However, AVs and Internet of Vehicles (IoV) are\nvulnerable to different types of cyber-attacks such as denial of service,\nspoofing, and sniffing attacks. In this paper, an intelligent intrusion\ndetection system (IDS) is proposed based on tree-structure machine learning\nmodels. The results from the implementation of the proposed intrusion detection\nsystem on standard data sets indicate that the system has the ability to\nidentify various cyber-attacks in the AV networks. Furthermore, the proposed\nensemble learning and feature selection approaches enable the proposed system\nto achieve high detection rate and low computational cost simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 21:35:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yang", "Li", ""], ["Moubayed", "Abdallah", ""], ["Hamieh", "Ismail", ""], ["Shami", "Abdallah", ""]]}, {"id": "1910.08636", "submitter": "Mandana Samiei", "authors": "Mandana Samiei, Tobias W\\\"urfl, Tristan Deleu, Martin Weiss, Francis\n  Dutil, Thomas Fevens, Genevi\\`eve Boucher, Sebastien Lemieux, Joseph Paul\n  Cohen", "title": "The TCGA Meta-Dataset Clinical Benchmark", "comments": "5 Pages, Submitted to MLCB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is bringing a paradigm shift to healthcare by changing the\nprocess of disease diagnosis and prognosis in clinics and hospitals. This\ndevelopment equips doctors and medical staff with tools to evaluate their\nhypotheses and hence make more precise decisions. Although most current\nresearch in the literature seeks to develop techniques and methods for\npredicting one particular clinical outcome, this approach is far from the\nreality of clinical decision making in which you have to consider several\nfactors simultaneously. In addition, it is difficult to follow the recent\nprogress concretely as there is a lack of consistency in benchmark datasets and\ntask definitions in the field of Genomics. To address the aforementioned\nissues, we provide a clinical Meta-Dataset derived from the publicly available\ndata hub called The Cancer Genome Atlas Program (TCGA) that contains 174 tasks.\nWe believe those tasks could be good proxy tasks to develop methods which can\nwork on a few samples of gene expression data. Also, learning to predict\nmultiple clinical variables using gene-expression data is an important task due\nto the variety of phenotypes in clinical problems and lack of samples for some\nof the rare variables. The defined tasks cover a wide range of clinical\nproblems including predicting tumor tissue site, white cell count, histological\ntype, family history of cancer, gender, and many others which we explain later\nin the paper. Each task represents an independent dataset. We use regression\nand neural network baselines for all the tasks using only 150 samples and\ncompare their performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 21:44:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Samiei", "Mandana", ""], ["W\u00fcrfl", "Tobias", ""], ["Deleu", "Tristan", ""], ["Weiss", "Martin", ""], ["Dutil", "Francis", ""], ["Fevens", "Thomas", ""], ["Boucher", "Genevi\u00e8ve", ""], ["Lemieux", "Sebastien", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1910.08639", "submitter": "Ilya Kuzovkin", "authors": "Ashish Kumar, Toby Buckley, John B. Lanier, Qiaozhi Wang, Alicia\n  Kavelaars, Ilya Kuzovkin", "title": "OffWorld Gym: open-access physical robotics environment for real-world\n  reinforcement learning benchmark and research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success stories of applied machine learning can be traced back to the\ndatasets and environments that were put forward as challenges for the\ncommunity. The challenge that the community sets as a benchmark is usually the\nchallenge that the community eventually solves. The ultimate challenge of\nreinforcement learning research is to train real agents to operate in the real\nenvironment, but until now there has not been a common real-world RL benchmark.\nIn this work, we present a prototype real-world environment from OffWorld Gym\n-- a collection of real-world environments for reinforcement learning in\nrobotics with free public remote access. Close integration into existing\necosystem allows the community to start using OffWorld Gym without any prior\nexperience in robotics and takes away the burden of managing a physical\nrobotics system, abstracting it under a familiar API. We introduce a navigation\ntask, where a robot has to reach a visual beacon on an uneven terrain using\nonly the camera input and provide baseline results in both the real environment\nand the simulated replica. To start training, visit https://gym.offworld.ai\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 21:58:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 08:51:54 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 05:19:37 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 02:59:34 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Kumar", "Ashish", ""], ["Buckley", "Toby", ""], ["Lanier", "John B.", ""], ["Wang", "Qiaozhi", ""], ["Kavelaars", "Alicia", ""], ["Kuzovkin", "Ilya", ""]]}, {"id": "1910.08640", "submitter": "Simran Kaur", "authors": "Simran Kaur, Jeremy Cohen, Zachary C. Lipton", "title": "Are Perceptually-Aligned Gradients a General Property of Robust\n  Classifiers?", "comments": "To appear in the \"Science Meets Engineering of Deep Learning\"\n  Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a standard convolutional neural network, optimizing over the input pixels\nto maximize the score of some target class will generally produce a\ngrainy-looking version of the original image. However, Santurkar et al. (2019)\ndemonstrated that for adversarially-trained neural networks, this optimization\nproduces images that uncannily resemble the target class. In this paper, we\nshow that these \"perceptually-aligned gradients\" also occur under randomized\nsmoothing, an alternative means of constructing adversarially-robust\nclassifiers. Our finding supports the hypothesis that perceptually-aligned\ngradients may be a general property of robust classifiers. We hope that our\nresults will inspire research aimed at explaining this link between\nperceptually-aligned gradients and adversarial robustness.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:02:41 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 16:02:06 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Kaur", "Simran", ""], ["Cohen", "Jeremy", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1910.08644", "submitter": "Fatima Batool", "authors": "Fatima Batool", "title": "Initialization methods for optimum average silhouette width clustering", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A unified clustering approach that can estimate number of clusters and\nproduce clustering against this number simultaneously is proposed. Average\nsilhouette width (ASW) is a widely used standard cluster quality index. A\ndistance based objective function that optimizes ASW for clustering is defined.\nThe proposed algorithm named as OSil, only, needs data observations as an input\nwithout any prior knowledge of the number of clusters. This work is about\nthorough investigation of the proposed methodology, its usefulness and\nlimitations. A vast spectrum of clustering structures were generated, and\nseveral well-known clustering methods including partitioning, hierarchical,\ndensity based, and spatial methods were consider as the competitor of the\nproposed methodology. Simulation reveals that OSil algorithm has shown superior\nperformance in terms of clustering quality than all clustering methods included\nin the study. OSil can find well separated, compact clusters and have shown\nbetter performance for the estimation of number of clusters as compared to\nseveral methods. Apart from the proposal of the new methodology and it's\ninvestigation the paper offers a systematic analysis on the estimation of\ncluster indices, some of which never appeared together in comparative\nsimulation setup before. The study offers many insightful findings useful for\nthe selection of the clustering methods and indices for clustering quality\nassessment.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:11:12 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 18:16:22 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 20:28:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Batool", "Fatima", ""]]}, {"id": "1910.08650", "submitter": "Mahdieh Abbasi", "authors": "Mahdieh Abbasi, Changjian Shui, Arezoo Rajabi, Christian Gagne, Rakesh\n  Bobba", "title": "Toward Metrics for Differentiating Out-of-Distribution Sets", "comments": "Workshop on Safety and Robustness in Decision Making, NeurIPS 2019", "journal-ref": "ECAI 2020 : 24th European Conference on Artificial Intelligence", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vanilla CNNs, as uncalibrated classifiers, suffer from classifying\nout-of-distribution (OOD) samples nearly as confidently as in-distribution\nsamples. To tackle this challenge, some recent works have demonstrated the\ngains of leveraging available OOD sets for training end-to-end calibrated CNNs.\nHowever, a critical question remains unanswered in these works: how to\ndifferentiate OOD sets for selecting the most effective one(s) that induce\ntraining such CNNs with high detection rates on unseen OOD sets? To address\nthis pivotal question, we provide a criterion based on generalization errors of\nAugmented-CNN, a vanilla CNN with an added extra class employed for rejection,\non in-distribution and unseen OOD sets. However, selecting the most effective\nOOD set by directly optimizing this criterion incurs a huge computational cost.\nInstead, we propose three novel computationally-efficient metrics for\ndifferentiating between OOD sets according to their \"protection\" level of\nin-distribution sub-manifolds. We empirically verify that the most protective\nOOD sets -- selected according to our metrics -- lead to A-CNNs with\nsignificantly lower generalization errors than the A-CNNs trained on the least\nprotective ones. We also empirically show the effectiveness of a protective OOD\nset for training well-generalized confidence-calibrated vanilla CNNs. These\nresults confirm that 1) all OOD sets are not equally effective for training\nwell-performing end-to-end models (i.e., A-CNNs and calibrated CNNs) for OOD\ndetection tasks and 2) the protection level of OOD sets is a viable factor for\nrecognizing the most effective one. Finally, across the image classification\ntasks, we exhibit A-CNN trained on the most protective OOD set can also detect\nblack-box FGS adversarial examples as their distance (measured by our metrics)\nis becoming larger from the protected sub-manifolds.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:26:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:33:37 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 16:15:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Shui", "Changjian", ""], ["Rajabi", "Arezoo", ""], ["Gagne", "Christian", ""], ["Bobba", "Rakesh", ""]]}, {"id": "1910.08655", "submitter": "Ren Hu", "authors": "Ren Hu, QiFeng Li", "title": "Ensemble learning based linear power flow", "comments": "5 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an ensemble learning-based linearization approach for\npower flow, which differs from the network-parameter based direct current (DC)\npower flow or other extended versions of linearization. As a novel data-driven\nlinearization through data mining, it firstly applies the polynomial regression\n(PR) as a basic learner to capture the linear relationships between the bus\nvoltage as the independent variable and the active or reactive power as the\ndependent variable in rectangular coordinates. Then, gradient boosting (GB) and\nbagging as ensemble learning methods are introduced to combine all basic\nlearners to boost the model performance. The fitted linear power flow model is\nalso relaxed to compute the optimal power flow (OPF). The simulating results of\nstandard IEEE cases indicate that (1) ensemble learning methods outperform PR\nand GB works better than bagging; (2) as for solving OPF, the data-driven model\nexcels the DC model and the SDP relaxation in the computational accuracy, and\nworks faster than ACOPF and SDPOPF.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:04:41 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hu", "Ren", ""], ["Li", "QiFeng", ""]]}, {"id": "1910.08657", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Ryan A. Rossi", "title": "Temporal Network Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal networks representing a stream of timestamped edges are seemingly\nubiquitous in the real-world. However, the massive size and continuous nature\nof these networks make them fundamentally challenging to analyze and leverage\nfor descriptive and predictive modeling tasks. In this work, we propose a\ngeneral framework for temporal network sampling with unbiased estimation. We\ndevelop online, single-pass sampling algorithms and unbiased estimators for\ntemporal network sampling. The proposed algorithms enable fast, accurate, and\nmemory-efficient statistical estimation of temporal network patterns and\nproperties. In addition, we propose a temporally decaying sampling algorithm\nwith unbiased estimators for studying networks that evolve in continuous time,\nwhere the strength of links is a function of time, and the motif patterns are\ntemporally-weighted. In contrast to the prior notion of a $\\bigtriangleup\nt$-temporal motif, the proposed formulation and algorithms for counting\ntemporally weighted motifs are useful for forecasting tasks in networks such as\npredicting future links, or a future time-series variable of nodes and links.\nFinally, extensive experiments on a variety of temporal networks from different\ndomains demonstrate the effectiveness of the proposed algorithms. A detailed\nablation study is provided to understand the impact of the various components\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:10:18 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:47:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1910.08663", "submitter": "Kevin Hsieh", "authors": "Kevin Hsieh", "title": "Machine Learning Systems for Highly-Distributed and Rapidly-Growing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usability and practicality of any machine learning (ML) applications are\nlargely influenced by two critical but hard-to-attain factors: low latency and\nlow cost. Unfortunately, achieving low latency and low cost is very challenging\nwhen ML depends on real-world data that are highly distributed and rapidly\ngrowing (e.g., data collected by mobile phones and video cameras all over the\nworld). Such real-world data pose many challenges in communication and\ncomputation. For example, when training data are distributed across data\ncenters that span multiple continents, communication among data centers can\neasily overwhelm the limited wide-area network bandwidth, leading to\nprohibitively high latency and high cost.\n  In this dissertation, we demonstrate that the latency and cost of ML on\nhighly-distributed and rapidly-growing data can be improved by one to two\norders of magnitude by designing ML systems that exploit the characteristics of\nML algorithms, ML model structures, and ML training/serving data. We support\nthis thesis statement with three contributions. First, we design a system that\nprovides both low-latency and low-cost ML serving (inferencing) over\nlarge-scale and continuously-growing datasets, such as videos. Second, we build\na system that makes ML training over geo-distributed datasets as fast as\ntraining within a single data center. Third, we present a first detailed study\nand a system-level solution on a fundamental and largely overlooked problem: ML\ntraining over non-IID (i.e., not independent and identically distributed) data\npartitions (e.g., facial images collected by cameras varies according to the\ndemographics of each camera's location).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:59:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hsieh", "Kevin", ""]]}, {"id": "1910.08665", "submitter": "Abhishek Singh", "authors": "Abhishek Singh, Anubhav Garg, Jinan Zhou, Shiv Ram Dubey, Debo Dutta", "title": "NASIB: Neural Architecture Search withIn Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) represents a class of methods to generate\nthe optimal neural network architecture and typically iterate over candidate\narchitectures till convergence over some particular metric like validation\nloss. They are constrained by the available computation resources, especially\nin enterprise environments. In this paper, we propose a new approach for NAS,\ncalled NASIB, which adapts and attunes to the computation resources (budget)\navailable by varying the exploration vs. exploitation trade-off. We reduce the\nexpert bias by searching over an augmented search space induced by\nSuperkernels. The proposed method can provide the architecture search useful\nfor different computation resources and different domains beyond image\nclassification of natural images where we lack bespoke architecture motifs and\ndomain expertise. We show, on CIFAR10, that itis possible to search over a\nspace that comprises of 12x more candidate operations than the traditional\nprior art in just 1.5 GPU days, while reaching close to state of the art\naccuracy. While our method searches over an exponentially larger search space,\nit could lead to novel architectures that require lesser domain expertise,\ncompared to the majority of the existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 00:12:39 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Singh", "Abhishek", ""], ["Garg", "Anubhav", ""], ["Zhou", "Jinan", ""], ["Dubey", "Shiv Ram", ""], ["Dutta", "Debo", ""]]}, {"id": "1910.08670", "submitter": "Feras Batarseh", "authors": "Feras A. Batarseh and Ajay Kulkarni", "title": "Context-Driven Data Mining through Bias Removal and Data Incompleteness\n  Mitigation", "comments": "1st Workshop on Evaluation and Experimental Design in Data Mining and\n  Machine Learning (EDML 2019) At SIAM - Society for Industrial and Applied\n  Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of data mining endeavors are majorly driven by data quality.\nThroughout these deployments, serious show-stopper problems are still\nunresolved, such as: data collection ambiguities, data imbalance, hidden biases\nin data, the lack of domain information, and data incompleteness. This paper is\nbased on the premise that context can aid in mitigating these issues. In a\ntraditional data science lifecycle, context is not considered. Context-driven\nData Science Lifecycle (C-DSL); the main contribution of this paper, is\ndeveloped to address these challenges. Two case studies (using data-sets from\nsports events) are developed to test C-DSL. Results from both case studies are\nevaluated using common data mining metrics such as: coefficient of\ndetermination (R2 value) and confusion matrices. The work presented in this\npaper aims to re-define the lifecycle and introduce tangible improvements to\nits outcomes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 00:42:46 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Batarseh", "Feras A.", ""], ["Kulkarni", "Ajay", ""]]}, {"id": "1910.08693", "submitter": "Yunzong Xu", "authors": "Jinzhi Bu, David Simchi-Levi, Yunzong Xu", "title": "Online Pricing with Offline Data: Phase Transition and Inverse Square\n  Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the impact of pre-existing offline data on online\nlearning, in the context of dynamic pricing. We study a single-product dynamic\npricing problem over a selling horizon of $T$ periods. The demand in each\nperiod is determined by the price of the product according to a linear demand\nmodel with unknown parameters. We assume that before the start of the selling\nhorizon, the seller already has some pre-existing offline data. The offline\ndata set contains $n$ samples, each of which is an input-output pair consisting\nof a historical price and an associated demand observation. The seller wants to\nutilize both the pre-existing offline data and the sequential online data to\nminimize the regret of the online learning process.\n  We characterize the joint effect of the size, location and dispersion of the\noffline data on the optimal regret of the online learning process.\nSpecifically, the size, location and dispersion of the offline data are\nmeasured by the number of historical samples $n$, the absolute difference\nbetween the average historical price and the optimal price $\\delta$, and the\nstandard deviation of the historical prices $\\sigma$, respectively. We show\nthat the optimal regret is $\\widetilde \\Theta\\left(\\sqrt{T}\\wedge\n\\frac{T}{(n\\wedge T)\\delta^2+n\\sigma^2}\\right)$, and design a learning\nalgorithm based on the \"optimism in the face of uncertainty\" principle, whose\nregret is optimal up to a logarithmic factor. Our results reveal surprising\ntransformations of the optimal regret rate with respect to the size of the\noffline data, which we refer to as phase transitions. In addition, our results\ndemonstrate that the location and dispersion of the offline data also have an\nintrinsic effect on the optimal regret, and we quantify this effect via the\ninverse-square law.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:36:05 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 23:46:13 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 23:08:11 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 02:41:42 GMT"}, {"version": "v5", "created": "Sat, 2 May 2020 20:55:34 GMT"}, {"version": "v6", "created": "Thu, 20 Aug 2020 03:56:05 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bu", "Jinzhi", ""], ["Simchi-Levi", "David", ""], ["Xu", "Yunzong", ""]]}, {"id": "1910.08697", "submitter": "Chenfei Shi", "authors": "Chenfei Shi, Yan Xue, Chuan Jiang, Hui Tian, Bei Liu", "title": "Gastroscopic Panoramic View: Application to Automatic Polyps Detection\n  under Gastroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endoscopic diagnosis is an important means for gastric polyp detection. In\nthis paper, a panoramic image of gastroscopy is developed, which can display\nthe inner surface of the stomach intuitively and comprehensively. Moreover, the\nproposed automatic detection solution can help doctors locate the polyps\nautomatically, and reduce missed diagnosis. The main contributions of this\npaper are: firstly, a gastroscopic panorama reconstruction method is developed.\nThe reconstruction does not require additional hardware devices, and can solve\nthe problem of texture dislocation and illumination imbalance properly;\nsecondly, an end-to-end multi-object detection for gastroscopic panorama is\ntrained based on deep learning framework. Compared with traditional solutions,\nthe automatic polyp detection system can locate all polyps in the inner wall of\nstomach in real time and assist doctors to find the lesions. Thirdly, the\nsystem was evaluated in the Affiliated Hospital of Zhejiang University. The\nresults show that the average error of the panorama is less than 2 mm, the\naccuracy of the polyp detection is 95%, and the recall rate is 99%. In\naddition, the research roadmap of this paper has guiding significance for\nendoscopy-assisted detection of other human soft cavities.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 04:07:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shi", "Chenfei", ""], ["Xue", "Yan", ""], ["Jiang", "Chuan", ""], ["Tian", "Hui", ""], ["Liu", "Bei", ""]]}, {"id": "1910.08701", "submitter": "Mert G\\\"urb\\\"uzbalaban", "authors": "Alireza Fallah, Mert Gurbuzbalaban, Asuman Ozdaglar, Umut Simsekli,\n  Lingjiong Zhu", "title": "Robust Distributed Accelerated Stochastic Gradient Methods for\n  Multi-Agent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed stochastic gradient (D-SG) method and its accelerated\nvariant (D-ASG) for solving decentralized strongly convex stochastic\noptimization problems where the objective function is distributed over several\ncomputational units, lying on a fixed but arbitrary connected communication\ngraph, subject to local communication constraints where noisy estimates of the\ngradients are available. We develop a framework which allows to choose the\nstepsize and the momentum parameters of these algorithms in a way to optimize\nperformance by systematically trading off the bias, variance, robustness to\ngradient noise and dependence to network effects. When gradients do not contain\nnoise, we also prove that distributed accelerated methods can \\emph{achieve\nacceleration}, requiring $\\mathcal{O}(\\kappa \\log(1/\\varepsilon))$ gradient\nevaluations and $\\mathcal{O}(\\kappa \\log(1/\\varepsilon))$ communications to\nconverge to the same fixed point with the non-accelerated variant where\n$\\kappa$ is the condition number and $\\varepsilon$ is the target accuracy. To\nour knowledge, this is the first acceleration result where the iteration\ncomplexity scales with the square root of the condition number in the context\nof \\emph{primal} distributed inexact first-order methods. For quadratic\nfunctions, we also provide finer performance bounds that are tight with respect\nto bias and variance terms. Finally, we study a multistage version of D-ASG\nwith parameters carefully varied over stages to ensure exact\n$\\mathcal{O}(-k/\\sqrt{\\kappa})$ linear decay in the bias term as well as\noptimal $\\mathcal{O}(\\sigma^2/k)$ in the variance term. We illustrate through\nnumerical experiments that our approach results in practical algorithms that\nare robust to gradient noise and that can outperform existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 04:17:29 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:19:52 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 17:03:20 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Fallah", "Alireza", ""], ["Gurbuzbalaban", "Mert", ""], ["Ozdaglar", "Asuman", ""], ["Simsekli", "Umut", ""], ["Zhu", "Lingjiong", ""]]}, {"id": "1910.08707", "submitter": "Ibrahim Jubran", "authors": "Ibrahim Jubran, Alaa Maalouf, Dan Feldman", "title": "Introduction to Coresets: Accurate Coresets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coreset (or core-set) of an input set is its small summation, such that\nsolving a problem on the coreset as its input, provably yields the same result\nas solving the same problem on the original (full) set, for a given family of\nproblems (models, classifiers, loss functions). Over the past decade, coreset\nconstruction algorithms have been suggested for many fundamental problems in\ne.g. machine/deep learning, computer vision, graphics, databases, and\ntheoretical computer science. This introductory paper was written following\nrequests from (usually non-expert, but also colleagues) regarding the many\ninconsistent coreset definitions, lack of available source code, the required\ndeep theoretical background from different fields, and the dense papers that\nmake it hard for beginners to apply coresets and develop new ones.\n  The paper provides folklore, classic and simple results including\nstep-by-step proofs and figures, for the simplest (accurate) coresets of very\nbasic problems, such as: sum of vectors, minimum enclosing ball, SVD/ PCA and\nlinear regression. Nevertheless, we did not find most of their constructions in\nthe literature. Moreover, we expect that putting them together in a\nretrospective context would help the reader to grasp modern results that\nusually extend and generalize these fundamental observations. Experts might\nappreciate the unified notation and comparison table that links between\nexisting results.\n  Open source code with example scripts are provided for all the presented\nalgorithms, to demonstrate their practical usage, and to support the readers\nwho are more familiar with programming than math.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 05:45:36 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jubran", "Ibrahim", ""], ["Maalouf", "Alaa", ""], ["Feldman", "Dan", ""]]}, {"id": "1910.08719", "submitter": "Hareesh Kumar", "authors": "Hareesh Kumar, Priyanka Mary Mammen, Krithi Ramamritham", "title": "Explainable AI: Deep Reinforcement Learning Agents for Residential\n  Demand Side Cost Savings in Smart Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by recent advancements in Deep Reinforcement Learning (RL), we have\ndeveloped an RL agent to manage the operation of storage devices in a household\nand is designed to maximize demand-side cost savings. The proposed technique is\ndata-driven, and the RL agent learns from scratch how to efficiently use the\nenergy storage device given variable tariff structures. In most of the studies,\nthe RL agent is considered as a black box, and how the agent has learned is\noften ignored. We explain the learning progression of the RL agent, and the\nstrategies it follows based on the capacity of the storage device.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 07:57:54 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:39:15 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kumar", "Hareesh", ""], ["Mammen", "Priyanka Mary", ""], ["Ramamritham", "Krithi", ""]]}, {"id": "1910.08720", "submitter": "Dmitry Kopitkov", "authors": "Dmitry Kopitkov and Vadim Indelman", "title": "Neural Spectrum Alignment: Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressiveness and generalization of deep models was recently addressed via\nthe connection between neural networks (NNs) and kernel learning, where\nfirst-order dynamics of NN during a gradient-descent (GD) optimization were\nrelated to gradient similarity kernel, also known as Neural Tangent Kernel\n(NTK). In the majority of works this kernel is considered to be time-invariant,\nwith its properties being defined entirely by NN architecture and independent\nof the learning task at hand. In contrast, in this paper we empirically explore\nthese properties along the optimization and show that in practical applications\nthe NTK changes in a very dramatic and meaningful way, with its top\neigenfunctions aligning toward the target function learned by NN. Moreover,\nthese top eigenfunctions serve as basis functions for NN output - a function\nrepresented by NN is spanned almost completely by them for the entire\noptimization process. Further, since the learning along top eigenfunctions is\ntypically fast, their alignment with the target function improves the overall\noptimization performance. In addition, we study how the neural spectrum is\naffected by learning rate decay, typically done by practitioners, showing\nvarious trends in the kernel behavior. We argue that the presented phenomena\nmay lead to a more complete theoretical understanding behind NN learning.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 08:02:26 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 15:26:05 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 12:43:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kopitkov", "Dmitry", ""], ["Indelman", "Vadim", ""]]}, {"id": "1910.08778", "submitter": "Alex Markham", "authors": "Alex Markham and Moritz Grosse-Wentrup", "title": "Measurement Dependence Inducing Latent Causal Models", "comments": "10 pages, 5 figures; presented at UAI 2020; changes from previous\n  version: updated abstract, fixed errors due to TeX compilation of UAI notice\n  and page numbers in some references, added published proceedings reference", "journal-ref": "Proceedings of the 36th Conference on Uncertainty in Artificial\n  Intelligence (UAI), PMLR 124:590-599, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of causal structure learning over measurement dependence\ninducing latent (MeDIL) causal models. We show that this task can be framed in\nterms of the graph theoretic problem of finding edge clique covers,resulting in\nan algorithm for returning minimal MeDIL causal models (minMCMs). This\nalgorithm is non-parametric, requiring no assumptions about linearity or\nGaussianity. Furthermore, despite rather weak assumptions aboutthe class of\nMeDIL causal models, we show that minimality in minMCMs implies some rather\nspecific and interesting properties. By establishing MeDIL causal models as a\nsemantics for edge clique covers, we also provide a starting point for future\nwork further connecting causal structure learning to developments in graph\ntheory and network science.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 14:21:54 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:43:45 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 10:48:07 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Markham", "Alex", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1910.08780", "submitter": "Egor Rotinov", "authors": "Egor Rotinov", "title": "Reverse Experience Replay", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an improvement in Deep Q-learning called Reverse\nExperience Replay (also RER) that solves the problem of sparse rewards and\nhelps to deal with reward maximizing tasks by sampling transitions successively\nin reverse order. On tasks with enough experience for training and enough\nExperience Replay memory capacity, Deep Q-learning Network with Reverse\nExperience Replay shows competitive results against both Double DQN, with a\nstandard Experience Replay, and vanilla DQN. Also, RER achieves significantly\nincreased results in tasks with a lack of experience and Replay memory\ncapacity.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 14:37:13 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 18:13:59 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Rotinov", "Egor", ""]]}, {"id": "1910.08795", "submitter": "Ekhine Irurozki", "authors": "Ekhine Irurozki, Jesus Lobo, Aritz Perez, Javier Del Ser", "title": "Rank aggregation for non-stationary data streams", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning over non-stationary ranking streams. The\nrankings can be interpreted as the preferences of a population and the\nnon-stationarity means that the distribution of preferences changes over time.\nOur goal is to learn, in an online manner, the current distribution of\nrankings. The bottleneck of this process is a rank aggregation problem.\n  We propose a generalization of the Borda algorithm for non-stationary ranking\nstreams. Moreover, we give bounds on the minimum number of samples required to\noutput the ground truth with high probability. Besides, we show how the optimal\nparameters are set. Then, we generalize the whole family of weighted voting\nrules (the family to which Borda belongs) to situations in which some rankings\nare more \\textit{reliable} than others and show that this generalization can\nsolve the problem of rank aggregation over non-stationary data streams.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 16:08:01 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 07:30:00 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 13:51:34 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Irurozki", "Ekhine", ""], ["Lobo", "Jesus", ""], ["Perez", "Aritz", ""], ["Del Ser", "Javier", ""]]}, {"id": "1910.08800", "submitter": "Etor Arza Gonzalez", "authors": "Etor Arza, Aritz Perez, Ekhine Irurozki, Josu Ceberio", "title": "Kernels of Mallows Models under the Hamming Distance for solving the\n  Quadratic Assignment Problem", "comments": "23 pages", "journal-ref": null, "doi": "10.1016/j.swevo.2020.100740", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Assignment Problem (QAP) is a well-known permutation-based\ncombinatorial optimization problem with real applications in industrial and\nlogistics environments. Motivated by the challenge that this NP-hard problem\nrepresents, it has captured the attention of the optimization community for\ndecades. As a result, a large number of algorithms have been proposed to tackle\nthis problem. Among these, exact methods are only able to solve instances of\nsize $n<40$. To overcome this limitation, many metaheuristic methods have been\napplied to the QAP.\n  In this work, we follow this direction by approaching the QAP through\nEstimation of Distribution Algorithms (EDAs). Particularly, a non-parametric\ndistance-based exponential probabilistic model is used. Based on the analysis\nof the characteristics of the QAP, and previous work in the area, we introduce\nKernels of Mallows Model under the Hamming distance to the context of EDAs.\nConducted experiments point out that the performance of the proposed algorithm\nin the QAP is superior to (i) the classical EDAs adapted to deal with the QAP,\nand also (ii) to the specific EDAs proposed in the literature to deal with\npermutation problems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 16:25:41 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 12:37:34 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 17:36:26 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Arza", "Etor", ""], ["Perez", "Aritz", ""], ["Irurozki", "Ekhine", ""], ["Ceberio", "Josu", ""]]}, {"id": "1910.08805", "submitter": "Siddharth Mitra", "authors": "Siddharth Mitra and Aditya Gopalan", "title": "On Adaptivity in Information-constrained Online Learning", "comments": "34th AAAI Conference on Artificial Intelligence (AAAI 2020). Short\n  version at 11th Optimization for Machine Learning workshop (OPT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to adapt to smoothly-varying ('easy') environments in well-known\nonline learning problems where acquiring information is expensive. For the\nproblem of label efficient prediction, which is a budgeted version of\nprediction with expert advice, we present an online algorithm whose regret\ndepends optimally on the number of labels allowed and $Q^*$ (the quadratic\nvariation of the losses of the best action in hindsight), along with a\nparameter-free counterpart whose regret depends optimally on $Q$ (the quadratic\nvariation of the losses of all the actions). These quantities can be\nsignificantly smaller than $T$ (the total time horizon), yielding an\nimprovement over existing, variation-independent results for the problem. We\nthen extend our analysis to handle label efficient prediction with bandit\nfeedback, i.e., label efficient bandits. Our work builds upon the framework of\noptimistic online mirror descent, and leverages second order corrections along\nwith a carefully designed hybrid regularizer that encodes the constrained\ninformation structure of the problem. We then consider revealing action-partial\nmonitoring games -- a version of label efficient prediction with additive\ninformation costs, which in general are known to lie in the \\textit{hard} class\nof games having minimax regret of order $T^{\\frac{2}{3}}$. We provide a\nstrategy with an $\\mathcal{O}((Q^*T)^{\\frac{1}{3}})$ bound for revealing action\ngames, along with one with a $\\mathcal{O}((QT)^{\\frac{1}{3}})$ bound for the\nfull class of hard partial monitoring games, both being strict improvements\nover current bounds.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 16:53:59 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 07:32:06 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mitra", "Siddharth", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1910.08809", "submitter": "Nicolas Carion", "authors": "Nicolas Carion, Gabriel Synnaeve, Alessandro Lazaric, Nicolas Usunier", "title": "A Structured Prediction Approach for Generalization in Cooperative\n  Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective coordination is crucial to solve multi-agent collaborative (MAC)\nproblems. While centralized reinforcement learning methods can optimally solve\nsmall MAC instances, they do not scale to large problems and they fail to\ngeneralize to scenarios different from those seen during training. In this\npaper, we consider MAC problems with some intrinsic notion of locality (e.g.,\ngeographic proximity) such that interactions between agents and tasks are\nlocally limited. By leveraging this property, we introduce a novel structured\nprediction approach to assign agents to tasks. At each step, the assignment is\nobtained by solving a centralized optimization problem (the inference\nprocedure) whose objective function is parameterized by a learned scoring\nmodel. We propose different combinations of inference procedures and scoring\nmodels able to represent coordination patterns of increasing complexity. The\nresulting assignment policy can be efficiently learned on small problem\ninstances and readily reused in problems with more agents and tasks (i.e.,\nzero-shot generalization). We report experimental results on a toy search and\nrescue problem and on several target selection scenarios in StarCraft: Brood\nWar, in which our model significantly outperforms strong rule-based baselines\non instances with 5 times more agents and tasks than those seen during\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 17:30:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Carion", "Nicolas", ""], ["Synnaeve", "Gabriel", ""], ["Lazaric", "Alessandro", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1910.08828", "submitter": "Mohammed Rayyan Sheriff", "authors": "Mohammed Rayyan Sheriff and Debasish Chatterjee", "title": "Dictionary Learning with Almost Sure Error Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dictionary is a database of standard vectors, so that other vectors /\nsignals are expressed as linear combinations of dictionary vectors, and the\ntask of learning a dictionary for a given data is to find a good dictionary so\nthat the representation of data points has desirable features. Dictionary\nlearning and the related matrix factorization methods have gained significant\nprominence recently due to their applications in Wide variety of fields like\nmachine learning, signal processing, statistics etc. In this article we study\nthe dictionary learning problem for achieving desirable features in the\nrepresentation of a given data with almost sure recovery constraints. We impose\nthe constraint that every sample is reconstructed properly to within a\npredefined threshold. This problem formulation is more challenging than the\nconventional dictionary learning, which is done by minimizing a regularised\ncost function. We make use of the duality results for linear inverse problems\nto obtain an equivalent reformulation in the form of a convex-concave min-max\nproblem. The resulting min-max problem is then solved using gradient\ndescent-ascent like algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 19:34:00 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 14:24:21 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 21:58:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sheriff", "Mohammed Rayyan", ""], ["Chatterjee", "Debasish", ""]]}, {"id": "1910.08842", "submitter": "Neel Guha", "authors": "Neel Guha, Zhecheng Wang, Matt Wytock, Arun Majumdar", "title": "Machine Learning for AC Optimal Power Flow", "comments": "3 pages, 2 tables. Presented at the Climate Change Workshop at ICML\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore machine learning methods for AC Optimal Powerflow (ACOPF) - the\ntask of optimizing power generation in a transmission network according while\nrespecting physical and engineering constraints. We present two formulations of\nACOPF as a machine learning problem: 1) an end-to-end prediction task where we\ndirectly predict the optimal generator settings, and 2) a constraint prediction\ntask where we predict the set of active constraints in the optimal solution. We\nvalidate these approaches on two benchmark grids.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 20:47:13 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Guha", "Neel", ""], ["Wang", "Zhecheng", ""], ["Wytock", "Matt", ""], ["Majumdar", "Arun", ""]]}, {"id": "1910.08862", "submitter": "Du Xu", "authors": "Di Xu, Tianhang Long, Junbin Gao", "title": "LSTM-Assisted Evolutionary Self-Expressive Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive volumes of high-dimensional data that evolves over time is\ncontinuously collected by contemporary information processing systems, which\nbrings up the problem of organizing this data into clusters, i.e. achieve the\npurpose of dimensional deduction, and meanwhile learning its temporal evolution\npatterns. In this paper, a framework for evolutionary subspace clustering,\nreferred to as LSTM-ESCM, is introduced, which aims at clustering a set of\nevolving high-dimensional data points that lie in a union of low-dimensional\nevolving subspaces. In order to obtain the parsimonious data representation at\neach time step, we propose to exploit the so-called self-expressive trait of\nthe data at each time point. At the same time, LSTM networks are implemented to\nextract the inherited temporal patterns behind data in an overall time frame.\nAn efficient algorithm has been proposed based on MATLAB. Next, experiments are\ncarried out on real-world datasets to demonstrate the effectiveness of our\nproposed approach. And the results show that the suggested algorithm\ndramatically outperforms other known similar approaches in terms of both run\ntime and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 23:58:17 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Di", ""], ["Long", "Tianhang", ""], ["Gao", "Junbin", ""]]}, {"id": "1910.08864", "submitter": "Yu Chen", "authors": "Yu Chen, Yuanyuan Yang, Yaochu Jin, Xiufen Zou", "title": "Identification of Interaction Clusters Using a Semi-supervised\n  Hierarchical Clustering Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Identifying interaction clusters of large gene regulatory\nnetworks (GRNs) is critical for its further investigation, while this task is\nvery challenging, attributed to data noise in experiment data, large scale of\nGRNs, and inconsistency between gene expression profiles and function modules,\netc. It is promising to semi-supervise this process by prior information, but\nshortage of prior information sometimes make it very challenging. Meanwhile, it\nis also annoying, and sometimes impossible to discovery gold standard for\nevaluation of clustering results.\\\\ Results: With assistance of an online\nenrichment tool, this research proposes a semi-supervised hierarchical\nclustering method via deconvolved correlation matrix~(SHC-DC) to discover\ninteraction clusters of large-scale GRNs. Three benchmark networks including a\n\\emph{Ecoli} network and two \\emph{Yeast} networks are employed to test\nsemi-supervision scheme of the proposed method. Then, SHC-DC is utilized to\ncluster genes in sleep study. Results demonstrates it can find interaction\nmodules that are generally enriched in various signal pathways. Besides the\nsignificant influence on blood level of interleukins, impact of sleep on\nimportant pathways mediated by them is also validated by the discovered\ninteraction modules.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 00:36:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chen", "Yu", ""], ["Yang", "Yuanyuan", ""], ["Jin", "Yaochu", ""], ["Zou", "Xiufen", ""]]}, {"id": "1910.08877", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Targeted Estimation of Heterogeneous Treatment Effect in Observational\n  Survival Analysis", "comments": null, "journal-ref": "j.jbi.2020.103474", "doi": "10.1016/j.jbi.2020.103474", "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The aim of clinical effectiveness research using repositories of electronic\nhealth records is to identify what health interventions 'work best' in\nreal-world settings. Since there are several reasons why the net benefit of\nintervention may differ across patients, current comparative effectiveness\nliterature focuses on investigating heterogeneous treatment effect and\npredicting whether an individual might benefit from an intervention. The\nmajority of this literature has concentrated on the estimation of the effect of\ntreatment on binary outcomes. However, many medical interventions are evaluated\nin terms of their effect on future events, which are subject to loss to\nfollow-up. In this study, we describe a framework for the estimation of\nheterogeneous treatment effect in terms of differences in time-to-event\n(survival) probabilities. We divide the problem into three phases: (1)\nestimation of treatment effect conditioned on unique sets of the covariate\nvector; (2) identification of features important for heterogeneity using an\nensemble of non-parametric variable importance methods; and (3) estimation of\ntreatment effect on the reference classes defined by the previously selected\nfeatures, using one-step Targeted Maximum Likelihood Estimation. We conducted a\nseries of simulation studies and found that this method performs well when\neither sample size or event rate is high enough and the number of covariates\ncontributing to the effect heterogeneity is moderate. An application of this\nmethod to a clinical case study was conducted by estimating the effect of oral\nanticoagulants on newly diagnosed non-valvular atrial fibrillation patients\nusing data from the UK Clinical Practice Research Datalink.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:27:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:31:03 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "1910.08880", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu", "title": "Sparse (group) learning with Lipschitz loss functions: a unified\n  analysis", "comments": "arXiv admin note: text overlap with arXiv:1810.03081", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of sparse estimators defined as minimizers of some\nempirical Lipschitz loss function---which include hinge, logistic and quantile\nregression losses---with a convex, sparse or group-sparse regularization. In\nparticular, we consider the L1-norm on the coefficients, its sorted Slope\nversion, and the Group L1-L2 extension. First, we propose a theoretical\nframework which simultaneously derives new L2 estimation upper bounds for all\nthree regularization schemes. For L1 and Slope regularizations, our bounds\nscale as $(k^*/n) \\log(p/k^*)$---$n\\times p$ is the size of the design matrix\nand $k^*$ the dimension of the theoretical loss minimizer $\\beta^*$---matching\nthe optimal minimax rate achieved for the least-squares case. For Group L1-L2\nregularization, our bounds scale as $(s^*/n) \\log\\left( G / s^* \\right) + m^* /\nn$---$G$ is the total number of groups and $m^*$ the number of coefficients in\nthe $s^*$ groups which contain $\\beta^*$---and improve over the least-squares\ncase. We additionally show that when the signal is strongly group-sparse Group\nL1-L2 is superior to L1 and Slope. Our bounds are achieved both in probability\nand in expectation, under common assumptions in the literature. Second, we\npropose an accelerated proximal algorithm which computes the convex estimators\nstudied when the number of variables is of the order of $100,000$. We\nadditionally compare their statistical performance of our estimators against\nstandard baselines for settings where the signal is either sparse or\ngroup-sparse. Our experiments findings reveal (i) the good empirical\nperformance of L1 and Slope regularizations for sparse binary classification\nproblems, (ii) the superiority of Group L1-L2 regularization for group-sparse\nclassification problems and (iii) the appealing properties of sparse quantile\nregression estimators for sparse regression problems with heteroscedastic\nnoise.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:56:37 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 04:02:41 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 18:54:50 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 19:15:25 GMT"}, {"version": "v5", "created": "Wed, 11 Dec 2019 18:04:59 GMT"}, {"version": "v6", "created": "Fri, 13 Dec 2019 23:49:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Dedieu", "Antoine", ""]]}, {"id": "1910.08883", "submitter": "Sambit Panda", "authors": "Sambit Panda, Cencheng Shen, Ronan Perry, Jelle Zorn, Antoine Lutz,\n  Carey E. Priebe, Joshua T. Vogelstein", "title": "Nonpar MANOVA via Independence Testing", "comments": "15 pages main + 4 pages appendix, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $k$-sample testing problem tests whether or not $k$ groups of data points\nare sampled from the same distribution. Multivariate analysis of variance\n(MANOVA) is currently the gold standard for $k$-sample testing but makes\nstrong, often inappropriate, parametric assumptions. Moreover, independence\ntesting and $k$-sample testing are tightly related, and there are many\nnonparametric multivariate independence tests with strong theoretical and\nempirical properties, including distance correlation (Dcorr) and\nHilbert-Schmidt-Independence-Criterion (Hsic). We prove that universally\nconsistent independence tests achieve universally consistent $k$-sample testing\nand that $k$-sample statistics like Energy and Maximum Mean Discrepancy (MMD)\nare exactly equivalent to Dcorr. Empirically evaluating these tests for\n$k$-sample scenarios demonstrates that these nonparametric independence tests\ntypically outperform MANOVA, even for Gaussian distributed settings. Finally,\nwe extend these non-parametric $k$-sample testing procedures to perform\nmultiway and multilevel tests. Thus, we illustrate the existence of many\ntheoretically motivated and empirically performant $k$-sample tests. A Python\npackage with all independence and k-sample tests called hyppo is available from\nhttps://hyppo.neurodata.io/.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 03:14:20 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 23:54:01 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 01:35:09 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Panda", "Sambit", ""], ["Shen", "Cencheng", ""], ["Perry", "Ronan", ""], ["Zorn", "Jelle", ""], ["Lutz", "Antoine", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1910.08902", "submitter": "Oluwaseyi Feyisetan", "authors": "Oluwaseyi Feyisetan and Borja Balle and Thomas Drake and Tom Diethe", "title": "Privacy- and Utility-Preserving Textual Analysis via Calibrated\n  Multivariate Perturbations", "comments": "Accepted at WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately learning from user data while providing quantifiable privacy\nguarantees provides an opportunity to build better ML models while maintaining\nuser trust. This paper presents a formal approach to carrying out privacy\npreserving text perturbation using the notion of dx-privacy designed to achieve\ngeo-indistinguishability in location data. Our approach applies carefully\ncalibrated noise to vector representation of words in a high dimension space as\ndefined by word embedding models. We present a privacy proof that satisfies\ndx-privacy where the privacy parameter epsilon provides guarantees with respect\nto a distance metric defined by the word embedding space. We demonstrate how\nepsilon can be selected by analyzing plausible deniability statistics backed up\nby large scale analysis on GloVe and fastText embeddings. We conduct privacy\naudit experiments against 2 baseline models and utility experiments on 3\ndatasets to demonstrate the tradeoff between privacy and utility for varying\nvalues of epsilon on different task types. Our results demonstrate practical\nutility (< 2% utility loss for training binary classifiers) while providing\nbetter privacy guarantees than baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:12:23 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Feyisetan", "Oluwaseyi", ""], ["Balle", "Borja", ""], ["Drake", "Thomas", ""], ["Diethe", "Tom", ""]]}, {"id": "1910.08903", "submitter": "Anubhav Jain", "authors": "Anubhav Jain, Avdesh Kumar, Saumya Balodi, Pravesh Biyani", "title": "Benchmark Dataset for Timetable Optimization of Bus Routes in the City\n  of New Delhi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public transport is one of the major forms of transportation in the world.\nThis makes it vital to ensure that public transport is efficient. This research\npresents a novel real-time GPS bus transit data for over 500 routes of buses\noperating in New Delhi. The data can be used for modeling various timetable\noptimization tasks as well as in other domains such as traffic management,\ntravel time estimation, etc. The paper also presents an approach to reduce the\nwaiting time of Delhi buses by analyzing the traffic behavior and proposing a\ntimetable. This algorithm serves as a benchmark for the dataset. The algorithm\nuses a constrained clustering algorithm for classification of trips. It further\nanalyses the data statistically to provide a timetable which is efficient in\nlearning the inter- and intra-month variations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:22:47 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jain", "Anubhav", ""], ["Kumar", "Avdesh", ""], ["Balodi", "Saumya", ""], ["Biyani", "Pravesh", ""]]}, {"id": "1910.08904", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "$hv$-Block Cross Validation is not a BIBD: a Note on the Paper by Jeff\n  Racine (2000)", "comments": "Technique report. 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note corrects a mistake in the paper \"consistent cross-validatory\nmodel-selection for dependent data: $hv$-block cross-validation\" by Racine\n(2000). In his paper, he implied that the therein proposed $hv$-block\ncross-validation is consistent in the sense of Shao (1993). To get this\nintuition, he relied on the speculation that $hv$-block is a balanced\nincomplete block design (BIBD). This note demonstrates that this is not the\ncase, and thus the theoretical consistency of $hv$-block remains an open\nquestion. In addition, I also provide a Python program counting the number of\noccurrences of each sample and each pair of samples.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:27:10 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1910.08909", "submitter": "Shuai Yang", "authors": "Shuai Yang, Wenqi Zhu, Yuesheng Zhu", "title": "Sparse-Dense Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering high-dimensional data\ninto a union of low-dimensional subspaces. Current subspace clustering\napproaches are usually based on a two-stage framework. In the first stage, an\naffinity matrix is generated from data. In the second one, spectral clustering\nis applied on the affinity matrix. However, the affinity matrix produced by\ntwo-stage methods cannot fully reveal the similarity between data points from\nthe same subspace (intra-subspace similarity), resulting in inaccurate\nclustering. Besides, most approaches fail to solve large-scale clustering\nproblems due to poor efficiency. In this paper, we first propose a new scalable\nsparse method called Iterative Maximum Correlation (IMC) to learn the affinity\nmatrix from data. Then we develop Piecewise Correlation Estimation (PCE) to\ndensify the intra-subspace similarity produced by IMC. Finally we extend our\nwork into a Sparse-Dense Subspace Clustering (SDSC) framework with a dense\nstage to optimize the affinity matrix for two-stage methods. We show that IMC\nis efficient when clustering large-scale data, and PCE ensures better\nperformance for IMC. We show the universality of our SDSC framework as well.\nExperiments on several data sets demonstrate the effectiveness of our\napproaches. Moreover, we are the first one to apply densification on affinity\nmatrix before spectral clustering, and SDSC constitutes the first attempt to\nbuild a universal three-stage subspace clustering framework.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 06:35:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yang", "Shuai", ""], ["Zhu", "Wenqi", ""], ["Zhu", "Yuesheng", ""]]}, {"id": "1910.08917", "submitter": "Oluwaseyi Feyisetan", "authors": "Oluwaseyi Feyisetan and Tom Diethe and Thomas Drake", "title": "Leveraging Hierarchical Representations for Preserving Privacy and\n  Utility in Text", "comments": "Accepted at ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guaranteeing a certain level of user privacy in an arbitrary piece of text is\na challenging issue. However, with this challenge comes the potential of\nunlocking access to vast data stores for training machine learning models and\nsupporting data driven decisions. We address this problem through the lens of\ndx-privacy, a generalization of Differential Privacy to non Hamming distance\nmetrics. In this work, we explore word representations in Hyperbolic space as a\nmeans of preserving privacy in text. We provide a proof satisfying dx-privacy,\nthen we define a probability distribution in Hyperbolic space and describe a\nway to sample from it in high dimensions. Privacy is provided by perturbing\nvector representations of words in high dimensional Hyperbolic space to obtain\na semantic generalization. We conduct a series of experiments to demonstrate\nthe tradeoff between privacy and utility. Our privacy experiments illustrate\nprotections against an authorship attribution algorithm while our utility\nexperiments highlight the minimal impact of our perturbations on several\ndownstream machine learning models. Compared to the Euclidean baseline, we\nobserve > 20x greater guarantees on expected privacy against comparable worst\ncase statistics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 07:16:29 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Feyisetan", "Oluwaseyi", ""], ["Diethe", "Tom", ""], ["Drake", "Thomas", ""]]}, {"id": "1910.08922", "submitter": "Surya Karthik Mukkavilli", "authors": "Yimeng Min, S. Karthik Mukkavilli, Yoshua Bengio", "title": "Predicting ice flow using machine learning", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS),\n  Workshop on Tackling Climate Change with Machine Learning, Vancouver, Canada,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though machine learning has achieved notable success in modeling sequential\nand spatial data for speech recognition and in computer vision, applications to\nremote sensing and climate science problems are seldom considered. In this\npaper, we demonstrate techniques from unsupervised learning of future video\nframe prediction, to increase the accuracy of ice flow tracking in\nmulti-spectral satellite images. As the volume of cryosphere data increases in\ncoming years, this is an interesting and important opportunity for machine\nlearning to address a global challenge for climate change, risk management from\nfloods, and conserving freshwater resources. Future frame prediction of ice\nmelt and tracking the optical flow of ice dynamics presents modeling\ndifficulties, due to uncertainties in global temperature increase, changing\nprecipitation patterns, occlusion from cloud cover, rapid melting and glacier\nretreat due to black carbon aerosol deposition, from wildfires or human fossil\nemissions. We show the adversarial learning method helps improve the accuracy\nof tracking the optical flow of ice dynamics compared to existing methods in\nclimate science. We present a dataset, IceNet, to encourage machine learning\nresearch and to help facilitate further applications in the areas of\ncryospheric science and climate change.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 07:56:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Min", "Yimeng", ""], ["Mukkavilli", "S. Karthik", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1910.08926", "submitter": "Mohamed Karim Belaid", "authors": "Van Bach Nguyen, Belaid Mohamed Karim, Bao Long Vu, J\\\"org\n  Schl\\\"otterer, Michael Granitzer", "title": "Policy Learning for Malaria Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sequential decision making is a typical problem in reinforcement learning\nwith plenty of algorithms to solve it. However, only a few of them can work\neffectively with a very small number of observations. In this report, we\nintroduce the progress to learn the policy for Malaria Control as a\nReinforcement Learning problem in the KDD Cup Challenge 2019 and propose\ndiverse solutions to deal with the limited observations problem. We apply the\nGenetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking to\nfind the optimal policy for five years in a row with only 20 episodes/100\nevaluations. We evaluate those algorithms and compare their performance with\nRandom Search as a baseline. Among these algorithms, Q-Learning with sequence\nbreaking has been submitted to the challenge and got ranked 7th in KDD Cup.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 08:19:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Nguyen", "Van Bach", ""], ["Karim", "Belaid Mohamed", ""], ["Vu", "Bao Long", ""], ["Schl\u00f6tterer", "J\u00f6rg", ""], ["Granitzer", "Michael", ""]]}, {"id": "1910.08945", "submitter": "Guokun Chi", "authors": "Guokun Chi and Min Jiang and Xing Gao and Weizhen Hu and Shihui Guo\n  and Kay Chen Tan", "title": "Online Bagging for Anytime Transfer Learning", "comments": "7 pages; SSCI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning techniques have been widely used in the reality that it is\ndifficult to obtain sufficient labeled data in the target domain, but a large\namount of auxiliary data can be obtained in the relevant source domain. But\nmost of the existing methods are based on offline data. In practical\napplications, it is often necessary to face online learning problems in which\nthe data samples are achieved sequentially. In this paper, We are committed to\napplying the ensemble approach to solving the problem of online transfer\nlearning so that it can be used in anytime setting. More specifically, we\npropose a novel online transfer learning framework, which applies the idea of\nonline bagging methods to anytime transfer learning problems, and constructs\nstrong classifiers through online iterations of the usefulness of multiple weak\nclassifiers. Further, our algorithm also provides two extension schemes to\nreduce the impact of negative transfer. Experiments on three real data sets\nshow that the effectiveness of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 10:25:03 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chi", "Guokun", ""], ["Jiang", "Min", ""], ["Gao", "Xing", ""], ["Hu", "Weizhen", ""], ["Guo", "Shihui", ""], ["Tan", "Kay Chen", ""]]}, {"id": "1910.08962", "submitter": "Samuel M\\\"uller", "authors": "Samuel M\\\"uller and Andreas Vlachos", "title": "Byte-Pair Encoding for Text-to-SQL Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence models provide a competitive approach to the task\nof mapping a question in natural language to an SQL query, also referred to as\ntext-to-SQL generation. The Byte-Pair Encoding algorithm (BPE) has previously\nbeen used to improve machine translation (MT) between natural languages. In\nthis work, we adapt BPE for text-to-SQL generation. As the datasets for this\ntask are rather small compared to MT, we present a novel stopping criterion\nthat prevents overfitting the BPE encoding to the training set. Additionally,\nwe present AST BPE, which is a version of BPE that uses the Abstract Syntax\nTree (AST) of the SQL statement to guide BPE merges and therefore produce BPE\nencodings that generalize better. We improved the accuracy of a strong\nattentive seq2seq baseline on five out of six English text-to-SQL tasks while\nreducing training time by more than 50% on four of them due to the shortened\ntargets. Finally, on two of these tasks we exceeded previously reported\naccuracies.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 12:32:20 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 12:02:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["M\u00fcller", "Samuel", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1910.08964", "submitter": "Fabio Massimo Zennaro", "authors": "Fabio Massimo Zennaro, Ke Chen", "title": "Towards Further Understanding of Sparse Filtering via Information\n  Bottleneck", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a formalization of feature distribution learning\n(FDL) in information-theoretic terms relying on the analytical approach and on\nthe tools already used in the study of the information bottleneck (IB). It has\nbeen conjectured that the behavior of FDL algorithms could be expressed as an\noptimization problem over two information-theoretic quantities: the mutual\ninformation of the data with the learned representations and the entropy of the\nlearned distribution. In particular, such a formulation was offered in order to\nexplain the success of the most prominent FDL algorithm, sparse filtering (SF).\nThis conjecture was, however, left unproven. In this work, we aim at providing\npreliminary empirical support to this conjecture by performing experiments\nreminiscent of the work done on deep neural networks in the context of the IB\nresearch. Specifically, we borrow the idea of using information planes to\nanalyze the behavior of the SF algorithm and gain insights on its dynamics. A\nconfirmation of the conjecture about the dynamics of FDL may provide solid\nground to develop information-theoretic tools to assess the quality of the\nlearning process in FDL, and it may be extended to other unsupervised learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 12:56:17 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zennaro", "Fabio Massimo", ""], ["Chen", "Ke", ""]]}, {"id": "1910.08965", "submitter": "Ningshan Zhang", "authors": "Ben Adlam, Corinna Cortes, Mehryar Mohri, Ningshan Zhang", "title": "Learning GANs and Ensembles Using Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) generate data based on minimizing a\ndivergence between two distributions. The choice of that divergence is\ntherefore critical. We argue that the divergence must take into account the\nhypothesis set and the loss function used in a subsequent learning task, where\nthe data generated by a GAN serves for training. Taking that structural\ninformation into account is also important to derive generalization guarantees.\nThus, we propose to use the discrepancy measure, which was originally\nintroduced for the closely related problem of domain adaptation and which\nprecisely takes into account the hypothesis set and the loss function. We show\nthat discrepancy admits favorable properties for training GANs and prove\nexplicit generalization guarantees. We present efficient algorithms using\ndiscrepancy for two tasks: training a GAN directly, namely DGAN, and mixing\npreviously trained generative models, namely EDGAN. Our experiments on toy\nexamples and several benchmark datasets show that DGAN is competitive with\nother GANs and that EDGAN outperforms existing GAN ensembles, such as AdaGAN.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 13:01:26 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:10:05 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Adlam", "Ben", ""], ["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Zhang", "Ningshan", ""]]}, {"id": "1910.08967", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Claudiu Ardei, Radu Tudor Ionescu, Marius Leordeanu", "title": "Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)", "comments": "Accepted at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant advances in recent years, Generative Adversarial\nNetworks (GANs) are still notoriously hard to train. In this paper, we propose\nthree novel curriculum learning strategies for training GANs. All strategies\nare first based on ranking the training images by their difficulty scores,\nwhich are estimated by a state-of-the-art image difficulty predictor. Our first\nstrategy is to divide images into gradually more difficult batches. Our second\nstrategy introduces a novel curriculum loss function for the discriminator that\ntakes into account the difficulty scores of the real images. Our third strategy\nis based on sampling from an evolving distribution, which favors the easier\nimages during the initial training stages and gradually converges to a uniform\ndistribution, in which samples are equally likely, regardless of difficulty. We\ncompare our curriculum learning strategies with the classic training procedure\non two tasks: image generation and image translation. Our experiments indicate\nthat all strategies provide faster convergence and superior results. For\nexample, our best curriculum learning strategy applied on spectrally normalized\nGANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like\nimages are real in 25.0% of the presented cases, while the SNGANs trained using\nthe classic procedure fooled the annotators in only 18.4% cases. Similarly, in\nimage translation, the human annotators preferred the images produced by the\nCycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5%\ncases and those produced by CycleGAN based on classic training in only 19.8%\ncases, 39.7% cases being labeled as ties.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 13:06:26 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:19:01 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Soviany", "Petru", ""], ["Ardei", "Claudiu", ""], ["Ionescu", "Radu Tudor", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1910.08974", "submitter": "Nan Lu", "authors": "Nan Lu, Tianyi Zhang, Gang Niu, Masashi Sugiyama", "title": "Mitigating Overfitting in Supervised Classification from Two Unlabeled\n  Datasets: A Consistent Risk Correction Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed unlabeled-unlabeled (UU) classification method allows\nus to train a binary classifier only from two unlabeled datasets with different\nclass priors. Since this method is based on the empirical risk minimization, it\nworks as if it is a supervised classification method, compatible with any model\nand optimizer. However, this method sometimes suffers from severe overfitting,\nwhich we would like to prevent in this paper. Our empirical finding in applying\nthe original UU method is that overfitting often co-occurs with the empirical\nrisk going negative, which is not legitimate. Therefore, we propose to wrap the\nterms that cause a negative empirical risk by certain correction functions.\nThen, we prove the consistency of the corrected risk estimator and derive an\nestimation error bound for the corrected risk minimizer. Experiments show that\nour proposal can successfully mitigate overfitting of the UU method and\nsignificantly improve the classification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 13:23:19 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 05:27:12 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 01:31:10 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 10:21:08 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lu", "Nan", ""], ["Zhang", "Tianyi", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.08978", "submitter": "Aleksandar Vakanski", "authors": "Aleksandar Vakanski, Min Xian, Phoebe Freer", "title": "Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n  Ultrasound Images", "comments": "16 pages, 5 figures", "journal-ref": "Ultrasound in Medicine and Biology, volume 46, issue 10, pages\n  2819-2833, 2020", "doi": "10.1016/j.ultrasmedbio.2020.06.015", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating human domain knowledge for breast tumor diagnosis is\nchallenging, since shape, boundary, curvature, intensity, or other common\nmedical priors vary significantly across patients and cannot be employed. This\nwork proposes a new approach for integrating visual saliency into a deep\nlearning model for breast tumor segmentation in ultrasound images. Visual\nsaliency refers to image maps containing regions that are more likely to\nattract radiologists visual attention. The proposed approach introduces\nattention blocks into a U-Net architecture, and learns feature representations\nthat prioritize spatial regions with high saliency levels. The validation\nresults demonstrate increased accuracy for tumor segmentation relative to\nmodels without salient attention layers. The approach achieved a Dice\nsimilarity coefficient of 90.5 percent on a dataset of 510 images. The salient\nattention model has potential to enhance accuracy and robustness in processing\nmedical images of other organs, by providing a means to incorporate\ntask-specific knowledge into deep learning architectures.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 13:36:33 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 02:55:49 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Vakanski", "Aleksandar", ""], ["Xian", "Min", ""], ["Freer", "Phoebe", ""]]}, {"id": "1910.09014", "submitter": "Daniel Irving Bernstein", "authors": "Daniel Irving Bernstein, Basil Saeed, Chandler Squires, Caroline Uhler", "title": "Ordering-Based Causal Structure Learning in the Presence of Latent\n  Variables", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a causal graph in the presence of latent\nconfounders given i.i.d.~samples from the model. While current algorithms for\ncausal structure discovery in the presence of latent confounders are\nconstraint-based, we here propose a score-based approach. We prove that under\nassumptions weaker than faithfulness, any sparsest independence map (IMAP) of\nthe distribution belongs to the Markov equivalence class of the true model.\nThis motivates the \\emph{Sparsest Poset} formulation - that posets can be\nmapped to minimal IMAPs of the true model such that the sparsest of these IMAPs\nis Markov equivalent to the true model. Motivated by this result, we propose a\ngreedy algorithm over the space of posets for causal structure discovery in the\npresence of latent confounders and compare its performance to the current\nstate-of-the-art algorithms FCI and FCI+ on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:36:06 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 21:04:46 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Bernstein", "Daniel Irving", ""], ["Saeed", "Basil", ""], ["Squires", "Chandler", ""], ["Uhler", "Caroline", ""]]}, {"id": "1910.09022", "submitter": "Yan Zheng", "authors": "Ruimin Shen, Yan Zheng, Jianye Hao, Yinfeng Chen, Changjie Fan", "title": "Diverse Behavior Is What Game AI Needs: Generating Varied Human-Like\n  Playing Styles Using Evolutionary Multi-Objective Deep Reinforcement Learning", "comments": "1. there is some discrepancy between some contributors with respect\n  to the order of the authors; 2. the paper is rather \"raw\" - significant\n  effort and improvement in terms of the paper's language and structure are\n  needed to make it ready for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  this paper has been withdrawn\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:57:52 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 04:31:10 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 02:32:58 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 04:20:37 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2020 01:34:23 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2020 03:01:55 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Shen", "Ruimin", ""], ["Zheng", "Yan", ""], ["Hao", "Jianye", ""], ["Chen", "Yinfeng", ""], ["Fan", "Changjie", ""]]}, {"id": "1910.09024", "submitter": "Jongmin Yu", "authors": "Jongmin Yu and Hyeontaek Oh", "title": "Boosting Network Weight Separability via Feed-Backward Reconstruction", "comments": "8 pages, 6 figures", "journal-ref": "in IEEE Access, vol. 8, pp. 214923-214931, 2020", "doi": "10.1109/ACCESS.2020.3041470", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new evaluation metric and boosting method for weight\nseparability in neural network design. In contrast to general visual\nrecognition methods designed to encourage both intra-class compactness and\ninter-class separability of latent features, we focus on estimating linear\nindependence of column vectors in weight matrix and improving the separability\nof weight vectors. To this end, we propose an evaluation metric for weight\nseparability based on semi-orthogonality of a matrix and Frobenius distance,\nand the feed-backward reconstruction loss which explicitly encourages weight\nseparability between the column vectors in the weight matrix. The experimental\nresults on image classification and face recognition demonstrate that the\nweight separability boosting via minimization of feed-backward reconstruction\nloss can improve the visual recognition performance, hence universally boosting\nthe performance on various visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:04:40 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 14:05:15 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Yu", "Jongmin", ""], ["Oh", "Hyeontaek", ""]]}, {"id": "1910.09030", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri, Shaowu Yuchi, Shahrokh Shahpar, Geoffrey Parks", "title": "Supporting Multi-point Fan Design with Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the idea of turbomachinery active subspace performance maps,\nthis paper studies dimension reduction in turbomachinery 3D CFD simulations.\nFirst, we show that these subspaces exist across different blades---under the\nsame parametrization---largely independent of their Mach number or Reynolds\nnumber. This is demonstrated via a numerical study on three different blades.\nThen, in an attempt to reduce the computational cost of identifying a suitable\ndimension reducing subspace, we examine statistical sufficient dimension\nreduction methods, including sliced inverse regression, sliced average variance\nestimation, principal Hessian directions and contour regression. Unsatisfied by\nthese results, we evaluate a new idea based on polynomial variable\nprojection---a non-linear least squares problem. Our results using polynomial\nvariable projection clearly demonstrate that one can accurately identify\ndimension reducing subspaces for turbomachinery functionals at a fraction of\nthe cost associated with prior methods. We apply these subspaces to the problem\nof comparing design configurations across different flight points on a working\nline of a fan blade. We demonstrate how designs that offer a healthy compromise\nbetween performance at cruise and sea-level conditions can be easily found by\nvisually inspecting their subspaces.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:39:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Seshadri", "Pranay", ""], ["Yuchi", "Shaowu", ""], ["Shahpar", "Shahrokh", ""], ["Parks", "Geoffrey", ""]]}, {"id": "1910.09036", "submitter": "Aude Genevay", "authors": "Aude Genevay, Gabriel Dulac-Arnold, Jean-Philippe Vert", "title": "Differentiable Deep Clustering with Cluster Size Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental unsupervised learning approach. Many clustering\nalgorithms -- such as $k$-means -- rely on the euclidean distance as a\nsimilarity measure, which is often not the most relevant metric for high\ndimensional data such as images. Learning a lower-dimensional embedding that\ncan better reflect the geometry of the dataset is therefore instrumental for\nperformance. We propose a new approach for this task where the embedding is\nperformed by a differentiable model such as a deep neural network. By rewriting\nthe $k$-means clustering algorithm as an optimal transport task, and adding an\nentropic regularization, we derive a fully differentiable loss function that\ncan be minimized with respect to both the embedding parameters and the cluster\nparameters via stochastic gradient descent. We show that this new formulation\ngeneralizes a recently proposed state-of-the-art method based on soft-$k$-means\nby adding constraints on the cluster sizes. Empirical evaluations on image\nclassification benchmarks suggest that compared to state-of-the-art methods,\nour optimal transport-based approach provide better unsupervised accuracy and\ndoes not require a pre-training phase.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:54:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Genevay", "Aude", ""], ["Dulac-Arnold", "Gabriel", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "1910.09040", "submitter": "Eli Chien", "authors": "Eli Chien, Pan Li, Olgica Milenkovic", "title": "Landing Probabilities of Random Walks for Seed-Set Expansion in\n  Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first known mean-field study of landing probabilities for\nrandom walks on hypergraphs. In particular, we examine clique-expansion and\ntensor methods and evaluate their mean-field characteristics over a class of\nrandom hypergraph models for the purpose of seed-set community expansion. We\ndescribe parameter regimes in which the two methods outperform each other and\npropose a hybrid expansion method that uses partial clique-expansion to reduce\nthe projection distortion and low-complexity tensor methods applied directly on\nthe partially expanded hypergraphs.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 18:14:30 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chien", "Eli", ""], ["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1910.09043", "submitter": "R\\'emi Besson", "authors": "R\\'emi Besson, Erwan Le Pennec and St\\'ephanie Allassonni\\`ere", "title": "Learning from both experts and data", "comments": null, "journal-ref": null, "doi": "10.3390/e21121208", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the problem of inferring a discrete probability\ndistribution using both expert knowledge and empirical data. This is an\nimportant issue for many applications where the scarcity of data prevents a\npurely empirical approach. In this context, it is common to rely first on an\ninitial domain knowledge a priori before proceeding to an online data\nacquisition. We are particularly interested in the intermediate regime where we\ndo not have enough data to do without the initial expert a priori of the\nexperts, but enough to correct it if necessary. We present here a novel way to\ntackle this issue with a method providing an objective way to choose the weight\nto be given to experts compared to data. We show, both empirically and\ntheoretically, that our proposed estimator is always more efficient than the\nbest of the two models (expert or data) within a constant.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 18:30:23 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:59:36 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Besson", "R\u00e9mi", ""], ["Pennec", "Erwan Le", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""]]}, {"id": "1910.09055", "submitter": "Fatih Furkan Yilmaz", "authors": "Fatih Furkan Yilmaz and Reinhard Heckel", "title": "Image recognition from raw labels collected without annotators", "comments": "Version changelog: Added content on ImageNet related experiments;\n  Re-structured the document to incorporate the new content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification problems are typically addressed by first collecting\nexamples with candidate labels, second cleaning the candidate labels manually,\nand third training a deep neural network on the clean examples. The manual\nlabeling step is often the most expensive one as it requires workers to label\nmillions of images. In this paper we propose to work without any explicitly\nlabeled data by i) directly training the deep neural network on the noisy\ncandidate labels, and ii) early stopping the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of standard\noverparameterized convolutional neural networks trained with (stochastic)\ngradient descent: Clean labels are fitted faster than noisy ones. We consider\ntwo classification problems, a subset of ImageNet and CIFAR-10. For both, we\nconstruct large candidate datasets without any explicit human annotations, that\nonly contain 10%-50% correctly labeled examples per class. We show that\ntraining on the candidate examples and regularizing through early stopping\ngives higher test performance for both problems than when training on the\noriginal, clean data. This is possible because the candidate datasets contain a\nhuge number of clean examples, and, as we show in this paper, the noise\ngenerated through the label collection process is not nearly as adversarial for\nlearning as the noise generated by randomly flipping labels.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 19:58:21 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 17:55:12 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 23:11:46 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Yilmaz", "Fatih Furkan", ""], ["Heckel", "Reinhard", ""]]}, {"id": "1910.09056", "submitter": "Saeid Naderiparizi", "authors": "Saeid Naderiparizi, Adam \\'Scibior, Andreas Munk, Mehrdad Ghadiri,\n  At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Bradley Gram-Hansen, Christian Schroeder de\n  Witt, Robert Zinkov, Philip H.S. Torr, Tom Rainforth, Yee Whye Teh, Frank\n  Wood", "title": "Amortized Rejection Sampling in Universal Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to amortized inference in probabilistic programs with\nunbounded loops can produce estimators with infinite variance. An instance of\nthis is importance sampling inference in programs that explicitly include\nrejection sampling as part of the user-programmed generative procedure. In this\npaper we develop a new and efficient amortized importance sampling estimator.\nWe prove finite variance of our estimator and empirically demonstrate our\nmethod's correctness and efficiency compared to existing alternatives on\ngenerative programs containing rejection sampling loops and discuss how to\nimplement our method in a generic probabilistic programming framework.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 20:04:20 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 00:11:55 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Naderiparizi", "Saeid", ""], ["\u015acibior", "Adam", ""], ["Munk", "Andreas", ""], ["Ghadiri", "Mehrdad", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Gram-Hansen", "Bradley", ""], ["de Witt", "Christian Schroeder", ""], ["Zinkov", "Robert", ""], ["Torr", "Philip H. S.", ""], ["Rainforth", "Tom", ""], ["Teh", "Yee Whye", ""], ["Wood", "Frank", ""]]}, {"id": "1910.09057", "submitter": "Wenlin Wang", "authors": "Wenlin Wang, Hongteng Xu, Guoyin Wang, Wenqi Wang, Lawrence Carin", "title": "Zero-Shot Recognition via Optimal Transport", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal transport (OT) framework for generalized zero-shot\nlearning (GZSL), seeking to distinguish samples for both seen and unseen\nclasses, with the assist of auxiliary attributes. The discrepancy between\nfeatures and attributes is minimized by solving an optimal transport problem.\n{Specifically, we build a conditional generative model to generate features\nfrom seen-class attributes, and establish an optimal transport between the\ndistribution of the generated features and that of the real features.} The\ngenerative model and the optimal transport are optimized iteratively with an\nattribute-based regularizer, that further enhances the discriminative power of\nthe generated features. A classifier is learned based on the features generated\nfor both the seen and unseen classes. In addition to generalized zero-shot\nlearning, our framework is also applicable to standard and transductive ZSL\nproblems. Experiments show that our optimal transport-based method outperforms\nstate-of-the-art methods on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 20:18:18 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 03:52:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Wenlin", ""], ["Xu", "Hongteng", ""], ["Wang", "Guoyin", ""], ["Wang", "Wenqi", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.09066", "submitter": "Jiawei Huang", "authors": "Jiawei Huang, Nan Jiang", "title": "From Importance Sampling to Doubly Robust Policy Gradient", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that on-policy policy gradient (PG) and its variance reduction\nvariants can be derived by taking finite difference of function evaluations\nsupplied by estimators from the importance sampling (IS) family for off-policy\nevaluation (OPE). Starting from the doubly robust (DR) estimator (Jiang & Li,\n2016), we provide a simple derivation of a very general and flexible form of\nPG, which subsumes the state-of-the-art variance reduction technique (Cheng et\nal., 2019) as its special case and immediately hints at further variance\nreduction opportunities overlooked by existing literature. We analyze the\nvariance of the new DR-PG estimator, compare it to existing methods as well as\nthe Cramer-Rao lower bound of policy gradient, and empirically show its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 21:27:36 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 18:46:30 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 00:36:45 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Huang", "Jiawei", ""], ["Jiang", "Nan", ""]]}, {"id": "1910.09086", "submitter": "Jindong Gu", "authors": "Jindong Gu, Volker Tresp", "title": "Contextual Prediction Difference Analysis for Explaining Individual\n  Image Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been devoted to understanding the decisions of deep neural\nnetworks in recent years. A number of model-aware saliency methods were\nproposed to explain individual classification decisions by creating saliency\nmaps. However, they are not applicable when the parameters and the gradients of\nthe underlying models are unavailable. Recently, model-agnostic methods have\nalso received attention. As one of them, \\textit{Prediction Difference\nAnalysis} (PDA), a probabilistic sound methodology, was proposed. In this work,\nwe first show that PDA can suffer from saturated classifiers. The saturation\nphenomenon of classifiers exists widely in current neural network-based\nclassifiers. To explain the decisions of saturated classifiers better, we\nfurther propose Contextual PDA, which runs hundreds of times faster than PDA.\nThe experiments show the superiority of our method by explaining image\nclassifications of the state-of-the-art deep convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:04:22 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 00:41:19 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "1910.09089", "submitter": "Akshayaa Magesh", "authors": "Akshayaa Magesh and Venugopal V. Veeravalli", "title": "Decentralized Heterogeneous Multi-Player Multi-Armed Bandits with\n  Non-Zero Rewards on Collisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a fully decentralized multi-player stochastic multi-armed bandit\nsetting where the players cannot communicate with each other and can observe\nonly their own actions and rewards. The environment may appear differently to\ndifferent players, $\\textit{i.e.}$, the reward distributions for a given arm\nare heterogeneous across players. In the case of a collision (when more than\none player plays the same arm), we allow for the colliding players to receive\nnon-zero rewards. The time-horizon $T$ for which the arms are played is\n\\emph{not} known to the players. Within this setup, where the number of players\nis allowed to be greater than the number of arms, we present a policy that\nachieves near order-optimal expected regret of order $O(\\log^{1 + \\delta} T)$\nfor some $0 < \\delta < 1$ over a time-horizon of duration $T$.\n  This paper is currently under review at IEEE Transactions on Information\nTheory.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:27:55 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 10:37:18 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 16:18:21 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Magesh", "Akshayaa", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1910.09090", "submitter": "Jinwei Zhao", "authors": "Jinwei Zhao, Qizhou Wang, Fuqiang Zhang, Wanli Qiu, Yufei Wang, Yu\n  Liu, Guo Xie, Weigang Ma, Bin Wang, Xinhong Hei", "title": "A game method for improving the interpretability of convolution neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real artificial intelligence always has been focused on by many machine\nlearning researchers, especially in the area of deep learning. However deep\nneural network is hard to be understood and explained, and sometimes, even\nmetaphysics. The reason is, we believe that: the network is essentially a\nperceptual model. Therefore, we believe that in order to complete complex\nintelligent activities from simple perception, it is necessary to con-struct\nanother interpretable logical network to form accurate and reasonable responses\nand explanations to external things. Researchers like Bolei Zhou and Quanshi\nZhang have found many explanatory rules for deep feature extraction aimed at\nthe feature extraction stage of convolution neural network. However, although\nresearchers like Marco Gori have also made great efforts to improve the\ninterpretability of the fully connected layers of the network, the problem is\nalso very difficult. This paper firstly analyzes its reason. Then a method of\nconstructing logical network based on the fully connected layers and extracting\nlogical relation between input and output of the layers is proposed. The game\nprocess between perceptual learning and logical abstract cognitive learning is\nimplemented to improve the interpretable performance of deep learning process\nand deep learning model. The benefits of our approach are illustrated on\nbenchmark data sets and in real-world experiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:32:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhao", "Jinwei", ""], ["Wang", "Qizhou", ""], ["Zhang", "Fuqiang", ""], ["Qiu", "Wanli", ""], ["Wang", "Yufei", ""], ["Liu", "Yu", ""], ["Xie", "Guo", ""], ["Ma", "Weigang", ""], ["Wang", "Bin", ""], ["Hei", "Xinhong", ""]]}, {"id": "1910.09091", "submitter": "Akshayaa Magesh", "authors": "Akshayaa Magesh and Venugopal V. Veeravalli", "title": "Multi-User MABs with User Dependent Rewards for Uncoordinated Spectrum\n  Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-user multi-armed bandits have emerged as a good model for uncoordinated\nspectrum access problems. In this paper we consider the scenario where users\ncannot communicate with each other. In addition, the environment may appear\ndifferently to different users, ${i.e.}$, the mean rewards as observed by\ndifferent users for the same channel may be different. With this setup, we\npresent a policy that achieves a regret of $O (\\log{T})$. This paper has been\naccepted at Asilomar Conference on Signals, Systems, and Computers 2019.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:32:58 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 21:36:49 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 18:58:41 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Magesh", "Akshayaa", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1910.09092", "submitter": "Michael Lingzhi Li", "authors": "Dimitris Bertsimas, Michael Lingzhi Li", "title": "Fast Exact Matrix Completion: A Unified Optimization Framework for\n  Matrix Completion", "comments": null, "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-43", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the problem of matrix completion with and without side\ninformation as a non-convex optimization problem. We design fastImpute based on\nnon-convex gradient descent and show it converges to a global minimum that is\nguaranteed to recover closely the underlying matrix while it scales to matrices\nof sizes beyond $10^5 \\times 10^5$. We report experiments on both synthetic and\nreal-world datasets that show fastImpute is competitive in both the accuracy of\nthe matrix recovered and the time needed across all cases. Furthermore, when a\nhigh number of entries are missing, fastImpute is over $75\\%$ lower in MAPE and\n$15$ times faster than current state-of-the-art matrix completion methods in\nboth the case with side information and without.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:37:06 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 22:23:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "1910.09093", "submitter": "Benjamin Petit", "authors": "Benjamin Petit, Loren Amdahl-Culleton, Yao Liu, Jimmy Smith,\n  Pierre-Luc Bacon", "title": "All-Action Policy Gradient Methods: A Numerical Integration Approach", "comments": "9 pages, 2 figures. NeurIPS 2019 Optimization Foundations of\n  Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While often stated as an instance of the likelihood ratio trick [Rubinstein,\n1989], the original policy gradient theorem [Sutton, 1999] involves an integral\nover the action space. When this integral can be computed, the resulting\n\"all-action\" estimator [Sutton, 2001] provides a conditioning effect [Bratley,\n1987] reducing the variance significantly compared to the REINFORCE estimator\n[Williams, 1992]. In this paper, we adopt a numerical integration perspective\nto broaden the applicability of the all-action estimator to general spaces and\nto any function class for the policy or critic components, beyond the Gaussian\ncase considered by [Ciosek, 2018]. In addition, we provide a new theoretical\nresult on the effect of using a biased critic which offers more guidance than\nthe previous \"compatible features\" condition of [Sutton, 1999]. We demonstrate\nthe benefit of our approach in continuous control tasks with nonlinear function\napproximation. Our results show improved performance and sample efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:42:48 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Petit", "Benjamin", ""], ["Amdahl-Culleton", "Loren", ""], ["Liu", "Yao", ""], ["Smith", "Jimmy", ""], ["Bacon", "Pierre-Luc", ""]]}, {"id": "1910.09106", "submitter": "Yoann Boget", "authors": "Yoann Boget (University of Neuch\\^atel)", "title": "Adversarial Regression. Generative Adversarial Networks for Non-Linear\n  Regression: Theory and Assessment", "comments": "Master thesis. Grade: 5.5/6", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial Regression is a proposition to perform high dimensional\nnon-linear regression with uncertainty estimation. We used Conditional\nGenerative Adversarial Network to obtain an estimate of the full predictive\ndistribution for a new observation. Generative Adversarial Networks (GAN) are\nimplicit generative models which produce samples from a distribution\napproximating the distribution of the data. The conditional version of it\n(CGAN) takes the following expression: $\\min\\limits_G \\max\\limits_D V(D, G) =\n\\mathbb{E}_{x\\sim p_{r}(x)} [log(D(x, y))] + \\mathbb{E}_{z\\sim p_{z}(z)} [log\n(1-D(G(z, y)))]$. An approximate solution can be found by training\nsimultaneously two neural networks to model D and G and feeding G with a random\nnoise vector $z$. After training, we have that $G(z, y)\\mathrel{\\dot\\sim}\np_{data}(x, y)$. By fixing $y$, we have $G(z|y) \\mathrel{\\dot\\sim}\np{data}(x|y)$. By sampling $z$, we can therefore obtain samples following\napproximately $p(x|y)$, which is the predictive distribution of $x$ for a new\n$y$. We ran experiments to test various loss functions, data distributions,\nsample size, size of the noise vector, etc. Even if we observed differences, no\nexperiment outperformed consistently the others. The quality of CGAN for\nregression relies on fine-tuning a range of hyperparameters. In a broader view,\nthe results show that CGANs are very promising methods to perform uncertainty\nestimation for high dimensional non-linear regression.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:01:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Boget", "Yoann", "", "University of Neuch\u00e2tel"]]}, {"id": "1910.09108", "submitter": "Jongmin Yu", "authors": "Jongmin Yu", "title": "Boosting Mapping Functionality of Neural Networks via Latent Feature\n  Generation based on Reversible Learning", "comments": "9 pages, 5 figures, have been submitted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses a boosting method for mapping functionality of neural\nnetworks in visual recognition such as image classification and face\nrecognition. We present reversible learning for generating and learning latent\nfeatures using the network itself. By generating latent features corresponding\nto hard samples and applying the generated features in a training stage,\nreversible learning can improve a mapping functionality without additional data\naugmentation or handling the bias of dataset. We demonstrate an efficiency of\nthe proposed method on the MNIST,Cifar-10/100, and Extremely Biased and poorly\ncategorized dataset (EBPC dataset). The experimental results show that the\nproposed method can outperform existing state-of-the-art methods in visual\nrecognition. Extensive analysis shows that our method can efficiently improve\nthe mapping capability of a network.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 01:46:32 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yu", "Jongmin", ""]]}, {"id": "1910.09113", "submitter": "Paul Soulos", "authors": "Paul Soulos, Tom McCoy, Tal Linzen, Paul Smolensky", "title": "Discovering the Compositional Structure of Vector Representations with\n  Role Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can neural networks perform so well on compositional tasks even though\nthey lack explicit compositional representations? We use a novel analysis\ntechnique called ROLE to show that recurrent neural networks perform well on\nsuch tasks by converging to solutions which implicitly represent symbolic\nstructure. This method uncovers a symbolic structure which, when properly\nembedded in vector space, closely approximates the encodings of a standard\nseq2seq network trained to perform the compositional SCAN task. We verify the\ncausal importance of the discovered symbolic structure by showing that, when we\nsystematically manipulate hidden embeddings based on this symbolic structure,\nthe model's output is changed in the way predicted by our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:12:51 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 16:38:33 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 20:23:51 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Soulos", "Paul", ""], ["McCoy", "Tom", ""], ["Linzen", "Tal", ""], ["Smolensky", "Paul", ""]]}, {"id": "1910.09115", "submitter": "Jiaming Song", "authors": "Jiaming Song and Yang Song and Stefano Ermon", "title": "Unsupervised Out-of-Distribution Detection with Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood from a generative model is a natural statistic for detecting\nout-of-distribution (OoD) samples. However, generative models have been shown\nto assign higher likelihood to OoD samples compared to ones from the training\ndistribution, preventing simple threshold-based detection rules. We demonstrate\nthat OoD detection fails even when using more sophisticated statistics based on\nthe likelihoods of individual samples. To address these issues, we propose a\nnew method that leverages batch normalization. We argue that batch\nnormalization for generative models challenges the traditional i.i.d. data\nassumption and changes the corresponding maximum likelihood objective. Based on\nthis insight, we propose to exploit in-batch dependencies for OoD detection.\nEmpirical results suggest that this leads to more robust detection for\nhigh-dimensional images.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:14:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Song", "Jiaming", ""], ["Song", "Yang", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.09119", "submitter": "Fei Deng", "authors": "Fei Deng, Zhuo Zhi, Sungjin Ahn", "title": "Generative Hierarchical Models for Parts, Objects, and Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional structures between parts and objects are inherent in natural\nscenes. Modeling such compositional hierarchies via unsupervised learning can\nbring various benefits such as interpretability and transferability, which are\nimportant in many downstream tasks. In this paper, we propose the first deep\nlatent variable model, called RICH, for learning Representation of\nInterpretable Compositional Hierarchies. At the core of RICH is a latent scene\ngraph representation that organizes the entities of a scene into a tree\nstructure according to their compositional relationships. During inference,\ntaking top-down approach, RICH is able to use higher-level representation to\nguide lower-level decomposition. This avoids the difficult problem of routing\nbetween parts and objects that is faced by bottom-up approaches. In experiments\non images containing multiple objects with different part compositions, we\ndemonstrate that RICH is able to learn the latent compositional hierarchy and\ngenerate imaginary scenes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:28:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Deng", "Fei", ""], ["Zhi", "Zhuo", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1910.09122", "submitter": "Chris Cannella", "authors": "Chris Cannella, Jie Ding, Mohammadreza Soltani, Vahid Tarokh", "title": "Perception-Distortion Trade-off with Restricted Boltzmann Machines", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new procedure for applying Restricted Boltzmann\nMachines (RBMs) to missing data inference tasks, based on linearization of the\neffective energy function governing the distribution of observations. We\ncompare the performance of our proposed procedure with those obtained using\nexisting reconstruction procedures trained on incomplete data. We place these\nperformance comparisons within the context of the perception-distortion\ntrade-off observed in other data reconstruction tasks, which has, until now,\nremained unexplored in tasks relying on incomplete training data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:39:28 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cannella", "Chris", ""], ["Ding", "Jie", ""], ["Soltani", "Mohammadreza", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.09126", "submitter": "Xiang Li", "authors": "Xiang Li, Wenhao Yang, Shusen Wang, Zhihua Zhang", "title": "Communication-Efficient Local Decentralized SGD Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the technique of local updates is a powerful tool in centralized\nsettings to improve communication efficiency via periodical communication. For\ndecentralized settings, it is still unclear how to efficiently combine local\nupdates and decentralized communication. In this work, we propose an algorithm\nnamed as LD-SGD, which incorporates arbitrary update schemes that alternate\nbetween multiple Local updates and multiple Decentralized SGDs, and provide an\nanalytical framework for LD-SGD. Under the framework, we present a sufficient\ncondition to guarantee the convergence. We show that LD-SGD converges to a\ncritical point for a wide range of update schemes when the objective is\nnon-convex and the training data are non-identically independent distributed.\nMoreover, our framework brings many insights into the design of update schemes\nfor decentralized optimization. As examples, we specify two update schemes and\nshow how they help improve communication efficiency. Specifically, the first\nscheme alternates the number of local and global update steps. From our\nanalysis, the ratio of the number of local updates to that of decentralized SGD\ntrades off communication and computation. The second scheme is to periodically\nshrink the length of local updates. We show that the decaying strategy helps\nimprove communication efficiency both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:51:54 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 03:26:10 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 07:09:43 GMT"}, {"version": "v4", "created": "Thu, 25 Mar 2021 05:33:56 GMT"}, {"version": "v5", "created": "Mon, 5 Apr 2021 12:06:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Xiang", ""], ["Yang", "Wenhao", ""], ["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1910.09152", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "A New Framework for Multi-Agent Reinforcement Learning -- Centralized\n  Training and Exploration with Decentralized Execution via Policy Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) is a booming area of artificial\nintelligence. Many practical applications of DRL naturally involve more than\none collaborative learners, making it important to study DRL in a multi-agent\ncontext. Previous research showed that effective learning in complex\nmulti-agent systems demands for highly coordinated environment exploration\namong all the participating agents. Many researchers attempted to cope with\nthis challenge through learning centralized value functions. However, the\ncommon strategy for every agent to learn their local policies directly often\nfail to nurture strong inter-agent collaboration and can be sample inefficient\nwhenever agents alter their communication channels. To address these issues, we\npropose a new framework known as centralized training and exploration with\ndecentralized execution via policy distillation. Guided by this framework and\nthe maximum-entropy learning technique, we will first train agents' policies\nwith shared global component to foster coordinated and effective learning.\nLocally executable policies will be derived subsequently from the trained\nglobal policies via policy distillation. Experiments show that our new\nframework and algorithm can achieve significantly better performance and higher\nsample efficiency than a cutting-edge baseline on several multi-agent DRL\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 05:07:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1910.09158", "submitter": "S Indrapriyadarsini", "authors": "S. Indrapriyadarsini, Shahrzad Mahboubi, Hiroshi Ninomiya and Hideki\n  Asai", "title": "Implementation of a modified Nesterov's Accelerated quasi-Newton Method\n  on Tensorflow", "comments": "Paper published in 2018 17th IEEE International Conference on Machine\n  Learning and Applications (ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00185", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies incorporate Nesterov's accelerated gradient method for the\nacceleration of gradient based training. The Nesterov's Accelerated\nQuasi-Newton (NAQ) method has shown to drastically improve the convergence\nspeed compared to the conventional quasi-Newton method. This paper implements\nNAQ for non-convex optimization on Tensorflow. Two modifications have been\nproposed to the original NAQ algorithm to ensure global convergence and\neliminate linesearch. The performance of the proposed algorithm - mNAQ is\nevaluated on standard non-convex function approximation benchmark problems and\nmicrowave circuit modelling problems. The results show that the improved\nalgorithm converges better and faster compared to first order optimizers such\nas AdaGrad, RMSProp, Adam, and the second order methods such as the\nquasi-Newton method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 05:59:19 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Indrapriyadarsini", "S.", ""], ["Mahboubi", "Shahrzad", ""], ["Ninomiya", "Hiroshi", ""], ["Asai", "Hideki", ""]]}, {"id": "1910.09161", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu, Henry Shaowu Yuchi, Minghe Zhang, Yao Xie", "title": "Sequential Adversarial Anomaly Detection for One-Class Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential anomaly detection problem in the one-class setting\nwhen only the anomalous sequences are available and propose an adversarial\nsequential detector by solving a minimax problem to find an optimal detector\nagainst the worst-case sequences from a generator. The generator captures the\ndependence in sequential events using the marked point process model. The\ndetector sequentially evaluates the likelihood of a test sequence and compares\nit with a time-varying threshold, also learned from data through the minimax\nproblem. We demonstrate our proposed method's good performance using numerical\nexperiments on simulations and proprietary large-scale credit card fraud\ndatasets. The proposed method can generally apply to detecting anomalous\nsequences.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:12:47 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 19:25:55 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 23:23:18 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 23:46:36 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhu", "Shixiang", ""], ["Yuchi", "Henry Shaowu", ""], ["Zhang", "Minghe", ""], ["Xie", "Yao", ""]]}, {"id": "1910.09168", "submitter": "Pritam Anand South Asian University", "authors": "Pritam Anand, Reshma Rastogi and Suresh Chandra", "title": "A $\\nu$- support vector quantile regression model with automatic\n  accuracy control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel '$\\nu$-support vector quantile regression'\n($\\nu$-SVQR) model for the quantile estimation. It can facilitate the automatic\ncontrol over accuracy by creating a suitable asymmetric $\\epsilon$-insensitive\nzone according to the variance present in data. The proposed $\\nu$-SVQR model\nuses the $\\nu$ fraction of training data points for the estimation of the\nquantiles. In the $\\nu$-SVQR model, training points asymptotically appear above\nand below of the asymmetric $\\epsilon$-insensitive tube in the ratio of\n$1-\\tau$ and $\\tau$. Further, there are other interesting properties of the\nproposed $\\nu$-SVQR model, which we have briefly described in this paper. These\nproperties have been empirically verified using the artificial and real world\ndataset also.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:40:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Anand", "Pritam", ""], ["Rastogi", "Reshma", ""], ["Chandra", "Suresh", ""]]}, {"id": "1910.09170", "submitter": "Sangwoo Mo", "authors": "Sangwoo Mo, Chiheon Kim, Sungwoong Kim, Minsu Cho, Jinwoo Shin", "title": "Mining GOLD Samples for Conditional GANs", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative adversarial networks (cGANs) have gained a\nconsiderable attention in recent years due to its class-wise controllability\nand superior quality for complex generation tasks. We introduce a simple yet\neffective approach to improving cGANs by measuring the discrepancy between the\ndata distribution and the model distribution on given samples. The proposed\nmeasure, coined the gap of log-densities (GOLD), provides an effective\nself-diagnosis for cGANs while being efficienty computed from the\ndiscriminator. We propose three applications of the GOLD: example re-weighting,\nrejection sampling, and active learning, which improve the training, inference,\nand data selection of cGANs, respectively. Our experimental results demonstrate\nthat the proposed methods outperform corresponding baselines for all three\napplications on different image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:49:32 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mo", "Sangwoo", ""], ["Kim", "Chiheon", ""], ["Kim", "Sungwoong", ""], ["Cho", "Minsu", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1910.09191", "submitter": "Zhuang Liu", "authors": "Zhuang Liu, Xuanlin Li, Bingyi Kang, Trevor Darrell", "title": "Regularization Matters in Policy Optimization -- An Empirical Study on\n  Continuous Control", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg .\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:00:33 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 10:19:31 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 13:53:13 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2021 04:57:59 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Zhuang", ""], ["Li", "Xuanlin", ""], ["Kang", "Bingyi", ""], ["Darrell", "Trevor", ""]]}, {"id": "1910.09223", "submitter": "Chao Zhang", "authors": "Chao Zhang, Jiahao Xie, Zebang Shen, Peilin Zhao, Tengfei Zhou, Hui\n  Qian", "title": "Aggregated Gradient Langevin Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a general Aggregated Gradient Langevin Dynamics\nframework (AGLD) for the Markov Chain Monte Carlo (MCMC) sampling. We\ninvestigate the nonasymptotic convergence of AGLD with a unified analysis for\ndifferent data accessing (e.g. random access, cyclic access and random\nreshuffle) and snapshot updating strategies, under convex and nonconvex\nsettings respectively. It is the first time that bounds for I/O friendly\nstrategies such as cyclic access and random reshuffle have been established in\nthe MCMC literature. The theoretic results also indicate that methods in AGLD\npossess the merits of both the low per-iteration computational complexity and\nthe short mixture time. Empirical studies demonstrate that our framework allows\nto derive novel schemes to generate high-quality samples for large-scale\nBayesian posterior learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:18:20 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhang", "Chao", ""], ["Xie", "Jiahao", ""], ["Shen", "Zebang", ""], ["Zhao", "Peilin", ""], ["Zhou", "Tengfei", ""], ["Qian", "Hui", ""]]}, {"id": "1910.09239", "submitter": "Jan Philip G\\\"opfert", "authors": "Jan Philip G\\\"opfert and Heiko Wersing and Barbara Hammer", "title": "Recovering Localized Adversarial Attacks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30487-4_24", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved great successes over recent\nyears, particularly in the domain of computer vision. They are fast,\nconvenient, and -- thanks to mature frameworks -- relatively easy to implement\nand deploy. However, their reasoning is hidden inside a black box, in spite of\na number of proposed approaches that try to provide human-understandable\nexplanations for the predictions of neural networks. It is still a matter of\ndebate which of these explainers are best suited for which situations, and how\nto quantitatively evaluate and compare them. In this contribution, we focus on\nthe capabilities of explainers for convolutional deep neural networks in an\nextreme situation: a setting in which humans and networks fundamentally\ndisagree. Deep neural networks are susceptible to adversarial attacks that\ndeliberately modify input samples to mislead a neural network's classification,\nwithout affecting how a human observer interprets the input. Our goal with this\ncontribution is to evaluate explainers by investigating whether they can\nidentify adversarially attacked regions of an image. In particular, we\nquantitatively and qualitatively investigate the capability of three popular\nexplainers of classifications -- classic salience, guided backpropagation, and\nLIME -- with respect to their ability to identify regions of attack as the\nexplanatory regions for the (incorrect) prediction in representative examples\nfrom image classification. We find that LIME outperforms the other explainers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:53:44 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["G\u00f6pfert", "Jan Philip", ""], ["Wersing", "Heiko", ""], ["Hammer", "Barbara", ""]]}, {"id": "1910.09246", "submitter": "Federico Cabitza", "authors": "Federico Cabitza and Andrea Campagner", "title": "Who wants accurate models? Arguing for a different metrics to take\n  classification models seriously", "comments": "https://github.com/AndreaCampagner/uncertainpy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the increasing availability of AI-based decision support, there is an\nincreasing need for their certification by both AI manufacturers and notified\nbodies, as well as the pragmatic (real-world) validation of these systems.\nTherefore, there is the need for meaningful and informative ways to assess the\nperformance of AI systems in clinical practice. Common metrics (like accuracy\nscores and areas under the ROC curve) have known problems and they do not take\ninto account important information about the preferences of clinicians and the\nneeds of their specialist practice, like the likelihood and impact of errors\nand the complexity of cases. In this paper, we present a new accuracy measure,\nthe H-accuracy (Ha), which we claim is more informative in the medical domain\n(and others of similar needs) for the elements it encompasses. We also provide\nproof that the H-accuracy is a generalization of the balanced accuracy and\nestablish a relation between the H-accuracy and the Net Benefit. Finally, we\nillustrate an experimentation in two user studies to show the descriptive power\nof the Ha score and how complementary and differently informative measures can\nbe derived from its formulation (a Python script to compute Ha is also made\navailable).\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 10:04:50 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 12:32:56 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Cabitza", "Federico", ""], ["Campagner", "Andrea", ""]]}, {"id": "1910.09255", "submitter": "Gaurav Singh", "authors": "Gaurav Singh, Zahra Sabet, John Shawe-Taylor, James Thomas", "title": "Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical\n  Text Tagging with Applications in PICO Annotation", "comments": "International Workshop on Health Intelligence (W3PHIAI-20); AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical text tagging systems are plagued by the dearth of labeled training\ndata. There have been recent attempts at using pre-trained encoders to deal\nwith this issue. Pre-trained encoder provides representation of the input text\nwhich is then fed to task-specific layers for classification. The entire\nnetwork is fine-tuned on the labeled data from the target task. Unfortunately,\na low-resource biomedical task often has too few labeled instances for\nsatisfactory fine-tuning. Also, if the label space is large, it contains few or\nno labeled instances for majority of the labels. Most biomedical tagging\nsystems treat labels as indexes, ignoring the fact that these labels are often\nconcepts expressed in natural language e.g. `Appearance of lesion on brain\nimaging'. To address these issues, we propose constructing extra labeled\ninstances using label-text (i.e. label's name) as input for the corresponding\nlabel-index (i.e. label's index). In fact, we propose a number of strategies\nfor manufacturing multiple artificial labeled instances from a single label.\nThe network is then fine-tuned on a combination of real and these newly\nconstructed artificial labeled instances. We evaluate the proposed approach on\nan important low-resource biomedical task called \\textit{PICO annotation},\nwhich requires tagging raw text describing clinical trials with labels\ncorresponding to different aspects of the trial i.e. PICO (Population,\nIntervention/Control, Outcome) characteristics of the trial. Our empirical\nresults show that the proposed method achieves a new state-of-the-art\nperformance for PICO annotation with very significant improvements over\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 10:19:53 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 15:10:16 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 22:29:49 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Singh", "Gaurav", ""], ["Sabet", "Zahra", ""], ["Shawe-Taylor", "John", ""], ["Thomas", "James", ""]]}, {"id": "1910.09259", "submitter": "Michael Pearce", "authors": "Michael Pearce, Matthias Poloczek, Juergen Branke", "title": "Bayesian Optimization Allowing for Common Random Numbers", "comments": "21 pages + Appendix, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a powerful tool for expensive stochastic black-box\noptimization problems such as simulation-based optimization or machine learning\nhyperparameter tuning. Many stochastic objective functions implicitly require a\nrandom number seed as input. By explicitly reusing a seed a user can exploit\ncommon random numbers, comparing two or more inputs under the same randomly\ngenerated scenario, such as a common customer stream in a job shop problem, or\nthe same random partition of training data into training and validation set for\na machine learning algorithm. With the aim of finding an input with the best\naverage performance over infinitely many seeds, we propose a novel Gaussian\nprocess model that jointly models both the output for each seed and the\naverage. We then introduce the Knowledge gradient for Common Random Numbers\nthat iteratively determines a combination of input and random seed to evaluate\nthe objective and automatically trades off reusing old seeds and querying new\nseeds, thus overcoming the need to evaluate inputs in batches or measuring\ndifferences of pairs as suggested in previous methods. We investigate the\nKnowledge Gradient for Common Random Numbers both theoretically and\nempirically, finding it achieves significant performance improvements with only\nmoderate added computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 10:43:07 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Pearce", "Michael", ""], ["Poloczek", "Matthias", ""], ["Branke", "Juergen", ""]]}, {"id": "1910.09266", "submitter": "Emad Grais", "authors": "Emad M. Grais, Fei Zhao, Mark D. Plumbley", "title": "Multi-Band Multi-Resolution Fully Convolutional Neural Networks for\n  Singing Voice Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with convolutional layers usually process the entire\nspectrogram of an audio signal with the same time-frequency resolutions, number\nof filters, and dimensionality reduction scale. According to the constant-Q\ntransform, good features can be extracted from audio signals if the low\nfrequency bands are processed with high frequency resolution filters and the\nhigh frequency bands with high time resolution filters. In the spectrogram of a\nmixture of singing voices and music signals, there is usually more information\nabout the voice in the low frequency bands than the high frequency bands. These\nraise the need for processing each part of the spectrogram differently. In this\npaper, we propose a multi-band multi-resolution fully convolutional neural\nnetwork (MBR-FCN) for singing voice separation. The MBR-FCN processes the\nfrequency bands that have more information about the target signals with more\nfilters and smaller dimentionality reduction scale than the bands with less\ninformation. Furthermore, the MBR-FCN processes the low frequency bands with\nhigh frequency resolution filters and the high frequency bands with high time\nresolution filters. Our experimental results show that the proposed MBR-FCN\nwith very few parameters achieves better singing voice separation performance\nthan other deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 11:29:29 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Grais", "Emad M.", ""], ["Zhao", "Fei", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1910.09281", "submitter": "Joshua Hare", "authors": "Joshua Hare", "title": "Dealing with Sparse Rewards in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully navigating a complex environment to obtain a desired outcome is\na difficult task, that up to recently was believed to be capable only by\nhumans. This perception has been broken down over time, especially with the\nintroduction of deep reinforcement learning, which has greatly increased the\ndifficulty of tasks that can be automated. However, for traditional\nreinforcement learning agents this requires an environment to be able to\nprovide frequent extrinsic rewards, which are not known or accessible for many\nreal-world environments. This project aims to explore and contrast existing\nreinforcement learning solutions that circumnavigate the difficulties of an\nenvironment that provide sparse rewards. Different reinforcement solutions will\nbe implemented over a several video game environments with varying difficulty\nand varying frequency of rewards, as to properly investigate the applicability\nof these solutions. This project introduces a novel reinforcement learning\nsolution by combining aspects of two existing state of the art sparse reward\nsolutions, curiosity driven exploration and unsupervised auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:06:28 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 14:18:31 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hare", "Joshua", ""]]}, {"id": "1910.09284", "submitter": "Andreas Barthelme", "authors": "Andreas Barthelme, Reinhard Wiesmayr and Wolfgang Utschick", "title": "Model Order Selection in DoA Scenarios via Cross-Entropy based Machine\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053029", "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a machine learning approach for estimating the\nnumber of incident wavefronts in a direction of arrival scenario. In contrast\nto previous works, a multilayer neural network with a cross-entropy objective\nis trained. Furthermore, we investigate an online training procedure that\nallows an adaption of the neural network to imperfections of an antenna array\nwithout explicitly calibrating the array manifold. We show via simulations that\nthe proposed method outperforms classical model order selection schemes based\non information criteria in terms of accuracy, especially for a small number of\nsnapshots and at low signal-to-noise-ratios. Also, the online training\nprocedure enables the neural network to adapt with only a few online training\nsamples, if initialized by offline training on artificial data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:13:24 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Barthelme", "Andreas", ""], ["Wiesmayr", "Reinhard", ""], ["Utschick", "Wolfgang", ""]]}, {"id": "1910.09293", "submitter": "Yang Qu", "authors": "Ming-Xi Wang, Yang Qu", "title": "Approximation capabilities of neural networks on unbounded domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that a shallow neural network with a monotone\nsigmoid, ReLU, ELU, Softplus, or LeakyReLU activation function can arbitrarily\nwell approximate any L^p(p>=2) integrable functions defined on R*[0,1]^n. We\nalso prove that a shallow neural network with a sigmoid, ReLU, ELU, Softplus,\nor LeakyReLU activation function expresses no nonzero integrable function\ndefined on the Euclidean plane. Together with a recent result that the deep\nReLU network can arbitrarily well approximate any integrable function on\nEuclidean spaces, we provide a new perspective on the advantage of multiple\nhidden layers in the context of ReLU networks. Lastly, we prove that the ReLU\nnetwork with depth 3 is a universal approximator in L^p(R^n).\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:25:29 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:02:34 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 21:24:22 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 00:04:32 GMT"}, {"version": "v5", "created": "Thu, 20 Feb 2020 22:32:35 GMT"}, {"version": "v6", "created": "Sun, 16 Aug 2020 14:31:59 GMT"}, {"version": "v7", "created": "Thu, 20 Aug 2020 08:24:48 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Ming-Xi", ""], ["Qu", "Yang", ""]]}, {"id": "1910.09309", "submitter": "Yinan Yu", "authors": "Yinan Yu and Tomas McKelvey", "title": "Learning Hierarchical Feature Space Using CLAss-specific Subspace\n  Multiple Kernel -- Metric Learning for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning for classification has been intensively studied over the last\ndecade. The idea is to learn a metric space induced from a normed vector space\non which data from different classes are well separated. Different measures of\nthe separation thus lead to various designs of the objective function in the\nmetric learning model. One classical metric is the Mahalanobis distance, where\na linear transformation matrix is designed and applied on the original dataset\nto obtain a new subspace equipped with the Euclidean norm. The kernelized\nversion has also been developed, followed by Multiple-Kernel learning models.\nIn this paper, we consider metric learning to be the identification of the best\nkernel function with respect to a high class separability in the corresponding\nmetric space. The contribution is twofold: 1) No pairwise computations are\nrequired as in most metric learning techniques; 2) Better flexibility and lower\ncomputational complexity is achieved using the CLAss-Specific (Multiple) Kernel\n- Metric Learning (CLAS(M)K-ML). The proposed techniques can be considered as a\npreprocessing step to any kernel method or kernel approximation technique. An\nextension to a hierarchical learning structure is also proposed to further\nimprove the classification performance, where on each layer, the CLASMK is\ncomputed based on a selected \"marginal\" subset and feature vectors are\nconstructed by concatenating the features from all previous layers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:44:04 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yu", "Yinan", ""], ["McKelvey", "Tomas", ""]]}, {"id": "1910.09313", "submitter": "Tobias Weber", "authors": "Tobias Weber, Dieter Kranzlm\\\"uller, Michael Fromm, Nelson Tavares de\n  Sousa", "title": "Using Supervised Learning to Classify Metadata of Research Data by\n  Discipline of Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated classification of metadata of research data by their discipline(s)\nof research can be used in scientometric research, by repository service\nproviders, and in the context of research data aggregation services. Openly\navailable metadata of the DataCite index for research data were used to compile\na large training and evaluation set comprised of 609,524 records, which is\npublished alongside this paper. These data allow to reproducibly assess\nclassification approaches, such as tree-based models and neural networks.\nAccording to our experiments with 20 base classes (multi-label classification),\nmulti-layer perceptron models perform best with a f1-macro score of 0.760\nclosely followed by Long Short-Term Memory models (f1-macro score of 0.755). A\npossible application of the trained classification models is the quantitative\nanalysis of trends towards interdisciplinarity of digital scholarly output or\nthe characterization of growth patterns of research data, stratified by\ndiscipline of research. Both applications perform at scale with the proposed\nmodels which are available for re-use.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:51:37 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Weber", "Tobias", ""], ["Kranzlm\u00fcller", "Dieter", ""], ["Fromm", "Michael", ""], ["de Sousa", "Nelson Tavares", ""]]}, {"id": "1910.09322", "submitter": "Nino Vieillard", "authors": "Nino Vieillard, Bruno Scherrer, Olivier Pietquin, Matthieu Geist", "title": "Momentum in Reinforcement Learning", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We adapt the optimization's concept of momentum to reinforcement learning.\nSeeing the state-action value functions as an analog to the gradients in\noptimization, we interpret momentum as an average of consecutive $q$-functions.\nWe derive Momentum Value Iteration (MoVI), a variation of Value Iteration that\nincorporates this momentum idea. Our analysis shows that this allows MoVI to\naverage errors over successive iterations. We show that the proposed approach\ncan be readily extended to deep learning. Specifically, we propose a simple\nimprovement on DQN based on MoVI, and experiment it on Atari games.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:51:38 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 10:25:46 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Vieillard", "Nino", ""], ["Scherrer", "Bruno", ""], ["Pietquin", "Olivier", ""], ["Geist", "Matthieu", ""]]}, {"id": "1910.09323", "submitter": "Jiacheng Zhu", "authors": "Shenghao Qin, Jiacheng Zhu, Jimmy Qin, Wenshuo Wang, Ding Zhao", "title": "Recurrent Attentive Neural Process for Sequential Data", "comments": "12 pages, 6 figures, NeurIPS 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural processes (NPs) learn stochastic processes and predict the\ndistribution of target output adaptively conditioned on a context set of\nobserved input-output pairs. Furthermore, Attentive Neural Process (ANP)\nimproved the prediction accuracy of NPs by incorporating attention mechanism\namong contexts and targets. In a number of real-world applications such as\nrobotics, finance, speech, and biology, it is critical to learn the temporal\norder and recurrent structure from sequential data. However, the capability of\nNPs capturing these properties is limited due to its permutation invariance\ninstinct. In this paper, we proposed the Recurrent Attentive Neural Process\n(RANP), or alternatively, Attentive Neural Process-RecurrentNeural\nNetwork(ANP-RNN), in which the ANP is incorporated into a recurrent neural\nnetwork. The proposed model encapsulates both the inductive biases of recurrent\nneural networks and also the strength of NPs for modelling uncertainty. We\ndemonstrate that RANP can effectively model sequential data and outperforms NPs\nand LSTMs remarkably in a 1D regression toy example as well as\nautonomous-driving applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:13:47 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Qin", "Shenghao", ""], ["Zhu", "Jiacheng", ""], ["Qin", "Jimmy", ""], ["Wang", "Wenshuo", ""], ["Zhao", "Ding", ""]]}, {"id": "1910.09328", "submitter": "Alexandra Gessner", "authors": "Alexandra Gessner, Oindrila Kanjilal, Philipp Hennig", "title": "Integrals over Gaussians under Linear Domain Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrals of linearly constrained multivariate Gaussian densities are a\nfrequent problem in machine learning and statistics, arising in tasks like\ngeneralized linear models and Bayesian optimization. Yet they are notoriously\nhard to compute, and to further complicate matters, the numerical values of\nsuch integrals may be very small. We present an efficient black-box algorithm\nthat exploits geometry for the estimation of integrals over a small, truncated\nGaussian volume, and to simulate therefrom. Our algorithm uses the\nHolmes-Diaconis-Ross (HDR) method combined with an analytic version of\nelliptical slice sampling (ESS). Adapted to the linear setting, ESS allows for\nrejection-free sampling, because intersections of ellipses and domain\nboundaries have closed-form solutions. The key idea of HDR is to decompose the\nintegral into easier-to-compute conditional probabilities by using a sequence\nof nested domains. Remarkably, it allows for direct computation of the\nlogarithm of the integral value and thus enables the computation of extremely\nsmall probability masses. We demonstrate the effectiveness of our tailored\ncombination of HDR and ESS on high-dimensional integrals and on entropy search\nfor Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:59:02 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 13:41:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Gessner", "Alexandra", ""], ["Kanjilal", "Oindrila", ""], ["Hennig", "Philipp", ""]]}, {"id": "1910.09338", "submitter": "Sven Gowal", "authors": "Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann,\n  Pushmeet Kohli", "title": "An Alternative Surrogate Loss for PGD-based Adversarial Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial testing methods based on Projected Gradient Descent (PGD) are\nwidely used for searching norm-bounded perturbations that cause the inputs of\nneural networks to be misclassified. This paper takes a deeper look at these\nmethods and explains the effect of different hyperparameters (i.e., optimizer,\nstep size and surrogate loss). We introduce the concept of MultiTargeted\ntesting, which makes clever use of alternative surrogate losses, and explain\nwhen and how MultiTargeted is guaranteed to find optimal perturbations.\nFinally, we demonstrate that MultiTargeted outperforms more sophisticated\nmethods and often requires less iterative steps than other variants of PGD\nfound in the literature. Notably, MultiTargeted ranks first on MadryLab's\nwhite-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST\nmodel to 88.36% (with $\\ell_\\infty$ perturbations of $\\epsilon = 0.3$) and the\naccuracy of their CIFAR-10 model to 44.03% (at $\\epsilon = 8/255$).\nMultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy\nof their CIFAR-10 model to 53.07% (with $\\ell_\\infty$ perturbations of\n$\\epsilon = 0.031$).\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:08:54 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gowal", "Sven", ""], ["Uesato", "Jonathan", ""], ["Qin", "Chongli", ""], ["Huang", "Po-Sen", ""], ["Mann", "Timothy", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1910.09340", "submitter": "Mohammad Saberian", "authors": "Mohammad Saberian and Pablo Delgado and Yves Raimond", "title": "Gradient Boosted Decision Tree Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method to build a neural network that is similar\nto an ensemble of decision trees. We first illustrate how to convert a learned\nensemble of decision trees to a single neural network with one hidden layer and\nan input transformation. We then relax some properties of this network such as\nthresholds and activation functions to train an approximately equivalent\ndecision tree ensemble. The final model, Hammock, is surprisingly simple: a\nfully connected two layers neural network where the input is quantized and\none-hot encoded. Experiments on large and small datasets show this simple\nmethod can achieve performance similar to that of Gradient Boosted Decision\nTrees.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:21:43 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 22:18:48 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Saberian", "Mohammad", ""], ["Delgado", "Pablo", ""], ["Raimond", "Yves", ""]]}, {"id": "1910.09347", "submitter": "Gabriel Terejanu", "authors": "Asif J. Chowdhury, Gabriel Terejanu", "title": "Approximate Sampling using an Accelerated Metropolis-Hastings based on\n  Bayesian Optimization and Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods have a drawback when working with a\ntarget distribution or likelihood function that is computationally expensive to\nevaluate, specially when working with big data. This paper focuses on\nMetropolis-Hastings (MH) algorithm for unimodal distributions. Here, an\nenhanced MH algorithm is proposed that requires less number of expensive\nfunction evaluations, has shorter burn-in period, and uses a better proposal\ndistribution. The main innovations include the use of Bayesian optimization to\nreach the high probability region quickly, emulating the target distribution\nusing Gaussian processes (GP), and using Laplace approximation of the GP to\nbuild a proposal distribution that captures the underlying correlation better.\nThe experiments show significant improvement over the regular MH. Statistical\ncomparison between the results from two algorithms is presented.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:17:03 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chowdhury", "Asif J.", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1910.09349", "submitter": "Alexander Terenin", "authors": "Steindor Saemundsson, Alexander Terenin, Katja Hofmann, Marc Peter\n  Deisenroth", "title": "Variational Integrator Networks for Physically Structured Embeddings", "comments": null, "journal-ref": "Artificial Intelligence and Statistics, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning workable representations of dynamical systems is becoming an\nincreasingly important problem in a number of application areas. By leveraging\nrecent work connecting deep neural networks to systems of differential\nequations, we propose \\emph{variational integrator networks}, a class of neural\nnetwork architectures designed to preserve the geometric structure of physical\nsystems. This class of network architectures facilitates accurate long-term\nprediction, interpretability, and data-efficient learning, while still\nremaining highly flexible and capable of modeling complex behavior. We\ndemonstrate that they can accurately learn dynamical systems from both noisy\nobservations in phase space and from image pixels within which the unknown\ndynamics are embedded.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:17:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 13:41:51 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Saemundsson", "Steindor", ""], ["Terenin", "Alexander", ""], ["Hofmann", "Katja", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1910.09351", "submitter": "Ming-Chuan Yang", "authors": "Ming-Chuan Yang, Meng Chang Chen", "title": "Theoretical Investigation of Composite Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work theoretically investigates the performance of a composite neural\nnetwork. A composite neural network is a rooted directed acyclic graph\ncombining a set of pre-trained and non-instantiated neural network models,\nwhere a pre-trained neural network model is well-crafted for a specific task\nand targeted to approximate a specific function with instantiated weights. The\nadvantages of adopting such a pre-trained model in a composite neural network\nare two folds. One is to benefit from other's intelligence and diligence, and\nthe other is saving the efforts in data preparation and resources and time in\ntraining. However, the overall performance of composite neural network is still\nnot clear. In this work, we prove that a composite neural network, with high\nprobability, performs better than any of its pre-trained components under\ncertain assumptions. In addition, if an extra pre-trained component is added to\na composite network, with high probability the overall performance will be\nimproved. In the empirical evaluations, distinctively different applications\nsupport the above findings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 10:47:32 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 08:23:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Ming-Chuan", ""], ["Chen", "Meng Chang", ""]]}, {"id": "1910.09356", "submitter": "Ramya Akula", "authors": "Ramya Akula, Ni Nguyen, Ivan Garibay", "title": "Supervised Machine Learning based Ensemble Model for Accurate Prediction\n  of Type 2 Diabetes", "comments": "9 Pages, # Tables and 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the American Diabetes Association(ADA), 30.3 million people in\nthe United States have diabetes, but only 7.2 million may be undiagnosed and\nunaware of their condition. Type 2 diabetes is usually diagnosed for most\npatients later on in life whereas the less common Type 1 diabetes is diagnosed\nearly on in life. People can live healthy and happy lives while living with\ndiabetes, but early detection produces a better overall outcome on most\npatient's health. Thus, to test the accurate prediction of Type 2 diabetes, we\nuse the patients' information from an electronic health records company called\nPractice Fusion, which has about 10,000 patient records from 2009 to 2012. This\ndata contains individual key biometrics, including age, diastolic and systolic\nblood pressure, gender, height, and weight. We use this data on popular machine\nlearning algorithms and for each algorithm, we evaluate the performance of\nevery model based on their classification accuracy, precision, sensitivity,\nspecificity/recall, negative predictive value, and F1 score. In our study, we\nfind that all algorithms other than Naive Bayes suffered from very low\nprecision. Hence, we take a step further and incorporate all the algorithms\ninto a weighted average or soft voting ensemble model where each algorithm will\ncount towards a majority vote towards the decision outcome of whether a patient\nhas diabetes or not. The accuracy of the Ensemble model on Practice Fusion is\n85\\%, by far our ensemble approach is new in this space. We firmly believe that\nthe weighted average ensemble model not only performed well in overall metrics\nbut also helped to recover wrong predictions and aid in accurate prediction of\nType 2 diabetes. Our accurate novel model can be used as an alert for the\npatients to seek medical evaluation in time.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:30:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Akula", "Ramya", ""], ["Nguyen", "Ni", ""], ["Garibay", "Ivan", ""]]}, {"id": "1910.09357", "submitter": "Di Chen", "authors": "Di Chen, Yada Zhu, Xiaodong Cui, Carla P. Gomes", "title": "Task-Based Learning via Task-Oriented Prediction Network with\n  Applications in Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications often involve domain-specific and task-based\nperformance objectives that are not captured by the standard machine learning\nlosses, but are critical for decision making. A key challenge for direct\nintegration of more meaningful domain and task-based evaluation criteria into\nan end-to-end gradient-based training process is the fact that often such\nperformance objectives are not necessarily differentiable and may even require\nadditional decision-making optimization processing. We propose the\nTask-Oriented Prediction Network (TOPNet), an end-to-end learning scheme that\nautomatically integrates task-based evaluation criteria into the learning\nprocess via a learnable surrogate loss function, which directly guides the\nmodel towards the task-based goal. A major benefit of the proposed TOPNet\nlearning scheme lies in its capability of automatically integrating\nnon-differentiable evaluation criteria, which makes it particularly suitable\nfor diversified and customized task-based evaluation criteria in real-world\ntasks. We validate the performance of TOPNet on two real-world financial\nprediction tasks, revenue surprise forecasting and credit risk modeling. The\nexperimental results demonstrate that TOPNet significantly outperforms both\ntraditional modeling with standard losses and modeling with hand-crafted\nheuristic differentiable surrogate losses.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 22:58:56 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 18:04:05 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 02:58:55 GMT"}, {"version": "v4", "created": "Fri, 26 Jun 2020 17:30:20 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Chen", "Di", ""], ["Zhu", "Yada", ""], ["Cui", "Xiaodong", ""], ["Gomes", "Carla P.", ""]]}, {"id": "1910.09358", "submitter": "Homayun Afrabandpey", "authors": "Homayun Afrabandpey, Tomi Peltola, Juho Piironen, Aki Vehtari and\n  Samuel Kaski", "title": "A Decision-Theoretic Approach for Model Interpretability in Bayesian\n  Framework", "comments": "This version contains more experiments including a comparison with\n  baseline methods from the literature and complemented some of the existing\n  results in the previous version", "journal-ref": "Machine Learning (2020)", "doi": "10.1007/s10994-020-05901-8", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A salient approach to interpretable machine learning is to restrict modeling\nto simple models. In the Bayesian framework, this can be pursued by restricting\nthe model structure and prior to favor interpretable models. Fundamentally,\nhowever, interpretability is about users' preferences, not the data generation\nmechanism; it is more natural to formulate interpretability as a utility\nfunction. In this work, we propose an interpretability utility, which\nexplicates the trade-off between explanation fidelity and interpretability in\nthe Bayesian framework. The method consists of two steps. First, a reference\nmodel, possibly a black-box Bayesian predictive model which does not compromise\naccuracy, is fitted to the training data. Second, a proxy model from an\ninterpretable model family that best mimics the predictive behaviour of the\nreference model is found by optimizing the interpretability utility function.\nThe approach is model agnostic -- neither the interpretable model nor the\nreference model are restricted to a certain class of models -- and the\noptimization problem can be solved using standard tools. Through experiments on\nreal-word data sets, using decision trees as interpretable models and Bayesian\nadditive regression models as reference models, we show that for the same level\nof interpretability, our approach generates more accurate models than the\nalternative of restricting the prior. We also propose a systematic way to\nmeasure stability of interpretabile models constructed by different\ninterpretability approaches and show that our proposed approach generates more\nstable models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:22:44 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:34:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Afrabandpey", "Homayun", ""], ["Peltola", "Tomi", ""], ["Piironen", "Juho", ""], ["Vehtari", "Aki", ""], ["Kaski", "Samuel", ""]]}, {"id": "1910.09359", "submitter": "Yinan Yu", "authors": "Samuel Scheidegger and Yinan Yu and Tomas McKelvey", "title": "Separable Convolutional Eigen-Filters (SCEF): Building Efficient CNNs\n  Using Redundancy Analysis", "comments": "key words: subspace analysis, convolutional neural networks, FLOPs,\n  number of parameters, depth, depthwise separable convolutions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been widely used in computer\nvision due to its effectiveness. While the high model complexity of CNN enables\nremarkable learning capacity, the large number of trainable parameters comes\nwith a high cost. In addition to the demand of a large amount of resources, the\nhigh complexity of the network can result in a high variance in its\ngeneralization performance from a statistical learning theory perspective. One\nway to reduce the complexity of a network without sacrificing its accuracy is\nto define and identify redundancies in order to remove them. In this work, we\npropose a method to observe and analyze redundancies in the weights of 2D\nconvolutional (Conv2D) filters. From our experiments, we observe that 1) the\nvectorized Conv2D filters exhibit low rank behaviors; 2) the effective ranks of\nthese filters typically decrease when the network goes deeper, and 3) these\neffective ranks are converging over training steps. Inspired by these\nobservations, we propose a new layer called Separable Convolutional\nEigen-Filters (SCEF) as an alternative parameterization to Conv2D filters. A\nSCEF layer can be easily implemented using the depthwise separable convolutions\ntrained with our proposed training strategy. In addition to the decreased\nnumber of trainable parameters by using SCEF, depthwise separable convolutions\nare known to be more computationally efficient compared to Conv2D operations,\nwhich reduces the runtime FLOPs as well. Experiments are conducted on the\nCIFAR-10 and ImageNet datasets by replacing the Conv2D layers with SCEF. The\nresults have shown an increased accuracy using about 2/3 of the original\nparameters and reduce the number of FLOPs to 2/3 of the base net.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:28:28 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 07:50:08 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Scheidegger", "Samuel", ""], ["Yu", "Yinan", ""], ["McKelvey", "Tomas", ""]]}, {"id": "1910.09368", "submitter": "Youssef Mourchid", "authors": "Youssef Mourchid, Benjamin Renoust, Olivier Roupin, Le Van, Hocine\n  Cherifi, Mohammed El Hassouni", "title": "Movienet: A Movie Multilayer Network Model using Visual and Textual\n  Semantic Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering content and stories in movies is one of the most important\nconcepts in multimedia content research studies. Network models have proven to\nbe an efficient choice for this purpose. When an audience watches a movie, they\nusually compare the characters and the relationships between them. For this\nreason, most of the models developed so far are based on social networks\nanalysis. They focus essentially on the characters at play. By analyzing\ncharacters' interactions, we can obtain a broad picture of the narration's\ncontent. Other works have proposed to exploit semantic elements such as scenes,\ndialogues, etc. However, they are always captured from a single facet.\nMotivated by these limitations, we introduce in this work a multilayer network\nmodel to capture the narration of a movie based on its script, its subtitles,\nand the movie content. After introducing the model and the extraction process\nfrom the raw data, we perform a comparative analysis of the whole 6-movie cycle\nof the Star Wars saga. Results demonstrate the effectiveness of the proposed\nframework for video content representation and analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:22:15 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mourchid", "Youssef", ""], ["Renoust", "Benjamin", ""], ["Roupin", "Olivier", ""], ["Van", "Le", ""], ["Cherifi", "Hocine", ""], ["Hassouni", "Mohammed El", ""]]}, {"id": "1910.09370", "submitter": "James Evans", "authors": "Feng Shi and James Evans", "title": "Science and Technology Advance through Surprise", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breakthrough discoveries and inventions involve unexpected combinations of\ncontents including problems, methods, and natural entities, and also diverse\ncontexts such as journals, subfields, and conferences. Drawing on data from\ntens of millions of research papers, patents, and researchers, we construct\nmodels that predict next year's content and context combinations with an AUC of\n95% based on embeddings constructed from high-dimensional stochastic block\nmodels, where the improbability of new combinations itself predicts up to 50%\nof the likelihood that they will gain outsized citations and major awards. Most\nof these breakthroughs occur when problems in one field are unexpectedly solved\nby researchers from a distant other. These findings demonstrate the critical\nrole of surprise in advance, and enable evaluation of scientific institutions\nranging from education and peer review to awards in supporting it.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 14:03:40 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 04:42:42 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Shi", "Feng", ""], ["Evans", "James", ""]]}, {"id": "1910.09373", "submitter": "Minghan Yang", "authors": "Minghan Yang, Andre Milzarek, Zaiwen Wen, Tong Zhang", "title": "A Stochastic Extra-Step Quasi-Newton Method for Nonsmooth Nonconvex\n  Optimization", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel stochastic extra-step quasi-Newton method is developed\nto solve a class of nonsmooth nonconvex composite optimization problems. We\nassume that the gradient of the smooth part of the objective function can only\nbe approximated by stochastic oracles. The proposed method combines general\nstochastic higher order steps derived from an underlying proximal type\nfixed-point equation with additional stochastic proximal gradient steps to\nguarantee convergence. Based on suitable bounds on the step sizes, we establish\nglobal convergence to stationary points in expectation and an extension of the\napproach using variance reduction techniques is discussed. Motivated by\nlarge-scale and big data applications, we investigate a stochastic\ncoordinate-type quasi-Newton scheme that allows to generate cheap and tractable\nstochastic higher order directions. Finally, the proposed algorithm is tested\non large-scale logistic regression and deep learning problems and it is shown\nthat it compares favorably with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:43:02 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yang", "Minghan", ""], ["Milzarek", "Andre", ""], ["Wen", "Zaiwen", ""], ["Zhang", "Tong", ""]]}, {"id": "1910.09383", "submitter": "Sarath Shekkizhar", "authors": "Sarath Shekkizhar and Antonio Ortega", "title": "Graph Construction from Data using Non Negative Kernel regression (NNK\n  Graphs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven graph constructions are often used in various applications,\nincluding several machine learning tasks, where the goal is to make predictions\nand discover patterns. However, learning an optimal graph from data is still a\nchallenging task. Weighted $K$-nearest neighbor and $\\epsilon$-neighborhood\nmethods are among the most common graph construction methods, due to their\ncomputational simplicity but the choice of parameters such as $K$ and\n$\\epsilon$ associated with these methods is often ad hoc and lacks a clear\ninterpretation. We formulate graph construction as the problem of finding a\nsparse signal approximation in kernel space, identifying key similarities\nbetween methods in signal approximation and existing graph learning methods. We\npropose non-negative kernel regression~(NNK), an improved approach for graph\nconstruction with interesting geometric and theoretical properties. We show\nexperimentally the efficiency of NNK graphs, its robustness to choice of\nsparsity $K$ and better performance over state of the art graph methods in semi\nsupervised learning tasks on real world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:58:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shekkizhar", "Sarath", ""], ["Ortega", "Antonio", ""]]}, {"id": "1910.09388", "submitter": "Zhi-Hua Zhou", "authors": "Yu-Jie Zhang and Peng Zhao and Zhi-Hua Zhou", "title": "An Unbiased Risk Estimator for Learning with Augmented Classes", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning with augmented classes (LAC),\nwhere augmented classes unobserved in the training data might emerge in the\ntesting phase. Previous studies generally attempt to discover augmented classes\nby exploiting geometric properties, achieving inspiring empirical performance\nyet lacking theoretical understandings particularly on the generalization\nability. In this paper we show that, by using unlabeled training data to\napproximate the potential distribution of augmented classes, an unbiased risk\nestimator of the testing distribution can be established for the LAC problem\nunder mild assumptions, which paves a way to develop a sound approach with\ntheoretical guarantees. Moreover, the proposed approach can adapt to complex\nchanging environments where augmented classes may appear and the prior of known\nclasses may change simultaneously. Extensive experiments confirm the\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:06:29 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 13:09:31 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhang", "Yu-Jie", ""], ["Zhao", "Peng", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1910.09394", "submitter": "Mehmet A. S\\\"uzen PhD", "authors": "Mehmet S\\\"uzen and Alper Yegenoglu", "title": "Generalised learning of time-series: Ornstein-Uhlenbeck processes", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In machine learning, statistics, econometrics and statistical physics\ncross-validation (CV) is used as a standard approach in quantifying the\ngeneralization performance of a statistical model. In practice, direct usage of\nCV is avoided for time-series due to several issues. A direct application of CV\nin time-series leads to the loss of serial correlations, a requirement of\npreserving any non-stationarity and the prediction of the past data using\nfuture data. In this work, we propose a meta-algorithm called reconstructive\ncross-validation (rCV ) that avoids all these issues. At first, k folds are\nformed with non-overlapping randomly selected subsets of the original\ntime-series. Then, we generate k new partial time-series by removing data\npoints from a given fold: every new partial time-series have missing points at\nrandom from a different entire fold. A suitable imputation or a smoothing\ntechnique is used to reconstruct k time-series. We call these reconstructions\nsecondary models. Thereafter, we build the primary k time-series models using\nnew time-series coming from the secondary models. The performance of the\nprimary models is evaluated simultaneously by computing the deviations from the\noriginally removed data points and out-of-sample (OSS) data. These amounts to\nreconstruction and prediction errors. If the secondary models use a technique\nthat retains the data points exactly, such as Gaussian process regression,\nthere will be no errors present on the data points that are not removed. By\nthis procedure serial correlations are retained, any non-stationarity is\npreserved within models and there will be no prediction of past data using the\nfuture data points. The cross-validation in time-series model can be practised\nwith rCV. Moreover, we can build time-series learning curves by repeating rCV\nprocedure with different k values.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:16:05 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 17:23:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["S\u00fczen", "Mehmet", ""], ["Yegenoglu", "Alper", ""]]}, {"id": "1910.09396", "submitter": "Jiahao Xie", "authors": "Jiahao Xie, Zebang Shen, Chao Zhang, Boyu Wang, Hui Qian", "title": "Efficient Projection-Free Online Methods with Stochastic Recursive\n  Gradient", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on projection-free methods for solving smooth Online\nConvex Optimization (OCO) problems. Existing projection-free methods either\nachieve suboptimal regret bounds or have high per-iteration computational\ncosts. To fill this gap, two efficient projection-free online methods called\nORGFW and MORGFW are proposed for solving stochastic and adversarial OCO\nproblems, respectively. By employing a recursive gradient estimator, our\nmethods achieve optimal regret bounds (up to a logarithmic factor) while\npossessing low per-iteration computational costs. Experimental results\ndemonstrate the efficiency of the proposed methods compared to\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:22:26 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 03:06:01 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Xie", "Jiahao", ""], ["Shen", "Zebang", ""], ["Zhang", "Chao", ""], ["Wang", "Boyu", ""], ["Qian", "Hui", ""]]}, {"id": "1910.09398", "submitter": "Martin Pawelczyk", "authors": "Martin Pawelczyk, Johannes Haug, Klaus Broelemann, Gjergji Kasneci", "title": "Learning Model-Agnostic Counterfactual Explanations for Tabular Data", "comments": "Update version: from Neurips Workshop to WWW publication. In\n  Proceedings of The Web Conference 2020 (WWW 20), April 20-24, 2020, Taipei,\n  Taiwan", "journal-ref": null, "doi": "10.1145/3366423.3380087", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual explanations can be obtained by identifying the smallest\nchange made to a feature vector to qualitatively influence a prediction; for\nexample, from 'loan rejected' to 'awarded' or from 'high risk of cardiovascular\ndisease' to 'low risk'. Previous approaches often emphasized that\ncounterfactuals should be easily interpretable to humans, motivating sparse\nsolutions with few changes to the feature vectors. However, these approaches\nwould not ensure that the produced counterfactuals be proximate (i.e., not\nlocal outliers) and connected to regions with substantial data density (i.e.,\nclose to correctly classified observations), two requirements known as\ncounterfactual faithfulness. These requirements are fundamental when making\nsuggestions to individuals that are indeed attainable. Our contribution is\ntwofold. On one hand, we suggest to complement the catalogue of counterfactual\nquality measures [1] using a criterion to quantify the degree of difficulty for\na certain counterfactual suggestion. On the other hand, drawing ideas from the\nmanifold learning literature, we develop a framework that generates attainable\ncounterfactuals. We suggest the counterfactual conditional heterogeneous\nvariational autoencoder (C-CHVAE) to identify attainable counterfactuals that\nlie within regions of high data density.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:23:13 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 14:14:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Pawelczyk", "Martin", ""], ["Haug", "Johannes", ""], ["Broelemann", "Klaus", ""], ["Kasneci", "Gjergji", ""]]}, {"id": "1910.09417", "submitter": "Amir Emad Marvasti", "authors": "Amir Emad Marvasti, Ehsan Emad Marvasti, Ulas Bagci, Hassan Foroosh", "title": "Maximum Probability Theorem: A Framework for Probabilistic Learning", "comments": "in IEEE Transactions on Artificial Intelligence", "journal-ref": null, "doi": "10.1109/TAI.2021.3086046", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical framework of probabilistic learning derived by\nMaximum Probability (MP) Theorem shown in the current paper. In this\nprobabilistic framework, a model is defined as an event in the probability\nspace, and a model or the associated event -- either the true underlying model\nor the parameterized model -- have a quantified probability measure. This\nquantification of a model's probability measure is derived by the MP Theorem,\nin which we have shown that an event's probability measure has an upper-bound\ngiven its conditional distribution on an arbitrary random variable. Through\nthis alternative framework, the notion of model parameters is encompassed in\nthe definition of the model or the associated event. Therefore, this framework\ndeviates from the conventional approach of assuming a prior on the model\nparameters. Instead, the regularizing effects of assuming prior over parameters\nis seen through maximizing probabilities of models or according to information\ntheory, minimizing the information content of a model. The probability of a\nmodel in our framework is invariant to reparameterization and is solely\ndependent on the model's likelihood function. Also, rather than maximizing the\nposterior in a conventional Bayesian setting, the objective function in our\nalternative framework is defined as the probability of set operations (e.g.\nintersection) on the event of the true underlying model and the event of the\nmodel at hand. Our theoretical framework, as a derivation of MP theorem, adds\nclarity to probabilistic learning through solidifying the definition of\nprobabilistic models, quantifying their probabilities, and providing a visual\nunderstanding of objective functions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:46:05 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 16:03:30 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 19:54:26 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 18:39:08 GMT"}, {"version": "v5", "created": "Mon, 14 Jun 2021 16:18:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Marvasti", "Amir Emad", ""], ["Marvasti", "Ehsan Emad", ""], ["Bagci", "Ulas", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1910.09444", "submitter": "Paul Duckworth", "authors": "Wolfgang Fr\\\"uhwirt and Paul Duckworth", "title": "Towards better healthcare: What could and should be automated?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While artificial intelligence (AI) and other automation technologies might\nlead to enormous progress in healthcare, they may also have undesired\nconsequences for people working in the field. In this interdisciplinary study,\nwe capture empirical evidence of not only what healthcare work could be\nautomated, but also what should be automated. We quantitatively investigate\nthese research questions by utilizing probabilistic machine learning models\ntrained on thousands of ratings, provided by both healthcare practitioners and\nautomation experts. Based on our findings, we present an analytical tool\n(Automatability-Desirability Matrix) to support policymakers and organizational\nleaders in developing practical strategies on how to harness the positive power\nof automation technologies, while accompanying change and empowering\nstakeholders in a participatory fashion.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:23:39 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Fr\u00fchwirt", "Wolfgang", ""], ["Duckworth", "Paul", ""]]}, {"id": "1910.09446", "submitter": "Hyeonwoo Yu", "authors": "Hyeonwoo Yu and Beomhee Lee", "title": "Zero-shot Learning via Simultaneous Generating and Learning", "comments": "To appear in the proceeding of NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the absence of training data for unseen classes, conventional\nzero-shot learning approaches mainly train their model on seen datapoints and\nleverage the semantic descriptions for both seen and unseen classes. Beyond\nexploiting relations between classes of seen and unseen, we present a deep\ngenerative model to provide the model with experience about both seen and\nunseen classes. Based on the variational auto-encoder with class-specific\nmulti-modal prior, the proposed method learns the conditional distribution of\nseen and unseen classes. In order to circumvent the need for samples of unseen\nclasses, we treat the non-existing data as missing examples. That is, our\nnetwork aims to find optimal unseen datapoints and model parameters, by\niteratively following the generating and learning strategy. Since we obtain the\nconditional generative model for both seen and unseen classes, classification\nas well as generation can be performed directly without any off-the-shell\nclassifiers. In experimental results, we demonstrate that the proposed\ngenerating and learning strategy makes the model achieve the outperforming\nresults compared to that trained only on the seen classes, and also to the\nseveral state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:25:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yu", "Hyeonwoo", ""], ["Lee", "Beomhee", ""]]}, {"id": "1910.09451", "submitter": "Mathieu Seurin", "authors": "Geoffrey Cideron, Mathieu Seurin, Florian Strub, and Olivier Pietquin", "title": "HIGhER : Improving instruction following with Hindsight Generation for\n  Experience Replay", "comments": "Accepted at ADPRL'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language creates a compact representation of the world and allows the\ndescription of unlimited situations and objectives through compositionality.\nWhile these characterizations may foster instructing, conditioning or\nstructuring interactive agent behavior, it remains an open-problem to correctly\nrelate language understanding and reinforcement learning in even simple\ninstruction following scenarios. This joint learning problem is alleviated\nthrough expert demonstrations, auxiliary losses, or neural inductive biases. In\nthis paper, we propose an orthogonal approach called Hindsight Generation for\nExperience Replay (HIGhER) that extends the Hindsight Experience Replay (HER)\napproach to the language-conditioned policy setting. Whenever the agent does\nnot fulfill its instruction, HIGhER learns to output a new directive that\nmatches the agent trajectory, and it relabels the episode with a positive\nreward. To do so, HIGhER learns to map a state into an instruction by using\npast successful trajectories, which removes the need to have external expert\ninterventions to relabel episodes as in vanilla HER. We show the efficiency of\nour approach in the BabyAI environment, and demonstrate how it complements\nother instruction following methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:31:29 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 15:36:42 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 16:01:45 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Cideron", "Geoffrey", ""], ["Seurin", "Mathieu", ""], ["Strub", "Florian", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1910.09457", "submitter": "Eyke H\\\"ullermeier", "authors": "Eyke H\\\"ullermeier and Willem Waegeman", "title": "Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction\n  to Concepts and Methods", "comments": "59 pages", "journal-ref": null, "doi": "10.1007/s10994-021-05946-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of uncertainty is of major importance in machine learning and\nconstitutes a key element of machine learning methodology. In line with the\nstatistical tradition, uncertainty has long been perceived as almost synonymous\nwith standard probability and probabilistic predictions. Yet, due to the\nsteadily increasing relevance of machine learning for practical applications\nand related issues such as safety requirements, new problems and challenges\nhave recently been identified by machine learning scholars, and these problems\nmay call for new methodological developments. In particular, this includes the\nimportance of distinguishing between (at least) two different types of\nuncertainty, often referred to as aleatoric and epistemic. In this paper, we\nprovide an introduction to the topic of uncertainty in machine learning as well\nas an overview of attempts so far at handling uncertainty in general and\nformalizing this distinction in particular.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:41:28 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 14:55:47 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 06:34:50 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["H\u00fcllermeier", "Eyke", ""], ["Waegeman", "Willem", ""]]}, {"id": "1910.09464", "submitter": "Yangjun Ruan", "authors": "Yangjun Ruan, Yuanhao Xiong, Sashank Reddi, Sanjiv Kumar, Cho-Jui\n  Hsieh", "title": "Learning to Learn by Zeroth-Order Oracle", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the learning to learn (L2L) framework, we cast the design of optimization\nalgorithms as a machine learning problem and use deep neural networks to learn\nthe update rules. In this paper, we extend the L2L framework to zeroth-order\n(ZO) optimization setting, where no explicit gradient information is available.\nOur learned optimizer, modeled as recurrent neural network (RNN), first\napproximates gradient by ZO gradient estimator and then produces parameter\nupdate utilizing the knowledge of previous iterations. To reduce high variance\neffect due to ZO gradient estimator, we further introduce another RNN to learn\nthe Gaussian sampling rule and dynamically guide the query direction sampling.\nOur learned optimizer outperforms hand-designed algorithms in terms of\nconvergence rate and final solution on both synthetic and practical ZO\noptimization tasks (in particular, the black-box adversarial attack task, which\nis one of the most widely used tasks of ZO optimization). We finally conduct\nextensive analytical experiments to demonstrate the effectiveness of our\nproposed optimizer.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:48:46 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 06:45:00 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Ruan", "Yangjun", ""], ["Xiong", "Yuanhao", ""], ["Reddi", "Sashank", ""], ["Kumar", "Sanjiv", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1910.09466", "submitter": "Giulio Franzese", "authors": "Rosa Candela, Giulio Franzese, Maurizio Filippone, Pietro Michiardi", "title": "Sparsification as a Remedy for Staleness in Distributed Asynchronous SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale machine learning is increasingly relying on distributed\noptimization, whereby several machines contribute to the training process of a\nstatistical model. In this work we study the performance of asynchronous,\ndistributed settings, when applying sparsification, a technique used to reduce\ncommunication overheads. In particular, for the first time in an asynchronous,\nnon-convex setting, we theoretically prove that, in presence of staleness,\nsparsification does not harm SGD performance: the ergodic convergence rate\nmatches the known result of standard SGD, that is $\\mathcal{O} \\left(\n1/\\sqrt{T} \\right)$. We also carry out an empirical study to complement our\ntheory, and confirm that the effects of sparsification on the convergence rate\nare negligible, when compared to 'vanilla' SGD, even in the challenging\nscenario of an asynchronous, distributed system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:51:16 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 15:01:06 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 08:41:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Candela", "Rosa", ""], ["Franzese", "Giulio", ""], ["Filippone", "Maurizio", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1910.09483", "submitter": "Hanbaek Lyu", "authors": "Hanbaek Lyu, Facundo Memoli, and David Sivakoff", "title": "Sampling random graph homomorphisms and applications to network data\n  analysis", "comments": "51 pages, 33 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph homomorphism is a map between two graphs that preserves adjacency\nrelations. We consider the problem of sampling a random graph homomorphism from\na graph $F$ into a large network $\\mathcal{G}$. We propose two complementary\nMCMC algorithms for sampling a random graph homomorphisms and establish bounds\non their mixing times and concentration of their time averages. Based on our\nsampling algorithms, we propose a novel framework for network data analysis\nthat circumvents some of the drawbacks in methods based on independent and\nneigborhood sampling. Various time averages of the MCMC trajectory give us\nvarious computable observables, including well-known ones such as homomorphism\ndensity and average clustering coefficient and their generalizations.\nFurthermore, we show that these network observables are stable with respect to\na suitably renormalized cut distance between networks. We provide various\nexamples and simulations demonstrating our framework through synthetic\nnetworks. We also apply our framework for network clustering and classification\nproblems using the Facebook100 dataset and Word Adjacency Networks of a set of\nclassic novels.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:20:03 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 07:31:57 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Lyu", "Hanbaek", ""], ["Memoli", "Facundo", ""], ["Sivakoff", "David", ""]]}, {"id": "1910.09499", "submitter": "Miaoyan Wang", "authors": "Zhuoyan Xu, Jiaxin Hu, and Miaoyan Wang", "title": "Generalized tensor regression with covariates on multiple modes", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of tensor-response regression given covariates on\nmultiple modes. Such data problems arise frequently in applications such as\nneuroimaging, network analysis, and spatial-temporal modeling. We propose a new\nfamily of tensor response regression models that incorporate covariates, and\nestablish the theoretical accuracy guarantees. Unlike earlier methods, our\nestimation allows high-dimensionality in both the tensor response and the\ncovariate matrices on multiple modes. An efficient alternating updating\nalgorithm is further developed. Our proposal handles a broad range of data\ntypes, including continuous, count, and binary observations. Through simulation\nand applications to two real datasets, we demonstrate the outperformance of our\napproach over the state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:43:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Zhuoyan", ""], ["Hu", "Jiaxin", ""], ["Wang", "Miaoyan", ""]]}, {"id": "1910.09504", "submitter": "Gautier Marti", "authors": "Gautier Marti", "title": "CorrGAN: Sampling Realistic Financial Correlation Matrices Using\n  Generative Adversarial Networks", "comments": null, "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP40776.2020.9053276", "report-no": null, "categories": "q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for sampling realistic financial correlation\nmatrices. This approach is based on generative adversarial networks.\nExperiments demonstrate that generative adversarial networks are able to\nrecover most of the known stylized facts about empirical correlation matrices\nestimated on asset returns. This is the first time such results are documented\nin the literature. Practical financial applications range from trading\nstrategies enhancement to risk and portfolio stress testing. Such generative\nmodels can also help ground empirical finance deeper into science by allowing\nfor falsifiability of statements and more objective comparison of empirical\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:46:55 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 13:50:55 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Marti", "Gautier", ""]]}, {"id": "1910.09505", "submitter": "Frederic Sala", "authors": "Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa,\n  Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James\n  Priest, Christopher R\\'e", "title": "Multi-Resolution Weak Supervision for Sequential Data", "comments": "NeurIPS 2019 (Conference on Neural Information Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since manually labeling training data is slow and expensive, recent\nindustrial and scientific research efforts have turned to weaker or noisier\nforms of supervision sources. However, existing weak supervision approaches\nfail to model multi-resolution sources for sequential data, like video, that\ncan assign labels to individual elements or collections of elements in a\nsequence. A key challenge in weak supervision is estimating the unknown\naccuracies and correlations of these sources without using labeled data.\nMulti-resolution sources exacerbate this challenge due to complex correlations\nand sample complexity that scales in the length of the sequence. We propose\nDugong, the first framework to model multi-resolution weak supervision sources\nwith complex correlations to assign probabilistic labels to training data.\nTheoretically, we prove that Dugong, under mild conditions, can uniquely\nrecover the unobserved accuracy and correlation parameters and use parameter\nsharing to improve sample complexity. Our method assigns clinician-validated\nlabels to population-scale biomedical video repositories, helping outperform\ntraditional supervision by 36.8 F1 points and addressing a key use case where\nmachine learning has been severely limited by the lack of expert labeled data.\nOn average, Dugong improves over traditional supervision by 16.0 F1 points and\nexisting weak supervision approaches by 24.2 F1 points across several video and\nsensor classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:48:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sala", "Frederic", ""], ["Varma", "Paroma", ""], ["Fries", "Jason", ""], ["Fu", "Daniel Y.", ""], ["Sagawa", "Shiori", ""], ["Khattar", "Saelig", ""], ["Ramamoorthy", "Ashwini", ""], ["Xiao", "Ke", ""], ["Fatahalian", "Kayvon", ""], ["Priest", "James", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1910.09527", "submitter": "Jan Kudlicka", "authors": "Jan Kudlicka and Lawrence M. Murray and Thomas B. Sch\\\"on and Fredrik\n  Lindsten", "title": "Particle filter with rejection control and unbiased estimator of the\n  marginal likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the combined use of resampling and partial rejection control in\nsequential Monte Carlo methods, also known as particle filters. While the\nvariance reducing properties of rejection control are known, there has not been\n(to the best of our knowledge) any work on unbiased estimation of the marginal\nlikelihood (also known as the model evidence or the normalizing constant) in\nthis type of particle filter. Being able to estimate the marginal likelihood\nwithout bias is highly relevant for model comparison, computation of\ninterpretable and reliable confidence intervals, and in exact approximation\nmethods, such as particle Markov chain Monte Carlo. In the paper we present a\nparticle filter with rejection control that enables unbiased estimation of the\nmarginal likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:47:22 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 15:54:31 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kudlicka", "Jan", ""], ["Murray", "Lawrence M.", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1910.09529", "submitter": "Yura Malitsky", "authors": "Yura Malitsky, Konstantin Mishchenko", "title": "Adaptive Gradient Descent without Descent", "comments": null, "journal-ref": "Proceedings of the 37-th International Conference on Machine\n  Learning, Online, PMLR 119, 2020", "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a strikingly simple proof that two rules are sufficient to\nautomate gradient descent: 1) don't increase the stepsize too fast and 2) don't\noverstep the local curvature. No need for functional values, no line search, no\ninformation about the function except for the gradients. By following these\nrules, you get a method adaptive to the local geometry, with convergence\nguarantees depending only on the smoothness in a neighborhood of a solution.\nGiven that the problem is convex, our method converges even if the global\nsmoothness constant is infinity. As an illustration, it can minimize arbitrary\ncontinuously twice-differentiable convex function. We examine its performance\non a range of convex and nonconvex problems, including logistic regression and\nmatrix factorization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:49:29 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 20:00:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Malitsky", "Yura", ""], ["Mishchenko", "Konstantin", ""]]}, {"id": "1910.09570", "submitter": "Joseph Paul Cohen", "authors": "Shawn Tan and Guillaume Androz and Ahmad Chamseddine and Pierre\n  Fecteau and Aaron Courville and Yoshua Bengio and Joseph Paul Cohen", "title": "Icentia11K: An Unsupervised Representation Learning Dataset for\n  Arrhythmia Subtype Discovery", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release the largest public ECG dataset of continuous raw signals for\nrepresentation learning containing 11 thousand patients and 2 billion labelled\nbeats. Our goal is to enable semi-supervised ECG models to be made as well as\nto discover unknown subtypes of arrhythmia and anomalous ECG signal events. To\nthis end, we propose an unsupervised representation learning task, evaluated in\na semi-supervised fashion. We provide a set of baselines for different feature\nextractors that can be built upon. Additionally, we perform qualitative\nevaluations on results from PCA embeddings, where we identify some clustering\nof known subtypes indicating the potential for representation learning in\narrhythmia sub-type discovery.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:02:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Tan", "Shawn", ""], ["Androz", "Guillaume", ""], ["Chamseddine", "Ahmad", ""], ["Fecteau", "Pierre", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1910.09573", "submitter": "David Madras", "authors": "David Madras, James Atwood, Alex D'Amour", "title": "Detecting Extrapolation with Local Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present local ensembles, a method for detecting extrapolation at test time\nin a pre-trained model. We focus on underdetermination as a key component of\nextrapolation: we aim to detect when many possible predictions are consistent\nwith the training data and model class. Our method uses local second-order\ninformation to approximate the variance of predictions across an ensemble of\nmodels from the same class. We compute this approximation by estimating the\nnorm of the component of a test point's gradient that aligns with the\nlow-curvature directions of the Hessian, and provide a tractable method for\nestimating this quantity. Experimentally, we show that our method is capable of\ndetecting when a pre-trained model is extrapolating on test data, with\napplications to out-of-distribution detection, detecting spurious correlates,\nand active learning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:05:52 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Madras", "David", ""], ["Atwood", "James", ""], ["D'Amour", "Alex", ""]]}, {"id": "1910.09578", "submitter": "Zhe Dong", "authors": "Zhe Dong, Deniz Oktay, Ben Poole, Alexander A. Alemi", "title": "On Predictive Information in RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain biological neurons demonstrate a remarkable capability to optimally\ncompress the history of sensory inputs while being maximally informative about\nthe future. In this work, we investigate if the same can be said of artificial\nneurons in recurrent neural networks (RNNs) trained with maximum likelihood.\nEmpirically, we find that RNNs are suboptimal in the information plane. Instead\nof optimally compressing past information, they extract additional information\nthat is not relevant for predicting the future. We show that constraining past\ninformation by injecting noise into the hidden state can improve RNNs in\nseveral ways: optimality in the predictive information plane, sample quality,\nheldout likelihood, and downstream classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:12:43 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 20:53:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Dong", "Zhe", ""], ["Oktay", "Deniz", ""], ["Poole", "Ben", ""], ["Alemi", "Alexander A.", ""]]}, {"id": "1910.09588", "submitter": "Zhe Dong", "authors": "Zhe Dong, Bryan A. Seybold, Kevin P. Murphy, Hung H. Bui", "title": "Collapsed Amortized Variational Inference for Switching Nonlinear\n  Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient inference method for switching nonlinear dynamical\nsystems. The key idea is to learn an inference network which can be used as a\nproposal distribution for the continuous latent variables, while performing\nexact marginalization of the discrete latent variables. This allows us to use\nthe reparameterization trick, and apply end-to-end training with stochastic\ngradient descent. We show that the proposed method can successfully segment\ntime series data, including videos and 3D human pose, into meaningful\n``regimes'' by using the piece-wise nonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:28:10 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 20:02:27 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Dong", "Zhe", ""], ["Seybold", "Bryan A.", ""], ["Murphy", "Kevin P.", ""], ["Bui", "Hung H.", ""]]}, {"id": "1910.09589", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Dimitris Berberidis, Georgios B. Giannakis", "title": "GraphSAC: Detecting anomalies in large-scale graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph-based sampling and consensus (GraphSAC) approach is introduced to\neffectively detect anomalous nodes in large-scale graphs. Existing approaches\nrely on connectivity and attributes of all nodes to assign an anomaly score per\nnode. However, nodal attributes and network links might be compromised by\nadversaries, rendering these holistic approaches vulnerable. Alleviating this\nlimitation, GraphSAC randomly draws subsets of nodes, and relies on graph-aware\ncriteria to judiciously filter out sets contaminated by anomalous nodes, before\nemploying a semi-supervised learning (SSL) module to estimate nominal label\ndistributions per node. These learned nominal distributions are minimally\naffected by the anomalous nodes, and hence can be directly adopted for anomaly\ndetection. Rigorous analysis provides performance guarantees for GraphSAC, by\nbounding the required number of draws. The per-draw complexity grows linearly\nwith the number of edges, which implies efficient SSL, while draws can be run\nin parallel, thereby ensuring scalability to large graphs. GraphSAC is tested\nunder different anomaly generation models based on random walks, clustered\nanomalies, as well as contemporary adversarial attacks for graph data.\nExperiments with real-world graphs showcase the advantage of GraphSAC relative\nto state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:30:03 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1910.09590", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis and Georgios B. Giannakis", "title": "Edge Dithering for Robust Adaptive Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) are vulnerable to perturbations of the\ngraph structure that are either random, or, adversarially designed. The\nperturbed links modify the graph neighborhoods, which critically affects the\nperformance of GCNs in semi-supervised learning (SSL) tasks. Aiming at\nrobustifying GCNs conditioned on the perturbed graph, the present paper\ngenerates multiple auxiliary graphs, each having its binary 0-1 edge weights\nflip values with probabilities designed to enhance robustness. The resultant\nedge-dithered auxiliary graphs are leveraged by an adaptive (A)GCN that\nperforms SSL. Robustness is enabled through learnable graph-combining weights\nalong with suitable regularizers. Relative to GCN, the novel AGCN achieves\nmarkedly improved performance in tests with noisy inputs, graph perturbations,\nand state-of-the-art adversarial attacks. Further experiments with protein\ninteraction networks showcase the competitive performance of AGCN for SSL over\nmultiple graphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:30:11 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1910.09594", "submitter": "Nicolas Skatchkovsky", "authors": "Nicolas Skatchkovsky, Hyeryung Jang, and Osvaldo Simeone", "title": "Federated Neuromorphic Learning of Spiking Neural Networks for Low-Power\n  Edge Intelligence", "comments": "submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) offer a promising alternative to conventional\nArtificial Neural Networks (ANNs) for the implementation of on-device low-power\nonline learning and inference. On-device training is, however, constrained by\nthe limited amount of data available at each device. In this paper, we propose\nto mitigate this problem via cooperative training through Federated Learning\n(FL). To this end, we introduce an online FL-based learning rule for networked\non-device SNNs, which we refer to as FL-SNN. FL-SNN leverages local feedback\nsignals within each SNN, in lieu of backpropagation, and global feedback\nthrough communication via a base station. The scheme demonstrates significant\nadvantages over separate training and features a flexible trade-off between\ncommunication load and accuracy via the selective exchange of synaptic weights.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:36:17 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Skatchkovsky", "Nicolas", ""], ["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "1910.09599", "submitter": "Johannes M\\\"uller", "authors": "Johannes M\\\"uller", "title": "On the space-time expressivity of ResNets", "comments": "Extended abstract of master's thesis; presented at the ICLR 2020\n  Workshop on Integration of Deep Neural Models and Differential Equations;\n  full version of the thesis available under\n  https://freidok.uni-freiburg.de/data/151788", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) are a deep learning architecture that\nsubstantially improved the state of the art performance in certain supervised\nlearning tasks. Since then, they have received continuously growing attention.\nResNets have a recursive structure $x_{k+1} = x_k + R_k(x_k)$ where $R_k$ is a\nneural network called a residual block. This structure can be seen as the Euler\ndiscretisation of an associated ordinary differential equation (ODE) which is\ncalled a neural ODE. Recently, ResNets were proposed as the space-time\napproximation of ODEs which are not of this neural type. To elaborate this\nconnection we show that by increasing the number of residual blocks as well as\ntheir expressivity the solution of an arbitrary ODE can be approximated in\nspace and time simultaneously by deep ReLU ResNets. Further, we derive\nestimates on the complexity of the residual blocks required to obtain a\nprescribed accuracy under certain regularity assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:43:50 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 18:55:45 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 18:06:36 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 20:58:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["M\u00fcller", "Johannes", ""]]}, {"id": "1910.09615", "submitter": "Yongshuai Liu", "authors": "Yongshuai Liu, Jiaxin Ding, Xin Liu", "title": "IPO: Interior-point Policy Optimization under Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study reinforcement learning (RL) algorithms to solve\nreal-world decision problems with the objective of maximizing the long-term\nreward as well as satisfying cumulative constraints. We propose a novel\nfirst-order policy optimization method, Interior-point Policy Optimization\n(IPO), which augments the objective with logarithmic barrier functions,\ninspired by the interior-point method. Our proposed method is easy to implement\nwith performance guarantees and can handle general types of cumulative\nmulticonstraint settings. We conduct extensive evaluations to compare our\napproach with state-of-the-art baselines. Our algorithm outperforms the\nbaseline algorithms, in terms of reward maximization and constraint\nsatisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:22:14 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Liu", "Yongshuai", ""], ["Ding", "Jiaxin", ""], ["Liu", "Xin", ""]]}, {"id": "1910.09620", "submitter": "Yunkai Zhang", "authors": "Yunkai Zhang, Qiao Jiang, Shurui Li, Xiaoyong Jin, Xueying Ma, Xifeng\n  Yan", "title": "You May Not Need Order in Time Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting with limited data is a challenging yet critical task.\nWhile transformers have achieved outstanding performances in time series\nforecasting, they often require many training samples due to the large number\nof trainable parameters. In this paper, we propose a training technique for\ntransformers that prepares the training windows through random sampling. As\ninput time steps need not be consecutive, the number of distinct samples\nincreases from linearly to combinatorially many. By breaking the temporal\norder, this technique also helps transformers to capture dependencies among\ntime steps in finer granularity. We achieve competitive results compared to the\nstate-of-the-art on real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:28:24 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhang", "Yunkai", ""], ["Jiang", "Qiao", ""], ["Li", "Shurui", ""], ["Jin", "Xiaoyong", ""], ["Ma", "Xueying", ""], ["Yan", "Xifeng", ""]]}, {"id": "1910.09626", "submitter": "Abhishek Panigrahi", "authors": "Abhishek Panigrahi, Raghav Somani, Navin Goyal, Praneeth Netrapalli", "title": "Non-Gaussianity of Stochastic Gradient Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What enables Stochastic Gradient Descent (SGD) to achieve better\ngeneralization than Gradient Descent (GD) in Neural Network training? This\nquestion has attracted much attention. In this paper, we study the distribution\nof the Stochastic Gradient Noise (SGN) vectors during the training. We observe\nthat for batch sizes 256 and above, the distribution is best described as\nGaussian at-least in the early phases of training. This holds across data-sets,\narchitectures, and other choices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:42:14 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 17:35:46 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Panigrahi", "Abhishek", ""], ["Somani", "Raghav", ""], ["Goyal", "Navin", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1910.09645", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Markov Random Fields for Collaborative Filtering", "comments": "9 pages", "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we model the dependencies among the items that are recommended\nto a user in a collaborative-filtering problem via a Gaussian Markov Random\nField (MRF). We build upon Besag's auto-normal parameterization and\npseudo-likelihood, which not only enables computationally efficient learning,\nbut also connects the areas of MRFs and sparse inverse covariance estimation\nwith autoencoders and neighborhood models, two successful approaches in\ncollaborative filtering. We propose a novel approximation for learning sparse\nMRFs, where the trade-off between recommendation-accuracy and training-time can\nbe controlled. At only a small fraction of the training-time compared to\nvarious baselines, including deep nonlinear models, the proposed approach\nachieved competitive ranking-accuracy on all three well-known data-sets used in\nour experiments, and notably a 20% gain in accuracy on the data-set with the\nlargest number of items.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:46:42 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1910.09652", "submitter": "Michael Arbel", "authors": "Michael Arbel and Arthur Gretton and Wuchen Li and Guido Montufar", "title": "Kernelized Wasserstein Natural Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems can be expressed as the optimization of some\ncost functional over a parametric family of probability distributions. It is\noften beneficial to solve such optimization problems using natural gradient\nmethods. These methods are invariant to the parametrization of the family, and\nthus can yield more effective optimization. Unfortunately, computing the\nnatural gradient is challenging as it requires inverting a high dimensional\nmatrix at each iteration. We propose a general framework to approximate the\nnatural gradient for the Wasserstein metric, by leveraging a dual formulation\nof the metric restricted to a Reproducing Kernel Hilbert Space. Our approach\nleads to an estimator for gradient direction that can trade-off accuracy and\ncomputational cost, with theoretical guarantees. We verify its accuracy on\nsimple examples, and show the advantage of using such an estimator in\nclassification tasks on Cifar10 and Cifar100 empirically.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:03:28 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 18:36:47 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 23:42:42 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 10:48:41 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Arbel", "Michael", ""], ["Gretton", "Arthur", ""], ["Li", "Wuchen", ""], ["Montufar", "Guido", ""]]}, {"id": "1910.09655", "submitter": "Fernando Gama", "authors": "Fernando Gama, Joan Bruna, Alejandro Ribeiro", "title": "Stability of Graph Neural Networks to Relative Perturbations", "comments": "Submitted to Int. Conf. on Acoustics, Speech and Signal Processing\n  (ICASSP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs), consisting of a cascade of layers applying a\ngraph convolution followed by a pointwise nonlinearity, have become a powerful\narchitecture to process signals supported on graphs. Graph convolutions (and\nthus, GNNs), rely heavily on knowledge of the graph for operation. However, in\nmany practical cases the GSO is not known and needs to be estimated, or might\nchange from training time to testing time. In this paper, we are set to study\nthe effect that a change in the underlying graph topology that supports the\nsignal has on the output of a GNN. We prove that graph convolutions with\nintegral Lipschitz filters lead to GNNs whose output change is bounded by the\nsize of the relative change in the topology. Furthermore, we leverage this\nresult to show that the main reason for the success of GNNs is that they are\nstable architectures capable of discriminating features on high eigenvalues,\nwhich is a feat that cannot be achieved by linear graph filters (which are\neither stable or discriminative, but cannot be both). Finally, we comment on\nthe use of this result to train GNNs with increased stability and run\nexperiments on movie recommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:03:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gama", "Fernando", ""], ["Bruna", "Joan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1910.09657", "submitter": "Gege Wen", "authors": "Gege Wen and Meng Tang and Sally M. Benson", "title": "Multiphase flow prediction with deep neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijggc.2020.103223", "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep neural network approach for predicting multiphase\nflow in heterogeneous domains with high computational efficiency. The deep\nneural network model is able to handle permeability heterogeneity in high\ndimensional systems, and can learn the interplay of viscous, gravity, and\ncapillary forces from small data sets. Using the example of carbon dioxide\n(CO2) storage, we demonstrate that the model can generate highly accurate\npredictions of a CO2 saturation distribution given a permeability field,\ninjection duration, injection rate, and injection location. The trained neural\nnetwork model has an excellent ability to interpolate and to a limited extent,\nthe ability to extrapolate beyond the training data ranges. To improve the\nprediction accuracy when the neural network model needs to extrapolate, we\npropose a transfer learning (fine-tuning) procedure that can quickly teach the\nneural network model new information without going through massive data\ncollection and retraining. Based on this trained neural network model, a\nweb-based tool is provided that allows users to perform CO2-water multiphase\nflow calculations online. With the tools provided in this paper, the deep\nneural network approach can provide a computationally efficient substitute for\nrepetitive forward multiphase flow simulations, which can be adopted to the\ncontext of history matching and uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:06:55 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wen", "Gege", ""], ["Tang", "Meng", ""], ["Benson", "Sally M.", ""]]}, {"id": "1910.09670", "submitter": "Kaiyi Ji", "authors": "Kaiyi Ji, Zhe Wang, Bowen Weng, Yi Zhou, Wei Zhang and Yingbin Liang", "title": "History-Gradient Aided Batch Size Adaptation for Variance Reduced\n  Algorithms", "comments": "46 pages, 23 figures; Published in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance-reduced algorithms, although achieve great theoretical performance,\ncan run slowly in practice due to the periodic gradient estimation with a large\nbatch of data. Batch-size adaptation thus arises as a promising approach to\naccelerate such algorithms. However, existing schemes either apply prescribed\nbatch-size adaption rule or exploit the information along optimization path via\nadditional backtracking and condition verification steps. In this paper, we\npropose a novel scheme, which eliminates backtracking line search but still\nexploits the information along optimization path by adapting the batch size via\nhistory stochastic gradients. We further theoretically show that such a scheme\nsubstantially reduces the overall complexity for popular variance-reduced\nalgorithms SVRG and SARAH/SPIDER for both conventional nonconvex optimization\nand reinforcement learning problems. To this end, we develop a new convergence\nanalysis framework to handle the dependence of the batch size on history\nstochastic gradients. Extensive experiments validate the effectiveness of the\nproposed batch-size adaptation scheme.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:58:07 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 21:41:27 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 18:53:30 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 03:12:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ji", "Kaiyi", ""], ["Wang", "Zhe", ""], ["Weng", "Bowen", ""], ["Zhou", "Yi", ""], ["Zhang", "Wei", ""], ["Liang", "Yingbin", ""]]}, {"id": "1910.09687", "submitter": "Shengye Wang", "authors": "Shengye Wang, Li Wan, Yang Yu, Ignacio Lopez Moreno", "title": "Signal Combination for Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google's multilingual speech recognition system combines low-level acoustic\nsignals with language-specific recognizer signals to better predict the\nlanguage of an utterance. This paper presents our experience with different\nsignal combination methods to improve overall language identification accuracy.\nWe compare the performance of a lattice-based ensemble model and a deep neural\nnetwork model to combine signals from recognizers with that of a baseline that\nonly uses low-level acoustic signals. Experimental results show that the deep\nneural network model outperforms the lattice-based ensemble model, and it\nreduced the error rate from 5.5% in the baseline to 4.3%, which is a 21.8%\nrelative reduction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:00:47 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 17:34:01 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wang", "Shengye", ""], ["Wan", "Li", ""], ["Yu", "Yang", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "1910.09688", "submitter": "Benson Chen", "authors": "Benson Chen, Tianxiao Shen, Tommi S. Jaakkola, Regina Barzilay", "title": "Learning to Make Generalizable and Diverse Predictions for\n  Retrosynthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for making generalizable and diverse retrosynthetic\nreaction predictions. Given a target compound, the task is to predict the\nlikely chemical reactants to produce the target. This generative task can be\nframed as a sequence-to-sequence problem by using the SMILES representations of\nthe molecules. Building on top of the popular Transformer architecture, we\npropose two novel pre-training methods that construct relevant auxiliary tasks\n(plausible reactions) for our problem. Furthermore, we incorporate a discrete\nlatent variable model into the architecture to encourage the model to produce a\ndiverse set of alternative predictions. On the 50k subset of reaction examples\nfrom the United States patent literature (USPTO-50k) benchmark dataset, our\nmodel greatly improves performance over the baseline, while also generating\npredictions that are more diverse.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:03:21 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chen", "Benson", ""], ["Shen", "Tianxiao", ""], ["Jaakkola", "Tommi S.", ""], ["Barzilay", "Regina", ""]]}, {"id": "1910.09701", "submitter": "Boxin Zhao", "authors": "Boxin Zhao, Y. Samuel Wang, Mladen Kolar", "title": "Direct Estimation of Differential Functional Graphical Models", "comments": "21 pages, 3 figures, to be published in NeurIPS 2019; added link to\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the difference between two functional\nundirected graphical models with shared structures. In many applications, data\nare naturally regarded as high-dimensional random function vectors rather than\nmultivariate scalars. For example, electroencephalography (EEG) data are more\nappropriately treated as functions of time. In these problems, not only can the\nnumber of functions measured per sample be large, but each function is itself\nan infinite dimensional object, making estimation of model parameters\nchallenging. We develop a method that directly estimates the difference of\ngraphs, avoiding separate estimation of each graph, and show it is consistent\nin certain high-dimensional settings. We illustrate finite sample properties of\nour method through simulation studies. Finally, we apply our method to EEG data\nto uncover differences in functional brain connectivity between alcoholics and\ncontrol subjects.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:05:44 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 16:53:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhao", "Boxin", ""], ["Wang", "Y. Samuel", ""], ["Kolar", "Mladen", ""]]}, {"id": "1910.09706", "submitter": "Uchenna Akujuobi", "authors": "Uchenna Akujuobi, Han Yufei, Qiannan Zhang, Xiangliang Zhang", "title": "Collaborative Graph Walk for Semi-supervised Multi-Label Node\n  Classification", "comments": "Accepted for IEEE International Conference on Data Mining (ICDM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study semi-supervised multi-label node classification\nproblem in attributed graphs. Classic solutions to multi-label node\nclassification follow two steps, first learn node embedding and then build a\nnode classifier on the learned embedding. To improve the discriminating power\nof the node embedding, we propose a novel collaborative graph walk, named\nMulti-Label-Graph-Walk, to finely tune node representations with the available\nlabel assignments in attributed graphs via reinforcement learning. The proposed\nmethod formulates the multi-label node classification task as simultaneous\ngraph walks conducted by multiple label-specific agents. Furthermore, policies\nof the label-wise graph walks are learned in a cooperative way to capture first\nthe predictive relation between node labels and structural attributes of\ngraphs; and second, the correlation among the multiple label-specific\nclassification tasks. A comprehensive experimental study demonstrates that the\nproposed method can achieve significantly better multi-label classification\nperformance than the state-of-the-art approaches and conduct more efficient\ngraph exploration.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:20:47 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 21:30:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Akujuobi", "Uchenna", ""], ["Yufei", "Han", ""], ["Zhang", "Qiannan", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1910.09714", "submitter": "Ahmadreza Momeni", "authors": "Yonatan Gur, Ahmadreza Momeni, Stefan Wager", "title": "Smoothness-Adaptive Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a non-parametric multi-armed bandit problem with stochastic\ncovariates, where a key complexity driver is the smoothness of payoff functions\nwith respect to covariates. Previous studies have focused on deriving\nminimax-optimal algorithms in cases where it is a priori known how smooth the\npayoff functions are. In practice, however, the smoothness of payoff functions\nis typically not known in advance, and misspecification of smoothness may\nseverely deteriorate the performance of existing methods. In this work, we\nconsider a framework where the smoothness of payoff functions is not known, and\nstudy when and how algorithms may adapt to unknown smoothness. First, we\nestablish that designing algorithms that adapt to unknown smoothness of payoff\nfunctions is, in general, impossible. However, under a self-similarity\ncondition (which does not reduce the minimax complexity of the dynamic\noptimization problem at hand), we establish that adapting to unknown smoothness\nis possible, and further devise a general policy for achieving\nsmoothness-adaptive performance. Our policy infers the smoothness of payoffs\nthroughout the decision-making process, while leveraging the structure of\nnon-adaptive off-the-shelf policies. We establish that for problem settings\nwith either differentiable or non-differentiable payoff functions this policy\nmatches (up to a logarithmic scale) the regret rate that is achievable when the\nsmoothness of payoffs is known a priori.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:57:55 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 01:12:19 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:22:14 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 18:30:25 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Gur", "Yonatan", ""], ["Momeni", "Ahmadreza", ""], ["Wager", "Stefan", ""]]}, {"id": "1910.09715", "submitter": "David Heckerman", "authors": "David Heckerman and Chris Meek", "title": "Embedded Bayesian Network Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": "Microsoft Research Technical Report MS-TR-97-06, March 1997", "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional probability models for local distribution functions in a\nBayesian network include decision trees, decision graphs, and causal\nindependence models. We describe a new probability model for discrete Bayesian\nnetworks, which we call an embedded Bayesian network classifier or EBNC. The\nmodel for a node $Y$ given parents $\\bf X$ is obtained from a (usually\ndifferent) Bayesian network for $Y$ and $\\bf X$ in which $\\bf X$ need not be\nthe parents of $Y$. We show that an EBNC is a special case of a softmax\npolynomial regression model. Also, we show how to identify a non-redundant set\nof parameters for an EBNC, and describe an asymptotic approximation for\nlearning the structure of Bayesian networks that contain EBNCs. Unlike the\ndecision tree, decision graph, and causal independence models, we are unaware\nof a semantic justification for the use of these models. Experiments are needed\nto determine whether the models presented in this paper are useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:02:51 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Heckerman", "David", ""], ["Meek", "Chris", ""]]}, {"id": "1910.09716", "submitter": "Mohammad Sadegh Norouzzadeh", "authors": "Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery, Neel Joshi,\n  Nebojsa Jojic, and Jeff Clune", "title": "A deep active learning system for species identification and counting in\n  camera trap images", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity conservation depends on accurate, up-to-date information about\nwildlife population distributions. Motion-activated cameras, also known as\ncamera traps, are a critical tool for population surveys, as they are cheap and\nnon-intrusive. However, extracting useful information from camera trap images\nis a cumbersome process: a typical camera trap survey may produce millions of\nimages that require slow, expensive manual review. Consequently, critical\ninformation is often lost due to resource limitations, and critical\nconservation questions may be answered too slowly to support decision-making.\nComputer vision is poised to dramatically increase efficiency in image-based\nbiodiversity surveys, and recent studies have harnessed deep learning\ntechniques for automatic information extraction from camera trap images.\nHowever, the accuracy of results depends on the amount, quality, and diversity\nof the data available to train models, and the literature has focused on\nprojects with millions of relevant, labeled training images. Many camera trap\nprojects do not have a large set of labeled images and hence cannot benefit\nfrom existing machine learning techniques. Furthermore, even projects that do\nhave labeled data from similar ecosystems have struggled to adopt deep learning\nmethods because image classification models overfit to specific image\nbackgrounds (i.e., camera locations). In this paper, we focus not on automating\nthe labeling of camera trap images, but on accelerating this process. We\ncombine the power of machine intelligence and human intelligence to build a\nscalable, fast, and accurate active learning system to minimize the manual work\nrequired to identify and count animals in camera trap images. Our proposed\nscheme can match the state of the art accuracy on a 3.2 million image dataset\nwith as few as 14,100 manual labels, which means decreasing manual labeling\neffort by over 99.5%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:03:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Norouzzadeh", "Mohammad Sadegh", ""], ["Morris", "Dan", ""], ["Beery", "Sara", ""], ["Joshi", "Neel", ""], ["Jojic", "Nebojsa", ""], ["Clune", "Jeff", ""]]}, {"id": "1910.09719", "submitter": "Boonserm Kijsirikul", "authors": "Panayu Keelawat, Nattapong Thammasan, Masayuki Numao, and Boonserm\n  Kijsirikul", "title": "Spatiotemporal Emotion Recognition using Deep CNN Based on EEG during\n  Music Listening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition based on EEG has become an active research area. As one\nof the machine learning models, CNN has been utilized to solve diverse problems\nincluding issues in this domain. In this work, a study of CNN and its\nspatiotemporal feature extraction has been conducted in order to explore\ncapabilities of the model in varied window sizes and electrode orders. Our\ninvestigation was conducted in subject-independent fashion. Results have shown\nthat temporal information in distinct window sizes significantly affects\nrecognition performance in both 10-fold and leave-one-subject-out cross\nvalidation. Spatial information from varying electrode order has modicum effect\non classification. SVM classifier depending on spatiotemporal knowledge on the\nsame dataset was previously employed and compared to these empirical results.\nEven though CNN and SVM have a homologous trend in window size effect, CNN\noutperformed SVM using leave-one-subject-out cross validation. This could be\ncaused by different extracted features in the elicitation process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:30:47 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Keelawat", "Panayu", ""], ["Thammasan", "Nattapong", ""], ["Numao", "Masayuki", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1910.09731", "submitter": "Xiang Wang", "authors": "Xiang Wang, Tie Liu", "title": "Multiple Sample Clustering", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering algorithms that view each object data as a single sample drawn\nfrom a certain distribution, Gaussian distribution, for example, has been a hot\ntopic for decades. Many clustering algorithms: such as k-means and spectral\nclustering are proposed based on the single sample assumption. However, in real\nlife, each input object can usually be the multiple samples drawn from a\ncertain hidden distribution. The traditional clustering algorithms cannot\nhandle such a situation. This calls for the multiple sample clustering\nalgorithm. But the traditional multiple sample clustering algorithms can only\nhandle scalar samples or samples from Gaussian distribution. This constrains\nthe application field of multiple sample clustering algorithms. In this paper,\nwe purpose a general framework for multiple sample clustering. Various\nalgorithms can be generated by this framework. We apply two specific cases of\nthis framework: Wasserstein distance version and Bhattacharyya distance version\non both synthetic data and stock price data. The simulation results show that\nthe sufficient statistic can greatly improve the clustering accuracy and\nstability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:29:16 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 00:22:50 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 04:53:05 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wang", "Xiang", ""], ["Liu", "Tie", ""]]}, {"id": "1910.09732", "submitter": "Xinjie Lan", "authors": "Xinjie Lan, Kenneth E. Barner", "title": "Explicitly Bayesian Regularizations in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generalization is essential for deep learning. In contrast to previous works\nclaiming that Deep Neural Networks (DNNs) have an implicit regularization\nimplemented by the stochastic gradient descent, we demonstrate explicitly\nBayesian regularizations in a specific category of DNNs, i.e., Convolutional\nNeural Networks (CNNs). First, we introduce a novel probabilistic\nrepresentation for the hidden layers of CNNs and demonstrate that CNNs\ncorrespond to Bayesian networks with the serial connection. Furthermore, we\nshow that the hidden layers close to the input formulate prior distributions,\nthus CNNs have explicitly Bayesian regularizations based on the Bayesian\nregularization theory. In addition, we clarify two recently observed empirical\nphenomena that are inconsistent with traditional theories of generalization.\nFinally, we validate the proposed theory on a synthetic dataset\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:32:03 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Lan", "Xinjie", ""], ["Barner", "Kenneth E.", ""]]}, {"id": "1910.09734", "submitter": "Chun-Na Li", "authors": "Chun-Na Li, Yuan-Hai Shao, Huajun Wang, Yu-Ting Zhao, Ling-Wei Huang,\n  Naihua Xiu and Nai-Yang Deng", "title": "Single and Union Non-parallel Support Vector Machine Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:34:34 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 00:03:40 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 06:56:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Chun-Na", ""], ["Shao", "Yuan-Hai", ""], ["Wang", "Huajun", ""], ["Zhao", "Yu-Ting", ""], ["Huang", "Ling-Wei", ""], ["Xiu", "Naihua", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1910.09739", "submitter": "Ming-Chuan Yang", "authors": "Ming-Chuan Yang, Meng Chang Chen", "title": "Composite Neural Network: Theory and Application to PM2.5 Prediction", "comments": "This version is accepted by IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the framework and performance issues of the composite\nneural network, which is composed of a collection of pre-trained and\nnon-instantiated neural network models connected as a rooted directed acyclic\ngraph for solving complicated applications. A pre-trained neural network model\nis generally well trained, targeted to approximate a specific function. Despite\na general belief that a composite neural network may perform better than a\nsingle component, the overall performance characteristics are not clear. In\nthis work, we construct the framework of a composite network, and prove that a\ncomposite neural network performs better than any of its pre-trained components\nwith a high probability bound. In addition, if an extra pre-trained component\nis added to a composite network, with high probability, the overall performance\nwill not be degraded. In the study, we explore a complicated application --\nPM2.5 prediction -- to illustrate the correctness of the proposed composite\nnetwork theory. In the empirical evaluations of PM2.5 prediction, the\nconstructed composite neural network models support the proposed theory and\nperform better than other machine learning models, demonstrate the advantages\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:43:17 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 04:35:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yang", "Ming-Chuan", ""], ["Chen", "Meng Chang", ""]]}, {"id": "1910.09745", "submitter": "Wen-Yu Chang", "authors": "Wen-Yu Chang, Tsung-Nan Lin", "title": "Vanishing Nodes: Another Phenomenon That Makes Training Deep Neural\n  Networks Difficult", "comments": "16 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the problem of vanishing/exploding gradients is a\nchallenge when training deep networks. In this paper, we describe another\nphenomenon, called vanishing nodes, that also increases the difficulty of\ntraining deep neural networks. As the depth of a neural network increases, the\nnetwork's hidden nodes have more highly correlated behavior. This results in\ngreat similarities between these nodes. The redundancy of hidden nodes thus\nincreases as the network becomes deeper. We call this problem vanishing nodes,\nand we propose the metric vanishing node indicator (VNI) for quantitatively\nmeasuring the degree of vanishing nodes. The VNI can be characterized by the\nnetwork parameters, which is shown analytically to be proportional to the depth\nof the network and inversely proportional to the network width. The theoretical\nresults show that the effective number of nodes vanishes to one when the VNI\nincreases to one (its maximal value), and that vanishing/exploding gradients\nand vanishing nodes are two different challenges that increase the difficulty\nof training deep neural networks. The numerical results from the experiments\nsuggest that the degree of vanishing nodes will become more evident during\nback-propagation training, and that when the VNI is equal to 1, the network\ncannot learn simple tasks (e.g. the XOR problem) even when the gradients are\nneither vanishing nor exploding. We refer to this kind of gradients as the\nwalking dead gradients, which cannot help the network converge when having a\nrelatively large enough scale. Finally, the experiments show that the\nlikelihood of failed training increases as the depth of the network increases.\nThe training will become much more difficult due to the lack of network\nrepresentation capability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 03:11:26 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chang", "Wen-Yu", ""], ["Lin", "Tsung-Nan", ""]]}, {"id": "1910.09754", "submitter": "Hamed Sarvari", "authors": "Hamed Sarvari, Carlotta Domeniconi, Bardh Prenkaj, Giovanni Stilo", "title": "Unsupervised Boosting-based Autoencoder Ensembles for Outlier Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders, as a dimensionality reduction technique, have been recently\napplied to outlier detection. However, neural networks are known to be\nvulnerable to overfitting, and therefore have limited potential in the\nunsupervised outlier detection setting. Current approaches to ensemble-based\nautoencoders do not generate a sufficient level of diversity to avoid the\noverfitting issue. To overcome the aforementioned limitations we develop a\nBoosting-based Autoencoder Ensemble approach (in short, BAE). BAE is an\nunsupervised ensemble method that, similarly to the boosting approach, builds\nan adaptive cascade of autoencoders to achieve improved and robust results. BAE\ntrains the autoencoder components sequentially by performing a weighted\nsampling of the data, aimed at reducing the amount of outliers used during\ntraining, and at injecting diversity in the ensemble. We perform extensive\nexperiments and show that the proposed methodology outperforms state-of-the-art\napproaches under a variety of conditions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 03:47:39 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sarvari", "Hamed", ""], ["Domeniconi", "Carlotta", ""], ["Prenkaj", "Bardh", ""], ["Stilo", "Giovanni", ""]]}, {"id": "1910.09763", "submitter": "Thomas Merkh", "authors": "Thomas Merkh and Guido Mont\\'ufar", "title": "Stochastic Feedforward Neural Networks: Universal Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we take a look at the universal approximation question for\nstochastic feedforward neural networks. In contrast to deterministic networks,\nwhich represent mappings from a set of inputs to a set of outputs, stochastic\nnetworks represent mappings from a set of inputs to a set of probability\ndistributions over the set of outputs. In particular, even if the sets of\ninputs and outputs are finite, the class of stochastic mappings in question is\nnot finite. Moreover, while for a deterministic function the values of all\noutput variables can be computed independently of each other given the values\nof the inputs, in the stochastic setting the values of the output variables may\nneed to be correlated, which requires that their values are computed jointly. A\nprominent class of stochastic feedforward networks which has played a key role\nin the resurgence of deep learning are deep belief networks. The\nrepresentational power of these networks has been studied mainly in the\ngenerative setting, as models of probability distributions without an input, or\nin the discriminative setting for the special case of deterministic mappings.\nWe study the representational power of deep sigmoid belief networks in terms of\ncompositions of linear transformations of probability distributions, Markov\nkernels, that can be expressed by the layers of the network. We investigate\ndifferent types of shallow and deep architectures, and the minimal number of\nlayers and units per layer that are sufficient and necessary in order for the\nnetwork to be able to approximate any given stochastic mapping from the set of\ninputs to the set of outputs arbitrarily well.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 04:49:43 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Merkh", "Thomas", ""], ["Mont\u00fafar", "Guido", ""]]}, {"id": "1910.09772", "submitter": "Rui Shu", "authors": "Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, Ben Poole", "title": "Weakly Supervised Disentanglement with Guarantees", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations that correspond to factors of variation\nin real-world data is critical to interpretable and human-controllable machine\nlearning. Recently, concerns about the viability of learning disentangled\nrepresentations in a purely unsupervised manner has spurred a shift toward the\nincorporation of weak supervision. However, there is currently no formalism\nthat identifies when and how weak supervision will guarantee disentanglement.\nTo address this issue, we provide a theoretical framework to assist in\nanalyzing the disentanglement guarantees (or lack thereof) conferred by weak\nsupervision when coupled with learning algorithms based on distribution\nmatching. We empirically verify the guarantees and limitations of several weak\nsupervision methods (restricted labeling, match-pairing, and rank-pairing),\ndemonstrating the predictive power and usefulness of our theoretical framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:21:51 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 21:39:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Shu", "Rui", ""], ["Chen", "Yining", ""], ["Kumar", "Abhishek", ""], ["Ermon", "Stefano", ""], ["Poole", "Ben", ""]]}, {"id": "1910.09778", "submitter": "Hye-jin Shim", "authors": "Hye-jin Shim, Hee-Soo Heo, Jee-weon Jung, and Ha-Jin Yu", "title": "Self-supervised pre-training with acoustic configurations for replay\n  spoofing detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing a dataset for replay spoofing detection requires a physical\nprocess of playing an utterance and re-recording it, presenting a challenge to\nthe collection of large-scale datasets. In this study, we propose a\nself-supervised framework for pretraining acoustic configurations using\ndatasets published for other tasks, such as speaker verification. Here,\nacoustic configurations refer to the environmental factors generated during the\nprocess of voice recording but not the voice itself, including microphone\ntypes, place and ambient noise levels. Specifically, we select pairs of\nsegments from utterances and train deep neural networks to determine whether\nthe acoustic configurations of the two segments are identical. We validate the\neffectiveness of the proposed method based on the ASVspoof 2019 physical access\ndataset utilizing two well-performing systems. The experimental results\ndemonstrate that the proposed method outperforms the baseline approach by 30%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:54:41 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 05:00:37 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Shim", "Hye-jin", ""], ["Heo", "Hee-Soo", ""], ["Jung", "Jee-weon", ""], ["Yu", "Ha-Jin", ""]]}, {"id": "1910.09779", "submitter": "Jiaming Song", "authors": "Jiaming Song and Stefano Ermon", "title": "Bridging the Gap Between $f$-GANs and Wasserstein GANs", "comments": "updated for ICML camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have enjoyed much success in learning\nhigh-dimensional distributions. Learning objectives approximately minimize an\n$f$-divergence ($f$-GANs) or an integral probability metric (Wasserstein GANs)\nbetween the model and the data distribution using a discriminator. Wasserstein\nGANs enjoy superior empirical performance, but in $f$-GANs the discriminator\ncan be interpreted as a density ratio estimator which is necessary in some GAN\napplications. In this paper, we bridge the gap between $f$-GANs and Wasserstein\nGANs (WGANs). First, we list two constraints over variational $f$-divergence\nestimation objectives that preserves the optimal solution. Next, we minimize\nover a Lagrangian relaxation of the constrained objective, and show that it\ngeneralizes critic objectives of both $f$-GAN and WGAN. Based on this\ngeneralization, we propose a novel practical objective, named KL-Wasserstein\nGAN (KL-WGAN). We demonstrate empirical success of KL-WGAN on synthetic\ndatasets and real-world image generation benchmarks, and achieve\nstate-of-the-art FID scores on CIFAR10 image generation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:55:03 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 20:53:50 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.09792", "submitter": "Jisoo Lee", "authors": "Jisoo Lee, Sae-Young Chung", "title": "Robust Training with Ensemble Consensus", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since deep neural networks are over-parameterized, they can memorize noisy\nexamples. We address such a memorization issue in the presence of label noise.\nFrom the fact that deep neural networks cannot generalize to neighborhoods of\nmemorized features, we hypothesize that noisy examples do not consistently\nincur small losses on the network under a certain perturbation. Based on this,\nwe propose a novel training method called Learning with Ensemble Consensus\n(LEC) that prevents overfitting to noisy examples by removing them based on the\nconsensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC\noutperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and\nCIFAR-100 in an efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:58:10 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 03:33:50 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:59:16 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lee", "Jisoo", ""], ["Chung", "Sae-Young", ""]]}, {"id": "1910.09804", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Shrikant Venkataramani, Zhepei Wang, Cem Subakan and\n  Paris Smaragdis", "title": "Two-Step Sound Source Separation: Training on Learned Latent Targets", "comments": "Submitted to ICASSP 2020", "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP40776.2020.9054172", "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-step training procedure for source separation\nvia a deep neural network. In the first step we learn a transform (and it's\ninverse) to a latent space where masking-based separation performance using\noracles is optimal. For the second step, we train a separation module that\noperates on the previously learned space. In order to do so, we also make use\nof a scale-invariant signal to distortion ratio (SI-SDR) loss function that\nworks in the latent space, and we prove that it lower-bounds the SI-SDR in the\ntime domain. We run various sound separation experiments that show how this\napproach can obtain better performance as compared to systems that learn the\ntransform and the separation module jointly. The proposed methodology is\ngeneral enough to be applicable to a large class of neural network end-to-end\nseparation systems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 07:49:21 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:45:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Venkataramani", "Shrikant", ""], ["Wang", "Zhepei", ""], ["Subakan", "Cem", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1910.09840", "submitter": "Sebastian Lapuschkin", "authors": "Maximilian Kohlbrenner, Alexander Bauer, Shinichi Nakajima, Alexander\n  Binder, Wojciech Samek, Sebastian Lapuschkin", "title": "Towards Best Practice in Explaining Neural Network Decisions with LRP", "comments": "7 pages, 4 figures, 1 table. fixed table row compared to v2.\n  Presented virtually at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within the last decade, neural network based predictors have demonstrated\nimpressive - and at times super-human - capabilities. This performance is often\npaid for with an intransparent prediction process and thus has sparked numerous\ncontributions in the novel field of explainable artificial intelligence (XAI).\nIn this paper, we focus on a popular and widely used method of XAI, the\nLayer-wise Relevance Propagation (LRP). Since its initial proposition LRP has\nevolved as a method, and a best practice for applying the method has tacitly\nemerged, based however on humanly observed evidence alone. In this paper we\ninvestigate - and for the first time quantify - the effect of this current best\npractice on feedforward neural networks in a visual object detection setting.\nThe results verify that the layer-dependent approach to LRP applied in recent\nliterature better represents the model's reasoning, and at the same time\nincreases the object localization and class discriminativity of LRP.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:58:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:06:45 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 20:00:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kohlbrenner", "Maximilian", ""], ["Bauer", "Alexander", ""], ["Nakajima", "Shinichi", ""], ["Binder", "Alexander", ""], ["Samek", "Wojciech", ""], ["Lapuschkin", "Sebastian", ""]]}, {"id": "1910.09851", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov", "title": "Orthogonal variance decomposition based feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing feature selection methods fail to properly account for interactions\nbetween features when evaluating feature subsets. In this paper, we attempt to\nremedy this issue by using orthogonal variance decomposition to evaluate\nfeatures. The orthogonality of the decomposition allows us to directly\ncalculate the total contribution of a feature to the output variance. Thus we\nobtain an efficient algorithm for feature evaluation which takes into account\ninteractions among features. Numerical experiments demonstrate that our method\naccurately identifies relevant features and improves the accuracy of numerical\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 09:18:39 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kamalov", "Firuz", ""]]}, {"id": "1910.09857", "submitter": "Nuri Mert Vural", "authors": "N. Mert Vural, Salih Erg\\\"ut and Suleyman S. Kozat", "title": "An Efficient and Effective Second-Order Training Algorithm for\n  LSTM-based Adaptive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adaptive (or online) nonlinear regression with\nLong-Short-Term-Memory (LSTM) based networks, i.e., LSTM-based adaptive\nlearning. In this context, we introduce an efficient Extended Kalman filter\n(EKF) based second-order training algorithm. Our algorithm is truly online,\ni.e., it does not assume any underlying data generating process and future\ninformation, except that the target sequence is bounded. Through an extensive\nset of experiments, we demonstrate significant performance gains achieved by\nour algorithm with respect to the state-of-the-art methods. Here, we mainly\nshow that our algorithm consistently provides 10 to 45\\% improvement in the\naccuracy compared to the widely-used adaptive methods Adam, RMSprop, and DEKF,\nand comparable performance to EKF with a 10 to 15 times reduction in the\nrun-time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 09:30:41 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 16:40:10 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 16:12:42 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 13:43:54 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 15:39:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vural", "N. Mert", ""], ["Erg\u00fct", "Salih", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1910.09862", "submitter": "Guillaume Doras", "authors": "Guillaume Doras, Geoffroy Peeters", "title": "A Prototypical Triplet Loss for Cover Detection", "comments": "Corrections after reviewers comments. Correct erroneous figure 5 in\n  original version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic cover detection -- the task of finding in a audio dataset all\ncovers of a query track -- has long been a challenging theoretical problem in\nMIR community. It also became a practical need for music composers societies\nrequiring to detect automatically if an audio excerpt embeds musical content\nbelonging to their catalog.\n  In a recent work, we addressed this problem with a convolutional neural\nnetwork mapping each track's dominant melody to an embedding vector, and\ntrained to minimize cover pairs distance in the embeddings space, while\nmaximizing it for non-covers. We showed in particular that training this model\nwith enough works having five or more covers yields state-of-the-art results.\n  This however does not reflect the realistic use case, where music catalogs\ntypically contain works with zero or at most one or two covers. We thus\nintroduce here a new test set incorporating these constraints, and propose two\ncontributions to improve our model's accuracy under these stricter conditions:\nwe replace dominant melody with multi-pitch representation as input data, and\ndescribe a novel prototypical triplet loss designed to improve covers\nclustering. We show that these changes improve results significantly for two\nconcrete use cases, large dataset lookup and live songs identification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 09:39:59 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 09:48:08 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Doras", "Guillaume", ""], ["Peeters", "Geoffroy", ""]]}, {"id": "1910.09876", "submitter": "Arnab Sanyal", "authors": "Arnab Sanyal, Peter A. Beerel, Keith M. Chugg", "title": "Neural Network Training with Approximate Logarithmic Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high computational complexity associated with training deep neural\nnetworks limits online and real-time training on edge devices. This paper\nproposed an end-to-end training and inference scheme that eliminates\nmultiplications by approximate operations in the log-domain which has the\npotential to significantly reduce implementation complexity. We implement the\nentire training procedure in the log-domain, with fixed-point data\nrepresentations. This training procedure is inspired by hardware-friendly\napproximations of log-domain addition which are based on look-up tables and\nbit-shifts. We show that our 16-bit log-based training can achieve\nclassification accuracy within approximately 1% of the equivalent\nfloating-point baselines for a number of commonly used datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 10:11:16 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sanyal", "Arnab", ""], ["Beerel", "Peter A.", ""], ["Chugg", "Keith M.", ""]]}, {"id": "1910.09918", "submitter": "Hajime Yoshino", "authors": "Hajime Yoshino", "title": "From complex to simple : hierarchical free-energy landscape renormalized\n  in deep neural networks", "comments": "61 pages, 20 figures, revised version, to appear in SciPost Phys Core", "journal-ref": "SciPost Phys. Core 2, 005 (2020)", "doi": "10.21468/SciPostPhysCore.2.2.005", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a statistical mechanical approach based on the replica method to\nstudy the design space of deep and wide neural networks constrained to meet a\nlarge number of training data. Specifically, we analyze the configuration space\nof the synaptic weights and neurons in the hidden layers in a simple\nfeed-forward perceptron network for two scenarios: a setting with random\ninputs/outputs and a teacher-student setting. By increasing the strength of\nconstraints,~i.e. increasing the number of training data, successive 2nd order\nglass transition (random inputs/outputs) or 2nd order crystalline transition\n(teacher-student setting) take place layer-by-layer starting next to the\ninputs/outputs boundaries going deeper into the bulk with the thickness of the\nsolid phase growing logarithmically with the data size. This implies the\ntypical storage capacity of the network grows exponentially fast with the\ndepth. In a deep enough network, the central part remains in the liquid phase.\nWe argue that in systems of finite width N, the weak bias field can remain in\nthe center and plays the role of a symmetry-breaking field that connects the\nopposite sides of the system. The successive glass transitions bring about a\nhierarchical free-energy landscape with ultrametricity, which evolves in space:\nit is most complex close to the boundaries but becomes renormalized into\nprogressively simpler ones in deeper layers. These observations provide clues\nto understand why deep neural networks operate efficiently. Finally, we present\nsome numerical simulations of learning which reveal spatially heterogeneous\nglassy dynamics truncated by a finite width $N$ effect.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:21:06 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 15:20:48 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 04:22:32 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 02:30:29 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Yoshino", "Hajime", ""]]}, {"id": "1910.09933", "submitter": "Suyi Li", "authors": "Suyi Li, Yong Cheng, Yang Liu, Wei Wang, Tianjian Chen", "title": "Abnormal Client Behavior Detection in Federated Learning", "comments": "7 pages, 1 figure, 2nd International Workshop on Federated Learning\n  for Data Privacy and Confidentiality, in Conjunction with NeurIPS 2019\n  (FL-NeurIPS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated learning systems, clients are autonomous in that their behaviors\nare not fully governed by the server. Consequently, a client may intentionally\nor unintentionally deviate from the prescribed course of federated model\ntraining, resulting in abnormal behaviors, such as turning into a malicious\nattacker or a malfunctioning client. Timely detecting those anomalous clients\nis therefore critical to minimize their adverse impacts. In this work, we\npropose to detect anomalous clients at the server side. In particular, we\ngenerate low-dimensional surrogates of model weight vectors and use them to\nperform anomaly detection. We evaluate our solution through experiments on\nimage classification model training over the FEMNIST dataset. Experimental\nresults show that the proposed detection-based approach significantly\noutperforms the conventional defense-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:48:07 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 06:13:55 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Li", "Suyi", ""], ["Cheng", "Yong", ""], ["Liu", "Yang", ""], ["Wang", "Wei", ""], ["Chen", "Tianjian", ""]]}, {"id": "1910.09943", "submitter": "Ilya Amburg", "authors": "Ilya Amburg, Nate Veldt, Austin R. Benson", "title": "Clustering in graphs and hypergraphs with categorical edge labels", "comments": "In Proceedings of The Web Conference 2020 (WWW '20), April 20-24,\n  2020, Taipei, Taiwan", "journal-ref": null, "doi": "10.1145/3366423.3380152", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern graph or network datasets often contain rich structure that goes\nbeyond simple pairwise connections between nodes. This calls for complex\nrepresentations that can capture, for instance, edges of different types as\nwell as so-called \"higher-order interactions\" that involve more than two nodes\nat a time. However, we have fewer rigorous methods that can provide insight\nfrom such representations. Here, we develop a computational framework for the\nproblem of clustering hypergraphs with categorical edge labels --- or different\ninteraction types --- where clusters corresponds to groups of nodes that\nfrequently participate in the same type of interaction.\n  Our methodology is based on a combinatorial objective function that is\nrelated to correlation clustering on graphs but enables the design of much more\nefficient algorithms that also seamlessly generalize to hypergraphs. When there\nare only two label types, our objective can be optimized in polynomial time,\nusing an algorithm based on minimum cuts. Minimizing our objective becomes\nNP-hard with more than two label types, but we develop fast approximation\nalgorithms based on linear programming relaxations that have theoretical\ncluster quality guarantees. We demonstrate the efficacy of our algorithms and\nthe scope of the model through problems in edge-label community detection,\nclustering with temporal data, and exploratory data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:01:16 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 19:26:53 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amburg", "Ilya", ""], ["Veldt", "Nate", ""], ["Benson", "Austin R.", ""]]}, {"id": "1910.09972", "submitter": "Yuki Saito", "authors": "Yuki Saito, Takuma Nakamura, Hirotaka Hachiya, Kenji Fukumizu", "title": "Exchangeable deep neural networks for set-to-set matching and learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58520-4_37", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching two different sets of items, called heterogeneous set-to-set\nmatching problem, has recently received attention as a promising problem. The\ndifficulties are to extract features to match a correct pair of different sets\nand also preserve two types of exchangeability required for set-to-set\nmatching: the pair of sets, as well as the items in each set, should be\nexchangeable. In this study, we propose a novel deep learning architecture to\naddress the abovementioned difficulties and also an efficient training\nframework for set-to-set matching. We evaluate the methods through experiments\nbased on two industrial applications: fashion set recommendation and group\nre-identification. In these experiments, we show that the proposed method\nprovides significant improvements and results compared with the\nstate-of-the-art methods, thereby validating our architecture for the\nheterogeneous set matching problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:42:39 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 09:34:59 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Saito", "Yuki", ""], ["Nakamura", "Takuma", ""], ["Hachiya", "Hirotaka", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1910.09979", "submitter": "Junjun Pan", "authors": "Junjun Pan, Michael K. Ng, Ye Liu, Xiongjun Zhang, Hong Yan", "title": "Orthogonal Nonnegative Tucker Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonnegative tensor data and propose an orthogonal\nnonnegative Tucker decomposition (ONTD). We discuss some properties of ONTD and\ndevelop a convex relaxation algorithm of the augmented Lagrangian function to\nsolve the optimization problem. The convergence of the algorithm is given. We\nemploy ONTD on the image data sets from the real world applications including\nface recognition, image representation, hyperspectral unmixing. Numerical\nresults are shown to illustrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 11:18:21 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 11:48:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Pan", "Junjun", ""], ["Ng", "Michael K.", ""], ["Liu", "Ye", ""], ["Zhang", "Xiongjun", ""], ["Yan", "Hong", ""]]}, {"id": "1910.09991", "submitter": "Fabrizio De Fausti", "authors": "Fabrizio De Fausti, Francesco Pugliese and Diego Zardetto", "title": "Towards Automated Website Classification by Deep Learning", "comments": null, "journal-ref": "Rivista di Statistica Ufficiale, n. 3/2020, Istat, pag. 9-50, ISSN\n  1828-1982", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the interest in Big Data sources has been steadily growing\nwithin the Official Statistic community. The Italian National Institute of\nStatistics (Istat) is currently carrying out several Big Data pilot studies.\nOne of these studies, the ICT Big Data pilot, aims at exploiting massive\namounts of textual data automatically scraped from the websites of Italian\nenterprises in order to predict a set of target variables (e.g. e-commerce)\nthat are routinely observed by the traditional ICT Survey. In this paper, we\nshow that Deep Learning techniques can successfully address this problem.\nEssentially, we tackle a text classification task: an algorithm must learn to\ninfer whether an Italian enterprise performs e-commerce from the textual\ncontent of its website. To reach this goal, we developed a sophisticated\nprocessing pipeline and evaluated its performance through extensive\nexperiments. Our pipeline uses Convolutional Neural Networks and relies on Word\nEmbeddings to encode raw texts into grayscale images (i.e. normalized numeric\nmatrices). Web-scraped texts are huge and have very low signal to noise ratio:\nto overcome these issues, we adopted a framework known as False Positive\nReduction, which has seldom (if ever) been applied before to text\nclassification tasks. Several original contributions enable our processing\npipeline to reach good classification results. Empirical evidence shows that\nour proposal outperforms all the alternative Machine Learning solutions already\ntested in Istat for the same task.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 14:07:17 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 23:07:25 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["De Fausti", "Fabrizio", ""], ["Pugliese", "Francesco", ""], ["Zardetto", "Diego", ""]]}, {"id": "1910.10013", "submitter": "Zheng-Hua Tan", "authors": "Saeid Samizade, Zheng-Hua Tan, Chao Shen and Xiaohong Guan", "title": "Adversarial Example Detection by Classification for Deep Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning systems are vulnerable to adversarial attacks and will\nhighly likely produce incorrect outputs under these attacks. There are\nwhite-box and black-box attacks regarding to adversary's access level to the\nvictim learning algorithm. To defend the learning systems from these attacks,\nexisting methods in the speech domain focus on modifying input signals and\ntesting the behaviours of speech recognizers. We, however, formulate the\ndefense as a classification problem and present a strategy for systematically\ngenerating adversarial example datasets: one for white-box attacks and one for\nblack-box attacks, containing both adversarial and normal examples. The\nwhite-box attack is a gradient-based method on Baidu DeepSpeech with the\nMozilla Common Voice database while the black-box attack is a gradient-free\nmethod on a deep model-based keyword spotting system with the Google Speech\nCommand dataset. The generated datasets are used to train a proposed\nConvolutional Neural Network (CNN), together with cepstral features, to detect\nadversarial examples. Experimental results show that, it is possible to\naccurately distinct between adversarial and normal examples for known attacks,\nin both single-condition and multi-condition training settings, while the\nperformance degrades dramatically for unknown attacks. The adversarial datasets\nand the source code are made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 14:46:00 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Samizade", "Saeid", ""], ["Tan", "Zheng-Hua", ""], ["Shen", "Chao", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1910.10024", "submitter": "Michael Patrick Sheehan", "authors": "Michael P. Sheehan, Antoine Gonon, Mike E. Davies", "title": "Compressive Learning for Semi-Parametric Models", "comments": "5 pages, 5 figure, submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the compressive learning theory, instead of solving a statistical learning\nproblem from the input data, a so-called sketch is computed from the data prior\nto learning. The sketch has to capture enough information to solve the problem\ndirectly from it, allowing to discard the dataset from the memory. This is\nuseful when dealing with large datasets as the size of the sketch does not\nscale with the size of the database. In this paper, we reformulate the original\ncompressive learning framework to explicitly cater for the class of\nsemi-parametric models. The reformulation takes account of the inherent\ntopology and structure of semi-parametric models, creating an intuitive pathway\nto the development of compressive learning algorithms. We apply our developed\nframework to both the semi-parametric models of independent component analysis\nand subspace clustering, demonstrating the robustness of the framework to\nexplicitly show when a compression in complexity can be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:00:29 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sheehan", "Michael P.", ""], ["Gonon", "Antoine", ""], ["Davies", "Mike E.", ""]]}, {"id": "1910.10046", "submitter": "Vanessa B\\\"ohm", "authors": "Vanessa B\\\"ohm, Fran\\c{c}ois Lanusse, Uro\\v{s} Seljak", "title": "Uncertainty Quantification with Generative Models", "comments": "accepted submission to the Bayesian Deep Learning NeurIPS 2019\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generative model-based approach to Bayesian inverse problems,\nsuch as image reconstruction from noisy and incomplete images. Our framework\naddresses two common challenges of Bayesian reconstructions: 1) It makes use of\ncomplex, data-driven priors that comprise all available information about the\nuncorrupted data distribution. 2) It enables computationally tractable\nuncertainty quantification in the form of posterior analysis in latent and data\nspace. The method is very efficient in that the generative model only has to be\ntrained once on an uncorrupted data set, after that, the procedure can be used\nfor arbitrary corruption types.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:27:41 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["B\u00f6hm", "Vanessa", ""], ["Lanusse", "Fran\u00e7ois", ""], ["Seljak", "Uro\u0161", ""]]}, {"id": "1910.10067", "submitter": "Jack Kenney B.S.", "authors": "Jack Kenney, John Valcore, Scott Riggs, Edward Rietman", "title": "Deep Learning Regression of VLSI Plasma Etch Metrology", "comments": "13 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer chip manufacturing, the study of etch patterns on silicon wafers,\nor metrology, occurs on the nano-scale and is therefore subject to large\nvariation from small, yet significant, perturbations in the manufacturing\nenvironment. An enormous amount of information can be gathered from a single\netch process, a sequence of actions taken to produce an etched wafer from a\nblank piece of silicon. Each final wafer, however, is costly to take\nmeasurements from, which limits the number of examples available to train a\npredictive model. Part of the significance of this work is the success we saw\nfrom the models despite the limited number of examples. In order to accommodate\nthe high dimensional process signatures, we isolated important sensor variables\nand applied domain-specific summarization on the data using multiple feature\nengineering techniques. We used a neural network architecture consisting of the\nsummarized inputs, a single hidden layer of 4032 units, and an output layer of\none unit. Two different models were learned, corresponding to the metrology\nmeasurements in the dataset, Recess and Remaining Mask. The outputs are related\nabstractly and do not form a two dimensional space, thus two separate models\nwere learned. Our results approach the error tolerance of the microscopic\nimaging system. The model can make predictions for a class of etch recipes that\ninclude the correct number of etch steps and plasma reactors with the\nappropriate sensors, which are chambers containing an ionized gas that\ndetermine the manufacture environment. Notably, this method is not restricted\nto some maximum process length due to the summarization techniques used. This\nallows the method to be adapted to new processes that satisfy the\naforementioned requirements. In order to automate semiconductor manufacturing,\nmodels like these will be needed throughout the process to evaluate production\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:10:37 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kenney", "Jack", ""], ["Valcore", "John", ""], ["Riggs", "Scott", ""], ["Rietman", "Edward", ""]]}, {"id": "1910.10071", "submitter": "Joaquin Perez-Lapillo", "authors": "Joaquin Perez-Lapillo, Oleksandr Galkin, Tillman Weyde", "title": "Improving singing voice separation with the Wave-U-Net using Minimum\n  Hyperspherical Energy", "comments": "Paper submitted to ICASSP 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning has surpassed traditional approaches to the\nproblem of singing voice separation. The Wave-U-Net is a recent deep network\narchitecture that operates directly on the time domain. The standard Wave-U-Net\nis trained with data augmentation and early stopping to prevent overfitting.\nMinimum hyperspherical energy (MHE) regularization has recently proven to\nincrease generalization in image classification problems by encouraging a\ndiversified filter configuration. In this work, we apply MHE regularization to\nthe 1D filters of the Wave-U-Net. We evaluated this approach for separating the\nvocal part from mixed music audio recordings on the MUSDB18 dataset. We found\nthat adding MHE regularization to the loss function consistently improves\nsinging voice separation, as measured in the Signal to Distortion Ratio on test\nrecordings, leading to the current best time-domain system for singing voice\nextraction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:14:25 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Perez-Lapillo", "Joaquin", ""], ["Galkin", "Oleksandr", ""], ["Weyde", "Tillman", ""]]}, {"id": "1910.10076", "submitter": "Mastaneh Torkamani-Azar", "authors": "Mastaneh Torkamani-Azar, Sumeyra Demir Kanik, Serap Aydin, and Mujdat\n  Cetin", "title": "Prediction of Reaction Time and Vigilance Variability from\n  Spatiospectral Features of Resting-State EEG in a Long Sustained Attention\n  Task", "comments": "11 pages, 6 figures; submitted to the Journal of Biomedical and\n  Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state brain networks represent the intrinsic state of the brain\nduring the majority of cognitive and sensorimotor tasks. However, no study has\nyet presented concise predictors of task-induced vigilance variability from\nspectrospatial features of the pre-task, resting-state electroencephalograms\n(EEG). We asked ten healthy volunteers (6 females, 4 males) to participate in\n105-minute fixed-sequence-varying-duration sessions of sustained attention to\nresponse task (SART). A novel and adaptive vigilance scoring scheme was\ndesigned based on the performance and response time in consecutive trials, and\ndemonstrated large inter-participant variability in terms of maintaining\nconsistent tonic performance. Multiple linear regression using feature\nrelevance analysis obtained significant predictors of the mean cumulative\nvigilance score (CVS), mean response time, and variabilities of these scores\nfrom the resting-state, band-power ratios of EEG signals, p<0.05. Single-layer\nneural networks trained with cross-validation also captured different\nassociations for the beta sub-bands. Increase in the gamma (28-48 Hz) and upper\nbeta ratios from the left central and temporal regions predicted slower\nreactions and more inconsistent vigilance as explained by the increased\nactivation of default mode network (DMN) and differences between the high- and\nlow-attention networks at temporal regions. Higher ratios of parietal alpha\nfrom the Brodmann's areas 18, 19, and 37 during the eyes-open states predicted\nslower responses but more consistent CVS and reactions associated with the\nsuperior ability in vigilance maintenance. The proposed framework and these\nfindings on the most stable and significant attention predictors from the\nintrinsic EEG power ratios can be used to model attention variations during the\ncalibration sessions of BCI applications and vigilance monitoring systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:41:31 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Torkamani-Azar", "Mastaneh", ""], ["Kanik", "Sumeyra Demir", ""], ["Aydin", "Serap", ""], ["Cetin", "Mujdat", ""]]}, {"id": "1910.10087", "submitter": "Pablo Moreno-Mu\\~noz", "authors": "Pablo Moreno-Mu\\~noz, David Ram\\'irez and Antonio Art\\'es-Rodr\\'iguez", "title": "Continual Learning for Infinite Hierarchical Change-Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change-point detection (CPD) aims to locate abrupt transitions in the\ngenerative model of a sequence of observations. When Bayesian methods are\nconsidered, the standard practice is to infer the posterior distribution of the\nchange-point locations. However, for complex models (high-dimensional or\nheterogeneous), it is not possible to perform reliable detection. To circumvent\nthis problem, we propose to use a hierarchical model, which yields observations\nthat belong to a lower-dimensional manifold. Concretely, we consider a\nlatent-class model with an unbounded number of categories, which is based on\nthe chinese-restaurant process (CRP). For this model we derive a continual\nlearning mechanism that is based on the sequential construction of the CRP and\nthe expectation-maximization (EM) algorithm with a stochastic maximization\nstep. Our results show that the proposed method is able to recursively infer\nthe number of underlying latent classes and perform CPD in a reliable manner.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:30:14 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Moreno-Mu\u00f1oz", "Pablo", ""], ["Ram\u00edrez", "David", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "1910.10106", "submitter": "Alessandro Lameiras Koerich", "authors": "Karl Michel Koerich, Mohammad Esmaeilpour, Sajjad Abdoli, Alceu de\n  Souza Britto Jr., Alessandro Lameiras Koerich", "title": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms", "comments": "8 pages", "journal-ref": "IEEE International Joint Conference on Neural Networks (IJCNN\n  2020), Glasgow, UK", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows the susceptibility of spectrogram-based audio classifiers to\nadversarial attacks and the transferability of such attacks to audio waveforms.\nSome commonly used adversarial attacks to images have been applied to\nMel-frequency and short-time Fourier transform spectrograms, and such perturbed\nspectrograms are able to fool a 2D convolutional neural network (CNN). Such\nattacks produce perturbed spectrograms that are visually imperceptible by\nhumans. Furthermore, the audio waveforms reconstructed from the perturbed\nspectrograms are also able to fool a 1D CNN trained on the original audio.\nExperimental results on a dataset of western music have shown that the 2D CNN\nachieves up to 81.87% of mean accuracy on legitimate examples and such\nperformance drops to 12.09% on adversarial examples. Likewise, the 1D CNN\nachieves up to 78.29% of mean accuracy on original audio samples and such\nperformance drops to 27.91% on adversarial audio waveforms reconstructed from\nthe perturbed spectrograms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:46:37 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 21:18:07 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 16:38:44 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 11:16:58 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Koerich", "Karl Michel", ""], ["Esmaeilpour", "Mohammad", ""], ["Abdoli", "Sajjad", ""], ["Britto", "Alceu de Souza", "Jr."], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1910.10114", "submitter": "Eda Bayram", "authors": "Eda Bayram, Dorina Thanou, Elif Vural and Pascal Frossard", "title": "Mask Combination of Multi-layer Graphs for Global Structure Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure inference is an important task for network data processing and\nanalysis in data science. In recent years, quite a few approaches have been\ndeveloped to learn the graph structure underlying a set of observations\ncaptured in a data space. Although real-world data is often acquired in\nsettings where relationships are influenced by a priori known rules, such\ndomain knowledge is still not well exploited in structure inference problems.\nIn this paper, we identify the structure of signals defined in a data space\nwhose inner relationships are encoded by multi-layer graphs. We aim at properly\nexploiting the information originating from each layer to infer the global\nstructure underlying the signals. We thus present a novel method for combining\nthe multiple graphs into a global graph using mask matrices, which are\nestimated through an optimization problem that accommodates the multi-layer\ngraph information and a signal representation model. The proposed mask\ncombination method also estimates the contribution of each graph layer in the\nstructure of signals. The experiments conducted both on synthetic and\nreal-world data suggest that integrating the multi-layer graph representation\nof the data in the structure inference framework enhances the learning\nprocedure considerably by adapting to the quality and the quantity of the input\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:59:26 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 08:59:28 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Bayram", "Eda", ""], ["Thanou", "Dorina", ""], ["Vural", "Elif", ""], ["Frossard", "Pascal", ""]]}, {"id": "1910.10122", "submitter": "Eugene Wong", "authors": "Eugene Wong", "title": "Class Mean Vectors, Self Monitoring and Self Learning for Neural\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the role of sample mean in building a neural network\nfor classification. This role is surprisingly extensive and includes: direct\ncomputation of weights without training, performance monitoring for samples\nwithout known classification, and self-training for unlabeled data.\nExperimental computation on a CIFAR-10 data set provides promising empirical\nevidence on the efficacy of a simple and widely applicable approach to some\ndifficult problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:13:45 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Wong", "Eugene", ""]]}, {"id": "1910.10143", "submitter": "Alexandra Luccioni", "authors": "Sharon Zhou, Alexandra Luccioni, Gautier Cosne, Michael S. Bernstein,\n  Yoshua Bengio", "title": "Establishing an Evaluation Metric to Quantify Climate Change Image\n  Realism", "comments": "Accepted to the NeurIPS 2019 Workshop, Tackling Climate Change with\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With success on controlled tasks, generative models are being increasingly\napplied to humanitarian applications [1,2]. In this paper, we focus on the\nevaluation of a conditional generative model that illustrates the consequences\nof climate change-induced flooding to encourage public interest and awareness\non the issue. Because metrics for comparing the realism of different modes in a\nconditional generative model do not exist, we propose several automated and\nhuman-based methods for evaluation. To do this, we adapt several existing\nmetrics, and assess the automated metrics against gold standard human\nevaluation. We find that using Fr\\'echet Inception Distance (FID) with\nembeddings from an intermediary Inception-V3 layer that precedes the auxiliary\nclassifier produces results most correlated with human realism. While\ninsufficient alone to establish a human-correlated automatic evaluation metric,\nwe believe this work begins to bridge the gap between human and automated\ngenerative evaluation procedures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:59:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhou", "Sharon", ""], ["Luccioni", "Alexandra", ""], ["Cosne", "Gautier", ""], ["Bernstein", "Michael S.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1910.10174", "submitter": "Ciar\\'an M. Gilligan-Lee", "authors": "Ciar\\'an M. Lee, Christopher Hart, Jonathan G. Richens, Saurabh Johri", "title": "Leveraging directed causal discovery to detect latent common causes", "comments": "Presented at Frontiers of AI-Assisted Care 2019 (Scientific\n  Symposium, Stanford Medicine). 13 pages, 5 figures, 4 tables. Comments\n  welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of causal relationships is a fundamental problem in science and\nmedicine. In recent years, many elegant approaches to discovering causal\nrelationships between two variables from observational data have been proposed.\nHowever, most of these deal only with purely directed causal relationships and\ncannot detect latent common causes. Here, we devise a general heuristic which\ntakes a causal discovery algorithm that can only distinguish purely directed\ncausal relations and modifies it to also detect latent common causes. We apply\nour method to two directed causal discovery algorithms, the Information\nGeometric Causal Inference of (Daniusis et al., 2010) and the Kernel\nConditional Deviance for Causal Inference of (Mitrovic, Sejdinovic, & Teh,\n2018), and extensively test on synthetic data -- detecting latent common causes\nin additive, multiplicative and complex noise regimes -- and on real data,\nwhere we are able to detect known common causes. In addition to detecting\nlatent common causes, our experiments demonstrate that both the modified\nalgorithms preserve the performance of the original in distinguishing directed\ncausal relations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 18:00:57 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 13:06:19 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 12:23:41 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee", "Ciar\u00e1n M.", ""], ["Hart", "Christopher", ""], ["Richens", "Jonathan G.", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.10196", "submitter": "Zhenxun Zhuang", "authors": "Zhenxun Zhuang, Yunlong Wang, Kezi Yu, Songtao Lu", "title": "No-regret Non-convex Online Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online meta-learning framework is designed for the continual lifelong\nlearning setting. It bridges two fields: meta-learning which tries to extract\nprior knowledge from past tasks for fast learning of future tasks, and\nonline-learning which deals with the sequential setting where problems are\nrevealed one by one. In this paper, we generalize the original framework from\nconvex to non-convex setting, and introduce the local regret as the alternative\nperformance measure. We then apply this framework to stochastic settings, and\nshow theoretically that it enjoys a logarithmic local regret, and is robust to\nany hyperparameter initialization. The empirical test on a real-world task\ndemonstrates its superiority compared with traditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 18:45:15 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 16:08:21 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 01:10:11 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2020 02:52:52 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhuang", "Zhenxun", ""], ["Wang", "Yunlong", ""], ["Yu", "Kezi", ""], ["Lu", "Songtao", ""]]}, {"id": "1910.10202", "submitter": "Muqiao Yang", "authors": "Muqiao Yang, Martin Q. Ma, Dongyu Li, Yao-Hung Hubert Tsai, Ruslan\n  Salakhutdinov", "title": "Complex Transformer: A Framework for Modeling Complex-Valued Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has received a surge of interest in a variety of fields\nin recent years, major deep learning models barely use complex numbers.\nHowever, speech, signal and audio data are naturally complex-valued after\nFourier Transform, and studies have shown a potentially richer representation\nof complex nets. In this paper, we propose a Complex Transformer, which\nincorporates the transformer model as a backbone for sequence modeling; we also\ndevelop attention and encoder-decoder network operating for complex input. The\nmodel achieves state-of-the-art performance on the MusicNet dataset and an\nIn-phase Quadrature (IQ) signal dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 19:21:12 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Yang", "Muqiao", ""], ["Ma", "Martin Q.", ""], ["Li", "Dongyu", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1910.10211", "submitter": "Amrith Setlur", "authors": "Amrith Setlur, Barnab\\'as P\\'ocz\\'os", "title": "Better Approximate Inference for Partial Likelihood Models with a Latent\n  Structure", "comments": null, "journal-ref": "NeurIPS 2019 Workshop on Learning with Temporal Point Processes", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal Point Processes (TPP) with partial likelihoods involving a latent\nstructure often entail an intractable marginalization, thus making inference\nhard. We propose a novel approach to Maximum Likelihood Estimation (MLE)\ninvolving approximate inference over the latent variables by minimizing a tight\nupper bound on the approximation gap. Given a discrete latent variable $Z$, the\nproposed approximation reduces inference complexity from $O(|Z|^c)$ to\n$O(|Z|)$. We use convex conjugates to determine this upper bound in a closed\nform and show that its addition to the optimization objective results in\nimproved results for models assuming proportional hazards as in Survival\nAnalysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 19:54:11 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 07:59:10 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Setlur", "Amrith", ""], ["P\u00f3cz\u00f3s", "Barnab\u00e1s", ""]]}, {"id": "1910.10245", "submitter": "Ryan Theisen", "authors": "Ryan Theisen, Jason M. Klusowski, Huan Wang, Nitish Shirish Keskar,\n  Caiming Xiong and Richard Socher", "title": "Global Capacity Measures for Deep ReLU Networks via Path Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical results on the statistical complexity of linear models have\ncommonly identified the norm of the weights $\\|w\\|$ as a fundamental capacity\nmeasure. Generalizations of this measure to the setting of deep networks have\nbeen varied, though a frequently identified quantity is the product of weight\nnorms of each layer. In this work, we show that for a large class of networks\npossessing a positive homogeneity property, similar bounds may be obtained\ninstead in terms of the norm of the product of weights. Our proof technique\ngeneralizes a recently proposed sampling argument, which allows us to\ndemonstrate the existence of sparse approximants of positive homogeneous\nnetworks. This yields covering number bounds, which can be converted to\ngeneralization bounds for multi-class classification that are comparable to,\nand in certain cases improve upon, existing results in the literature. Finally,\nwe investigate our sampling procedure empirically, which yields results\nconsistent with our theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 21:49:44 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Theisen", "Ryan", ""], ["Klusowski", "Jason M.", ""], ["Wang", "Huan", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1910.10252", "submitter": "Kangkang Wang", "authors": "Kangkang Wang, Rajiv Mathews, Chlo\\'e Kiddon, Hubert Eichner,\n  Fran\\c{c}oise Beaufays, Daniel Ramage", "title": "Federated Evaluation of On-device Personalization", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed, on-device computation framework that\nenables training global models without exporting sensitive user data to\nservers. In this work, we describe methods to extend the federation framework\nto evaluate strategies for personalization of global models. We present tools\nto analyze the effects of personalization and evaluate conditions under which\npersonalization yields desirable models. We report on our experiments\npersonalizing a language model for a virtual keyboard for smartphones with a\npopulation of tens of millions of users. We show that a significant fraction of\nusers benefit from personalization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:16:15 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Wang", "Kangkang", ""], ["Mathews", "Rajiv", ""], ["Kiddon", "Chlo\u00e9", ""], ["Eichner", "Hubert", ""], ["Beaufays", "Fran\u00e7oise", ""], ["Ramage", "Daniel", ""]]}, {"id": "1910.10262", "submitter": "Ali Hasan", "authors": "Ali Hasan, Jo\\~ao M. Pereira, Robert Ravier, Sina Farsiu, Vahid Tarokh", "title": "Learning Partial Differential Equations from Data Using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for estimating unknown partial differential equations\nfrom noisy data, using a deep learning approach. Given noisy samples of a\nsolution to an unknown PDE, our method interpolates the samples using a neural\nnetwork, and extracts the PDE by equating derivatives of the neural network\napproximation. Our method applies to PDEs which are linear combinations of\nuser-defined dictionary functions, and generalizes previous methods that only\nconsider parabolic PDEs. We introduce a regularization scheme that prevents the\nfunction approximation from overfitting the data and forces it to be a solution\nof the underlying PDE. We validate the model on simulated data generated by the\nknown PDEs and added Gaussian noise, and we study our method under different\nlevels of noise. We also compare the error of our method with a Cramer-Rao\nlower bound for an ordinary differential equation. Our results indicate that\nour method outperforms other methods in estimating PDEs, especially in the low\nsignal-to-noise regime.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:38:50 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Hasan", "Ali", ""], ["Pereira", "Jo\u00e3o M.", ""], ["Ravier", "Robert", ""], ["Farsiu", "Sina", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.10266", "submitter": "Uchenna Akujuobi", "authors": "Uchenna Akujuobi, Qiannan Zhang, Han Yufei, Xiangliang Zhang", "title": "Recurrent Attention Walk for Semi-supervised Classification", "comments": "Accepted for WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the graph-based semi-supervised learning for\nclassifying nodes in attributed networks, where the nodes and edges possess\ncontent information. Recent approaches like graph convolution networks and\nattention mechanisms have been proposed to ensemble the first-order neighbors\nand incorporate the relevant neighbors. However, it is costly (especially in\nmemory) to consider all neighbors without a prior differentiation. We propose\nto explore the neighborhood in a reinforcement learning setting and find a walk\npath well-tuned for classifying the unlabelled target nodes. We let an agent\n(of node classification task) walk over the graph and decide where to direct to\nmaximize classification accuracy. We define the graph walk as a partially\nobservable Markov decision process (POMDP). The proposed method is flexible for\nworking in both transductive and inductive setting. Extensive experiments on\nfour datasets demonstrate that our proposed method outperforms several\nstate-of-the-art methods. Several case studies also illustrate the meaningful\nmovement trajectory made by the agent.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:53:32 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Akujuobi", "Uchenna", ""], ["Zhang", "Qiannan", ""], ["Yufei", "Han", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1910.10271", "submitter": "Michal Yemini", "authors": "Michal Yemini, Amir Leshem, Anelia Somekh-Baruch", "title": "The Restless Hidden Markov Bandit with Linear Rewards and Side\n  Information", "comments": "Accepted for publication in the IEEE Transactions on Signal\n  Processing. A summary of the results presented in this paper was accepted to\n  the 59th Conference on Decision and Control (CDC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a model for the hidden Markovian bandit problem with\nlinear rewards. As opposed to current work on Markovian bandits, we do not\nassume that the state is known to the decision maker before making the\ndecision. Furthermore, we assume structural side information where the decision\nmaker knows in advance that there are two types of hidden states; one is common\nto all arms and evolves according to a Markovian distribution, and the other is\nunique to each arm and is distributed according to an i.i.d. process that is\nunique to each arm. We present an algorithm and regret analysis to this\nproblem. Surprisingly, we can recover the hidden states and maintain\nlogarithmic regret in the case of a convex polytope action set. Furthermore, we\nshow that the structural side information leads to expected regret that does\nnot depend on the number of extreme points in the action space. Therefore, we\nobtain practical solutions even in high dimensional problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 23:13:44 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:50:59 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 05:20:14 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 19:14:41 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Yemini", "Michal", ""], ["Leshem", "Amir", ""], ["Somekh-Baruch", "Anelia", ""]]}, {"id": "1910.10277", "submitter": "Sephora Madjiheurem", "authors": "Sephora Madjiheurem and Laura Toni", "title": "State2vec: Off-Policy Successor Features Approximators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in reinforcement learning (RL) is the design of agents that\nare able to generalize across tasks that share common dynamics. A viable\nsolution is meta-reinforcement learning, which identifies common structures\namong past tasks to be then generalized to new tasks (meta-test). In\nmeta-training, the RL agent learns state representations that encode prior\ninformation from a set of tasks, used to generalize the value function\napproximation. This has been proposed in the literature as successor\nrepresentation approximators. While promising, these methods do not generalize\nwell across optimal policies, leading to sampling-inefficiency during meta-test\nphases. In this paper, we propose state2vec, an efficient and low-complexity\nframework for learning successor features which (i) generalize across policies,\n(ii) ensure sample-efficiency during meta-test. We extend the well known\nnode2vec framework to learn state embeddings that account for the discounted\nfuture state transitions in RL. The proposed off-policy state2vec captures the\ngeometry of the underlying state space, making good basis functions for linear\nvalue function approximation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 23:22:58 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Madjiheurem", "Sephora", ""], ["Toni", "Laura", ""]]}, {"id": "1910.10294", "submitter": "Mohit Rajpal", "authors": "Mohit Rajpal and Bryan Kian Hsiang Low", "title": "A Unifying Framework of Bilinear LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unifying framework of bilinear LSTMs that can\nrepresent and utilize the nonlinear interaction of the input features present\nin sequence datasets for achieving superior performance over a linear LSTM and\nyet not incur more parameters to be learned. To realize this, our unifying\nframework allows the expressivity of the linear vs. bilinear terms to be\nbalanced by correspondingly trading off between the hidden state vector size\nvs. approximation quality of the weight matrix in the bilinear term so as to\noptimize the performance of our bilinear LSTM, while not incurring more\nparameters to be learned. We empirically evaluate the performance of our\nbilinear LSTM in several language-based sequence learning tasks to demonstrate\nits general applicability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 00:50:29 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Rajpal", "Mohit", ""], ["Low", "Bryan Kian Hsiang", ""]]}, {"id": "1910.10307", "submitter": "Vahdat Abdelzad", "authors": "Vahdat Abdelzad, Krzysztof Czarnecki, Rick Salay, Taylor Denounden,\n  Sachin Vernekar, Buu Phan", "title": "Detecting Out-of-Distribution Inputs in Deep Neural Networks Using an\n  Early-Layer Output", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve superior performance in challenging tasks such\nas image classification. However, deep classifiers tend to incorrectly classify\nout-of-distribution (OOD) inputs, which are inputs that do not belong to the\nclassifier training distribution. Several approaches have been proposed to\ndetect OOD inputs, but the detection task is still an ongoing challenge. In\nthis paper, we propose a new OOD detection approach that can be easily applied\nto an existing classifier and does not need to have access to OOD samples. The\ndetector is a one-class classifier trained on the output of an early layer of\nthe original classifier fed with its original training set. We apply our\napproach to several low- and high-dimensional datasets and compare it to the\nstate-of-the-art detection approaches. Our approach achieves substantially\nbetter results over multiple metrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 01:27:48 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Abdelzad", "Vahdat", ""], ["Czarnecki", "Krzysztof", ""], ["Salay", "Rick", ""], ["Denounden", "Taylor", ""], ["Vernekar", "Sachin", ""], ["Phan", "Buu", ""]]}, {"id": "1910.10308", "submitter": "Yilin Kang", "authors": "Yilin Kang, Yong Liu, Weiping Wang", "title": "Weighted Distributed Differential Privacy ERM: Convex and Non-convex", "comments": null, "journal-ref": null, "doi": "10.1016/j.cose.2021.102275", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning is an approach allowing different parties to\nlearn a model over all data sets without disclosing their own data. In this\npaper, we propose a weighted distributed differential privacy (WD-DP) empirical\nrisk minimization (ERM) method to train a model in distributed setting,\nconsidering different weights of different clients. We guarantee differential\nprivacy by gradient perturbation, adding Gaussian noise, and advance the\nstate-of-the-art on gradient perturbation method in distributed setting. By\ndetailed theoretical analysis, we show that in distributed setting, the noise\nbound and the excess empirical risk bound can be improved by considering\ndifferent weights held by multiple parties. Moreover, considering that the\nconstraint of convex loss function in ERM is not easy to achieve in some\nsituations, we generalize our method to non-convex loss functions which satisfy\nPolyak-Lojasiewicz condition. Experiments on real data sets show that our\nmethod is more reliable and we improve the performance of distributed\ndifferential privacy ERM, especially in the case that data scale on different\nclients is uneven.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 01:28:51 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:27:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kang", "Yilin", ""], ["Liu", "Yong", ""], ["Wang", "Weiping", ""]]}, {"id": "1910.10320", "submitter": "Shuhan Tan", "authors": "Shuhan Tan, Xingchao Peng, Kate Saenko", "title": "Class-imbalanced Domain Adaptation: An Empirical Odyssey", "comments": "ECCV 2020 Workshops - TASK-CV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is a promising way to generalize deep models\nto novel domains. However, the current literature assumes that the label\ndistribution is domain-invariant and only aligns the feature distributions or\nvice versa. In this work, we explore the more realistic task of\nClass-imbalanced Domain Adaptation: How to align feature distributions across\ndomains while the label distributions of the two domains are also different?\nTaking a practical step towards this problem, we constructed the first\nbenchmark with 22 cross-domain tasks from 6real-image datasets. We conducted\ncomprehensive experiments on 10 recent domain adaptation methods and find most\nof them are very fragile in the face of coexisting feature and label\ndistribution shift. Towards a better solution, we further proposed a feature\nand label distribution CO-ALignment (COAL) model with a novel combination of\nexisting ideas. COAL is empirically shown to outperform the most recent domain\nadaptation methods on our benchmarks. We believe the provided benchmarks,\nempirical analysis results, and the COAL baseline could stimulate and\nfacilitate future research towards this important problem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:35:46 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 07:58:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tan", "Shuhan", ""], ["Peng", "Xingchao", ""], ["Saenko", "Kate", ""]]}, {"id": "1910.10335", "submitter": "Amila Silva", "authors": "Amila Silva, Shanika Karunasekera, Christopher Leckie and Ling Luo", "title": "USTAR: Online Multimodal Embedding for Modeling User-Guided\n  Spatiotemporal Activity", "comments": "10 pages, IEEE International Conference on Big Data 2019 (IEEE Big\n  Data 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building spatiotemporal activity models for people's activities in urban\nspaces is important for understanding the ever-increasing complexity of urban\ndynamics. With the emergence of Geo-Tagged Social Media (GTSM) records,\nprevious studies demonstrate the potential of GTSM records for spatiotemporal\nactivity modeling. State-of-the-art methods for this task embed different\nmodalities (location, time, and text) of GTSM records into a single embedding\nspace. However, they ignore Non-GeoTagged Social Media (NGTSM) records, which\ngenerally account for the majority of posts (e.g., more than 95\\% in Twitter),\nand could represent a great source of information to alleviate the sparsity of\nGTSM records. Furthermore, in the current spatiotemporal embedding techniques,\nless focus has been given to the users, who exhibit spatially motivated\nbehaviors. To bridge this research gap, this work proposes USTAR, a novel\nonline learning method for User-guided SpatioTemporal Activity Representation,\nwhich (1) embeds locations, time, and text along with users into the same\nembedding space to capture their correlations; (2) uses a novel collaborative\nfiltering approach based on two different empirically studied user behaviors to\nincorporate both NGTSM and GTSM records in learning; and (3) introduces a novel\nsampling technique to learn spatiotemporal representations in an online fashion\nto accommodate recent information into the embedding space, while avoiding\noverfitting to recent records and frequently appearing units in social media\nstreams. Our results show that USTAR substantially improves the\nstate-of-the-art for region retrieval and keyword retrieval and its potential\nto be applied to other downstream applications such as local event detection.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:02:20 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Silva", "Amila", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""], ["Luo", "Ling", ""]]}, {"id": "1910.10341", "submitter": "Suya Wu", "authors": "Suya Wu, Enmao Diao, Jie Ding, Vahid Tarokh", "title": "Deep Clustering of Compressed Variational Embeddings", "comments": "Submitted to the IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP), Barcelona, Spain, May 2020", "journal-ref": null, "doi": "10.1109/DCC47342.2020.00051", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the ever-increasing demands for limited communication bandwidth\nand low-power consumption, we propose a new methodology, named joint\nVariational Autoencoders with Bernoulli mixture models (VAB), for performing\nclustering in the compressed data domain. The idea is to reduce the data\ndimension by Variational Autoencoders (VAEs) and group data representations by\nBernoulli mixture models (BMMs). Once jointly trained for compression and\nclustering, the model can be decomposed into two parts: a data vendor that\nencodes the raw data into compressed data, and a data consumer that classifies\nthe received (compressed) data. In this way, the data vendor benefits from data\nsecurity and communication bandwidth, while the data consumer benefits from low\ncomputational complexity. To enable training using the gradient descent\nalgorithm, we propose to use the Gumbel-Softmax distribution to resolve the\ninfeasibility of the back-propagation algorithm when assessing categorical\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:14:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wu", "Suya", ""], ["Diao", "Enmao", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.10352", "submitter": "Liang Lu", "authors": "Liang Lu", "title": "A Transformer with Interleaved Self-attention and Convolution for Hybrid\n  Acoustic Models", "comments": "5 pages, submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer with self-attention has achieved great success in the area of\nnature language processing. Recently, there have been a few studies on\ntransformer for end-to-end speech recognition, while its application for hybrid\nacoustic model is still very limited. In this paper, we revisit the\ntransformer-based hybrid acoustic model, and propose a model structure with\ninterleaved self-attention and 1D convolution, which is proven to have faster\nconvergence and higher recognition accuracy. We also study several aspects of\nthe transformer model, including the impact of the positional encoding feature,\ndropout regularization, as well as training with and without time restriction.\nWe show competitive recognition results on the public Librispeech dataset when\ncompared to the Kaldi baseline at both cross entropy training and sequence\ntraining stages. For reproducible research, we release our source code and\nrecipe within the PyKaldi2 toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:57:51 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lu", "Liang", ""]]}, {"id": "1910.10356", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu", "title": "EdgeAI: A Vision for Deep Learning in IoT Era", "comments": "To appear in IEEE Design and Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant computational requirements of deep learning present a major\nbottleneck for its large-scale adoption on hardware-constrained IoT-devices.\nHere, we envision a new paradigm called EdgeAI to address major impediments\nassociated with deploying deep networks at the edge. Specifically, we discuss\nthe existing directions in computation-aware deep learning and describe two new\nchallenges in the IoT era: (1) Data-independent deployment of learning, and (2)\nCommunication-aware distributed inference. We further present new directions\nfrom our recent research to alleviate the latter two challenges. Overcoming\nthese challenges is crucial for rapid adoption of learning on IoT-devices in\norder to truly enable EdgeAI.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:16:32 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Suda", "Naveen", ""], ["Marculescu", "Radu", ""]]}, {"id": "1910.10362", "submitter": "John Miller", "authors": "John Miller, Smitha Milli, Moritz Hardt", "title": "Strategic Classification is Causal Modeling in Disguise", "comments": "This paper was previously titled \"Strategic Adaptation to\n  Classifiers: A Causal Perspective.\" The current version subsumes all previous\n  versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consequential decision-making incentivizes individuals to strategically adapt\ntheir behavior to the specifics of the decision rule. While a long line of work\nhas viewed strategic adaptation as gaming and attempted to mitigate its\neffects, recent work has instead sought to design classifiers that incentivize\nindividuals to improve a desired quality. Key to both accounts is a cost\nfunction that dictates which adaptations are rational to undertake. In this\nwork, we develop a causal framework for strategic adaptation. Our causal\nperspective clearly distinguishes between gaming and improvement and reveals an\nimportant obstacle to incentive design. We prove any procedure for designing\nclassifiers that incentivize improvement must inevitably solve a non-trivial\ncausal inference problem. Moreover, we show a similar result holds for\ndesigning cost functions that satisfy the requirements of previous work. With\nthe benefit of hindsight, our results show much of the prior work on strategic\nclassification is causal modeling in disguise.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:36:59 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 07:41:51 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 01:24:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Miller", "John", ""], ["Milli", "Smitha", ""], ["Hardt", "Moritz", ""]]}, {"id": "1910.10367", "submitter": "Sanjay Thakur", "authors": "Sanjay Thakur, Herke Van Hoof, Gunshi Gupta, David Meger", "title": "Unifying Variational Inference and PAC-Bayes for Supervised Learning\n  that Scales", "comments": "13 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural Network based controllers hold enormous potential to learn complex,\nhigh-dimensional functions. However, they are prone to overfitting and\nunwarranted extrapolations. PAC Bayes is a generalized framework which is more\nresistant to overfitting and that yields performance bounds that hold with\narbitrarily high probability even on the unjustified extrapolations. However,\noptimizing to learn such a function and a bound is intractable for complex\ntasks. In this work, we propose a method to simultaneously learn such a\nfunction and estimate performance bounds that scale organically to\nhigh-dimensions, non-linear environments without making any explicit\nassumptions about the environment. We build our approach on a parallel that we\ndraw between the formulations called ELBO and PAC Bayes when the risk metric is\nnegative log likelihood. Through our experiments on multiple high dimensional\nMuJoCo locomotion tasks, we validate the correctness of our theory, show its\nability to generalize better, and investigate the factors that are important\nfor its learning. The code for all the experiments is available at\nhttps://bit.ly/2qv0JjA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:48:26 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 15:37:20 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Thakur", "Sanjay", ""], ["Van Hoof", "Herke", ""], ["Gupta", "Gunshi", ""], ["Meger", "David", ""]]}, {"id": "1910.10368", "submitter": "Dongmin Park", "authors": "Dongmin Park, Susik Yoon, Hwanjun Song, Jae-Gil Lee", "title": "MLAT: Metric Learning for kNN in Streaming Time Series", "comments": "In Proc. 5th Workshop on Mining and Learning from Time Series\n  (MiLeTS) in conjunction with KDD'19", "journal-ref": "MileTS. Workshop on SIGKDD'19. (2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good distance measure for distance-based classification in time\nseries leads to significant performance improvement in many tasks.\nSpecifically, it is critical to effectively deal with variations and temporal\ndependencies in time series. However, existing metric learning approaches focus\non tackling variations mainly using a strict alignment of two sequences,\nthereby being not able to capture temporal dependencies. To overcome this\nlimitation, we propose MLAT, which covers both alignment and temporal\ndependencies at the same time. MLAT achieves the alignment effect as well as\npreserves temporal dependencies by augmenting a given time series using a\nsliding window. Furthermore, MLAT employs time-invariant metric learning to\nderive the most appropriate distance measure from the augmented samples which\ncan also capture the temporal dependencies among them well. We show that MLAT\noutperforms other existing algorithms in the extensive experiments on various\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:48:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Park", "Dongmin", ""], ["Yoon", "Susik", ""], ["Song", "Hwanjun", ""], ["Lee", "Jae-Gil", ""]]}, {"id": "1910.10386", "submitter": "Felix McGregor Mr", "authors": "Felix McGregor, Arnu Pretorius, Johan du Preez, Steve Kroon", "title": "Stabilising priors for robust Bayesian deep learning", "comments": "3 pages, accepted at Bayesian Deep learning workshop NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural networks (BNNs) have developed into useful tools for\nprobabilistic modelling due to recent advances in variational inference\nenabling large scale BNNs. However, BNNs remain brittle and hard to train,\nespecially: (1) when using deep architectures consisting of many hidden layers\nand (2) in situations with large weight variances. We use signal propagation\ntheory to quantify these challenges and propose self-stabilising priors. This\nis achieved by a reformulation of the ELBO to allow the prior to influence\nnetwork signal propagation. Then, we develop a stabilising prior, where the\ndistributional parameters of the prior are adjusted before each forward pass to\nensure stability of the propagating signal. This stabilised signal propagation\nleads to improved convergence and robustness making it possible to train deeper\nnetworks and in more noisy settings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 07:01:17 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["McGregor", "Felix", ""], ["Pretorius", "Arnu", ""], ["Preez", "Johan du", ""], ["Kroon", "Steve", ""]]}, {"id": "1910.10397", "submitter": "Heung-Chang Lee", "authors": "Heung-Chang Lee, Do-Guk Kim, Bohyung Han", "title": "Efficient Decoupled Neural Architecture Search by Structure and\n  Operation Sampling", "comments": null, "journal-ref": "IEEE ICASSP 2020", "doi": "10.1109/ICASSP40776.2020.9053197", "report-no": "9053197", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural architecture search algorithm via reinforcement\nlearning by decoupling structure and operation search processes. Our approach\nsamples candidate models from the multinomial distribution on the policy\nvectors defined on the two search spaces independently. The proposed technique\nimproves the efficiency of architecture search process significantly compared\nto the conventional methods based on reinforcement learning with the RNN\ncontrollers while achieving competitive accuracy and model size in target\ntasks. Our policy vectors are easily interpretable throughout the training\nprocedure, which allows to analyze the search progress and the discovered\narchitectures; the black-box characteristics of the RNN controllers hamper\nunderstanding training progress in terms of policy parameter updates. Our\nexperiments demonstrate outstanding performance compared to the\nstate-of-the-art methods with a fraction of search cost.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:00:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lee", "Heung-Chang", ""], ["Kim", "Do-Guk", ""], ["Han", "Bohyung", ""]]}, {"id": "1910.10398", "submitter": "Christoph Angermann", "authors": "Christoph Angermann and Markus Haltmeier", "title": "Random 2.5D U-net for Fully 3D Segmentation", "comments": "Submission for joint MICCAI-Workshops on Computing and Visualization\n  for Intravascular Imaging and Computer Assisted Stenting (CVII-STENT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are state-of-the-art for various segmentation\ntasks. While for 2D images these networks are also computationally efficient,\n3D convolutions have huge storage requirements and therefore, end-to-end\ntraining is limited by GPU memory and data size. To overcome this issue, we\nintroduce a network structure for volumetric data without 3D convolution\nlayers. The main idea is to include projections from different directions to\ntransform the volumetric data to a sequence of images, where each image\ncontains information of the full data. We then apply 2D convolutions to these\nprojection images and lift them again to volumetric data using a trainable\nreconstruction algorithm. The proposed architecture can be applied end-to-end\nto very large data volumes without cropping or sliding-window techniques. For a\ntested sparse binary segmentation task, it outperforms already known standard\napproaches and is more resistant to generation of artefacts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:02:09 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Angermann", "Christoph", ""], ["Haltmeier", "Markus", ""]]}, {"id": "1910.10428", "submitter": "Mahdi Boloursaz Mashhadi", "authors": "Mahdi Boloursaz Mashhadi, Qianqian Yang, Deniz Gunduz", "title": "CNN-based Analog CSI Feedback in FDD MIMO-OFDM Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multiple-input multiple-output (MIMO) systems require downlink\nchannel state information (CSI) at the base station (BS) to better utilize the\navailable spatial diversity and multiplexing gains. However, in a frequency\ndivision duplex (FDD) massive MIMO system, CSI feedback overhead degrades the\noverall spectral efficiency. Convolutional neural network (CNN)-based CSI\nfeedback compression schemes has received a lot of attention recently due to\nsignificant improvements in compression efficiency; however, they still require\nreliable feedback links to convey the compressed CSI information to the BS.\nInstead, we propose here a CNN-based analog feedback scheme, called\nAnalogDeepCMC, which directly maps the downlink CSI to uplink channel input.\nCorresponding noisy channel outputs are used by another CNN to reconstruct the\nDL channel estimate. Not only the proposed outperforms existing digital CSI\nfeedback schemes in terms of the achievable downlink rate, but also simplifies\nthe operation as it does not require explicit quantization, coding and\nmodulation, and provides a low-latency alternative particularly in rapidly\nchanging MIMO channels, where the CSI needs to be estimated and fed back\nperiodically.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:26:13 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Mashhadi", "Mahdi Boloursaz", ""], ["Yang", "Qianqian", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1910.10453", "submitter": "Jihong Park", "authors": "Anis Elgabli, Jihong Park, Amrit S. Bedi, Chaouki Ben Issaid, Mehdi\n  Bennis, Vaneet Aggarwal", "title": "Q-GADMM: Quantized Group ADMM for Communication Efficient Decentralized\n  Machine Learning", "comments": "19 pages, 8 figures; to appear in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a communication-efficient decentralized machine\nlearning (ML) algorithm, coined quantized group ADMM (Q-GADMM). To reduce the\nnumber of communication links, every worker in Q-GADMM communicates only with\ntwo neighbors, while updating its model via the group alternating direction\nmethod of multipliers (GADMM). Moreover, each worker transmits the quantized\ndifference between its current model and its previously quantized model,\nthereby decreasing the communication payload size. However, due to the lack of\ncentralized entity in decentralized ML, the spatial sparsity and payload\ncompression may incur error propagation, hindering model training convergence.\nTo overcome this, we develop a novel stochastic quantization method to\nadaptively adjust model quantization levels and their probabilities, while\nproving the convergence of Q-GADMM for convex objective functions. Furthermore,\nto demonstrate the feasibility of Q-GADMM for non-convex and stochastic\nproblems, we propose quantized stochastic GADMM (Q-SGADMM) that incorporates\ndeep neural network architectures and stochastic sampling. Simulation results\ncorroborate that Q-GADMM significantly outperforms GADMM in terms of\ncommunication efficiency while achieving the same accuracy and convergence\nspeed for a linear regression task. Similarly, for an image classification task\nusing DNN, Q-SGADMM achieves significantly less total communication cost with\nidentical accuracy and convergence speed compared to its counterpart without\nquantization, i.e., stochastic GADMM (SGADMM).\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 10:47:06 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 13:40:21 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 17:31:00 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 19:33:13 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 09:37:46 GMT"}, {"version": "v6", "created": "Sat, 3 Oct 2020 18:28:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Elgabli", "Anis", ""], ["Park", "Jihong", ""], ["Bedi", "Amrit S.", ""], ["Issaid", "Chaouki Ben", ""], ["Bennis", "Mehdi", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1910.10455", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Danda Pani Paudel, Guanju Li, Jiqing Wu, Radu Timofte,\n  Luc Van Gool", "title": "Divide-and-Conquer Adversarial Learning for High-Resolution Image and\n  Video Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a divide-and-conquer inspired adversarial learning\n(DACAL) approach for photo enhancement. The key idea is to decompose the photo\nenhancement process into hierarchically multiple sub-problems, which can be\nbetter conquered from bottom to up. On the top level, we propose a\nperception-based division to learn additive and multiplicative components,\nrequired to translate a low-quality image or video into its high-quality\ncounterpart. On the intermediate level, we use a frequency-based division with\ngenerative adversarial network (GAN) to weakly supervise the photo enhancement\nprocess. On the lower level, we design a dimension-based division that enables\nthe GAN model to better approximates the distribution distance on multiple\nindependent one-dimensional data to train the GAN model. While considering all\nthree hierarchies, we develop multiscale and recurrent training approaches to\noptimize the image and video enhancement process in a weakly-supervised manner.\nBoth quantitative and qualitative results clearly demonstrate that the proposed\nDACAL achieves the state-of-the-art performance for high-resolution image and\nvideo enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:00:51 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Li", "Guanju", ""], ["Wu", "Jiqing", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1910.10479", "submitter": "Yong-Siang Shih", "authors": "Yong-Siang Shih, Wei-Cheng Chang, Yiming Yang", "title": "XL-Editor: Post-editing Sentences with XLNet", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural sequence generation models achieve initial success for many NLP\napplications, the canonical decoding procedure with left-to-right generation\norder (i.e., autoregressive) in one-pass can not reflect the true nature of\nhuman revising a sentence to obtain a refined result. In this work, we propose\nXL-Editor, a novel training framework that enables state-of-the-art generalized\nautoregressive pretraining methods, XLNet specifically, to revise a given\nsentence by the variable-length insertion probability. Concretely, XL-Editor\ncan (1) estimate the probability of inserting a variable-length sequence into a\nspecific position of a given sentence; (2) execute post-editing operations such\nas insertion, deletion, and replacement based on the estimated variable-length\ninsertion probability; (3) complement existing sequence-to-sequence models to\nrefine the generated sequences. Empirically, we first demonstrate better\npost-editing capabilities of XL-Editor over XLNet on the text insertion and\ndeletion tasks, which validates the effectiveness of our proposed framework.\nFurthermore, we extend XL-Editor to the unpaired text style transfer task,\nwhere transferring the target style onto a given sentence can be naturally\nviewed as post-editing the sentence into the target style. XL-Editor achieves\nsignificant improvement in style transfer accuracy and also maintains coherent\nsemantic of the original sentence, showing the broad applicability of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 21:39:03 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Shih", "Yong-Siang", ""], ["Chang", "Wei-Cheng", ""], ["Yang", "Yiming", ""]]}, {"id": "1910.10485", "submitter": "Gabriele Prato", "authors": "Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh", "title": "Fully Quantized Transformer for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural machine translation methods employ massive amounts of\nparameters. Drastically reducing computational costs of such methods without\naffecting performance has been up to this point unsuccessful. To this end, we\npropose FullyQT: an all-inclusive quantization strategy for the Transformer. To\nthe best of our knowledge, we are the first to show that it is possible to\navoid any loss in translation quality with a fully quantized Transformer.\nIndeed, compared to full-precision, our 8-bit models score greater or equal\nBLEU on most tasks. Comparing ourselves to all previously proposed methods, we\nachieve state-of-the-art quantization results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 01:29:12 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 22:44:17 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 19:06:55 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Prato", "Gabriele", ""], ["Charlaix", "Ella", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "1910.10487", "submitter": "David Donahue", "authors": "David Donahue, Yuanliang Meng, Anna Rumshisky", "title": "Memory-Augmented Recurrent Networks for Dialogue Coherence", "comments": "Honors project, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent dialogue approaches operate by reading each word in a conversation\nhistory, and aggregating accrued dialogue information into a single state. This\nfixed-size vector is not expandable and must maintain a consistent format over\ntime. Other recent approaches exploit an attention mechanism to extract useful\ninformation from past conversational utterances, but this introduces an\nincreased computational complexity. In this work, we explore the use of the\nNeural Turing Machine (NTM) to provide a more permanent and flexible storage\nmechanism for maintaining dialogue coherence. Specifically, we introduce two\nseparate dialogue architectures based on this NTM design. The first design\nfeatures a sequence-to-sequence architecture with two separate NTM modules, one\nfor each participant in the conversation. The second memory architecture\nincorporates a single NTM module, which stores parallel context information for\nboth speakers. This second design also replaces the sequence-to-sequence\narchitecture with a neural language model, to allow for longer context of the\nNTM and greater understanding of the dialogue history. We report perplexity\nperformance for both models, and compare them to existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:59:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Donahue", "David", ""], ["Meng", "Yuanliang", ""], ["Rumshisky", "Anna", ""]]}, {"id": "1910.10488", "submitter": "David Donahue", "authors": "David Donahue, Vladislav Lialin, Anna Rumshisky", "title": "Injecting Hierarchy with U-Net Transformers", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:48:46 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 19:41:09 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Donahue", "David", ""], ["Lialin", "Vladislav", ""], ["Rumshisky", "Anna", ""]]}, {"id": "1910.10491", "submitter": "Rajhersh Patel", "authors": "Raj Patel, Carlotta Domeniconi", "title": "Estimator Vectors: OOV Word Embeddings based on Subword and Context Clue\n  Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic representations of words have been successfully extracted from\nunlabeled corpuses using neural network models like word2vec. These\nrepresentations are generally high quality and are computationally inexpensive\nto train, making them popular. However, these approaches generally fail to\napproximate out of vocabulary (OOV) words, a task humans can do quite easily,\nusing word roots and context clues. This paper proposes a neural network model\nthat learns high quality word representations, subword representations, and\ncontext clue representations jointly. Learning all three types of\nrepresentations together enhances the learning of each, leading to enriched\nword vectors, along with strong estimates for OOV words, via the combination of\nthe corresponding context clue and subword embeddings. Our model, called\nEstimator Vectors (EV), learns strong word embeddings and is competitive with\nstate of the art methods for OOV estimation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:17:07 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Patel", "Raj", ""], ["Domeniconi", "Carlotta", ""]]}, {"id": "1910.10513", "submitter": "Puning Zhao", "authors": "Puning Zhao, Lifeng Lai", "title": "Minimax Rate Optimal Adaptive Nearest Neighbor Classification and\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k Nearest Neighbor (kNN) method is a simple and popular statistical method\nfor classification and regression. For both classification and regression\nproblems, existing works have shown that, if the distribution of the feature\nvector has bounded support and the probability density function is bounded away\nfrom zero in its support, the convergence rate of the standard kNN method, in\nwhich k is the same for all test samples, is minimax optimal. On the contrary,\nif the distribution has unbounded support, we show that there is a gap between\nthe convergence rate achieved by the standard kNN method and the minimax bound.\nTo close this gap, we propose an adaptive kNN method, in which different k is\nselected for different samples. Our selection rule does not require precise\nknowledge of the underlying distribution of features. The new proposed method\nsignificantly outperforms the standard one. We characterize the convergence\nrate of the proposed adaptive method, and show that it matches the minimax\nlower bound.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:56:53 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Zhao", "Puning", ""], ["Lai", "Lifeng", ""]]}, {"id": "1910.10536", "submitter": "Marc Ru{\\ss}wurm", "authors": "Marc Ru{\\ss}wurm, Marco K\\\"orner", "title": "Self-attention for raw optical Satellite Time Series Classification", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 169,\n  November 2020, Pages 421-435", "doi": "10.1016/j.isprsjprs.2020.06.006", "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of available Earth observation data has increased dramatically in\nthe recent years. Efficiently making use of the entire body information is a\ncurrent challenge in remote sensing and demands for light-weight\nproblem-agnostic models that do not require region- or problem-specific expert\nknowledge. End-to-end trained deep learning models can make use of raw sensory\ndata by learning feature extraction and classification in one step solely from\ndata. Still, many methods proposed in remote sensing research require implicit\nfeature extraction through data preprocessing or explicit design of features.\n  In this work, we compare recent deep learning models on crop type\nclassification on raw and preprocessed Sentinel 2 data. We concentrate on the\ncommon neural network architectures for time series, i.e., 1D-convolutions,\nrecurrence, a shallow random forest baseline, and focus on the novel\nself-attention architecture. Our central findings are that data preprocessing\nstill increased the overall classification performance for all models while the\nchoice of model was less crucial. Self-attention and recurrent neural networks,\nby their architecture, outperformed convolutional neural networks on raw\nsatellite time series. We explore this by a feature importance analysis based\non gradient back-propagation that exploits the differentiable nature of deep\nlearning models. Further, we qualitatively show how self-attention scores focus\nselectively on few classification-relevant observations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 12:56:16 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 15:25:48 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 12:04:50 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ru\u00dfwurm", "Marc", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1910.10562", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Arun K. Kuchibhotla, Aaditya K. Ramdas", "title": "Nested conformal prediction and quantile out-of-bag ensemble methods", "comments": "38 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a popular tool for providing valid prediction sets\nfor classification and regression problems, without relying on any\ndistributional assumptions on the data. While the traditional description of\nconformal prediction starts with a nonconformity score, we provide an alternate\n(but equivalent) view that starts with a sequence of nested sets and calibrates\nthem to find a valid prediction set. The nested framework subsumes all\nnonconformity scores, including recent proposals based on quantile regression\nand density estimation. While these ideas were originally derived based on\nsample splitting, our framework seamlessly extends them to other aggregation\nschemes like cross-conformal, jackknife+ and out-of-bag methods. We use the\nframework to derive a new algorithm (QOOB, pronounced cube) that combines four\nideas: quantile regression, cross-conformalization, ensemble methods and\nout-of-bag predictions. We develop a computationally efficient implementation\nof cross-conformal, that is also used by QOOB. In a detailed numerical\ninvestigation, QOOB performs either the best or close to the best on all\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:44:38 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:26:16 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 03:29:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Kuchibhotla", "Arun K.", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "1910.10566", "submitter": "Sophie Giffard-Roisin", "authors": "Sophie Giffard-Roisin, Mo Yang, Guillaume Charpiat, Christina\n  Kumler-Bonfanti, Bal\\'azs K\\'egl and Claire Monteleoni", "title": "Tropical Cyclone Track Forecasting using Fused Deep Learning from\n  Aligned Reanalysis Data", "comments": null, "journal-ref": null, "doi": "10.3389/fdata.2020.00001", "report-no": null, "categories": "physics.ao-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forecast of tropical cyclone trajectories is crucial for the protection\nof people and property. Although forecast dynamical models can provide\nhigh-precision short-term forecasts, they are computationally demanding, and\ncurrent statistical forecasting models have much room for improvement given\nthat the database of past hurricanes is constantly growing. Machine learning\nmethods, that can capture non-linearities and complex relations, have only been\nscarcely tested for this application. We propose a neural network model fusing\npast trajectory data and reanalysis atmospheric images (wind and pressure 3D\nfields). We use a moving frame of reference that follows the storm center for\nthe 24h tracking forecast. The network is trained to estimate the longitude and\nlatitude displacement of tropical cyclones and depressions from a large\ndatabase from both hemispheres (more than 3000 storms since 1979, sampled at a\n6 hour frequency). The advantage of the fused network is demonstrated and a\ncomparison with current forecast models shows that deep learning methods could\nprovide a valuable and complementary prediction. Moreover, our method can give\na forecast for a new storm in a few seconds, which is an important asset for\nreal-time forecasts compared to traditional forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:56:23 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 09:03:59 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Giffard-Roisin", "Sophie", ""], ["Yang", "Mo", ""], ["Charpiat", "Guillaume", ""], ["Kumler-Bonfanti", "Christina", ""], ["K\u00e9gl", "Bal\u00e1zs", ""], ["Monteleoni", "Claire", ""]]}, {"id": "1910.10583", "submitter": "Soroosh Shafieezadeh-Abadeh", "authors": "Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Man-Chung Yue, Daniel\n  Kuhn, Wolfram Wiesemann", "title": "Optimistic Distributionally Robust Optimization for Nonparametric\n  Likelihood Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function is a fundamental component in Bayesian statistics.\nHowever, evaluating the likelihood of an observation is computationally\nintractable in many applications. In this paper, we propose a non-parametric\napproximation of the likelihood that identifies a probability measure which\nlies in the neighborhood of the nominal measure and that maximizes the\nprobability of observing the given sample point. We show that when the\nneighborhood is constructed by the Kullback-Leibler divergence, by moment\nconditions or by the Wasserstein distance, then our \\textit{optimistic\nlikelihood} can be determined through the solution of a convex optimization\nproblem, and it admits an analytical expression in particular cases. We also\nshow that the posterior inference problem with our optimistic likelihood\napproximation enjoys strong theoretical performance guarantees, and it performs\ncompetitively in a probabilistic classification task.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:34:40 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Shafieezadeh-Abadeh", "Soroosh", ""], ["Yue", "Man-Chung", ""], ["Kuhn", "Daniel", ""], ["Wiesemann", "Wolfram", ""]]}, {"id": "1910.10593", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Rex Ying, Matilde Padovano, Raia Hadsell,\n  Charles Blundell", "title": "Neural Execution of Graph Algorithms", "comments": "To appear at ICLR 2020. 13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are a powerful representational tool for solving\nproblems on graph-structured inputs. In almost all cases so far, however, they\nhave been applied to directly recovering a final solution from raw inputs,\nwithout explicit guidance on how to structure their problem-solving. Here,\ninstead, we focus on learning in the space of algorithms: we train several\nstate-of-the-art GNN architectures to imitate individual steps of classical\ngraph algorithms, parallel (breadth-first search, Bellman-Ford) as well as\nsequential (Prim's algorithm). As graph algorithms usually rely on making\ndiscrete decisions within neighbourhoods, we hypothesise that\nmaximisation-based message passing neural networks are best-suited for such\nobjectives, and validate this claim empirically. We also demonstrate how\nlearning in the space of algorithms can yield new opportunities for positive\ntransfer between tasks---showing how learning a shortest-path algorithm can be\nsubstantially improved when simultaneously learning a reachability algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:50:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:47:33 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Ying", "Rex", ""], ["Padovano", "Matilde", ""], ["Hadsell", "Raia", ""], ["Blundell", "Charles", ""]]}, {"id": "1910.10596", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Michalis K. Titsias, Andriy Mnih", "title": "Sparse Orthogonal Variational Inference for Gaussian Processes", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new interpretation of sparse variational approximations for\nGaussian processes using inducing points, which can lead to more scalable\nalgorithms than previous methods. It is based on decomposing a Gaussian process\nas a sum of two independent processes: one spanned by a finite basis of\ninducing points and the other capturing the remaining variation. We show that\nthis formulation recovers existing approximations and at the same time allows\nto obtain tighter lower bounds on the marginal likelihood and new stochastic\nvariational inference algorithms. We demonstrate the efficiency of these\nalgorithms in several Gaussian process models ranging from standard regression\nto multi-class classification using (deep) convolutional Gaussian processes and\nreport state-of-the-art results on CIFAR-10 among purely GP-based models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:01:28 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:08:41 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 15:47:20 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Shi", "Jiaxin", ""], ["Titsias", "Michalis K.", ""], ["Mnih", "Andriy", ""]]}, {"id": "1910.10597", "submitter": "Aditya Modi", "authors": "Aditya Modi, Nan Jiang, Ambuj Tewari, Satinder Singh", "title": "Sample Complexity of Reinforcement Learning using Linearly Combined\n  Model Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) methods have been shown to be capable of learning\nintelligent behavior in rich domains. However, this has largely been done in\nsimulated domains without adequate focus on the process of building the\nsimulator. In this paper, we consider a setting where we have access to an\nensemble of pre-trained and possibly inaccurate simulators (models). We\napproximate the real environment using a state-dependent linear combination of\nthe ensemble, where the coefficients are determined by the given state features\nand some unknown parameters. Our proposed algorithm provably learns a\nnear-optimal policy with a sample complexity polynomial in the number of\nunknown parameters, and incurs no dependence on the size of the state (or\naction) space. As an extension, we also consider the more challenging problem\nof model selection, where the state features are unknown and can be chosen from\na large candidate set. We provide exponential lower bounds that illustrate the\nfundamental hardness of this problem, and develop a provably efficient\nalgorithm under additional natural assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:02:30 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Modi", "Aditya", ""], ["Jiang", "Nan", ""], ["Tewari", "Ambuj", ""], ["Singh", "Satinder", ""]]}, {"id": "1910.10679", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "A Useful Taxonomy for Adversarial Robustness of Neural Networks", "comments": "NRL Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks and defenses are currently active areas of research for\nthe deep learning community. A recent review paper divided the defense\napproaches into three categories; gradient masking, robust optimization, and\nadversarial example detection. We divide gradient masking and robust\noptimization differently: (1) increasing intra-class compactness and\ninter-class separation of the feature vectors improves adversarial robustness,\nand (2) marginalization or removal of non-robust image features also improves\nadversarial robustness. By reframing these topics differently, we provide a\nfresh perspective that provides insight into the underlying factors that enable\ntraining more robust networks and can help inspire novel solutions. In\naddition, there are several papers in the literature of adversarial defenses\nthat claim there is a cost for adversarial robustness, or a trade-off between\nrobustness and accuracy but, under this proposed taxonomy, we hypothesis that\nthis is not universal. We follow up on our taxonomy with several challenges to\nthe deep learning research community that builds on the connections and\ninsights in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:33:15 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1910.10682", "submitter": "Deepak Bhaskar Acharya", "authors": "Deepak Bhaskar Acharya, Dr. Huaming Zhang", "title": "Feature Selection and Extraction for Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been a latest hot research topic in data\nscience, due to the fact that they use the ubiquitous data structure graphs as\nthe underlying elements for constructing and training neural networks. In a\nGNN, each node has numerous features associated with it. The entire task (for\nexample, classification, or clustering) utilizes the features of the nodes to\nmake decisions, at node level or graph level. In this paper, (1) we extend the\nfeature selection algorithm presented in via Gumbel Softmax to GNNs. We conduct\na series of experiments on our feature selection algorithms, using various\nbenchmark datasets: Cora, Citeseer and Pubmed. (2) We implement a mechanism to\nrank the extracted features. We demonstrate the effectiveness of our\nalgorithms, for both feature selection and ranking. For the Cora dataset, (1)\nwe use the algorithm to select 225 features out of 1433 features. Our\nexperimental results demonstrate their effectiveness for the same\nclassification problem. (2) We extract features such that they are linear\ncombinations of the original features, where the coefficients for each\nextracted features are non-negative and sum up to one. We propose an algorithm\nto rank the extracted features in the sense that when using them for the same\nclassification problem, the accuracy goes down gradually for the extracted\nfeatures within the rank 1 - 50, 51 - 100, 100 - 150, and 151 - 200.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:35:55 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 00:03:52 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Acharya", "Deepak Bhaskar", ""], ["Zhang", "Dr. Huaming", ""]]}, {"id": "1910.10683", "submitter": "Colin Raffel", "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n  Narang, Michael Matena, Yanqi Zhou, Wei Li and Peter J. Liu", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "comments": "Final version as published in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:37:36 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 15:13:50 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 13:10:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Raffel", "Colin", ""], ["Shazeer", "Noam", ""], ["Roberts", "Adam", ""], ["Lee", "Katherine", ""], ["Narang", "Sharan", ""], ["Matena", "Michael", ""], ["Zhou", "Yanqi", ""], ["Li", "Wei", ""], ["Liu", "Peter J.", ""]]}, {"id": "1910.10685", "submitter": "Jennifer Wei", "authors": "Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee, Richard C.\n  Gerkin, Al\\'an Aspuru-Guzik, and Alexander B. Wiltschko", "title": "Machine Learning for Scent: Learning Generalizable Perceptual\n  Representations of Small Molecules", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the relationship between a molecule's structure and its odor\nremains a difficult, decades-old task. This problem, termed quantitative\nstructure-odor relationship (QSOR) modeling, is an important challenge in\nchemistry, impacting human nutrition, manufacture of synthetic fragrance, the\nenvironment, and sensory neuroscience. We propose the use of graph neural\nnetworks for QSOR, and show they significantly out-perform prior methods on a\nnovel data set labeled by olfactory experts. Additional analysis shows that the\nlearned embeddings from graph neural networks capture a meaningful odor space\nrepresentation of the underlying relationship between structure and odor, as\ndemonstrated by strong performance on two challenging transfer learning tasks.\nMachine learning has already had a large impact on the senses of sight and\nsound. Based on these early results with graph neural networks for molecular\nproperties, we hope machine learning can eventually do for olfaction what it\nhas already done for vision and hearing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:38:47 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 17:57:31 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Sanchez-Lengeling", "Benjamin", ""], ["Wei", "Jennifer N.", ""], ["Lee", "Brian K.", ""], ["Gerkin", "Richard C.", ""], ["Aspuru-Guzik", "Al\u00e1n", ""], ["Wiltschko", "Alexander B.", ""]]}, {"id": "1910.10692", "submitter": "Yizhe Zhu", "authors": "Kameron Decker Harris, Yizhe Zhu", "title": "Deterministic tensor completion with hypergraph expanders", "comments": "35 pages, 4 figures. To appear in SIAM Journal on Mathematics of Data\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:51:13 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:28:33 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:48:23 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 16:50:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Harris", "Kameron Decker", ""], ["Zhu", "Yizhe", ""]]}, {"id": "1910.10699", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Dilip Krishnan, Phillip Isola", "title": "Contrastive Representation Distillation", "comments": "ICLR 2020. Project Page: http://hobbitlong.github.io/CRD/, Code:\n  http://github.com/HobbitLong/RepDistiller", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Often we wish to transfer representational knowledge from one neural network\nto another. Examples include distilling a large network into a smaller one,\ntransferring knowledge from one sensory modality to a second, or ensembling a\ncollection of models into a single estimator. Knowledge distillation, the\nstandard approach to these problems, minimizes the KL divergence between the\nprobabilistic outputs of a teacher and student network. We demonstrate that\nthis objective ignores important structural knowledge of the teacher network.\nThis motivates an alternative objective by which we train a student to capture\nsignificantly more information in the teacher's representation of the data. We\nformulate this objective as contrastive learning. Experiments demonstrate that\nour resulting new objective outperforms knowledge distillation and other\ncutting-edge distillers on a variety of knowledge transfer tasks, including\nsingle model compression, ensemble distillation, and cross-modal transfer. Our\nmethod sets a new state-of-the-art in many transfer tasks, and sometimes even\noutperforms the teacher network when combined with knowledge distillation.\nCode: http://github.com/HobbitLong/RepDistiller.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:59:18 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 10:09:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tian", "Yonglong", ""], ["Krishnan", "Dilip", ""], ["Isola", "Phillip", ""]]}, {"id": "1910.10754", "submitter": "Heejin Jeong", "authors": "Heejin Jeong, Brent Schlotfeldt, Hamed Hassani, Manfred Morari, Daniel\n  D. Lee, George J. Pappas", "title": "Learning Q-network for Active Information Acquisition", "comments": "IROS 2019, Video https://youtu.be/0ZFyOWJ2ulo", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Reinforcement Learning approach for solving\nthe Active Information Acquisition problem, which requires an agent to choose a\nsequence of actions in order to acquire information about a process of interest\nusing on-board sensors. The classic challenges in the information acquisition\nproblem are the dependence of a planning algorithm on known models and the\ndifficulty of computing information-theoretic cost functions over arbitrary\ndistributions. In contrast, the proposed framework of reinforcement learning\ndoes not require any knowledge on models and alleviates the problems during an\nextended training stage. It results in policies that are efficient to execute\nonline and applicable for real-time control of robotic systems. Furthermore,\nthe state-of-the-art planning methods are typically restricted to short\nhorizons, which may become problematic with local minima. Reinforcement\nlearning naturally handles the issue of planning horizon in information\nproblems as it maximizes a discounted sum of rewards over a long finite or\ninfinite time horizon. We discuss the potential benefits of the proposed\nframework and compare the performance of the novel algorithm to an existing\ninformation acquisition method for multi-target tracking scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:21:16 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Jeong", "Heejin", ""], ["Schlotfeldt", "Brent", ""], ["Hassani", "Hamed", ""], ["Morari", "Manfred", ""], ["Lee", "Daniel D.", ""], ["Pappas", "George J.", ""]]}, {"id": "1910.10769", "submitter": "Blake Zimmerman", "authors": "Blake E. Zimmerman (1,2), Sara Johnson (2), Henrik Od\\'een (3), Jill\n  Shea (4), Markus D. Foote (1,2), Nicole Winkler (4), Sarang C. Joshi (1,2),\n  Allison Payne (3) ((1) Scientific Computing and Imaging Institute, University\n  of Utah, (2) Department of Biomedical Engineering, University of Utah, (3)\n  Department of Radiology and Imaging Sciences, University of Utah, (4)\n  Department of Surgery, University of Utah)", "title": "Learning Multiparametric Biomarkers for Assessing MR-Guided Focused\n  Ultrasound Treatment of Malignant Tumors", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TBME.2020.3024826", "report-no": null, "categories": "eess.IV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noninvasive MR-guided focused ultrasound (MRgFUS) treatments are promising\nalternatives to the surgical removal of malignant tumors. A significant\nchallenge is assessing the viability of treated tissue during and immediately\nafter MRgFUS procedures. Current clinical assessment uses the nonperfused\nvolume (NPV) biomarker immediately after treatment from contrast-enhanced MRI.\nThe NPV has variable accuracy, and the use of contrast agent prevents\ncontinuing MRgFUS treatment if tumor coverage is inadequate. This work presents\na novel, noncontrast, learned multiparametric MR biomarker that can be used\nduring treatment for intratreatment assessment, validated in a VX2 rabbit tumor\nmodel. A deep convolutional neural network was trained on noncontrast\nmultiparametric MR images using the NPV biomarker from follow-up MR imaging\n(3-5 days after MRgFUS treatment) as the accurate label of nonviable tissue. A\nnovel volume-conserving registration algorithm yielded a voxel-wise correlation\nbetween treatment and follow-up NPV, providing a rigorous validation of the\nbiomarker. The learned noncontrast multiparametric MR biomarker predicted the\nfollow-up NPV with an average DICE coefficient of 0.71, substantially\noutperforming the current clinical standard (DICE coefficient = 0.53).\nNoncontrast multiparametric MR imaging integrated with a deep convolutional\nneural network provides a more accurate prediction of MRgFUS treatment outcome\nthan current contrast-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:02:43 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 19:24:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zimmerman", "Blake E.", ""], ["Johnson", "Sara", ""], ["Od\u00e9en", "Henrik", ""], ["Shea", "Jill", ""], ["Foote", "Markus D.", ""], ["Winkler", "Nicole", ""], ["Joshi", "Sarang C.", ""], ["Payne", "Allison", ""]]}, {"id": "1910.10775", "submitter": "Eli Bingham", "authors": "Fritz Obermeyer, Eli Bingham, Martin Jankowiak, Du Phan, Jonathan P.\n  Chen", "title": "Functional Tensors for Probabilistic Programming", "comments": "Submitted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a significant challenge to design probabilistic programming systems\nthat can accommodate a wide variety of inference strategies within a unified\nframework. Noting that the versatility of modern automatic differentiation\nframeworks is based in large part on the unifying concept of tensors, we\ndescribe a software abstraction for integration --functional tensors-- that\ncaptures many of the benefits of tensors, while also being able to describe\ncontinuous probability distributions. Moreover, functional tensors are a\nnatural candidate for generalized variable elimination and parallel-scan\nfiltering algorithms that enable parallel exact inference for a large family of\ntractable modeling motifs. We demonstrate the versatility of functional tensors\nby integrating them into the modeling frontend and inference backend of the\nPyro programming language. In experiments we show that the resulting framework\nenables a large variety of inference strategies, including those that mix exact\nand approximate inference.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:36:09 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 19:36:31 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Obermeyer", "Fritz", ""], ["Bingham", "Eli", ""], ["Jankowiak", "Martin", ""], ["Phan", "Du", ""], ["Chen", "Jonathan P.", ""]]}, {"id": "1910.10777", "submitter": "Hagit Grushka-Cohen", "authors": "Hagit Grushka-Cohen, Ofer Biller, Oded Sofer, Lior Rokach and Bracha\n  Shapira", "title": "Diversifying Database Activity Monitoring with Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database activity monitoring (DAM) systems are commonly used by organizations\nto protect the organizational data, knowledge and intellectual properties. In\norder to protect organizations database DAM systems have two main roles,\nmonitoring (documenting activity) and alerting to anomalous activity. Due to\nhigh-velocity streams and operating costs, such systems are restricted to\nexamining only a sample of the activity. Current solutions use policies,\nmanually crafted by experts, to decide which transactions to monitor and log.\nThis limits the diversity of the data collected. Bandit algorithms, which use\nreward functions as the basis for optimization while adding diversity to the\nrecommended set, have gained increased attention in recommendation systems for\nimproving diversity.\n  In this work, we redefine the data sampling problem as a special case of the\nmulti-armed bandit (MAB) problem and present a novel algorithm, which combines\nexpert knowledge with random exploration. We analyze the effect of diversity on\ncoverage and downstream event detection tasks using a simulated dataset. In\ndoing so, we find that adding diversity to the sampling using the bandit-based\napproach works well for this task and maximizing population coverage without\ndecreasing the quality in terms of issuing alerts about events.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:39:51 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Grushka-Cohen", "Hagit", ""], ["Biller", "Ofer", ""], ["Sofer", "Oded", ""], ["Rokach", "Lior", ""], ["Shapira", "Bracha", ""]]}, {"id": "1910.10781", "submitter": "Raghavendra Reddy Pappagari", "authors": "Raghavendra Pappagari, Piotr \\.Zelasko, Jes\\'us Villalba, Yishay\n  Carmiel and Najim Dehak", "title": "Hierarchical Transformers for Long Document Classification", "comments": "4 figures, 7 pages", "journal-ref": "Automatic Speech Recognition and Understanding Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  BERT, which stands for Bidirectional Encoder Representations from\nTransformers, is a recently introduced language representation model based upon\nthe transfer learning paradigm. We extend its fine-tuning procedure to address\none of its major limitations - applicability to inputs longer than a few\nhundred words, such as transcripts of human call conversations. Our method is\nconceptually simple. We segment the input into smaller chunks and feed each of\nthem into the base model. Then, we propagate each output through a single\nrecurrent layer, or another transformer, followed by a softmax activation. We\nobtain the final classification decision after the last segment has been\nconsumed. We show that both BERT extensions are quick to fine-tune and converge\nafter as little as 1 epoch of training on a small, domain-specific data set. We\nsuccessfully apply them in three different tasks involving customer call\nsatisfaction prediction and topic classification, and obtain a significant\nimprovement over the baseline models in two of them.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:51:50 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pappagari", "Raghavendra", ""], ["\u017belasko", "Piotr", ""], ["Villalba", "Jes\u00fas", ""], ["Carmiel", "Yishay", ""], ["Dehak", "Najim", ""]]}, {"id": "1910.10783", "submitter": "Alexander Levine", "authors": "Alexander Levine, Soheil Feizi", "title": "Wasserstein Smoothing: Certified Robustness against Wasserstein\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last couple of years, several adversarial attack methods based on\ndifferent threat models have been proposed for the image classification\nproblem. Most existing defenses consider additive threat models in which sample\nperturbations have bounded L_p norms. These defenses, however, can be\nvulnerable against adversarial attacks under non-additive threat models. An\nexample of an attack method based on a non-additive threat model is the\nWasserstein adversarial attack proposed by Wong et al. (2019), where the\ndistance between an image and its adversarial example is determined by the\nWasserstein metric (\"earth-mover distance\") between their normalized pixel\nintensities. Until now, there has been no certifiable defense against this type\nof attack. In this work, we propose the first defense with certified robustness\nagainst Wasserstein Adversarial attacks using randomized smoothing. We develop\nthis certificate by considering the space of possible flows between images, and\nrepresenting this space such that Wasserstein distance between images is\nupper-bounded by L_1 distance in this flow-space. We can then apply existing\nrandomized smoothing certificates for the L_1 metric. In MNIST and CIFAR-10\ndatasets, we find that our proposed defense is also practically effective,\ndemonstrating significantly improved accuracy under Wasserstein adversarial\nattack compared to unprotected models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:54:27 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Levine", "Alexander", ""], ["Feizi", "Soheil", ""]]}, {"id": "1910.10786", "submitter": "Reazul Hasan Russel", "authors": "Bahram Behzadian, Reazul Hasan Russel, Marek Petrik, Chin Pang Ho", "title": "Optimizing Percentile Criterion Using Robust MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing reliable policies in reinforcement\nlearning problems with limited data. In particular, we compute policies that\nachieve good returns with high confidence when deployed. This objective, known\nas the \\emph{percentile criterion}, can be optimized using Robust MDPs~(RMDPs).\nRMDPs generalize MDPs to allow for uncertain transition probabilities chosen\nadversarially from given ambiguity sets. We show that the RMDP solution's\nsub-optimality depends on the spans of the ambiguity sets along the value\nfunction. We then propose new algorithms that minimize the span of ambiguity\nsets defined by weighted $L_1$ and $L_\\infty$ norms. Our primary focus is on\nBayesian guarantees, but we also describe how our methods apply to frequentist\nguarantees and derive new concentration inequalities for weighted $L_1$ and\n$L_\\infty$ norms. Experimental results indicate that our optimized ambiguity\nsets improve significantly on prior construction methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:00:11 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:08:55 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 22:34:25 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Behzadian", "Bahram", ""], ["Russel", "Reazul Hasan", ""], ["Petrik", "Marek", ""], ["Ho", "Chin Pang", ""]]}, {"id": "1910.10791", "submitter": "Wei Deng", "authors": "Wei Deng, Xiao Zhang, Faming Liang, Guang Lin", "title": "An Adaptive Empirical Bayesian Method for Sparse Deep Learning", "comments": "Accepted by NeurIPS 2019; Update the assumption on the regularity of\n  Poisson equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive empirical Bayesian method for sparse deep\nlearning, where the sparsity is ensured via a class of self-adaptive\nspike-and-slab priors. The proposed method works by alternatively sampling from\nan adaptive hierarchical posterior distribution using stochastic gradient\nMarkov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters\nusing stochastic approximation (SA). We further prove the convergence of the\nproposed method to the asymptotically correct distribution under mild\nconditions. Empirical applications of the proposed method lead to the\nstate-of-the-art performance on MNIST and Fashion MNIST with shallow\nconvolutional neural networks and the state-of-the-art compression performance\non CIFAR10 with Residual Networks. The proposed method also improves resistance\nto adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 18:57:23 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Deng", "Wei", ""], ["Zhang", "Xiao", ""], ["Liang", "Faming", ""], ["Lin", "Guang", ""]]}, {"id": "1910.10797", "submitter": "Oscar Leong", "authors": "Oscar Leong and Wesam Sakla", "title": "Low Shot Learning with Untrained Neural Networks for Imaging Inverse\n  Problems", "comments": "Deep Inverse NeurIPS 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing deep neural networks as natural image priors to solve inverse\nproblems either requires large amounts of data to sufficiently train expressive\ngenerative models or can succeed with no data via untrained neural networks.\nHowever, very few works have considered how to interpolate between these no- to\nhigh-data regimes. In particular, how can one use the availability of a small\namount of data (even $5-25$ examples) to one's advantage in solving these\ninverse problems and can a system's performance increase as the amount of data\nincreases as well? In this work, we consider solving linear inverse problems\nwhen given a small number of examples of images that are drawn from the same\ndistribution as the image of interest. Comparing to untrained neural networks\nthat use no data, we show how one can pre-train a neural network with a few\ngiven examples to improve reconstruction results in compressed sensing and\nsemantic image recovery problems such as colorization. Our approach leads to\nimproved reconstruction as the amount of available data increases and is on par\nwith fully trained generative models, while requiring less than $1 \\%$ of the\ndata needed to train a generative model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:23:22 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Leong", "Oscar", ""], ["Sakla", "Wesam", ""]]}, {"id": "1910.10798", "submitter": "Zhen Liu", "authors": "Zhen Liu, Borui Xiao, Yuemeng Li, Yong Fan", "title": "Context-endcoding for neural network based skull stripping in magnetic\n  resonance imaging", "comments": "draft for skull strip", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skull stripping is usually the first step for most brain analysisprocess in\nmagnetic resonance images. A lot of deep learn-ing neural network based methods\nhave been developed toachieve higher accuracy. Since the 3D deep learning\nmodelssuffer from high computational cost and are subject to GPUmemory limit\nchallenge, a variety of 2D deep learning meth-ods have been developed. However,\nexisting 2D deep learn-ing methods are not equipped to effectively capture 3D\nse-mantic information that is needed to achieve higher accuracy.In this paper,\nwe propose a context-encoding method to em-power the 2D network to capture the\n3D context information.For the context-encoding method, firstly we encode the\n2Dfeatures of original 2D network, secondly we encode the sub-volume of 3D MRI\nimages, finally we fuse the encoded 2Dfeatures and 3D features with semantic\nencoding classifica-tion loss. To get computational efficiency, although we\nen-code the sub-volume of 3D MRI images instead of buildinga 3D neural network,\nextensive experiments on three bench-mark Datasets demonstrate our method can\nachieve superioraccuracy to state-of-the-art alternative methods with the\ndicescore 99.6% on NFBS and 99.09 % on LPBA40 and 99.17 %on OASIS.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:23:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Liu", "Zhen", ""], ["Xiao", "Borui", ""], ["Li", "Yuemeng", ""], ["Fan", "Yong", ""]]}, {"id": "1910.10806", "submitter": "Vishwa Karia", "authors": "Vishwa Karia, Wenhao Zhang, Arash Naeim, Ramin Ramezani", "title": "GenSample: A Genetic Algorithm for Oversampling in Imbalanced Datasets", "comments": "7 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalanced datasets are ubiquitous. Classification performance on imbalanced\ndatasets is generally poor for the minority class as the classifier cannot\nlearn decision boundaries well. However, in sensitive applications like fraud\ndetection, medical diagnosis, and spam identification, it is extremely\nimportant to classify the minority instances correctly. In this paper, we\npresent a novel technique based on genetic algorithms, GenSample, for\noversampling the minority class in imbalanced datasets. GenSample decides the\nrate of oversampling a minority example by taking into account the difficulty\nin learning that example, along with the performance improvement achieved by\noversampling it. This technique terminates the oversampling process when the\nperformance of the classifier begins to deteriorate. Consequently, it produces\nsynthetic data only as long as a performance boost is obtained. The algorithm\nwas tested on 9 real-world imbalanced datasets of varying sizes and imbalance\nratios. It achieved the highest F-Score on 8 out of 9 datasets, confirming its\nability to better handle imbalanced data compared to other existing\nmethodologies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:48:54 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Karia", "Vishwa", ""], ["Zhang", "Wenhao", ""], ["Naeim", "Arash", ""], ["Ramezani", "Ramin", ""]]}, {"id": "1910.10809", "submitter": "Laurent Barthes", "authors": "Mohamed Djallel Dilmi, Laurent Barth\\`es, C\\'ecile Mallet, Aymeric\n  Chazottes", "title": "Study of the impact of climate change on precipitation in Paris area\n  using method based on iterative multiscale dynamic time warping (IMS-DTW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the impact of climate change on precipitation is constrained by\nfinding a way to evaluate the evolution of precipitation variability over time.\nClassical approaches (feature-based) have shown their limitations for this\nissue due to the intermittent and irregular nature of precipitation. In this\nstudy, we present a novel variant of the Dynamic time warping method\nquantifying the dissimilarity between two rainfall time series based on shapes\ncomparisons, for clustering annual time series recorded at daily scale. This\nshape based approach considers the whole information (variability, trends and\nintermittency). We further labeled each cluster using a feature-based approach.\nWhile testing the proposed approach on the time series of Paris Montsouris, we\nfound that the precipitation variability increased over the years in Paris\narea.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:34:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Dilmi", "Mohamed Djallel", ""], ["Barth\u00e8s", "Laurent", ""], ["Mallet", "C\u00e9cile", ""], ["Chazottes", "Aymeric", ""]]}, {"id": "1910.10831", "submitter": "Alexander Alemi", "authors": "Alexander A. Alemi", "title": "Variational Predictive Information Bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic papers, Zellner demonstrated that Bayesian inference could be\nderived as the solution to an information theoretic functional. Below we derive\na generalized form of this functional as a variational lower bound of a\npredictive information bottleneck objective. This generalized functional\nencompasses most modern inference procedures and suggests novel ones.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 22:48:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Alemi", "Alexander A.", ""]]}, {"id": "1910.10835", "submitter": "Steven Chen", "authors": "Steven W. Chen, Tianyu Wang, Nikolay Atanasov, Vijay Kumar, and\n  Manfred Morari", "title": "Large Scale Model Predictive Control with Neural Networks and Primal\n  Active Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an explicit-implicit procedure to compute a model\npredictive control (MPC) law with guarantees on recursive feasibility and\nasymptotic stability. The approach combines an offline-trained fully-connected\nneural network with an online primal active set solver. The neural network\nprovides a control input initialization while the primal active set method\nensures recursive feasibility and asymptotic stability. The neural network is\ntrained with a primal-dual loss function, aiming to generate control sequences\nthat are primal feasible and meet a desired level of suboptimality. Since the\nneural network alone does not guarantee constraint satisfaction, its output is\nused to warm start the primal active set method online. We demonstrate that\nthis approach scales to large problems with thousands of optimization\nvariables, which are challenging for current approaches. Our method achieves a\n2x reduction in online inference time compared to the best method in a\nbenchmark suite of different solver and initialization strategies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 23:15:18 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 21:50:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Steven W.", ""], ["Wang", "Tianyu", ""], ["Atanasov", "Nikolay", ""], ["Kumar", "Vijay", ""], ["Morari", "Manfred", ""]]}, {"id": "1910.10840", "submitter": "Patrik Reizinger", "authors": "Patrik Reizinger and M\\'arton Szemenyei", "title": "Attention-based Curiosity-driven Exploration in Deep Reinforcement\n  Learning", "comments": "Submitted to ICASSP2020, 5 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning enables to train an agent via interaction with the\nenvironment. However, in the majority of real-world scenarios, the extrinsic\nfeedback is sparse or not sufficient, thus intrinsic reward formulations are\nneeded to successfully train the agent. This work investigates and extends the\nparadigm of curiosity-driven exploration. First, a probabilistic approach is\ntaken to exploit the advantages of the attention mechanism, which is\nsuccessfully applied in other domains of Deep Learning. Combining them, we\npropose new methods, such as AttA2C, an extension of the Actor-Critic\nframework. Second, another curiosity-based approach - ICM - is extended. The\nproposed model utilizes attention to emphasize features for the dynamic models\nwithin ICM, moreover, we also modify the loss function, resulting in a new\ncuriosity formulation, which we call rational curiosity. The corresponding\nimplementation can be found at https://github.com/rpatrik96/AttA2C/.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 23:31:21 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Reizinger", "Patrik", ""], ["Szemenyei", "M\u00e1rton", ""]]}, {"id": "1910.10860", "submitter": "Sangkyun Lee", "authors": "Sangkyun Lee, Piotr Sobczyk, Malgorzata Bogdan", "title": "Structure Learning of Gaussian Markov Random Fields with False Discovery\n  Rate Control", "comments": null, "journal-ref": "https://www.mdpi.com/2073-8994/11/10/1311", "doi": "10.3390/symxx010005", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new estimation procedure for discovering the\nstructure of Gaussian Markov random fields (MRFs) with false discovery rate\n(FDR) control, making use of the sorted l1-norm (SL1) regularization. A\nGaussian MRF is an acyclic graph representing a multivariate Gaussian\ndistribution, where nodes are random variables and edges represent the\nconditional dependence between the connected nodes. Since it is possible to\nlearn the edge structure of Gaussian MRFs directly from data, Gaussian MRFs\nprovide an excellent way to understand complex data by revealing the dependence\nstructure among many inputs features, such as genes, sensors, users, documents,\netc. In learning the graphical structure of Gaussian MRFs, it is desired to\ndiscover the actual edges of the underlying but unknown probabilistic graphical\nmodel-it becomes more complicated when the number of random variables\n(features) p increases, compared to the number of data points n. In particular,\nwhen p >> n, it is statistically unavoidable for any estimation procedure to\ninclude false edges. Therefore, there have been many trials to reduce the false\ndetection of edges, in particular, using different types of regularization on\nthe learning parameters. Our method makes use of the SL1 regularization,\nintroduced recently for model selection in linear regression. We focus on the\nbenefit of SL1 regularization that it can be used to control the FDR of\ndetecting important random variables. Adapting SL1 for probabilistic graphical\nmodels, we show that SL1 can be used for the structure learning of Gaussian\nMRFs using our suggested procedure nsSLOPE (neighborhood selection Sorted L-One\nPenalized Estimation), controlling the FDR of detecting edges.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:45:26 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Lee", "Sangkyun", ""], ["Sobczyk", "Piotr", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1910.10866", "submitter": "Asiri Wijesinghe", "authors": "Asiri Wijesinghe and Qing Wang", "title": "DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel spectral convolutional neural network (CNN) model on graph\nstructured data, namely Distributed Feedback-Looped Networks (DFNets). This\nmodel is incorporated with a robust class of spectral graph filters, called\nfeedback-looped filters, to provide better localization on vertices, while\nstill attaining fast convergence and linear memory requirements. Theoretically,\nfeedback-looped filters can guarantee convergence w.r.t. a specified error\nbound, and be applied universally to any graph without knowing its structure.\nFurthermore, the propagation rule of this model can diversify features from the\npreceding layers to produce strong gradient flows. We have evaluated our model\nusing two benchmark tasks: semi-supervised document classification on citation\nnetworks and semi-supervised entity classification on a knowledge graph. The\nexperimental results show that our model considerably outperforms the\nstate-of-the-art methods in both benchmark tasks over all datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 01:07:11 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 01:17:14 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 09:22:38 GMT"}, {"version": "v4", "created": "Tue, 24 Dec 2019 01:00:17 GMT"}, {"version": "v5", "created": "Fri, 17 Jan 2020 01:39:56 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Wijesinghe", "Asiri", ""], ["Wang", "Qing", ""]]}, {"id": "1910.10871", "submitter": "Andrew Tomkins", "authors": "Benjamin Spector and Ravi Kumar and Andrew Tomkins", "title": "Preventing Adversarial Use of Datasets through Fair Core-Set\n  Construction", "comments": "6 pages, 2 figures, NeurIPS 2019 Privacy In Machine Learning Workshop\n  (PriML 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose improving the privacy properties of a dataset by publishing only a\nstrategically chosen \"core-set\" of the data containing a subset of the\ninstances. The core-set allows strong performance on primary tasks, but forces\npoor performance on unwanted tasks. We give methods for both linear models and\nneural networks and demonstrate their efficacy on data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 01:28:58 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Spector", "Benjamin", ""], ["Kumar", "Ravi", ""], ["Tomkins", "Andrew", ""]]}, {"id": "1910.10873", "submitter": "Lin Chen", "authors": "Lin Chen, Qian Yu, Hannah Lawrence, Amin Karbasi", "title": "Minimax Regret of Switching-Constrained Online Convex Optimization: No\n  Phase Transition", "comments": "First two authors contributed equally. Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of switching-constrained online convex optimization\n(OCO), where the player has a limited number of opportunities to change her\naction. While the discrete analog of this online learning task has been studied\nextensively, previous work in the continuous setting has neither established\nthe minimax rate nor algorithmically achieved it. In this paper, we show that $\nT $-round switching-constrained OCO with fewer than $ K $ switches has a\nminimax regret of $ \\Theta(\\frac{T}{\\sqrt{K}}) $. In particular, it is at least\n$ \\frac{T}{\\sqrt{2K}} $ for one dimension and at least $ \\frac{T}{\\sqrt{K}} $\nfor higher dimensions. The lower bound in higher dimensions is attained by an\northogonal subspace argument. In one dimension, a novel adversarial strategy\nyields the lower bound of $O(\\frac{T}{\\sqrt{K}})$, but a precise minimax\nanalysis including constants is more involved. To establish the tighter\none-dimensional result, we introduce the \\emph{fugal game} relaxation, whose\nminimax regret lower bounds that of switching-constrained OCO. We show that the\nminimax regret of the fugal game is at least $ \\frac{T}{\\sqrt{2K}} $ and\nthereby establish the optimal minimax lower bound in one dimension. To\nestablish the dimension-independent upper bound, we next show that a\nmini-batching algorithm provides an $ O(\\frac{T}{\\sqrt{K}}) $ upper bound, and\ntherefore conclude that the minimax regret of switching-constrained OCO is $\n\\Theta(\\frac{T}{\\sqrt{K}}) $ for any $K$. This is in sharp contrast to its\ndiscrete counterpart, the switching-constrained prediction-from-experts\nproblem, which exhibits a phase transition in minimax regret between the\nlow-switching and high-switching regimes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 01:36:09 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 05:51:55 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 16:58:07 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Lin", ""], ["Yu", "Qian", ""], ["Lawrence", "Hannah", ""], ["Karbasi", "Amin", ""]]}, {"id": "1910.10890", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, Ilias Zadik", "title": "Inference in High-Dimensional Linear Regression via Lattice Basis\n  Reduction and Integer Relation Detection", "comments": "56 pages. Parts of the material of this manuscript were presented at\n  NeurIPS 2018, and ISIT 2019. This submission subsumes the content of\n  arXiv:1803.06716", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the high-dimensional linear regression problem, where the\nalgorithmic goal is to efficiently infer an unknown feature vector\n$\\beta^*\\in\\mathbb{R}^p$ from its linear measurements, using a small number $n$\nof samples. Unlike most of the literature, we make no sparsity assumption on\n$\\beta^*$, but instead adopt a different regularization: In the noiseless\nsetting, we assume $\\beta^*$ consists of entries, which are either rational\nnumbers with a common denominator $Q\\in\\mathbb{Z}^+$ (referred to as\n$Q$-rationality); or irrational numbers supported on a rationally independent\nset of bounded cardinality, known to learner; collectively called as the\nmixed-support assumption. Using a novel combination of the PSLQ integer\nrelation detection, and LLL lattice basis reduction algorithms, we propose a\npolynomial-time algorithm which provably recovers a $\\beta^*\\in\\mathbb{R}^p$\nenjoying the mixed-support assumption, from its linear measurements\n$Y=X\\beta^*\\in\\mathbb{R}^n$ for a large class of distributions for the random\nentries of $X$, even with one measurement $(n=1)$. In the noisy setting, we\npropose a polynomial-time, lattice-based algorithm, which recovers a\n$\\beta^*\\in\\mathbb{R}^p$ enjoying $Q$-rationality, from its noisy measurements\n$Y=X\\beta^*+W\\in\\mathbb{R}^n$, even with a single sample $(n=1)$. We further\nestablish for large $Q$, and normal noise, this algorithm tolerates\ninformation-theoretically optimal level of noise. We then apply these ideas to\ndevelop a polynomial-time, single-sample algorithm for the phase retrieval\nproblem. Our methods address the single-sample $(n=1)$ regime, where the\nsparsity-based methods such as LASSO and Basis Pursuit are known to fail.\nFurthermore, our results also reveal an algorithmic connection between the\nhigh-dimensional linear regression problem, and the integer relation detection,\nrandomized subset-sum, and shortest vector problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:41:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "1910.10897", "submitter": "Avnish Narayan", "authors": "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Avnish Narayan,\n  Hayden Shively, Adithya Bellathur, Karol Hausman, Chelsea Finn, Sergey Levine", "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta\n  Reinforcement Learning", "comments": "This is an update version of a manuscript that originally appeared at\n  CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are\n  available at: https://github.com/rlworkgroup/metaworld, and the baselines can\n  be found at https://github.com/rlworkgroup/garage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-reinforcement learning algorithms can enable robots to acquire new\nskills much more quickly, by leveraging prior experience to learn how to learn.\nHowever, much of the current research on meta-reinforcement learning focuses on\ntask distributions that are very narrow. For example, a commonly used\nmeta-reinforcement learning benchmark uses different running velocities for a\nsimulated robot as different tasks. When policies are meta-trained on such\nnarrow task distributions, they cannot possibly generalize to more quickly\nacquire entirely new tasks. Therefore, if the aim of these methods is to enable\nfaster acquisition of entirely new behaviors, we must evaluate them on task\ndistributions that are sufficiently broad to enable generalization to new\nbehaviors. In this paper, we propose an open-source simulated benchmark for\nmeta-reinforcement learning and multi-task learning consisting of 50 distinct\nrobotic manipulation tasks. Our aim is to make it possible to develop\nalgorithms that generalize to accelerate the acquisition of entirely new,\nheld-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and\nmulti-task learning algorithms on these tasks. Surprisingly, while each task\nand its variations (e.g., with different object positions) can be learned with\nreasonable success, these algorithms struggle to learn with multiple tasks at\nthe same time, even with as few as ten distinct training tasks. Our analysis\nand open-source environments pave the way for future research in multi-task\nlearning and meta-learning that can enable meaningful generalization, thereby\nunlocking the full potential of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 03:19:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:45:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yu", "Tianhe", ""], ["Quillen", "Deirdre", ""], ["He", "Zhanpeng", ""], ["Julian", "Ryan", ""], ["Narayan", "Avnish", ""], ["Shively", "Hayden", ""], ["Bellathur", "Adithya", ""], ["Hausman", "Karol", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.10923", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot", "title": "ERM and RERM are optimal estimators for regression problems when\n  malicious outliers corrupt the labels", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Empirical Risk Minimizers (ERM) and Regularized Empirical Risk\nMinimizers (RERM) for regression problems with convex and $L$-Lipschitz loss\nfunctions. We consider a setting where $|\\cO|$ malicious outliers contaminate\nthe labels. In that case, under a local Bernstein condition, we show that the\n$L_2$-error rate is bounded by $ r_N + AL |\\cO|/N$, where $N$ is the total\nnumber of observations, $r_N$ is the $L_2$-error rate in the non-contaminated\nsetting and $A$ is a parameter coming from the local Bernstein condition. When\n$r_N$ is minimax-rate-optimal in a non-contaminated setting, the rate $r_N +\nAL|\\cO|/N$ is also minimax-rate-optimal when $|\\cO|$ outliers contaminate the\nlabel. The main results of the paper can be used for many non-regularized and\nregularized procedures under weak assumptions on the noise. We present results\nfor Huber's M-estimators (without penalization or regularized by the\n$\\ell_1$-norm) and for general regularized learning problems in reproducible\nkernel Hilbert spaces when the noise can be heavy-tailed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 05:48:45 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:46:49 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chinot", "Geoffrey", ""]]}, {"id": "1910.10937", "submitter": "Vinod Raman", "authors": "Vinod Raman and Daniel T. Zhang and Young Hun Jung and Ambuj Tewari", "title": "Online Boosting for Multilabel Ranking with Top-k Feedback", "comments": "Under review for AISTATS 2021. Fixed small errors throughout the\n  manuscript and added new content comparing/contrasting various randomization\n  procedures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present online boosting algorithms for multilabel ranking with top-k\nfeedback, where the learner only receives information about the top k items\nfrom the ranking it provides. We propose a novel surrogate loss function and\nunbiased estimator, allowing weak learners to update themselves with limited\ninformation. Using these techniques we adapt full information multilabel\nranking algorithms (Jung and Tewari, 2018) to the top-k feedback setting and\nprovide theoretical performance bounds which closely match the bounds of their\nfull information counterparts, with the cost of increased sample complexity.\nThese theoretical results are further substantiated by our experiments, which\nshow a small gap in performance between the algorithms for the top-k feedback\nsetting and that for the full information setting across various datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:44:01 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 07:02:52 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 04:26:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Raman", "Vinod", ""], ["Zhang", "Daniel T.", ""], ["Jung", "Young Hun", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1910.10944", "submitter": "Adish Singla", "authors": "Farnam Mansouri, Yuxin Chen, Ara Vartanian, Xiaojin Zhu, Adish Singla", "title": "Preference-Based Batch and Sequential Teaching: Towards a Unified View\n  of Models", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic machine teaching studies the interaction between a teacher and a\nlearner where the teacher selects labeled examples aiming at teaching a target\nhypothesis. In a quest to lower teaching complexity and to achieve more natural\nteacher-learner interactions, several teaching models and complexity measures\nhave been proposed for both the batch settings (e.g., worst-case, recursive,\npreference-based, and non-clashing models) as well as the sequential settings\n(e.g., local preference-based model). To better understand the connections\nbetween these different batch and sequential models, we develop a novel\nframework which captures the teaching process via preference functions\n$\\Sigma$. In our framework, each function $\\sigma \\in \\Sigma$ induces a\nteacher-learner pair with teaching complexity as $\\TD(\\sigma)$. We show that\nthe above-mentioned teaching models are equivalent to specific types/families\nof preference functions in our framework. This equivalence, in turn, allows us\nto study the differences between two important teaching models, namely $\\sigma$\nfunctions inducing the strongest batch (i.e., non-clashing) model and $\\sigma$\nfunctions inducing a weak sequential (i.e., local preference-based) model.\nFinally, we identify preference functions inducing a novel family of sequential\nmodels with teaching complexity linear in the VC dimension of the hypothesis\nclass: this is in contrast to the best known complexity result for the batch\nmodels which is quadratic in the VC dimension.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:03:55 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Mansouri", "Farnam", ""], ["Chen", "Yuxin", ""], ["Vartanian", "Ara", ""], ["Zhu", "Xiaojin", ""], ["Singla", "Adish", ""]]}, {"id": "1910.10945", "submitter": "Xuedong Shang", "authors": "Xuedong Shang, Rianne de Heide, Emilie Kaufmann, Pierre M\\'enard,\n  Michal Valko", "title": "Fixed-Confidence Guarantees for Bayesian Best-Arm Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and provide new insights on the sampling rule called Top-Two\nThompson Sampling (TTTS). In particular, we justify its use for\nfixed-confidence best-arm identification. We further propose a variant of TTTS\ncalled Top-Two Transportation Cost (T3C), which disposes of the computational\nburden of TTTS. As our main contribution, we provide the first sample\ncomplexity analysis of TTTS and T3C when coupled with a very natural Bayesian\nstopping rule, for bandits with Gaussian rewards, solving one of the open\nquestions raised by Russo (2016). We also provide new posterior convergence\nresults for TTTS under two models that are commonly used in practice: bandits\nwith Gaussian and Bernoulli rewards and conjugate priors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:07:35 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 07:36:18 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 12:37:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shang", "Xuedong", ""], ["de Heide", "Rianne", ""], ["Kaufmann", "Emilie", ""], ["M\u00e9nard", "Pierre", ""], ["Valko", "Michal", ""]]}, {"id": "1910.10953", "submitter": "JianYu Wang", "authors": "Jianyu Wang and Xiao-Lei Zhang", "title": "Deep topic modeling by multilayer bootstrap network and lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling is widely studied for the dimension reduction and analysis of\ndocuments. However, it is formulated as a difficult optimization problem.\nCurrent approximate solutions also suffer from inaccurate model- or\ndata-assumptions. To deal with the above problems, we propose a polynomial-time\ndeep topic model with no model and data assumptions. Specifically, we first\napply multilayer bootstrap network (MBN), which is an unsupervised deep model,\nto reduce the dimension of documents, and then use the low-dimensional data\nrepresentations or their clustering results as the target of supervised Lasso\nfor topic word discovery. To our knowledge, this is the first time that MBN and\nLasso are applied to unsupervised topic modeling. Experimental comparison\nresults with five representative topic models on the 20-newsgroups and TDT2\ncorpora illustrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:35:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wang", "Jianyu", ""], ["Zhang", "Xiao-Lei", ""]]}, {"id": "1910.10986", "submitter": "Xin Yao", "authors": "Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, Lifeng Sun", "title": "Adversarial Feature Alignment: Avoid Catastrophic Forgetting in\n  Incremental Task Lifelong Learning", "comments": null, "journal-ref": "Neural Computation, Volume 31, Issue 11, November 2019,\n  p.2266-2291", "doi": "10.1162/neco_a_01232", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are able to master a variety of knowledge and skills with\nongoing learning. By contrast, dramatic performance degradation is observed\nwhen new tasks are added to an existing neural network model. This phenomenon,\ntermed as \\emph{Catastrophic Forgetting}, is one of the major roadblocks that\nprevent deep neural networks from achieving human-level artificial\nintelligence. Several research efforts, e.g. \\emph{Lifelong} or\n\\emph{Continual} learning algorithms, have been proposed to tackle this\nproblem. However, they either suffer from an accumulating drop in performance\nas the task sequence grows longer, or require to store an excessive amount of\nmodel parameters for historical memory, or cannot obtain competitive\nperformance on the new tasks. In this paper, we focus on the incremental\nmulti-task image classification scenario. Inspired by the learning process of\nhuman students, where they usually decompose complex tasks into easier goals,\nwe propose an adversarial feature alignment method to avoid catastrophic\nforgetting. In our design, both the low-level visual features and high-level\nsemantic features serve as soft targets and guide the training process in\nmultiple stages, which provide sufficient supervised information of the old\ntasks and help to reduce forgetting. Due to the knowledge distillation and\nregularization phenomenons, the proposed method gains even better performance\nthan finetuning on the new tasks, which makes it stand out from other methods.\nExtensive experiments in several typical lifelong learning scenarios\ndemonstrate that our method outperforms the state-of-the-art methods in both\naccuracies on new tasks and performance preservation on old tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:23:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yao", "Xin", ""], ["Huang", "Tianchi", ""], ["Wu", "Chenglei", ""], ["Zhang", "Rui-Xiao", ""], ["Sun", "Lifeng", ""]]}, {"id": "1910.11016", "submitter": "Tarun Yenamandra", "authors": "Tarun Yenamandra, Florian Bernard, Jiayi Wang, Franziska Mueller,\n  Christian Theobalt", "title": "Convex Optimisation for Inverse Kinematics", "comments": null, "journal-ref": null, "doi": "10.1109/3DV.2019.00043", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inverse kinematics (IK), where one wants to find\nthe parameters of a given kinematic skeleton that best explain a set of\nobserved 3D joint locations. The kinematic skeleton has a tree structure, where\neach node is a joint that has an associated geometric transformation that is\npropagated to all its child nodes. The IK problem has various applications in\nvision and graphics, for example for tracking or reconstructing articulated\nobjects, such as human hands or bodies. Most commonly, the IK problem is\ntackled using local optimisation methods. A major downside of these approaches\nis that, due to the non-convex nature of the problem, such methods are prone to\nconverge to unwanted local optima and therefore require a good initialisation.\nIn this paper we propose a convex optimisation approach for the IK problem\nbased on semidefinite programming, which admits a polynomial-time algorithm\nthat globally solves (a relaxation of) the IK problem. Experimentally, we\ndemonstrate that the proposed method significantly outperforms local\noptimisation methods using different real-world skeletons.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:25:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yenamandra", "Tarun", ""], ["Bernard", "Florian", ""], ["Wang", "Jiayi", ""], ["Mueller", "Franziska", ""], ["Theobalt", "Christian", ""]]}, {"id": "1910.11044", "submitter": "Josue Orellana", "authors": "Natalie Klein, Josue Orellana, Scott Brincat, Earl K. Miller, and\n  Robert E. Kass", "title": "Torus Graphs for Multivariate Phase Coupling Analysis", "comments": "N.K. and J.O. contributed equally to this work. Peer reviewed\n  version, in press at The Annals of Applied Statistics. 10 main Figures,\n  supplementary text appended with 11 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angular measurements are often modeled as circular random variables, where\nthere are natural circular analogues of moments, including correlation. Because\na product of circles is a torus, a d-dimensional vector of circular random\nvariables lies on a d-dimensional torus. For such vectors we present here a\nclass of graphical models, which we call torus graphs, based on the full\nexponential family with pairwise interactions. The topological distinction\nbetween a torus and Euclidean space has several important consequences.\n  Our development was motivated by the problem of identifying phase coupling\namong oscillatory signals recorded from multiple electrodes in the brain:\noscillatory phases across electrodes might tend to advance or recede together,\nindicating coordination across brain areas. The data analyzed here consisted of\n24 phase angles measured repeatedly across 840 experimental trials\n(replications) during a memory task, where the electrodes were in 4 distinct\nbrain regions, all known to be active while memories are being stored or\nretrieved. In realistic numerical simulations, we found that a standard\npairwise assessment, known as phase locking value, is unable to describe\nmultivariate phase interactions, but that torus graphs can accurately identify\nconditional associations. Torus graphs generalize several more restrictive\napproaches that have appeared in various scientific literatures, and produced\nintuitive results in the data we analyzed. Torus graphs thus unify multivariate\nanalysis of circular data and present fertile territory for future research.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 12:10:15 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Klein", "Natalie", ""], ["Orellana", "Josue", ""], ["Brincat", "Scott", ""], ["Miller", "Earl K.", ""], ["Kass", "Robert E.", ""]]}, {"id": "1910.11067", "submitter": "Cat Le", "authors": "Cat P. Le, Yi Zhou, Jie Ding, Vahid Tarokh", "title": "Supervised Encoding for Discrete Representation Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054118", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical supervised classification tasks search for a nonlinear mapping that\nmaps each encoded feature directly to a probability mass over the labels. Such\na learning framework typically lacks the intuition that encoded features from\nthe same class tend to be similar and thus has little interpretability for the\nlearned features. In this paper, we propose a novel supervised learning model\nnamed Supervised-Encoding Quantizer (SEQ). The SEQ applies a quantizer to\ncluster and classify the encoded features. We found that the quantizer provides\nan interpretable graph where each cluster in the graph represents a class of\ndata samples that have a particular style. We also trained a decoder that can\ndecode convex combinations of the encoded features from similar and different\nclusters and provide guidance on style transfer between sub-classes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:42:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Le", "Cat P.", ""], ["Zhou", "Yi", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.11080", "submitter": "Alexander Usvyatsov", "authors": "Alexander Usvyatsov", "title": "On sample complexity of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider functions defined by deep neural networks as definable objects in\nan o-miminal expansion of the real field, and derive an almost linear (in the\nnumber of weights) bound on sample complexity of such networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:38:08 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Usvyatsov", "Alexander", ""]]}, {"id": "1910.11086", "submitter": "Ethan Harris", "authors": "Ethan Harris, Daniela Mihai, Jonathon Hare", "title": "Spatial and Colour Opponency in Anatomically Constrained Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colour vision has long fascinated scientists, who have sought to understand\nboth the physiology of the mechanics of colour vision and the psychophysics of\ncolour perception. We consider representations of colour in anatomically\nconstrained convolutional deep neural networks. Following ideas from\nneuroscience, we classify cells in early layers into groups relating to their\nspectral and spatial functionality. We show the emergence of single and double\nopponent cells in our networks and characterise how the distribution of these\ncells changes under the constraint of a retinal bottleneck. Our experiments not\nonly open up a new understanding of how deep networks process spatial and\ncolour information, but also provide new tools to help understand the black box\nof deep learning. The code for all experiments is avaialable at\n\\url{https://github.com/ecs-vlc/opponency}.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:28:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Harris", "Ethan", ""], ["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "1910.11089", "submitter": "Jiahuan Luo", "authors": "Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu\n  and Qiang Yang", "title": "Real-World Image Datasets for Federated Learning", "comments": "This paper is published at the 2nd International Workshop on\n  Federated Learning for Data Privacy and Confidentiality, in Conjunction with\n  NeurIPS 2019 (FL-NeurIPS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a new machine learning paradigm which allows data\nparties to build machine learning models collaboratively while keeping their\ndata secure and private. While research efforts on federated learning have been\ngrowing tremendously in the past two years, most existing works still depend on\npre-existing public datasets and artificial partitions to simulate data\nfederations due to the lack of high-quality labeled data generated from\nreal-world edge applications. Consequently, advances on benchmark and model\nevaluations for federated learning have been lagging behind. In this paper, we\nintroduce a real-world image dataset. The dataset contains more than 900 images\ngenerated from 26 street cameras and 7 object categories annotated with\ndetailed bounding box. The data distribution is non-IID and unbalanced,\nreflecting the characteristic real-world federated learning scenarios. Based on\nthis dataset, we implemented two mainstream object detection algorithms (YOLO\nand Faster R-CNN) and provided an extensive benchmark on model performance,\nefficiency, and communication in a federated learning setting. Both the dataset\nand algorithms are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:33:26 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 09:22:56 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 06:31:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Luo", "Jiahuan", ""], ["Wu", "Xueyang", ""], ["Luo", "Yun", ""], ["Huang", "Anbu", ""], ["Huang", "Yunfeng", ""], ["Liu", "Yang", ""], ["Yang", "Qiang", ""]]}, {"id": "1910.11090", "submitter": "Dimitrios Kollias", "authors": "Aritra Banerjee and Dimitrios Kollias", "title": "Emotion Generation and Recognition: A StarGAN Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of this ISO is to use StarGAN (A type of GAN model) to perform\ntraining and testing on an emotion dataset resulting in a emotion recognition\nwhich can be generated by the valence arousal score of the 7 basic expressions.\nWe have created an entirely new dataset consisting of 4K videos. This dataset\nconsists of all the basic 7 types of emotions: Happy, Sad, Angry, Surprised,\nFear, Disgust, Neutral. We have performed face detection and alignment followed\nby annotating basic valence arousal values to the frames/images in the dataset\ndepending on the emotions manually. Then the existing StarGAN model is trained\non our created dataset after which some manual subjects were chosen to test the\nefficiency of the trained StarGAN model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:24:46 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Banerjee", "Aritra", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.11093", "submitter": "Ivan Sosnovik", "authors": "Ivan Sosnovik, Micha{\\l} Szmaja, Arnold Smeulders", "title": "Scale-Equivariant Steerable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Convolutional Neural Networks (CNNs) has been\nsubstantially attributed to their built-in property of translation\nequivariance. However, CNNs do not have embedded mechanisms to handle other\ntypes of transformations. In this work, we pay attention to scale changes,\nwhich regularly appear in various tasks due to the changing distances between\nthe objects and the camera. First, we introduce the general theory for building\nscale-equivariant convolutional networks with steerable filters. We develop\nscale-convolution and generalize other common blocks to be scale-equivariant.\nWe demonstrate the computational efficiency and numerical stability of the\nproposed method. We compare the proposed models to the previously developed\nmethods for scale equivariance and local scale invariance. We demonstrate\nstate-of-the-art results on MNIST-scale dataset and on STL-10 dataset in the\nsupervised learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:46:34 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:09:17 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Sosnovik", "Ivan", ""], ["Szmaja", "Micha\u0142", ""], ["Smeulders", "Arnold", ""]]}, {"id": "1910.11095", "submitter": "Mohammed Bouchouia", "authors": "Mohammed Bouchouia, Fran\\c{c}ois Portier", "title": "High dimensional regression for regenerative time-series: an application\n  to road traffic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical predictive model in which a high-dimensional time-series\nregenerates at the end of each day is used to model road traffic. Due to the\nregeneration, prediction is based on a daily modeling using a vector\nautoregressive model that combines linearly the past observations of the day.\nDue to the high-dimension, the learning algorithm follows from an\nL1-penalization of the regression coefficients. Excess risk bounds are\nestablished under the high-dimensional framework in which the number of road\nsections goes to infinity with the number of observed days. Considering\nfloating car data observed in an urban area, the approach is compared to\nstate-of-the-art methods including neural networks. In addition of being highly\ncompetitive in terms of prediction, it enables the identification of the most\ndeterminant sections of the road network.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:46:17 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:14:59 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 00:13:10 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 09:47:08 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 07:57:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bouchouia", "Mohammed", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1910.11104", "submitter": "Lucas C. Uzal", "authors": "Facundo Tuesca, Lucas C. Uzal", "title": "Exploiting video sequences for unsupervised disentangling in generative\n  adversarial networks", "comments": "This preprint is the result of the work done for the undergraduate\n  dissertation of F. Tuesca supervised by L.C. Uzal and presented in June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present an adversarial training algorithm that exploits\ncorrelations in video to learn --without supervision-- an image generator model\nwith a disentangled latent space. The proposed methodology requires only a few\nmodifications to the standard algorithm of Generative Adversarial Networks\n(GAN) and involves training with sets of frames taken from short videos. We\ntrain our model over two datasets of face-centered videos which present\ndifferent people speaking or moving the head: VidTIMIT and YouTube Faces\ndatasets. We found that our proposal allows us to split the generator latent\nspace into two subspaces. One of them controls content attributes, those that\ndo not change along short video sequences. For the considered datasets, this is\nthe identity of the generated face. The other subspace controls motion\nattributes, those attributes that are observed to change along short videos. We\nobserved that these motion attributes are face expressions, head orientation,\nlips and eyes movement. The presented experiments provide quantitative and\nqualitative evidence supporting that the proposed methodology induces a\ndisentangling of this two kinds of attributes in the latent space.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:37:43 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Tuesca", "Facundo", ""], ["Uzal", "Lucas C.", ""]]}, {"id": "1910.11105", "submitter": "Lior Wolf", "authors": "Barak Battash, Lior Wolf", "title": "Adaptive and Iteratively Improving Recurrent Lateral Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current leading computer vision models are typically feed forward neural\nmodels, in which the output of one computational block is passed to the next\none sequentially. This is in sharp contrast to the organization of the primate\nvisual cortex, in which feedback and lateral connections are abundant. In this\nwork, we propose a computational model for the role of lateral connections in a\ngiven block, in which the weights of the block vary dynamically as a function\nof its activations, and the input from the upstream blocks is iteratively\nreintroduced. We demonstrate how this novel architectural modification can lead\nto sizable gains in performance, when applied to visual action recognition\nwithout pretraining and that it outperforms the literature architectures with\nrecurrent feedback processing on ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:58:26 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Battash", "Barak", ""], ["Wolf", "Lior", ""]]}, {"id": "1910.11106", "submitter": "David Donahue", "authors": "David Donahue", "title": "Label-Conditioned Next-Frame Video Generation with Neural Flows", "comments": "Computer Vision class project, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art video generation systems employ Generative\nAdversarial Networks (GANs) or Variational Autoencoders (VAEs) to produce novel\nvideos. However, VAE models typically produce blurry outputs when faced with\nsub-optimal conditioning of the input, and GANs are known to be unstable for\nlarge output sizes. In addition, the output videos of these models are\ndifficult to evaluate, partly because the GAN loss function is not an accurate\nmeasure of convergence. In this work, we propose using a state-of-the-art\nneural flow generator called Glow to generate videos conditioned on a textual\nlabel, one frame at a time. Neural flow models are more stable than standard\nGANs, as they only optimize a single cross entropy loss function, which is\nmonotonic and avoids the circular convergence issues of the GAN minimax\nobjective. In addition, we also show how to condition Glow on external context,\nwhile still preserving the invertible nature of each \"flow\" layer. Finally, we\nevaluate the proposed Glow model by calculating cross entropy on a held-out\nvalidation set of videos, in order to compare multiple versions of the proposed\nmodel via an ablation study. We show generated videos and discuss future\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:08:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Donahue", "David", ""]]}, {"id": "1910.11111", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Viktoriia Sharmanska and Stefanos Zafeiriou", "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a\n  Single Network", "comments": "filed as a patent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial behavior analysis has a long history of studies in the\nintersection of computer vision, physiology and psychology. However it is only\nrecently, with the collection of large-scale datasets and powerful machine\nlearning methods such as deep neural networks, that automatic facial behavior\nanalysis started to thrive. Three of its iconic tasks are automatic recognition\nof basic expressions (e.g. happy, sad, surprised), estimation of continuous\nemotions (e.g., valence and arousal), and detection of facial action units\n(activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these\ntasks have been mostly studied independently collecting a dataset for the task.\nWe present the first and the largest study of all facial behaviour tasks\nlearned jointly in a single multi-task, multi-domain and multi-label network,\nwhich we call FaceBehaviorNet. For this we utilize all publicly available\ndatasets in the community (around 5M images) that study facial behaviour tasks\nin-the-wild. We demonstrate that training jointly an end-to-end network for all\ntasks has consistently better performance than training each of the single-task\nnetworks. Furthermore, we propose two simple strategies for coupling the tasks\nduring training, co-annotation and distribution matching, and show the\nadvantages of this approach. Finally we show that FaceBehaviorNet has learned\nfeatures that encapsulate all aspects of facial behaviour, and can be\nsuccessfully applied to perform tasks (compound emotion recognition) beyond the\nones that it has been trained in a zero- and few-shot learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:45:41 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 23:35:29 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 02:35:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.11117", "submitter": "Shubham Dokania", "authors": "Shubham Dokania, Vasudev Singh", "title": "Graph Representation learning for Audio & Music genre Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music genre is arguably one of the most important and discriminative\ninformation for music and audio content. Visual representation based approaches\nhave been explored on spectrograms for music genre classification. However,\nlack of quality data and augmentation techniques makes it difficult to employ\ndeep learning techniques successfully. We discuss the application of graph\nneural networks on such task due to their strong inductive bias, and show that\ncombination of CNN and GNN is able to achieve state-of-the-art results on\nGTZAN, and AudioSet (Imbalanced Music) datasets. We also discuss the role of\nSiamese Neural Networks as an analogous to GNN for learning edge similarity\nweights. Furthermore, we also perform visual analysis to understand the\nfield-of-view of our model into the spectrogram based on genre labels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:59:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Dokania", "Shubham", ""], ["Singh", "Vasudev", ""]]}, {"id": "1910.11118", "submitter": "Kyle Robinson", "authors": "Kyle Robinson, Dan Brown", "title": "Shallow Art: Art Extension Through Simple Machine Learning", "comments": "5 pages, 9 figures, presented at the 10th International Conference on\n  Computational Creativity (ICCC 2019)", "journal-ref": "Proceedings of the 10th International Conference on Computational\n  Creativity (2019) 316-320 [ISBN: 978-989-54160-1-1]", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shallow Art presents, implements, and tests the use of simple single-output\nclassification and regression models for the purpose of art generation. Various\nmachine learning algorithms are trained on collections of computer generated\nimages, artworks from Vincent van Gogh, and artworks from Rembrandt van Rijn.\nThese models are then provided half of an image and asked to complete the\nmissing side. The resulting images are displayed, and we explore implications\nfor computational creativity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:47:06 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Robinson", "Kyle", ""], ["Brown", "Dan", ""]]}, {"id": "1910.11123", "submitter": "Hamed Majidifard", "authors": "Hamed Majidifard, Peng Jin, Yaw Adu-Gyamfi, William G. Buttlar", "title": "Pavement Image Datasets: A New Benchmark Dataset to Classify and Densify\n  Pavement Distresses", "comments": null, "journal-ref": null, "doi": "10.1177/0361198120907283", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement distresses detection using road images remains a\nchallenging topic in the computer vision research community. Recent\ndevelopments in deep learning has led to considerable research activity\ndirected towards improving the efficacy of automated pavement distress\nidentification and rating. Deep learning models require a large ground truth\ndata set, which is often not readily available in the case of pavements. In\nthis study, a labeled dataset approach is introduced as a first step towards a\nmore robust, easy-to-deploy pavement condition assessment system. The technique\nis termed herein as the Pavement Image Dataset (PID) method. The dataset\nconsists of images captured from two camera views of an identical pavement\nsegment, i.e., a wide-view and a top-down view. The wide-view images were used\nto classify the distresses and to train the deep learning frameworks, while the\ntop-down view images allowed calculation of distress density, which will be\nused in future studies aimed at automated pavement rating. For the wide view\ngroup dataset, 7,237 images were manually annotated and distresses classified\ninto nine categories. Images were extracted using the Google Application\nProgramming Interface (API), selecting street-view images using a python-based\ncode developed for this project. The new dataset was evaluated using two\nmainstream deep learning frameworks: You Only Look Once (YOLO v2) and Faster\nRegion Convolution Neural Network (Faster R-CNN). Accuracy scores using the F1\nindex were found to be 0.84 for YOLOv2 and 0.65 for the Faster R-CNN model\nruns; both quite acceptable considering the convenience of utilizing Google\nmaps images.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:55:37 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:10:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Majidifard", "Hamed", ""], ["Jin", "Peng", ""], ["Adu-Gyamfi", "Yaw", ""], ["Buttlar", "William G.", ""]]}, {"id": "1910.11133", "submitter": "Prem Seetharaman", "authors": "Prem Seetharaman, Gordon Wichern, Jonathan Le Roux, Bryan Pardo", "title": "Bootstrapping deep music separation from primitive auditory grouping\n  principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating an audio scene such as a cocktail party into constituent,\nmeaningful components is a core task in computer audition. Deep networks are\nthe state-of-the-art approach. They are trained on synthetic mixtures of audio\nmade from isolated sound source recordings so that ground truth for the\nseparation is known. However, the vast majority of available audio is not\nisolated. The brain uses primitive cues that are independent of the\ncharacteristics of any particular sound source to perform an initial\nsegmentation of the audio scene. We present a method for bootstrapping a deep\nmodel for music source separation without ground truth by using multiple\nprimitive cues. We apply our method to train a network on a large set of\nunlabeled music recordings from YouTube to separate vocals from accompaniment\nwithout the need for ground truth isolated sources or artificial training\nmixtures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:44:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Seetharaman", "Prem", ""], ["Wichern", "Gordon", ""], ["Roux", "Jonathan Le", ""], ["Pardo", "Bryan", ""]]}, {"id": "1910.11142", "submitter": "Yury Maximov", "authors": "Valerii Likhosherstov, Yury Maximov, Michael Chertkov", "title": "Tractable Minor-free Generalization of Planar Zero-field Ising Models", "comments": "32 pages. arXiv admin note: substantial text overlap with\n  arXiv:1906.06431, arXiv:1812.09587", "journal-ref": null, "doi": "10.1088/1742-5468/abcaf1", "report-no": null, "categories": "cs.DS math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of zero-field Ising models over $N$ binary\nvariables/spins obtained by consecutive \"gluing\" of planar and $O(1)$-sized\ncomponents and subsets of at most three vertices into a tree. The\npolynomial-time algorithm of the dynamic programming type for solving exact\ninference (computing partition function) and exact sampling (generating i.i.d.\nsamples) consists in a sequential application of an efficient (for planar) or\nbrute-force (for $O(1)$-sized) inference and sampling to the components as a\nblack box. To illustrate the utility of the new family of tractable graphical\nmodels, we first build a polynomial algorithm for inference and sampling of\nzero-field Ising models over $K_{3,3}$-minor-free topologies and over\n$K_{5}$-minor-free topologies -- both are extensions of the planar zero-field\nIsing models -- which are neither genus - nor treewidth-bounded. Second, we\ndemonstrate empirically an improvement in the approximation quality of the\nNP-hard problem of inference over the square-grid Ising model in a\nnode-dependent non-zero \"magnetic\" field.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 10:39:48 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Likhosherstov", "Valerii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1910.11144", "submitter": "Hossein Baktash", "authors": "Hossein Baktash (CRISAM, SUT), Emanuele Natale (COATI), Laurent\n  Viennot (GANG)", "title": "A Comparative Study of Neural Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been an increasing desire to evaluate neural networks\nlocally on computationally-limited devices in order to exploit their recent\neffectiveness for several applications; such effectiveness has nevertheless\ncome together with a considerable increase in the size of modern neural\nnetworks, which constitute a major downside in several of the aforementioned\ncomputationally-limited settings. There has thus been a demand of compression\ntechniques for neural networks. Several proposal in this direction have been\nmade, which famously include hashing-based methods and pruning-based ones.\nHowever, the evaluation of the efficacy of these techniques has so far been\nheterogeneous, with no clear evidence in favor of any of them over the others.\nThe goal of this work is to address this latter issue by providing a\ncomparative study. While most previous studies test the capability of a\ntechnique in reducing the number of parameters of state-of-the-art networks ,\nwe follow [CWT + 15] in evaluating their performance on basic ar-chitectures on\nthe MNIST dataset and variants of it, which allows for a clearer analysis of\nsome aspects of their behavior. To the best of our knowledge, we are the first\nto directly compare famous approaches such as HashedNet, Optimal Brain Damage\n(OBD), and magnitude-based pruning with L1 and L2 regularization among them and\nagainst equivalent-size feed-forward neural networks with simple\n(fully-connected) and structural (convolutional) neural networks. Rather\nsurprisingly, our experiments show that (iterative) pruning-based methods are\nsubstantially better than the HashedNet architecture, whose compression doesn't\nappear advantageous to a carefully chosen convolutional network. We also show\nthat, when the compression level is high, the famous OBD pruning heuristics\ndeteriorates to the point of being less efficient than simple magnitude-based\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 14:08:51 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Baktash", "Hossein", "", "CRISAM, SUT"], ["Natale", "Emanuele", "", "COATI"], ["Viennot", "Laurent", "", "GANG"]]}, {"id": "1910.11148", "submitter": "Qiegen Liu", "authors": "Zhuonan He, Jinjie Zhou, Dong Liang, Yuhao Wang, Qiegen Liu", "title": "Learning Priors in High-frequency Domain for Inverse Imaging\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed inverse problems in imaging remain an active research topic in\nseveral decades, with new approaches constantly emerging. Recognizing that the\npopular dictionary learning and convolutional sparse coding are both\nessentially modeling the high-frequency component of an image, which convey\nmost of the semantic information such as texture details, in this work we\npropose a novel multi-profile high-frequency transform-guided denoising\nautoencoder as prior (HF-DAEP). To achieve this goal, we first extract a set of\nmulti-profile high-frequency components via a specific transformation and add\nthe artificial Gaussian noise to these high-frequency components as training\nsamples. Then, as the high-frequency prior information is learned, we\nincorporate it into classical iterative reconstruction process by proximal\ngradient descent technique. Preliminary results on highly under-sampled\nmagnetic resonance imaging and sparse-view computed tomography reconstruction\ndemonstrate that the proposed method can efficiently reconstruct feature\ndetails and present advantages over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:15:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["He", "Zhuonan", ""], ["Zhou", "Jinjie", ""], ["Liang", "Dong", ""], ["Wang", "Yuhao", ""], ["Liu", "Qiegen", ""]]}, {"id": "1910.11160", "submitter": "Rulin Shao", "authors": "Rulin Shao, Hongyu He, Hui Liu, Dianbo Liu", "title": "Stochastic Channel-Based Federated Learning for Medical Data Privacy\n  Preserving", "comments": "6 pages including references, 2 figures, Machine Learning for Health\n  (ML4H) at NeurIPS 2019 - Extended Abstract. arXiv admin note: substantial\n  text overlap with arXiv:1910.02115", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural network has achieved unprecedented success in the medical\ndomain. This success depends on the availability of massive and representative\ndatasets. However, data collection is often prevented by privacy concerns and\npeople want to take control over their sensitive information during both\ntraining and using processes. To address this problem, we propose a\nprivacy-preserving method for the distributed system, Stochastic Channel-Based\nFederated Learning (SCBF), which enables the participants to train a\nhigh-performance model cooperatively without sharing their inputs.\nSpecifically, we design, implement and evaluate a channel-based update\nalgorithm for the central server in a distributed system, which selects the\nchannels with regard to the most active features in a training loop and uploads\nthem as learned information from local datasets. A pruning process is applied\nto the algorithm based on the validation set, which serves as a model\naccelerator. In the experiment, our model presents better performances and\nhigher saturating speed than the Federated Averaging method which reveals all\nthe parameters of local models to the server when updating. We also demonstrate\nthat the saturating rate of performance could be promoted by introducing a\npruning process. And further improvement could be achieved by tuning the\npruning rate. Our experiment shows that 57% of the time is saved by the pruning\nprocess with only a reduction of 0.0047 in AUCROC performance and a reduction\nof 0.0068 in AUCPR.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:08:55 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:26:57 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 15:00:57 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Shao", "Rulin", ""], ["He", "Hongyu", ""], ["Liu", "Hui", ""], ["Liu", "Dianbo", ""]]}, {"id": "1910.11162", "submitter": "Mathias Perslev", "authors": "Mathias Perslev, Michael Hejselbak Jensen, Sune Darkner, Poul\n  J{\\o}rgen Jennum, Christian Igel", "title": "U-Time: A Fully Convolutional Network for Time Series Segmentation\n  Applied to Sleep Staging", "comments": "To appear in Advances in Neural Information Processing Systems\n  (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are becoming more and more popular for the analysis of\nphysiological time-series. The most successful deep learning systems in this\ndomain combine convolutional and recurrent layers to extract useful features to\nmodel temporal relations. Unfortunately, these recurrent models are difficult\nto tune and optimize. In our experience, they often require task-specific\nmodifications, which makes them challenging to use for non-experts. We propose\nU-Time, a fully feed-forward deep learning approach to physiological time\nseries segmentation developed for the analysis of sleep data. U-Time is a\ntemporal fully convolutional network based on the U-Net architecture that was\noriginally proposed for image segmentation. U-Time maps sequential inputs of\narbitrary length to sequences of class labels on a freely chosen temporal\nscale. This is done by implicitly classifying every individual time-point of\nthe input signal and aggregating these classifications over fixed intervals to\nform the final predictions. We evaluated U-Time for sleep stage classification\non a large collection of sleep electroencephalography (EEG) datasets. In all\ncases, we found that U-Time reaches or outperforms current state-of-the-art\ndeep learning models while being much more robust in the training process and\nwithout requiring architecture or hyperparameter adaptation across tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 14:20:47 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Perslev", "Mathias", ""], ["Jensen", "Michael Hejselbak", ""], ["Darkner", "Sune", ""], ["Jennum", "Poul J\u00f8rgen", ""], ["Igel", "Christian", ""]]}, {"id": "1910.11163", "submitter": "Chae-Yeun Park", "authors": "Chae-Yeun Park and Michael J. Kastoryano", "title": "Geometry of learning neural quantum states", "comments": "18 pages, 10 Figures, close to a version published in Physical Review\n  Research", "journal-ref": "Phys. Rev. Research 2, 023232 (2020)", "doi": "10.1103/PhysRevResearch.2.023232", "report-no": null, "categories": "quant-ph cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining insights from machine learning and quantum Monte Carlo, the\nstochastic reconfiguration method with neural network Ansatz states is a\npromising new direction for high-precision ground state estimation of quantum\nmany-body problems. Even though this method works well in practice, little is\nknown about the learning dynamics. In this paper, we bring to light several\nhidden details of the algorithm by analyzing the learning landscape. In\nparticular, the spectrum of the quantum Fisher matrix of complex restricted\nBoltzmann machine states exhibits a universal initial dynamics, but the\nconverged spectrum can dramatically change across a phase transition. In\ncontrast to the spectral properties of the quantum Fisher matrix, the actual\nweights of the network at convergence do not reveal much information about the\nsystem or the dynamics. Furthermore, we identify a new measure of correlation\nin the state by analyzing entanglement in eigenvectors. We show that,\ngenerically, the learning landscape modes with least entanglement have largest\neigenvalue, suggesting that correlations are encoded in large flat valleys of\nthe learning landscape, favoring stable representations of the ground state.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 14:21:13 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 12:05:01 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Park", "Chae-Yeun", ""], ["Kastoryano", "Michael J.", ""]]}, {"id": "1910.11219", "submitter": "Onur Teymur", "authors": "Onur Teymur and Sarah Filippi", "title": "A Bayesian nonparametric test for conditional independence", "comments": null, "journal-ref": "Foundations of Data Science (2020) 2(2):155-172", "doi": "10.3934/fods.2020009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a Bayesian nonparametric method for quantifying the\nrelative evidence in a dataset in favour of the dependence or independence of\ntwo variables conditional on a third. The approach uses Polya tree priors on\nspaces of conditional probability densities, accounting for uncertainty in the\nform of the underlying distributions in a nonparametric way. The Bayesian\nperspective provides an inherently symmetric probability measure of conditional\ndependence or independence, a feature particularly advantageous in causal\ndiscovery and not employed in existing procedures of this type.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:23:49 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:33:53 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Teymur", "Onur", ""], ["Filippi", "Sarah", ""]]}, {"id": "1910.11240", "submitter": "Xiao Liang", "authors": "Seyed Omid Sajedi and Xiao Liang", "title": "Intensity-Based Feature Selection for Near Real-Time Damage Diagnosis of\n  Building Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near real-time damage diagnosis of building structures after extreme events\n(e.g., earthquakes) is of great importance in structural health monitoring.\nUnlike conventional methods that are usually time-consuming and require human\nexpertise, pattern recognition algorithms have the potential to interpret\nsensor recordings as soon as this information is available. This paper proposes\na robust framework to build a damage prediction model for building structures.\nSupport vector machines are used to predict the existence as well as the\nprobable location of the damage. The model is designed to consider\nprobabilistic approaches in determining hazard intensity given the existing\nattenuation models in performance-based earthquake engineering. Performance of\nthe model regarding accurate and safe predictions is enhanced using Bayesian\noptimization. The proposed framework is evaluated on a reinforced concrete\nmoment frame. Targeting a selected large earthquake scenario, 6,240 nonlinear\ntime history analyses are performed using OpenSees. Simulation results are\nengineered to extract low-dimensional intensity-based features that can be used\nas damage indicators. For the given case study, the proposed model achieves a\npromising accuracy of 83.1% to identify damage location, demonstrating the\ngreat potential of model capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:54:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Sajedi", "Seyed Omid", ""], ["Liang", "Xiao", ""]]}, {"id": "1910.11242", "submitter": "Prabhakar Gupta", "authors": "Prabhakar Gupta", "title": "A context sensitive real-time Spell Checker with language adaptability", "comments": "7 pages, 6 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel language adaptable spell checking system which detects\nspelling errors and suggests context sensitive corrections in real-time. We\nshow that our system can be extended to new languages with minimal\nlanguage-specific processing. Available literature majorly discusses spell\ncheckers for English but there are no publicly available systems which can be\nextended to work for other languages out of the box. Most of the systems do not\nwork in real-time. We explain the process of generating a language's word\ndictionary and n-gram probability dictionaries using Wikipedia-articles data\nand manually curated video subtitles. We present the results of generating a\nlist of suggestions for a misspelled word. We also propose three approaches to\ncreate noisy channel datasets of real-world typographic errors. We compare our\nsystem with industry-accepted spell checker tools for 11 languages. Finally, we\nshow the performance of our system on synthetic datasets for 24 languages.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:00:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gupta", "Prabhakar", ""]]}, {"id": "1910.11247", "submitter": "Philip N. Garner", "authors": "Philip N. Garner and Sibo Tong", "title": "A Bayesian Approach to Recurrence in Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2020.2976978", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by reiterating that common neural network activation functions have\nsimple Bayesian origins. In this spirit, we go on to show that Bayes's theorem\nalso implies a simple recurrence relation; this leads to a Bayesian recurrent\nunit with a prescribed feedback formulation. We show that introduction of a\ncontext indicator leads to a variable feedback that is similar to the forget\nmechanism in conventional recurrent units. A similar approach leads to a\nprobabilistic input gate. The Bayesian formulation leads naturally to the two\npass algorithm of the Kalman smoother or forward-backward algorithm, meaning\nthat inference naturally depends upon future inputs as well as past ones.\nExperiments on speech recognition confirm that the resulting architecture can\nperform as well as a bidirectional recurrent network with the same number of\nparameters as a unidirectional one. Further, when configured explicitly\nbidirectionally, the architecture can exceed the performance of a conventional\nbidirectional recurrence.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:48:52 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 14:45:05 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 16:49:04 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Garner", "Philip N.", ""], ["Tong", "Sibo", ""]]}, {"id": "1910.11299", "submitter": "Andrei Patrascu", "authors": "Paul Irofti, Andrei Patrascu, Andra Baltoiu", "title": "Quick survey of graph-based fraud detection methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, anomaly detection is the problem of distinguishing between normal\ndata samples with well defined patterns or signatures and those that do not\nconform to the expected profiles. Financial transactions, customer reviews,\nsocial media posts are all characterized by relational information. In these\nnetworks, fraudulent behaviour may appear as a distinctive graph edge, such as\nspam message, a node or a larger subgraph structure, such as when a group of\nclients engage in money laundering schemes. Most commonly, these networks are\nrepresented as attributed graphs, with numerical features complementing\nrelational information. We present a survey on anomaly detection techniques\nused for fraud detection that exploit both the graph structure underlying the\ndata and the contextual information contained in the attributes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:30:09 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 07:28:08 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 12:56:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Irofti", "Paul", ""], ["Patrascu", "Andrei", ""], ["Baltoiu", "Andra", ""]]}, {"id": "1910.11313", "submitter": "Andrei Patrascu", "authors": "Andra Baltoiu, Andrei Patrascu, Paul Irofti", "title": "Community-Level Anomaly Detection for Anti-Money Laundering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in networks often boils down to identifying an underlying\ngraph structure on which the abnormal occurrence rests on. Financial fraud\nschemes are one such example, where more or less intricate schemes are employed\nin order to elude transaction security protocols. We investigate the problem of\nlearning graph structure representations using adaptations of dictionary\nlearning aimed at encoding connectivity patterns. In particular, we adapt\ndictionary learning strategies to the specificity of network topologies and\npropose new methods that impose Laplacian structure on the dictionaries\nthemselves. In one adaption we focus on classifying topologies by working\ndirectly on the graph Laplacian and cast the learning problem to accommodate\nits 2D structure. We tackle the same problem by learning dictionaries which\nconsist of vectorized atomic Laplacians, and provide a block coordinate descent\nscheme to solve the new dictionary learning formulation. Imposing Laplacian\nstructure on the dictionaries is also proposed in an adaptation of the Single\nBlock Orthogonal learning method. Results on synthetic graph datasets\ncomprising different graph topologies confirm the potential of dictionaries to\ndirectly represent graph structure information.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:44:37 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Baltoiu", "Andra", ""], ["Patrascu", "Andrei", ""], ["Irofti", "Paul", ""]]}, {"id": "1910.11339", "submitter": "Christian Hennig", "authors": "Fatima Batool and Christian Hennig", "title": "Clustering with the Average Silhouette Width", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Average Silhouette Width (ASW; Rousseeuw (1987)) is a popular cluster\nvalidation index to estimate the number of clusters. Here we address the\nquestion whether it also is suitable as a general objective function to be\noptimized for finding a clustering. We will propose two algorithms (the\nstandard version OSil and a fast version FOSil) and compare them with existing\nclustering methods in an extensive simulation study covering the cases of a\nknown and unknown number of clusters. Real data sets are also analysed, partly\nexploring the use of the new methods with non-Euclidean distances. We will also\nshow that the ASW satisfies some axioms that have been proposed for cluster\nquality functions (Ackerman and Ben-David (2009)). The new methods prove useful\nand sensible in many cases, but some weaknesses are also highlighted. These\nalso concern the use of the ASW for estimating the number of clusters together\nwith other methods, which is of general interest due to the popularity of the\nASW for this task.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:41:30 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 20:19:27 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 13:23:12 GMT"}, {"version": "v4", "created": "Sun, 22 Nov 2020 00:00:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Batool", "Fatima", ""], ["Hennig", "Christian", ""]]}, {"id": "1910.11356", "submitter": "Ciar\\'an M. Lee", "authors": "Anish Dhir, Ciar\\'an M. Lee", "title": "Integrating overlapping datasets using bivariate causal discovery", "comments": "Accepted to the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-20). 8+2 pages, 3+3 figures. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal knowledge is vital for effective reasoning in science, as causal\nrelations, unlike correlations, allow one to reason about the outcomes of\ninterventions. Algorithms that can discover causal relations from observational\ndata are based on the assumption that all variables have been jointly measured\nin a single dataset. In many cases this assumption fails. Previous approaches\nto overcoming this shortcoming devised algorithms that returned all joint\ncausal structures consistent with the conditional independence information\ncontained in each individual dataset. But, as conditional independence tests\nonly determine causal structure up to Markov equivalence, the number of\nconsistent joint structures returned by these approaches can be quite large.\nThe last decade has seen the development of elegant algorithms for discovering\ncausal relations beyond conditional independence, which can distinguish among\nMarkov equivalent structures. In this work we adapt and extend these so-called\nbivariate causal discovery algorithms to the problem of learning consistent\ncausal structures from multiple datasets with overlapping variables belonging\nto the same generating process, providing a sound and complete algorithm that\noutperforms previous approaches on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:01:03 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 13:20:14 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Dhir", "Anish", ""], ["Lee", "Ciar\u00e1n M.", ""]]}, {"id": "1910.11363", "submitter": "Vickram Rajendran", "authors": "Vickram Rajendran, William LeVine", "title": "Accurate Layerwise Interpretable Competence Estimation", "comments": "Proceedings of the 33rd Conference in Neural Information Processing\n  Systems (2019), 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating machine learning performance 'in the wild' is both an important\nand unsolved problem. In this paper, we seek to examine, understand, and\npredict the pointwise competence of classification models. Our contributions\nare twofold: First, we establish a statistically rigorous definition of\ncompetence that generalizes the common notion of classifier confidence; second,\nwe present the ALICE (Accurate Layerwise Interpretable Competence Estimation)\nScore, a pointwise competence estimator for any classifier. By considering\ndistributional, data, and model uncertainty, ALICE empirically shows accurate\ncompetence estimation in common failure situations such as class-imbalanced\ndatasets, out-of-distribution datasets, and poorly trained models. Our\ncontributions allow us to accurately predict the competence of any\nclassification model given any input and error function. We compare our score\nwith state-of-the-art confidence estimators such as model confidence and Trust\nScore, and show significant improvements in competence prediction over these\nmethods on datasets such as DIGITS, CIFAR10, and CIFAR100.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:10:35 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Rajendran", "Vickram", ""], ["LeVine", "William", ""]]}, {"id": "1910.11368", "submitter": "Viet Lai", "authors": "Viet Dac Lai and Thien Huu Nguyen", "title": "Extending Event Detection to New Types with Learning from Keywords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditional event detection classifies a word or a phrase in a given sentence\nfor a set of predefined event types. The limitation of such predefined set is\nthat it prevents the adaptation of the event detection models to new event\ntypes. We study a novel formulation of event detection that describes types via\nseveral keywords to match the contexts in documents. This facilitates the\noperation of the models to new types. We introduce a novel feature-based\nattention mechanism for convolutional neural networks for event detection in\nthe new formulation. Our extensive experiments demonstrate the benefits of the\nnew formulation for new type extension for event detection as well as the\nproposed attention mechanism for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:20:48 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Lai", "Viet Dac", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "1910.11369", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel", "title": "Structured Prediction with Projection Oracles", "comments": "In proceedings of NeurIPS 2019 (v2: minor modifications in Appendix\n  A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a general framework for deriving loss functions for\nstructured prediction. In our framework, the user chooses a convex set\nincluding the output space and provides an oracle for projecting onto that set.\nGiven that oracle, our framework automatically generates a corresponding convex\nand smooth loss function. As we show, adding a projection as output layer\nprovably makes the loss smaller. We identify the marginal polytope, the output\nspace's convex hull, as the best convex set on which to project. However,\nbecause the projection onto the marginal polytope can sometimes be expensive to\ncompute, we allow to use any convex superset instead, with potentially\ncheaper-to-compute projection. Since efficient projection algorithms are\navailable for numerous convex sets, this allows us to construct loss functions\nfor a variety of tasks. On the theoretical side, when combined with calibrated\ndecoding, we prove that our loss functions can be used as a consistent\nsurrogate for a (potentially non-convex) target loss function of interest. We\ndemonstrate our losses on label ranking, ordinal regression and multilabel\nclassification, confirming the improved accuracy enabled by projections.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:26:18 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 16:20:06 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Blondel", "Mathieu", ""]]}, {"id": "1910.11374", "submitter": "Bruno Scalzo Dees", "authors": "Jean P. Chereau, Bruno Scalzo Dees, Danilo P. Mandic", "title": "Robust Principal Component Analysis Based On Maximum Correntropy Power\n  Iterations", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is recognised as a quintessential data\nanalysis technique when it comes to describing linear relationships between the\nfeatures of a dataset. However, the well-known sensitivity of PCA to\nnon-Gaussian samples and/or outliers often makes it unreliable in practice. To\nthis end, a robust formulation of PCA is derived based on the maximum\ncorrentropy criterion (MCC) so as to maximise the expected likelihood of\nGaussian distributed reconstruction errors. In this way, the proposed solution\nreduces to a generalised power iteration, whereby: (i) robust estimates of the\nprincipal components are obtained even in the presence of outliers; (ii) the\nnumber of principal components need not be specified in advance; and (iii) the\nentire set of principal components can be obtained, unlike existing approaches.\nThe advantages of the proposed maximum correntropy power iteration (MCPI) are\ndemonstrated through an intuitive numerical example.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:51:27 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Chereau", "Jean P.", ""], ["Dees", "Bruno Scalzo", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1910.11385", "submitter": "David Widmann", "authors": "David Widmann and Fredrik Lindsten and Dave Zachariah", "title": "Calibration tests in multi-class classification: A unifying framework", "comments": "Corrected version that 1) fixes the ECE evaluation with bins of\n  uniform size (does not affect our conclusions and discussions) and 2)\n  contains additional experimental results in the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In safety-critical applications a probabilistic model is usually required to\nbe calibrated, i.e., to capture the uncertainty of its predictions accurately.\nIn multi-class classification, calibration of the most confident predictions\nonly is often not sufficient. We propose and study calibration measures for\nmulti-class classification that generalize existing measures such as the\nexpected calibration error, the maximum calibration error, and the maximum mean\ncalibration error. We propose and evaluate empirically different consistent and\nunbiased estimators for a specific class of measures based on matrix-valued\nkernels. Importantly, these estimators can be interpreted as test statistics\nassociated with well-defined bounds and approximations of the p-value under the\nnull hypothesis that the model is calibrated, significantly improving the\ninterpretability of calibration measures, which otherwise lack any meaningful\nunit or scale.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 19:13:19 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 17:01:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Widmann", "David", ""], ["Lindsten", "Fredrik", ""], ["Zachariah", "Dave", ""]]}, {"id": "1910.11424", "submitter": "Abhinav Gupta", "authors": "Cinjon Resnick, Abhinav Gupta, Jakob Foerster, Andrew M. Dai,\n  Kyunghyun Cho", "title": "Capacity, Bandwidth, and Compositionality in Emergent Language Learning", "comments": "The first two authors contributed equally. Accepted at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works have discussed the propensity, or lack thereof, for\nemergent languages to exhibit properties of natural languages. A favorite in\nthe literature is learning compositionality. We note that most of those works\nhave focused on communicative bandwidth as being of primary importance. While\nimportant, it is not the only contributing factor. In this paper, we\ninvestigate the learning biases that affect the efficacy and compositionality\nof emergent languages. Our foremost contribution is to explore how capacity of\na neural network impacts its ability to learn a compositional language. We\nadditionally introduce a set of evaluation metrics with which we analyze the\nlearned languages. Our hypothesis is that there should be a specific range of\nmodel capacity and channel bandwidth that induces compositional structure in\nthe resulting language and consequently encourages systematic generalization.\nWhile we empirically see evidence for the bottom of this range, we curiously do\nnot find evidence for the top part of the range and believe that this is an\nopen question for the community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:06:38 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 22:36:24 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 07:54:53 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Resnick", "Cinjon", ""], ["Gupta", "Abhinav", ""], ["Foerster", "Jakob", ""], ["Dai", "Andrew M.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1910.11432", "submitter": "Chengshu Li", "authors": "Chengshu Li, Fei Xia, Roberto Martin-Martin, Silvio Savarese", "title": "HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation\n  with Mobile Manipulators", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most common navigation tasks in human environments require auxiliary arm\ninteractions, e.g. opening doors, pressing buttons and pushing obstacles away.\nThis type of navigation tasks, which we call Interactive Navigation, requires\nthe use of mobile manipulators: mobile bases with manipulation capabilities.\nInteractive Navigation tasks are usually long-horizon and composed of\nheterogeneous phases of pure navigation, pure manipulation, and their\ncombination. Using the wrong part of the embodiment is inefficient and hinders\nprogress. We propose HRL4IN, a novel Hierarchical RL architecture for\nInteractive Navigation tasks. HRL4IN exploits the exploration benefits of HRL\nover flat RL for long-horizon tasks thanks to temporally extended commitments\ntowards subgoals. Different from other HRL solutions, HRL4IN handles the\nheterogeneous nature of the Interactive Navigation task by creating subgoals in\ndifferent spaces in different phases of the task. Moreover, HRL4IN selects\ndifferent parts of the embodiment to use for each phase, improving energy\nefficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL\nalgorithm, on Interactive Navigation in two environments - a 2D grid-world\nenvironment and a 3D environment with physics simulation. We show that HRL4IN\nsignificantly outperforms its baselines in terms of task performance and energy\nefficiency. More information is available at\nhttps://sites.google.com/view/hrl4in.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:34:29 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Li", "Chengshu", ""], ["Xia", "Fei", ""], ["Martin-Martin", "Roberto", ""], ["Savarese", "Silvio", ""]]}, {"id": "1910.11436", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, Cesare Alippi", "title": "Hierarchical Representation Learning in Graph Neural Networks with Node\n  Decimation Pooling", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.3044146", "report-no": null, "categories": "cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph neural networks (GNNs), pooling operators compute local summaries of\ninput graphs to capture their global properties, and they are fundamental for\nbuilding deep GNNs that learn hierarchical representations. In this work, we\npropose the Node Decimation Pooling (NDP), a pooling operator for GNNs that\ngenerates coarser graphs while preserving the overall graph topology. During\ntraining, the GNN learns new node representations and fits them to a pyramid of\ncoarsened graphs, which is computed offline in a pre-processing stage. NDP\nconsists of three steps. First, a node decimation procedure selects the nodes\nbelonging to one side of the partition identified by a spectral algorithm that\napproximates the \\maxcut{} solution. Afterwards, the selected nodes are\nconnected with Kron reduction to form the coarsened graph. Finally, since the\nresulting graph is very dense, we apply a sparsification procedure that prunes\nthe adjacency matrix of the coarsened graph to reduce the computational cost in\nthe GNN. Notably, we show that it is possible to remove many edges without\nsignificantly altering the graph structure. Experimental results show that NDP\nis more efficient compared to state-of-the-art graph pooling operators while\nreaching, at the same time, competitive performance on a significant variety of\ngraph classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:42:12 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 14:57:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Grattarola", "Daniele", ""], ["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""]]}, {"id": "1910.11452", "submitter": "Ananth Balashankar", "authors": "Ananth Balashankar, Alyssa Lees", "title": "Fairness Sample Complexity and the Case for Human Intervention", "comments": "Where is the Human? Bridging the Gap Between AI and HCI, CHI Workshop\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of building machine learning systems that incorporate standards\nof fairness and accountability, we explore explicit subgroup sample complexity\nbounds. The work is motivated by the observation that classifier predictions\nfor real world datasets often demonstrate drastically different metrics, such\nas accuracy, when subdivided by specific sensitive variable subgroups. The\nreasons for these discrepancies are varied and not limited to the influence of\nmitigating variables, institutional bias, underlying population distributions\nas well as sampling bias. Among the numerous definitions of fairness that\nexist, we argue that at a minimum, principled ML practices should ensure that\nclassification predictions are able to mirror the underlying sub-population\ndistributions. However, as the number of sensitive variables increase,\npopulations meeting at the intersectionality of these variables may simply not\nexist or may not be large enough to provide accurate samples for\nclassification. In these increasingly likely scenarios, we make the case for\nhuman intervention and applying situational and individual definitions of\nfairness. In this paper we present lower bounds of subgroup sample complexity\nfor metric-fair learning based on the theory of Probably Approximately Metric\nFair Learning. We demonstrate that for a classifier to approach a definition of\nfairness in terms of specific sensitive variables, adequate subgroup population\nsamples need to exist and the model dimensionality has to be aligned with\nsubgroup population distributions. In cases where this is not feasible, we\npropose an approach using individual fairness definitions for achieving\nalignment. We look at two commonly explored UCI datasets under this lens and\nsuggest human interventions for data collection for specific subgroups to\nachieve approximate individual fairness for linear hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 23:10:59 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Balashankar", "Ananth", ""], ["Lees", "Alyssa", ""]]}, {"id": "1910.11482", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Naimul Khan", "title": "Human Action Recognition Using Deep Multilevel Multimodal (M2) Fusion of\n  Depth and Inertial Sensors", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal fusion frameworks for Human Action Recognition (HAR) using depth\nand inertial sensor data have been proposed over the years. In most of the\nexisting works, fusion is performed at a single level (feature level or\ndecision level), missing the opportunity to fuse rich mid-level features\nnecessary for better classification. To address this shortcoming, in this\npaper, we propose three novel deep multilevel multimodal fusion frameworks to\ncapitalize on different fusion strategies at various stages and to leverage the\nsuperiority of multilevel fusion. At input, we transform the depth data into\ndepth images called sequential front view images (SFIs) and inertial sensor\ndata into signal images. Each input modality, depth and inertial, is further\nmade multimodal by taking convolution with the Prewitt filter. Creating\n\"modality within modality\" enables further complementary and discriminative\nfeature extraction through Convolutional Neural Networks (CNNs). CNNs are\ntrained on input images of each modality to learn low-level, high-level and\ncomplex features. Learned features are extracted and fused at different stages\nof the proposed frameworks to combine discriminative and complementary\ninformation. These highly informative features are served as input to a\nmulti-class Support Vector Machine (SVM). We evaluate the proposed frameworks\non three publicly available multimodal HAR datasets, namely, UTD Multimodal\nHuman Action Dataset (MHAD), Berkeley MHAD, and UTD-MHAD Kinect V2.\nExperimental results show the supremacy of the proposed fusion frameworks over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:29:58 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "1910.11492", "submitter": "Vikas Ramachandra", "authors": "Vikas Ramachandra", "title": "Causal inference for climate change events from satellite image time\n  series using computer vision and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for causal inference using satellite image time series,\nin order to determine the treatment effects of interventions which impact\nclimate change, such as deforestation. Simply put, the aim is to quantify the\n'before versus after' effect of climate related human driven interventions,\nsuch as urbanization; as well as natural disasters, such as hurricanes and\nforest fires. As a concrete example, we focus on quantifying forest tree cover\nchange/ deforestation due to human led causes. The proposed method involves the\nfollowing steps. First, we uae computer vision and machine learning/deep\nlearning techniques to detect and quantify forest tree coverage levels over\ntime, at every time epoch. We then look at this time series to identify\nchangepoints. Next, we estimate the expected (forest tree cover) values using a\nBayesian structural causal model and projecting/forecasting the counterfactual.\nThis is compared to the values actually observed post intervention, and the\ndifference in the two values gives us the effect of the intervention (as\ncompared to the non intervention scenario, i.e. what would have possibly\nhappened without the intervention). As a specific use case, we analyze\ndeforestation levels before and after the hyperinflation event (intervention)\nin Brazil (which ended in 1993-94), for the Amazon rainforest region, around\nRondonia, Brazil. For this deforestation use case, using our causal inference\nframework can help causally attribute change/reduction in forest tree cover and\nincreasing deforestation rates due to human activities at various points in\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 02:16:15 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ramachandra", "Vikas", ""]]}, {"id": "1910.11508", "submitter": "Tong Zhang", "authors": "Cong Fang and Hanze Dong and Tong Zhang", "title": "Over Parameterized Two-level Neural Networks Can Learn Near Optimal\n  Feature Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, over-parameterized neural networks have been extensively analyzed\nin the literature. However, the previous studies cannot satisfactorily explain\nwhy fully trained neural networks are successful in practice. In this paper, we\npresent a new theoretical framework for analyzing over-parameterized neural\nnetworks which we call neural feature repopulation. Our analysis can\nsatisfactorily explain the empirical success of two level neural networks that\nare trained by standard learning algorithms. Our key theoretical result is that\nin the limit of infinite number of hidden neurons, over-parameterized two-level\nneural networks trained via the standard (noisy) gradient descent learns a\nwell-defined feature distribution (population), and the limiting feature\ndistribution is nearly optimal for the underlying learning task under certain\nconditions. Empirical studies confirm that predictions of our theory are\nconsistent with the results observed in real practice.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 03:06:06 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Fang", "Cong", ""], ["Dong", "Hanze", ""], ["Zhang", "Tong", ""]]}, {"id": "1910.11519", "submitter": "Raef Bassily", "authors": "Noga Alon, Raef Bassily, Shay Moran", "title": "Limits of Private Learning with Access to Public Data", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems where the training set consists of two types of\nexamples: private and public. The goal is to design a learning algorithm that\nsatisfies differential privacy only with respect to the private examples. This\nsetting interpolates between private learning (where all examples are private)\nand classical learning (where all examples are public).\n  We study the limits of learning in this setting in terms of private and\npublic sample complexities. We show that any hypothesis class of VC-dimension\n$d$ can be agnostically learned up to an excess error of $\\alpha$ using only\n(roughly) $d/\\alpha$ public examples and $d/\\alpha^2$ private labeled examples.\nThis result holds even when the public examples are unlabeled. This gives a\nquadratic improvement over the standard $d/\\alpha^2$ upper bound on the public\nsample complexity (where private examples can be ignored altogether if the\npublic examples are labeled). Furthermore, we give a nearly matching lower\nbound, which we prove via a generic reduction from this setting to the one of\nprivate learning without public data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 04:27:26 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Alon", "Noga", ""], ["Bassily", "Raef", ""], ["Moran", "Shay", ""]]}, {"id": "1910.11525", "submitter": "Umar Islambekov", "authors": "Umar Islambekov and Yulia Gel", "title": "Unsupervised Space-Time Clustering using Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new clustering algorithm for space-time data based on\nthe concepts of topological data analysis and in particular, persistent\nhomology. Employing persistent homology - a flexible mathematical tool from\nalgebraic topology used to extract topological information from data - in\nunsupervised learning is an uncommon and a novel approach. A notable aspect of\nthis methodology consists in analyzing data at multiple resolutions which\nallows to distinguish true features from noise based on the extent of their\npersistence. We evaluate the performance of our algorithm on synthetic data and\ncompare it to other well-known clustering algorithms such as K-means,\nhierarchical clustering and DBSCAN. We illustrate its application in the\ncontext of a case study of water quality in the Chesapeake Bay.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 04:51:30 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Islambekov", "Umar", ""], ["Gel", "Yulia", ""]]}, {"id": "1910.11540", "submitter": "Kenji Yamanishi", "authors": "Kenji Yamanishi", "title": "Descriptive Dimensionality and Its Characterization of MDL-based\n  Learning and Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new notion of dimensionality of probabilistic models\nfrom an information-theoretic view point. We call it the \"descriptive\ndimension\"(Ddim). We show that Ddim coincides with the number of independent\nparameters for the parametric class, and can further be extended to real-valued\ndimensionality when a number of models are mixed. The paper then derives the\nrate of convergence of the MDL (Minimum Description Length) learning algorithm\nwhich outputs a normalized maximum likelihood (NML) distribution with model of\nthe shortest NML codelength. The paper proves that the rate is governed by\nDdim. The paper also derives error probabilities of the MDL-based test for\nmultiple model change detection. It proves that they are also governed by Ddim.\nThrough the analysis, we demonstrate that Ddim is an intrinsic quantity which\ncharacterizes the performance of the MDL-based learning and change detection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 05:55:25 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Yamanishi", "Kenji", ""]]}, {"id": "1910.11552", "submitter": "Jie He", "authors": "Jie He, Tao Chen, Zhijun Zhang", "title": "A Gegenbauer Neural Network with Regularized Weights Direct\n  Determination for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-hidden layer feed forward neural networks (SLFNs) are widely used in\npattern classification problems, but a huge bottleneck encountered is the slow\nspeed and poor performance of the traditional iterative gradient-based learning\nalgorithms. Although the famous extreme learning machine (ELM) has successfully\naddressed the problems of slow convergence, it still has computational\nrobustness problems brought by input weights and biases randomly assigned.\nThus, in order to overcome the aforementioned problems, in this paper, a novel\ntype neural network based on Gegenbauer orthogonal polynomials, termed as GNN,\nis constructed and investigated. This model could overcome the computational\nrobustness problems of ELM, while still has comparable structural simplicity\nand approximation capability. Based on this, we propose a regularized weights\ndirect determination (R-WDD) based on equality-constrained optimization to\ndetermine the optimal output weights. The R-WDD tends to minimize the empirical\nrisks and structural risks of the network, thus to lower the risk of over\nfitting and improve the generalization ability. This leads us to a the final\nGNN with R-WDD, which is a unified learning mechanism for binary and\nmulti-class classification problems. Finally, as is verified in the various\ncomparison experiments, GNN with R-WDD tends to have comparable (or even\nbetter) generalization performances, computational scalability and efficiency,\nand classification robustness, compared to least square support vector machine\n(LS-SVM), ELM with Gaussian kernel.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 07:04:21 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["He", "Jie", ""], ["Chen", "Tao", ""], ["Zhang", "Zhijun", ""]]}, {"id": "1910.11555", "submitter": "Zhiqing Sun", "authors": "Zhiqing Sun, Zhuohan Li, Haoqing Wang, Zi Lin, Di He, Zhi-Hong Deng", "title": "Fast Structured Decoding for Sequence Models", "comments": "Accepted to NeurIPS 2019 (Previous title: Structured Decoding for\n  Non-Autoregressive Machine Translation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive sequence models achieve state-of-the-art performance in\ndomains like machine translation. However, due to the autoregressive\nfactorization nature, these models suffer from heavy latency during inference.\nRecently, non-autoregressive sequence models were proposed to reduce the\ninference time. However, these models assume that the decoding process of each\ntoken is conditionally independent of others. Such a generation process\nsometimes makes the output sentence inconsistent, and thus the learned\nnon-autoregressive models could only achieve inferior accuracy compared to\ntheir autoregressive counterparts. To improve then decoding consistency and\nreduce the inference cost at the same time, we propose to incorporate a\nstructured inference module into the non-autoregressive models. Specifically,\nwe design an efficient approximation for Conditional Random Fields (CRF) for\nnon-autoregressive sequence models, and further propose a dynamic transition\ntechnique to model positional contexts in the CRF. Experiments in machine\ntranslation show that while increasing little latency (8~14ms), our model could\nachieve significantly better translation performance than previous\nnon-autoregressive models on different translation datasets. In particular, for\nthe WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely\noutperforms the previous non-autoregressive baselines and is only 0.61 lower in\nBLEU than purely autoregressive models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 07:32:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 08:25:23 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Sun", "Zhiqing", ""], ["Li", "Zhuohan", ""], ["Wang", "Haoqing", ""], ["Lin", "Zi", ""], ["He", "Di", ""], ["Deng", "Zhi-Hong", ""]]}, {"id": "1910.11583", "submitter": "Esma Balkir", "authors": "Esma Balkir, Masha Naslidnyk, Dave Palfrey, Arpit Mittal", "title": "Using Pairwise Occurrence Information to Improve Knowledge Graph\n  Completion on Large-Scale Datasets", "comments": "8 pages, 3 figures, accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear models such as DistMult and ComplEx are effective methods for\nknowledge graph (KG) completion. However, they require large batch sizes, which\nbecomes a performance bottleneck when training on large scale datasets due to\nmemory constraints. In this paper we use occurrences of entity-relation pairs\nin the dataset to construct a joint learning model and to increase the quality\nof sampled negatives during training. We show on three standard datasets that\nwhen these two techniques are combined, they give a significant improvement in\nperformance, especially when the batch size and the number of generated\nnegative examples are low relative to the size of the dataset. We then apply\nour techniques to a dataset containing 2 million entities and demonstrate that\nour model outperforms the baseline by 2.8% absolute on hits@1.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 09:05:16 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Balkir", "Esma", ""], ["Naslidnyk", "Masha", ""], ["Palfrey", "Dave", ""], ["Mittal", "Arpit", ""]]}, {"id": "1910.11585", "submitter": "Amin Ghiasi", "authors": "Ali Shafahi, Amin Ghiasi, Furong Huang, Tom Goldstein", "title": "Label Smoothing and Logit Squeezing: A Replacement for Adversarial\n  Training?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is one of the strongest defenses against adversarial\nattacks, but it requires adversarial examples to be generated for every\nmini-batch during optimization. The expense of producing these examples during\ntraining often precludes adversarial training from use on complex image\ndatasets. In this study, we explore the mechanisms by which adversarial\ntraining improves classifier robustness, and show that these mechanisms can be\neffectively mimicked using simple regularization methods, including label\nsmoothing and logit squeezing. Remarkably, using these simple regularization\nmethods in combination with Gaussian noise injection, we are able to achieve\nstrong adversarial robustness -- often exceeding that of adversarial training\n-- using no adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 09:06:15 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Shafahi", "Ali", ""], ["Ghiasi", "Amin", ""], ["Huang", "Furong", ""], ["Goldstein", "Tom", ""]]}, {"id": "1910.11599", "submitter": "Saad Mohamad", "authors": "Saad Mohamad and Abdelhamid Bouchachia", "title": "Online Gaussian LDA for Unsupervised Pattern Mining from Utility Usage\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-intrusive load monitoring (NILM) aims at separating a whole-home energy\nsignal into its appliance components. Such method can be harnessed to provide\nvarious services to better manage and control energy consumption (optimal\nplanning and saving). NILM has been traditionally approached from signal\nprocessing and electrical engineering perspectives. Recently, machine learning\nhas started to play an important role in NILM. While most work has focused on\nsupervised algorithms, unsupervised approaches can be more interesting and of\npractical use in real case scenarios. Specifically, they do not require\nlabelled training data to be acquired from individual appliances and the\nalgorithm can be deployed to operate on the measured aggregate data directly.\nIn this paper, we propose a fully unsupervised NILM framework based on Bayesian\nhierarchical mixture models. In particular, we develop a new method based on\nGaussian Latent Dirichlet Allocation (GLDA) in order to extract global\ncomponents that summarise the energy signal. These components provide a\nrepresentation of the consumption patterns. Designed to cope with big data, our\nalgorithm, unlike existing NILM ones, does not focus on appliance recognition.\nTo handle this massive data, GLDA works online. Another novelty of this work\ncompared to the existing NILM is that the data involves different utilities\n(e.g, electricity, water and gas) as well as some sensors measurements.\nFinally, we propose different evaluation methods to analyse the results which\nshow that our algorithm finds useful patterns.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 09:46:52 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Mohamad", "Saad", ""], ["Bouchachia", "Abdelhamid", ""]]}, {"id": "1910.11605", "submitter": "Koyel Mukherjee", "authors": "Koyel Mukherjee, Alind Khare, Ashish Verma", "title": "A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training\n  of DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks on image datasets generally require extensive\nexperimentation to find the optimal learning rate regime. Especially, for the\ncases of adversarial training or for training a newly synthesized model, one\nwould not know the best learning rate regime beforehand. We propose an\nautomated algorithm for determining the learning rate trajectory, that works\nacross datasets and models for both natural and adversarial training, without\nrequiring any dataset/model specific tuning. It is a stand-alone,\nparameterless, adaptive approach with no computational overhead. We\ntheoretically discuss the algorithm's convergence behavior. We empirically\nvalidate our algorithm extensively. Our results show that our proposed approach\n\\emph{consistently} achieves top-level accuracy compared to SOTA baselines in\nthe literature in natural as well as adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:23:12 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Mukherjee", "Koyel", ""], ["Khare", "Alind", ""], ["Verma", "Ashish", ""]]}, {"id": "1910.11617", "submitter": "Hoang Duy Trinh", "authors": "Hoang Duy Trinh, Angel Fernandez Gambin, Lorenza Giupponi, Michele\n  Rossi, Paolo Dini", "title": "Mobile Traffic Classification through Physical Channel Fingerprinting: a\n  Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic classification of applications and services is an invaluable\nfeature for new generation mobile networks. Here, we propose and validate\nalgorithms to perform this task, at runtime, from the raw physical channel of\nan operative mobile network, without having to decode and/or decrypt the\ntransmitted flows. Towards this, we decode Downlink Control Information (DCI)\nmessages carried within the LTE Physical Downlink Control CHannel (PDCCH). DCI\nmessages are sent by the radio cell in clear text and, in this paper, are\nutilized to classify the applications and services executed at the connected\nmobile terminals. Two datasets are collected through a large measurement\ncampaign: one labeled, used to train the classification algorithms, and one\nunlabeled, collected from four radio cells in the metropolitan area of\nBarcelona, in Spain. Among other approaches, our Convolutional Neural Network\n(CNN) classifier provides the highest classification accuracy of 99%. The CNN\nclassifier is then augmented with the capability of rejecting sessions whose\npatterns do not conform to those learned during the training phase, and is\nsubsequently utilized to attain a fine grained decomposition of the traffic for\nthe four monitored radio cells, in an online and unsupervised fashion.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:05:15 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 12:58:41 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 18:44:12 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Trinh", "Hoang Duy", ""], ["Gambin", "Angel Fernandez", ""], ["Giupponi", "Lorenza", ""], ["Rossi", "Michele", ""], ["Dini", "Paolo", ""]]}, {"id": "1910.11623", "submitter": "Alexis Laignelet", "authors": "Batuhan G\\\"uler, Alexis Laignelet, Panos Parpas", "title": "Towards Robust and Stable Deep Learning Algorithms for Forward Backward\n  Stochastic Differential Equations", "comments": "Accepted at NeurIPS 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in quantitative finance such as optimal trade execution, risk\nmanagement of options, and optimal asset allocation involve the solution of\nhigh dimensional and nonlinear Partial Differential Equations (PDEs). The\nconnection between PDEs and systems of Forward-Backward Stochastic Differential\nEquations (FBSDEs) enables the use of advanced simulation techniques to be\napplied even in the high dimensional setting. Unfortunately, when the\nunderlying application contains nonlinear terms, then classical methods both\nfor simulation and numerical methods for PDEs suffer from the curse of\ndimensionality. Inspired by the success of deep learning, several researchers\nhave recently proposed to address the solution of FBSDEs using deep learning.\nWe discuss the dynamical systems point of view of deep learning and compare\nseveral architectures in terms of stability, generalization, and robustness. In\norder to speed up the computations, we propose to use a multilevel\ndiscretization technique. Our preliminary results suggest that the multilevel\ndiscretization method improves solutions times by an order of magnitude\ncompared to existing methods without sacrificing stability or robustness.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:16:01 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["G\u00fcler", "Batuhan", ""], ["Laignelet", "Alexis", ""], ["Parpas", "Panos", ""]]}, {"id": "1910.11632", "submitter": "Michael J. Klaiber", "authors": "Michael J. Klaiber, Sebastian Vogel, Axel Acosta, Robert Korn,\n  Leonardo Ecco, Kristine Back, Andre Guntoro, Ingo Feldner", "title": "An End-to-End HW/SW Co-Design Methodology to Design Efficient Deep\n  Neural Network Systems using Virtual Models", "comments": null, "journal-ref": "Embedded Systems Week 2019, INTelligent Embedded Systems\n  Architectures and Applications Workshop 2019", "doi": "10.1145/3372394.3372396", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end performance estimation and measurement of deep neural network\n(DNN) systems become more important with increasing complexity of DNN systems\nconsisting of hardware and software components. The methodology proposed in\nthis paper aims at a reduced turn-around time for evaluating different design\nchoices of hardware and software components of DNN systems. This reduction is\nachieved by moving the performance estimation from the implementation phase to\nthe concept phase by employing virtual hardware models instead of gathering\nmeasurement results from physical prototypes. Deep learning compilers introduce\nhardware-specific transformations and are, therefore, considered a part of the\ndesign flow of virtual system models to extract end-to-end performance\nestimations. To validate the run-time accuracy of the proposed methodology, a\nsystem processing the DilatedVGG DNN is realized both as virtual system model\nand as hardware implementation. The results show that up to 92 % accuracy can\nbe reached in predicting the processing time of the DNN inference.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:42:00 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 11:35:18 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Klaiber", "Michael J.", ""], ["Vogel", "Sebastian", ""], ["Acosta", "Axel", ""], ["Korn", "Robert", ""], ["Ecco", "Leonardo", ""], ["Back", "Kristine", ""], ["Guntoro", "Andre", ""], ["Feldner", "Ingo", ""]]}, {"id": "1910.11703", "submitter": "Vikram Krishnamurthy", "authors": "William Hoiles and Vikram Krishnamurthy and Kunal Pattanayak", "title": "Rationally Inattentive Inverse Reinforcement Learning Explains YouTube\n  Commenting Behavior", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.09640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel application of inverse reinforcement learning with\nbehavioral economics constraints to model, learn and predict the commenting\nbehavior of YouTube viewers. Each group of users is modeled as a rationally\ninattentive Bayesian agent which solves a contextual bandit problem. Our\nmethodology integrates three key components. First, to identify distinct\ncommenting patterns, we use deep embedded clustering to estimate framing\ninformation (essential extrinsic features) that clusters users into distinct\ngroups.Second, we present an inverse reinforcement learning algorithm that uses\nBayesian revealed preferences to test for rationality: does there exist a\nutility function that rationalizes the given data, and if yes, can it be used\nto predict commenting behavior? Finally, we impose behavioral economics\nconstraints stemming from rational inattention to characterize the attention\nspan of groups of users. The test imposes a R{\\'e}nyi mutual information cost\nconstraint which impacts how the agent can select attention strategies to\nmaximize their expected utility. After a careful analysis of a massive YouTube\ndataset, our surprising result is that in most YouTube user groups, the\ncommenting behavior is consistent with optimizing a Bayesian utility with\nrationally inattentive constraints. The paper also highlights how the rational\ninattention model can accurately predict commenting behavior. The massive\nYouTube dataset and analysis used in this paper are available on GitHub and\ncompletely reproducible.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:46:30 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 17:42:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hoiles", "William", ""], ["Krishnamurthy", "Vikram", ""], ["Pattanayak", "Kunal", ""]]}, {"id": "1910.11710", "submitter": "Zhiqin Xu", "authors": "Wei Cai, Zhi-Qin John Xu", "title": "Multi-scale Deep Neural Networks for Solving High Dimensional PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the idea of radial scaling in frequency domain and\nactivation functions with compact support to produce a multi-scale DNN\n(MscaleDNN), which will have the multi-scale capability in approximating high\nfrequency and high dimensional functions and speeding up the solution of high\ndimensional PDEs. Numerical results on high dimensional function fitting and\nsolutions of high dimensional PDEs, using loss functions with either Ritz\nenergy or least squared PDE residuals, have validated the increased power of\nmulti-scale resolution and high frequency capturing of the proposed MscaleDNN.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:26:39 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Cai", "Wei", ""], ["Xu", "Zhi-Qin John", ""]]}, {"id": "1910.11721", "submitter": "Zhibing Zhao", "authors": "Zhibing Zhao, Lirong Xia", "title": "Learning Mixtures of Plackett-Luce Models from Structured Partial Orders", "comments": "15 pages, 5 figures, accepted by NeurIPS 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of ranking models have been widely used for heterogeneous\npreferences. However, learning a mixture model is highly nontrivial, especially\nwhen the dataset consists of partial orders. In such cases, the parameter of\nthe model may not be even identifiable. In this paper, we focus on three\npopular structures of partial orders: ranked top-$l_1$, $l_2$-way, and choice\ndata over a subset of alternatives. We prove that when the dataset consists of\ncombinations of ranked top-$l_1$ and $l_2$-way (or choice data over up to $l_2$\nalternatives), mixture of $k$ Plackett-Luce models is not identifiable when\n$l_1+l_2\\le 2k-1$ ($l_2$ is set to $1$ when there are no $l_2$-way orders). We\nalso prove that under some combinations, including ranked top-$3$, ranked\ntop-$2$ plus $2$-way, and choice data over up to $4$ alternatives, mixtures of\ntwo Plackett-Luce models are identifiable. Guided by our theoretical results,\nwe propose efficient generalized method of moments (GMM) algorithms to learn\nmixtures of two Plackett-Luce models, which are proven consistent. Our\nexperiments demonstrate the efficacy of our algorithms. Moreover, we show that\nwhen full rankings are available, learning from different marginal events\n(partial orders) provides tradeoffs between statistical efficiency and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:39:34 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zhao", "Zhibing", ""], ["Xia", "Lirong", ""]]}, {"id": "1910.11758", "submitter": "Florian Mai", "authors": "Prabhu Teja Sivaprasad (1 and 2), Florian Mai (1 and 2), Thijs Vogels\n  (2), Martin Jaggi (2), Fran\\c{c}ois Fleuret (2 and 3) ((1) Idiap Research\n  Institute, (2) EPFL, (3) University of Geneva)", "title": "Optimizer Benchmarking Needs to Account for Hyperparameter Tuning", "comments": "published at International Conference on Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of optimizers, particularly in deep learning, depends\nconsiderably on their chosen hyperparameter configuration. The efficacy of\noptimizers is often studied under near-optimal problem-specific\nhyperparameters, and finding these settings may be prohibitively costly for\npractitioners. In this work, we argue that a fair assessment of optimizers'\nperformance must take the computational cost of hyperparameter tuning into\naccount, i.e., how easy it is to find good hyperparameter configurations using\nan automatic hyperparameter search. Evaluating a variety of optimizers on an\nextensive set of standard datasets and architectures, our results indicate that\nAdam is the most practical solution, particularly in low-budget scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:27:00 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:21:17 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 12:15:51 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 14:55:09 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sivaprasad", "Prabhu Teja", "", "1 and 2"], ["Mai", "Florian", "", "1 and 2"], ["Vogels", "Thijs", "", "2 and 3"], ["Jaggi", "Martin", "", "2 and 3"], ["Fleuret", "Fran\u00e7ois", "", "2 and 3"]]}, {"id": "1910.11779", "submitter": "Flavien Prost", "authors": "Flavien Prost, Hai Qian, Qiuwen Chen, Ed H. Chi, Jilin Chen, Alex\n  Beutel", "title": "Toward a better trade-off between performance and fairness with\n  kernel-based distribution matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As recent literature has demonstrated how classifiers often carry unintended\nbiases toward some subgroups, deploying machine learned models to users demands\ncareful consideration of the social consequences. How should we address this\nproblem in a real-world system? How should we balance core performance and\nfairness metrics? In this paper, we introduce a MinDiff framework for\nregularizing classifiers toward different fairness metrics and analyze a\ntechnique with kernel-based statistical dependency tests. We run a thorough\nstudy on an academic dataset to compare the Pareto frontier achieved by\ndifferent regularization approaches, and apply our kernel-based method to two\nlarge-scale industrial systems demonstrating real-world improvements.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:03:11 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Prost", "Flavien", ""], ["Qian", "Hai", ""], ["Chen", "Qiuwen", ""], ["Chi", "Ed H.", ""], ["Chen", "Jilin", ""], ["Beutel", "Alex", ""]]}, {"id": "1910.11800", "submitter": "Alessia Amelio Dr.", "authors": "Radmila Jankovi\\'c, Ivan Mihajlovi\\'c, Alessia Amelio", "title": "Time Series Vector Autoregression Prediction of the Ecological Footprint\n  based on Energy Parameters", "comments": "8 pages, 3 figures, accepted at 5th Jubilee Virtual International\n  Conference on Science, Technology and Management in Energy (eNergetics 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.DM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sustainability became the most important component of world development, as\ncountries worldwide fight the battle against the climate change. To understand\nthe effects of climate change, the ecological footprint, along with the\nbiocapacity should be observed. The big part of the ecological footprint, the\ncarbon footprint, is most directly associated with the energy, and specifically\nfuel sources. This paper develops a time series vector autoregression\nprediction model of the ecological footprint based on energy parameters. The\nobjective of the paper is to forecast the EF based solely on energy parameters\nand determine the relationship between the energy and the EF. The dataset\nincluded global yearly observations of the variables for the period 1971-2014.\nPredictions were generated for every variable that was used in the model for\nthe period 2015-2024. The results indicate that the ecological footprint of\nconsumption will continue increasing, as well as the primary energy consumption\nfrom different sources. However, the energy consumption from coal sources is\npredicted to have a declining trend.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:30:40 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Jankovi\u0107", "Radmila", ""], ["Mihajlovi\u0107", "Ivan", ""], ["Amelio", "Alessia", ""]]}, {"id": "1910.11831", "submitter": "Lingxi Xie", "authors": "Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian", "title": "Stabilizing DARTS with Amended Gradient Estimation on Architectural\n  Parameters", "comments": "22 pages, 12 figures, submitted to ICML 2020, updated experiments on\n  Penn Treebank", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DARTS is a popular algorithm for neural architecture search (NAS). Despite\nits great advantage in search efficiency, DARTS often suffers weak stability,\nwhich reflects in the large variation among individual trials as well as the\nsensitivity to the hyper-parameters of the search process. This paper owes such\ninstability to an optimization gap between the super-network and its\nsub-networks, namely, improving the validation accuracy of the super-network\ndoes not necessarily lead to a higher expectation on the performance of the\nsampled sub-networks. Then, we point out that the gap is due to the inaccurate\nestimation of the architectural gradients, based on which we propose an amended\nestimation method. Mathematically, our method guarantees a bounded error from\nthe true gradients while the original estimation does not. Our approach bridges\nthe gap from two aspects, namely, amending the estimation on the architectural\ngradients, and unifying the hyper-parameter settings in the search and\nre-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our\napproach largely improves search stability and, more importantly, enables\nDARTS-based approaches to explore much larger search spaces that have not been\ninvestigated before.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:31:25 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 08:59:51 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 04:22:46 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 04:19:28 GMT"}, {"version": "v5", "created": "Mon, 4 May 2020 10:19:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bi", "Kaifeng", ""], ["Hu", "Changping", ""], ["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Wei", "Longhui", ""], ["Tian", "Qi", ""]]}, {"id": "1910.11843", "submitter": "Yang Zhou", "authors": "Yangxin Lin, Ping Wang, Yang Zhou, Fan Ding, Chen Wang, Huachun Tan", "title": "Platoon trajectories generation: A unidirectional interconnected\n  LSTM-based car following model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Car following models have been widely applied and made remarkable\nachievements in traffic engineering. However, the traffic micro-simulation\naccuracy of car following models in a platoon level, especially during traffic\noscillations, still needs to be enhanced. Rather than using traditional\nindividual car following models, we proposed a new trajectory generation\napproach to generate platoon level trajectories given the first leading\nvehicle's trajectory. In this paper, we discussed the temporal and spatial\nerror propagation issue for the traditional approach by a car following block\ndiagram representation. Based on the analysis, we pointed out that error comes\nfrom the training method and the model structure. In order to fix that, we\nadopt two improvements on the basis of the traditional LSTM based car following\nmodel. We utilized a scheduled sampling technique during the training process\nto solve the error propagation in the temporal dimension. Furthermore, we\ndeveloped a unidirectional interconnected LSTM model structure to extract\ntrajectories features from the perspective of the platoon. As indicated by the\nsystematic empirical experiments, the proposed novel structure could\nefficiently reduce the temporal and spatial error propagation. Compared with\nthe traditional LSTM based car following model, the proposed model has almost\n40% less error. The findings will benefit the design and analysis of\nmicro-simulation for platoon level car following models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:58:00 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Lin", "Yangxin", ""], ["Wang", "Ping", ""], ["Zhou", "Yang", ""], ["Ding", "Fan", ""], ["Wang", "Chen", ""], ["Tan", "Huachun", ""]]}, {"id": "1910.11858", "submitter": "Colin White", "authors": "Colin White, Willie Neiswanger, Yash Savani", "title": "BANANAS: Bayesian Optimization with Neural Architectures for Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past half-decade, many methods have been considered for neural\narchitecture search (NAS). Bayesian optimization (BO), which has long had\nsuccess in hyperparameter optimization, has recently emerged as a very\npromising strategy for NAS when it is coupled with a neural predictor. Recent\nwork has proposed different instantiations of this framework, for example,\nusing Bayesian neural networks or graph convolutional networks as the\npredictive model within BO. However, the analyses in these papers often focus\non the full-fledged NAS algorithm, so it is difficult to tell which individual\ncomponents of the framework lead to the best performance.\n  In this work, we give a thorough analysis of the \"BO + neural predictor\"\nframework by identifying five main components: the architecture encoding,\nneural predictor, uncertainty calibration method, acquisition function, and\nacquisition optimization strategy. We test several different methods for each\ncomponent and also develop a novel path-based encoding scheme for neural\narchitectures, which we show theoretically and empirically scales better than\nother encodings. Using all of our analyses, we develop a final algorithm called\nBANANAS, which achieves state-of-the-art performance on NAS search spaces. We\nadhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate\nbest practices, and our code is available at\nhttps://github.com/naszilla/naszilla.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:35:49 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 08:39:44 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 15:28:47 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["White", "Colin", ""], ["Neiswanger", "Willie", ""], ["Savani", "Yash", ""]]}, {"id": "1910.11868", "submitter": "Yakup Ceki Papo", "authors": "Yakup Ceki Papo", "title": "Bias-Variance Tradeoff in a Sliding Window Implementation of the\n  Stochastic Gradient Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework to analyze stochastic gradient algorithms in\na mean squared error (MSE) sense using the asymptotic normality result of the\nstochastic gradient descent (SGD) iterates. We perform this analysis by taking\nthe asymptotic normality result and applying it to the finite iteration case.\nSpecifically, we look at problems where the gradient estimators are biased and\nhave reduced variance and compare the iterates generated by these gradient\nestimators to the iterates generated by the SGD algorithm. We use the work of\nFabian to characterize the mean and the variance of the distribution of the\niterates in terms of the bias and the covariance matrix of the gradient\nestimators. We introduce the sliding window SGD (SW-SGD) algorithm, with its\nproof of convergence, which incurs a lower MSE than the SGD algorithm on\nquadratic and convex problems. Lastly, we present some numerical results to\nshow the effectiveness of this framework and the superiority of SW-SGD\nalgorithm over the SGD algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 18:38:41 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Papo", "Yakup Ceki", ""]]}, {"id": "1910.11901", "submitter": "Xinwei Chen", "authors": "Xinwei Chen, Marlin W. Ulmer, Barrett W. Thomas", "title": "Deep Q-Learning for Same-Day Delivery with Vehicles and Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider same-day delivery with vehicles and drones.\nCustomers make delivery requests over the course of the day, and the dispatcher\ndynamically dispatches vehicles and drones to deliver the goods to customers\nbefore their delivery deadline. Vehicles can deliver multiple packages in one\nroute but travel relatively slowly due to the urban traffic. Drones travel\nfaster, but they have limited capacity and require charging or battery swaps.\nTo exploit the different strengths of the fleets, we propose a deep Q-learning\napproach. Our method learns the value of assigning a new customer to either\ndrones or vehicles as well as the option to not offer service at all. In a\nsystematic computational analysis, we show the superiority of our policy\ncompared to benchmark policies and the effectiveness of our deep Q-learning\napproach. We also show that our policy can maintain effectiveness when the\nfleet size changes moderately. Experiments on data drawn from varied\nspatial/temporal distributions demonstrate that our trained policies can cope\nwith changes in the input data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 18:46:24 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 01:08:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Xinwei", ""], ["Ulmer", "Marlin W.", ""], ["Thomas", "Barrett W.", ""]]}, {"id": "1910.11914", "submitter": "Lea Marion Trenkwalder", "authors": "Walter L. Boyajian and Jens Clausen and Lea M. Trenkwalder and Vedran\n  Dunjko and Hans J. Briegel", "title": "On the convergence of projective-simulation-based reinforcement learning\n  in Markov decision processes", "comments": "20 pages, 2 figures, v3: a few minor updates to match journal\n  version. Order of authors changed", "journal-ref": "Quantum Mach. Intell. 2, 13 (2020)", "doi": "10.1007/s42484-020-00023-9", "report-no": null, "categories": "cs.LG cs.AI quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the interest in leveraging quantum effects for enhancing\nmachine learning tasks has significantly increased. Many algorithms speeding up\nsupervised and unsupervised learning were established. The first framework in\nwhich ways to exploit quantum resources specifically for the broader context of\nreinforcement learning were found is projective simulation. Projective\nsimulation presents an agent-based reinforcement learning approach designed in\na manner which may support quantum walk-based speed-ups. Although classical\nvariants of projective simulation have been benchmarked against common\nreinforcement learning algorithms, very few formal theoretical analyses have\nbeen provided for its performance in standard learning scenarios. In this\npaper, we provide a detailed formal discussion of the properties of this model.\nSpecifically, we prove that one version of the projective simulation model,\nunderstood as a reinforcement learning approach, converges to optimal behavior\nin a large class of Markov decision processes. This proof shows that a\nphysically-inspired approach to reinforcement learning can guarantee to\nconverge.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 19:46:04 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 17:31:01 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 15:49:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Boyajian", "Walter L.", ""], ["Clausen", "Jens", ""], ["Trenkwalder", "Lea M.", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1910.11923", "submitter": "Eran Malach", "authors": "Eran Malach, Shai Shalev-Shwartz", "title": "Learning Boolean Circuits with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While on some natural distributions, neural-networks are trained efficiently\nusing gradient-based algorithms, it is known that learning them is\ncomputationally hard in the worst-case. To separate hard from easy to learn\ndistributions, we observe the property of local correlation: correlation\nbetween local patterns of the input and the target label. We focus on learning\ndeep neural-networks using a gradient-based algorithm, when the target function\nis a tree-structured Boolean circuit. We show that in this case, the existence\nof correlation between the gates of the circuit and the target label determines\nwhether the optimization succeeds or fails. Using this result, we show that\nneural-networks can learn the (log n)-parity problem for most product\ndistributions. These results hint that local correlation may play an important\nrole in separating easy/hard to learn distributions. We also obtain a novel\ndepth separation result, in which we show that a shallow network cannot express\nsome functions, while there exists an efficient gradient-based algorithm that\ncan learn the very same functions using a deep network. The negative\nexpressivity result for shallow networks is obtained by a reduction from\nresults in communication complexity, that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:26:13 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 11:51:02 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Malach", "Eran", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1910.11945", "submitter": "Guangtao Wang", "authors": "Guangtao Wang and Rex Ying and Jing Huang and Jure Leskovec", "title": "Improving Graph Attention Networks with Large Margin-based Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Attention Networks (GATs) are the state-of-the-art neural architecture\nfor representation learning with graphs. GATs learn attention functions that\nassign weights to nodes so that different nodes have different influences in\nthe feature aggregation steps. In practice, however, induced attention\nfunctions are prone to over-fitting due to the increasing number of parameters\nand the lack of direct supervision on attention weights. GATs also suffer from\nover-smoothing at the decision boundary of nodes. Here we propose a framework\nto address their weaknesses via margin-based constraints on attention during\ntraining. We first theoretically demonstrate the over-smoothing behavior of\nGATs and then develop an approach using constraint on the attention weights\naccording to the class boundary and feature aggregation pattern. Furthermore,\nto alleviate the over-fitting problem, we propose additional constraints on the\ngraph structure. Extensive experiments and ablation studies on common benchmark\ndatasets demonstrate the effectiveness of our method, which leads to\nsignificant improvements over the previous state-of-the-art graph attention\nmethods on all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 21:24:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Guangtao", ""], ["Ying", "Rex", ""], ["Huang", "Jing", ""], ["Leskovec", "Jure", ""]]}, {"id": "1910.11950", "submitter": "Andreas Munk", "authors": "Andreas Munk, Adam \\'Scibior, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Andrew\n  Stewart, Goran Fernlund, Anoush Poursartip, Frank Wood", "title": "Deep Probabilistic Surrogate Networks for Universal Simulator\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for automatically structuring and training fast,\napproximate, deep neural surrogates of existing stochastic simulators. Unlike\ntraditional approaches to surrogate modeling, our surrogates retain the\ninterpretable structure of the reference simulators. The particular way we\nachieve this allows us to replace the reference simulator with the surrogate\nwhen undertaking amortized inference in the probabilistic programming sense.\nThe fidelity and speed of our surrogates allow for not only faster \"forward\"\nstochastic simulation but also for accurate and substantially faster inference.\nWe support these claims via experiments that involve a commercial\ncomposite-materials curing simulator. Employing our surrogate modeling\ntechnique makes inference an order of magnitude faster, opening up the\npossibility of doing simulator-based, non-invasive, just-in-time parts quality\ntesting; in this case inferring safety-critical latent internal temperature\nprofiles of composite materials undergoing curing from surface temperature\nprofile measurements.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 21:55:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Munk", "Andreas", ""], ["\u015acibior", "Adam", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Stewart", "Andrew", ""], ["Fernlund", "Goran", ""], ["Poursartip", "Anoush", ""], ["Wood", "Frank", ""]]}, {"id": "1910.11956", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol\n  Hausman", "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and\n  Reinforcement Learning", "comments": "Published at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present relay policy learning, a method for imitation and reinforcement\nlearning that can solve multi-stage, long-horizon robotic tasks. This general\nand universally-applicable, two-phase approach consists of an imitation\nlearning stage that produces goal-conditioned hierarchical policies, and a\nreinforcement learning phase that finetunes these policies for task\nperformance. Our method, while not necessarily perfect at imitation learning,\nis very amenable to further improvement via environment interaction, allowing\nit to scale to challenging long-horizon tasks. We simplify the long-horizon\npolicy learning problem by using a novel data-relabeling algorithm for learning\ngoal-conditioned hierarchical policies, where the low-level only acts for a\nfixed number of steps, regardless of the goal achieved. While we rely on\ndemonstration data to bootstrap policy learning, we do not assume access to\ndemonstrations of every specific tasks that is being solved, and instead\nleverage unstructured and unsegmented demonstrations of semantically meaningful\nbehaviors that are not only less burdensome to provide, but also can greatly\nfacilitate further improvement using reinforcement learning. We demonstrate the\neffectiveness of our method on a number of multi-stage, long-horizon\nmanipulation tasks in a challenging kitchen simulation environment. Videos are\navailable at https://relay-policy-learning.github.io/\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 23:01:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Gupta", "Abhishek", ""], ["Kumar", "Vikash", ""], ["Lynch", "Corey", ""], ["Levine", "Sergey", ""], ["Hausman", "Karol", ""]]}, {"id": "1910.11958", "submitter": "Matt Whitehill", "authors": "Matt Whitehill, Shuang Ma, Daniel McDuff, Yale Song", "title": "Multi-Reference Neural TTS Stylization with Adversarial Cycle\n  Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-reference style transfer models for Text-to-Speech (TTS)\nperform sub-optimally on disjoints datasets, where one dataset contains only a\nsingle style class for one of the style dimensions. These models generally fail\nto produce style transfer for the dimension that is underrepresented in the\ndataset. In this paper, we propose an adversarial cycle consistency training\nscheme with paired and unpaired triplets to ensure the use of information from\nall style dimensions. During training, we incorporate unpaired triplets with\nrandomly selected reference audio samples and encourage the synthesized speech\nto preserve the appropriate styles using adversarial cycle consistency. We use\nthis method to transfer emotion from a dataset containing four emotions to a\ndataset with only a single emotion. This results in a 78% improvement in style\ntransfer (based on emotion classification) with minimal reduction in fidelity\nand naturalness. In subjective evaluations our method was consistently rated as\ncloser to the reference style than the baseline. Synthesized speech samples are\navailable at: https://sites.google.com/view/adv-cycle-consistent-tts\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 23:11:27 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Whitehill", "Matt", ""], ["Ma", "Shuang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "1910.11961", "submitter": "Andreas Munk", "authors": "William Harvey, Andreas Munk, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin,\n  Alexander Bergholm, Frank Wood", "title": "Attention for Inference Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to automatic amortized inference in universal\nprobabilistic programs which improves performance compared to current methods.\nOur approach is a variation of inference compilation (IC) which leverages deep\nneural networks to approximate a posterior distribution over latent variables\nin a probabilistic program. A challenge with existing IC network architectures\nis that they can fail to model long-range dependencies between latent\nvariables. To address this, we introduce an attention mechanism that attends to\nthe most salient variables previously sampled in the execution of a\nprobabilistic program. We demonstrate that the addition of attention allows the\nproposal distributions to better match the true posterior, enhancing inference\nabout latent variables in simulators.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 23:21:37 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Harvey", "William", ""], ["Munk", "Andreas", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Bergholm", "Alexander", ""], ["Wood", "Frank", ""]]}, {"id": "1910.11972", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Kernel Optimal Orthogonality Weighting: A Balancing Approach to\n  Estimating Effects of Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific questions require estimating the effects of continuous\ntreatments. Outcome modeling and weighted regression based on the generalized\npropensity score are the most commonly used methods to evaluate continuous\neffects. However, these techniques may be sensitive to model misspecification,\nextreme weights or both. In this paper, we propose Kernel Optimal Orthogonality\nWeighting (KOOW), a convex optimization-based method, for estimating the\neffects of continuous treatments. KOOW finds weights that minimize the\nworst-case penalized functional covariance between the continuous treatment and\nthe confounders. By minimizing this quantity, KOOW successfully provides\nweights that orthogonalize confounders and the continuous treatment, thus\nproviding optimal covariate balance, while controlling for extreme weights. We\nvaluate its comparative performance in a simulation study. Using data from the\nWomen's Health Initiative observational study, we apply KOOW to evaluate the\neffect of red meat consumption on blood pressure.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 01:03:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1910.11998", "submitter": "Haibin Yu", "authors": "Haibin Yu, Yizhou Chen, Zhongxiang Dai, Kian Hsiang Low, Patrick\n  Jaillet", "title": "Implicit Posterior Variational Inference for Deep Gaussian Processes", "comments": "33rd Annual Conference on Neural Information Processing Systems\n  (NeurIPS-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-layer deep Gaussian process (DGP) model is a hierarchical composition\nof GP models with a greater expressive power. Exact DGP inference is\nintractable, which has motivated the recent development of deterministic and\nstochastic approximation methods. Unfortunately, the deterministic\napproximation methods yield a biased posterior belief while the stochastic one\nis computationally costly. This paper presents an implicit posterior\nvariational inference (IPVI) framework for DGPs that can ideally recover an\nunbiased posterior belief and still preserve time efficiency. Inspired by\ngenerative adversarial networks, our IPVI framework achieves this by casting\nthe DGP inference problem as a two-player game in which a Nash equilibrium,\ninterestingly, coincides with an unbiased posterior belief. This consequently\ninspires us to devise a best-response dynamics algorithm to search for a Nash\nequilibrium (i.e., an unbiased posterior belief). Empirical evaluation shows\nthat IPVI outperforms the state-of-the-art approximation methods for DGPs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:07:01 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Yu", "Haibin", ""], ["Chen", "Yizhou", ""], ["Dai", "Zhongxiang", ""], ["Low", "Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1910.12001", "submitter": "Faisal Almutairi", "authors": "Faisal M. Almutairi, Charilaos I. Kanatsoulis, and Nicholas D.\n  Sidiropoulos", "title": "PREMA: Principled Tensor Data Recovery from Multiple Aggregated Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional data have become ubiquitous and are frequently encountered\nin situations where the information is aggregated over multiple data atoms. The\naggregation can be over time or other features, such as geographical location.\nWe often have access to multiple aggregated views of the same data, each\naggregated in one or more dimensions, especially when data are collected or\nmeasured by different agencies. For instance, item sales can be aggregated\ntemporally, and over groups of stores based on their location or affiliation.\nHowever, data mining and machine learning models benefit from detailed data for\npersonalized analysis and prediction. Thus, data disaggregation algorithms are\nbecoming increasingly important in various domains. The goal of this paper is\nto reconstruct finer-scale data from multiple coarse views, aggregated over\ndifferent (subsets of) dimensions. The proposed method, called PREMA, leverages\nlow-rank tensor factorization tools to fuse the multiple views and provide\nrecovery guarantees under certain conditions. PREMA can tackle challenging\nscenarios, such as missing or partially observed data, double aggregation, and\neven blind disaggregation (without knowledge of the aggregation patterns) using\na variant of PREMA called B-PREMA. To showcase the effectiveness of PREMA, the\npaper includes extensive experiments using real data from different domains:\nretail sales, crime counts, and weather observations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:37:30 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 18:38:31 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Almutairi", "Faisal M.", ""], ["Kanatsoulis", "Charilaos I.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1910.12004", "submitter": "Eduardo Fonseca", "authors": "Eduardo Fonseca, Frederic Font, Xavier Serra", "title": "Model-agnostic Approaches to Handling Noisy Labels When Training Sound\n  Event Classifiers", "comments": "WASPAA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Label noise is emerging as a pressing issue in sound event classification.\nThis arises as we move towards larger datasets that are difficult to annotate\nmanually, but it is even more severe if datasets are collected automatically\nfrom online repositories, where labels are inferred through automated\nheuristics applied to the audio content or metadata. While learning from noisy\nlabels has been an active area of research in computer vision, it has received\nlittle attention in sound event classification. Most recent computer vision\napproaches against label noise are relatively complex, requiring complex\nnetworks or extra data resources. In this work, we evaluate simple and\nefficient model-agnostic approaches to handling noisy labels when training\nsound event classifiers, namely label smoothing regularization, mixup and\nnoise-robust loss functions. The main advantage of these methods is that they\ncan be easily incorporated to existing deep learning pipelines without need for\nnetwork modifications or extra resources. We report results from experiments\nconducted with the FSDnoisy18k dataset. We show that these simple methods can\nbe effective in mitigating the effect of label noise, providing up to 2.5\\% of\naccuracy boost when incorporated to two different CNNs, while requiring minimal\nintervention and computational overhead.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:53:01 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Fonseca", "Eduardo", ""], ["Font", "Frederic", ""], ["Serra", "Xavier", ""]]}, {"id": "1910.12008", "submitter": "Kristy Choi", "authors": "Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon", "title": "Fair Generative Modeling via Weak Supervision", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world datasets are often biased with respect to key demographic factors\nsuch as race and gender. Due to the latent nature of the underlying factors,\ndetecting and mitigating bias is especially challenging for unsupervised\nmachine learning. We present a weakly supervised algorithm for overcoming\ndataset bias for deep generative models. Our approach requires access to an\nadditional small, unlabeled reference dataset as the supervision signal, thus\nsidestepping the need for explicit labels on the underlying bias factors. Using\nthis supplementary dataset, we detect the bias in existing datasets via a\ndensity ratio technique and learn generative models which efficiently achieve\nthe twin goals of: 1) data efficiency by using training examples from both\nbiased and reference datasets for learning; and 2) data generation close in\ndistribution to the reference dataset at test time. Empirically, we demonstrate\nthe efficacy of our approach which reduces bias w.r.t. latent factors by an\naverage of up to 34.6% over baselines for comparable image generation using\ngenerative adversarial networks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 06:40:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 11:11:27 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Choi", "Kristy", ""], ["Grover", "Aditya", ""], ["Singh", "Trisha", ""], ["Shu", "Rui", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.12016", "submitter": "Hao Kong", "authors": "Hao Kong, Canyi Lu, Zhouchen Lin", "title": "Tensor Q-Rank: New Data Dependent Definition of Tensor Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the \\textit{Tensor Nuclear Norm~(TNN)} regularization based on\nt-SVD has been widely used in various low tubal-rank tensor recovery tasks.\nHowever, these models usually require smooth change of data along the third\ndimension to ensure their low rank structures. In this paper, we propose a new\ndefinition of data dependent tensor rank named \\textit{tensor Q-rank} by a\nlearnable orthogonal matrix $\\mathbf{Q}$, and further introduce a unified data\ndependent low rank tensor recovery model. According to the low rank hypothesis,\nwe introduce two explainable selection method of $\\mathbf{Q}$, under which the\ndata tensor may have a more significant low tensor Q-rank structure than that\nof low tubal-rank structure. Specifically, maximizing the variance of singular\nvalue distribution leads to Variance Maximization Tensor Q-Nuclear\nnorm~(VMTQN), while minimizing the value of nuclear norm through manifold\noptimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN).\nMoreover, we apply these two models to the low rank tensor completion problem,\nand then give an effective algorithm and briefly analyze why our method works\nbetter than TNN based methods in the case of complex data with low sampling\nrate. Finally, experimental results on real-world datasets demonstrate the\nsuperiority of our proposed model in the tensor completion problem with respect\nto other tensor rank regularization models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 07:53:21 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 18:39:21 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 05:30:41 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 08:50:52 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kong", "Hao", ""], ["Lu", "Canyi", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1910.12024", "submitter": "Saiprasad Ravishankar", "authors": "Zhipeng Li, Siqi Ye, Yong Long, and Saiprasad Ravishankar", "title": "SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT\n  Image Reconstruction", "comments": "Accepted to International Conference on Computer Vision (ICCV) -\n  Learning for Computational Imaging (LCI) Workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed growing interest in machine learning-based models\nand techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can\ntypically be categorized into supervised learning methods and unsupervised or\nmodel-based learning methods. Supervised learning methods have recently shown\nsuccess in image restoration tasks. However, they often rely on large training\nsets. Model-based learning methods such as dictionary or transform learning do\nnot require large or paired training sets and often have good generalization\nproperties, since they learn general properties of CT image sets. Recent works\nhave shown the promising reconstruction performance of methods such as\nPWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches\ninto a learned union of transforms. In this paper, we propose a new\nSupervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image\nreconstruction that combines the benefits of supervised learning methods and\n(unsupervised) transform learning-based methods such as PWLS-ULTRA that involve\nhighly image-adaptive clustering. The SUPER model consists of several layers,\neach of which includes a deep network learned in a supervised manner and an\nunsupervised iterative method that involves image-adaptive components. The\nSUPER reconstruction algorithms are learned in a greedy manner from training\ndata. The proposed SUPER learning methods dramatically outperform both the\nconstituent supervised learning-based networks and iterative algorithms for\nLDCT, and use much fewer iterations in the iterative reconstruction modules.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:04:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Zhipeng", ""], ["Ye", "Siqi", ""], ["Long", "Yong", ""], ["Ravishankar", "Saiprasad", ""]]}, {"id": "1910.12027", "submitter": "Han Zhang", "authors": "Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee", "title": "Consistency Regularization for Generative Adversarial Networks", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are known to be difficult to train,\ndespite considerable research effort. Several regularization techniques for\nstabilizing training have been proposed, but they introduce non-trivial\ncomputational overheads and interact poorly with existing techniques like\nspectral normalization. In this work, we propose a simple, effective training\nstabilizer based on the notion of consistency regularization---a popular\ntechnique in the semi-supervised learning literature. In particular, we augment\ndata passing into the GAN discriminator and penalize the sensitivity of the\ndiscriminator to these augmentations. We conduct a series of experiments to\ndemonstrate that consistency regularization works effectively with spectral\nnormalization and various GAN architectures, loss functions and optimizer\nsettings. Our method achieves the best FID scores for unconditional image\ngeneration compared to other regularization methods on CIFAR-10 and CelebA.\nMoreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art\nFID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from\n8.73 to 6.66 on ImageNet-2012.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:06:03 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 21:43:54 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhang", "Han", ""], ["Zhang", "Zizhao", ""], ["Odena", "Augustus", ""], ["Lee", "Honglak", ""]]}, {"id": "1910.12043", "submitter": "Shogo Iwazaki", "authors": "Shogo Iwazaki, Yu Inatsu, Ichiro Takeuchi", "title": "Bayesian Experimental Design for Finding Reliable Level Set under Input\n  Uncertainty", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the manufacturing industry, it is often necessary to repeat expensive\noperational testing of machine in order to identify the range of input\nconditions under which the machine operates properly. Since it is often\ndifficult to accurately control the input conditions during the actual usage of\nthe machine, there is a need to guarantee the performance of the machine after\nproperly incorporating the possible variation in input conditions. In this\npaper, we formulate this practical manufacturing scenario as an Input Uncertain\nReliable Level Set Estimation (IU-rLSE) problem, and provide an efficient\nalgorithm for solving it. The goal of IU-rLSE is to identify the input range in\nwhich the outputs smaller/greater than a desired threshold can be obtained with\nhigh probability when the input uncertainty is properly taken into\nconsideration. We propose an active learning method to solve the IU-rLSE\nproblem efficiently, theoretically analyze its accuracy and convergence, and\nillustrate its empirical performance through numerical experiments on\nartificial and real data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 10:30:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Iwazaki", "Shogo", ""], ["Inatsu", "Yu", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1910.12061", "submitter": "Ramya Hebbalaguppe", "authors": "Srinidhi Hegde, Ranjitha Prasad, Ramya Hebbalaguppe, Vishwajith Kumar", "title": "Variational Student: Learning Compact and Sparser Networks in Knowledge\n  Distillation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The holy grail in deep neural network research is porting the memory- and\ncomputation-intensive network models on embedded platforms with a minimal\ncompromise in model accuracy. To this end, we propose a novel approach, termed\nas Variational Student, where we reap the benefits of compressibility of the\nknowledge distillation (KD) framework, and sparsity inducing abilities of\nvariational inference (VI) techniques. Essentially, we build a sparse student\nnetwork, whose sparsity is induced by the variational parameters found via\noptimizing a loss function based on VI, leveraging the knowledge learnt by an\naccurate but complex pre-trained teacher network. Further, for sparsity\nenhancement, we also employ a Block Sparse Regularizer on a concatenated tensor\nof teacher and student network weights. We demonstrate that the marriage of KD\nand the VI techniques inherits compression properties from the KD framework,\nand enhances levels of sparsity from the VI approach, with minimal compromise\nin the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN)\nand illustrate a memory footprint reduction of 64x and 213x on these MLP and\nCNN variants, respectively, without a need to retrain the teacher network.\nFurthermore, in the low data regime, we observed that our method outperforms\nstate-of-the-art Bayesian techniques in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 13:18:54 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hegde", "Srinidhi", ""], ["Prasad", "Ranjitha", ""], ["Hebbalaguppe", "Ramya", ""], ["Kumar", "Vishwajith", ""]]}, {"id": "1910.12084", "submitter": "Alessandro Lameiras Koerich", "authors": "Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich", "title": "Detection of Adversarial Attacks and Characterization of Adversarial\n  Subspace", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have always been a serious threat for any data-driven\nmodel. In this paper, we explore subspaces of adversarial examples in unitary\nvector domain, and we propose a novel detector for defending our models trained\nfor environmental sound classification. We measure chordal distance between\nlegitimate and malicious representation of sounds in unitary space of\ngeneralized Schur decomposition and show that their manifolds lie far from each\nother. Our front-end detector is a regularized logistic regression which\ndiscriminates eigenvalues of legitimate and adversarial spectrograms. The\nexperimental results on three benchmarking datasets of environmental sounds\nrepresented by spectrograms reveal high detection rate of the proposed detector\nfor eight types of adversarial attacks and outperforms other detection\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:14:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Esmaeilpour", "Mohammad", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "1910.12086", "submitter": "Miguel A. Roman", "authors": "Miguel A. Rom\\'an, Antonio Pertusa, Jorge Calvo-Zaragoza", "title": "A holistic approach to polyphonic music transcription with neural\n  networks", "comments": "Source code available at https://github.com/mangelroman/audio2score", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework based on neural networks to extract music scores\ndirectly from polyphonic audio in an end-to-end fashion. Most previous\nAutomatic Music Transcription (AMT) methods seek a piano-roll representation of\nthe pitches, that can be further transformed into a score by incorporating\ntempo estimation, beat tracking, key estimation or rhythm quantization. Unlike\nthese methods, our approach generates music notation directly from the input\naudio in a single stage. For this, we use a Convolutional Recurrent Neural\nNetwork (CRNN) with Connectionist Temporal Classification (CTC) loss function\nwhich does not require annotated alignments of audio frames with the score\nrhythmic information. We trained our model using as input Haydn, Mozart, and\nBeethoven string quartets and Bach chorales synthesized with different tempos\nand expressive performances. The output is a textual representation of\nfour-voice music scores based on **kern format. Although the proposed approach\nis evaluated in a simplified scenario, results show that this model can learn\nto transcribe scores directly from audio signals, opening a promising avenue\ntowards complete AMT.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:19:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Rom\u00e1n", "Miguel A.", ""], ["Pertusa", "Antonio", ""], ["Calvo-Zaragoza", "Jorge", ""]]}, {"id": "1910.12091", "submitter": "Evgeny Burnaev", "authors": "Sergei Ivanov and Sergei Sviridov and Evgeny Burnaev", "title": "Understanding Isomorphism Bias in Graph Data Sets", "comments": "19 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a rapid increase in classification methods on\ngraph structured data. Both in graph kernels and graph neural networks, one of\nthe implicit assumptions of successful state-of-the-art models was that\nincorporating graph isomorphism features into the architecture leads to better\nempirical performance. However, as we discover in this work, commonly used data\nsets for graph classification have repeating instances which cause the problem\nof isomorphism bias, i.e. artificially increasing the accuracy of the models by\nmemorizing target information from the training set. This prevents fair\ncompetition of the algorithms and raises a question of the validity of the\nobtained results. We analyze 54 data sets, previously extensively used for\ngraph-related tasks, on the existence of isomorphism bias, give a set of\nrecommendations to machine learning practitioners to properly set up their\nmodels, and open source new data sets for the future experiments.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:52:42 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:14:17 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ivanov", "Sergei", ""], ["Sviridov", "Sergei", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1910.12132", "submitter": "Florence Regol", "authors": "Soumyasundar Pal, Florence Regol, Mark Coates", "title": "Bayesian Graph Convolutional Neural Networks Using Non-Parametric Graph\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks (GCNN) have been successfully applied to\nmany different graph based learning tasks including node and graph\nclassification, matrix completion, and learning of node embeddings. Despite\ntheir impressive performance, the techniques have a limited capability to\nincorporate the uncertainty in the underlined graph structure. In order to\naddress this issue, a Bayesian GCNN (BGCN) framework was recently proposed. In\nthis framework, the observed graph is considered to be a random realization\nfrom a parametric random graph model and the joint Bayesian inference of the\ngraph and GCNN weights is performed. In this paper, we propose a non-parametric\ngenerative model for graphs and incorporate it within the BGCN framework. In\naddition to the observed graph, our approach effectively uses the node features\nand training labels in the posterior inference of graphs and attains superior\nor comparable performance in benchmark node classification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 20:27:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Pal", "Soumyasundar", ""], ["Regol", "Florence", ""], ["Coates", "Mark", ""]]}, {"id": "1910.12134", "submitter": "Shengyi Huang", "authors": "Shengyi Huang, Santiago Onta\\~n\\'on", "title": "Comparing Observation and Action Representations for Deep Reinforcement\n  Learning in $\\mu$RTS", "comments": "Presented in the AIIDE 2019 Workshop on Artificial Intelligence for\n  Strategy Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a preliminary study comparing different observation and\naction space representations for Deep Reinforcement Learning (DRL) in the\ncontext of Real-time Strategy (RTS) games. Specifically, we compare two\nrepresentations: (1) a global representation where the observation represents\nthe whole game state, and the RL agent needs to choose which unit to issue\nactions to, and which actions to execute; and (2) a local representation where\nthe observation is represented from the point of view of an individual unit,\nand the RL agent picks actions for each unit independently. We evaluate these\nrepresentations in $\\mu$RTS showing that the local representation seems to\noutperform the global representation when training agents with the task of\nharvesting resources.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 20:30:36 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 03:03:22 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 22:43:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Huang", "Shengyi", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "1910.12156", "submitter": "Ming Yu", "authors": "Ming Yu, Zhuoran Yang, Mladen Kolar, Zhaoran Wang", "title": "Convergent Policy Optimization for Safe Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the safe reinforcement learning problem with nonlinear function\napproximation, where policy optimization is formulated as a constrained\noptimization problem with both the objective and the constraint being nonconvex\nfunctions. For such a problem, we construct a sequence of surrogate convex\nconstrained optimization problems by replacing the nonconvex functions locally\nwith convex quadratic functions obtained from policy gradient estimators. We\nprove that the solutions to these surrogate problems converge to a stationary\npoint of the original nonconvex problem. Furthermore, to extend our theoretical\nresults, we apply our algorithm to examples of optimal control and multi-agent\nreinforcement learning with safety constraints.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 23:40:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Yu", "Ming", ""], ["Yang", "Zhuoran", ""], ["Kolar", "Mladen", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1910.12159", "submitter": "Mahdieh Shabanian", "authors": "Mahdieh Shabanian, Eugene C. Eckstein, Hao Chen, John P. DeVincenzo", "title": "Classification of Neurodevelopmental Age in Normal Infants Using 3D-CNN\n  based on Brain MRI", "comments": "6 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain development is rapid during infancy and early childhood. Many\ndisease processes impair this development. Therefore, brain developmental age\nestimation (BDAE) is essential for all diseases affecting cognitive\ndevelopment. Brain magnetic resonance imaging (MRI) of infants shows brain\ngrowth and morphologic patterns during childhood. Therefore, we can estimate\nthe developmental age from brain images. However, MRI analysis is\ntime-consuming because each scan contains millions of data points (voxels). We\ninvestigated the three-dimensional convolutional neural network (3D CNN), a\ndeep learning algorithm, to rapidly classify neurodevelopmental age with high\naccuracy based on MRIs. MRIs from normal newborns were obtained from the\nNational Institute of Mental Health (NIMH) Data Archive. Age categories of\npediatric MRIs were 3 wks + 1 wk, 1 yr + 2 wks, and 3 yrs + 4 wks. We trained a\nBDAE method using T1, T2, and proton density (PD) images from MRI scans of 112\nindividuals using 3D CNN. Compared with the known age, our method has a\nsensitivity of 99% and specificity of 98.3%. Moreover, our 3D CNN model has\nbetter performance in neurodevelopmental age estimation than does 2D CNN.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 00:08:55 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 01:26:19 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Shabanian", "Mahdieh", ""], ["Eckstein", "Eugene C.", ""], ["Chen", "Hao", ""], ["DeVincenzo", "John P.", ""]]}, {"id": "1910.12162", "submitter": "Jong Chul Ye", "authors": "Mathews Jacob, Merry P. Mani, and Jong Chul Ye", "title": "Structured Low-Rank Algorithms: Theory, MR Applications, and Links to\n  Machine Learning", "comments": "Accepted for IEEE Signal Processing Magazine", "journal-ref": null, "doi": "10.1109/MSP.2019.2950432", "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey, we provide a detailed review of recent advances in the\nrecovery of continuous domain multidimensional signals from their few\nnon-uniform (multichannel) measurements using structured low-rank matrix\ncompletion formulation. This framework is centered on the fundamental duality\nbetween the compactness (e.g., sparsity) of the continuous signal and the rank\nof a structured matrix, whose entries are functions of the signal. This\nproperty enables the reformulation of the signal recovery as a low-rank\nstructured matrix completion, which comes with performance guarantees. We will\nalso review fast algorithms that are comparable in complexity to current\ncompressed sensing methods, which enables the application of the framework to\nlarge-scale magnetic resonance (MR) recovery problems. The remarkable\nflexibility of the formulation can be used to exploit signal properties that\nare difficult to capture by current sparse and low-rank optimization\nstrategies. We demonstrate the utility of the framework in a wide range of MR\nimaging (MRI) applications, including highly accelerated imaging,\ncalibration-free acquisition, MR artifact correction, and ungated dynamic MRI.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 01:46:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jacob", "Mathews", ""], ["Mani", "Merry P.", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1910.12163", "submitter": "Xupeng Shi", "authors": "Xupeng Shi, A. Adam Ding", "title": "Understanding and Quantifying Adversarial Examples Existence in Linear\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-art deep neural networks (DNN) are vulnerable to attacks by\nadversarial examples: a carefully designed small perturbation to the input,\nthat is imperceptible to human, can mislead DNN. To understand the root cause\nof adversarial examples, we quantify the probability of adversarial example\nexistence for linear classifiers. Previous mathematical definition of\nadversarial examples only involves the overall perturbation amount, and we\npropose a more practical relevant definition of strong adversarial examples\nthat separately limits the perturbation along the signal direction also. We\nshow that linear classifiers can be made robust to strong adversarial examples\nattack in cases where no adversarial robust linear classifiers exist under the\nprevious definition. The quantitative formulas are confirmed by numerical\nexperiments using a linear support vector machine (SVM) classifier. The results\nsuggest that designing general strong-adversarial-robust learning systems is\nfeasible but only through incorporating human knowledge of the underlying\nclassification problem.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 01:57:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shi", "Xupeng", ""], ["Ding", "A. Adam", ""]]}, {"id": "1910.12166", "submitter": "Kaiyi Ji", "authors": "Kaiyi Ji, Zhe Wang, Yi Zhou and Yingbin Liang", "title": "Improved Zeroth-Order Variance Reduced Algorithms and Analysis for\n  Nonconvex Optimization", "comments": "Published in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two types of zeroth-order stochastic algorithms have recently been designed\nfor nonconvex optimization respectively based on the first-order techniques\nSVRG and SARAH/SPIDER. This paper addresses several important issues that are\nstill open in these methods. First, all existing SVRG-type zeroth-order\nalgorithms suffer from worse function query complexities than either\nzeroth-order gradient descent (ZO-GD) or stochastic gradient descent (ZO-SGD).\nIn this paper, we propose a new algorithm ZO-SVRG-Coord-Rand and develop a new\nanalysis for an existing ZO-SVRG-Coord algorithm proposed in Liu et al. 2018b,\nand show that both ZO-SVRG-Coord-Rand and ZO-SVRG-Coord (under our new\nanalysis) outperform other exiting SVRG-type zeroth-order methods as well as\nZO-GD and ZO-SGD. Second, the existing SPIDER-type algorithm SPIDER-SZO (Fang\net al. 2018) has superior theoretical performance, but suffers from the\ngeneration of a large number of Gaussian random variables as well as a\n$\\sqrt{\\epsilon}$-level stepsize in practice. In this paper, we develop a new\nalgorithm ZO-SPIDER-Coord, which is free from Gaussian variable generation and\nallows a large constant stepsize while maintaining the same convergence rate\nand query complexity, and we further show that ZO-SPIDER-Coord automatically\nachieves a linear convergence rate as the iterate enters into a local PL region\nwithout restart and algorithmic modification.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 02:48:03 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ji", "Kaiyi", ""], ["Wang", "Zhe", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""]]}, {"id": "1910.12179", "submitter": "Che Wang", "authors": "Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, Keith Ross", "title": "BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement\n  Learning", "comments": "27 pages(15 pages for appendix); Published in 34th Conference on\n  Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a surge in research in batch Deep Reinforcement\nLearning (DRL), which aims for learning a high-performing policy from a given\ndataset without additional interactions with the environment. We propose a new\nalgorithm, Best-Action Imitation Learning (BAIL), which strives for both\nsimplicity and performance. BAIL learns a V function, uses the V function to\nselect actions it believes to be high-performing, and then uses those actions\nto train a policy network using imitation learning. For the MuJoCo benchmark,\nwe provide a comprehensive experimental study of BAIL, comparing its\nperformance to four other batch Q-learning and imitation-learning schemes for a\nlarge variety of batch datasets. Our experiments show that BAIL's performance\nis much higher than the other schemes, and is also computationally much faster\nthan the batch Q-learning schemes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 04:43:19 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 11:26:19 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 05:28:24 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 07:11:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Xinyue", ""], ["Zhou", "Zijian", ""], ["Wang", "Zheng", ""], ["Wang", "Che", ""], ["Wu", "Yanqiu", ""], ["Ross", "Keith", ""]]}, {"id": "1910.12180", "submitter": "Saeid Hosseini", "authors": "Saeed Najafipour, Saeid Hosseini, Wen Hua, Mohammad Reza Kangavari,\n  Xiaofang Zhou", "title": "SoulMate: Short-text author linking through Multi-aspect\n  temporal-textual embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking authors of short-text contents has important usages in many\napplications, including Named Entity Recognition (NER) and human community\ndetection. However, certain challenges lie ahead. Firstly, the input short-text\ncontents are noisy, ambiguous, and do not follow the grammatical rules.\nSecondly, traditional text mining methods fail to effectively extract concepts\nthrough words and phrases. Thirdly, the textual contents are temporally skewed,\nwhich can affect the semantic understanding by multiple time facets. Finally,\nusing the complementary knowledge-bases makes the results biased to the content\nof the external database and deviates the understanding and interpretation away\nfrom the real nature of the given short text corpus. To overcome these\nchallenges, we devise a neural network-based temporal-textual framework that\ngenerates the tightly connected author subgraphs from microblog short-text\ncontents. Our approach, on the one hand, computes the relevance score (edge\nweight) between the authors through considering a portmanteau of contents and\nconcepts, and on the other hand, employs a stack-wise graph cutting algorithm\nto extract the communities of the related authors. Experimental results show\nthat compared to other knowledge-centered competitors, our multi-aspect vector\nspace model can achieve a higher performance in linking short-text authors.\nAdditionally, given the author linking task, the more comprehensive the dataset\nis, the higher the significance of the extracted concepts will be.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 04:53:35 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Najafipour", "Saeed", ""], ["Hosseini", "Saeid", ""], ["Hua", "Wen", ""], ["Kangavari", "Mohammad Reza", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1910.12191", "submitter": "Sabri Boughorbel", "authors": "Sabri Boughorbel, Fethi Jarray, Neethu Venugopal, Shabir Moosa,\n  Haithum Elhadi, Michel Makhlouf", "title": "Federated Uncertainty-Aware Learning for Distributed Hospital EHR Data", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that applying Machine Learning to Electronic Health\nRecords (EHR) can strongly accelerate precision medicine. This requires\ndeveloping models based on diverse EHR sources. Federated Learning (FL) has\nenabled predictive modeling using distributed training which lifted the need of\nsharing data and compromising privacy. Since models are distributed in FL, it\nis attractive to devise ensembles of Deep Neural Networks that also assess\nmodel uncertainty. We propose a new FL model called Federated Uncertainty-Aware\nLearning Algorithm (FUALA) that improves on Federated Averaging (FedAvg) in the\ncontext of EHR. FUALA embeds uncertainty information in two ways: It reduces\nthe contribution of models with high uncertainty in the aggregated model. It\nalso introduces model ensembling at prediction time by keeping the last layers\nof each hospital from the final round. In FUALA, the Federator (central node)\nsends at each round the average model to all hospitals as well as a randomly\nassigned hospital model update to estimate its generalization on that hospital\nown data. Each hospital sends back its model update as well a generalization\nestimation of the assigned model. At prediction time, the model outputs C\npredictions for each sample where C is the number of hospital models. The\nexperimental analysis conducted on a cohort of 87K deliveries for the task of\npreterm-birth prediction showed that the proposed approach outperforms FedAvg\nwhen evaluated on out-of-distribution data. We illustrated how uncertainty\ncould be measured using the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 06:33:34 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Boughorbel", "Sabri", ""], ["Jarray", "Fethi", ""], ["Venugopal", "Neethu", ""], ["Moosa", "Shabir", ""], ["Elhadi", "Haithum", ""], ["Makhlouf", "Michel", ""]]}, {"id": "1910.12194", "submitter": "Rita Fioresi", "authors": "R. Fioresi, P. Chaudhari, S. Soatto", "title": "A geometric interpretation of stochastic gradient descent using\n  diffusion metrics", "comments": null, "journal-ref": null, "doi": "10.3390/e22010101", "report-no": null, "categories": "cs.LG gr-qc math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a key ingredient in the training of deep\nneural networks and yet its geometrical significance appears elusive. We study\na deterministic model in which the trajectories of our dynamical systems are\ndescribed via geodesics of a family of metrics arising from the diffusion\nmatrix. These metrics encode information about the highly non-isotropic\ngradient noise in SGD. We establish a parallel with General Relativity models,\nwhere the role of the electromagnetic field is played by the gradient of the\nloss function. We compute an example of a two layer network.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 06:47:16 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Fioresi", "R.", ""], ["Chaudhari", "P.", ""], ["Soatto", "S.", ""]]}, {"id": "1910.12203", "submitter": "Vaibhav Vaibhav", "authors": "Vaibhav Vaibhav, Raghuram Mandyam Annasamy, Eduard Hovy", "title": "Do Sentence Interactions Matter? Leveraging Sentence Level\n  Representations for Fake News Classification", "comments": "Accepted at TextGraphs - EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rising growth of fake news and misleading information through online\nmedia outlets demands an automatic method for detecting such news articles. Of\nthe few limited works which differentiate between trusted vs other types of\nnews article (satire, propaganda, hoax), none of them model sentence\ninteractions within a document. We observe an interesting pattern in the way\nsentences interact with each other across different kind of news articles. To\ncapture this kind of information for long news articles, we propose a graph\nneural network-based model which does away with the need of feature engineering\nfor fine grained fake news classification. Through experiments, we show that\nour proposed method beats strong neural baselines and achieves state-of-the-art\naccuracy on existing datasets. Moreover, we establish the generalizability of\nour model by evaluating its performance in out-of-domain scenarios. Code is\navailable at https://github.com/MysteryVaibhav/fake_news_semantics\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 07:44:33 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Vaibhav", "Vaibhav", ""], ["Annasamy", "Raghuram Mandyam", ""], ["Hovy", "Eduard", ""]]}, {"id": "1910.12204", "submitter": "Yotam Gigi", "authors": "Yotam Gigi, Ami Wiesel, Sella Nevo, Gal Elidan, Avinatan Hassidim,\n  Yossi Matias", "title": "Spectral Algorithm for Low-rank Multitask Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning, i.e. taking advantage of the relatedness of individual\ntasks in order to improve performance on all of them, is a core challenge in\nthe field of machine learning. We focus on matrix regression tasks where the\nrank of the weight matrix is constrained to reduce sample complexity. We\nintroduce the common mechanism regression (CMR) model which assumes a shared\nleft low-rank component across all tasks, but allows an individual per-task\nright low-rank component. This dramatically reduces the number of samples\nneeded for accurate estimation. The problem of jointly recovering the common\nand the local components has a non-convex bi-linear structure. We overcome this\nhurdle and provide a provably beneficial non-iterative spectral algorithm.\nAppealingly, the solution has favorable behavior as a function of the number of\nrelated tasks and the small number of samples available for each one. We\ndemonstrate the efficacy of our approach for the challenging task of remote\nriver discharge estimation across multiple river sites, where data for each\ntask is naturally scarce. In this scenario sharing a low-rank component between\nthe tasks translates to a shared spectral reflection of the water, which is a\ntrue underlying physical model. We also show the benefit of the approach on the\nmarkedly different setting of image classification where the common component\ncan be interpreted as the shared convolution filters.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 08:00:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Gigi", "Yotam", ""], ["Wiesel", "Ami", ""], ["Nevo", "Sella", ""], ["Elidan", "Gal", ""], ["Hassidim", "Avinatan", ""], ["Matias", "Yossi", ""]]}, {"id": "1910.12227", "submitter": "Ali Shahin Shamsabadi", "authors": "Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro", "title": "EdgeFool: An Adversarial Image Enhancement Filter", "comments": null, "journal-ref": "Proceedings of the 45th IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP)2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are intentionally perturbed images that mislead\nclassifiers. These images can, however, be easily detected using denoising\nalgorithms, when high-frequency spatial perturbations are used, or can be\nnoticed by humans, when perturbations are large. In this paper, we propose\nEdgeFool, an adversarial image enhancement filter that learns structure-aware\nadversarial perturbations. EdgeFool generates adversarial images with\nperturbations that enhance image details via training a fully convolutional\nneural network end-to-end with a multi-task loss function. This loss function\naccounts for both image detail enhancement and class misleading objectives. We\nevaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using\ntwo datasets (ImageNet and Private-Places365) and compare it with six\nadversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv,\nNon-targeted and Private Fast Gradient Sign Methods). Code is available at\nhttps://github.com/smartcameras/EdgeFool.git.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 10:16:26 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:08:19 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Shamsabadi", "Ali Shahin", ""], ["Oh", "Changjae", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1910.12232", "submitter": "Guy Jacob", "authors": "Neta Zmora, Guy Jacob, Lev Zlotnik, Bar Elharar, Gal Novik", "title": "Neural Network Distiller: A Python Package For DNN Compression Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents the philosophy, design and feature-set of Neural Network\nDistiller, an open-source Python package for DNN compression research.\nDistiller is a library of DNN compression algorithms implementations, with\ntools, tutorials and sample applications for various learning tasks. Its target\nusers are both engineers and researchers, and the rich content is complemented\nby a design-for-extensibility to facilitate new research. Distiller is\nopen-source and is available on Github at\nhttps://github.com/NervanaSystems/distiller.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 10:42:15 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zmora", "Neta", ""], ["Jacob", "Guy", ""], ["Zlotnik", "Lev", ""], ["Elharar", "Bar", ""], ["Novik", "Gal", ""]]}, {"id": "1910.12240", "submitter": "Yue Wang", "authors": "Yue Wang and Justin M. Solomon", "title": "PRNet: Self-Supervised Learning for Partial-to-Partial Registration", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a simple, flexible, and general framework titled Partial\nRegistration Network (PRNet), for partial-to-partial point cloud registration.\nInspired by recently-proposed learning-based methods for registration, we use\ndeep networks to tackle non-convexity of the alignment and partial\ncorrespondence problems. While previous learning-based methods assume the\nentire shape is visible, PRNet is suitable for partial-to-partial registration,\noutperforming PointNetLK, DCP, and non-learning methods on synthetic data.\nPRNet is self-supervised, jointly learning an appropriate geometric\nrepresentation, a keypoint detector that finds points in common between partial\nviews, and keypoint-to-keypoint correspondences. We show PRNet predicts\nkeypoints and correspondences consistently across views and objects.\nFurthermore, the learned representation is transferable to classification.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 11:26:16 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:58:14 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wang", "Yue", ""], ["Solomon", "Justin M.", ""]]}, {"id": "1910.12241", "submitter": "Danhao Zhu", "authors": "Danhao Zhu, Xin-yu Dai, Jiajun Chen", "title": "Pre-train and Learn: Preserve Global Information for Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have shown great power in learning on attributed\ngraphs. However, it is still a challenge for GNNs to utilize information\nfaraway from the source node. Moreover, general GNNs require graph attributes\nas input, so they cannot be appled to plain graphs. In the paper, we propose\nnew models named G-GNNs (Global information for GNNs) to address the above\nlimitations. First, the global structure and attribute features for each node\nare obtained via unsupervised pre-training, which preserve the global\ninformation associated to the node. Then, using the global features and the raw\nnetwork attributes, we propose a parallel framework of GNNs to learn different\naspects from these features. The proposed learning methods can be applied to\nboth plain graphs and attributed graphs. Extensive experiments have shown that\nG-GNNs can outperform other state-of-the-art models on three standard\nevaluation graphs. Specially, our methods establish new benchmark records on\nCora (84.31\\%) and Pubmed (80.95\\%) when learning on attributed graphs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 11:27:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhu", "Danhao", ""], ["Dai", "Xin-yu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1910.12249", "submitter": "Jianbang Ding", "authors": "Jianbang Ding, Xuancheng Ren, Ruixuan Luo, Xu Sun", "title": "An Adaptive and Momental Bound Method for Stochastic Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires intricate initialization and careful\nselection of learning rates. The emergence of stochastic gradient optimization\nmethods that use adaptive learning rates based on squared past gradients, e.g.,\nAdaGrad, AdaDelta, and Adam, eases the job slightly. However, such methods have\nalso been proven problematic in recent studies with their own pitfalls\nincluding non-convergence issues and so on. Alternative variants have been\nproposed for enhancement, such as AMSGrad, AdaShift and AdaBound. In this work,\nwe identify a new problem of adaptive learning rate methods that exhibits at\nthe beginning of learning where Adam produces extremely large learning rates\nthat inhibit the start of learning. We propose the Adaptive and Momental Bound\n(AdaMod) method to restrict the adaptive learning rates with adaptive and\nmomental upper bounds. The dynamic learning rate bounds are based on the\nexponential moving averages of the adaptive learning rates themselves, which\nsmooth out unexpected large learning rates and stabilize the training of deep\nneural networks. Our experiments verify that AdaMod eliminates the extremely\nlarge learning rates throughout the training and brings significant\nimprovements especially on complex networks such as DenseNet and Transformer,\ncompared to Adam. Our implementation is available at:\nhttps://github.com/lancopku/AdaMod\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 12:22:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ding", "Jianbang", ""], ["Ren", "Xuancheng", ""], ["Luo", "Ruixuan", ""], ["Sun", "Xu", ""]]}, {"id": "1910.12252", "submitter": "Jen Ning Lim", "authors": "Jen Ning Lim, Makoto Yamada, Bernhard Sch\\\"olkopf, Wittawat Jitkrittum", "title": "Kernel Stein Tests for Multiple Model Comparison", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of non-parametric multiple model comparison: given $l$\ncandidate models, decide whether each candidate is as good as the best one(s)\nor worse than it. We propose two statistical tests, each controlling a\ndifferent notion of decision errors. The first test, building on the post\nselection inference framework, provably controls the number of best models that\nare wrongly declared worse (false positive rate). The second test is based on\nmultiple correction, and controls the proportion of the models declared worse\nbut are in fact as good as the best (false discovery rate). We prove that under\nappropriate conditions the first test can yield a higher true positive rate\nthan the second. Experimental results on toy and real (CelebA, Chicago Crime\ndata) problems show that the two tests have high true positive rates with\nwell-controlled error rates. By contrast, the naive approach of choosing the\nmodel with the lowest score without correction leads to more false positives.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 12:45:03 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lim", "Jen Ning", ""], ["Yamada", "Makoto", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Jitkrittum", "Wittawat", ""]]}, {"id": "1910.12258", "submitter": "Rodrigo de Lamare", "authors": "Q. Jiang, S. Li, Z. Zhu, H. Bai, X. He and R. C. de Lamare", "title": "Compressed Sensing with Probability-based Prior Information", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the design of a sensing matrix along with a sparse\nrecovery algorithm by utilizing the probability-based prior information for\ncompressed sensing system. With the knowledge of the probability for each atom\nof the dictionary being used, a diagonal weighted matrix is obtained and then\nthe sensing matrix is designed by minimizing a weighted function such that the\nGram of the equivalent dictionary is as close to the Gram of dictionary as\npossible. An analytical solution for the corresponding sensing matrix is\nderived which leads to low computational complexity. We also exploit this prior\ninformation through the sparse recovery stage and propose a probability-driven\northogonal matching pursuit algorithm that improves the accuracy of the\nrecovery. Simulations for synthetic data and application scenarios of\nsurveillance video are carried out to compare the performance of the proposed\nmethods with some existing algorithms. The results reveal that the proposed CS\nsystem outperforms existing CS systems.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:12:34 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jiang", "Q.", ""], ["Li", "S.", ""], ["Zhu", "Z.", ""], ["Bai", "H.", ""], ["He", "X.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "1910.12259", "submitter": "Peter M. Roth", "authors": "Mina Basirat, Peter M. Roth", "title": "L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained\n  Visual Categorization", "comments": "Accepted: Winter Conference on Applications of Computer Vision (WACV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks paved the way for significant improvements in image\nvisual categorization during the last years. However, even though the tasks are\nhighly varying, differing in complexity and difficulty, existing solutions\nmostly build on the same architectural decisions. This also applies to the\nselection of activation functions (AFs), where most approaches build on\nRectified Linear Units (ReLUs). In this paper, however, we show that the choice\nof a proper AF has a significant impact on the classification accuracy, in\nparticular, if fine, subtle details are of relevance. Therefore, we propose to\nmodel the degree of absence and the presence of features via the AF by using\npiece-wise linear functions, which we refer to as L*ReLU. In this way, we can\nensure the required properties, while still inheriting the benefits in terms of\ncomputational efficiency from ReLUs. We demonstrate our approach for the task\nof Fine-grained Visual Categorization (FGVC), running experiments on seven\ndifferent benchmark datasets. The results do not only demonstrate superior\nresults but also that for different tasks, having different characteristics,\ndifferent AFs are selected.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:15:32 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Basirat", "Mina", ""], ["Roth", "Peter M.", ""]]}, {"id": "1910.12263", "submitter": "Tomasz Ku\\'smierczyk", "authors": "Eliezer de Souza da Silva, Tomasz Ku\\'smierczyk, Marcelo Hartmann,\n  Arto Klami", "title": "Prior specification via prior predictive matching: Poisson matrix\n  factorization and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter optimization for machine learning models is typically carried\nout by some sort of cross-validation procedure or global optimization, both of\nwhich require running the learning algorithm numerous times. We show that for\nBayesian hierarchical models there is an appealing alternative that allows\nselecting good hyperparameters without learning the model parameters during the\nprocess at all, facilitated by the prior predictive distribution that\nmarginalizes out the model parameters. We propose an approach that matches\nsuitable statistics of the prior predictive distribution with ones provided by\nan expert and apply the general concept for matrix factorization models. For\nsome Poisson matrix factorization models we can analytically obtain exact\nhyperparameters, including the number of factors, and for more complex models\nwe propose a model-independent optimization procedure.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:56:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["da Silva", "Eliezer de Souza", ""], ["Ku\u015bmierczyk", "Tomasz", ""], ["Hartmann", "Marcelo", ""], ["Klami", "Arto", ""]]}, {"id": "1910.12281", "submitter": "Vladimir Puzyrev", "authors": "Vladimir Puzyrev", "title": "Deep convolutional autoencoder for cryptocurrency market analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study attempts to analyze patterns in cryptocurrency markets using a\nspecial type of deep neural networks, namely a convolutional autoencoder. The\nmethod extracts the dominant features of market behavior and classifies the 40\nstudied cryptocurrencies into several classes for twelve 6-month periods\nstarting from 15th May 2013. Transitions from one class to another with time\nare related to the maturement of cryptocurrencies. In speculative\ncryptocurrency markets, these findings have potential implications for\ninvestment and trading strategies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 15:14:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Puzyrev", "Vladimir", ""]]}, {"id": "1910.12288", "submitter": "Axel Brando Guillaumes", "authors": "Axel Brando, Jose A. Rodr\\'iguez-Serrano, Jordi Vitri\\`a, Alberto\n  Rubio", "title": "Modelling heterogeneous distributions with an Uncountable Mixture of\n  Asymmetric Laplacians", "comments": "12 pages, 4 figures, Paper accepted as poster at the 33rd Conference\n  on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression tasks, aleatoric uncertainty is commonly addressed by\nconsidering a parametric distribution of the output variable, which is based on\nstrong assumptions such as symmetry, unimodality or by supposing a restricted\nshape. These assumptions are too limited in scenarios where complex shapes,\nstrong skews or multiple modes are present. In this paper, we propose a generic\ndeep learning framework that learns an Uncountable Mixture of Asymmetric\nLaplacians (UMAL), which will allow us to estimate heterogeneous distributions\nof the output variable and shows its connections to quantile regression.\nDespite having a fixed number of parameters, the model can be interpreted as an\ninfinite mixture of components, which yields a flexible approximation for\nheterogeneous distributions. Apart from synthetic cases, we apply this model to\nroom price forecasting and to predict financial operations in personal bank\naccounts. We demonstrate that UMAL produces proper distributions, which allows\nus to extract richer insights and to sharpen decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 15:50:08 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 11:15:25 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Brando", "Axel", ""], ["Rodr\u00edguez-Serrano", "Jose A.", ""], ["Vitri\u00e0", "Jordi", ""], ["Rubio", "Alberto", ""]]}, {"id": "1910.12306", "submitter": "Vinoj Yasanga Jayasundara Magalle Hewa Mr.", "authors": "Vinoj Jayasundara, Nghi Duy Quoc Bui, Lingxiao Jiang, David Lo", "title": "TreeCaps: Tree-Structured Capsule Networks for Program Source Code\n  Processing", "comments": "in NeurIPS Workshop on ML for Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program comprehension is a fundamental task in software development and\nmaintenance processes. Software developers often need to understand a large\namount of existing code before they can develop new features or fix bugs in\nexisting programs. Being able to process programming language code\nautomatically and provide summaries of code functionality accurately can\nsignificantly help developers to reduce time spent in code navigation and\nunderstanding, and thus increase productivity. Different from natural language\narticles, source code in programming languages often follows rigid syntactical\nstructures and there can exist dependencies among code elements that are\nlocated far away from each other through complex control flows and data flows.\nExisting studies on tree-based convolutional neural networks (TBCNN) and gated\ngraph neural networks (GGNN) are not able to capture essential semantic\ndependencies among code elements accurately. In this paper, we propose novel\ntree-based capsule networks (TreeCaps) and relevant techniques for processing\nprogram code in an automated way that encodes code syntactical structures and\ncaptures code dependencies more accurately. Based on evaluation on programs\nwritten in different programming languages, we show that our TreeCaps-based\napproach can outperform other approaches in classifying the functionalities of\nmany programs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 17:28:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jayasundara", "Vinoj", ""], ["Bui", "Nghi Duy Quoc", ""], ["Jiang", "Lingxiao", ""], ["Lo", "David", ""]]}, {"id": "1910.12308", "submitter": "Giorgi Nadiradze", "authors": "Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Ilia Markov,\n  Shigang Li, Dan Alistarh", "title": "Decentralized SGD with Asynchronous, Local and Quantized Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to scale distributed optimization to large node counts has been\none of the main enablers of recent progress in machine learning. To this end,\nseveral techniques have been explored, such as asynchronous, decentralized, or\nquantized communication--which significantly reduce the cost of\nsynchronization, and the ability for nodes to perform several local model\nupdates before communicating--which reduces the frequency of synchronization.\n  In this paper, we show that these techniques, which have so far been\nconsidered independently, can be jointly leveraged to minimize distribution\ncost for training neural network models via stochastic gradient descent (SGD).\nWe consider a setting with minimal coordination: we have a large number of\nnodes on a communication graph, each with a local subset of data, performing\nindependent SGD updates onto their local models. After some number of local\nupdates, each node chooses an interaction partner uniformly at random from its\nneighbors, and averages a possibly quantized version of its local model with\nthe neighbor's model. Our first contribution is in proving that, even under\nsuch a relaxed setting, SGD can still be guaranteed to converge under standard\nassumptions. The proof is based on a new connection with parallel\nload-balancing processes, and improves existing techniques by jointly handling\ndecentralization, asynchrony, quantization, and local updates, and by bounding\ntheir impact. On the practical side, we implement variants of our algorithm and\ndeploy them onto distributed environments, and show that they can successfully\nconverge and scale for large-scale image classification and translation tasks,\nmatching or even slightly improving the accuracy of previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 17:40:26 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:24:00 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 18:32:01 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nadiradze", "Giorgi", ""], ["Sabour", "Amirmojtaba", ""], ["Davies", "Peter", ""], ["Markov", "Ilia", ""], ["Li", "Shigang", ""], ["Alistarh", "Dan", ""]]}, {"id": "1910.12316", "submitter": "Georgios Detorakis", "authors": "Georgios Detorakis, Sourav Dutta, Abhishek Khanna, Matthew Jerry,\n  Suman Datta, Emre Neftci", "title": "Inherent Weight Normalization in Stochastic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplicative stochasticity such as Dropout improves the robustness and\ngeneralizability of deep neural networks. Here, we further demonstrate that\nalways-on multiplicative stochasticity combined with simple threshold neurons\nare sufficient operations for deep neural networks. We call such models Neural\nSampling Machines (NSM). We find that the probability of activation of the NSM\nexhibits a self-normalizing property that mirrors Weight Normalization, a\npreviously studied mechanism that fulfills many of the features of Batch\nNormalization in an online fashion. The normalization of activities during\ntraining speeds up convergence by preventing internal covariate shift caused by\nchanges in the input distribution. The always-on stochasticity of the NSM\nconfers the following advantages: the network is identical in the inference and\nlearning phases, making the NSM suitable for online learning, it can exploit\nstochasticity inherent to a physical substrate such as analog non-volatile\nmemories for in-memory computing, and it is suitable for Monte Carlo sampling,\nwhile requiring almost exclusively addition and comparison operations. We\ndemonstrate NSMs on standard classification benchmarks (MNIST and CIFAR) and\nevent-based classification benchmarks (N-MNIST and DVS Gestures). Our results\nshow that NSMs perform comparably or better than conventional artificial neural\nnetworks with the same architecture.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 18:21:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Detorakis", "Georgios", ""], ["Dutta", "Sourav", ""], ["Khanna", "Abhishek", ""], ["Jerry", "Matthew", ""], ["Datta", "Suman", ""], ["Neftci", "Emre", ""]]}, {"id": "1910.12336", "submitter": "Patrick Schwab", "authors": "Patrick Schwab, Walter Karlen", "title": "CXPlain: Causal Explanations for Model Interpretation under Uncertainty", "comments": "To appear in Advances in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature importance estimates that inform users about the degree to which\ngiven inputs influence the output of a predictive model are crucial for\nunderstanding, validating, and interpreting machine-learning models. However,\nproviding fast and accurate estimates of feature importance for\nhigh-dimensional data, and quantifying the uncertainty of such estimates remain\nopen challenges. Here, we frame the task of providing explanations for the\ndecisions of machine-learning models as a causal learning task, and train\ncausal explanation (CXPlain) models that learn to estimate to what degree\ncertain inputs cause outputs in another machine-learning model. CXPlain can,\nonce trained, be used to explain the target model in little time, and enables\nthe quantification of the uncertainty associated with its feature importance\nestimates via bootstrap ensembling. We present experiments that demonstrate\nthat CXPlain is significantly more accurate and faster than existing\nmodel-agnostic methods for estimating feature importance. In addition, we\nconfirm that the uncertainty estimates provided by CXPlain ensembles are\nstrongly correlated with their ability to accurately estimate feature\nimportance on held-out data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:59:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Schwab", "Patrick", ""], ["Karlen", "Walter", ""]]}, {"id": "1910.12342", "submitter": "Shane Barratt", "authors": "Shane Barratt and Guillermo Angeris and Stephen Boyd", "title": "Minimizing a Sum of Clipped Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a sum of clipped convex functions;\napplications include clipped empirical risk minimization and clipped control.\nWhile the problem of minimizing the sum of clipped convex functions is NP-hard,\nwe present some heuristics for approximately solving instances of these\nproblems. These heuristics can be used to find good, if not global, solutions\nand appear to work well in practice. We also describe an alternative\nformulation, based on the perspective transformation, which makes the problem\namenable to mixed-integer convex programming and yields computationally\ntractable lower bounds. We illustrate one of our heuristic methods by applying\nit to various examples and use the perspective transformation to certify that\nthe solutions are relatively close to the global optimum. This paper is\naccompanied by an open-source implementation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:34:01 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:18:28 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Barratt", "Shane", ""], ["Angeris", "Guillermo", ""], ["Boyd", "Stephen", ""]]}, {"id": "1910.12358", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Arash Mehrjou, Si Kai Lee, Anant Raj", "title": "Dual Instrumental Variable Regression", "comments": "Advances in Neural Information Processing Systems 33 (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for non-linear instrumental variable (IV)\nregression, DualIV, which simplifies traditional two-stage methods via a dual\nformulation. Inspired by problems in stochastic programming, we show that\ntwo-stage procedures for non-linear IV regression can be reformulated as a\nconvex-concave saddle-point problem. Our formulation enables us to circumvent\nthe first-stage regression which is a potential bottleneck in real-world\napplications. We develop a simple kernel-based algorithm with an analytic\nsolution based on this formulation. Empirical results show that we are\ncompetitive to existing, more complicated algorithms for non-linear\ninstrumental variable regression.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:36:26 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 20:43:18 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 14:24:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Muandet", "Krikamol", ""], ["Mehrjou", "Arash", ""], ["Lee", "Si Kai", ""], ["Raj", "Anant", ""]]}, {"id": "1910.12359", "submitter": "Melih Yesilli", "authors": "Melih C. Yesilli, Sarah Tymochko, Firas A. Khasawneh, Elizabeth Munch", "title": "Chatter Diagnosis in Milling Using Supervised Learning and Topological\n  Features Vector", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2019.00200", "report-no": null, "categories": "eess.SP cs.CG cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatter detection has become a prominent subject of interest due to its\neffect on cutting tool life, surface finish and spindle of machine tool. Most\nof the existing methods in chatter detection literature are based on signal\nprocessing and signal decomposition. In this study, we use topological features\nof data simulating cutting tool vibrations, combined with four supervised\nmachine learning algorithms to diagnose chatter in the milling process.\nPersistence diagrams, a method of representing topological features, are not\neasily used in the context of machine learning, so they must be transformed\ninto a form that is more amenable. Specifically, we will focus on two different\nmethods for featurizing persistence diagrams, Carlsson coordinates and template\nfunctions. In this paper, we provide classification results for simulated data\nfrom various cutting configurations, including upmilling and downmilling, in\naddition to the same data with some added noise. Our results show that Carlsson\nCoordinates and Template Functions yield accuracies as high as 96% and 95%,\nrespectively. We also provide evidence that these topological methods are noise\nrobust descriptors for chatter detection.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:39:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Yesilli", "Melih C.", ""], ["Tymochko", "Sarah", ""], ["Khasawneh", "Firas A.", ""], ["Munch", "Elizabeth", ""]]}, {"id": "1910.12360", "submitter": "Zheng Wang", "authors": "Zheng Wang, Shandian Zhe", "title": "Conditional Expectation Propagation", "comments": "10 pages, 5 figures, UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation propagation (EP) is a powerful approximate inference algorithm.\nHowever, a critical barrier in applying EP is that the moment matching in\nmessage updates can be intractable. Handcrafting approximations is usually\ntricky, and lacks generalizability. Importance sampling is very expensive.\nWhile Laplace propagation provides a good solution, it has to run numerical\noptimizations to find Laplace approximations in every update, which is still\nquite inefficient. To overcome these practical barriers, we propose conditional\nexpectation propagation (CEP) that performs conditional moment matching given\nthe variables outside each message, and then takes expectation w.r.t the\napproximate posterior of these variables. The conditional moments are often\nanalytical and much easier to derive. In the most general case, we can use\n(fully) factorized messages to represent the conditional moments by quadrature\nformulas. We then compute the expectation of the conditional moments via Taylor\napproximations when necessary. In this way, our algorithm can always conduct\nefficient, analytical fixed point iterations. Experiments on several popular\nmodels for which standard EP is available or unavailable demonstrate the\nadvantages of CEP in both inference quality and computational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:41:19 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 16:44:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Wang", "Zheng", ""], ["Zhe", "Shandian", ""]]}, {"id": "1910.12362", "submitter": "David Cortes", "authors": "David Cortes", "title": "Distance approximation using Isolation Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work briefly explores the possibility of approximating spatial distance\n(alternatively, similarity) between data points using the Isolation Forest\nmethod envisioned for outlier detection. The logic is similar to that of\nisolation: the more similar or closer two points are, the more random splits it\nwill take to separate them. The separation depth between two points can be\nstandardized in the same way as the isolation depth, transforming it into a\ndistance metric that is limited in range, centered, and in compliance with the\naxioms of distance. This metric presents some desirable properties such as\nbeing invariant to the scales of variables or being able to account for\nnon-linear relationships between variables, which other metrics such as\nEuclidean or Mahalanobis distance do not. Extensions to the Isolation Forest\nmethod are also proposed for handling categorical variables and missing values,\nresulting in a more generalizable and robust metric.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:47:32 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 15:17:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cortes", "David", ""]]}, {"id": "1910.12363", "submitter": "Dan Oneata", "authors": "Dan Oneata, Cosmin George Alexandru, Marius Stanescu, Octavian Pascu,\n  Alexandru Magan, Adrian Postelnicu, Horia Cucu", "title": "The Quo Vadis submission at Traffic4cast 2019", "comments": "Extended abstract for the Traffic4cast competition from NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe the submission of the Quo Vadis team to the Traffic4cast\ncompetition, which was organized as part of the NeurIPS 2019 series of\nchallenges. Our system consists of a temporal regression module, implemented as\n$1\\times1$ 2d convolutions, augmented with spatio-temporal biases. We have\nfound that using biases is a straightforward and efficient way to include\nseasonal patterns and to improve the performance of the temporal regression\nmodel. Our implementation obtains a mean squared error of $9.47\\times 10^{-3}$\non the test data, placing us on the eight place team-wise. We also present our\nattempts at incorporating spatial correlations into the model; however,\ncontrary to our expectations, adding this type of auxiliary information did not\nbenefit the main system. Our code is available at\nhttps://github.com/danoneata/traffic4cast.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:50:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Oneata", "Dan", ""], ["Alexandru", "Cosmin George", ""], ["Stanescu", "Marius", ""], ["Pascu", "Octavian", ""], ["Magan", "Alexandru", ""], ["Postelnicu", "Adrian", ""], ["Cucu", "Horia", ""]]}, {"id": "1910.12369", "submitter": "Tito Spadini", "authors": "Tito Spadini, Dimitri Leandro de Oliveira Silva, Ricardo Suyama", "title": "Sound Event Recognition in a Smart City Surveillance Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Due to the growing demand for improving surveillance capabilities in smart\ncities, systems need to be developed to provide better monitoring capabilities\nto competent authorities, agencies responsible for strategic resource\nmanagement, and emergency call centers. This work assumes that, as a\ncomplementary monitoring solution, the use of a system capable of detecting the\noccurrence of sound events, performing the Sound Events Recognition (SER) task,\nis highly convenient. In order to contribute to the classification of such\nevents, this paper explored several classifiers over the SESA dataset, composed\nof audios of three hazard classes (gunshots, explosions, and sirens) and a\nclass of casual sounds that could be misinterpreted as some of the other\nsounds. The best result was obtained by SGD, with an accuracy of 72.13% with\n6.81 ms classification time, reinforcing the viability of such an approach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 22:17:26 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 20:36:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Spadini", "Tito", ""], ["Silva", "Dimitri Leandro de Oliveira", ""], ["Suyama", "Ricardo", ""]]}, {"id": "1910.12370", "submitter": "Aya Abdelsalam Ismail", "authors": "Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Pessoa, H\\'ector Corrada\n  Bravo and Soheil Feizi", "title": "Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural\n  Networks", "comments": null, "journal-ref": "Neurips 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts to improve the interpretability of deep neural networks use\nsaliency to characterize the importance of input features to predictions made\nby models. Work on interpretability using saliency-based methods on Recurrent\nNeural Networks (RNNs) has mostly targeted language tasks, and their\napplicability to time series data is less understood. In this work we analyze\nsaliency-based methods for RNNs, both classical and gated cell architectures.\nWe show that RNN saliency vanishes over time, biasing detection of salient\nfeatures only to later time steps and are, therefore, incapable of reliably\ndetecting important features at arbitrary time intervals. To address this\nvanishing saliency problem, we propose a novel RNN cell structure (input-cell\nattention), which can extend any RNN cell architecture. At each time step,\ninstead of only looking at the current input vector, input-cell attention uses\na fixed-size matrix embedding, each row of the matrix attending to different\ninputs from current or previous time steps. Using synthetic data, we show that\nthe saliency map produced by the input-cell attention RNN is able to faithfully\ndetect important features regardless of their occurrence in time. We also apply\nthe input-cell attention RNN on a neuroscience task analyzing functional\nMagnetic Resonance Imaging (fMRI) data for human subjects performing a variety\nof tasks. In this case, we use saliency to characterize brain regions (input\nfeatures) for which activity is important to distinguish between tasks. We show\nthat standard RNN architectures are only capable of detecting important brain\nregions in the last few time steps of the fMRI data, while the input-cell\nattention model is able to detect important brain region activity across time\nwithout latter time step biases.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 22:31:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ismail", "Aya Abdelsalam", ""], ["Gunady", "Mohamed", ""], ["Pessoa", "Luiz", ""], ["Bravo", "H\u00e9ctor Corrada", ""], ["Feizi", "Soheil", ""]]}, {"id": "1910.12379", "submitter": "Yuxin Chen", "authors": "Nikhil Ghosh, Yuxin Chen, Yisong Yue", "title": "Landmark Ordinal Embedding", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to learn a low-dimensional Euclidean representation\nfrom a set of constraints of the form \"item j is closer to item i than item k\".\nExisting approaches for this \"ordinal embedding\" problem require expensive\noptimization procedures, which cannot scale to handle increasingly larger\ndatasets. To address this issue, we propose a landmark-based strategy, which we\ncall Landmark Ordinal Embedding (LOE). Our approach trades off statistical\nefficiency for computational efficiency by exploiting the low-dimensionality of\nthe latent embedding. We derive bounds establishing the statistical consistency\nof LOE under the popular Bradley-Terry-Luce noise model. Through a rigorous\nanalysis of the computational complexity, we show that LOE is significantly\nmore efficient than conventional ordinal embedding approaches as the number of\nitems grows. We validate these characterizations empirically on both synthetic\nand real datasets. We also present a practical approach that achieves the \"best\nof both worlds\", by using LOE to warm-start existing methods that are more\nstatistically efficient but computationally expensive.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 23:26:32 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ghosh", "Nikhil", ""], ["Chen", "Yuxin", ""], ["Yue", "Yisong", ""]]}, {"id": "1910.12381", "submitter": "Junichi Yamagishi", "authors": "Yi Zhao, Xin Wang, Lauri Juvela, Junichi Yamagishi", "title": "Transferring neural speech waveform synthesizers to musical instrument\n  sounds generation", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural waveform synthesizers such as WaveNet, WaveGlow, and the\nneural-source-filter (NSF) model have shown good performance in speech\nsynthesis despite their different methods of waveform generation. The\nsimilarity between speech and music audio synthesis techniques suggests\ninteresting avenues to explore in terms of the best way to apply speech\nsynthesizers in the music domain. This work compares three neural synthesizers\nused for musical instrument sounds generation under three scenarios: training\nfrom scratch on music data, zero-shot learning from the speech domain, and\nfine-tuning-based adaptation from the speech to the music domain. The results\nof a large-scale perceptual test demonstrated that the performance of three\nsynthesizers improved when they were pre-trained on speech data and fine-tuned\non music data, which indicates the usefulness of knowledge from speech data for\nmusic audio generation. Among the synthesizers, WaveGlow showed the best\npotential in zero-shot learning while NSF performed best in the other scenarios\nand could generate samples that were perceptually close to natural audio.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 23:47:43 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 02:31:10 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zhao", "Yi", ""], ["Wang", "Xin", ""], ["Juvela", "Lauri", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "1910.12383", "submitter": "Junichi Yamagishi", "authors": "Yusuke Yasuda, Xin Wang, Junichi Yamagishi", "title": "Effect of choice of probability distribution, randomness, and search\n  methods for alignment modeling in sequence-to-sequence text-to-speech\n  synthesis using hard alignment", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence text-to-speech (TTS) is dominated by\nsoft-attention-based methods. Recently, hard-attention-based methods have been\nproposed to prevent fatal alignment errors, but their sampling method of\ndiscrete alignment is poorly investigated. This research investigates various\ncombinations of sampling methods and probability distributions for alignment\ntransition modeling in a hard-alignment-based sequence-to-sequence TTS method\ncalled SSNT-TTS. We clarify the common sampling methods of discrete variables\nincluding greedy search, beam search, and random sampling from a Bernoulli\ndistribution in a more general way. Furthermore, we introduce the binary\nConcrete distribution to model discrete variables more properly. The results of\na listening test shows that deterministic search is more preferable than\nstochastic search, and the binary Concrete distribution is robust with\nstochastic search for natural alignment transition.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 00:01:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Yasuda", "Yusuke", ""], ["Wang", "Xin", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "1910.12385", "submitter": "Ruizhe Zhao", "authors": "Ruizhe Zhao, Brian Vogel, Tanvir Ahmed", "title": "Adaptive Loss Scaling for Mixed Precision Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed precision training (MPT) is becoming a practical technique to improve\nthe speed and energy efficiency of training deep neural networks by leveraging\nthe fast hardware support for IEEE half-precision floating point that is\navailable in existing GPUs. MPT is typically used in combination with a\ntechnique called loss scaling, that works by scaling up the loss value up\nbefore the start of backpropagation in order to minimize the impact of\nnumerical underflow on training. Unfortunately, existing methods make this loss\nscale value a hyperparameter that needs to be tuned per-model, and a single\nscale cannot be adapted to different layers at different training stages. We\nintroduce a loss scaling-based training method called adaptive loss scaling\nthat makes MPT easier and more practical to use, by removing the need to tune a\nmodel-specific loss scale hyperparameter. We achieve this by introducing\nlayer-wise loss scale values which are automatically computed during training\nto deal with underflow more effectively than existing methods. We present\nexperimental results on a variety of networks and tasks that show our approach\ncan shorten the time to convergence and improve accuracy compared to the\nexisting state-of-the-art MPT and single-precision floating point\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 00:13:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Ruizhe", ""], ["Vogel", "Brian", ""], ["Ahmed", "Tanvir", ""]]}, {"id": "1910.12388", "submitter": "Sneha Aenugu", "authors": "Sneha Aenugu", "title": "A memory enhanced LSTM for modeling complex temporal dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Gamma-LSTM, an enhanced long short term memory\n(LSTM) unit, to enable learning of hierarchical representations through\nmultiple stages of temporal abstractions. Gamma memory, a hierarchical memory\nunit, forms the central memory of Gamma-LSTM with gates to regulate the\ninformation flow into various levels of hierarchy, thus providing the unit with\na control to pick the appropriate level of hierarchy to process the input at a\ngiven instant of time. We demonstrate better performance of Gamma-LSTM model\nregular and stacked LSTMs in two settings (pixel-by-pixel MNIST digit\nclassification and natural language inference) placing emphasis on the ability\nto generalize over long sequences.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:09:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Aenugu", "Sneha", ""]]}, {"id": "1910.12389", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Variable Selection with Copula Entropy", "comments": "To appear at Chinese Journal of Applied Probability and Statistics.\n  Code is available on GitHub at https://github.com/majianthu/aps2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is of significant importance for classification and\nregression tasks in machine learning and statistical applications where both\npredictability and explainability are needed. In this paper, a Copula Entropy\n(CE) based method for variable selection which use CE based ranks to select\nvariables is proposed. The method is both model-free and tuning-free.\nComparison experiments between the proposed method and traditional variable\nselection methods, such as Distance Correlation, Hilbert-Schmidt Independence\nCriterion, Stepwise Selection, regularized generalized linear models and\nAdaptive LASSO, were conducted on the UCI heart disease data. Experimental\nresults show that CE based method can select the `right' variables out more\neffectively and derive better interpretable results than traditional methods do\nwithout sacrificing accuracy performance. It is believed that CE based variable\nselection can help to build more explainable models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 00:47:00 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 11:22:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "1910.12406", "submitter": "Shubhanshu Shekhar", "authors": "Shubhanshu Shekhar, Tara Javidi, Mohammad Ghavamzadeh", "title": "Adaptive Sampling for Estimating Multiple Probability Distributions", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of allocating samples to a finite set of discrete\ndistributions in order to learn them uniformly well in terms of four common\ndistance measures: $\\ell_2^2$, $\\ell_1$, $f$-divergence, and separation\ndistance. To present a unified treatment of these distances, we first propose a\ngeneral optimistic tracking algorithm and analyze its sample allocation\nperformance w.r.t.~an oracle. We then instantiate this algorithm for the four\ndistance measures and derive bounds on the regret of their resulting allocation\nschemes. We verify our theoretical findings through some experiments. Finally,\nwe show that the techniques developed in the paper can be easily extended to\nthe related setting of minimizing the average error (in terms of the four\ndistances) in learning a set of distributions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 02:18:40 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 01:32:56 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Shekhar", "Shubhanshu", ""], ["Javidi", "Tara", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1910.12414", "submitter": "Lin Chen", "authors": "Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab S. Mirrokni", "title": "Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing approximate nearest neighbors in high dimensional spaces is a\ncentral problem in large-scale data mining with a wide range of applications in\nmachine learning and data science. A popular and effective technique in\ncomputing nearest neighbors approximately is the locality-sensitive hashing\n(LSH) scheme. In this paper, we aim to develop LSH schemes for distance\nfunctions that measure the distance between two probability distributions,\nparticularly for f-divergences as well as a generalization to capture mutual\ninformation loss. First, we provide a general framework to design LHS schemes\nfor f-divergence distance functions and develop LSH schemes for the generalized\nJensen-Shannon divergence and triangular discrimination in this framework. We\nshow a two-sided approximation result for approximation of the generalized\nJensen-Shannon divergence by the Hellinger distance, which may be of\nindependent interest. Next, we show a general method of reducing the problem of\ndesigning an LSH scheme for a Krein kernel (which can be expressed as the\ndifference of two positive definite kernels) to the problem of maximum inner\nproduct search. We exemplify this method by applying it to the mutual\ninformation loss, due to its several important applications such as model\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:07:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Thomas", ""], ["Mirrokni", "Vahab S.", ""]]}, {"id": "1910.12417", "submitter": "Raha Moraffah", "authors": "Raha Moraffah, Kai Shu, Adrienne Raglin, Huan Liu", "title": "Deep causal representation learning for unsupervised domain adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies show that the representations learned by deep neural networks can be\ntransferred to similar prediction tasks in other domains for which we do not\nhave enough labeled data. However, as we transition to higher layers in the\nmodel, the representations become more task-specific and less generalizable.\nRecent research on deep domain adaptation proposed to mitigate this problem by\nforcing the deep model to learn more transferable feature representations\nacross domains. This is achieved by incorporating domain adaptation methods\ninto deep learning pipeline. The majority of existing models learn the\ntransferable feature representations which are highly correlated with the\noutcome. However, correlations are not always transferable. In this paper, we\npropose a novel deep causal representation learning framework for unsupervised\ndomain adaptation, in which we propose to learn domain-invariant causal\nrepresentations of the input from the source domain. We simulate a virtual\ntarget domain using reweighted samples from the source domain and estimate the\ncausal effect of features on the outcomes. The extensive comparative study\ndemonstrates the strengths of the proposed model for unsupervised domain\nadaptation via causal representations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:16:01 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Moraffah", "Raha", ""], ["Shu", "Kai", ""], ["Raglin", "Adrienne", ""], ["Liu", "Huan", ""]]}, {"id": "1910.12424", "submitter": "Mingrui Zhang", "authors": "Mingrui Zhang, Lin Chen, Hamed Hassani, Amin Karbasi", "title": "Online Continuous Submodular Maximization: From Full-Information to\n  Bandit Feedback", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose three online algorithms for submodular\nmaximisation. The first one, Mono-Frank-Wolfe, reduces the number of\nper-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$\n[chen2018projection] to 1, and achieves a $(1-1/e)$-regret bound of\n$O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm\nfor continuous DR-submodular maximization, which achieves a $(1-1/e)$-regret\nbound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit\nalgorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which\nattains a $(1-1/e)$-regret bound of $O(T^{8/9})$ in the responsive bandit\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:24:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Mingrui", ""], ["Chen", "Lin", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1910.12428", "submitter": "Jingbo Liu", "authors": "Jingbo Liu and Philippe Rigollet", "title": "Power analysis of knockoff filters for correlated designs", "comments": "Accepted to Neurips 2019. The conference version includes the\n  contents of this version excluding the appendices. v3 on arXiv corrected some\n  typos in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knockoff filter introduced by Barber and Cand\\`es 2016 is an elegant\nframework for controlling the false discovery rate in variable selection. While\nempirical results indicate that this methodology is not too conservative, there\nis no conclusive theoretical result on its power. When the predictors are\ni.i.d. Gaussian, it is known that as the signal to noise ratio tend to\ninfinity, the knockoff filter is consistent in the sense that one can make FDR\ngo to 0 and power go to 1 simultaneously. In this work we study the case where\nthe predictors have a general covariance matrix $\\Sigma$. We introduce a simple\nfunctional called effective signal deficiency (ESD) of the covariance matrix\n$\\Sigma$ that predicts consistency of various variable selection methods. In\nparticular, ESD reveals that the structure of the precision matrix\n$\\Sigma^{-1}$ plays a central role in consistency and therefore, so does the\nconditional independence structure of the predictors. To leverage this\nconnection, we introduce Conditional Independence knockoff, a simple procedure\nthat is able to compete with the more sophisticated knockoff filters and that\nis defined when the predictors obey a Gaussian tree graphical models (or when\nthe graph is sufficiently sparse). Our theoretical results are supported by\nnumerical evidence on synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:48:42 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:40:50 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 21:12:13 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Liu", "Jingbo", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1910.12430", "submitter": "Akshay Agrawal", "authors": "Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven\n  Diamond, Zico Kolter", "title": "Differentiable Convex Optimization Layers", "comments": "In NeurIPS 2019. Code available at\n  https://www.github.com/cvxgrp/cvxpylayers. Authors in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown how to embed differentiable optimization problems (that\nis, problems whose solutions can be backpropagated through) as layers within\ndeep learning architectures. This method provides a useful inductive bias for\ncertain problems, but existing software for differentiable optimization layers\nis rigid and difficult to apply to new settings. In this paper, we propose an\napproach to differentiating through disciplined convex programs, a subclass of\nconvex optimization problems used by domain-specific languages (DSLs) for\nconvex optimization. We introduce disciplined parametrized programming, a\nsubset of disciplined convex programming, and we show that every disciplined\nparametrized program can be represented as the composition of an affine map\nfrom parameters to problem data, a solver, and an affine map from the solver's\nsolution to a solution of the original problem (a new form we refer to as\naffine-solver-affine form). We then demonstrate how to efficiently\ndifferentiate through each of these components, allowing for end-to-end\nanalytical differentiation through the entire convex program. We implement our\nmethodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex\noptimization, and additionally implement differentiable layers for disciplined\nconvex programs in PyTorch and TensorFlow 2.0. Our implementation significantly\nlowers the barrier to using convex optimization problems in differentiable\nprograms. We present applications in linear machine learning models and in\nstochastic control, and we show that our layer is competitive (in execution\ntime) compared to specialized differentiable solvers from past work.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 04:08:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Agrawal", "Akshay", ""], ["Amos", "Brandon", ""], ["Barratt", "Shane", ""], ["Boyd", "Stephen", ""], ["Diamond", "Steven", ""], ["Kolter", "Zico", ""]]}, {"id": "1910.12453", "submitter": "Yunzhi Zhang", "authors": "Yunzhi Zhang, Ignasi Clavera, Boren Tsai, Pieter Abbeel", "title": "Asynchronous Methods for Model-Based Reinforcement Learning", "comments": "10 pages, CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in the area of model-based reinforcement\nlearning. State-of-the-art algorithms are now able to match the asymptotic\nperformance of model-free methods while being significantly more data\nefficient. However, this success has come at a price: state-of-the-art\nmodel-based methods require significant computation interleaved with data\ncollection, resulting in run times that take days, even if the amount of agent\ninteraction might be just hours or even minutes. When considering the goal of\nlearning in real-time on real robots, this means these state-of-the-art\nmodel-based algorithms still remain impractical. In this work, we propose an\nasynchronous framework for model-based reinforcement learning methods that\nbrings down the run time of these algorithms to be just the data collection\ntime. We evaluate our asynchronous framework on a range of standard MuJoCo\nbenchmarks. We also evaluate our asynchronous framework on three real-world\nrobotic manipulation tasks. We show how asynchronous learning not only speeds\nup learning w.r.t wall-clock time through parallelization, but also further\nreduces the sample complexity of model-based approaches by means of improving\nthe exploration and by means of effectively avoiding the policy overfitting to\nthe deficiencies of learned dynamics models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 05:45:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Yunzhi", ""], ["Clavera", "Ignasi", ""], ["Tsai", "Boren", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1910.12454", "submitter": "Alexander Potapov", "authors": "Alexander Potapov, Ian Colbert, Ken Kreutz-Delgado, Alexander\n  Cloninger, and Srinjoy Das", "title": "PT-MMD: A Novel Statistical Framework for the Evaluation of Generative\n  Systems", "comments": "Will be presented at the Asilomar Conference on Signals, Systems, and\n  Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic-sampling-based Generative Neural Networks, such as Restricted\nBoltzmann Machines and Generative Adversarial Networks, are now used for\napplications such as denoising, image occlusion removal, pattern completion,\nand motion synthesis. In scenarios which involve performing such inference\ntasks with these models, it is critical to determine metrics that allow for\nmodel selection and/or maintenance of requisite generative performance under\npre-specified implementation constraints. In this paper, we propose a new\nmetric for evaluating generative model performance based on $p$-values derived\nfrom the combined use of Maximum Mean Discrepancy (MMD) and permutation-based\n(PT-based) resampling, which we refer to as PT-MMD. We demonstrate the\neffectiveness of this metric for two cases: (1) Selection of bitwidth and\nactivation function complexity to achieve minimum power-at-performance for\nRestricted Boltzmann Machines; (2) Quantitative comparison of images generated\nby two types of Generative Adversarial Networks (PGAN and WGAN) to facilitate\nmodel selection in order to maximize the fidelity of generated images. For\nthese applications, our results are shown using Euclidean and Haar-based\nkernels for the PT-MMD two sample hypothesis test. This demonstrates the\ncritical role of distance functions in comparing generated images against their\ncorresponding ground truth counterparts as what would be perceived by human\nusers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 05:50:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Potapov", "Alexander", ""], ["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Cloninger", "Alexander", ""], ["Das", "Srinjoy", ""]]}, {"id": "1910.12462", "submitter": "Ankur Goswami", "authors": "Ankur Goswami, Joshua McGrath, Shanan Peters, Theodoros Rekatsinas", "title": "Fine-Grained Object Detection over Scientific Document Images with\n  Region Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study the problem of object detection over scanned images of scientific\ndocuments. We consider images that contain objects of varying aspect ratios and\nsizes and range from coarse elements such as tables and figures to fine\nelements such as equations and section headers. We find that current object\ndetectors fail to produce properly localized region proposals over such page\nobjects. We revisit the original R-CNN model and present a method for\ngenerating fine-grained proposals over document elements. We also present a\nregion embedding model that uses the convolutional maps of a proposal's\nneighbors as context to produce an embedding for each proposal. This region\nembedding is able to capture the semantic relationships between a target region\nand its surrounding context. Our end-to-end model produces an embedding for\neach proposal, then classifies each proposal by using a multi-head attention\nmodel that attends to the most important neighbors of a proposal. To evaluate\nour model, we collect and annotate a dataset of publications from heterogeneous\njournals. We show that our model, referred to as Attentive-RCNN, yields a 17%\nmAP improvement compared to standard object detection models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:39:02 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:25:24 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Goswami", "Ankur", ""], ["McGrath", "Joshua", ""], ["Peters", "Shanan", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1910.12469", "submitter": "Qitian Wu", "authors": "Qitian Wu, Zixuan Zhang, Xiaofeng Gao, Junchi Yan, Guihai Chen", "title": "Learning Latent Process from High-Dimensional Event Sequences via\n  Efficient Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We target modeling latent dynamics in high-dimension marked event sequences\nwithout any prior knowledge about marker relations. Such problem has been\nrarely studied by previous works which would have fundamental difficulty to\nhandle the arisen challenges: 1) the high-dimensional markers and unknown\nrelation network among them pose intractable obstacles for modeling the latent\ndynamic process; 2) one observed event sequence may concurrently contain\nseveral different chains of interdependent events; 3) it is hard to well define\nthe distance between two high-dimension event sequences. To these ends, in this\npaper, we propose a seminal adversarial imitation learning framework for\nhigh-dimension event sequence generation which could be decomposed into: 1) a\nlatent structural intensity model that estimates the adjacent nodes without\nexplicit networks and learns to capture the temporal dynamics in the latent\nspace of markers over observed sequence; 2) an efficient random walk based\ngeneration model that aims at imitating the generation process of\nhigh-dimension event sequences from a bottom-up view; 3) a discriminator\nspecified as a seq2seq network optimizing the rewards to help the generator\noutput event sequences as real as possible. Experimental results on both\nsynthetic and real-world datasets demonstrate that the proposed method could\neffectively detect the hidden network among markers and make decent prediction\nfor future marked events, even when the number of markers scales to million\nlevel.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:11:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wu", "Qitian", ""], ["Zhang", "Zixuan", ""], ["Gao", "Xiaofeng", ""], ["Yan", "Junchi", ""], ["Chen", "Guihai", ""]]}, {"id": "1910.12481", "submitter": "Justin Cosentino", "authors": "Justin Cosentino, Jun Zhu", "title": "Generative Well-intentioned Networks", "comments": "17 pages, 9 figures, 8 tables, to appear in the proceedings of\n  Advances in Neural Information Processing Systems (NeurIPS), Vancouver,\n  Canada, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Generative Well-intentioned Networks (GWINs), a novel framework\nfor increasing the accuracy of certainty-based, closed-world classifiers. A\nconditional generative network recovers the distribution of observations that\nthe classifier labels correctly with high certainty. We introduce a reject\noption to the classifier during inference, allowing the classifier to reject an\nobservation instance rather than predict an uncertain label. These rejected\nobservations are translated by the generative network to high-certainty\nrepresentations, which are then relabeled by the classifier. This architecture\nallows for any certainty-based classifier or rejection function and is not\nlimited to multilayer perceptrons. The capability of this framework is assessed\nusing benchmark classification datasets and shows that GWINs significantly\nimprove the accuracy of uncertain observations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:40:41 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cosentino", "Justin", ""], ["Zhu", "Jun", ""]]}, {"id": "1910.12490", "submitter": "Wasim Huleihel", "authors": "Wasim Huleihel, Arya Mazumdar, Muriel M\\'edard, and Soumyabrata Pal", "title": "Same-Cluster Querying for Overlapping Clusters", "comments": "43 pages, accepted at NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping clusters are common in models of many practical data-segmentation\napplications. Suppose we are given $n$ elements to be clustered into $k$\npossibly overlapping clusters, and an oracle that can interactively answer\nqueries of the form \"do elements $u$ and $v$ belong to the same cluster?\" The\ngoal is to recover the clusters with minimum number of such queries. This\nproblem has been of recent interest for the case of disjoint clusters. In this\npaper, we look at the more practical scenario of overlapping clusters, and\nprovide upper bounds (with algorithms) on the sufficient number of queries. We\nprovide algorithmic results under both arbitrary (worst-case) and statistical\nmodeling assumptions. Our algorithms are parameter free, efficient, and work in\nthe presence of random noise. We also derive information-theoretic lower bounds\non the number of queries needed, proving that our algorithms are order optimal.\nFinally, we test our algorithms over both synthetic and real-world data,\nshowing their practicality and effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 08:06:20 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Huleihel", "Wasim", ""], ["Mazumdar", "Arya", ""], ["M\u00e9dard", "Muriel", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "1910.12511", "submitter": "Sebastian Curi", "authors": "Sebastian Curi, Kfir. Y. Levy, Stefanie Jegelka, Andreas Krause", "title": "Adaptive Sampling for Stochastic Risk-Averse Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-stakes machine learning applications, it is crucial to not only\nperform well on average, but also when restricted to difficult examples. To\naddress this, we consider the problem of training models in a risk-averse\nmanner. We propose an adaptive sampling algorithm for stochastically optimizing\nthe Conditional Value-at-Risk (CVaR) of a loss distribution, which measures its\nperformance on the $\\alpha$ fraction of most difficult examples. We use a\ndistributionally robust formulation of the CVaR to phrase the problem as a\nzero-sum game between two players, and solve it efficiently using regret\nminimization. Our approach relies on sampling from structured Determinantal\nPoint Processes (DPPs), which enables scaling it to large data sets. Finally,\nwe empirically demonstrate its effectiveness on large-scale convex and\nnon-convex learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 09:12:36 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 16:26:40 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 13:02:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Curi", "Sebastian", ""], ["Levy", "Kfir. Y.", ""], ["Jegelka", "Stefanie", ""], ["Krause", "Andreas", ""]]}, {"id": "1910.12521", "submitter": "Belhal Karimi", "authors": "Belhal Karimi, Hoi-To Wai, Eric Moulines, Marc Lavielle", "title": "On the Global Convergence of (Fast) Incremental Expectation Maximization\n  Methods", "comments": "25 pages, Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is one of the most popular algorithm for inference in latent\ndata models. The original formulation of the EM algorithm does not scale to\nlarge data set, because the whole data set is required at each iteration of the\nalgorithm. To alleviate this problem, Neal and Hinton have proposed an\nincremental version of the EM (iEM) in which at each iteration the conditional\nexpectation of the latent data (E-step) is updated only for a mini-batch of\nobservations. Another approach has been proposed by Capp\\'e and Moulines in\nwhich the E-step is replaced by a stochastic approximation step, closely\nrelated to stochastic gradient. In this paper, we analyze incremental and\nstochastic version of the EM algorithm as well as the variance reduced-version\nof Chen et. al. in a common unifying framework. We also introduce a new version\nincremental version, inspired by the SAGA algorithm by Defazio et. al. We\nestablish non-asymptotic convergence bounds for global convergence. Numerical\napplications are presented in this article to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 09:51:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Wai", "Hoi-To", ""], ["Moulines", "Eric", ""], ["Lavielle", "Marc", ""]]}, {"id": "1910.12548", "submitter": "Alexander Hepburn", "authors": "Alexander Hepburn, Valero Laparra, Jes\\'us Malo, Ryan McConville, Raul\n  Santos-Rodriguez", "title": "PerceptNet: A Human Visual System Inspired Neural Network for Estimating\n  Perceptual Distance", "comments": null, "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP), Abu\n  Dhabi, United Arab Emirates, 2020, pp. 121-125", "doi": "10.1109/ICIP40778.2020.9190691", "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the vision community has devised algorithms to estimate the\ndistance between an original image and images that have been subject to\nperturbations. Inspiration was usually taken from the human visual perceptual\nsystem and how the system processes different perturbations in order to\nreplicate to what extent it determines our ability to judge image quality.\nWhile recent works have presented deep neural networks trained to predict human\nperceptual quality, very few borrow any intuitions from the human visual\nsystem. To address this, we present PerceptNet, a convolutional neural network\nwhere the architecture has been chosen to reflect the structure and various\nstages in the human visual system. We evaluate PerceptNet on various\ntraditional perception datasets and note strong performance on a number of them\nas compared with traditional image quality metrics. We also show that including\na nonlinearity inspired by the human visual system in classical deep neural\nnetworks architectures can increase their ability to judge perceptual\nsimilarity. Compared to similar deep learning methods, the performance is\nsimilar, although our network has a number of parameters that is several orders\nof magnitude less.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:48:30 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:33:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hepburn", "Alexander", ""], ["Laparra", "Valero", ""], ["Malo", "Jes\u00fas", ""], ["McConville", "Ryan", ""], ["Santos-Rodriguez", "Raul", ""]]}, {"id": "1910.12554", "submitter": "Yingbo Gao", "authors": "Yingbo Gao, Christian Herold, Weiyue Wang, Hermann Ney", "title": "Exploring Kernel Functions in the Softmax Layer for Contextual Word\n  Classification", "comments": "IWSLT2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prominently used in support vector machines and logistic regressions, kernel\nfunctions (kernels) can implicitly map data points into high dimensional spaces\nand make it easier to learn complex decision boundaries. In this work, by\nreplacing the inner product function in the softmax layer, we explore the use\nof kernels for contextual word classification. In order to compare the\nindividual kernels, experiments are conducted on standard language modeling and\nmachine translation tasks. We observe a wide range of performances across\ndifferent kernel settings. Extending the results, we look at the gradient\nproperties, investigate various mixture strategies and examine the\ndisambiguation abilities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 11:06:21 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Gao", "Yingbo", ""], ["Herold", "Christian", ""], ["Wang", "Weiyue", ""], ["Ney", "Hermann", ""]]}, {"id": "1910.12566", "submitter": "Ginestra Bianconi", "authors": "Ginestra Bianconi, Sergey N. Dorogovtsev", "title": "The spectral dimension of simplicial complexes: a renormalization group\n  theory", "comments": "(30 pages, 5 figures)", "journal-ref": null, "doi": "10.1088/1742-5468/ab5d0e", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes are increasingly used to study complex system structure\nand dynamics including diffusion, synchronization and epidemic spreading. The\nspectral dimension of the graph Laplacian is known to determine the diffusion\nproperties at long time scales. Using the renormalization group here we\ncalculate the spectral dimension of the graph Laplacian of two classes of\nnon-amenable $d$ dimensional simplicial complexes: the Apollonian networks and\nthe pseudo-fractal networks. We analyse the scaling of the spectral dimension\nwith the topological dimension $d$ for $d\\to \\infty$ and we point out that\nrandomness such as the one present in Network Geometry with Flavor can diminish\nthe value of the spectral dimension of these structures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 11:29:44 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:51:05 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 19:01:50 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bianconi", "Ginestra", ""], ["Dorogovtsev", "Sergey N.", ""]]}, {"id": "1910.12577", "submitter": "Chunxi Tan", "authors": "Ruijian Han, Kani Chen and Chunxi Tan", "title": "Curiosity-Driven Recommendation Strategy for Adaptive Learning via Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of recommendations strategies in the adaptive learning system\nfocuses on utilizing currently available information to provide\nindividual-specific learning instructions for learners. As a critical motivate\nfor human behaviors, curiosity is essentially the drive to explore knowledge\nand seek information. In a psychologically inspired view, we aim to incorporate\nthe element of curiosity for guiding learners to study spontaneously. In this\npaper, a curiosity-driven recommendation policy is proposed under the\nreinforcement learning framework, allowing for a both efficient and enjoyable\npersonalized learning mode. Given intrinsic rewards from a well-designed\npredictive model, we apply the actor-critic method to approximate the policy\ndirectly through neural networks. Numeric analyses with a large continuous\nknowledge state space and concrete learning scenarios are used to further\ndemonstrate the power of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 02:59:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Han", "Ruijian", ""], ["Chen", "Kani", ""], ["Tan", "Chunxi", ""]]}, {"id": "1910.12586", "submitter": "Yongkai Wu", "authors": "Yongkai Wu, Lu Zhang, Xintao Wu, Hanghang Tong", "title": "PC-Fairness: A Unified Framework for Measuring Causality-based Fairness", "comments": "Accepted as a poster to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend of fair machine learning is to define fairness as\ncausality-based notions which concern the causal connection between protected\nattributes and decisions. However, one common challenge of all causality-based\nfairness notions is identifiability, i.e., whether they can be uniquely\nmeasured from observational data, which is a critical barrier to applying these\nnotions to real-world situations. In this paper, we develop a framework for\nmeasuring different causality-based fairness. We propose a unified definition\nthat covers most of previous causality-based fairness notions, namely the\npath-specific counterfactual fairness (PC fairness). Based on that, we propose\na general method in the form of a constrained optimization problem for bounding\nthe path-specific counterfactual fairness under all unidentifiable situations.\nExperiments on synthetic and real-world datasets show the correctness and\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 23:00:53 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wu", "Yongkai", ""], ["Zhang", "Lu", ""], ["Wu", "Xintao", ""], ["Tong", "Hanghang", ""]]}, {"id": "1910.12587", "submitter": "Tyler Lee", "authors": "Tyler Lee, Ting Gong, Suchismita Padhy, Andrew Rouditchenko, Anthony\n  Ndirango", "title": "Label-efficient audio classification through multitask learning and\n  self-supervision", "comments": "Presented at ICLR 2019 Limited Labeled Data (LLD) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has been incredibly successful in modeling tasks with\nlarge, carefully curated labeled datasets, its application to problems with\nlimited labeled data remains a challenge. The aim of the present work is to\nimprove the label efficiency of large neural networks operating on audio data\nthrough a combination of multitask learning and self-supervised learning on\nunlabeled data. We trained an end-to-end audio feature extractor based on\nWaveNet that feeds into simple, yet versatile task-specific neural networks. We\ndescribe several easily implemented self-supervised learning tasks that can\noperate on any large, unlabeled audio corpus. We demonstrate that, in scenarios\nwith limited labeled training data, one can significantly improve the\nperformance of three different supervised classification tasks individually by\nup to 6% through simultaneous training with these additional self-supervised\ntasks. We also show that incorporating data augmentation into our multitask\nsetting leads to even further gains in performance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 00:58:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lee", "Tyler", ""], ["Gong", "Ting", ""], ["Padhy", "Suchismita", ""], ["Rouditchenko", "Andrew", ""], ["Ndirango", "Anthony", ""]]}, {"id": "1910.12589", "submitter": "Ramya Akula", "authors": "Ramya Akula, Zachary Wieselthier, Laura Martin, Ivan Garibay", "title": "Forecasting the Success of Television Series using Machine Learning", "comments": "9 Pages, 10 Figures and 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Television is an ever-evolving multi billion dollar industry. The success of\na television show in an increasingly technological society is a vast\nmulti-variable formula. The art of success is not just something that happens,\nbut is studied, replicated, and applied. Hollywood can be unpredictable\nregarding success, as many movies and sitcoms that are hyped up and promise to\nbe a hit end up being box office failures and complete disappointments. In\ncurrent studies, linguistic exploration is being performed on the relationship\nbetween Television series and target community of viewers. Having a decision\nsupport system that can display sound and predictable results would be needed\nto build confidence in the investment of a new TV series. The models presented\nin this study use data to study and determine what makes a sitcom successful.\nIn this paper, we use descriptive and predictive modeling techniques to assess\nthe continuing success of television comedies: The Office, Big Bang Theory,\nArrested Development, Scrubs, and South Park. The factors that are tested for\nstatistical significance on episode ratings are character presence, director,\nand writer. These statistics show that while characters are indeed crucial to\nthe shows themselves, the creation and direction of the shows pose implication\nupon the ratings and therefore the success of the shows. We use machine\nlearning based forecasting models to accurately predict the success of shows.\nThe models represent a baseline to understanding the success of a television\nshow and how producers can increase the success of current television shows or\nutilize this data in the creation of future shows. Due to the many factors that\ngo into a series, the empirical analysis in this work shows that there is no\none-fits-all model to forecast the rating or success of a television show.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:46:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Akula", "Ramya", ""], ["Wieselthier", "Zachary", ""], ["Martin", "Laura", ""], ["Garibay", "Ivan", ""]]}, {"id": "1910.12590", "submitter": "Tedd Kourkounakis", "authors": "Tedd Kourkounakis, Amirhossein Hajavi, Ali Etemad", "title": "Detecting Multiple Speech Disfluencies using a Deep Residual Network\n  with Bidirectional Long Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stuttering is a speech impediment affecting tens of millions of people on an\neveryday basis. Even with its commonality, there is minimal data and research\non the identification and classification of stuttered speech. This paper\ntackles the problem of detection and classification of different forms of\nstutter. As opposed to most existing works that identify stutters with language\nmodels, our work proposes a model that relies solely on acoustic features,\nallowing for identification of several variations of stutter disfluencies\nwithout the need for speech recognition. Our model uses a deep residual network\nand bidirectional long short-term memory layers to classify different types of\nstutters and achieves an average miss rate of 10.03%, outperforming the\nstate-of-the-art by almost 27%\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:32:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kourkounakis", "Tedd", ""], ["Hajavi", "Amirhossein", ""], ["Etemad", "Ali", ""]]}, {"id": "1910.12597", "submitter": "Richard Scruggs", "authors": "Richard Scruggs, Ryan S. Baker, Bruce M. McLaren", "title": "Extending Deep Knowledge Tracing: Inferring Interpretable Knowledge and\n  Predicting Post-System Performance", "comments": "8 pages, accepted to the International Conference on Computers in\n  Education", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent student knowledge modeling algorithms such as Deep Knowledge Tracing\n(DKT) and Dynamic Key-Value Memory Networks (DKVMN) have been shown to produce\naccurate predictions of problem correctness within the same learning system.\nHowever, these algorithms do not attempt to directly infer student knowledge.\nIn this paper we present an extension to these algorithms to also infer\nknowledge. We apply this extension to DKT and DKVMN, resulting in knowledge\nestimates that correlate better with a posttest than knowledge estimates from\nBayesian Knowledge Tracing (BKT), an algorithm designed to infer knowledge, and\nanother classic algorithm, Performance Factors Analysis (PFA). We also apply\nour extension to correctness predictions from BKT and PFA, finding that\nknowledge estimates produced with it correlate better with the posttest than\nBKT and PFA's standard knowledge estimates. These findings are significant\nsince the primary aim of education is to prepare students for later experiences\noutside of the immediate learning activity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:21:20 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 19:33:54 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Scruggs", "Richard", ""], ["Baker", "Ryan S.", ""], ["McLaren", "Bruce M.", ""]]}, {"id": "1910.12612", "submitter": "Duc Le", "authors": "Duc Le, Thilo Koehler, Christian Fuegen, Michael L. Seltzer", "title": "G2G: TTS-Driven Pronunciation Learning for Graphemic Hybrid ASR", "comments": "To appear at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grapheme-based acoustic modeling has recently been shown to outperform\nphoneme-based approaches in both hybrid and end-to-end automatic speech\nrecognition (ASR), even on non-phonemic languages like English. However,\ngraphemic ASR still has problems with rare long-tail words that do not follow\nthe standard spelling conventions seen in training, such as entity names. In\nthis work, we present a novel method to train a statistical\ngrapheme-to-grapheme (G2G) model on text-to-speech data that can rewrite an\narbitrary character sequence into more phonetically consistent forms. We show\nthat using G2G to provide alternative pronunciations during decoding reduces\nWord Error Rate by 3% to 11% relative over a strong graphemic baseline and\nbridges the gap on rare name recognition with an equivalent phonetic setup.\nUnlike many previously proposed methods, our method does not require any change\nto the acoustic model training procedure. This work reaffirms the efficacy of\ngrapheme-based modeling and shows that specialized linguistic knowledge, when\navailable, can be leveraged to improve graphemic ASR.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 21:49:50 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 21:24:48 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Le", "Duc", ""], ["Koehler", "Thilo", ""], ["Fuegen", "Christian", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "1910.12618", "submitter": "David Obst", "authors": "David Obst and Badih Ghattas and Sandra Claudel and Jairo Cugliari and\n  Yannig Goude and Georges Oppenheim", "title": "Textual Data for Time Series Forecasting", "comments": "-Added e-mail addresses of authors. -Added author who didn't appear\n  on the paper's arXiv page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While ubiquitous, textual sources of information such as company reports,\nsocial media posts, etc. are hardly included in prediction algorithms for time\nseries, despite the relevant information they may contain. In this work, openly\naccessible daily weather reports from France and the United-Kingdom are\nleveraged to predict time series of national electricity consumption, average\ntemperature and wind-speed with a single pipeline. Two methods of numerical\nrepresentation of text are considered, namely traditional Term Frequency -\nInverse Document Frequency (TF-IDF) as well as our own neural word embedding.\nUsing exclusively text, we are able to predict the aforementioned time series\nwith sufficient accuracy to be used to replace missing data. Furthermore the\nproposed word embeddings display geometric properties relating to the behavior\nof the time series and context similarity between words.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 07:47:56 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 09:39:06 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Obst", "David", ""], ["Ghattas", "Badih", ""], ["Claudel", "Sandra", ""], ["Cugliari", "Jairo", ""], ["Goude", "Yannig", ""], ["Oppenheim", "Georges", ""]]}, {"id": "1910.12620", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif, Karim Armanious, Karim Guirguis, Jayasankar T.\n  Sajeev, Bin Yang", "title": "AeGAN: Time-Frequency Speech Denoising via Generative Adversarial\n  Networks", "comments": "5 pages, 4 figures and 2 Tables. Accepted in EUSIPCO 2020", "journal-ref": null, "doi": "10.23919/Eusipco47968.2020.9287606", "report-no": null, "categories": "eess.AS cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) systems are of vital importance nowadays\nin commonplace tasks such as speech-to-text processing and language\ntranslation. This created the need for an ASR system that can operate in\nrealistic crowded environments. Thus, speech enhancement is a valuable building\nblock in ASR systems and other applications such as hearing aids, smartphones\nand teleconferencing systems. In this paper, a generative adversarial network\n(GAN) based framework is investigated for the task of speech enhancement, more\nspecifically speech denoising of audio tracks. A new architecture based on\nCasNet generator and an additional feature-based loss are incorporated to get\nrealistically denoised speech phonetics. Finally, the proposed framework is\nshown to outperform other learning and traditional model-based speech\nenhancement approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:27:22 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 19:55:22 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 00:10:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Armanious", "Karim", ""], ["Guirguis", "Karim", ""], ["Sajeev", "Jayasankar T.", ""], ["Yang", "Bin", ""]]}, {"id": "1910.12624", "submitter": "L\\'eo Neufcourt", "authors": "L\\'eo Neufcourt, Yuchen Cao, Samuel Giuliani, Witold Nazarewicz, Erik\n  Olsen, Oleg B. Tarasov", "title": "Beyond the proton drip line: Bayesian analysis of proton-emitting nuclei", "comments": null, "journal-ref": "Phys. Rev. C 101, 014319 (2020)", "doi": "10.1103/PhysRevC.101.014319", "report-no": null, "categories": "nucl-th nucl-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limits of the nuclear landscape are determined by nuclear binding\nenergies. Beyond the proton drip lines, where the separation energy becomes\nnegative, there is not enough binding energy to prevent protons from escaping\nthe nucleus. Predicting properties of unstable nuclear states in the vast\nterritory of proton emitters poses an appreciable challenge for nuclear theory\nas it often involves far extrapolations. In addition, significant discrepancies\nbetween nuclear models in the proton-rich territory call for quantified\npredictions. With the help of Bayesian methodology, we mix a family of nuclear\nmass models corrected with statistical emulators trained on the experimental\nmass measurements, in the proton-rich region of the nuclear chart. Separation\nenergies were computed within nuclear density functional theory using several\nSkyrme and Gogny energy density functionals. We also considered mass\npredictions based on two models used in astrophysical studies. Quantified\npredictions were obtained for each model using Bayesian Gaussian processes\ntrained on separation-energy residuals and combined via Bayesian model\naveraging. We obtained a good agreement between averaged predictions of\nstatistically corrected models and experiment. In particular, we quantified\nmodel results for one- and two-proton separation energies and derived\nprobabilities of proton emission. This information enabled us to produce a\nquantified landscape of proton-rich nuclei. The most promising candidates for\ntwo-proton decay studies have been identified. The methodology used in this\nwork has broad applications to model-based extrapolations of various nuclear\nobservables. It also provides a reliable uncertainty quantification of\ntheoretical predictions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 12:55:13 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 16:47:57 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Neufcourt", "L\u00e9o", ""], ["Cao", "Yuchen", ""], ["Giuliani", "Samuel", ""], ["Nazarewicz", "Witold", ""], ["Olsen", "Erik", ""], ["Tarasov", "Oleg B.", ""]]}, {"id": "1910.12625", "submitter": "Erwei Wang", "authors": "Erwei Wang, James J. Davis, Peter Y. K. Cheung, George A.\n  Constantinides", "title": "LUTNet: Learning FPGA Configurations for Highly Efficient Neural Network\n  Inference", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.00938.\n  Accepted manuscript uploaded 02/03/20. DOA 01/03/20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that deep neural networks contain significant redundancy,\nand thus that high classification accuracy can be achieved even when weights\nand activations are quantized down to binary values. Network binarization on\nFPGAs greatly increases area efficiency by replacing resource-hungry\nmultipliers with lightweight XNOR gates. However, an FPGA's fundamental\nbuilding block, the K-LUT, is capable of implementing far more than an XNOR: it\ncan perform any K-input Boolean operation. Inspired by this observation, we\npropose LUTNet, an end-to-end hardware-software framework for the construction\nof area-efficient FPGA-based neural network accelerators using the native LUTs\nas inference operators. We describe the realization of both unrolled and tiled\nLUTNet architectures, with the latter facilitating smaller, less power-hungry\ndeployment over the former while sacrificing area and energy efficiency along\nwith throughput. For both varieties, we demonstrate that the exploitation of\nLUT flexibility allows for far heavier pruning than possible in prior works,\nresulting in significant area savings while achieving comparable accuracy.\nAgainst the state-of-the-art binarized neural network implementation, we\nachieve up to twice the area efficiency for several standard network models\nwhen inferencing popular datasets. We also demonstrate that even greater energy\nefficiency improvements are obtainable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:04:56 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 23:26:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wang", "Erwei", ""], ["Davis", "James J.", ""], ["Cheung", "Peter Y. K.", ""], ["Constantinides", "George A.", ""]]}, {"id": "1910.12626", "submitter": "Alisa Liu", "authors": "Alisa Liu, Prem Seetharaman, Bryan Pardo", "title": "Model selection for deep audio source separation via clustering analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio source separation is the process of separating a mixture (e.g. a pop\nband recording) into isolated sounds from individual sources (e.g. just the\nlead vocals). Deep learning models are the state-of-the-art in source\nseparation, given that the mixture to be separated is similar to the mixtures\nthe deep model was trained on. This requires the end user to know enough about\neach model's training to select the correct model for a given audio mixture. In\nthis work, we automate selection of the appropriate model for an audio mixture.\nWe present a confidence measure that does not require ground truth to estimate\nseparation quality, given a deep model and audio mixture. We use this\nconfidence measure to automatically select the model output with the best\npredicted separation quality. We compare our confidence-based ensemble approach\nto using individual models with no selection, to an oracle that always selects\nthe best model and to a random model selector. Results show our\nconfidence-based ensemble significantly outperforms the random ensemble over\ngeneral mixtures and approaches oracle performance for music mixtures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:09:31 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 19:28:28 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Alisa", ""], ["Seetharaman", "Prem", ""], ["Pardo", "Bryan", ""]]}, {"id": "1910.12647", "submitter": "Mehrad Moradshahi", "authors": "Mehrad Moradshahi, Hamid Palangi, Monica S. Lam, Paul Smolensky,\n  Jianfeng Gao", "title": "HUBERT Untangles BERT to Improve Transfer across NLP Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HUBERT which combines the structured-representational power of\nTensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional\nTransformer language model. We show that there is shared structure between\ndifferent NLP datasets that HUBERT, but not BERT, is able to learn and\nleverage. We validate the effectiveness of our model on the GLUE benchmark and\nHANS dataset. Our experiment results show that untangling data-specific\nsemantics from general language structure is key for better transfer among NLP\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 06:25:25 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 23:42:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Moradshahi", "Mehrad", ""], ["Palangi", "Hamid", ""], ["Lam", "Monica S.", ""], ["Smolensky", "Paul", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1910.12656", "submitter": "Telmo Silva Filho", "authors": "Meelis Kull, Miquel Perello-Nieto, Markus K\\\"angsepp, Telmo Silva\n  Filho, Hao Song, Peter Flach", "title": "Beyond temperature scaling: Obtaining well-calibrated multiclass\n  probabilities with Dirichlet calibration", "comments": "Accepted for presentation at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class probabilities predicted by most multiclass classifiers are\nuncalibrated, often tending towards over-confidence. With neural networks,\ncalibration can be improved by temperature scaling, a method to learn a single\ncorrective multiplicative factor for inputs to the last softmax layer. On\nnon-neural models the existing methods apply binary calibration in a pairwise\nor one-vs-rest fashion.\n  We propose a natively multiclass calibration method applicable to classifiers\nfrom any model class, derived from Dirichlet distributions and generalising the\nbeta calibration method from binary classification. It is easily implemented\nwith neural nets since it is equivalent to log-transforming the uncalibrated\nprobabilities, followed by one linear layer and softmax. Experiments\ndemonstrate improved probabilistic predictions according to multiple measures\n(confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of\ndatasets and classifiers. Parameters of the learned Dirichlet calibration map\nprovide insights to the biases in the uncalibrated model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 13:23:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kull", "Meelis", ""], ["Perello-Nieto", "Miquel", ""], ["K\u00e4ngsepp", "Markus", ""], ["Filho", "Telmo Silva", ""], ["Song", "Hao", ""], ["Flach", "Peter", ""]]}, {"id": "1910.12680", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Nur Sila Gulgec, Zheng Shi, Neil Deshmukh, Shamim Pakzad, and Martin\n  Tak\\'a\\v{c}", "title": "FD-Net with Auxiliary Time Steps: Fast Prediction of PDEs using\n  Hessian-Free Trust-Region Methods", "comments": "Paper accepted to NeurIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the underlying physical behavior of complex systems is a crucial,\nbut less well-understood topic in many engineering disciplines. This study\nproposes a finite-difference inspired convolutional neural network framework to\nlearn hidden partial differential equations from given data and iteratively\nestimate future dynamical behavior. The methodology designs the filter sizes\nsuch that they mimic the finite difference between the neighboring points. By\nlearning the governing equation, the network predicts the future evolution of\nthe solution by using only a few trainable parameters. In this paper, we\nprovide numerical results to compare the efficiency of the second-order\nTrust-Region Conjugate Gradient (TRCG) method with the first-order ADAM\noptimizer.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 13:50:57 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 01:55:23 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Gulgec", "Nur Sila", ""], ["Shi", "Zheng", ""], ["Deshmukh", "Neil", ""], ["Pakzad", "Shamim", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1910.12686", "submitter": "Daria Fokina", "authors": "Daria Fokina and Ivan Oseledets", "title": "Growing axons: greedy learning of neural networks with application to\n  function approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning deep neural network models that is based\non a greedy learning approach: we add one basis function at a time, and a new\nbasis function is generated as a non-linear activation function applied to a\nlinear combination of the previous basis functions. Such a method (growing deep\nneural network by one neuron at a time) allows us to compute much more accurate\napproximants for several model problems in function approximation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:00:04 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 20:48:09 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Fokina", "Daria", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1910.12697", "submitter": "Aditya Deshmukh", "authors": "Aditya Deshmukh, Srikrishna Bhashyam, Venugopal V. Veeravalli", "title": "Sequential Controlled Sensing for Composite Multihypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multi-hypothesis testing with controlled sensing of\nobservations is considered. The distribution of observations collected under\neach control is assumed to follow a single-parameter exponential family\ndistribution. The goal is to design a policy to find the true hypothesis with\nminimum expected delay while ensuring that the probability of error is below a\ngiven constraint. The decision-maker can control the delay by intelligently\nchoosing the control for observation collection in each time slot. We derive a\npolicy that satisfies the given constraint on the error probability. We also\nshow that the policy is asymptotically optimal in the sense that it\nasymptotically achieves an information-theoretic lower bound on the expected\ndelay.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 22:21:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Deshmukh", "Aditya", ""], ["Bhashyam", "Srikrishna", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1910.12714", "submitter": "Maxime Mouchet", "authors": "Maxime Mouchet (Lab-STICC, IMT Atlantique - INFO), Sandrine Vaton\n  (Lab-STICC, IMT Atlantique - INFO), Thierry Chonavel (Lab-STICC, IMT\n  Atlantique - SC), Emile Aben, Jasper den Hertog", "title": "Large-Scale Characterization and Segmentation of Internet Path Delays\n  with Infinite HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Round-Trip Times are one of the most commonly collected performance metrics\nin computer networks. Measurement platforms such as RIPE Atlas provide\nresearchers and network operators with an unprecedented amount of historical\nInternet delay measurements. It would be very useful to automate the processing\nof these measurements (statistical characterization of paths performance,\nchange detection, recognition of recurring patterns, etc.). Humans are pretty\ngood at finding patterns in network measurements but it can be difficult to\nautomate this to enable many time series being processed at the same time. In\nthis article we introduce a new model, the HDP-HMM or infinite hidden Markov\nmodel, whose performance in trace segmentation is very close to human\ncognition. This is obtained at the cost of a greater complexity and the\nambition of this article is to make the theory accessible to network monitoring\nand management researchers. We demonstrate that this model provides very\naccurate results on a labeled dataset and on RIPE Atlas and CAIDA MANIC data.\nThis method has been implemented in Atlas and we introduce the publicly\naccessible Web API.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:35:44 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mouchet", "Maxime", "", "Lab-STICC, IMT Atlantique - INFO"], ["Vaton", "Sandrine", "", "Lab-STICC, IMT Atlantique - INFO"], ["Chonavel", "Thierry", "", "Lab-STICC, IMT\n  Atlantique - SC"], ["Aben", "Emile", ""], ["Hertog", "Jasper den", ""]]}, {"id": "1910.12717", "submitter": "Christian Soize", "authors": "Christian Soize and Roger Ghanem", "title": "Sampling of Bayesian posteriors with a non-Gaussian probabilistic\n  learning on manifolds from a small dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the challenge presented by small-data to the task of\nBayesian inference. A novel methodology, based on manifold learning and\nmanifold sampling, is proposed for solving this computational statistics\nproblem under the following assumptions: 1) neither the prior model nor the\nlikelihood function are Gaussian and neither can be approximated by a Gaussian\nmeasure; 2) the number of functional input (system parameters) and functional\noutput (quantity of interest) can be large; 3) the number of available\nrealizations of the prior model is small, leading to the small-data challenge\ntypically associated with expensive numerical simulations; the number of\nexperimental realizations is also small; 4) the number of the posterior\nrealizations required for decision is much larger than the available initial\ndataset. The method and its mathematical aspects are detailed. Three\napplications are presented for validation: The first two involve mathematical\nconstructions aimed to develop intuition around the method and to explore its\nperformance. The third example aims to demonstrate the operational value of the\nmethod using a more complex application related to the statistical inverse\nidentification of the non-Gaussian matrix-valued random elasticity field of a\ndamaged biological tissue (osteoporosis in a cortical bone) using ultrasonic\nwaves.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:37:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Soize", "Christian", ""], ["Ghanem", "Roger", ""]]}, {"id": "1910.12735", "submitter": "Wenlin Wang", "authors": "Wenlin Wang, Hongteng Xu, Ruiyi Zhang, Wenqi Wang, Piyush Rai,\n  Lawrence Carin", "title": "Learning to Recommend from Sparse Data via Generative User Feedback", "comments": "To appear in AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional collaborative filtering (CF) based recommender systems tend to\nperform poorly when the user-item interactions/ratings are highly scarce. To\naddress this, we propose a learning framework that improves collaborative\nfiltering with a synthetic feedback loop (CF-SFL) to simulate the user\nfeedback. The proposed framework consists of a \"recommender\" and a \"virtual\nuser\". The \"recommender\" is formulated as a CF model, recommending items\naccording to observed user preference. The \"virtual user\" estimates rewards\nfrom the recommended items and generates a \\emph{feedback} in addition to the\nobserved user preference. The \"recommender\" connected with the \"virtual user\"\nconstructs a closed loop, that recommends users with items and imitates the\n\\emph{unobserved} feedback of the users to the recommended items. The synthetic\nfeedback is used to augment the observed user preference and improve\nrecommendation results. Theoretically, such model design can be interpreted as\ninverse reinforcement learning, which can be learned effectively via rollout\n(simulation). Experimental results show that the proposed framework is able to\nenrich the learning of user preference and boost the performance of existing\ncollaborative filtering methods on multiple datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 01:31:07 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:17:42 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wang", "Wenlin", ""], ["Xu", "Hongteng", ""], ["Zhang", "Ruiyi", ""], ["Wang", "Wenqi", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.12744", "submitter": "Saeed Saremi", "authors": "Saeed Saremi", "title": "On approximating $\\nabla f$ with neural networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a feedforward neural network $\\psi: \\mathbb{R}^d\\rightarrow\n\\mathbb{R}^d$ such that $\\psi\\approx \\nabla f$, where $f:\\mathbb{R}^d\n\\rightarrow \\mathbb{R}$ is a smooth function, therefore $\\psi$ must satisfy\n$\\partial_j \\psi_i = \\partial_i \\psi_j$ pointwise. We prove a theorem that a\n$\\psi$ network with more than one hidden layer can only represent one feature\nin its first hidden layer; this is a dramatic departure from the well-known\nresults for one hidden layer. The proof of the theorem is straightforward,\nwhere two backward paths and a weight-tying matrix play the key roles. We then\npresent the alternative, the implicit parametrization, where the neural network\nis $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and $\\nabla \\phi \\approx \\nabla\nf$; in addition, a \"soft analysis\" of $\\nabla \\phi$ gives a dual perspective on\nthe theorem. Throughout, we come back to recent probabilistic models that are\nformulated as $\\nabla \\phi \\approx \\nabla f$, and conclude with a critique of\ndenoising autoencoders.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:12:42 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 12:41:30 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Saremi", "Saeed", ""]]}, {"id": "1910.12748", "submitter": "Cheol Young Park", "authors": "Seung Joon Nam, Han Min Kim, Thomas Kang, Cheol Young Park", "title": "A Study of Machine Learning Models in Predicting the Intention of\n  Adolescents to Smoke Cigarettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of electronic cigarette (e-cigarette) is increasing among\nadolescents. This is problematic since consuming nicotine at an early age can\ncause harmful effects in developing teenager's brain and health. Additionally,\nthe use of e-cigarette has a possibility of leading to the use of cigarettes,\nwhich is more severe. There were many researches about e-cigarette and\ncigarette that mostly focused on finding and analyzing causes of smoking using\nconventional statistics. However, there is a lack of research on developing\nprediction models, which is more applicable to anti-smoking campaign, about\ne-cigarette and cigarette. In this paper, we research the prediction models\nthat can be used to predict an individual e-cigarette user's (including\nnon-e-cigarette users) intention to smoke cigarettes, so that one can be early\ninformed about the risk of going down the path of smoking cigarettes. To\nconstruct the prediction models, five machine learning (ML) algorithms are\nexploited and tested for their accuracy in predicting the intention to smoke\ncigarettes among never smokers using data from the 2018 National Youth Tobacco\nSurvey (NYTS). In our investigation, the Gradient Boosting Classifier, one of\nthe prediction models, shows the highest accuracy out of all the other models.\nAlso, with the best prediction model, we made a public website that enables\nusers to input information to predict their intentions of smoking cigarettes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:17:24 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 12:48:51 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Nam", "Seung Joon", ""], ["Kim", "Han Min", ""], ["Kang", "Thomas", ""], ["Park", "Cheol Young", ""]]}, {"id": "1910.12749", "submitter": "Rafael Rego Drumond", "authors": "Rafael Rego Drumond, Lukas Brinkmeyer, Josif Grabocka, Lars\n  Schmidt-Thieme", "title": "HIDRA: Head Initialization across Dynamic targets for Robust\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of gradient-based optimization strategies depends heavily on\nthe initial weights of the parametric model. Recent works show that there exist\nweight initializations from which optimization procedures can find the\ntask-specific parameters faster than from uniformly random initializations and\nthat such a weight initialization can be learned by optimizing a specific model\narchitecture across similar tasks via MAML (Model-Agnostic Meta-Learning).\nCurrent methods are limited to populations of classification tasks that share\nthe same number of classes due to the static model architectures used during\nmeta-learning. In this paper, we present HIDRA, a meta-learning approach that\nenables training and evaluating across tasks with any number of target\nvariables. We show that Model-Agnostic Meta-Learning trains a distribution for\nall the neurons in the output layer and a specific weight initialization for\nthe ones in the hidden layers. HIDRA explores this by learning one master\nneuron, which is used to initialize any number of output neurons for a new\ntask. Extensive experiments on the Miniimagenet and Omniglot data sets\ndemonstrate that HIDRA improves over standard approaches while generalizing to\ntasks with any number of target variables. Moreover, our approach is shown to\nrobustify low-capacity models in learning across complex tasks with a high\nnumber of classes for which regular MAML fails to learn any feasible\ninitialization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:18:19 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 17:14:15 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Drumond", "Rafael Rego", ""], ["Brinkmeyer", "Lukas", ""], ["Grabocka", "Josif", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1910.12756", "submitter": "Nikita Zhivotovskiy", "authors": "Olivier Bousquet, Nikita Zhivotovskiy", "title": "Fast classification rates without standard margin assumptions", "comments": "29 pages, 1 figure; presentation changed according to referees\n  suggestion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of learning rates for classes with finite\nVC dimension. It is well known that fast learning rates up to\n$O\\left(\\frac{d}{n}\\right)$ are achievable by the empirical risk minimization\nalgorithm (ERM) if low noise or margin assumptions are satisfied. These usually\nrequire the optimal Bayes classifier to be in the class, and it has been shown\nthat when this is not the case, the fast rates cannot be achieved even in the\nnoise free case. In this paper, we further investigate the question of the fast\nrates under the misspecification, when the Bayes classifier is not in the class\n(also called the agnostic setting).\n  First, we consider classification with a reject option, namely Chow's reject\noption model, and show that by slightly lowering the impact of hard instances,\na learning rate of order $O\\left(\\frac{d}{n}\\log \\frac{n}{d}\\right)$ is always\nachievable in the agnostic setting by a specific learning algorithm. Similar\nresults were only known under special versions of margin assumptions. We also\nshow that the performance of the proposed algorithm is never worse than the\nperformance of ERM.\n  Based on those results, we derive the necessary and sufficient conditions for\nclassification (without a reject option) with fast rates in the agnostic\nsetting achievable by improper learners. This simultaneously extends the work\nof Massart and N\\'{e}d\\'{e}lec (Ann. of Statistics, 2006), which studied this\nquestion in the case where the Bayesian optimal rule belongs to the class, and\nthe work of Ben-David and Urner (COLT, 2014), which allows the misspecification\nbut is limited to the no noise setting. Our result also provides the first\ngeneral setup in statistical learning theory in which an improper learning\nalgorithm may significantly improve the learning rate for non-convex losses.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:34:57 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 17:24:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bousquet", "Olivier", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "1910.12757", "submitter": "Aditya Mantha", "authors": "Aditya Mantha, Yokila Arora, Shubham Gupta, Praveenkumar Kanumala,\n  Zhiwei Liu, Stephen Guo, Kannan Achan", "title": "A Large-Scale Deep Architecture for Personalized Grocery Basket\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing consumer adoption of online grocery shopping through platforms\nsuch as Amazon Fresh, Instacart, and Walmart Grocery, there is a pressing\nbusiness need to provide relevant recommendations throughout the customer\njourney. In this paper, we introduce a production within-basket grocery\nrecommendation system, RTT2Vec, which generates real-time personalized product\nrecommendations to supplement the user's current grocery basket. We conduct\nextensive offline evaluation of our system and demonstrate a 9.4% uplift in\nprediction metrics over baseline state-of-the-art within-basket recommendation\nmodels. We also propose an approximate inference technique 11.6x times faster\nthan exact inference approaches. In production, our system has resulted in an\nincrease in average basket size, improved product discovery, and enabled faster\nuser check-out\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:54:55 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 18:24:28 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 02:03:18 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Mantha", "Aditya", ""], ["Arora", "Yokila", ""], ["Gupta", "Shubham", ""], ["Kanumala", "Praveenkumar", ""], ["Liu", "Zhiwei", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "1910.12760", "submitter": "Sofiene Jerbi", "authors": "Sofiene Jerbi, Lea M. Trenkwalder, Hendrik Poulsen Nautrup, Hans J.\n  Briegel, Vedran Dunjko", "title": "Quantum enhancements for deep reinforcement learning in large spaces", "comments": "Significant number of new analyses and results", "journal-ref": "PRX Quantum 2, 010328 (2021)", "doi": "10.1103/PRXQuantum.2.010328", "report-no": null, "categories": "quant-ph cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the field of quantum machine learning has drawn\nsignificant attention due to the prospect of bringing genuine computational\nadvantages to now widespread algorithmic methods. However, not all domains of\nmachine learning have benefited equally from quantum enhancements. Notably,\ndeep learning and reinforcement learning, despite their tremendous success in\nthe classical domain, both individually and combined, remain relatively\nunaddressed by the quantum community. Arguably, one reason behind this is the\nsystematic use in these domains of models and methods without prominent\ncomputational bottlenecks, leaving little room for quantum improvements. In\nthis work, we study the state-of-the-art neural-network approaches for\nreinforcement learning with quantum enhancements in mind. We demonstrate the\nsubstantial learning advantage that models with a sampling bottleneck can\nprovide over conventional neural network architectures in complex learning\nenvironments. These so-called energy-based models, like deep energy-based\nreinforcement learning, and deep projective simulation that we also introduce\nin this work, effectively allow to trade off learning performance for\nefficiency of computation. To alleviate the additional computational costs, we\npropose to leverage future and near-term quantum algorithms, resulting in\noverall more advantageous learning algorithms. This is achieved using\ncutting-edge and new quantum computing machinery to speed-up classical sampling\nmethods and by employing generalized models to gain an additional quantum\nadvantage.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:41:19 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 17:32:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jerbi", "Sofiene", ""], ["Trenkwalder", "Lea M.", ""], ["Nautrup", "Hendrik Poulsen", ""], ["Briegel", "Hans J.", ""], ["Dunjko", "Vedran", ""]]}, {"id": "1910.12774", "submitter": "George Chen", "authors": "Wei Ma, George H. Chen", "title": "Missing Not at Random in Matrix Completion: The Effectiveness of\n  Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption", "comments": "Advances in Neural Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is often applied to data with entries missing not at random\n(MNAR). For example, consider a recommendation system where users tend to only\nreveal ratings for items they like. In this case, a matrix completion method\nthat relies on entries being revealed at uniformly sampled row and column\nindices can yield overly optimistic predictions of unseen user ratings.\nRecently, various papers have shown that we can reduce this bias in MNAR matrix\ncompletion if we know the probabilities of different matrix entries being\nmissing. These probabilities are typically modeled using logistic regression or\nnaive Bayes, which make strong assumptions and lack guarantees on the accuracy\nof the estimated probabilities. In this paper, we suggest a simple approach to\nestimating these probabilities that avoids these shortcomings. Our approach\nfollows from the observation that missingness patterns in real data often\nexhibit low nuclear norm structure. We can then estimate the missingness\nprobabilities by feeding the (always fully-observed) binary matrix specifying\nwhich entries are revealed or missing to an existing nuclear-norm-constrained\nmatrix completion algorithm by Davenport et al. [2014]. Thus, we tackle MNAR\nmatrix completion by solving a different matrix completion problem first that\nrecovers missingness probabilities. We establish finite-sample error bounds for\nhow accurate these probability estimates are and how well these estimates\ndebias standard matrix completion losses for the original matrix to be\ncompleted. Our experiments show that the proposed debiasing strategy can\nimprove a variety of existing matrix completion algorithms, and achieves\ndownstream matrix completion accuracy at least as good as logistic regression\nand naive Bayes debiasing baselines that require additional auxiliary\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:01:47 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:25:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ma", "Wei", ""], ["Chen", "George H.", ""]]}, {"id": "1910.12778", "submitter": "Jiajin Li", "authors": "Jiajin Li, Sen Huang, Anthony Man-Cho So", "title": "A First-Order Algorithmic Framework for Wasserstein Distributionally\n  Robust Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein distance-based distributionally robust optimization (DRO) has\nreceived much attention lately due to its ability to provide a robustness\ninterpretation of various learning models. Moreover, many of the DRO problems\nthat arise in the learning context admits exact convex reformulations and hence\ncan be tackled by off-the-shelf solvers. Nevertheless, the use of such solvers\nseverely limits the applicability of DRO in large-scale learning problems, as\nthey often rely on general purpose interior-point algorithms. On the other\nhand, there are very few works that attempt to develop fast iterative methods\nto solve these DRO problems, which typically possess complicated structures. In\nthis paper, we take a first step towards resolving the above difficulty by\ndeveloping a first-order algorithmic framework for tackling a class of\nWasserstein distance-based distributionally robust logistic regression (DRLR)\nproblem. Specifically, we propose a novel linearized proximal ADMM to solve the\nDRLR problem, whose objective is convex but consists of a smooth term plus two\nnon-separable non-smooth terms. We prove that our method enjoys a sublinear\nconvergence rate. Furthermore, we conduct three different experiments to show\nits superb performance on both synthetic and real-world datasets. In\nparticular, our method can achieve the same accuracy up to 800+ times faster\nthan the standard off-the-shelf solver.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:03:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Jiajin", ""], ["Huang", "Sen", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1910.12783", "submitter": "Lingzhou Hong", "authors": "Lingzhou Hong, Alfredo Garcia, and Ceyhun Eksin", "title": "Distributed Networked Learning with Correlated Data", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider a distributed estimation method in a setting with heterogeneous\nstreams of correlated data distributed across nodes in a network. In the\nconsidered approach, linear models are estimated locally (i.e., with only local\ndata) subject to a network regularization term that penalizes a local model\nthat differs from neighboring models. We analyze computation dynamics\n(associated with stochastic gradient updates) and information exchange\n(associated with exchanging current models with neighboring nodes). We provide\na finite-time characterization of convergence of the weighted ensemble average\nestimate and compare this result to federated learning, an alternative approach\nto estimation wherein a single model is updated by locally generated gradient\nupdates. This comparison highlights the trade-off between speed vs precision:\nwhile model updates take place at a faster rate in federated learning, the\nproposed networked approach to estimation enables the identification of models\nwith higher precision. We illustrate the method's general applicability in two\nexamples: estimating a Markov random field using wireless sensor networks and\nmodeling prey escape behavior of flocking birds based on a publicly available\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:14:02 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 23:38:45 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Hong", "Lingzhou", ""], ["Garcia", "Alfredo", ""], ["Eksin", "Ceyhun", ""]]}, {"id": "1910.12794", "submitter": "Dilin Wang", "authors": "Dilin Wang, Ziyang Tang, Chandrajit Bajaj, Qiang Liu", "title": "Stein Variational Gradient Descent With Matrix-Valued Kernels", "comments": "Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient descent (SVGD) is a particle-based inference\nalgorithm that leverages gradient information for efficient approximate\ninference. In this work, we enhance SVGD by leveraging preconditioning\nmatrices, such as the Hessian and Fisher information matrix, to incorporate\ngeometric information into SVGD updates. We achieve this by presenting a\ngeneralization of SVGD that replaces the scalar-valued kernels in vanilla SVGD\nwith more general matrix-valued kernels. This yields a significant extension of\nSVGD, and more importantly, allows us to flexibly incorporate various\npreconditioning matrices to accelerate the exploration in the probability\nlandscape. Empirical results show that our method outperforms vanilla SVGD and\na variety of baseline approaches over a range of real-world Bayesian inference\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:43:48 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:54:26 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wang", "Dilin", ""], ["Tang", "Ziyang", ""], ["Bajaj", "Chandrajit", ""], ["Liu", "Qiang", ""]]}, {"id": "1910.12795", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom Mitchell, Eric P.\n  Xing", "title": "Learning Data Manipulation for Augmentation and Weighting", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating data, such as weighting data examples or augmenting with new\ninstances, has been increasingly used to improve model training. Previous work\nhas studied various rule- or learning-based approaches designed for specific\ntypes of data manipulation. In this work, we propose a new method that supports\nlearning different manipulation schemes with the same gradient-based algorithm.\nOur approach builds upon a recent connection of supervised learning and\nreinforcement learning (RL), and adapts an off-the-shelf reward learning\nalgorithm from RL for joint data manipulation learning and model training.\nDifferent parameterization of the \"data reward\" function instantiates different\nmanipulation schemes. We showcase data augmentation that learns a text\ntransformation network, and data weighting that dynamically adapts the data\nsample importance. Experiments show the resulting algorithms significantly\nimprove the image and text classification performance in low data regime and\nclass-imbalance problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:46:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hu", "Zhiting", ""], ["Tan", "Bowen", ""], ["Salakhutdinov", "Ruslan", ""], ["Mitchell", "Tom", ""], ["Xing", "Eric P.", ""]]}, {"id": "1910.12799", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki and Atsushi Nitanda", "title": "Deep learning is adaptive to intrinsic dimensionality of model\n  smoothness in anisotropic Besov space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has exhibited superior performance for various tasks,\nespecially for high-dimensional datasets, such as images. To understand this\nproperty, we investigate the approximation and estimation ability of deep\nlearning on {\\it anisotropic Besov spaces}. The anisotropic Besov space is\ncharacterized by direction-dependent smoothness and includes several function\nclasses that have been investigated thus far. We demonstrate that the\napproximation error and estimation error of deep learning only depend on the\naverage value of the smoothness parameters in all directions. Consequently, the\ncurse of dimensionality can be avoided if the smoothness of the target function\nis highly anisotropic. Unlike existing studies, our analysis does not require a\nlow-dimensional structure of the input data. We also investigate the minimax\noptimality of deep learning and compare its performance with that of the kernel\nmethod (more generally, linear estimators). The results show that deep learning\nhas better dependence on the input dimensionality if the target function\npossesses anisotropic smoothness, and it achieves an adaptive rate for\nfunctions with spatially inhomogeneous smoothness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:52:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Suzuki", "Taiji", ""], ["Nitanda", "Atsushi", ""]]}, {"id": "1910.12800", "submitter": "Xing Zhao", "authors": "Xing Zhao, Ping Lu, Yanyan Zhang, Jianxiong Chen, and Xiaoyang Li", "title": "Attenuating Random Noise in Seismic Data by a Deep Learning Approach", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the geophysical field, seismic noise attenuation has been considered as a\ncritical and long-standing problem, especially for the pre-stack data\nprocessing. Here, we propose a model to leverage the deep-learning model for\nthis task. Rather than directly applying an existing de-noising model from\nordinary images to the seismic data, we have designed a particular\ndeep-learning model, based on residual neural networks. It is named as\nN2N-Seismic, which has a strong ability to recover the seismic signals back to\nintact condition with the preservation of primary signals. The proposed model,\nachieving with great success in attenuating noise, has been tested on two\ndifferent seismic datasets. Several metrics show that our method outperforms\nconventional approaches in terms of Signal-to-Noise-Ratio, Mean-Squared-Error,\nPhase Spectrum, etc. Moreover, robust tests in terms of effectively removing\nrandom noise from any dataset with strong and weak noises have been extensively\nscrutinized in making sure that the proposed model is able to maintain a good\nlevel of adaptation while dealing with large variations of noise\ncharacteristics and intensities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:53:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Xing", ""], ["Lu", "Ping", ""], ["Zhang", "Yanyan", ""], ["Chen", "Jianxiong", ""], ["Li", "Xiaoyang", ""]]}, {"id": "1910.12806", "submitter": "Jinoh Kim", "authors": "Makiya Nakashima, Alex Sim, Youngsoo Kim, Jonghyun Kim, Jinoh Kim", "title": "An Ensemble Approach toward Automated Variable Selection for Network\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While variable selection is essential to optimize the learning complexity by\nprioritizing features, automating the selection process is preferred since it\nrequires laborious efforts with intensive analysis otherwise. However, it is\nnot an easy task to enable the automation due to several reasons. First,\nselection techniques often need a condition to terminate the reduction process,\nfor example, by using a threshold or the number of features to stop, and\nsearching an adequate stopping condition is highly challenging. Second, it is\nuncertain that the reduced variable set would work well; our preliminary\nexperimental result shows that well-known selection techniques produce\ndifferent sets of variables as a result of reduction (even with the same\ntermination condition), and it is hard to estimate which of them would work the\nbest in future testing. In this paper, we demonstrate the potential power of\nour approach to the automation of selection process that incorporates\nwell-known selection methods identifying important variables. Our experimental\nresults with two public network traffic data (UNSW-NB15 and IDS2017) show that\nour proposed method identifies a small number of core variables, with which it\nis possible to approximate the performance to the one with the entire\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:03:02 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Nakashima", "Makiya", ""], ["Sim", "Alex", ""], ["Kim", "Youngsoo", ""], ["Kim", "Jonghyun", ""], ["Kim", "Jinoh", ""]]}, {"id": "1910.12807", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek, Quan Vuong, Robert Loftin, Katja Hofmann", "title": "Better Exploration with Optimistic Actor-Critic", "comments": "20 pages (including supplement)", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor-critic methods, a type of model-free Reinforcement Learning, have been\nsuccessfully applied to challenging tasks in continuous control, often\nachieving state-of-the art performance. However, wide-scale adoption of these\nmethods in real-world domains is made difficult by their poor sample\nefficiency. We address this problem both theoretically and empirically. On the\ntheoretical side, we identify two phenomena preventing efficient exploration in\nexisting state-of-the-art algorithms such as Soft Actor Critic. First,\ncombining a greedy actor update with a pessimistic estimate of the critic leads\nto the avoidance of actions that the agent does not know about, a phenomenon we\ncall pessimistic underexploration. Second, current algorithms are directionally\nuninformed, sampling actions with equal probability in opposite directions from\nthe current mean. This is wasteful, since we typically need actions taken along\ncertain directions much more than others. To address both of these phenomena,\nwe introduce a new algorithm, Optimistic Actor Critic, which approximates a\nlower and upper confidence bound on the state-action value function. This\nallows us to apply the principle of optimism in the face of uncertainty to\nperform directed exploration using the upper bound while still using the lower\nbound to avoid overestimation. We evaluate OAC in several challenging\ncontinuous control tasks, achieving state-of the art sample efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:06:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ciosek", "Kamil", ""], ["Vuong", "Quan", ""], ["Loftin", "Robert", ""], ["Hofmann", "Katja", ""]]}, {"id": "1910.12809", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara, Jiawei Huang, Nan Jiang", "title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical investigations into off-policy evaluation in\nreinforcement learning using function approximators for (marginalized)\nimportance weights and value functions. Our contributions include: (1) A new\nestimator, MWL, that directly estimates importance ratios over the state-action\ndistributions, removing the reliance on knowledge of the behavior policy as in\nprior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by\nswapping the roles of importance weights and value-functions in MWL. MQL has an\nintuitive interpretation of minimizing average Bellman errors and can be\ncombined with MWL in a doubly robust manner. (3) Several additional results\nthat offer further insights into these methods, including the sample complexity\nanalyses of MWL and MQL, their asymptotic optimality in the tabular setting,\nhow the learned importance weights depend the choice of the discriminator\nclass, and how our methods provide a unified view of some old and new\nalgorithms in RL.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:08:25 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 21:21:33 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 01:49:19 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 23:04:03 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Huang", "Jiawei", ""], ["Jiang", "Nan", ""]]}, {"id": "1910.12815", "submitter": "Kimia Nadjahi", "authors": "Kimia Nadjahi, Valentin De Bortoli, Alain Durmus, Roland Badeau, Umut\n  \\c{S}im\\c{s}ekli", "title": "Approximate Bayesian Computation with the Sliced-Wasserstein Distance", "comments": "Accepted at ICASSP 2020 (publication and oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a popular method for approximate\ninference in generative models with intractable but easy-to-sample likelihood.\nIt constructs an approximate posterior distribution by finding parameters for\nwhich the simulated data are close to the observations in terms of summary\nstatistics. These statistics are defined beforehand and might induce a loss of\ninformation, which has been shown to deteriorate the quality of the\napproximation. To overcome this problem, Wasserstein-ABC has been recently\nproposed, and compares the datasets via the Wasserstein distance between their\nempirical distributions, but does not scale well to the dimension or the number\nof samples. We propose a new ABC technique, called Sliced-Wasserstein ABC and\nbased on the Sliced-Wasserstein distance, which has better computational and\nstatistical properties. We derive two theoretical results showing the\nasymptotical consistency of our approach, and we illustrate its advantages on\nsynthetic data and an image denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:18:25 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:09:02 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Nadjahi", "Kimia", ""], ["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Badeau", "Roland", ""], ["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1910.12819", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna\n  Narayanan, Mingyuan Zhou, Xiaoning Qian", "title": "Semi-Implicit Stochastic Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic recurrent neural networks with latent random variables of complex\ndependency structures have shown to be more successful in modeling sequential\ndata than deterministic deep models. However, the majority of existing methods\nhave limited expressive power due to the Gaussian assumption of latent\nvariables. In this paper, we advocate learning implicit latent representations\nusing semi-implicit variational inference to further increase model\nflexibility. Semi-implicit stochastic recurrent neural network(SIS-RNN) is\ndeveloped to enrich inferred model posteriors that may have no analytic density\nfunctions, as long as independent random samples can be generated via\nreparameterization. Extensive experiments in different tasks on real-world\ndatasets show that SIS-RNN outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:24:41 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 03:02:25 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Hasanzadeh", "Arman", ""], ["Duffield", "Nick", ""], ["Narayanan", "Krishna", ""], ["Zhou", "Mingyuan", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1910.12824", "submitter": "J\\\"org Franke", "authors": "J\\\"org K.H. Franke, Gregor K\\\"ohler, Noor Awad, Frank Hutter", "title": "Neural Architecture Evolution in Deep Reinforcement Learning for\n  Continuous Control", "comments": "NeurIPS 2019 MetaLearn Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Deep Reinforcement Learning algorithms still heavily rely on\nhandcrafted neural network architectures. We propose a novel approach to\nautomatically find strong topologies for continuous control tasks while only\nadding a minor overhead in terms of interactions in the environment. To achieve\nthis, we combine Neuroevolution techniques with off-policy training and propose\na novel architecture mutation operator. Experiments on five continuous control\nbenchmarks show that the proposed Actor-Critic Neuroevolution algorithm often\noutperforms the strong Actor-Critic baseline and is capable of automatically\nfinding topologies in a sample-efficient manner which would otherwise have to\nbe found by expensive architecture search.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:33:26 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 21:38:04 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 11:54:44 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Franke", "J\u00f6rg K. H.", ""], ["K\u00f6hler", "Gregor", ""], ["Awad", "Noor", ""], ["Hutter", "Frank", ""]]}, {"id": "1910.12827", "submitter": "Michael Chang", "authors": "Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner,\n  Chelsea Finn, Jiajun Wu, Joshua B. Tenenbaum, Sergey Levine", "title": "Entity Abstraction in Visual Model-Based Reinforcement Learning", "comments": "Accepted at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tests the hypothesis that modeling a scene in terms of entities\nand their local interactions, as opposed to modeling the scene globally,\nprovides a significant benefit in generalizing to physical tasks in a\ncombinatorial space the learner has not encountered before. We present\nobject-centric perception, prediction, and planning (OP3), which to the best of\nour knowledge is the first fully probabilistic entity-centric dynamic latent\nvariable framework for model-based reinforcement learning that acquires entity\nrepresentations from raw visual observations without supervision and uses them\nto predict and plan. OP3 enforces entity-abstraction -- symmetric processing of\neach entity representation with the same locally-scoped function -- which\nenables it to scale to model different numbers and configurations of objects\nfrom those in training. Our approach to solving the key technical challenge of\ngrounding these entity representations to actual objects in the environment is\nto frame this variable binding problem as an inference problem, and we develop\nan interactive inference algorithm that uses temporal continuity and\ninteractive feedback to bind information about object properties to the entity\nvariables. On block-stacking tasks, OP3 generalizes to novel block\nconfigurations and more objects than observed during training, outperforming an\noracle model that assumes access to object supervision and achieving two to\nthree times better accuracy than a state-of-the-art video prediction model that\ndoes not exhibit entity abstraction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:37:46 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:57:33 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 23:16:31 GMT"}, {"version": "v4", "created": "Sat, 11 Apr 2020 19:50:41 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 14:51:15 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Veerapaneni", "Rishi", ""], ["Co-Reyes", "John D.", ""], ["Chang", "Michael", ""], ["Janner", "Michael", ""], ["Finn", "Chelsea", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.12832", "submitter": "Kanthi Sarpatwar", "authors": "Kanthi Sarpatwar and Karthikeyan Shanmugam and Venkata\n  Sitaramagiridharganesh Ganapavarapu and Ashish Jagmohan and Roman Vaculin", "title": "Differentially Private Distributed Data Summarization under Covariate\n  Shift", "comments": "To appear in the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision AI marketplaces to be platforms where consumers, with very less\ndata for a target task, can obtain a relevant model by accessing many private\ndata sources with vast number of data samples. One of the key challenges is to\nconstruct a training dataset that matches a target task without compromising on\nprivacy of the data sources. To this end, we consider the following distributed\ndata summarizataion problem. Given K private source datasets denoted by\n$[D_i]_{i\\in [K]}$ and a small target validation set $D_v$, which may involve a\nconsiderable covariate shift with respect to the sources, compute a summary\ndataset $D_s\\subseteq \\bigcup_{i\\in [K]} D_i$ such that its statistical\ndistance from the validation dataset $D_v$ is minimized. We use the popular\nMaximum Mean Discrepancy as the measure of statistical distance. The\nnon-private problem has received considerable attention in prior art, for\nexample in prototype selection (Kim et al., NIPS 2016). Our work is the first\nto obtain strong differential privacy guarantees while ensuring the quality\nguarantees of the non-private version. We study this problem in a Parsimonious\nCurator Privacy Model, where a trusted curator coordinates the summarization\nprocess while minimizing the amount of private information accessed. Our\ncentral result is a novel protocol that (a) ensures the curator accesses at\nmost $O(K^{\\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees\non the leakage of information between the data owners and (c) closely matches\nthe best known non-private greedy algorithm. Our protocol uses two hash\nfunctions, one inspired by the Rahimi-Recht random features method and the\nsecond leverages state of the art differential privacy mechanisms. We introduce\na novel \"noiseless\" differentially private auctioning protocol for winner\nnotification and demonstrate the efficacy of our protocol using real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:45:12 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:47:05 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Sarpatwar", "Kanthi", ""], ["Shanmugam", "Karthikeyan", ""], ["Ganapavarapu", "Venkata Sitaramagiridharganesh", ""], ["Jagmohan", "Ashish", ""], ["Vaculin", "Roman", ""]]}, {"id": "1910.12837", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan, Roman Vershynin", "title": "Online Stochastic Gradient Descent with Arbitrary Initialization Solves\n  Non-smooth, Non-convex Phase Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NA math.IT math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent literature, a general two step procedure has been formulated for\nsolving the problem of phase retrieval. First, a spectral technique is used to\nobtain a constant-error initial estimate, following which, the estimate is\nrefined to arbitrary precision by first-order optimization of a non-convex loss\nfunction. Numerical experiments, however, seem to suggest that simply running\nthe iterative schemes from a random initialization may also lead to\nconvergence, albeit at the cost of slightly higher sample complexity. In this\npaper, we prove that, in fact, constant step size online stochastic gradient\ndescent (SGD) converges from arbitrary initializations for the non-smooth,\nnon-convex amplitude squared loss objective. In this setting, online SGD is\nalso equivalent to the randomized Kaczmarz algorithm from numerical analysis.\nOur analysis can easily be generalized to other single index models. It also\nmakes use of new ideas from stochastic process theory, including the notion of\na summary state space, which we believe will be of use for the broader field of\nnon-convex optimization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:46:49 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Tan", "Yan Shuo", ""], ["Vershynin", "Roman", ""]]}, {"id": "1910.12853", "submitter": "Shiyu Chang", "authors": "Shiyu Chang, Yang Zhang, Mo Yu, Tommi S. Jaakkola", "title": "A Game Theoretic Approach to Class-wise Selective Rationalization", "comments": "Accepted by Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection of input features such as relevant pieces of text has become a\ncommon technique of highlighting how complex neural predictors operate. The\nselection can be optimized post-hoc for trained models or incorporated directly\ninto the method itself (self-explaining). However, an overall selection does\nnot properly capture the multi-faceted nature of useful rationales such as pros\nand cons for decisions. To this end, we propose a new game theoretic approach\nto class-dependent rationalization, where the method is specifically trained to\nhighlight evidence supporting alternative conclusions. Each class involves\nthree players set up competitively to find evidence for factual and\ncounterfactual scenarios. We show theoretically in a simplified scenario how\nthe game drives the solution towards meaningful class-dependent rationales. We\nevaluate the method in single- and multi-aspect sentiment classification tasks\nand demonstrate that the proposed method is able to identify both factual\n(justifying the ground truth label) and counterfactual (countering the ground\ntruth label) rationales consistent with human rationalization. The code for our\nmethod is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:59:02 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Yu", "Mo", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1910.12854", "submitter": "Keith Burghardt", "authors": "Yuzi He and Keith Burghardt and Kristina Lerman", "title": "Learning Fair and Interpretable Representations via Linear\n  Orthogonalization", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce human error and prejudice, many high-stakes decisions have been\nturned over to machine algorithms. However, recent research suggests that this\ndoes not remove discrimination, and can perpetuate harmful stereotypes. While\nalgorithms have been developed to improve fairness, they typically face at\nleast one of three shortcomings: they are not interpretable, their prediction\nquality deteriorates quickly compared to unbiased equivalents, and they are not\neasily transferable across models. To address these shortcomings, we propose a\ngeometric method that removes correlations between data and any number of\nprotected variables. Further, we can control the strength of debiasing through\nan adjustable parameter to address the trade-off between prediction quality and\nfairness. The resulting features are interpretable and can be used with many\npopular models, such as linear regression, random forest, and multilayer\nperceptrons. The resulting predictions are found to be more accurate and fair\ncompared to several state-of-the-art fair AI algorithms across a variety of\nbenchmark datasets. Our work shows that debiasing data is a simple and\neffective solution toward improving fairness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:59:31 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:59:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["He", "Yuzi", ""], ["Burghardt", "Keith", ""], ["Lerman", "Kristina", ""]]}, {"id": "1910.12892", "submitter": "Douwe Kiela", "authors": "Qi Liu, Maximilian Nickel, Douwe Kiela", "title": "Hyperbolic Graph Neural Networks", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from graph-structured data is an important task in machine learning\nand artificial intelligence, for which Graph Neural Networks (GNNs) have shown\ngreat promise. Motivated by recent advances in geometric representation\nlearning, we propose a novel GNN architecture for learning representations on\nRiemannian manifolds with differentiable exponential and logarithmic maps. We\ndevelop a scalable algorithm for modeling the structural properties of graphs,\ncomparing Euclidean and hyperbolic geometry. In our experiments, we show that\nhyperbolic GNNs can lead to substantial improvements on various benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:01:10 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Qi", ""], ["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1910.12911", "submitter": "Maximilian Igl", "authors": "Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek,\n  Cheng Zhang, Sam Devlin, Katja Hofmann", "title": "Generalization in Reinforcement Learning with Selective Noise Injection\n  and Information Bottleneck", "comments": "Published at Neurips 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for policies to generalize to new environments is key to the\nbroad application of RL agents. A promising approach to prevent an agent's\npolicy from overfitting to a limited set of training environments is to apply\nregularization techniques originally developed for supervised learning.\nHowever, there are stark differences between supervised learning and RL. We\ndiscuss those differences and propose modifications to existing regularization\ntechniques in order to better adapt them to RL. In particular, we focus on\nregularization techniques relying on the injection of noise into the learned\nfunction, a family that includes some of the most widely used approaches such\nas Dropout and Batch Normalization. To adapt them to RL, we propose Selective\nNoise Injection (SNI), which maintains the regularizing effect the injected\nnoise has, while mitigating the adverse effects it has on the gradient quality.\nFurthermore, we demonstrate that the Information Bottleneck (IB) is a\nparticularly well suited regularization technique for RL as it is effective in\nthe low-data regime encountered early on in training RL agents. Combining the\nIB with SNI, we significantly outperform current state of the art results,\nincluding on the recently proposed generalization benchmark Coinrun.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:51:54 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Igl", "Maximilian", ""], ["Ciosek", "Kamil", ""], ["Li", "Yingzhen", ""], ["Tschiatschek", "Sebastian", ""], ["Zhang", "Cheng", ""], ["Devlin", "Sam", ""], ["Hofmann", "Katja", ""]]}, {"id": "1910.12913", "submitter": "Hafiz Imtiaz", "authors": "Hafiz Imtiaz, Jafar Mohammadi, Rogers Silva, Bradley Baker, Sergey M.\n  Plis, Anand D. Sarwate, Vince Calhoun", "title": "Improved Differentially Private Decentralized Source Separation for fMRI\n  Data", "comments": "\\c{opyright} 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works. arXiv admin note: text overlap with\n  arXiv:1904.10059", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation algorithms such as independent component analysis\n(ICA) are widely used in the analysis of neuroimaging data. In order to\nleverage larger sample sizes, different data holders/sites may wish to\ncollaboratively learn feature representations. However, such datasets are often\nprivacy-sensitive, precluding centralized analyses that pool the data at a\nsingle site. In this work, we propose a differentially private algorithm for\nperforming ICA in a decentralized data setting. Conventional approaches to\ndecentralized differentially private algorithms may introduce too much noise\ndue to the typically small sample sizes at each site. We propose a novel\nprotocol that uses correlated noise to remedy this problem. We show that our\nalgorithm outperforms existing approaches on synthetic and real neuroimaging\ndatasets and demonstrate that it can sometimes reach the same level of utility\nas the corresponding non-private algorithm. This indicates that it is possible\nto have meaningful utility while preserving privacy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:57:57 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:55:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Imtiaz", "Hafiz", ""], ["Mohammadi", "Jafar", ""], ["Silva", "Rogers", ""], ["Baker", "Bradley", ""], ["Plis", "Sergey M.", ""], ["Sarwate", "Anand D.", ""], ["Calhoun", "Vince", ""]]}, {"id": "1910.12933", "submitter": "Rex Ying", "authors": "Ines Chami, Rex Ying, Christopher R\\'e, Jure Leskovec", "title": "Hyperbolic Graph Convolutional Neural Networks", "comments": "Published at Conference NeurIPS 2019. First 2 authors have equal\n  contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks (GCNs) embed nodes in a graph into\nEuclidean space, which has been shown to incur a large distortion when\nembedding real-world graphs with scale-free or hierarchical structure.\nHyperbolic geometry offers an exciting alternative, as it enables embeddings\nwith much smaller distortion. However, extending GCNs to hyperbolic geometry\npresents several unique challenges because it is not clear how to define neural\nnetwork operations, such as feature transformation and aggregation, in\nhyperbolic space. Furthermore, since input features are often Euclidean, it is\nunclear how to transform the features into hyperbolic embeddings with the right\namount of curvature. Here we propose Hyperbolic Graph Convolutional Neural\nNetwork (HGCN), the first inductive hyperbolic GCN that leverages both the\nexpressiveness of GCNs and hyperbolic geometry to learn inductive node\nrepresentations for hierarchical and scale-free graphs. We derive GCN\noperations in the hyperboloid model of hyperbolic space and map Euclidean input\nfeatures to embeddings in hyperbolic spaces with different trainable curvature\nat each layer. Experiments demonstrate that HGCN learns embeddings that\npreserve hierarchical structure, and leads to improved performance when\ncompared to Euclidean analogs, even with very low dimensional embeddings:\ncompared to state-of-the-art GCNs, HGCN achieves an error reduction of up to\n63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node\nclassification, also improving state-of-the art on the Pubmed dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:41:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chami", "Ines", ""], ["Ying", "Rex", ""], ["R\u00e9", "Christopher", ""], ["Leskovec", "Jure", ""]]}, {"id": "1910.12937", "submitter": "Fan Chen", "authors": "Fan Chen, Yini Zhang, and Karl Rohe", "title": "Targeted sampling from massive block model graphs with personalized\n  PageRank", "comments": "61 pages, 5 figures", "journal-ref": "J. R. Stat. Soc. B (2020), 82: 99-126", "doi": "10.1111/rssb.12349", "report-no": null, "categories": "cs.SI cs.CC cs.DL stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper provides statistical theory and intuition for personalized PageRank\n(called \"PPR\"): a popular technique that samples a small community from a\nmassive network. We study a setting where the entire network is expensive to\nobtain thoroughly or to maintain, but we can start from a seed node of interest\nand \"crawl\" the network to find other nodes through their connections. By\ncrawling the graph in a designed way, the PPR vector can be approximated\nwithout querying the entire massive graph, making it an alternative to snowball\nsampling. Using the degree-corrected stochastic block model, we study whether\nthe PPR vector can select nodes that belong to the same block as the seed node.\nWe provide a simple and interpretable form for the PPR vector, highlighting its\nbiases towards high degree nodes outside the target block. We examine a simple\nadjustment based on node degrees and establish consistency results for PPR\nclustering that allows for directed graphs. These results are enabled by recent\ntechnical advances showing the elementwise convergence of eigenvectors. We\nillustrate the method with the massive Twitter friendship graph, which we crawl\nby using the Twitter application programming interface. We find that the\nadjusted and unadjusted PPR techniques are complementary approaches, where the\nadjustment makes the results particularly localized around the seed node, and\nthat the bias adjustment greatly benefits from degree regularization.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:48:57 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 17:07:19 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Zhang", "Yini", ""], ["Rohe", "Karl", ""]]}, {"id": "1910.12939", "submitter": "Monisha Yuvaraj", "authors": "Umar Islambekov, Monisha Yuvaraj and Yulia R. Gel", "title": "Harnessing the power of Topological Data Analysis to detect change\n  points in time series", "comments": "11 pages, 3 Figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel geometry-oriented methodology, based on the emerging\ntools of topological data analysis, into the change point detection framework.\nThe key rationale is that change points are likely to be associated with\nchanges in geometry behind the data generating process. While the applications\nof topological data analysis to change point detection are potentially very\nbroad, in this paper we primarily focus on integrating topological concepts\nwith the existing nonparametric methods for change point detection. In\nparticular, the proposed new geometry-oriented approach aims to enhance\ndetection accuracy of distributional regime shift locations. Our simulation\nstudies suggest that integration of topological data analysis with some\nexisting algorithms for change point detection leads to consistently more\naccurate detection results. We illustrate our new methodology in application to\nthe two closely related environmental time series datasets -ice phenology of\nthe Lake Baikal and the North Atlantic Oscillation indices, in a research query\nfor a possible association between their estimated regime shift locations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:52:23 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Islambekov", "Umar", ""], ["Yuvaraj", "Monisha", ""], ["Gel", "Yulia R.", ""]]}, {"id": "1910.12944", "submitter": "Justin Leo", "authors": "Justin Leo, Jugal Kalita", "title": "Moving Towards Open Set Incremental Learning: Readily Discovering New\n  Authors", "comments": "Accepted to Future of Information and Communication Conference (FICC)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of textual data often yields important information. Most\nclassifiers work in a closed world setting where the classifier is trained on a\nknown corpus, and then it is tested on unseen examples that belong to one of\nthe classes seen during training. Despite the usefulness of this design, often\nthere is a need to classify unseen examples that do not belong to any of the\nclasses on which the classifier was trained. This paper describes the open set\nscenario where unseen examples from previously unseen classes are handled while\ntesting. This further examines a process of enhanced open set classification\nwith a deep neural network that discovers new classes by clustering the\nexamples identified as belonging to unknown classes, followed by a process of\nretraining the classifier with newly recognized classes. Through this process\nthe model moves to an incremental learning model where it continuously finds\nand learns from novel classes of data that have been identified automatically.\nThis paper also develops a new metric that measures multiple attributes of\nclustering open set data. Multiple experiments across two author attribution\ndata sets demonstrate the creation an incremental model that produces excellent\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:01:54 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Leo", "Justin", ""], ["Kalita", "Jugal", ""]]}, {"id": "1910.12947", "submitter": "Minshuo Chen", "authors": "Minshuo Chen, Xingguo Li, Tuo Zhao", "title": "On Generalization Bounds of a Family of Recurrent Neural Networks", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely applied to sequential data\nanalysis. Due to their complicated modeling structures, however, the theory\nbehind is still largely missing. To connect theory and practice, we study the\ngeneralization properties of vanilla RNNs as well as their variants, including\nMinimal Gated Unit (MGU), Long Short Term Memory (LSTM), and Convolutional\n(Conv) RNNs. Specifically, our theory is established under the PAC-Learning\nframework. The generalization bound is presented in terms of the spectral norms\nof the weight matrices and the total number of parameters. We also establish\nrefined generalization bounds with additional norm assumptions, and draw a\ncomparison among these bounds. We remark: (1) Our generalization bound for\nvanilla RNNs is significantly tighter than the best of existing results; (2) We\nare not aware of any other generalization bounds for MGU, LSTM, and Conv RNNs\nin the exiting literature; (3) We demonstrate the advantages of these variants\nin generalization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:12:16 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 03:15:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chen", "Minshuo", ""], ["Li", "Xingguo", ""], ["Zhao", "Tuo", ""]]}, {"id": "1910.12948", "submitter": "Gilles Vandewiele", "authors": "Gilles Vandewiele and Femke Ongenae and Filip De Turck", "title": "GENDIS: GENetic DIscovery of Shapelets", "comments": null, "journal-ref": null, "doi": "10.3390/s21041059", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the time series classification domain, shapelets are small time series\nthat are discriminative for a certain class. It has been shown that classifiers\nare able to achieve state-of-the-art results on a plethora of datasets by\ntaking as input distances from the input time series to different\ndiscriminative shapelets. Additionally, these shapelets can easily be\nvisualized and thus possess an interpretable characteristic, making them very\nappealing in critical domains, such as the health care domain, where\nlongitudinal data is ubiquitous. In this study, a new paradigm for shapelet\ndiscovery is proposed, which is based upon evolutionary computation. The\nadvantages of the proposed approach are that (i) it is gradient-free, which\ncould allow to escape from local optima more easily and to find suited\ncandidates more easily and supports non-differentiable objectives, (ii) no\nbrute-force search is required, which drastically reduces the computational\ncomplexity by several orders of magnitude, (iii) the total amount of shapelets\nand length of each of these shapelets are evolved jointly with the shapelets\nthemselves, alleviating the need to specify this beforehand, (iv) entire sets\nare evaluated at once as opposed to single shapelets, which results in smaller\nfinal sets with less similar shapelets that result in similar predictive\nperformances, and (v) discovered shapelets do not need to be a subsequence of\nthe input time series. We present the results of experiments which validate the\nenumerated advantages.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:51:35 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 12:23:35 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Vandewiele", "Gilles", ""], ["Ongenae", "Femke", ""], ["De Turck", "Filip", ""]]}, {"id": "1910.12954", "submitter": "Abram Magner", "authors": "Abram Magner and Mayank Baranwal and Alfred O. Hero III", "title": "Fundamental Limits of Deep Graph Convolutional Networks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) are a widely used method for graph\nrepresentation learning. To elucidate the capabilities and limitations of GCNs,\nwe investigate their power, as a function of their number of layers, to\ndistinguish between different random graph models (corresponding to different\nclass-conditional distributions in a classification problem) on the basis of\nthe embeddings of their sample graphs. In particular, the graph models that we\nconsider arise from graphons, which are the most general possible\nparameterizations of infinite exchangeable graph models and which are the\ncentral objects of study in the theory of dense graph limits. We give a precise\ncharacterization of the set of pairs of graphons that are indistinguishable by\na GCN with nonlinear activation functions coming from a certain broad class if\nits depth is at least logarithmic in the size of the sample graph. This\ncharacterization is in terms of a degree profile closeness property. Outside\nthis class, a very simple GCN architecture suffices for distinguishability. We\nthen exhibit a concrete, infinite class of graphons arising from stochastic\nblock models that are well-separated in terms of cut distance and are\nindistinguishable by a GCN. These results theoretically match empirical\nobservations of several prior works. To prove our results, we exploit a\nconnection to random walks on graphs. Finally, we give empirical results on\nsynthetic and real graph classification datasets, indicating that\nindistinguishable graph distributions arise in practice.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:28:59 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 18:17:33 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Magner", "Abram", ""], ["Baranwal", "Mayank", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1910.12958", "submitter": "Gabriel Peyr\\'e", "authors": "Thibault S\\'ejourn\\'e, Jean Feydy, Fran\\c{c}ois-Xavier Vialard, Alain\n  Trouv\\'e, Gabriel Peyr\\'e", "title": "Sinkhorn Divergences for Unbalanced Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport induces the Earth Mover's (Wasserstein) distance between\nprobability distributions, a geometric divergence that is relevant to a wide\nrange of problems. Over the last decade, two relaxations of optimal transport\nhave been studied in depth: unbalanced transport, which is robust to the\npresence of outliers and can be used when distributions don't have the same\ntotal mass; entropy-regularized transport, which is robust to sampling noise\nand lends itself to fast computations using the Sinkhorn algorithm. This paper\ncombines both lines of work to put robust optimal transport on solid ground.\nOur main contribution is a generalization of the Sinkhorn algorithm to\nunbalanced transport: our method alternates between the standard Sinkhorn\nupdates and the pointwise application of a contractive function. This implies\nthat entropic transport solvers on grid images, point clouds and sampled\ndistributions can all be modified easily to support unbalanced transport, with\na proof of linear convergence that holds in all settings. We then show how to\nuse this method to define pseudo-distances on the full space of positive\nmeasures that satisfy key geometric axioms: (unbalanced) Sinkhorn divergences\nare differentiable, positive, definite, convex, statistically robust and avoid\nany \"entropic bias\" towards a shrinkage of the measures' supports.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:40:37 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:02:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["S\u00e9journ\u00e9", "Thibault", ""], ["Feydy", "Jean", ""], ["Vialard", "Fran\u00e7ois-Xavier", ""], ["Trouv\u00e9", "Alain", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1910.12960", "submitter": "Yuanhao Lai", "authors": "Yuanhao Lai and Ian McLeod", "title": "Ensemble Quantile Classifier", "comments": null, "journal-ref": "Computational Statistics and Data Analysis (2019) 106849", "doi": "10.1016/j.csda.2019.106849", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the median-based classifier and the quantile-based classifier are useful\nfor discriminating high-dimensional data with heavy-tailed or skewed inputs.\nBut these methods are restricted as they assign equal weight to each variable\nin an unregularized way. The ensemble quantile classifier is a more flexible\nregularized classifier that provides better performance with high-dimensional\ndata, asymmetric data or when there are many irrelevant extraneous inputs. The\nimproved performance is demonstrated by a simulation study as well as an\napplication to text categorization. It is proven that the estimated parameters\nof the ensemble quantile classifier consistently estimate the minimal\npopulation loss under suitable general model assumptions. It is also shown that\nthe ensemble quantile classifier is Bayes optimal under suitable assumptions\nwith asymmetric Laplace distribution inputs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:41:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Lai", "Yuanhao", ""], ["McLeod", "Ian", ""]]}, {"id": "1910.12980", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Yujia Li, Chenglong Wang, Rishabh Singh, Po-Sen Huang,\n  Pushmeet Kohli", "title": "Learning Transferable Graph Exploration", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of efficient exploration of unseen\nenvironments, a key challenge in AI. We propose a `learning to explore'\nframework where we learn a policy from a distribution of environments. At test\ntime, presented with an unseen environment from the same distribution, the\npolicy aims to generalize the exploration strategy to visit the maximum number\nof unique states in a limited number of steps. We particularly focus on\nenvironments with graph-structured state-spaces that are encountered in many\nimportant real-world applications like software testing and map building. We\nformulate this task as a reinforcement learning problem where the `exploration'\nagent is rewarded for transitioning to previously unseen environment states and\nemploy a graph-structured memory to encode the agent's past trajectory.\nExperimental results demonstrate that our approach is extremely effective for\nexploration of spatial maps; and when applied on the challenging problems of\ncoverage-guided software-testing of domain-specific programs and real-world\nmobile applications, it outperforms methods that have been hand-engineered by\nhuman experts.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 21:43:22 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Dai", "Hanjun", ""], ["Li", "Yujia", ""], ["Wang", "Chenglong", ""], ["Singh", "Rishabh", ""], ["Huang", "Po-Sen", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1910.12991", "submitter": "Aaron Schein", "authors": "Aaron Schein, Scott W. Linderman, Mingyuan Zhou, David M. Blei, and\n  Hanna Wallach", "title": "Poisson-Randomized Gamma Dynamical Systems", "comments": "To appear in the Proceedings of the 32nd Advances in Neural\n  Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a\nmodel for sequentially observed count tensors that encodes a strong inductive\nbias toward sparsity and burstiness. The PRGDS is based on a new motif in\nBayesian latent variable modeling, an alternating chain of discrete Poisson and\ncontinuous gamma latent states that is analytically convenient and\ncomputationally tractable. This motif yields closed-form complete conditionals\nfor all variables by way of the Bessel distribution and a novel discrete\ndistribution that we call the shifted confluent hypergeometric distribution. We\ndraw connections to closely related models and compare the PRGDS to these\nmodels in studies of real-world count data sets of text, international events,\nand neural spike trains. We find that a sparse variant of the PRGDS, which\nallows the continuous gamma latent states to take values of exactly zero, often\nobtains better predictive performance than other models and is uniquely capable\nof inferring latent structures that are highly localized in time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 22:26:29 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Schein", "Aaron", ""], ["Linderman", "Scott W.", ""], ["Zhou", "Mingyuan", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1910.12993", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Alan Yang, Negar Kiyavash, Kun Zhang", "title": "Characterizing Distribution Equivalence and Structure Learning for\n  Cyclic and Acyclic Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main approach to defining equivalence among acyclic directed causal\ngraphical models is based on the conditional independence relationships in the\ndistributions that the causal models can generate, in terms of the Markov\nequivalence. However, it is known that when cycles are allowed in the causal\nstructure, conditional independence may not be a suitable notion for\nequivalence of two structures, as it does not reflect all the information in\nthe distribution that is useful for identification of the underlying structure.\nIn this paper, we present a general, unified notion of equivalence for linear\nGaussian causal directed graphical models, whether they are cyclic or acyclic.\nIn our proposed definition of equivalence, two structures are equivalent if\nthey can generate the same set of data distributions. We also propose a weaker\nnotion of equivalence called quasi-equivalence, which we show is the extent of\nidentifiability from observational data. We propose analytic as well as\ngraphical methods for characterizing the equivalence of two structures.\nAdditionally, we propose a score-based method for learning the structure from\nobservational data, which successfully deals with both acyclic and cyclic\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 22:30:53 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:23:42 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 14:21:59 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Yang", "Alan", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1910.13003", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Zhen Liu, James M. Rehg, Le Song", "title": "Neural Similarity Learning", "comments": "NeurIPS 2019 (v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inner product-based convolution has been the founding stone of convolutional\nneural networks (CNNs), enabling end-to-end learning of visual representation.\nBy generalizing inner product with a bilinear matrix, we propose the neural\nsimilarity which serves as a learnable parametric similarity measure for CNNs.\nNeural similarity naturally generalizes the convolution and enhances\nflexibility. Further, we consider the neural similarity learning (NSL) in order\nto learn the neural similarity adaptively from training data. Specifically, we\npropose two different ways of learning the neural similarity: static NSL and\ndynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a\ndynamic inference network. By regularizing the bilinear matrix, NSL can be\nviewed as learning the shape of kernel and the similarity measure\nsimultaneously. We further justify the effectiveness of NSL with a theoretical\nviewpoint. Most importantly, NSL shows promising performance in visual\nrecognition and few-shot learning, validating the superiority of NSL over the\ninner product-based convolution counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 23:06:56 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 16:59:32 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 10:39:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Liu", "Weiyang", ""], ["Liu", "Zhen", ""], ["Rehg", "James M.", ""], ["Song", "Le", ""]]}, {"id": "1910.13010", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Georgios\n  Piliouras", "title": "Poincar\\'e Recurrence, Cycles and Spurious Equilibria in\n  Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games", "comments": "To appear in NeurIPS 2019 (Spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a wide class of non-convex non-concave min-max games that\ngeneralizes over standard bilinear zero-sum games. In this class, players\ncontrol the inputs of a smooth function whose output is being applied to a\nbilinear zero-sum game. This class of games is motivated by the indirect nature\nof the competition in Generative Adversarial Networks, where players control\nthe parameters of a neural network while the actual competition happens between\nthe distributions that the generator and discriminator capture. We establish\ntheoretically, that depending on the specific instance of the problem\ngradient-descent-ascent dynamics can exhibit a variety of behaviors\nantithetical to convergence to the game theoretically meaningful min-max\nsolution. Specifically, different forms of recurrent behavior (including\nperiodicity and Poincar\\'e recurrence) are possible as well as convergence to\nspurious (non-min-max) equilibria for a positive measure of initial conditions.\nAt the technical level, our analysis combines tools from optimization theory,\ngame theory and dynamical systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 23:59:25 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Flokas", "Lampros", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Piliouras", "Georgios", ""]]}, {"id": "1910.13016", "submitter": "Kacper Sokol", "authors": "Kacper Sokol and Alexander Hepburn and Raul Santos-Rodriguez and Peter\n  Flach", "title": "bLIMEy: Surrogate Prediction Explanations Beyond LIME", "comments": "2019 Workshop on Human-Centric Machine Learning (HCML 2019); 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate explainers of black-box machine learning predictions are of\nparamount importance in the field of eXplainable Artificial Intelligence since\nthey can be applied to any type of data (images, text and tabular), are\nmodel-agnostic and are post-hoc (i.e., can be retrofitted). The Local\nInterpretable Model-agnostic Explanations (LIME) algorithm is often mistakenly\nunified with a more general framework of surrogate explainers, which may lead\nto a belief that it is the solution to surrogate explainability. In this paper\nwe empower the community to \"build LIME yourself\" (bLIMEy) by proposing a\nprincipled algorithmic framework for building custom local surrogate explainers\nof black-box model predictions, including LIME itself. To this end, we\ndemonstrate how to decompose the surrogate explainers family into\nalgorithmically independent and interoperable modules and discuss the influence\nof these component choices on the functional capabilities of the resulting\nexplainer, using the example of LIME.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 00:21:31 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Sokol", "Kacper", ""], ["Hepburn", "Alexander", ""], ["Santos-Rodriguez", "Raul", ""], ["Flach", "Peter", ""]]}, {"id": "1910.13018", "submitter": "Marmar Orooji", "authors": "Marmar Orooji, Jianhua Chen", "title": "Predicting Louisiana Public High School Dropout through Imbalanced\n  Learning Techniques", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is motivated by the magnitude of the problem of Louisiana high\nschool dropout and its negative impacts on individual and public well-being.\nOur goal is to predict students who are at risk of high school dropout, by\nexamining Louisiana administrative dataset. Due to the imbalanced nature of the\ndataset, imbalanced learning techniques including resampling, case weighting,\nand cost-sensitive learning have been applied to enhance the prediction\nperformance on the rare class. Performance metrics used in this study are\nF-measure, recall and precision of the rare class. We compare the performance\nof several machine learning algorithms such as neural networks, decision trees\nand bagging trees in combination with the imbalanced learning approaches using\nan administrative dataset of size of 366k+ from Louisiana Department of\nEducation. Experiments show that application of imbalanced learning methods\nproduces good results on recall but decreases precision, whereas base\nclassifiers without regard of imbalanced data handling gives better precision\nbut poor recall. Overall application of imbalanced learning techniques is\nbeneficial, yet more studies are desired to improve precision.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 00:30:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Orooji", "Marmar", ""], ["Chen", "Jianhua", ""]]}, {"id": "1910.13021", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis and Georgios\n  Piliouras", "title": "Efficiently avoiding saddle points with zero order methods: No gradients\n  required", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the case of derivative-free algorithms for non-convex\noptimization, also known as zero order algorithms, that use only function\nevaluations rather than gradients. For a wide variety of gradient approximators\nbased on finite differences, we establish asymptotic convergence to second\norder stationary points using a carefully tailored application of the Stable\nManifold Theorem. Regarding efficiency, we introduce a noisy zero-order method\nthat converges to second order stationary points, i.e avoids saddle points. Our\nalgorithm uses only $\\tilde{\\mathcal{O}}(1 / \\epsilon^2)$ approximate gradient\ncalculations and, thus, it matches the converge rate guarantees of their exact\ngradient counterparts up to constants. In contrast to previous work, our\nconvergence rate analysis avoids imposing additional dimension dependent\nslowdowns in the number of iterations required for non-convex zero order\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 00:45:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Flokas", "Lampros", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Piliouras", "Georgios", ""]]}, {"id": "1910.13025", "submitter": "Chunfeng Cui", "authors": "Chunfeng Cui, Kaiqi Zhang, Talgat Daulbaev, Julia Gusak, Ivan\n  Oseledets, Zheng Zhang", "title": "Active Subspace of Neural Networks: Structural Analysis and Universal\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active subspace is a model reduction method widely used in the uncertainty\nquantification community. In this paper, we propose analyzing the internal\nstructure and vulnerability and deep neural networks using active subspace.\nFirstly, we employ the active subspace to measure the number of \"active\nneurons\" at each intermediate layer and reduce the number of neurons from\nseveral thousands to several dozens. This motivates us to change the network\nstructure and to develop a new and more compact network, referred to as\n{ASNet}, that has significantly fewer model parameters. Secondly, we propose\nanalyzing the vulnerability of a neural network using active subspace and\nfinding an additive universal adversarial attack vector that can misclassify a\ndataset with a high probability. Our experiments on CIFAR-10 show that ASNet\ncan achieve 23.98$\\times$ parameter and 7.30$\\times$ flops reduction. The\nuniversal active subspace attack vector can achieve around 20% higher attack\nratio compared with the existing approach in all of our numerical experiments.\nThe PyTorch codes for this paper are available online.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 01:03:23 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 12:25:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Cui", "Chunfeng", ""], ["Zhang", "Kaiqi", ""], ["Daulbaev", "Talgat", ""], ["Gusak", "Julia", ""], ["Oseledets", "Ivan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1910.13051", "submitter": "Angus Dempster", "authors": "Angus Dempster, Fran\\c{c}ois Petitjean, Geoffrey I. Webb", "title": "ROCKET: Exceptionally fast and accurate time series classification using\n  random convolutional kernels", "comments": "27 pages, 23 figures", "journal-ref": null, "doi": "10.1007/s10618-020-00701-z", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for time series classification that attain state-of-the-art\naccuracy have high computational complexity, requiring significant training\ntime even for smaller datasets, and are intractable for larger datasets.\nAdditionally, many existing methods focus on a single type of feature such as\nshape or frequency. Building on the recent success of convolutional neural\nnetworks for time series classification, we show that simple linear classifiers\nusing random convolutional kernels achieve state-of-the-art accuracy with a\nfraction of the computational expense of existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:48:56 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Dempster", "Angus", ""], ["Petitjean", "Fran\u00e7ois", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1910.13052", "submitter": "Feng Zhou", "authors": "Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, Fang Chen", "title": "Scalable Inference for Nonparametric Hawkes Process Using\n  P\\'{o}lya-Gamma Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the sigmoid Gaussian Hawkes process model: the\nbaseline intensity and triggering kernel of Hawkes process are both modeled as\nthe sigmoid transformation of random trajectories drawn from Gaussian processes\n(GP). By introducing auxiliary latent random variables (branching structure,\nP\\'{o}lya-Gamma random variables and latent marked Poisson processes), the\nlikelihood is converted to two decoupled components with a Gaussian form which\nallows for an efficient conjugate analytical inference. Using the augmented\nlikelihood, we derive an expectation-maximization (EM) algorithm to obtain the\nmaximum a posteriori (MAP) estimate. Furthermore, we extend the EM algorithm to\nan efficient approximate Bayesian inference algorithm: mean-field variational\ninference. We demonstrate the performance of two algorithms on simulated\nfictitious data. Experiments on real data show that our proposed inference\nalgorithms can recover well the underlying prompting characteristics\nefficiently.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:49:19 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhou", "Feng", ""], ["Li", "Zhidong", ""], ["Fan", "Xuhui", ""], ["Wang", "Yang", ""], ["Sowmya", "Arcot", ""], ["Chen", "Fang", ""]]}, {"id": "1910.13067", "submitter": "The Canh Dinh", "authors": "Canh T. Dinh, Nguyen H. Tran, Minh N. H. Nguyen, Choong Seon Hong, Wei\n  Bao, Albert Y. Zomaya, Vincent Gramoli", "title": "Federated Learning over Wireless Networks: Convergence Analysis and\n  Resource Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TNET.2020.3035770", "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing interest in a fast-growing machine learning technique\ncalled Federated Learning, in which the model training is distributed over\nmobile user equipments (UEs), exploiting UEs' local computation and training\ndata. Despite its advantages in data privacy-preserving, Federated Learning\n(FL) still has challenges in heterogeneity across UEs' data and physical\nresources. We first propose a FL algorithm which can handle the heterogeneous\nUEs' data challenge without further assumptions except strongly convex and\nsmooth loss functions. We provide the convergence rate characterizing the\ntrade-off between local computation rounds of UE to update its local model and\nglobal communication rounds to update the FL global model. We then employ the\nproposed FL algorithm in wireless networks as a resource allocation\noptimization problem that captures the trade-off between the FL convergence\nwall clock time and energy consumption of UEs with heterogeneous computing and\npower resources. Even though the wireless resource allocation problem of FL is\nnon-convex, we exploit this problem's structure to decompose it into three\nsub-problems and analyze their closed-form solutions as well as insights to\nproblem design. Finally, we illustrate the theoretical analysis for the new\nalgorithm with Tensorflow experiments and extensive numerical results for the\nwireless resource allocation sub-problems. The experiment results not only\nverify the theoretical convergence but also show that our proposed algorithm\noutperforms the vanilla FedAvg algorithm in terms of convergence rate and\ntesting accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:31:28 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 03:31:59 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 00:16:29 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 03:09:33 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Dinh", "Canh T.", ""], ["Tran", "Nguyen H.", ""], ["Nguyen", "Minh N. H.", ""], ["Hong", "Choong Seon", ""], ["Bao", "Wei", ""], ["Zomaya", "Albert Y.", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1910.13089", "submitter": "Md Mahfuzur Rahman", "authors": "Md Mahfuzur Rahman, Daniel Pimentel-Alarcon", "title": "GLIMPS: A Greedy Mixed Integer Approach for Super Robust Matched\n  Subspace Detection", "comments": "8 pages, 5 figures, 57th Allerton Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to diverse nature of data acquisition and modern applications, many\ncontemporary problems involve high dimensional datum $\\x \\in \\R^\\d$ whose\nentries often lie in a union of subspaces and the goal is to find out which\nentries of $\\x$ match with a particular subspace $\\sU$, classically called\n\\emph {matched subspace detection}. Consequently, entries that match with one\nsubspace are considered as inliers w.r.t the subspace while all other entries\nare considered as outliers. Proportion of outliers relative to each subspace\nvaries based on the degree of coordinates from subspaces. This problem is a\ncombinatorial NP-hard in nature and has been immensely studied in recent years.\nExisting approaches can solve the problem when outliers are sparse. However, if\noutliers are abundant or in other words if $\\x$ contains coordinates from a\nfair amount of subspaces, this problem can't be solved with acceptable accuracy\nor within a reasonable amount of time. This paper proposes a two-stage approach\ncalled \\emph{Greedy Linear Integer Mixed Programmed Selector} (GLIMPS) for this\nabundant-outliers setting, which combines a greedy algorithm and mixed integer\nformulation and can tolerate over 80\\% outliers, outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 05:15:40 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Rahman", "Md Mahfuzur", ""], ["Pimentel-Alarcon", "Daniel", ""]]}, {"id": "1910.13092", "submitter": "Huong Ha", "authors": "Huong Ha, Santu Rana, Sunil Gupta, Thanh Nguyen, Hung Tran-The, Svetha\n  Venkatesh", "title": "Bayesian Optimization with Unknown Search Space", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying Bayesian optimization in problems wherein the search space is\nunknown is challenging. To address this problem, we propose a systematic volume\nexpansion strategy for the Bayesian optimization. We devise a strategy to\nguarantee that in iterative expansions of the search space, our method can find\na point whose function value within epsilon of the objective function maximum.\nWithout the need to specify any parameters, our algorithm automatically\ntriggers a minimal expansion required iteratively. We derive analytic\nexpressions for when to trigger the expansion and by how much to expand. We\nalso provide theoretical analysis to show that our method achieves\nepsilon-accuracy after a finite number of iterations. We demonstrate our method\non both benchmark test functions and machine learning hyper-parameter tuning\ntasks and demonstrate that our method outperforms baselines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 05:31:24 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ha", "Huong", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Nguyen", "Thanh", ""], ["Tran-The", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1910.13101", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Walter Talbott, Carlos Guestrin, Joshua M. Susskind", "title": "Adversarial Fisher Vectors for Unsupervised Representation Learning", "comments": "Accepted as spotlight presentation to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine Generative Adversarial Networks (GANs) through the lens of deep\nEnergy Based Models (EBMs), with the goal of exploiting the density model that\nfollows from this formulation. In contrast to a traditional view where the\ndiscriminator learns a constant function when reaching convergence, here we\nshow that it can provide useful information for downstream tasks, e.g., feature\nextraction for classification. To be concrete, in the EBM formulation, the\ndiscriminator learns an unnormalized density function (i.e., the negative\nenergy term) that characterizes the data manifold. We propose to evaluate both\nthe generator and the discriminator by deriving corresponding Fisher Score and\nFisher Information from the EBM. We show that by assuming that the generated\nexamples form an estimate of the learned density, both the Fisher Information\nand the normalized Fisher Vectors are easy to compute. We also show that we are\nable to derive a distance metric between examples and between sets of examples.\nWe conduct experiments showing that the GAN-induced Fisher Vectors demonstrate\ncompetitive performance as unsupervised feature extractors for classification\nand perceptual similarity tasks. Code is available at\n\\url{https://github.com/apple/ml-afv}.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 06:10:48 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Guestrin", "Carlos", ""], ["Susskind", "Joshua M.", ""]]}, {"id": "1910.13113", "submitter": "Kazuhiro Fukui", "authors": "Kazuhiro Fukui, Naoya Sogi, Takumi Kobayashi, Jing-Hao Xue, Atsuto\n  Maki", "title": "Discriminant analysis based on projection onto generalized difference\n  subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a new type of discriminant analysis based on the\northogonal projection of data onto a generalized difference subspace (GDS). In\nour previous work, we have demonstrated that GDS projection works as the\nquasi-orthogonalization of class subspaces, which is an effective feature\nextraction for subspace based classifiers. Interestingly, GDS projection also\nworks as a discriminant feature extraction through a similar mechanism to the\nFisher discriminant analysis (FDA). A direct proof of the connection between\nGDS projection and FDA is difficult due to the significant difference in their\nformulations. To avoid the difficulty, we first introduce geometrical Fisher\ndiscriminant analysis (gFDA) based on a simplified Fisher criterion. Our\nsimplified Fisher criterion is derived from a heuristic yet practically\nplausible principle: the direction of the sample mean vector of a class is in\nmost cases almost equal to that of the first principal component vector of the\nclass, under the condition that the principal component vectors are calculated\nby applying the principal component analysis (PCA) without data centering. gFDA\ncan work stably even under few samples, bypassing the small sample size (SSS)\nproblem of FDA. Next, we prove that gFDA is equivalent to GDS projection with a\nsmall correction term. This equivalence ensures GDS projection to inherit the\ndiscriminant ability from FDA via gFDA. Furthermore, to enhance the\nperformances of gFDA and GDS projection, we normalize the projected vectors on\nthe discriminant spaces. Extensive experiments using the extended Yale B+\ndatabase and the CMU face database show that gFDA and GDS projection have\nequivalent or better performance than the original FDA and its extensions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 06:56:17 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 03:19:56 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Fukui", "Kazuhiro", ""], ["Sogi", "Naoya", ""], ["Kobayashi", "Takumi", ""], ["Xue", "Jing-Hao", ""], ["Maki", "Atsuto", ""]]}, {"id": "1910.13124", "submitter": "Guillaume Godin", "authors": "Fabio Capela, Vincent Nouchi, Ruud Van Deursen, Igor V. Tetko and\n  Guillaume Godin", "title": "Multitask Learning On Graph Neural Networks Applied To Molecular\n  Property Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prediction of molecular properties, including physico-chemical properties, is\na challenging task in chemistry. Herein we present a new state-of-the-art\nmultitask prediction method based on existing graph neural network models. We\nhave used different architectures for our models and the results clearly\ndemonstrate that multitask learning can improve model performance.\nAdditionally, a significant reduction of variance in the models has been\nobserved. Most importantly, datasets with a small amount of data points reach\nbetter results without the need of augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 07:53:50 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 12:40:24 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Capela", "Fabio", ""], ["Nouchi", "Vincent", ""], ["Van Deursen", "Ruud", ""], ["Tetko", "Igor V.", ""], ["Godin", "Guillaume", ""]]}, {"id": "1910.13140", "submitter": "Neo Christopher Chung <", "authors": "Lennart Brocki, Neo Christopher Chung", "title": "Concept Saliency Maps to Visualize Relevant Features in Deep Generative\n  Models", "comments": "18th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating, explaining, and visualizing high-level concepts in generative\nmodels, such as variational autoencoders (VAEs), is challenging in part due to\na lack of known prediction classes that are required to generate saliency maps\nin supervised learning. While saliency maps may help identify relevant features\n(e.g., pixels) in the input for classification tasks of deep neural networks,\nsimilar frameworks are understudied in unsupervised learning. Therefore, we\nintroduce a new method of obtaining saliency maps for latent representations of\nknown or novel high-level concepts, often called concept vectors in generative\nmodels. Concept scores, analogous to class scores in classification tasks, are\ndefined as dot products between concept vectors and encoded input data, which\ncan be readily used to compute the gradients. The resulting concept saliency\nmaps are shown to highlight input features deemed important for high-level\nconcepts. Our method is applied to the VAE's latent space of CelebA dataset in\nwhich known attributes such as \"smiles\" and \"hats\" are used to elucidate\nrelevant facial features. Furthermore, our application to spatial\ntranscriptomic (ST) data of a mouse olfactory bulb demonstrates the potential\nof latent representations of morphological layers and molecular features in\nadvancing our understanding of complex biological systems. By extending the\npopular method of saliency maps to generative models, the proposed concept\nsaliency maps help improve interpretability of latent variable models in deep\nlearning.\n  Codes to reproduce and to implement concept saliency maps:\nhttps://github.com/lenbrocki/concept-saliency-maps\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:15:25 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Brocki", "Lennart", ""], ["Chung", "Neo Christopher", ""]]}, {"id": "1910.13141", "submitter": "Atsushi Yaguchi", "authors": "Atsushi Yaguchi, Taiji Suzuki, Shuhei Nitta, Yukinobu Sakata, Akiyuki\n  Tanizawa", "title": "Decomposable-Net: Scalable Low-Rank Compression for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing deep neural networks (DNNs) is important for real-world\napplications operating on resource-constrained devices. However, it is not\nstraightforward to change the model size (i.e., computational complexity) once\ntraining and compression are completed, calling for retraining to construct\nmodels suitable for different devices. In this paper, we propose a novel\nmethod, Decomposable-Net (the network decomposable in any size), which allows\nflexible changes to model size without retraining. We decompose weight matrices\nin the DNNs via singular value decomposition and adjust ranks according to the\ntarget model size. Unlike the existing methods, (1) we propose a learning\nmethod that explicitly minimizes losses for both of full-rank and low-rank\nnetworks, which is designed not only to maintain the performance of a full-rank\nnetwork but also to improve multiple low-rank networks in a single model. (2)\nWe also provide a mathematical analysis for the scalability of the\napproximation error with respect to the rank in each layer. Moreover, on the\nbasis of the analysis, (3) we introduce a simple criterion for rank selection\nthat effectively suppresses approximation error. In experiments on\nimage-classification tasks on CIFAR-10/100 and ImageNet datasets,\nDecomposable-Net yields favorable performance in a broader range of compressed\nmodels. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\\%$\nwith $0.27\\times$MACs on the ImageNet classification task with ResNet-50,\ncompared to low-rank tensor (Tucker) decomposition ($67.4\\% / 0.30\\times$) and\nuniversally slimmable networks ($70.6\\% / 0.26\\times$).\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:15:40 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 07:32:42 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Yaguchi", "Atsushi", ""], ["Suzuki", "Taiji", ""], ["Nitta", "Shuhei", ""], ["Sakata", "Yukinobu", ""], ["Tanizawa", "Akiyuki", ""]]}, {"id": "1910.13148", "submitter": "Maxim Kuznetsov", "authors": "Maksim Kuznetsov and Daniil Polykovskiy and Dmitry Vetrov and\n  Alexander Zhebrak", "title": "A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for\n  Generative Models", "comments": "NeurIPS 2019; GitHub: https://github.com/insilicomedicine/TRIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models produce realistic objects in many domains, including text,\nimage, video, and audio synthesis. Most popular models---Generative Adversarial\nNetworks (GANs) and Variational Autoencoders (VAEs)---usually employ a standard\nGaussian distribution as a prior. Previous works show that the richer family of\nprior distributions may help to avoid the mode collapse problem in GANs and to\nimprove the evidence lower bound in VAEs. We propose a new family of prior\ndistributions---Tensor Ring Induced Prior (TRIP)---that packs an exponential\nnumber of Gaussians into a high-dimensional lattice with a relatively small\nnumber of parameters. We show that these priors improve Fr\\'echet Inception\nDistance for GANs and Evidence Lower Bound for VAEs. We also study generative\nmodels with TRIP in the conditional generation setup with missing conditions.\nAltogether, we propose a novel plug-and-play framework for generative models\nthat can be utilized in any GAN and VAE-like architectures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:33:09 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Kuznetsov", "Maksim", ""], ["Polykovskiy", "Daniil", ""], ["Vetrov", "Dmitry", ""], ["Zhebrak", "Alexander", ""]]}, {"id": "1910.13153", "submitter": "Garrett Bernstein", "authors": "Garrett Bernstein and Daniel Sheldon", "title": "Differentially Private Bayesian Linear Regression", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is an important tool across many fields that work with\nsensitive human-sourced data. Significant prior work has focused on producing\ndifferentially private point estimates, which provide a privacy guarantee to\nindividuals while still allowing modelers to draw insights from data by\nestimating regression coefficients. We investigate the problem of Bayesian\nlinear regression, with the goal of computing posterior distributions that\ncorrectly quantify uncertainty given privately released statistics. We show\nthat a naive approach that ignores the noise injected by the privacy mechanism\ndoes a poor job in realistic data settings. We then develop noise-aware methods\nthat perform inference over the privacy mechanism and produce correct\nposteriors across a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:47:23 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Bernstein", "Garrett", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1910.13157", "submitter": "Eran Treister", "authors": "Jonathan Ephrath, Moshe Eliasof, Lars Ruthotto, Eldad Haber and Eran\n  Treister", "title": "LeanConvNets: Low-cost Yet Effective Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.2972775", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become indispensable for solving\nmachine learning tasks in speech recognition, computer vision, and other areas\nthat involve high-dimensional data. A CNN filters the input feature using a\nnetwork containing spatial convolution operators with compactly supported\nstencils. In practice, the input data and the hidden features consist of a\nlarge number of channels, which in most CNNs are fully coupled by the\nconvolution operators. This coupling leads to immense computational cost in the\ntraining and prediction phase. In this paper, we introduce LeanConvNets that\nare derived by sparsifying fully-coupled operators in existing CNNs. Our goal\nis to improve the efficiency of CNNs by reducing the number of weights,\nfloating point operations and latency times, with minimal loss of accuracy. Our\nlean convolution operators involve tuning parameters that controls the\ntrade-off between the network's accuracy and computational costs. These\nconvolutions can be used in a wide range of existing networks, and we exemplify\ntheir use in residual networks (ResNets). Using a range of benchmark problems\nfrom image classification and semantic segmentation, we demonstrate that the\nresulting LeanConvNet's accuracy is close to state-of-the-art networks while\nbeing computationally less expensive. In our tests, the lean versions of ResNet\nin most cases outperform comparable reduced architectures such as MobileNets\nand ShuffleNets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:51:10 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 10:50:12 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ephrath", "Jonathan", ""], ["Eliasof", "Moshe", ""], ["Ruthotto", "Lars", ""], ["Haber", "Eldad", ""], ["Treister", "Eran", ""]]}, {"id": "1910.13181", "submitter": "Talip Ucar", "authors": "Talip Ucar", "title": "Bridging the ELBO and MMD", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in training generative models such as the variational\nauto encoder (VAE) is avoiding posterior collapse. When the generator has too\nmuch capacity, it is prone to ignoring latent code. This problem is exacerbated\nwhen the dataset is small, and the latent dimension is high. The root of the\nproblem is the ELBO objective, specifically the Kullback-Leibler (KL)\ndivergence term in objective function \\citep{zhao2019infovae}. This paper\nproposes a new objective function to replace the KL term with one that emulates\nthe maximum mean discrepancy (MMD) objective. It also introduces a new\ntechnique, named latent clipping, that is used to control distance between\nsamples in latent space. A probabilistic autoencoder model, named $\\mu$-VAE, is\ndesigned and trained on MNIST and MNIST Fashion datasets, using the new\nobjective function and is shown to outperform models trained with ELBO and\n$\\beta$-VAE objective. The $\\mu$-VAE is less prone to posterior collapse, and\ncan generate reconstructions and new samples in good quality. Latent\nrepresentations learned by $\\mu$-VAE are shown to be good and can be used for\ndownstream tasks such as classification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:32:40 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ucar", "Talip", ""]]}, {"id": "1910.13188", "submitter": "Kuen-Han Tsai", "authors": "Kuen-Han Tsai and Hsuan-Tien Lin", "title": "Learning from Label Proportions with Consistency Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning from label proportions (LLP) involves training\nclassifiers with weak labels on bags of instances, rather than strong labels on\nindividual instances. The weak labels only contain the label proportion of each\nbag. The LLP problem is important for many practical applications that only\nallow label proportions to be collected because of data privacy or annotation\ncost, and has recently received lots of research attention. Most existing works\nfocus on extending supervised learning models to solve the LLP problem, but the\nweak learning nature makes it hard to further improve LLP performance with a\nsupervised angle. In this paper, we take a different angle from semi-supervised\nlearning. In particular, we propose a novel model inspired by consistency\nregularization, a popular concept in semi-supervised learning that encourages\nthe model to produce a decision boundary that better describes the data\nmanifold. With the introduction of consistency regularization, we further\nextend our study to non-uniform bag-generation and validation-based\nparameter-selection procedures that better match practical needs. Experiments\nnot only justify that LLP with consistency regularization achieves superior\nperformance, but also demonstrate the practical usability of the proposed\nprocedures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:46:15 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Tsai", "Kuen-Han", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1910.13197", "submitter": "Ghodai Abdelrahman", "authors": "Ghodai Abdelrahman and Qing Wang", "title": "Knowledge Tracing with Sequential Key-Value Memory Networks", "comments": null, "journal-ref": "Proceedings of the 42Nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (2019)", "doi": "10.1145/3331184.3331195", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can machines trace human knowledge like humans? Knowledge tracing (KT) is a\nfundamental task in a wide range of applications in education, such as massive\nopen online courses (MOOCs), intelligent tutoring systems, educational games,\nand learning management systems. It models dynamics in a student's knowledge\nstates in relation to different learning concepts through their interactions\nwith learning activities. Recently, several attempts have been made to use deep\nlearning models for tackling the KT problem. Although these deep learning\nmodels have shown promising results, they have limitations: either lack the\nability to go deeper to trace how specific concepts in a knowledge state are\nmastered by a student, or fail to capture long-term dependencies in an exercise\nsequence. In this paper, we address these limitations by proposing a novel deep\nlearning model for knowledge tracing, namely Sequential Key-Value Memory\nNetworks (SKVMN). This model unifies the strengths of recurrent modelling\ncapacity and memory capacity of the existing deep learning KT models for\nmodelling student learning. We have extensively evaluated our proposed model on\nfive benchmark datasets. The experimental results show that (1) SKVMN\noutperforms the state-of-the-art KT models on all datasets, (2) SKVMN can\nbetter discover the correlation between latent concepts and questions, and (3)\nSKVMN can trace the knowledge state of students dynamics, and a leverage\nsequential dependencies in an exercise sequence for improved predication\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:10:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Abdelrahman", "Ghodai", ""], ["Wang", "Qing", ""]]}, {"id": "1910.13204", "submitter": "Bulat Ibragimov", "authors": "Bulat Ibragimov and Gleb Gusev", "title": "Minimal Variance Sampling in Stochastic Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Boosting (SGB) is a widely used approach to\nregularization of boosting models based on decision trees. It was shown that,\nin many cases, random sampling at each iteration can lead to better\ngeneralization performance of the model and can also decrease the learning\ntime. Different sampling approaches were proposed, where probabilities are not\nuniform, and it is not currently clear which approach is the most effective. In\nthis paper, we formulate the problem of randomization in SGB in terms of\noptimization of sampling probabilities to maximize the estimation accuracy of\nsplit scoring used to train decision trees. This optimization problem has a\nclosed-form nearly optimal solution, and it leads to a new sampling technique,\nwhich we call Minimal Variance Sampling (MVS). The method both decreases the\nnumber of examples needed for each iteration of boosting and increases the\nquality of the model significantly as compared to the state-of-the art sampling\nmethods. The superiority of the algorithm was confirmed by introducing MVS as a\nnew default option for subsampling in CatBoost, a gradient boosting library\nachieving state-of-the-art quality on various machine learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:26:24 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ibragimov", "Bulat", ""], ["Gusev", "Gleb", ""]]}, {"id": "1910.13212", "submitter": "Mimansa Jaiswal", "authors": "Mimansa Jaiswal, Emily Mower Provost", "title": "Privacy Enhanced Multimodal Neural Representations for Emotion\n  Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mobile applications and virtual conversational agents now aim to\nrecognize and adapt to emotions. To enable this, data are transmitted from\nusers' devices and stored on central servers. Yet, these data contain sensitive\ninformation that could be used by mobile applications without user's consent\nor, maliciously, by an eavesdropping adversary. In this work, we show how\nmultimodal representations trained for a primary task, here emotion\nrecognition, can unintentionally leak demographic information, which could\noverride a selected opt-out option by the user. We analyze how this leakage\ndiffers in representations obtained from textual, acoustic, and multimodal\ndata. We use an adversarial learning paradigm to unlearn the private\ninformation present in a representation and investigate the effect of varying\nthe strength of the adversarial component on the primary task and on the\nprivacy metric, defined here as the inability of an attacker to predict\nspecific demographic information. We evaluate this paradigm on multiple\ndatasets and show that we can improve the privacy metric while not\nsignificantly impacting the performance on the primary task. To the best of our\nknowledge, this is the first work to analyze how the privacy metric differs\nacross modalities and how multiple privacy concerns can be tackled while still\nmaintaining performance on emotion recognition.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:49:30 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Jaiswal", "Mimansa", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1910.13233", "submitter": "George Papamakarios", "authors": "George Papamakarios", "title": "Neural Density Estimation and Likelihood-free Inference", "comments": "PhD thesis submitted to the University of Edinburgh in April 2019.\n  Includes in full the following articles: arXiv:1605.06376, arXiv:1705.07057,\n  arXiv:1805.07226", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I consider two problems in machine learning and statistics: the problem of\nestimating the joint probability density of a collection of random variables,\nknown as density estimation, and the problem of inferring model parameters when\ntheir likelihood is intractable, known as likelihood-free inference. The\ncontribution of the thesis is a set of new methods for addressing these\nproblems that are based on recent advances in neural networks and deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 12:52:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Papamakarios", "George", ""]]}, {"id": "1910.13255", "submitter": "Yosi Shrem", "authors": "Yosi Shrem, Matthew Goldrick, Joseph Keshet", "title": "Dr.VOT : Measuring Positive and Negative Voice Onset Time in the Wild", "comments": "interspeech 2019", "journal-ref": "interspeech 2019", "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice Onset Time (VOT), a key measurement of speech for basic research and\napplied medical studies, is the time between the onset of a stop burst and the\nonset of voicing. When the voicing onset precedes burst onset the VOT is\nnegative; if voicing onset follows the burst, it is positive. In this work, we\npresent a deep-learning model for accurate and reliable measurement of VOT in\nnaturalistic speech. The proposed system addresses two critical issues: it can\nmeasure positive and negative VOT equally well, and it is trained to be robust\nto variation across annotations. Our approach is based on the structured\nprediction framework, where the feature functions are defined to be RNNs. These\nlearn to capture segmental variation in the signal. Results suggest that our\nmethod substantially improves over the current state-of-the-art. In contrast to\nprevious work, our Deep and Robust VOT annotator, Dr.VOT, can successfully\nestimate negative VOTs while maintaining state-of-the-art performance on\npositive VOTs. This high level of performance generalizes to new corpora\nwithout further retraining. Index Terms: structured prediction, multi-task\nlearning, adversarial training, recurrent neural networks, sequence\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 12:42:52 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Shrem", "Yosi", ""], ["Goldrick", "Matthew", ""], ["Keshet", "Joseph", ""]]}, {"id": "1910.13268", "submitter": "Kush Varshney", "authors": "Newton M. Kinyanjui, Timothy Odonga, Celia Cintas, Noel C. F. Codella,\n  Rameswar Panda, Prasanna Sattigeri, and Kush R. Varshney", "title": "Estimating Skin Tone and Effects on Classification Performance in\n  Dermatology Datasets", "comments": "NeurIPS 2019 Workshop on Fair ML for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision and deep learning have led to\nbreakthroughs in the development of automated skin image analysis. In\nparticular, skin cancer classification models have achieved performance higher\nthan trained expert dermatologists. However, no attempt has been made to\nevaluate the consistency in performance of machine learning models across\npopulations with varying skin tones. In this paper, we present an approach to\nestimate skin tone in benchmark skin disease datasets, and investigate whether\nmodel performance is dependent on this measure. Specifically, we use individual\ntypology angle (ITA) to approximate skin tone in dermatology datasets. We look\nat the distribution of ITA values to better understand skin color\nrepresentation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a\ncollection of dermoscopic images of skin lesions for the detection of skin\ncancer, and 2) the SD-198 dataset, a collection of clinical images capturing a\nwide variety of skin diseases. To estimate ITA, we first develop segmentation\nmodels to isolate non-diseased areas of skin. We find that the majority of the\ndata in the the two datasets have ITA values between 34.5{\\deg} and 48{\\deg},\nwhich are associated with lighter skin, and is consistent with\nunder-representation of darker skinned populations in these datasets. We also\nfind no measurable correlation between performance of machine learning model\nand ITA values, though more comprehensive data is needed for further\nvalidation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 13:48:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Kinyanjui", "Newton M.", ""], ["Odonga", "Timothy", ""], ["Cintas", "Celia", ""], ["Codella", "Noel C. F.", ""], ["Panda", "Rameswar", ""], ["Sattigeri", "Prasanna", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1910.13292", "submitter": "Luis Miralles PHD", "authors": "Luis Miralles, M. Atif Qureshi, Brian Mac Namee", "title": "Real-time Bidding campaigns optimization using attribute selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-Time Bidding is nowadays one of the most promising systems in the online\nadvertising ecosystem. In the presented study, the performance of RTB campaigns\nis improved by optimising the parameters of the users' profiles and the\npublishers' websites. Most studies about optimising RTB campaigns are focused\non the bidding strategy. In contrast, the objective of our research consists of\noptimising RTB campaigns by finding out configurations that maximise both the\nnumber of impressions and their average profitability. The experiments\ndemonstrate that, when the number of required visits by advertisers is low, it\nis easy to find configurations with high average profitability, but as the\nrequired number of visits increases, the average profitability tends to go\ndown. Additionally, configuration optimisation has been combined with other\ninteresting strategies to increase, even more, the campaigns' profitability.\nAlong with parameter configuration the study considers the following\ncomplementary strategies to increase profitability: i) selecting multiple\nconfigurations with a small number of visits instead of a unique configuration\nwith a large number, ii) discarding visits according to the thresholds of cost\nand profitability, iii) analysing a reduced space of the dataset and\nextrapolating the solution, and iv) increasing the search space by including\nsolutions below the required number of visits. The developed campaign\noptimisation methodology could be offered by RTB platforms to advertisers to\nmake their campaigns more profitable.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:30:15 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Miralles", "Luis", ""], ["Qureshi", "M. Atif", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1910.13295", "submitter": "Pedro Herruzo", "authors": "Pedro Herruzo and Josep L. Larriba-Pey", "title": "Recurrent Autoencoder with Skip Connections and Exogenous Variables for\n  Traffic Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The increasing complexity of mobility plus the growing population in cities,\ntogether with the importance of privacy when sharing data from vehicles or any\ndevice, makes traffic forecasting that uses data from infrastructure and\ncitizens an open and challenging task. In this paper, we introduce a novel\napproach to deal with predictions of speed, volume, and main traffic direction,\nin a new aggregated way of traffic data presented as videos. The approach\nleverages the continuity in a sequence of frames and its dynamics, learning to\npredict changing areas in a low dimensional space and then, recovering static\nfeatures when reconstructing the original space. Exogenous variables like\nweather, time and calendar are also added in the model. Furthermore, we\nintroduce a novel sampling approach for sequences that ensures diversity when\ncreating batches, running in parallel to the optimization process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:04:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Herruzo", "Pedro", ""], ["Larriba-Pey", "Josep L.", ""]]}, {"id": "1910.13314", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Blaz \\v{S}krlj, Jan Kralj, Nada Lavra\\v{c}", "title": "Symbolic Graph Embedding using Frequent Pattern Mining", "comments": "Published at DS2019 conference as a full paper", "journal-ref": null, "doi": "10.1007/978-3-030-33778-0_21", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data mining is becoming ubiquitous in many fields of study. It\noffers insights into behaviour of complex, real-world systems which cannot be\nmodeled directly using propositional learning. We propose Symbolic Graph\nEmbedding (SGE), an algorithm aimed to learn symbolic node representations.\nBuilt on the ideas from the field of inductive logic programming, SGE first\nsamples a given node's neighborhood and interprets it as a transaction\ndatabase, which is used for frequent pattern mining to identify logical\nconjuncts of items that co-occur frequently in a given context. Such patterns\nare in this work used as features to represent individual nodes, yielding\ninterpretable, symbolic node embeddings. The proposed SGE approach on a venue\nclassification task outperforms shallow node embedding methods such as\nDeepWalk, and performs similarly to metapath2vec, a black-box representation\nlearner that can exploit node and edge types in a given graph. The proposed SGE\napproach performs especially well when small amounts of data are used for\nlearning, scales to graphs with millions of nodes and edges, and can be run on\nan of-the-shelf laptop.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:13:18 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["\u0160krlj", "Blaz", ""], ["Kralj", "Jan", ""], ["Lavra\u010d", "Nada", ""]]}, {"id": "1910.13324", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Hongseok Yang, Yee Whye Teh and Tom Rainforth", "title": "Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic\n  Programs with Stochastic Support", "comments": "Published at the 37th International Conference on Machine Learning\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal probabilistic programming systems (PPSs) provide a powerful\nframework for specifying rich probabilistic models. They further attempt to\nautomate the process of drawing inferences from these models, but doing this\nsuccessfully is severely hampered by the wide range of non--standard models\nthey can express. As a result, although one can specify complex models in a\nuniversal PPS, the provided inference engines often fall far short of what is\nrequired. In particular, we show that they produce surprisingly unsatisfactory\nperformance for models where the support varies between executions, often doing\nno better than importance sampling from the prior. To address this, we\nintroduce a new inference framework: Divide, Conquer, and Combine, which\nremains efficient for such models, and show how it can be implemented as an\nautomated and generic PPS inference engine. We empirically demonstrate\nsubstantial performance improvements over existing approaches on three\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:36:56 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 23:21:21 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 17:26:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhou", "Yuan", ""], ["Yang", "Hongseok", ""], ["Teh", "Yee Whye", ""], ["Rainforth", "Tom", ""]]}, {"id": "1910.13325", "submitter": "Leszek Spalek Dr", "authors": "John Armitage, Leszek J. Spalek, Malgorzata Nguyen, Mark Nikolka, Ian\n  E. Jacobs, Lorena Mara\\~n\\'on, Iyad Nasrallah, Guillaume Schweicher, Ivan\n  Dimov, Dimitrios Simatos, Iain McCulloch, Christian B. Nielsen, Gareth\n  Conduit and Henning Sirringhaus", "title": "Fragment Graphical Variational AutoEncoding for Screening Molecules with\n  Small Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.mtrl-sci cs.LG physics.app-ph physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the majority of molecular optimization tasks, predictive machine learning\n(ML) models are limited due to the unavailability and cost of generating big\nexperimental datasets on the specific task. To circumvent this limitation, ML\nmodels are trained on big theoretical datasets or experimental indicators of\nmolecular suitability that are either publicly available or inexpensive to\nacquire. These approaches produce a set of candidate molecules which have to be\nranked using limited experimental data or expert knowledge. Under the\nassumption that structure is related to functionality, here we use a molecular\nfragment-based graphical autoencoder to generate unique structural fingerprints\nto efficiently search through the candidate set. We demonstrate that\nfragment-based graphical autoencoding reduces the error in predicting physical\ncharacteristics such as the solubility and partition coefficient in the small\ndata regime compared to other extended circular fingerprints and string based\napproaches. We further demonstrate that this approach is capable of providing\ninsight into real world molecular optimization problems, such as searching for\nstabilization additives in organic semiconductors by accurately predicting 92%\nof test molecules given 69 training examples. This task is a model example of\nblack box molecular optimization as there is minimal theoretical and\nexperimental knowledge to accurately predict the suitability of the additives.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:35:13 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:18:31 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Armitage", "John", ""], ["Spalek", "Leszek J.", ""], ["Nguyen", "Malgorzata", ""], ["Nikolka", "Mark", ""], ["Jacobs", "Ian E.", ""], ["Mara\u00f1\u00f3n", "Lorena", ""], ["Nasrallah", "Iyad", ""], ["Schweicher", "Guillaume", ""], ["Dimov", "Ivan", ""], ["Simatos", "Dimitrios", ""], ["McCulloch", "Iain", ""], ["Nielsen", "Christian B.", ""], ["Conduit", "Gareth", ""], ["Sirringhaus", "Henning", ""]]}, {"id": "1910.13327", "submitter": "Michael Riegler", "authors": "Steven A. Hicks and Jorunn M. Andersen and Oliwia Witczak and Vajira\n  Thambawita and P{\\aa}ll Halvorsen and Hugo L. Hammer and Trine B. Haugen and\n  Michael A. Riegler", "title": "Machine Learning-Based Analysis of Sperm Videos and Participant Data for\n  Male Fertility Prediction", "comments": "Preprint, accepted by Nature Scientific Reports for publication\n  24.10.2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automatic analysis of clinical data are usually targeted towards\na specific modality and do not make use of all relevant data available. In the\nfield of male human reproduction, clinical and biological data are not used to\nits fullest potential. Manual evaluation of a semen sample using a microscope\nis time-consuming and requires extensive training. Furthermore, the validity of\nmanual semen analysis has been questioned due to limited reproducibility, and\noften high inter-personnel variation. The existing computer-aided sperm\nanalyzer systems are not recommended for routine clinical use due to\nmethodological challenges caused by the consistency of the semen sample. Thus,\nthere is a need for an improved methodology. We use modern and classical\nmachine learning techniques together with a dataset consisting of 85 videos of\nhuman semen samples and related participant data to automatically predict sperm\nmotility. Used techniques include simple linear regression and more\nsophisticated methods using convolutional neural networks. Our results indicate\nthat sperm motility prediction based on deep learning using sperm motility\nvideos is rapid to perform and consistent. The algorithms performed worse when\nparticipant data was added. In conclusion, machine learning-based automatic\nanalysis may become a valuable tool in male infertility investigation and\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:38:47 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hicks", "Steven A.", ""], ["Andersen", "Jorunn M.", ""], ["Witczak", "Oliwia", ""], ["Thambawita", "Vajira", ""], ["Halvorsen", "P\u00e5ll", ""], ["Hammer", "Hugo L.", ""], ["Haugen", "Trine B.", ""], ["Riegler", "Michael A.", ""]]}, {"id": "1910.13349", "submitter": "Yue Wang", "authors": "Yue Wang, Ziyu Jiang, Xiaohan Chen, Pengfei Xu, Yang Zhao, Yingyan\n  Lin, Zhangyang Wang", "title": "E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been increasingly deployed to edge\ndevices. Hence, many efforts have been made towards efficient CNN inference in\nresource-constrained platforms. This paper attempts to explore an orthogonal\ndirection: how to conduct more energy-efficient training of CNNs, so as to\nenable on-device training. We strive to reduce the energy cost during training,\nby dropping unnecessary computations from three complementary levels:\nstochastic mini-batch dropping on the data level; selective layer update on the\nmodel level; and sign prediction for low-cost, low-precision back-propagation,\non the algorithm level. Extensive simulations and ablation studies, with real\nenergy measurements from an FPGA board, confirm the superiority of our proposed\nstrategies and demonstrate remarkable energy savings for training. For example,\nwhen training ResNet-74 on CIFAR-10, we achieve aggressive energy savings of\n>90% and >60%, while incurring a top-1 accuracy loss of only about 2% and 1.2%,\nrespectively. When training ResNet-110 on CIFAR-100, an over 84% training\nenergy saving is achieved without degrading inference accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:07:42 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 00:51:06 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 23:03:56 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 00:40:00 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Wang", "Yue", ""], ["Jiang", "Ziyu", ""], ["Chen", "Xiaohan", ""], ["Xu", "Pengfei", ""], ["Zhao", "Yang", ""], ["Lin", "Yingyan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1910.13372", "submitter": "Pieter Robberechts", "authors": "Pieter Robberechts, Rud Derie, Pieter Van den Berghe, Joeri Gerlo,\n  Dirk De Clercq, Veerle Segers, Jesse Davis", "title": "Predicting gait events from tibial acceleration in rearfoot running: a\n  structured machine learning approach", "comments": null, "journal-ref": "Gait & Posture, Volume 84, 2021, Pages 87-92", "doi": "10.1016/j.gaitpost.2020.10.035", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait event detection of the initial contact and toe off is essential for\nrunning gait analysis, allowing the derivation of parameters such as stance\ntime. Heuristic-based methods exist to estimate these key gait events from\ntibial accelerometry. However, these methods are tailored to very specific\nacceleration profiles, which may offer complications when dealing with larger\ndata sets and inherent biological variability. Therefore, this paper\ninvestigates whether a structured machine learning approach can achieve a more\naccurate prediction of running gait event timings from tibial accelerometry.\nForce-based event detection acted as the criterion measure in order to assess\nthe accuracy, repeatability and sensitivity of the predicted gait events. A\nheuristic method and two structured machine learning methods were employed to\nderive initial contact, toe off and stance time from tibial acceleration\nsignals. Both a structured perceptron model (median absolute error of stance\ntime estimation: 10.00 $\\pm$ 8.73 ms) and a structured recurrent neural network\nmodel (median absolute error of stance time estimation: 6.50 $\\pm$ 5.74 ms)\nsignificantly outperformed the existing heuristic approach (median absolute\nerror of stance time estimation: 11.25 $\\pm$ 9.52 ms) on data from 93 rearfoot\nrunners. Thus, results indicate that a structured recurrent neural network\nmachine learning model offers the most accurate and consistent estimation of\nthe gait events and its derived stance time during level overground running.\nThe machine learning methods seem less affected by intra- and inter-subject\nvariation within the data, allowing for accurate and efficient automated data\noutput during rearfoot overground running. Furthermore offering possibilities\nfor real-time monitoring and biofeedback during prolonged measurements, even\noutside the laboratory.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:36:04 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 15:17:18 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 12:20:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Robberechts", "Pieter", ""], ["Derie", "Rud", ""], ["Berghe", "Pieter Van den", ""], ["Gerlo", "Joeri", ""], ["De Clercq", "Dirk", ""], ["Segers", "Veerle", ""], ["Davis", "Jesse", ""]]}, {"id": "1910.13376", "submitter": "Gero Szepannek", "authors": "Gero Szepannek", "title": "How Much Can We See? A Note on Quantifying Explainability of Machine\n  Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most popular approaches to understanding feature effects of modern\nblack box machine learning models are partial dependence plots (PDP). These\nplots are easy to understand but only able to visualize low order dependencies.\nThe paper is about the question 'How much can we see?': A framework is\ndeveloped to quantify the explainability of arbitrary machine learning models,\ni.e. up to what degree the visualization as given by a PDP is able to explain\nthe predictions of the model. The result allows for a judgement whether an\nattempt to explain a black box model is sufficient or not.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:39:35 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 08:20:40 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Szepannek", "Gero", ""]]}, {"id": "1910.13389", "submitter": "Jacky Zhang", "authors": "Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, Oluwasanmi Koyejo", "title": "Learning Sparse Distributions using Iterative Hard Thresholding", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative hard thresholding (IHT) is a projected gradient descent algorithm,\nknown to achieve state of the art performance for a wide range of structured\nestimation problems, such as sparse inference. In this work, we consider IHT as\na solution to the problem of learning sparse discrete distributions. We study\nthe hardness of using IHT on the space of measures. As a practical alternative,\nwe propose a greedy approximate projection which simultaneously captures\nappropriate notions of sparsity in distributions, while satisfying the simplex\nconstraint, and investigate the convergence behavior of the resulting procedure\nin various settings. Our results show, both in theory and practice, that IHT\ncan achieve state of the art results for learning sparse distributions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:53:47 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 05:05:11 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 00:10:10 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Zhang", "Jacky Y.", ""], ["Khanna", "Rajiv", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1910.13393", "submitter": "Santiago Paternain Mr", "authors": "Santiago Paternain, Luiz F.O. Chamon, Miguel Calvo-Fullana, Alejandro\n  Ribeiro", "title": "Constrained Reinforcement Learning Has Zero Duality Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents must often deal with conflicting requirements, such as\ncompleting tasks using the least amount of time/energy, learning multiple\ntasks, or dealing with multiple opponents. In the context of reinforcement\nlearning~(RL), these problems are addressed by (i)~designing a reward function\nthat simultaneously describes all requirements or (ii)~combining modular value\nfunctions that encode them individually. Though effective, these methods have\ncritical downsides. Designing good reward functions that balance different\nobjectives is challenging, especially as the number of objectives grows.\nMoreover, implicit interference between goals may lead to performance plateaus\nas they compete for resources, particularly when training on-policy. Similarly,\nselecting parameters to combine value functions is at least as hard as\ndesigning an all-encompassing reward, given that the effect of their values on\nthe overall policy is not straightforward. The later is generally addressed by\nformulating the conflicting requirements as a constrained RL problem and solved\nusing Primal-Dual methods. These algorithms are in general not guaranteed to\nconverge to the optimal solution since the problem is not convex. This work\nprovides theoretical support to these approaches by establishing that despite\nits non-convexity, this problem has zero duality gap, i.e., it can be solved\nexactly in the dual domain, where it becomes convex. Finally, we show this\nresult basically holds if the policy is described by a good\nparametrization~(e.g., neural networks) and we connect this result with\nprimal-dual algorithms present in the literature and we establish the\nconvergence to the optimal solution.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:56:21 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Paternain", "Santiago", ""], ["Chamon", "Luiz F. O.", ""], ["Calvo-Fullana", "Miguel", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1910.13398", "submitter": "Wu Lin", "authors": "Wu Lin, Mohammad Emtiyaz Khan, Mark Schmidt", "title": "Stein's Lemma for the Reparameterization Trick with Exponential Family\n  Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method (Stein, 1973; 1981) is a powerful tool for statistical\napplications, and has had a significant impact in machine learning. Stein's\nlemma plays an essential role in Stein's method. Previous applications of\nStein's lemma either required strong technical assumptions or were limited to\nGaussian distributions with restricted covariance structures. In this work, we\nextend Stein's lemma to exponential-family mixture distributions including\nGaussian distributions with full covariance structures. Our generalization\nenables us to establish a connection between Stein's lemma and the\nreparamterization trick to derive gradients of expectations of a large class of\nfunctions under weak assumptions. Using this connection, we can derive many new\nreparameterizable gradient-identities that goes beyond the reach of existing\nworks. For example, we give gradient identities when expectation is taken with\nrespect to Student's t-distribution, skew Gaussian, exponentially modified\nGaussian, and normal inverse Gaussian.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:59:22 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Lin", "Wu", ""], ["Khan", "Mohammad Emtiyaz", ""], ["Schmidt", "Mark", ""]]}, {"id": "1910.13401", "submitter": "Diyan Teng", "authors": "Diyan Teng, Rashmi Kulkarni, Justin McGloin", "title": "Model enhancement and personalization using weakly supervised learning\n  for multi-modal mobile sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Always-on sensing of mobile device user's contextual information is critical\nto many intelligent use cases nowadays such as healthcare, drive assistance,\nvoice UI. State-of-the-art approaches for predicting user context have proved\nthe value to leverage multiple sensing modalities for better accuracy. However,\nthose context inference algorithms that run on application processor nowadays\ntend to drain heavy amount of power, making them not suitable for an always-on\nimplementation. We claim that not every sensing modality is suitable to be\nactivated all the time and it remains challenging to build an inference engine\nusing power friendly sensing modalities. Meanwhile, due to the diverse\npopulation, we find it challenging to learn a context inference model that\ngeneralizes well, with limited training data, especially when only using\nalways-on low power sensors. In this work, we propose an approach to leverage\nthe opportunistically-on counterparts in device to improve the always-on\nprediction model, leading to a personalized solution. We model this problem\nusing a weakly supervised learning framework and provide both theoretical and\nexperimental results to validate our design. The proposed framework achieves\nsatisfying result in the IMU based activity recognition application we\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:00:48 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Teng", "Diyan", ""], ["Kulkarni", "Rashmi", ""], ["McGloin", "Justin", ""]]}, {"id": "1910.13406", "submitter": "Meire Fortunato", "authors": "Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri\\`a\n  Puigdom\\`enech Badia, Gavin Buttimore, Charlie Deck, Joel Z Leibo, Charles\n  Blundell", "title": "Generalization of Reinforcement Learners with Working and Episodic\n  Memory", "comments": "NeurIPS 2019. Equal contribution of first 4 authors", "journal-ref": "33rd Conference on Neural Information Processing Systems (Neurips\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory is an important aspect of intelligence and plays a role in many deep\nreinforcement learning models. However, little progress has been made in\nunderstanding when specific memory systems help more than others and how well\nthey generalize. The field also has yet to see a prevalent consistent and\nrigorous approach for evaluating agent performance on holdout data. In this\npaper, we aim to develop a comprehensive methodology to test different kinds of\nmemory in an agent and assess how well the agent can apply what it learns in\ntraining to a holdout set that differs from the training set along dimensions\nthat we suggest are relevant for evaluating memory-specific generalization. To\nthat end, we first construct a diverse set of memory tasks that allow us to\nevaluate test-time generalization across multiple dimensions. Second, we\ndevelop and perform multiple ablations on an agent architecture that combines\nmultiple memory systems, observe its baseline models, and investigate its\nperformance against the task suite.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:07:53 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 01:08:38 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Fortunato", "Meire", ""], ["Tan", "Melissa", ""], ["Faulkner", "Ryan", ""], ["Hansen", "Steven", ""], ["Badia", "Adri\u00e0 Puigdom\u00e8nech", ""], ["Buttimore", "Gavin", ""], ["Deck", "Charlie", ""], ["Leibo", "Joel Z", ""], ["Blundell", "Charles", ""]]}, {"id": "1910.13408", "submitter": "Kate Duffy", "authors": "Kate Duffy, Thomas Vandal, Weile Wang, Ramakrishna Nemani and Auroop\n  R. Ganguly", "title": "Deep Learning Emulation of Multi-Angle Implementation of Atmospheric\n  Correction (MAIAC)", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New generation geostationary satellites make solar reflectance observations\navailable at a continental scale with unprecedented spatiotemporal resolution\nand spectral range. Generating quality land monitoring products requires\ncorrection of the effects of atmospheric scattering and absorption, which vary\nin time and space according to geometry and atmospheric composition. Many\natmospheric radiative transfer models, including that of Multi-Angle\nImplementation of Atmospheric Correction (MAIAC), are too computationally\ncomplex to be run in real time, and rely on precomputed look-up tables.\nAdditionally, uncertainty in measurements and models for remote sensing\nreceives insufficient attention, in part due to the difficulty of obtaining\nsufficient ground measurements. In this paper, we present an adaptation of\nBayesian Deep Learning (BDL) to emulation of the MAIAC atmospheric correction\nalgorithm. Emulation approaches learn a statistical model as an efficient\napproximation of a physical model, while machine learning methods have\ndemonstrated performance in extracting spatial features and learning complex,\nnonlinear mappings. We demonstrate stable surface reflectance retrieval by\nemulation (R2 between MAIAC and emulator SR are 0.63, 0.75, 0.86, 0.84, 0.95,\nand 0.91 for Blue, Green, Red, NIR, SWIR1, and SWIR2 bands, respectively),\naccurate cloud detection (86\\%), and well-calibrated, geolocated uncertainty\nestimates. Our results support BDL-based emulation as an accurate and efficient\n(up to 6x speedup) method for approximation atmospheric correction, where\nbuilt-in uncertainty estimates stand to open new opportunities for model\nassessment and support informed use of SR-derived quantities in multiple\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:11:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Duffy", "Kate", ""], ["Vandal", "Thomas", ""], ["Wang", "Weile", ""], ["Nemani", "Ramakrishna", ""], ["Ganguly", "Auroop R.", ""]]}, {"id": "1910.13413", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Lenon Minorics, and Patrick Bl\\\"obaum", "title": "Feature relevance quantification in explainable AI: A causal problem", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss promising recent contributions on quantifying feature relevance\nusing Shapley values, where we observed some confusion on which probability\ndistribution is the right one for dropped features. We argue that the confusion\nis based on not carefully distinguishing between observational and\ninterventional conditional probabilities and try a clarification based on\nPearl's seminal work on causality. We conclude that unconditional rather than\nconditional expectations provide the right notion of dropping features in\ncontradiction to the theoretical justification of the software package SHAP.\nParts of SHAP are unaffected because unconditional expectations (which we argue\nto be conceptually right) are used as approximation for the conditional ones,\nwhich encouraged others to `improve' SHAP in a way that we believe to be\nflawed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:18:39 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 09:40:41 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Janzing", "Dominik", ""], ["Minorics", "Lenon", ""], ["Bl\u00f6baum", "Patrick", ""]]}, {"id": "1910.13425", "submitter": "Pratik Kayal", "authors": "Pratik Kayal, Mayank Singh, Pawan Goyal", "title": "Weakly-Supervised Deep Learning for Domain Invariant Sentiment\n  Classification", "comments": "5 Pages, 3 tables", "journal-ref": null, "doi": "10.1145/3371158.3371194", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of learning a sentiment classification model that adapts well to any\ntarget domain, different from the source domain, is a challenging problem.\nMajority of the existing approaches focus on learning a common representation\nby leveraging both source and target data during training. In this paper, we\nintroduce a two-stage training procedure that leverages weakly supervised\ndatasets for developing simple lift-and-shift-based predictive models without\nbeing exposed to the target domain during the training phase. Experimental\nresults show that transfer with weak supervision from a source domain to\nvarious target domains provides performance very close to that obtained via\nsupervised training on the target domain itself.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:43:37 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 06:59:48 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Kayal", "Pratik", ""], ["Singh", "Mayank", ""], ["Goyal", "Pawan", ""]]}, {"id": "1910.13427", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, \\'Ulfar Erlingsson, Nicolas Papernot", "title": "Distribution Density, Tails, and Outliers in Machine Learning: Metrics\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop techniques to quantify the degree to which a given (training or\ntesting) example is an outlier in the underlying distribution. We evaluate five\nmethods to score examples in a dataset by how well-represented the examples\nare, for different plausible definitions of \"well-represented\", and apply these\nto four common datasets: MNIST, Fashion-MNIST, CIFAR-10, and ImageNet. Despite\nbeing independent approaches, we find all five are highly correlated,\nsuggesting that the notion of being well-represented can be quantified. Among\nother uses, we find these methods can be combined to identify (a) prototypical\nexamples (that match human expectations); (b) memorized training examples; and,\n(c) uncommon submodes of the dataset. Further, we show how we can utilize our\nmetrics to determine an improved ordering for curriculum learning, and impact\nadversarial robustness. We release all metric values on training and test sets\nwe studied.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:44:35 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Carlini", "Nicholas", ""], ["Erlingsson", "\u00dalfar", ""], ["Papernot", "Nicolas", ""]]}, {"id": "1910.13444", "submitter": "David Barajas-Solano", "authors": "Liu Yang and Sean Treichler and Thorsten Kurth and Keno Fischer and\n  David Barajas-Solano and Josh Romero and Valentin Churavy and Alexandre\n  Tartakovsky and Michael Houston and Prabhat and George Karniadakis", "title": "Highly-scalable, physics-informed GANs for learning solutions of\n  stochastic PDEs", "comments": "3rd Deep Learning on Supercomputers Workshop (DLS) at SC19", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification for forward and inverse problems is a central\nchallenge across physical and biomedical disciplines. We address this challenge\nfor the problem of modeling subsurface flow at the Hanford Site by combining\nstochastic computational models with observational data using physics-informed\nGAN models. The geographic extent, spatial heterogeneity, and multiple\ncorrelation length scales of the Hanford Site require training a\ncomputationally intensive GAN model to thousands of dimensions. We develop a\nhierarchical scheme for exploiting domain parallelism, map discriminators and\ngenerators to multiple GPUs, and employ efficient communication schemes to\nensure training stability and convergence. We developed a highly optimized\nimplementation of this scheme that scales to 27,500 NVIDIA Volta GPUs and 4584\nnodes on the Summit supercomputer with a 93.1% scaling efficiency, achieving\npeak and sustained half-precision rates of 1228 PF/s and 1207 PF/s.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:47:19 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yang", "Liu", ""], ["Treichler", "Sean", ""], ["Kurth", "Thorsten", ""], ["Fischer", "Keno", ""], ["Barajas-Solano", "David", ""], ["Romero", "Josh", ""], ["Churavy", "Valentin", ""], ["Tartakovsky", "Alexandre", ""], ["Houston", "Michael", ""], ["Prabhat", "", ""], ["Karniadakis", "George", ""]]}, {"id": "1910.13445", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Haoze Wu, Clark Barrett, Raghuram Ramanujan, Jure\n  Leskovec", "title": "G2SAT: Learning to Generate SAT Formulas", "comments": "Accepted by NeurIPS 2019. Equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem\nand is fundamental to computer science, with a wide array of applications in\nplanning, verification, and theorem proving. Developing and evaluating\npractical SAT solvers relies on extensive empirical testing on a set of\nreal-world benchmark formulas. However, the availability of such real-world SAT\nformulas is limited. While these benchmark formulas can be augmented with\nsynthetically generated ones, existing approaches for doing so are heavily\nhand-crafted and fail to simultaneously capture a wide range of characteristics\nexhibited by real-world SAT instances. In this work, we present G2SAT, the\nfirst deep generative framework that learns to generate SAT formulas from a\ngiven set of input formulas. Our key insight is that SAT formulas can be\ntransformed into latent bipartite graph representations which we model using a\nspecialized deep generative neural network. We show that G2SAT can generate SAT\nformulas that closely resemble given real-world SAT instances, as measured by\nboth graph metrics and SAT solver behavior. Further, we show that our synthetic\nSAT formulas could be used to improve SAT solver performance on real-world\nbenchmarks, which opens up new opportunities for the continued development of\nSAT solvers and a deeper understanding of their performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 07:48:50 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["You", "Jiaxuan", ""], ["Wu", "Haoze", ""], ["Barrett", "Clark", ""], ["Ramanujan", "Raghuram", ""], ["Leskovec", "Jure", ""]]}, {"id": "1910.13461", "submitter": "Marjan Ghazvininejad", "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\n  Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 18:01:00 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Lewis", "Mike", ""], ["Liu", "Yinhan", ""], ["Goyal", "Naman", ""], ["Ghazvininejad", "Marjan", ""], ["Mohamed", "Abdelrahman", ""], ["Levy", "Omer", ""], ["Stoyanov", "Ves", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1910.13476", "submitter": "Nicolas Garcia Trillos", "authors": "Jeff Calder and Nicolas Garcia Trillos", "title": "Improved spectral convergence rates for graph Laplacians on\n  epsilon-graphs and k-NN graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we improve the spectral convergence rates for graph-based\napproximations of Laplace-Beltrami operators constructed from random data. We\nutilize regularity of the continuum eigenfunctions and strong pointwise\nconsistency results to prove that spectral convergence rates are the same as\nthe pointwise consistency rates for graph Laplacians. In particular, for an\noptimal choice of the graph connectivity $\\varepsilon$, our results show that\nthe eigenvalues and eigenvectors of the graph Laplacian converge to those of\nthe Laplace-Beltrami operator at a rate of $O(n^{-1/(m+4)})$, up to log\nfactors, where $m$ is the manifold dimension and $n$ is the number of vertices\nin the graph. Our approach is general and allows us to analyze a large variety\nof graph constructions that include $\\varepsilon$-graphs and $k$-NN graphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 18:49:15 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 11:36:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Calder", "Jeff", ""], ["Trillos", "Nicolas Garcia", ""]]}, {"id": "1910.13496", "submitter": "Kim Andrea Nicoli", "authors": "Kim A. Nicoli, Shinichi Nakajima, Nils Strodthoff, Wojciech Samek,\n  Klaus-Robert M\u007f\\\"uller, Pan Kessel", "title": "Asymptotically unbiased estimation of physical observables with neural\n  samplers", "comments": "5 figures", "journal-ref": "Phys. Rev. E 101, 023304 (2020)", "doi": "10.1103/PhysRevE.101.023304", "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for the estimation of observables with\ngenerative neural samplers focusing on modern deep generative neural networks\nthat provide an exact sampling probability. In this framework, we present\nasymptotically unbiased estimators for generic observables, including those\nthat explicitly depend on the partition function such as free energy or\nentropy, and derive corresponding variance estimators. We demonstrate their\npractical applicability by numerical experiments for the 2d Ising model which\nhighlight the superiority over existing methods. Our approach greatly enhances\nthe applicability of generative neural samplers to real-world physical systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 19:40:08 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 16:55:06 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nicoli", "Kim A.", ""], ["Nakajima", "Shinichi", ""], ["Strodthoff", "Nils", ""], ["Samek", "Wojciech", ""], ["M\u007f\u00fcller", "Klaus-Robert", ""], ["Kessel", "Pan", ""]]}, {"id": "1910.13503", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis, Hal Daum\\'e III, Jennifer Wortman Vaughan, Hanna\n  Wallach", "title": "Weight of Evidence as a Basis for Human-Oriented Explanations", "comments": "Human-Centric Machine Learning (HCML) Workshop @ NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is an elusive but highly sought-after characteristic of\nmodern machine learning methods. Recent work has focused on interpretability\nvia $\\textit{explanations}$, which justify individual model predictions. In\nthis work, we take a step towards reconciling machine explanations with those\nthat humans produce and prefer by taking inspiration from the study of\nexplanation in philosophy, cognitive science, and the social sciences. We\nidentify key aspects in which these human explanations differ from current\nmachine explanations, distill them into a list of desiderata, and formalize\nthem into a framework via the notion of $\\textit{weight of evidence}$ from\ninformation theory. Finally, we instantiate this framework in two simple\napplications and show it produces intuitive and comprehensible explanations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 19:53:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Daum\u00e9", "Hal", "III"], ["Vaughan", "Jennifer Wortman", ""], ["Wallach", "Hanna", ""]]}, {"id": "1910.13511", "submitter": "Erdem Koyuncu", "authors": "Samuele Battaglino and Erdem Koyuncu", "title": "A Generalization of Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional principal component analysis (PCA) finds a principal vector that\nmaximizes the sum of second powers of principal components. We consider a\ngeneralized PCA that aims at maximizing the sum of an arbitrary convex function\nof principal components. We present a gradient ascent algorithm to solve the\nproblem. For the kernel version of generalized PCA, we show that the solutions\ncan be obtained as fixed points of a simple single-layer recurrent neural\nnetwork. We also evaluate our algorithms on different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:07:22 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 22:13:44 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Battaglino", "Samuele", ""], ["Koyuncu", "Erdem", ""]]}, {"id": "1910.13516", "submitter": "Raghu Bollapragada", "authors": "Raghu Bollapragada and Stefan M. Wild", "title": "Adaptive Sampling Quasi-Newton Methods for Derivative-Free Stochastic\n  Optimization", "comments": "7 pages, NeurIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic zero-order optimization problems, which arise in\nsettings from simulation optimization to reinforcement learning. We propose an\nadaptive sampling quasi-Newton method where we estimate the gradients of a\nstochastic function using finite differences within a common random number\nframework. We employ modified versions of a norm test and an inner product\nquasi-Newton test to control the sample sizes used in the stochastic\napproximations. We provide preliminary numerical experiments to illustrate\npotential performance benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:14:08 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Bollapragada", "Raghu", ""], ["Wild", "Stefan M.", ""]]}, {"id": "1910.13521", "submitter": "Sajjad Azami", "authors": "Hamid Shayestehmanesh, Sajjad Azami, Nishant A. Mehta", "title": "Dying Experts: Efficient Algorithms with Optimal Regret Bounds", "comments": "18 Pages, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of decision-theoretic online learning in which the set of\nexperts that are available to Learner can shrink over time. This is a\nrestricted version of the well-studied sleeping experts problem, itself a\ngeneralization of the fundamental game of prediction with expert advice.\nSimilar to many works in this direction, our benchmark is the ranking regret.\nVarious results suggest that achieving optimal regret in the fully adversarial\nsleeping experts problem is computationally hard. This motivates our relaxation\nwhere any expert that goes to sleep will never again wake up. We call this\nsetting \"dying experts\" and study it in two different cases: the case where the\nlearner knows the order in which the experts will die and the case where the\nlearner does not. In both cases, we provide matching upper and lower bounds on\nthe ranking regret in the fully adversarial setting. Furthermore, we present\nnew, computationally efficient algorithms that obtain our optimal upper bounds.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:28:53 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Shayestehmanesh", "Hamid", ""], ["Azami", "Sajjad", ""], ["Mehta", "Nishant A.", ""]]}, {"id": "1910.13524", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Christopher K. Wikle", "title": "Deep Integro-Difference Equation Models for Spatio-Temporal Forecasting", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100408", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integro-difference equation (IDE) models describe the conditional dependence\nbetween the spatial process at a future time point and the process at the\npresent time point through an integral operator. Nonlinearity or temporal\ndependence in the dynamics is often captured by allowing the operator\nparameters to vary temporally, or by re-fitting a model with a\ntemporally-invariant linear operator in a sliding window. Both procedures tend\nto be excellent for prediction purposes over small time horizons, but are\ngenerally time-consuming and, crucially, do not provide a global prior model\nfor the temporally-varying dynamics that is realistic. Here, we tackle these\ntwo issues by using a deep convolution neural network (CNN) in a hierarchical\nstatistical IDE framework, where the CNN is designed to extract process\ndynamics from the process' most recent behaviour. Once the CNN is fitted,\nprobabilistic forecasting can be done extremely quickly online using an\nensemble Kalman filter with no requirement for repeated parameter estimation.\nWe conduct an experiment where we train the model using 13 years of daily\nsea-surface temperature data in the North Atlantic Ocean. Forecasts are seen to\nbe accurate and calibrated. A key advantage of our approach is that the CNN\nprovides a global prior model for the dynamics that is realistic,\ninterpretable, and computationally efficient. We show the versatility of the\napproach by successfully producing 10-minute nowcasts of weather radar\nreflectivities in Sydney using the same model that was trained on daily\nsea-surface temperature data in the North Atlantic Ocean.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:49:25 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 21:21:49 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 23:24:41 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1910.13540", "submitter": "Augustus Odena", "authors": "Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo\n  Larochelle, Augustus Odena", "title": "Small-GAN: Speeding Up GAN Training Using Core-sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Brock et al. (2018) suggests that Generative Adversarial\nNetworks (GANs) benefit disproportionately from large mini-batch sizes.\nUnfortunately, using large batches is slow and expensive on conventional\nhardware. Thus, it would be nice if we could generate batches that were\neffectively large though actually small. In this work, we propose a method to\ndo this, inspired by the use of Coreset-selection in active learning. When\ntraining a GAN, we draw a large batch of samples from the prior and then\ncompress that batch using Coreset-selection. To create effectively large\nbatches of 'real' images, we create a cached dataset of Inception activations\nof each training image, randomly project them down to a smaller dimension, and\nthen use Coreset-selection on those projected activations at training time. We\nconduct experiments showing that this technique substantially reduces training\ntime and memory usage for modern GAN variants, that it reduces the fraction of\ndropped modes in a synthetic dataset, and that it allows GANs to reach a new\nstate of the art in anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:26:05 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Sinha", "Samarth", ""], ["Zhang", "Han", ""], ["Goyal", "Anirudh", ""], ["Bengio", "Yoshua", ""], ["Larochelle", "Hugo", ""], ["Odena", "Augustus", ""]]}, {"id": "1910.13556", "submitter": "Jonathan Gordon", "authors": "Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James\n  Requeima, Yann Dubois, Richard E. Turner", "title": "Convolutional Conditional Neural Processes", "comments": "Accepted at International Conference on Learning Representations 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Convolutional Conditional Neural Process (ConvCNP), a new\nmember of the Neural Process family that models translation equivariance in the\ndata. Translation equivariance is an important inductive bias for many learning\nproblems including time series modelling, spatial data, and images. The model\nembeds data sets into an infinite-dimensional function space as opposed to a\nfinite-dimensional vector space. To formalize this notion, we extend the theory\nof neural representations of sets to include functional representations, and\ndemonstrate that any translation-equivariant embedding can be represented using\na convolutional deep set. We evaluate ConvCNPs in several settings,\ndemonstrating that they achieve state-of-the-art performance compared to\nexisting NPs. We demonstrate that building in translation equivariance enables\nzero-shot generalization to challenging, out-of-domain tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:56:00 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 14:26:44 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 13:23:19 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 13:07:49 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 13:20:06 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Gordon", "Jonathan", ""], ["Bruinsma", "Wessel P.", ""], ["Foong", "Andrew Y. K.", ""], ["Requeima", "James", ""], ["Dubois", "Yann", ""], ["Turner", "Richard E.", ""]]}, {"id": "1910.13561", "submitter": "Mohamed Medhat Gaber", "authors": "Safwan Shatnawi, Mohamed Medhat Gaber, Mihaela Cocea", "title": "A Heuristically Modified FP-Tree for Ontology Learning with Applications\n  in Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a heuristically modified FP-Tree for ontology learning from text.\nUnlike previous research, for concept extraction, we use a regular expression\nparser approach widely adopted in compiler construction, i.e., deterministic\nfinite automata (DFA). Thus, the concepts are extracted from unstructured\ndocuments. For ontology learning, we use a frequent pattern mining approach and\nemploy a rule mining heuristic function to enhance its quality. This process\ndoes not rely on predefined lexico-syntactic patterns, thus, it is applicable\nfor different subjects. We employ the ontology in a question-answering system\nfor students' content-related questions. For validation, we used textbook\nquestions/answers and questions from online course forums. Subject experts\nrated the quality of the system's answers on a subset of questions and their\nratings were used to identify the most appropriate automatic semantic text\nsimilarity metric to use as a validation metric for all answers. The Latent\nSemantic Analysis was identified as the closest to the experts' ratings. We\ncompared the use of our ontology with the use of Text2Onto for the\nquestion-answering system and found that with our ontology 80% of the questions\nwere answered, while with Text2Onto only 28.4% were answered, thanks to the\nfiner grained hierarchy our approach is able to produce.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 22:12:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Shatnawi", "Safwan", ""], ["Gaber", "Mohamed Medhat", ""], ["Cocea", "Mihaela", ""]]}, {"id": "1910.13565", "submitter": "Andrew Wilson", "authors": "Gregory W. Benton, Wesley J. Maddox, Jayson P. Salkey, Julio Albinati,\n  Andrew Gordon Wilson", "title": "Function-Space Distributions over Kernels", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are flexible function approximators, with inductive biases\ncontrolled by a covariance kernel. Learning the kernel is the key to\nrepresentation learning and strong predictive performance. In this paper, we\ndevelop functional kernel learning (FKL) to directly infer functional\nposteriors over kernels. In particular, we place a transformed Gaussian process\nover a spectral density, to induce a non-parametric distribution over kernel\nfunctions. The resulting approach enables learning of rich representations,\nwith support for any stationary kernel, uncertainty over the values of the\nkernel, and an interpretable specification of a prior directly over kernels,\nwithout requiring sophisticated initialization or manual intervention. We\nperform inference through elliptical slice sampling, which is especially well\nsuited to marginalizing posteriors with the strongly correlated priors typical\nto function space modelling. We develop our approach for non-uniform,\nlarge-scale, multi-task, and multidimensional data, and show promising\nperformance in a wide range of settings, including interpolation,\nextrapolation, and kernel recovery experiments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 22:26:38 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Benton", "Gregory W.", ""], ["Maddox", "Wesley J.", ""], ["Salkey", "Jayson P.", ""], ["Albinati", "Julio", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1910.13573", "submitter": "Neil Deshmukh", "authors": "Neil Deshmukh, Selin Gumustop, Romane Gauriau, Varun Buch, Bradley\n  Wright, Christopher Bridge, Ram Naidu, Katherine Andriole, and Bernardo Bizzo", "title": "Semi-Supervised Natural Language Approach for Fine-Grained\n  Classification of Medical Reports", "comments": "Accepted for IEEE publication & presented at MIT URTC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although machine learning has become a powerful tool to augment doctors in\nclinical analysis, the immense amount of labeled data that is necessary to\ntrain supervised learning approaches burdens each development task as time and\nresource intensive. The vast majority of dense clinical information is stored\nin written reports, detailing pertinent patient information. The challenge with\nutilizing natural language data for standard model development is due to the\ncomplex nature of the modality. In this research, a model pipeline was\ndeveloped to utilize an unsupervised approach to train an encoder-language\nmodel, a recurrent network, to generate document encodings; which then can be\nused as features passed into a decoder-classifier model that requires\nmagnitudes less labeled data than previous approaches to differentiate between\nfine-grained disease classes accurately. The language model was trained on\nunlabeled radiology reports from the Massachusetts General Hospital Radiology\nDepartment (n=218,159) and terminated with a loss of 1.62. The classification\nmodels were trained on three labeled datasets of head CT studies of reported\npatients, presenting large vessel occlusion (n=1403), acute ischemic strokes\n(n=331), and intracranial hemorrhage (n=4350), to identify a variety of\ndifferent findings directly from the radiology report data; resulting in AUCs\nof 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute\nischemic stroke, and intracranial hemorrhage datasets. The output encodings are\nable to be used in conjunction with imaging data, to create models that can\nprocess a multitude of different modalities. The ability to automatically\nextract relevant features from textual data allows for faster model development\nand integration of textual modality, overall, allowing clinical reports to\nbecome a more viable input for more encompassing and accurate deep learning\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 23:25:59 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 03:18:19 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Deshmukh", "Neil", ""], ["Gumustop", "Selin", ""], ["Gauriau", "Romane", ""], ["Buch", "Varun", ""], ["Wright", "Bradley", ""], ["Bridge", "Christopher", ""], ["Naidu", "Ram", ""], ["Andriole", "Katherine", ""], ["Bizzo", "Bernardo", ""]]}, {"id": "1910.13574", "submitter": "Amir Mosavi Prof", "authors": "Sanaz Mojrian, Gergo Pinter, Javad Hassannataj Joloudari, Imre Felde,\n  Narjes Nabipour, Laszlo Nadai, Amir Mosavi", "title": "Hybrid Machine Learning Model of Extreme Learning Machine Radial basis\n  function for Breast Cancer Detection and Diagnosis; a Multilayer Fuzzy Expert\n  System", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammography is often used as the most common laboratory method for the\ndetection of breast cancer, yet associated with the high cost and many side\neffects. Machine learning prediction as an alternative method has shown\npromising results. This paper presents a method based on a multilayer fuzzy\nexpert system for the detection of breast cancer using an extreme learning\nmachine (ELM) classification model integrated with radial basis function (RBF)\nkernel called ELM-RBF, considering the Wisconsin dataset. The performance of\nthe proposed model is further compared with a linear-SVM model. The proposed\nmodel outperforms the linear-SVM model with RMSE, R2, MAPE equal to 0.1719,\n0.9374 and 0.0539, respectively. Furthermore, both models are studied in terms\nof criteria of accuracy, precision, sensitivity, specificity, validation, true\npositive rate (TPR), and false-negative rate (FNR). The ELM-RBF model for these\ncriteria presents better performance compared to the SVM model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 23:33:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Mojrian", "Sanaz", ""], ["Pinter", "Gergo", ""], ["Joloudari", "Javad Hassannataj", ""], ["Felde", "Imre", ""], ["Nabipour", "Narjes", ""], ["Nadai", "Laszlo", ""], ["Mosavi", "Amir", ""]]}, {"id": "1910.13593", "submitter": "Tyler Lee", "authors": "Tyler Lee and Anthony Ndirango", "title": "Generalization in multitask deep neural classifiers: a statistical\n  physics approach", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper understanding of the striking generalization abilities of deep\nneural networks presents an enduring puzzle. Recently, there has been a growing\nbody of numerically-grounded theoretical work that has contributed important\ninsights to the theory of learning in deep neural nets. There has also been a\nrecent interest in extending these analyses to understanding how multitask\nlearning can further improve the generalization capacity of deep neural nets.\nThese studies deal almost exclusively with regression tasks which are amenable\nto existing analytical techniques. We develop an analytic theory of the\nnonlinear dynamics of generalization of deep neural networks trained to solve\nclassification tasks using softmax outputs and cross-entropy loss, addressing\nboth single task and multitask settings. We do so by adapting techniques from\nthe statistical physics of disordered systems, accounting for both finite size\ndatasets and correlated outputs induced by the training dynamics. We discuss\nthe validity of our theoretical results in comparison to a comprehensive suite\nof numerical experiments. Our analysis provides theoretical support for the\nintuition that the performance of multitask learning is determined by the\nnoisiness of the tasks and how well their input features align with each other.\nHighly related, clean tasks benefit each other, whereas unrelated, clean tasks\ncan be detrimental to individual task performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:25:04 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Lee", "Tyler", ""], ["Ndirango", "Anthony", ""]]}, {"id": "1910.13598", "submitter": "Farzin Haddadpour", "authors": "Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck R.\n  Cadambe", "title": "Local SGD with Periodic Averaging: Tighter Analysis and Adaptive\n  Synchronization", "comments": "Paper accepted to NeurIPS 2019 - We fixed a flaw in the earlier\n  version regarding the dependency on constants but this change does not affect\n  the communication complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead is one of the key challenges that hinders the\nscalability of distributed optimization algorithms. In this paper, we study\nlocal distributed SGD, where data is partitioned among computation nodes, and\nthe computation nodes perform local updates with periodically exchanging the\nmodel among the workers to perform averaging. While local SGD is empirically\nshown to provide promising results, a theoretical understanding of its\nperformance remains open. We strengthen convergence analysis for local SGD, and\nshow that local SGD can be far less expensive and applied far more generally\nthan current theory suggests. Specifically, we show that for loss functions\nthat satisfy the Polyak-{\\L}ojasiewicz condition, $O((pT)^{1/3})$ rounds of\ncommunication suffice to achieve a linear speed up, that is, an error of\n$O(1/pT)$, where $T$ is the total number of model updates at each worker. This\nis in contrast with previous work which required higher number of communication\nrounds, as well as was limited to strongly convex loss functions, for a similar\nasymptotic performance. We also develop an adaptive synchronization scheme that\nprovides a general condition for linear speed up. Finally, we validate the\ntheory with experimental results, running over AWS EC2 clouds and an internal\nGPU cluster.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:35:38 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 16:04:26 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Kamani", "Mohammad Mahdi", ""], ["Mahdavi", "Mehrdad", ""], ["Cadambe", "Viveck R.", ""]]}, {"id": "1910.13601", "submitter": "Guansong Pang", "authors": "Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel", "title": "Deep Weakly-supervised Anomaly Detection", "comments": "Theoretical results are refined and extended. Significant more\n  empirical results are added, including results on detecting previously\n  unknown anomalies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is typically posited as an unsupervised learning task in\nthe literature due to the prohibitive cost and difficulty to obtain large-scale\nlabeled anomaly data, but this ignores the fact that a very small number\n(e.g.,, a few dozens) of labeled anomalies can often be made available with\nsmall/trivial cost in many real-world anomaly detection applications. To\nleverage such labeled anomaly data, we study an important anomaly detection\nproblem termed weakly-supervised anomaly detection, in which, in addition to a\nlarge amount of unlabeled data, a limited number of labeled anomalies are\navailable during modeling. Learning with the small labeled anomaly data enables\nanomaly-informed modeling, which helps identify anomalies of interest and\naddress the notorious high false positives in unsupervised anomaly detection.\nHowever, the problem is especially challenging, since (i) the limited amount of\nlabeled anomaly data often, if not always, cannot cover all types of anomalies\nand (ii) the unlabeled data is often dominated by normal instances but has\nanomaly contamination. We address the problem by formulating it as a pairwise\nrelation prediction task. Particularly, our approach defines a two-stream\nordinal regression neural network to learn the relation of randomly sampled\ninstance pairs, i.e., whether the instance pair contains two labeled anomalies,\none labeled anomaly, or just unlabeled data instances. The resulting model\neffectively leverages both the labeled and unlabeled data to substantially\naugment the training data and learn well-generalized representations of\nnormality and abnormality. Comprehensive empirical results on 40 real-world\ndatasets show that our approach (i) significantly outperforms four\nstate-of-the-art methods in detecting both of the known and previously unseen\nanomalies and (ii) is substantially more data-efficient.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:40:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 04:59:36 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 05:21:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pang", "Guansong", ""], ["Shen", "Chunhua", ""], ["Jin", "Huidong", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1910.13603", "submitter": "S\\'ebastien Arnold", "authors": "S\\'ebastien M.R. Arnold, Shariq Iqbal, Fei Sha", "title": "When MAML Can Adapt Fast and How to Assist When It Cannot", "comments": "Accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Agnostic Meta-Learning (MAML) and its variants have achieved success in\nmeta-learning tasks on many datasets and settings. On the other hand, we have\njust started to understand and analyze how they are able to adapt fast to new\ntasks. For example, one popular hypothesis is that the algorithms learn good\nrepresentations for transfer, as in multi-task learning. In this work, we\ncontribute by providing a series of empirical and theoretical studies, and\ndiscover several interesting yet previously unknown properties of the\nalgorithm. We find MAML adapts better with a deep architecture even if the\ntasks need only a shallow one (and thus, no representation learning is needed).\nWhile echoing previous findings by others that the bottom layers in deep\narchitectures enable representation learning, we also find that upper layers\nenable fast adaptation by being meta-learned to perform adaptive gradient\nupdate when generalizing to new tasks. Motivated by these findings, we study\nseveral meta-optimization approaches and propose a new one for learning to\noptimize adaptively. Those approaches attain stronger performance in\nmeta-learning both shallower and deeper architectures than MAML.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:50:42 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 04:39:00 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 23:55:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Arnold", "S\u00e9bastien M. R.", ""], ["Iqbal", "Shariq", ""], ["Sha", "Fei", ""]]}, {"id": "1910.13613", "submitter": "Yuxiao Liu", "authors": "Yuxiao Liu, Bolun Xu, Audun Botterud, Ning Zhang, and Chongqing Kang", "title": "Bounding Regression Errors in Data-driven Power Grid Steady-state Models", "comments": "11 pages, 10 fugures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven models analyze power grids under incomplete physical information,\nand their accuracy has been mostly validated empirically using certain training\nand testing datasets. This paper explores error bounds for data-driven models\nunder all possible training and testing scenarios, and proposes an evaluation\nimplementation based on Rademacher complexity theory. We answer key questions\nfor data-driven models: how much training data is required to guarantee a\ncertain error bound, and how partial physical knowledge can be utilized to\nreduce the required amount of data. Our results are crucial for the evaluation\nand application of data-driven models in power grid analysis. We demonstrate\nthe proposed method by finding generalization error bounds for two\napplications, i.e. branch flow linearization and external network equivalent\nunder different degrees of physical knowledge. Results identify how the bounds\ndecrease with additional power grid physical knowledge or more training data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:20:10 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 14:39:58 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Liu", "Yuxiao", ""], ["Xu", "Bolun", ""], ["Botterud", "Audun", ""], ["Zhang", "Ning", ""], ["Kang", "Chongqing", ""]]}, {"id": "1910.13614", "submitter": "Ruosong Wang", "authors": "Simon S. Du, Ruosong Wang, Mengdi Wang, Lin F. Yang", "title": "Continuous Control with Contexts, Provably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in artificial intelligence is to build an agent that\ngeneralizes and adapts to unseen environments. A common strategy is to build a\ndecoder that takes the context of the unseen new environment as input and\ngenerates a policy accordingly. The current paper studies how to build a\ndecoder for the fundamental continuous control task, linear quadratic regulator\n(LQR), which can model a wide range of real-world physical environments. We\npresent a simple algorithm for this problem, which uses upper confidence bound\n(UCB) to refine the estimate of the decoder and balance the\nexploration-exploitation trade-off. Theoretically, our algorithm enjoys a\n$\\widetilde{O}\\left(\\sqrt{T}\\right)$ regret bound in the online setting where\n$T$ is the number of environments the agent played. This also implies after\nplaying $\\widetilde{O}\\left(1/\\epsilon^2\\right)$ environments, the agent is\nable to transfer the learned knowledge to obtain an $\\epsilon$-suboptimal\npolicy for an unseen environment. To our knowledge, this is first provably\nefficient algorithm to build a decoder in the continuous control setting. While\nour main focus is theoretical, we also present experiments that demonstrate the\neffectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:20:58 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Ruosong", ""], ["Wang", "Mengdi", ""], ["Yang", "Lin F.", ""]]}, {"id": "1910.13616", "submitter": "Risto Vuorio", "authors": "Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J. Lim", "title": "Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-agnostic meta-learners aim to acquire meta-learned parameters from\nsimilar tasks to adapt to novel tasks from the same distribution with few\ngradient updates. With the flexibility in the choice of models, those\nframeworks demonstrate appealing performance on a variety of domains such as\nfew-shot image classification and reinforcement learning. However, one\nimportant limitation of such frameworks is that they seek a common\ninitialization shared across the entire task distribution, substantially\nlimiting the diversity of the task distributions that they are able to learn\nfrom. In this paper, we augment MAML with the capability to identify the mode\nof tasks sampled from a multimodal task distribution and adapt quickly through\ngradient updates. Specifically, we propose a multimodal MAML (MMAML) framework,\nwhich is able to modulate its meta-learned prior parameters according to the\nidentified mode, allowing more efficient fast adaptation. We evaluate the\nproposed model on a diverse set of few-shot learning tasks, including\nregression, image classification, and reinforcement learning. The results not\nonly demonstrate the effectiveness of our model in modulating the meta-learned\nprior in response to the characteristics of tasks but also show that training\non a multimodal distribution can produce an improvement over unimodal training.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:35:19 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Vuorio", "Risto", ""], ["Sun", "Shao-Hua", ""], ["Hu", "Hexiang", ""], ["Lim", "Joseph J.", ""]]}, {"id": "1910.13618", "submitter": "Chen Dan", "authors": "Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, Pradeep Ravikumar", "title": "Optimal Analysis of Subset-Selection Based L_p Low Rank Approximation", "comments": "20 pages, accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the low rank approximation problem of any given matrix $A$ over\n$\\mathbb{R}^{n\\times m}$ and $\\mathbb{C}^{n\\times m}$ in entry-wise $\\ell_p$\nloss, that is, finding a rank-$k$ matrix $X$ such that $\\|A-X\\|_p$ is\nminimized. Unlike the traditional $\\ell_2$ setting, this particular variant is\nNP-Hard. We show that the algorithm of column subset selection, which was an\nalgorithmic foundation of many existing algorithms, enjoys approximation ratio\n$(k+1)^{1/p}$ for $1\\le p\\le 2$ and $(k+1)^{1-1/p}$ for $p\\ge 2$. This improves\nupon the previous $O(k+1)$ bound for $p\\ge 1$\n\\cite{chierichetti2017algorithms}. We complement our analysis with lower\nbounds; these bounds match our upper bounds up to constant $1$ when $p\\geq 2$.\nAt the core of our techniques is an application of \\emph{Riesz-Thorin\ninterpolation theorem} from harmonic analysis, which might be of independent\ninterest to other algorithmic designs and analysis more broadly.\n  As a consequence of our analysis, we provide better approximation guarantees\nfor several other algorithms with various time complexity. For example, to make\nthe algorithm of column subset selection computationally efficient, we analyze\na polynomial time bi-criteria algorithm which selects $O(k\\log m)$ columns. We\nshow that this algorithm has an approximation ratio of $O((k+1)^{1/p})$ for\n$1\\le p\\le 2$ and $O((k+1)^{1-1/p})$ for $p\\ge 2$. This improves over the\nbest-known bound with an $O(k+1)$ approximation ratio. Our bi-criteria\nalgorithm also implies an exact-rank method in polynomial time with a slightly\nlarger approximation ratio.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:42:20 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dan", "Chen", ""], ["Wang", "Hong", ""], ["Zhang", "Hongyang", ""], ["Zhou", "Yuchen", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1910.13631", "submitter": "Yijun Bian", "authors": "Yijun Bian, Huanhuan Chen", "title": "When does Diversity Help Generalization in Classification Ensembles?", "comments": "Accepted by the IEEE Transactions on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2021.3053165", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles, as a widely used and effective technique in the machine learning\ncommunity, succeed within a key element -- \"diversity.\" The relationship\nbetween diversity and generalization, unfortunately, is not entirely understood\nand remains an open research issue. To reveal the effect of diversity on the\ngeneralization of classification ensembles, we investigate three issues on\ndiversity, i.e., the measurement of diversity, the relationship between the\nproposed diversity and the generalization error, and the utilization of this\nrelationship for ensemble pruning. In the diversity measurement, we measure\ndiversity by error decomposition inspired by regression ensembles, which\ndecomposes the error of classification ensembles into accuracy and diversity.\nThen we formulate the relationship between the measured diversity and ensemble\nperformance through the theorem of margin and generalization and observe that\nthe generalization error is reduced effectively only when the measured\ndiversity is increased in a few specific ranges, while in other ranges larger\ndiversity is less beneficial to increasing the generalization of an ensemble.\nBesides, we propose two pruning methods based on diversity management to\nutilize this relationship, which could increase diversity appropriately and\nshrink the size of the ensemble without much-decreasing performance. Empirical\nresults validate the reasonableness of the proposed relationship between\ndiversity and ensemble generalization error and the effectiveness of the\nproposed pruning methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 02:39:08 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 05:25:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bian", "Yijun", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1910.13659", "submitter": "Quanquan Gu", "authors": "Lingxiao Wang and Bargav Jayaraman and David Evans and Quanquan Gu", "title": "Efficient Privacy-Preserving Stochastic Nonconvex Optimization", "comments": "28 pages, 5 figures, 6 tables. This version improves the algorithm\n  and the presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many solutions for privacy-preserving convex empirical risk\nminimization (ERM) have been developed, privacy-preserving nonconvex ERM\nremains a challenge. We study nonconvex ERM, which takes the form of minimizing\na finite-sum of nonconvex loss functions over a training set. We propose a new\ndifferentially private stochastic gradient descent algorithm for nonconvex ERM\nthat achieves strong privacy guarantees efficiently, and provide a tight\nanalysis of its privacy and utility guarantees, as well as its gradient\ncomplexity. Our algorithm substantially reduces gradient complexity while\nmatching the best previous utility guarantee given by Wang et al. (2017). We\nextend our algorithm to the distributed setting using secure multi-party\ncomputation, and show it is possible for a distributed algorithm to match the\nprivacy and utility guarantees of a centralized algorithm in this setting. Our\nexperiments on benchmark nonconvex ERM problems demonstrate superior\nperformance in terms of both training cost and utility gains compared with\nprevious differentially private methods using the same privacy budgets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:32:56 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 17:43:19 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wang", "Lingxiao", ""], ["Jayaraman", "Bargav", ""], ["Evans", "David", ""], ["Gu", "Quanquan", ""]]}, {"id": "1910.13672", "submitter": "Mojtaba Sahraee Ardakan", "authors": "M. Emami, M. Sahraee-Ardakan, S. Rangan, A. K. Fletcher", "title": "Input-Output Equivalence of Unitary and Contractive RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unitary recurrent neural networks (URNNs) have been proposed as a method to\novercome the vanishing and exploding gradient problem in modeling data with\nlong-term dependencies. A basic question is how restrictive is the unitary\nconstraint on the possible input-output mappings of such a network? This work\nshows that for any contractive RNN with ReLU activations, there is a URNN with\nat most twice the number of hidden states and the identical input-output\nmapping. Hence, with ReLU activations, URNNs are as expressive as general RNNs.\nIn contrast, for certain smooth activations, it is shown that the input-output\nmapping of an RNN cannot be matched with a URNN, even with an arbitrary number\nof states. The theoretical results are supported by experiments on modeling of\nslowly-varying dynamical systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:02:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Emami", "M.", ""], ["Sahraee-Ardakan", "M.", ""], ["Rangan", "S.", ""], ["Fletcher", "A. K.", ""]]}, {"id": "1910.13673", "submitter": "Mingyuan Zhou", "authors": "Zhendong Wang and Mingyuan Zhou", "title": "Thompson Sampling via Local Uncertainty", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is an efficient algorithm for sequential decision making,\nwhich exploits the posterior uncertainty to address the\nexploration-exploitation dilemma. There has been significant recent interest in\nintegrating Bayesian neural networks into Thompson sampling. Most of these\nmethods rely on global variable uncertainty for exploration. In this paper, we\npropose a new probabilistic modeling framework for Thompson sampling, where\nlocal latent variable uncertainty is used to sample the mean reward.\nVariational inference is used to approximate the posterior of the local\nvariable, and semi-implicit structure is further introduced to enhance its\nexpressiveness. Our experimental results on eight contextual bandit benchmark\ndatasets show that Thompson sampling guided by local uncertainty achieves\nstate-of-the-art performance while having low computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:05:30 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:38:02 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:18:02 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wang", "Zhendong", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1910.13726", "submitter": "Matteo Turchetta", "authors": "Matteo Turchetta, Felix Berkenkamp, Andreas Krause", "title": "Safe Exploration for Interactive Machine Learning", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Interactive Machine Learning (IML), we iteratively make decisions and\nobtain noisy observations of an unknown function. While IML methods, e.g.,\nBayesian optimization and active learning, have been successful in\napplications, on real-world systems they must provably avoid unsafe decisions.\nTo this end, safe IML algorithms must carefully learn about a priori unknown\nconstraints without making unsafe decisions. Existing algorithms for this\nproblem learn about the safety of all decisions to ensure convergence. This is\nsample-inefficient, as it explores decisions that are not relevant for the\noriginal IML objective. In this paper, we introduce a novel framework that\nrenders any existing unsafe IML algorithm safe. Our method works as an add-on\nthat takes suggested decisions as input and exploits regularity assumptions in\nterms of a Gaussian process prior in order to efficiently learn about their\nsafety. As a result, we only explore the safe set when necessary for the IML\nproblem. We apply our framework to safe Bayesian optimization and to safe\nexploration in deterministic Markov Decision Processes (MDP), which have been\nanalyzed separately before. Our method outperforms other algorithms\nempirically.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 09:12:48 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Turchetta", "Matteo", ""], ["Berkenkamp", "Felix", ""], ["Krause", "Andreas", ""]]}, {"id": "1910.13728", "submitter": "Jia Guo", "authors": "Jia Guo and Chenyang Yang", "title": "Structure of Deep Neural Networks with a Priori Information in Wireless\n  Tasks", "comments": "6 pages, 2 figures, Submitted to ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been employed for designing wireless\nnetworks in many aspects, such as transceiver optimization, resource\nallocation, and information prediction. Existing works either use\nfully-connected DNN or the DNNs with specific structures that are designed in\nother domains. In this paper, we show that a priori information widely existed\nin wireless tasks is permutation invariant. For these tasks, we propose a DNN\nwith special structure, where the weight matrices between layers of the DNN\nonly consist of two smaller sub-matrices. By such way of parameter sharing, the\nnumber of model parameters reduces, giving rise to low sample and computational\ncomplexity for training a DNN. We take predictive resource allocation as an\nexample to show how the designed DNN can be applied for learning the optimal\npolicy with unsupervised learning. Simulations results validate our analysis\nand show dramatic gain of the proposed structure in terms of reducing training\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 09:15:38 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 07:08:13 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Guo", "Jia", ""], ["Yang", "Chenyang", ""]]}, {"id": "1910.13800", "submitter": "Abhishek Ghose", "authors": "Abhishek Ghose", "title": "Rational Kernels: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many kinds of data are naturally amenable to being treated as sequences. An\nexample is text data, where a text may be seen as a sequence of words. Another\nexample is clickstream data, where a data instance is a sequence of clicks made\nby a visitor to a website. This is also common for data originating in the\ndomains of speech processing and computational biology. Using such data with\nstatistical learning techniques can often prove to be cumbersome since most of\nthem only allow fixed-length feature vectors as input. In casting the data to\nfixed-length feature vectors to suit these techniques, we lose the convenience,\nand possibly information, a good sequence-based representation can offer. The\nframework of rational kernels partly addresses this problem by providing an\nelegant representation for sequences, for algorithms that use kernel functions.\nIn this report, we take a comprehensive look at this framework, its various\nextensions and applications. We start with an overview of the core ideas, where\nwe look at the characterization of rational kernels, and then extend our\ndiscussion to extensions, applications and use at scale. Rational kernels\nrepresent a family of kernels, and thus, learning an appropriate rational\nkernel instead of picking one, suggests a convenient way to use them; we\nexplore this idea in our concluding section. Rational kernels are not as\npopular as the many other learning techniques in use today; however, we hope\nthat this summary effectively shows that not only is their theory\nwell-developed, but also that various practical aspects have been carefully\nstudied over time.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 19:16:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Ghose", "Abhishek", ""]]}, {"id": "1910.13804", "submitter": "Thomas Adler", "authors": "Thomas Adler, Manuel Erhard, Mario Krenn, Johannes Brandstetter,\n  Johannes Kofler, Sepp Hochreiter", "title": "Quantum Optical Experiments Modeled by Long Short-Term Memory", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how machine learning is able to model experiments in quantum\nphysics. Quantum entanglement is a cornerstone for upcoming quantum\ntechnologies such as quantum computation and quantum cryptography. Of\nparticular interest are complex quantum states with more than two particles and\na large number of entangled quantum levels. Given such a multiparticle\nhigh-dimensional quantum state, it is usually impossible to reconstruct an\nexperimental setup that produces it. To search for interesting experiments, one\nthus has to randomly create millions of setups on a computer and calculate the\nrespective output states. In this work, we show that machine learning models\ncan provide significant improvement over random search. We demonstrate that a\nlong short-term memory (LSTM) neural network can successfully learn to model\nquantum experiments by correctly predicting output state characteristics for\ngiven setups without the necessity of computing the states themselves. This\napproach not only allows for faster search but is also an essential step\ntowards automated design of multiparticle high-dimensional quantum experiments\nusing generative machine learning models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 12:35:46 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Adler", "Thomas", ""], ["Erhard", "Manuel", ""], ["Krenn", "Mario", ""], ["Brandstetter", "Johannes", ""], ["Kofler", "Johannes", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1910.13824", "submitter": "Henry Martin", "authors": "Henry Martin, Ye Hong, Dominik Bucher, Christian Rupprecht, Ren\\'e\n  Buffat", "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab", "comments": null, "journal-ref": null, "doi": "10.3929/ethz-b-000388707", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the IARAI competition traffic4cast was to predict the city-wide\ntraffic status within a 15-minute time window, based on information from the\nprevious hour. The traffic status was given as multi-channel images (one pixel\nroughly corresponds to 100x100 meters), where one channel indicated the traffic\nvolume, another one the average speed of vehicles, and a third one their rough\nheading. As part of our work on the competition, we evaluated many different\nnetwork architectures, analyzed the statistical properties of the given data in\ndetail, and thought about how to transform the problem to be able to take\nadditional spatio-temporal context-information into account, such as the street\nnetwork, the positions of traffic lights, or the weather. This document\nsummarizes our efforts that led to our best submission, and gives some insights\nabout which other approaches we evaluated, and why they did not work as well as\nimagined.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:39:14 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 10:10:48 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Martin", "Henry", ""], ["Hong", "Ye", ""], ["Bucher", "Dominik", ""], ["Rupprecht", "Christian", ""], ["Buffat", "Ren\u00e9", ""]]}, {"id": "1910.13827", "submitter": "Nikhil Oswal", "authors": "Nikhil Oswal", "title": "Predicting Rainfall using Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall prediction is one of the challenging and uncertain tasks which has a\nsignificant impact on human society. Timely and accurate predictions can help\nto proactively reduce human and financial loss. This study presents a set of\nexperiments which involve the use of prevalent machine learning techniques to\nbuild models to predict whether it is going to rain tomorrow or not based on\nweather data for that particular day in major cities of Australia. This\ncomparative study is conducted concentrating on three aspects: modeling inputs,\nmodeling methods, and pre-processing techniques. The results provide a\ncomparison of various evaluation metrics of these machine learning techniques\nand their reliability to predict the rainfall by analyzing the weather data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 01:42:15 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Oswal", "Nikhil", ""]]}, {"id": "1910.13830", "submitter": "Tharun Kumar Reddy Medini", "authors": "Tharun Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, Anshumali\n  Shrivastava", "title": "Extreme Classification in Log Memory using Count-Min Sketch: A Case\n  Study of Amazon Search with 50M Products", "comments": "Published at NeurIPS 2019. arXiv admin note: text overlap with\n  arXiv:1810.04254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, it has been shown that many hard AI tasks, especially in\nNLP, can be naturally modeled as extreme classification problems leading to\nimproved precision. However, such models are prohibitively expensive to train\ndue to the memory blow-up in the last layer. For example, a reasonable softmax\nlayer for the dataset of interest in this paper can easily reach well beyond\n100 billion parameters (>400 GB memory). To alleviate this problem, we present\nMerged-Average Classifiers via Hashing (MACH), a generic K-classification\nalgorithm where memory provably scales at O(logK) without any strong assumption\non the classes. MACH is subtly a count-min sketch structure in disguise, which\nuses universal hashing to reduce classification with a large number of classes\nto few embarrassingly parallel and independent classification tasks with a\nsmall (constant) number of classes. MACH naturally provides a technique for\nzero communication model parallelism. We experiment with 6 datasets; some\nmulticlass and some multilabel, and show consistent improvement over respective\nstate-of-the-art baselines. In particular, we train an end-to-end deep\nclassifier on a private product search dataset sampled from Amazon Search\nEngine with 70 million queries and 49.46 million products. MACH outperforms, by\na significant margin,the state-of-the-art extreme classification models\ndeployed on commercial search engines: Parabel and dense embedding models. Our\nlargest model has 6.4 billion parameters and trains in less than 35 hours on a\nsingle p3.16x machine. Our training times are 7-10x faster, and our memory\nfootprints are 2-4x smaller than the best baselines. This training time is also\nsignificantly lower than the one reported by Google's mixture of experts (MoE)\nlanguage model on a comparable model size and hardware.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:41:28 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Medini", "Tharun", ""], ["Huang", "Qixuan", ""], ["Wang", "Yiqiu", ""], ["Mohan", "Vijai", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.13832", "submitter": "Juri Kuronen", "authors": "Juri Kuronen, Jukka Corander and Johan Pensar", "title": "Learning pairwise Markov network structures using correlation\n  neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are widely studied and used throughout multivariate\nstatistics and computer science. In particular, the problem of learning the\nstructure of Markov networks from data without invoking chordality assumptions\nin order to retain expressiveness of the model class has been given a\nconsiderable attention in the recent literature, where numerous\nconstraint-based or score-based methods have been introduced. Here we develop a\nnew search algorithm for the network score-optimization that has several\ncomputational advantages and scales well to high-dimensional data sets. The key\nobservation behind the algorithm is that the neighborhood of a variable can be\nefficiently captured using local penalized likelihood ratio (PLR) tests by\nexploiting an exponential decay of correlations across the neighborhood with an\nincreasing graph-theoretic distance from the focus node. The candidate\nneighborhoods are then processed by a two-stage hill-climbing (HC) algorithm.\nOur approach, termed fully as PLRHC-BIC$_{0.5}$, compares favorably against the\nstate-of-the-art methods in all our experiments spanning both low- and\nhigh-dimensional networks and a wide range of sample sizes. An efficient\nimplementation of PLRHC-BIC$_{0.5}$ is freely available from the URL:\nhttps://github.com/jurikuronen/plrhc.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:11:17 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kuronen", "Juri", ""], ["Corander", "Jukka", ""], ["Pensar", "Johan", ""]]}, {"id": "1910.13850", "submitter": "Fernando Garc\\'ia-Redondo", "authors": "Fernando Garc\\'ia-Redondo, Shidhartha Das, Glen Rosendale", "title": "Training DNN IoT Applications for Deployment On Analog NVM Crossbars", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206822", "report-no": null, "categories": "cs.LG cs.ET stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trend towards energy-efficiency, security and privacy has led to a recent\nfocus on deploying DNNs on microcontrollers. However, limits on compute and\nmemory resources restrict the size and the complexity of the ML models\ndeployable in these systems. Computation-In-Memory architectures based on\nresistive nonvolatile memory (NVM) technologies hold great promise of\nsatisfying the compute and memory demands of high-performance and low-power,\ninherent in modern DNNs. Nevertheless, these technologies are still immature\nand suffer from both the intrinsic analog-domain noise problems and the\ninability of representing negative weights in the NVM structures, incurring in\nlarger crossbar sizes with concomitant impact on ADCs and DACs. In this paper,\nwe provide a training framework for addressing these challenges and\nquantitatively evaluate the circuit-level efficiency gains thus accrued. We\nmake two contributions: Firstly, we propose a training algorithm that\neliminates the need for tuning individual layers of a DNN ensuring uniformity\nacross layer weights and activations. This ensures analog-blocks that can be\nreused and peripheral hardware substantially reduced. Secondly, using NAS\nmethods, we propose the use of unipolar-weighted (either all-positive or\nall-negative weights) matrices/sub-matrices. Weight unipolarity obviates the\nneed for doubling crossbar area leading to simplified analog periphery. We\nvalidate our methodology with CIFAR10 and HAR applications by mapping to\ncrossbars using 4-bit and 2-bit devices. We achieve up to 92:91% accuracy (95%\nfloating-point) using 2-bit only-positive weights for HAR. A combination of the\nproposed techniques leads to 80% area improvement and up to 45% energy\nreduction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:49:53 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 16:59:48 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 09:43:17 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Garc\u00eda-Redondo", "Fernando", ""], ["Das", "Shidhartha", ""], ["Rosendale", "Glen", ""]]}, {"id": "1910.13852", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Ali H. Sayed", "title": "Linear Speedup in Saddle-Point Escape for Decentralized Non-Convex\n  Optimization", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under appropriate cooperation protocols and parameter choices, fully\ndecentralized solutions for stochastic optimization have been shown to match\nthe performance of centralized solutions and result in linear speedup (in the\nnumber of agents) relative to non-cooperative approaches in the strongly-convex\nsetting. More recently, these results have been extended to the pursuit of\nfirst-order stationary points in non-convex environments. In this work, we\nexamine in detail the dependence of second-order convergence guarantees on the\nspectral properties of the combination policy for non-convex multi agent\noptimization. We establish linear speedup in saddle-point escape time in the\nnumber of agents for symmetric combination policies and study the potential for\nfurther improvement by employing asymmetric combination weights. The results\nimply that a linear speedup can be expected in the pursuit of second-order\nstationary points, which exclude local maxima as well as strict saddle-points\nand correspond to local or even global minima in many important learning\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:51:47 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1910.13886", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega", "title": "Risk bounds for reservoir computing", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the practices of reservoir computing in the framework of\nstatistical learning theory. In particular, we derive finite sample upper\nbounds for the generalization error committed by specific families of reservoir\ncomputing systems when processing discrete-time inputs under various hypotheses\non their dependence structure. Non-asymptotic bounds are explicitly written\ndown in terms of the multivariate Rademacher complexities of the reservoir\nsystems and the weak dependence structure of the signals that are being\nhandled. This allows, in particular, to determine the minimal number of\nobservations needed in order to guarantee a prescribed estimation accuracy with\nhigh probability for a given reservoir family. At the same time, the asymptotic\nbehavior of the devised bounds guarantees the consistency of the empirical risk\nminimization procedure for various hypothesis classes of reservoir functionals.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:28:46 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gonon", "Lukas", ""], ["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1910.13895", "submitter": "Gail Weiss", "authors": "Gail Weiss, Yoav Goldberg, and Eran Yahav", "title": "Learning Deterministic Weighted Automata with Queries and\n  Counterexamples", "comments": "Presented in NeurIPS 2019. Update: fix email address, add reference\n  to github repo (available at https://github.com/tech-srl/weighted_lstar )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for extraction of a probabilistic deterministic\nfinite automaton (PDFA) from a given black-box language model, such as a\nrecurrent neural network (RNN). The algorithm is a variant of the\nexact-learning algorithm L*, adapted to a probabilistic setting with noise. The\nkey insight is the use of conditional probabilities for observations, and the\nintroduction of a local tolerance when comparing them. When applied to RNNs,\nour algorithm often achieves better word error rate (WER) and normalised\ndistributed cumulative gain (NDCG) than that achieved by spectral extraction of\nweighted finite automata (WFA) from the same networks. PDFAs are substantially\nmore expressive than n-grams, and are guaranteed to be stochastic and\ndeterministic - unlike spectrally extracted WFAs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:37:33 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 10:29:51 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Weiss", "Gail", ""], ["Goldberg", "Yoav", ""], ["Yahav", "Eran", ""]]}, {"id": "1910.13930", "submitter": "Xilei Zhao", "authors": "Xilei Zhao, Zhengze Zhou, Xiang Yan, Pascal Van Hentenryck", "title": "Distilling Black-Box Travel Mode Choice Model for Behavioral\n  Interpretation", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has proved to be very successful for making predictions in\ntravel behavior modeling. However, most machine-learning models have complex\nmodel structures and offer little or no explanation as to how they arrive at\nthese predictions. Interpretations about travel behavior models are essential\nfor decision makers to understand travelers' preferences and plan policy\ninterventions accordingly. Therefore, this paper proposes to apply and extend\nthe model distillation approach, a model-agnostic machine-learning\ninterpretation method, to explain how a black-box travel mode choice model\nmakes predictions for the entire population and subpopulations of interest.\nModel distillation aims at compressing knowledge from a complex model (teacher)\ninto an understandable and interpretable model (student). In particular, the\npaper integrates model distillation with market segmentation to generate more\ninsights by accounting for heterogeneity. Furthermore, the paper provides a\ncomprehensive comparison of student models with the benchmark model (decision\ntree) and the teacher model (gradient boosting trees) to quantify the fidelity\nand accuracy of the students' interpretations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:29:58 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Zhao", "Xilei", ""], ["Zhou", "Zhengze", ""], ["Yan", "Xiang", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1910.13938", "submitter": "Qiuling Yang", "authors": "Qiuling Yang, Alireza Sadeghi, Gang Wang, Georgios B. Giannakis, Jian\n  Sun", "title": "A Statistical Learning Approach to Reactive Power Control in\n  Distribution Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pronounced variability due to the growth of renewable energy sources,\nflexible loads, and distributed generation is challenging residential\ndistribution systems. This context, motivates well fast, efficient, and robust\nreactive power control. Real-time optimal reactive power control is possible in\ntheory by solving a non-convex optimization problem based on the exact model of\ndistribution flow. However, lack of high-precision instrumentation and reliable\ncommunications, as well as the heavy computational burden of non-convex\noptimization solvers render computing and implementing the optimal control\nchallenging in practice. Taking a statistical learning viewpoint, the\ninput-output relationship between each grid state and the corresponding optimal\nreactive power control is parameterized in the present work by a deep neural\nnetwork, whose unknown weights are learned offline by minimizing the power loss\nover a number of historical and simulated training pairs. In the inference\nphase, one just feeds the real-time state vector into the learned neural\nnetwork to obtain the `optimal' reactive power control with only several\nmatrix-vector multiplications. The merits of this novel statistical learning\napproach are computational efficiency as well as robustness to random input\nperturbations. Numerical tests on a 47-bus distribution network using real data\ncorroborate these practical merits.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:38:52 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yang", "Qiuling", ""], ["Sadeghi", "Alireza", ""], ["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""], ["Sun", "Jian", ""]]}, {"id": "1910.13951", "submitter": "Pedro Mercado", "authors": "Pedro Mercado, Francesco Tudisco, Matthias Hein", "title": "Generalized Matrix Means for Semi-Supervised Learning with Multilayer\n  Graphs", "comments": "Accepted in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of semi-supervised learning on multilayer graphs by taking\ninto account both labeled and unlabeled observations together with the\ninformation encoded by each individual graph layer. We propose a regularizer\nbased on the generalized matrix mean, which is a one-parameter family of matrix\nmeans that includes the arithmetic, geometric and harmonic means as particular\ncases. We analyze it in expectation under a Multilayer Stochastic Block Model\nand verify numerically that it outperforms state of the art methods. Moreover,\nwe introduce a matrix-free numerical scheme based on contour integral\nquadratures and Krylov subspace solvers that scales to large sparse multilayer\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:06:35 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Mercado", "Pedro", ""], ["Tudisco", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1910.13962", "submitter": "Igor Gitman", "authors": "Igor Gitman, Hunter Lang, Pengchuan Zhang, Lin Xiao", "title": "Understanding the Role of Momentum in Stochastic Gradient Methods", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of momentum in stochastic gradient methods has become a widespread\npractice in machine learning. Different variants of momentum, including\nheavy-ball momentum, Nesterov's accelerated gradient (NAG), and\nquasi-hyperbolic momentum (QHM), have demonstrated success on various tasks.\nDespite these empirical successes, there is a lack of clear understanding of\nhow the momentum parameters affect convergence and various performance measures\nof different algorithms. In this paper, we use the general formulation of QHM\nto give a unified analysis of several popular algorithms, covering their\nasymptotic convergence conditions, stability regions, and properties of their\nstationary distributions. In addition, by combining the results on convergence\nrates and stationary distributions, we obtain sometimes counter-intuitive\npractical guidelines for setting the learning rate and momentum parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:27:34 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gitman", "Igor", ""], ["Lang", "Hunter", ""], ["Zhang", "Pengchuan", ""], ["Xiao", "Lin", ""]]}, {"id": "1910.13969", "submitter": "Vittorio Tiozzo", "authors": "Giuseppe Carlo Calafiore, Marisa Hillary Morales, Vittorio Tiozzo,\n  Serge Marquie", "title": "A Classifiers Voting Model for Exit Prediction of Privately Held\n  Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the exit (e.g. bankrupt, acquisition, etc.) of privately held\ncompanies is a current and relevant problem for investment firms. The\ndifficulty of the problem stems from the lack of reliable, quantitative and\npublicly available data. In this paper, we contribute to this endeavour by\nconstructing an exit predictor model based on qualitative data, which blends\nthe outcomes of three classifiers, namely, a Logistic Regression model, a\nRandom Forest model, and a Support Vector Machine model. The output of the\ncombined model is selected on the basis of the majority of the output classes\nof the component models. The models are trained using data extracted from the\nThomson Reuters Eikon repository of 54697 US and European companies over the\n1996-2011 time span. Experiments have been conducted for predicting whether the\ncompany eventually either gets acquired or goes public (IPO), against the\ncomplementary event that it remains private or goes bankrupt, in the considered\ntime window. Our model achieves a 63\\% predictive accuracy, which is quite a\nvaluable figure for Private Equity investors, who typically expect very high\nreturns from successful investments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:38:08 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Calafiore", "Giuseppe Carlo", ""], ["Morales", "Marisa Hillary", ""], ["Tiozzo", "Vittorio", ""], ["Marquie", "Serge", ""]]}, {"id": "1910.13983", "submitter": "Michiel Bakker", "authors": "Michiel A. Bakker, Duy Patrick Tu, Humberto River\\'on Vald\\'es,\n  Krishna P. Gummadi, Kush R. Varshney, Adrian Weller, Alex Pentland", "title": "DADI: Dynamic Discovery of Fair Information with Adversarial\n  Reinforcement Learning", "comments": "Accepted at NeurIPS 2019 HCML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for dynamic adversarial discovery of information\n(DADI), motivated by a scenario where information (a feature set) is used by\nthird parties with unknown objectives. We train a reinforcement learning agent\nto sequentially acquire a subset of the information while balancing accuracy\nand fairness of predictors downstream. Based on the set of already acquired\nfeatures, the agent decides dynamically to either collect more information from\nthe set of available features or to stop and predict using the information that\nis currently available. Building on previous work exploring adversarial\nrepresentation learning, we attain group fairness (demographic parity) by\nrewarding the agent with the adversary's loss, computed over the final feature\nset. Importantly, however, the framework provides a more general starting point\nfor fair or private dynamic information discovery. Finally, we demonstrate\nempirically, using two real-world datasets, that we can trade-off fairness and\npredictive performance\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:54:22 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Bakker", "Michiel A.", ""], ["Tu", "Duy Patrick", ""], ["Vald\u00e9s", "Humberto River\u00f3n", ""], ["Gummadi", "Krishna P.", ""], ["Varshney", "Kush R.", ""], ["Weller", "Adrian", ""], ["Pentland", "Alex", ""]]}, {"id": "1910.13984", "submitter": "Ali Vakilian", "authors": "Piotr Indyk, Ali Vakilian, Yang Yuan", "title": "Learning-Based Low-Rank Approximations", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a \"learning-based\" algorithm for the low-rank decomposition\nproblem: given an $n \\times d$ matrix $A$, and a parameter $k$, compute a\nrank-$k$ matrix $A'$ that minimizes the approximation loss $\\|A-A'\\|_F$. The\nalgorithm uses a training set of input matrices in order to optimize its\nperformance. Specifically, some of the most efficient approximate algorithms\nfor computing low-rank approximations proceed by computing a projection $SA$,\nwhere $S$ is a sparse random $m \\times n$ \"sketching matrix\", and then\nperforming the singular value decomposition of $SA$. We show how to replace the\nrandom matrix $S$ with a \"learned\" matrix of the same sparsity to reduce the\nerror.\n  Our experiments show that, for multiple types of data sets, a learned sketch\nmatrix can substantially reduce the approximation loss compared to a random\nmatrix $S$, sometimes by one order of magnitude. We also study mixed matrices\nwhere only some of the rows are trained and the remaining ones are random, and\nshow that matrices still offer improved performance while retaining worst-case\nguarantees.\n  Finally, to understand the theoretical aspects of our approach, we study the\nspecial case of $m=1$. In particular, we give an approximation algorithm for\nminimizing the empirical loss, with approximation factor depending on the\nstable rank of matrices in the training set. We also show generalization bounds\nfor the sketch matrix learning problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:54:50 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Indyk", "Piotr", ""], ["Vakilian", "Ali", ""], ["Yuan", "Yang", ""]]}, {"id": "1910.13993", "submitter": "Litu Rout", "authors": "Litu Rout", "title": "Is Supervised Learning With Adversarial Features Provably Better Than\n  Sole Supervision?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have shown promising results on a wide\nvariety of complex tasks. Recent experiments show adversarial training provides\nuseful gradients to the generator that helps attain better performance. In this\npaper, we intend to theoretically analyze whether supervised learning with\nadversarial features can outperform sole supervision, or not. First, we show\nthat supervised learning without adversarial features suffer from vanishing\ngradient issue in near optimal region. Second, we analyze how adversarial\nlearning augmented with supervised signal mitigates this vanishing gradient\nissue. Finally, we prove our main result that shows supervised learning with\nadversarial features can be better than sole supervision (under some mild\nassumptions). We support our main result on two fronts (i) expected empirical\nrisk and (ii) rate of convergence.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:20:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:42:18 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rout", "Litu", ""]]}, {"id": "1910.14025", "submitter": "Chen Li", "authors": "Linmei Hu, Chen Li, Chuan Shi, Cheng Yang, Chao Shao", "title": "Graph Neural News Recommendation with Long-term and Short-term Interest\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the information explosion of news articles, personalized news\nrecommendation has become important for users to quickly find news that they\nare interested in. Existing methods on news recommendation mainly include\ncollaborative filtering methods which rely on direct user-item interactions and\ncontent based methods which characterize the content of user reading history.\nAlthough these methods have achieved good performances, they still suffer from\ndata sparse problem, since most of them fail to extensively exploit high-order\nstructure information (similar users tend to read similar news articles) in\nnews recommendation systems. In this paper, we propose to build a heterogeneous\ngraph to explicitly model the interactions among users, news and latent topics.\nThe incorporated topic information would help indicate a user's interest and\nalleviate the sparsity of user-item interactions. Then we take advantage of\ngraph neural networks to learn user and news representations that encode\nhigh-order structure information by propagating embeddings over the graph. The\nlearned user embeddings with complete historic user clicks capture the users'\nlong-term interests. We also consider a user's short-term interest using the\nrecent reading history with an attention based LSTM model. Experimental results\non real-world datasets show that our proposed model significantly outperforms\nstate-of-the-art methods on news recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:04:43 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:20:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hu", "Linmei", ""], ["Li", "Chen", ""], ["Shi", "Chuan", ""], ["Yang", "Cheng", ""], ["Shao", "Chao", ""]]}, {"id": "1910.14026", "submitter": "Federico Orsini", "authors": "Federico Orsini, Massimiliano Gastaldi, Luca Mantecchini, Riccardo\n  Rossi", "title": "Neural networks trained with WiFi traces to predict airport passenger\n  behavior", "comments": "Post-print of paper presented at the 2019 6th International\n  Conference on Models and Technologies for Intelligent Transportation Systems\n  (MT-ITS)", "journal-ref": "2019 6th International Conference on Models and Technologies for\n  Intelligent Transportation Systems (MT-ITS)", "doi": "10.1109/MTITS.2019.8883365", "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of neural networks to predict airport passenger activity choices\ninside the terminal is presented in this paper. Three network architectures are\nproposed: Feedforward Neural Networks (FNN), Long Short-Term Memory (LSTM)\nnetworks, and a combination of the two. Inputs to these models are both static\n(passenger and trip characteristics) and dynamic (real-time passenger\ntracking). A real-world case study exemplifies the application of these models,\nusing anonymous WiFi traces collected at Bologna Airport to train the networks.\nThe performance of the models were evaluated according to the misclassification\nrate of passenger activity choices. In the LSTM approach, two different\nmulti-step forecasting strategies are tested. According to our findings, the\ndirect LSTM approach provides better results than the FNN, especially when the\nprediction horizon is relatively short (20 minutes or less).\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:11:38 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Orsini", "Federico", ""], ["Gastaldi", "Massimiliano", ""], ["Mantecchini", "Luca", ""], ["Rossi", "Riccardo", ""]]}, {"id": "1910.14033", "submitter": "Coline Devin", "authors": "Coline Devin, Daniel Geng, Pieter Abbeel, Trevor Darrell, Sergey\n  Levine", "title": "Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control", "comments": "In NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents situated in real-world environments must be able to master\nlarge repertoires of skills. While a single short skill can be learned quickly,\nit would be impractical to learn every task independently. Instead, the agent\nshould share knowledge across behaviors such that each task can be learned\nefficiently, and such that the resulting model can generalize to new tasks,\nespecially ones that are compositions or subsets of tasks seen previously. A\npolicy conditioned on a goal or demonstration has the potential to share\nknowledge between tasks if it sees enough diversity of inputs. However, these\nmethods may not generalize to a more complex task at test time. We introduce\ncompositional plan vectors (CPVs) to enable a policy to perform compositions of\ntasks without additional supervision. CPVs represent trajectories as the sum of\nthe subtasks within them. We show that CPVs can be learned within a one-shot\nimitation learning framework without any additional supervision or information\nabout task hierarchy, and enable a demonstration-conditioned policy to\ngeneralize to tasks that sequence twice as many skills as the tasks seen during\ntraining.\n  Analogously to embeddings such as word2vec in NLP, CPVs can also support\nsimple arithmetic operations -- for example, we can add the CPVs for two\ndifferent tasks to command an agent to compose both tasks, without any\nadditional training.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:50:42 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 01:00:16 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Devin", "Coline", ""], ["Geng", "Daniel", ""], ["Abbeel", "Pieter", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.14055", "submitter": "Hao Sun", "authors": "Hao Sun, Zhizhong Li, Xiaotong Liu, Dahua Lin, Bolei Zhou", "title": "Policy Continuation with Hindsight Inverse Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Solving goal-oriented tasks is an important but challenging problem in\nreinforcement learning (RL). For such tasks, the rewards are often sparse,\nmaking it difficult to learn a policy effectively. To tackle this difficulty,\nwe propose a new approach called Policy Continuation with Hindsight Inverse\nDynamics (PCHID). This approach learns from Hindsight Inverse Dynamics based on\nHindsight Experience Replay, enabling the learning process in a self-imitated\nmanner and thus can be trained with supervised learning. This work also extends\nit to multi-step settings with Policy Continuation. The proposed method is\ngeneral, which can work in isolation or be combined with other on-policy and\noff-policy algorithms. On two multi-goal tasks GridWorld and FetchReach, PCHID\nsignificantly improves the sample efficiency as well as the final performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:00:21 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 04:18:43 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Sun", "Hao", ""], ["Li", "Zhizhong", ""], ["Liu", "Xiaotong", ""], ["Lin", "Dahua", ""], ["Zhou", "Bolei", ""]]}, {"id": "1910.14056", "submitter": "Hao Sun", "authors": "Hao Sun, Jiadong Guo, Edward J. Kim, Robert J. Brunner", "title": "Unsupervised Star Galaxy Classification with Cascade Variational\n  Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of data in astronomy provides great challenges for\nmachine learning research. Previously, supervised learning methods achieved\nsatisfactory recognition accuracy for the star-galaxy classification task,\nbased on manually labeled data set. In this work, we propose a novel\nunsupervised approach for the star-galaxy recognition task, namely Cascade\nVariational Auto-Encoder (CasVAE). Our empirical results show our method\noutperforms the baseline model in both accuracy and stability.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:00:23 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Sun", "Hao", ""], ["Guo", "Jiadong", ""], ["Kim", "Edward J.", ""], ["Brunner", "Robert J.", ""]]}, {"id": "1910.14072", "submitter": "Miguel Ib\\'a\\~nez Berganza", "authors": "Miguel Ib\\'a\\~nez-Berganza, Ambra Amico, Gian Luca Lancia, Federico\n  Maggiore, Bernardo Monechi, Vittorio Loreto", "title": "Unsupervised inference approach to facial attractiveness", "comments": "main article (10 pages, 4 figures) + supplementary information (22\n  pages, 10 figures). minor typos corrected. Federico Maggiore added as author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perception of facial beauty is a complex phenomenon depending on many,\ndetailed and global facial features influencing each other. In the machine\nlearning community this problem is typically tackled as a problem of supervised\ninference. However, it has been conjectured that this approach does not capture\nthe complexity of the phenomenon. A recent original experiment\n(Ib\\'a\\~nez-Berganza et al., Scientific Reports 9, 8364, 2019) allowed\ndifferent human subjects to navigate the face-space and ``sculpt'' their\npreferred modification of a reference facial portrait. Here we present an\nunsupervised inference study of the set of sculpted facial vectors in that\nexperiment. We first infer minimal, interpretable, and faithful probabilistic\nmodels (through Maximum Entropy and artificial neural networks) of the\npreferred facial variations, that capture the origin of the observed\ninter-subject diversity in the sculpted faces. The application of such\ngenerative models to the supervised classification of the gender of the\nsculpting subjects, reveals an astonishingly high prediction accuracy. This\nresult suggests that much relevant information regarding the subjects may\ninfluence (and be elicited from) her/his facial preference criteria, in\nagreement with the multiple motive theory of attractiveness proposed in\nprevious works.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:23:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 18:43:45 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 13:58:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ib\u00e1\u00f1ez-Berganza", "Miguel", ""], ["Amico", "Ambra", ""], ["Lancia", "Gian Luca", ""], ["Maggiore", "Federico", ""], ["Monechi", "Bernardo", ""], ["Loreto", "Vittorio", ""]]}, {"id": "1910.14095", "submitter": "Justin Lovelace", "authors": "Justin R. Lovelace, Nathan C. Hurley, Adrian D. Haimovich, and Bobak\n  J. Mortazavi", "title": "Explainable Prediction of Adverse Outcomes Using Clinical Notes", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical notes contain a large amount of clinically valuable information that\nis ignored in many clinical decision support systems due to the difficulty that\ncomes with mining that information. Recent work has found success leveraging\ndeep learning models for the prediction of clinical outcomes using clinical\nnotes. However, these models fail to provide clinically relevant and\ninterpretable information that clinicians can utilize for informed clinical\ncare. In this work, we augment a popular convolutional model with an attention\nmechanism and apply it to unstructured clinical notes for the prediction of ICU\nreadmission and mortality. We find that the addition of the attention mechanism\nleads to competitive performance while allowing for the straightforward\ninterpretation of predictions. We develop clear visualizations to present\nimportant spans of text for both individual predictions and high-risk cohorts.\nWe then conduct a qualitative analysis and demonstrate that our model is\nconsistently attending to clinically meaningful portions of the narrative for\nall of the outcomes that we explore.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:30:14 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 16:14:39 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Lovelace", "Justin R.", ""], ["Hurley", "Nathan C.", ""], ["Haimovich", "Adrian D.", ""], ["Mortazavi", "Bobak J.", ""]]}, {"id": "1910.14096", "submitter": "Usama Muneeb", "authors": "Usama Muneeb, Erdem Koyuncu, Yasaman Keshtkarjahromi, Hulya Seferoglu,\n  Mehmet Fatih Erden, Ahmet Enis Cetin", "title": "Robust and Computationally-Efficient Anomaly Detection using\n  Powers-of-Two Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and computationally efficient anomaly detection in videos is a problem\nin video surveillance systems. We propose a technique to increase robustness\nand reduce computational complexity in a Convolutional Neural Network (CNN)\nbased anomaly detector that utilizes the optical flow information of video\ndata. We reduce the complexity of the network by denoising the intermediate\nlayer outputs of the CNN and by using powers-of-two weights, which replaces the\ncomputationally expensive multiplication operations with bit-shift operations.\nDenoising operation during inference forces small valued intermediate layer\noutputs to zero. The number of zeros in the network significantly increases as\na result of denoising, we can implement the CNN about 10% faster than a\ncomparable network while detecting all the anomalies in the testing set. It\nturns out that denoising operation also provides robustness because the\ncontribution of small intermediate values to the final result is negligible.\nDuring training we also generate motion vector images by a Generative\nAdversarial Network (GAN) to improve the robustness of the overall system. We\nexperimentally observe that the resulting system is robust to background\nmotion.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:30:24 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Muneeb", "Usama", ""], ["Koyuncu", "Erdem", ""], ["Keshtkarjahromi", "Yasaman", ""], ["Seferoglu", "Hulya", ""], ["Erden", "Mehmet Fatih", ""], ["Cetin", "Ahmet Enis", ""]]}, {"id": "1910.14098", "submitter": "Sahar Hojjatinia", "authors": "Sahar Hojjatinia, Constantino M. Lagoa", "title": "Comparison of Different Spike Sorting Subtechniques Based on Rat Brain\n  Basolateral Amygdala Neuronal Activity", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing electrophysiological recordings of brain neuronal activity and\ntheir analysis provide a basis for exploring the structure of brain function\nand nervous system investigation. The recorded signals are typically a\ncombination of spikes and noise. High amounts of background noise and\npossibility of electric signaling recording from several neurons adjacent to\nthe recording site have led scientists to develop neuronal signal processing\ntools such as spike sorting to facilitate brain data analysis. Spike sorting\nplays a pivotal role in understanding the electrophysiological activity of\nneuronal networks. This process prepares recorded data for interpretations of\nneurons interactions and understanding the overall structure of brain\nfunctions. Spike sorting consists of three steps: spike detection, feature\nextraction, and spike clustering. There are several methods to implement each\nof spike sorting steps. This paper provides a systematic comparison of various\nspike sorting sub-techniques applied to real extracellularly recorded data from\na rat brain basolateral amygdala. An efficient sorted data resulted from\ncareful choice of spike sorting sub-methods leads to better interpretation of\nthe brain structures connectivity under different conditions, which is a very\nsensitive concept in diagnosis and treatment of neurological disorders. Here,\nspike detection is performed by appropriate choice of threshold level via three\ndifferent approaches. Feature extraction is done through PCA and Kernel PCA\nmethods, which Kernel PCA outperforms. We have applied four different\nalgorithms for spike clustering including K-means, Fuzzy C-means, Bayesian and\nFuzzy maximum likelihood estimation. As one requirement of most clustering\nalgorithms, optimal number of clusters is achieved through validity indices for\neach method. Finally, the sorting results are evaluated using inter-spike\ninterval histograms.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 00:44:24 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Hojjatinia", "Sahar", ""], ["Lagoa", "Constantino M.", ""]]}, {"id": "1910.14106", "submitter": "Soumyabrata Pal", "authors": "Akshay Krishnamurthy, Arya Mazumdar, Andrew McGregor, Soumyabrata Pal", "title": "Sample Complexity of Learning Mixtures of Sparse Linear Regressions", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of learning mixtures of linear regressions, the goal is to\nlearn a collection of signal vectors from a sequence of (possibly noisy) linear\nmeasurements, where each measurement is evaluated on an unknown signal drawn\nuniformly from this collection. This setting is quite expressive and has been\nstudied both in terms of practical applications and for the sake of\nestablishing theoretical guarantees. In this paper, we consider the case where\nthe signal vectors are sparse; this generalizes the popular compressed sensing\nparadigm. We improve upon the state-of-the-art results as follows: In the noisy\ncase, we resolve an open question of Yin et al. (IEEE Transactions on\nInformation Theory, 2019) by showing how to handle collections of more than two\nvectors and present the first robust reconstruction algorithm, i.e., if the\nsignals are not perfectly sparse, we still learn a good sparse approximation of\nthe signals. In the noiseless case, as well as in the noisy case, we show how\nto circumvent the need for a restrictive assumption required in the previous\nwork. Our techniques are quite different from those in the previous work: for\nthe noiseless case, we rely on a property of sparse polynomials and for the\nnoisy case, we provide new connections to learning Gaussian mixtures and use\nideas from the theory of error-correcting codes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:53:49 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Mazumdar", "Arya", ""], ["McGregor", "Andrew", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "1910.14107", "submitter": "Rana Abou Khamis", "authors": "Rana Abou Khamis, Omair Shafiq, Ashraf Matrawy", "title": "Investigating Resistance of Deep Learning-based IDS against Adversaries\n  using min-max Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of adversarial attacks against machine learning models,\nseveral concerns have emerged about potential vulnerabilities in designing deep\nneural network-based intrusion detection systems (IDS). In this paper, we study\nthe resilience of deep learning-based intrusion detection systems against\nadversarial attacks. We apply the min-max (or saddle-point) approach to train\nintrusion detection systems against adversarial attack samples in NSW-NB 15\ndataset. We have the max approach for generating adversarial samples that\nachieves maximum loss and attack deep neural networks. On the other side, we\nutilize the existing min approach [2] [9] as a defense strategy to optimize\nintrusion detection systems that minimize the loss of the incorporated\nadversarial samples during the adversarial training. We study and measure the\neffectiveness of the adversarial attack methods as well as the resistance of\nthe adversarially trained models against such attacks. We find that the\nadversarial attack methods that were designed in binary domains can be used in\ncontinuous domains and exhibit different misclassification levels. We finally\nshow that principal component analysis (PCA) based feature reduction can boost\nthe robustness in intrusion detection system (IDS) using a deep neural network\n(DNN).\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:55:26 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Khamis", "Rana Abou", ""], ["Shafiq", "Omair", ""], ["Matrawy", "Ashraf", ""]]}, {"id": "1910.14120", "submitter": "Ananth Balashankar", "authors": "Ananth Balashankar, Alyssa Lees, Chris Welty, Lakshminarayanan\n  Subramanian", "title": "What is Fair? Exploring Pareto-Efficiency for Fairness Constrained\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential for learned models to amplify existing societal biases has been\nbroadly recognized. Fairness-aware classifier constraints, which apply equality\nmetrics of performance across subgroups defined on sensitive attributes such as\nrace and gender, seek to rectify inequity but can yield non-uniform degradation\nin performance for skewed datasets. In certain domains, imbalanced degradation\nof performance can yield another form of unintentional bias. In the spirit of\nconstructing fairness-aware algorithms as societal imperative, we explore an\nalternative: Pareto-Efficient Fairness (PEF). Theoretically, we prove that PEF\nidentifies the operating point on the Pareto curve of subgroup performances\nclosest to the fairness hyperplane, maximizing multiple subgroup accuracy.\nEmpirically we demonstrate that PEF outperforms by achieving Pareto levels in\naccuracy for all subgroups compared to strict fairness constraints in several\nUCI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 20:32:01 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Balashankar", "Ananth", ""], ["Lees", "Alyssa", ""], ["Welty", "Chris", ""], ["Subramanian", "Lakshminarayanan", ""]]}, {"id": "1910.14134", "submitter": "Yibo Jiang", "authors": "Yibo Jiang, Nakul Verma", "title": "Meta-Learning to Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental and wide-spread techniques in\nexploratory data analysis. Yet, the basic approach to clustering has not really\nchanged: a practitioner hand-picks a task-specific clustering loss to optimize\nand fit the given data to reveal the underlying cluster structure. Some types\nof losses---such as k-means, or its non-linear version: kernelized k-means\n(centroid based), and DBSCAN (density based)---are popular choices due to their\ngood empirical performance on a range of applications. Although every so often\nthe clustering output using these standard losses fails to reveal the\nunderlying structure, and the practitioner has to custom-design their own\nvariation. In this work we take an intrinsically different approach to\nclustering: rather than fitting a dataset to a specific clustering loss, we\ntrain a recurrent model that learns how to cluster. The model uses as training\npairs examples of datasets (as input) and its corresponding cluster identities\n(as output). By providing multiple types of training datasets as inputs, our\nmodel has the ability to generalize well on unseen datasets (new clustering\ntasks). Our experiments reveal that by training on simple synthetically\ngenerated datasets or on existing real datasets, we can achieve better\nclustering performance on unseen real-world datasets when compared with\nstandard benchmark clustering techniques. Our meta clustering model works well\neven for small datasets where the usual deep learning models tend to perform\nworse.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:06:15 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Yibo", ""], ["Verma", "Nakul", ""]]}, {"id": "1910.14137", "submitter": "Ben Adlam", "authors": "Ben Adlam, Charles Weill, and Amol Kapoor", "title": "Investigating Under and Overfitting in Wasserstein Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate under and overfitting in Generative Adversarial Networks\n(GANs), using discriminators unseen by the generator to measure generalization.\nWe find that the model capacity of the discriminator has a significant effect\non the generator's model quality, and that the generator's poor performance\ncoincides with the discriminator underfitting. Contrary to our expectations, we\nfind that generators with large model capacities relative to the discriminator\ndo not show evidence of overfitting on CIFAR10, CIFAR100, and CelebA.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:10:36 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Adlam", "Ben", ""], ["Weill", "Charles", ""], ["Kapoor", "Amol", ""]]}, {"id": "1910.14145", "submitter": "Anna Wigren", "authors": "Anna Wigren, Riccardo Sven Risuleo, Lawrence Murray, Fredrik Lindsten", "title": "Parameter elimination in particle Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference in state-space models is challenging due to\nhigh-dimensional state trajectories. A viable approach is particle Markov chain\nMonte Carlo, combining MCMC and sequential Monte Carlo to form \"exact\napproximations\" to otherwise intractable MCMC methods. The performance of the\napproximation is limited to that of the exact method. We focus on particle\nGibbs and particle Gibbs with ancestor sampling, improving their performance\nbeyond that of the underlying Gibbs sampler (which they approximate) by\nmarginalizing out one or more parameters. This is possible when the parameter\nprior is conjugate to the complete data likelihood. Marginalization yields a\nnon-Markovian model for inference, but we show that, in contrast to the general\ncase, this method still scales linearly in time. While marginalization can be\ncumbersome to implement, recent advances in probabilistic programming have\nenabled its automation. We demonstrate how the marginalized methods are viable\nas efficient inference backends in probabilistic programming, and demonstrate\nwith examples in ecology and epidemiology.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:23:05 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Wigren", "Anna", ""], ["Risuleo", "Riccardo Sven", ""], ["Murray", "Lawrence", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1910.14147", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, Cho-Jui Hsieh", "title": "A Unified Framework for Data Poisoning Attack to Graph-based\n  Semi-supervised Learning", "comments": "NeurIPS 2019 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a general framework for data poisoning attacks to\ngraph-based semi-supervised learning (G-SSL). In this framework, we first unify\ndifferent tasks, goals, and constraints into a single formula for data\npoisoning attack in G-SSL, then we propose two specialized algorithms to\nefficiently solve two important cases --- poisoning regression tasks under\n$\\ell_2$-norm constraint and classification tasks under $\\ell_0$-norm\nconstraint. In the former case, we transform it into a non-convex trust region\nproblem and show that our gradient-based algorithm with delicate initialization\nand update scheme finds the (globally) optimal perturbation. For the latter\ncase, although it is an NP-hard integer programming problem, we propose a\nprobabilistic solver that works much better than the classical greedy method.\nLastly, we test our framework on real datasets and evaluate the robustness of\nG-SSL algorithms. For instance, on the MNIST binary classification problem\n(50000 training data with 50 labeled), flipping two labeled data is enough to\nmake the model perform like random guess (around 50\\% error).\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:30:12 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Liu", "Xuanqing", ""], ["Si", "Si", ""], ["Zhu", "Xiaojin", ""], ["Li", "Yang", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1910.14162", "submitter": "Beidi Chen", "authors": "Beidi Chen, Yingchen Xu, Anshumali Shrivastava", "title": "Lsh-sampling Breaks the Computation Chicken-and-egg Loop in Adaptive\n  Stochastic Gradient Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent or SGD is the most popular optimization algorithm\nfor large-scale problems. SGD estimates the gradient by uniform sampling with\nsample size one. There have been several other works that suggest faster\nepoch-wise convergence by using weighted non-uniform sampling for better\ngradient estimates. Unfortunately, the per-iteration cost of maintaining this\nadaptive distribution for gradient estimation is more than calculating the full\ngradient itself, which we call the chicken-and-the-egg loop. As a result, the\nfalse impression of faster convergence in iterations, in reality, leads to\nslower convergence in time. In this paper, we break this barrier by providing\nthe first demonstration of a scheme, Locality sensitive hashing (LSH) sampled\nStochastic Gradient Descent (LGD), which leads to superior gradient estimation\nwhile keeping the sampling cost per iteration similar to that of the uniform\nsampling. Such an algorithm is possible due to the sampling view of LSH, which\ncame to light recently. As a consequence of superior and fast estimation, we\nreduce the running time of all existing gradient descent algorithms, that\nrelies on gradient estimates including Adam, Ada-grad, etc. We demonstrate the\neffectiveness of our proposal with experiments on linear models as well as the\nnon-linear BERT, which is a recent popular deep learning based language\nrepresentation model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:28:54 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Chen", "Beidi", ""], ["Xu", "Yingchen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.14166", "submitter": "Charlie Dickens", "authors": "Graham Cormode, Charlie Dickens", "title": "Iterative Hessian Sketch in Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable algorithms to solve optimization and regression tasks even\napproximately, are needed to work with large datasets. In this paper we study\nefficient techniques from matrix sketching to solve a variety of convex\nconstrained regression problems. We adopt \"Iterative Hessian Sketching\" (IHS)\nand show that the fast CountSketch and sparse Johnson-Lindenstrauss Transforms\nyield state-of-the-art accuracy guarantees under IHS, while drastically\nimproving the time cost. As a result, we obtain significantly faster algorithms\nfor constrained regression, for both sparse and dense inputs. Our empirical\nresults show that we can summarize data roughly 100x faster for sparse data,\nand, surprisingly, 10x faster on dense data! Consequently, solutions accurate\nto within machine precision of the optimal solution can be found much faster\nthan the previous state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:40:07 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Cormode", "Graham", ""], ["Dickens", "Charlie", ""]]}, {"id": "1910.14175", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Bindya Venkatesh and Deepta Rajan", "title": "Learn-By-Calibrating: Using Calibration as a Training Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration error is commonly adopted for evaluating the quality of\nuncertainty estimators in deep neural networks. In this paper, we argue that\nsuch a metric is highly beneficial for training predictive models, even when we\ndo not explicitly measure the uncertainties. This is conceptually similar to\nheteroscedastic neural networks that produce variance estimates for each\nprediction, with the key difference that we do not place a Gaussian prior on\nthe predictions. We propose a novel algorithm that performs simultaneous\ninterval estimation for different calibration levels and effectively leverages\nthe intervals to refine the mean estimates. Our results show that, our approach\nis consistently superior to existing regularization strategies in deep\nregression models. Finally, we propose to augment partial dependence plots, a\nmodel-agnostic interpretability tool, with expected prediction intervals to\nreveal interesting dependencies between data and the target.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 23:13:40 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Venkatesh", "Bindya", ""], ["Rajan", "Deepta", ""]]}, {"id": "1910.14179", "submitter": "Jayaraman J. Thiagarajan", "authors": "Bindya Venkatesh and Jayaraman J. Thiagarajan", "title": "Heteroscedastic Calibration of Uncertainty Estimators in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of uncertainty quantification (UQ) in deep learning has become\ncrucial with growing use of predictive models in high-risk applications. Though\na large class of methods exists for measuring deep uncertainties, in practice,\nthe resulting estimates are found to be poorly calibrated, thus making it\nchallenging to translate them into actionable insights. A common workaround is\nto utilize a separate recalibration step, which adjusts the estimates to\ncompensate for the miscalibration. Instead, we propose to repurpose the\nheteroscedastic regression objective as a surrogate for calibration and enable\nany existing uncertainty estimator to be inherently calibrated. In addition to\neliminating the need for recalibration, this also regularizes the training\nprocess. Using regression experiments, we demonstrate the effectiveness of the\nproposed heteroscedastic calibration with two popular uncertainty estimators.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 23:22:39 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Venkatesh", "Bindya", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1910.14186", "submitter": "Ambar Pal", "authors": "Ambar Pal, Connor Lane, Ren\\'e Vidal, Benjamin D. Haeffele", "title": "On the Regularization Properties of Structured Dropout", "comments": "Accepted at Computer Vision and Pattern Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and its extensions (eg. DropBlock and DropConnect) are popular\nheuristics for training neural networks, which have been shown to improve\ngeneralization performance in practice. However, a theoretical understanding of\ntheir optimization and regularization properties remains elusive. Recent work\nshows that in the case of single hidden-layer linear networks, Dropout is a\nstochastic gradient descent method for minimizing a regularized loss, and that\nthe regularizer induces solutions that are low-rank and balanced. In this work\nwe show that for single hidden-layer linear networks, DropBlock induces\nspectral k-support norm regularization, and promotes solutions that are\nlow-rank and have factors with equal norm. We also show that the global\nminimizer for DropBlock can be computed in closed form, and that DropConnect is\nequivalent to Dropout. We then show that some of these results can be extended\nto a general class of Dropout-strategies, and, with some assumptions, to deep\nnon-linear networks when Dropout is applied to the last layer. We verify our\ntheoretical claims and assumptions experimentally with commonly used network\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 23:58:34 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 11:25:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pal", "Ambar", ""], ["Lane", "Connor", ""], ["Vidal", "Ren\u00e9", ""], ["Haeffele", "Benjamin D.", ""]]}, {"id": "1910.14207", "submitter": "Anastasiia Razdaibiedina", "authors": "Anastasia Razdaibiedina, Jeevaa Velayutham, Miti Modi", "title": "Multi-defect microscopy image restoration under limited data conditions", "comments": "NeurIPS 2019 Medical Imaging workhop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are becoming widely used for restoration of defects\nassociated with fluorescence microscopy imaging. One of the major challenges in\napplication of such methods is the availability of training data. In this work,\nwe propose a unified method for reconstruction of multi-defect fluorescence\nmicroscopy images when training data is limited. Our approach consists of two\nstages: first, we perform data augmentation using Generative Adversarial\nNetwork (GAN) with conditional instance normalization (CIN); second, we train a\nconditional GAN (cGAN) on paired ground-truth and defected images to perform\nrestoration. The experiments on three common types of imaging defects with\ndifferent amounts of training data show that the proposed method gives\ncomparable results or outperforms CARE, deblurGAN and CycleGAN in restored\nimage quality when available data is limited.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 01:55:01 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 20:14:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Razdaibiedina", "Anastasia", ""], ["Velayutham", "Jeevaa", ""], ["Modi", "Miti", ""]]}, {"id": "1910.14209", "submitter": "Liyao Lyu", "authors": "Liyao Lyu, Zhiwen Zhang, Jingrun Chen", "title": "A QMC-deep learning method for diffusivity estimation in random domains", "comments": null, "journal-ref": null, "doi": "10.4208/nmtma.OA-2020-0032", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exciton diffusion plays a vital role in the function of many organic\nsemiconducting opto-electronic devices, where an accurate description requires\nprecise control of heterojunctions. This poses a challenging problem because\nthe parameterization of heterojunctions in high-dimensional random space is far\nbeyond the capability of classical simulation tools. Here, we develop a novel\nmethod based on quasi-Monte Carlo sampling to generate the training data set\nand deep neural network to extract a function for exciton diffusion length on\nsurface roughness with high accuracy and unprecedented efficiency, yielding an\nabundance of information over the entire parameter space. Our method provides a\nnew strategy to analyze the impact of interfacial ordering on exciton diffusion\nand is expected to assist experimental design with tailored opto-electronic\nfunctionalities.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:00:25 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 16:51:06 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Lyu", "Liyao", ""], ["Zhang", "Zhiwen", ""], ["Chen", "Jingrun", ""]]}, {"id": "1910.14212", "submitter": "Youssef Mroueh", "authors": "Youssef Mroueh, Tom Sercu, Mattia Rigotti, Inkit Padhi, Cicero Dos\n  Santos", "title": "Sobolev Independence Criterion", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Sobolev Independence Criterion (SIC), an interpretable\ndependency measure between a high dimensional random variable X and a response\nvariable Y . SIC decomposes to the sum of feature importance scores and hence\ncan be used for nonlinear feature selection. SIC can be seen as a gradient\nregularized Integral Probability Metric (IPM) between the joint distribution of\nthe two random variables and the product of their marginals. We use sparsity\ninducing gradient penalties to promote input sparsity of the critic of the IPM.\nIn the kernel version we show that SIC can be cast as a convex optimization\nproblem by introducing auxiliary variables that play an important role in\nfeature selection as they are normalized feature importance scores. We then\npresent a neural version of SIC where the critic is parameterized as a\nhomogeneous neural network, improving its representation power as well as its\ninterpretability. We conduct experiments validating SIC for feature selection\nin synthetic and real-world experiments. We show that SIC enables reliable and\ninterpretable discoveries, when used in conjunction with the holdout\nrandomization test and knockoffs to control the False Discovery Rate. Code is\navailable at http://github.com/ibm/sic.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:17:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mroueh", "Youssef", ""], ["Sercu", "Tom", ""], ["Rigotti", "Mattia", ""], ["Padhi", "Inkit", ""], ["Santos", "Cicero Dos", ""]]}, {"id": "1910.14215", "submitter": "Rebecca Russell", "authors": "Rebecca L. Russell and Christopher Reale", "title": "Multivariate Uncertainty in Deep Learning", "comments": "To be published in IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has the potential to dramatically impact navigation and\ntracking state estimation problems critical to autonomous vehicles and\nrobotics. Measurement uncertainties in state estimation systems based on Kalman\nand other Bayes filters are typically assumed to be a fixed covariance matrix.\nThis assumption is risky, particularly for \"black box\" deep learning models, in\nwhich uncertainty can vary dramatically and unexpectedly. Accurate\nquantification of multivariate uncertainty will allow for the full potential of\ndeep learning to be used more safely and reliably in these applications. We\nshow how to model multivariate uncertainty for regression problems with neural\nnetworks, incorporating both aleatoric and epistemic sources of heteroscedastic\nuncertainty. We train a deep uncertainty covariance matrix model in two ways:\ndirectly using a multivariate Gaussian density loss function, and indirectly\nusing end-to-end training through a Kalman filter. We experimentally show in a\nvisual tracking problem the large impact that accurate multivariate uncertainty\nquantification can have on Kalman filter performance for both in-domain and\nout-of-domain evaluation data. We additionally show in a challenging visual\nodometry problem how end-to-end filter training can allow uncertainty\npredictions to compensate for filter weaknesses.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:25:16 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 22:54:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Russell", "Rebecca L.", ""], ["Reale", "Christopher", ""]]}, {"id": "1910.14238", "submitter": "Jianxin Ma", "authors": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu", "title": "Learning Disentangled Representations for Recommendation", "comments": "To appear in the Proceedings of the Thirty-third Conference on Neural\n  Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User behavior data in recommender systems are driven by the complex\ninteractions of many latent factors behind the users' decision making\nprocesses. The factors are highly entangled, and may range from high-level ones\nthat govern user intentions, to low-level ones that characterize a user's\npreference when executing an intention. Learning representations that uncover\nand disentangle these latent factors can bring enhanced robustness,\ninterpretability, and controllability. However, learning such disentangled\nrepresentations from user behavior is challenging, and remains largely\nneglected by the existing literature. In this paper, we present the MACRo-mIcro\nDisentangled Variational Auto-Encoder (MacridVAE) for learning disentangled\nrepresentations from user behavior. Our approach achieves macro disentanglement\nby inferring the high-level concepts associated with user intentions (e.g., to\nbuy a shirt or a cellphone), while capturing the preference of a user regarding\nthe different concepts separately. A micro-disentanglement regularizer,\nstemming from an information-theoretic interpretation of VAEs, then forces each\ndimension of the representations to independently reflect an isolated low-level\nfactor (e.g., the size or the color of a shirt). Empirical results show that\nour approach can achieve substantial improvement over the state-of-the-art\nbaselines. We further demonstrate that the learned representations are\ninterpretable and controllable, which can potentially lead to a new paradigm\nfor recommendation where users are given fine-grained control over targeted\naspects of the recommendation lists.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:31:56 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ma", "Jianxin", ""], ["Zhou", "Chang", ""], ["Cui", "Peng", ""], ["Yang", "Hongxia", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1910.14241", "submitter": "Avinash Kori", "authors": "Avinash Kori, Manik Sharma", "title": "Dynamic Regularizer with an Informative Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regularization methods, specifically those which directly alter weights like\n$L_1$ and $L_2$, are an integral part of many learning algorithms. Both the\nregularizers mentioned above are formulated by assuming certain priors in the\nparameter space and these assumptions, in some cases, induce sparsity in the\nparameter space. Regularizers help in transferring beliefs one has on the\ndataset or the parameter space by introducing adequate terms in the loss\nfunction. Any kind of formulation represents a specific set of beliefs: $L_1$\nregularization conveys that the parameter space should be sparse whereas $L_2$\nregularization conveys that the parameter space should be bounded and\ncontinuous. These regularizers in turn leverage certain priors to express these\ninherent beliefs. A better understanding of how the prior affects the behavior\nof the parameters and how the priors can be updated based on the dataset can\ncontribute greatly in improving the generalization capabilities of a function\nestimator. In this work, we introduce a weakly informative prior and then\nfurther extend it to an informative prior in order to formulate a\nregularization penalty, which shows better results in terms of inducing\nsparsity experimentally, when compared to regularizers based only on Gaussian\nand Laplacian priors. Experimentally, we verify that a regularizer based on an\nadapted prior improves the generalization capabilities of any network. We\nillustrate the performance of the proposed method on the MNIST and CIFAR-10\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:40:03 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kori", "Avinash", ""], ["Sharma", "Manik", ""]]}, {"id": "1910.14265", "submitter": "George Tucker", "authors": "Dieterich Lawson, George Tucker, Bo Dai, Rajesh Ranganath", "title": "Energy-Inspired Models: Learning with Sampler-Induced Distributions", "comments": "Presented at the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-based models (EBMs) are powerful probabilistic models, but suffer from\nintractable sampling and density evaluation due to the partition function. As a\nresult, inference in EBMs relies on approximate sampling algorithms, leading to\na mismatch between the model and inference. Motivated by this, we consider the\nsampler-induced distribution as the model of interest and maximize the\nlikelihood of this model. This yields a class of energy-inspired models (EIMs)\nthat incorporate learned energy functions while still providing exact samples\nand tractable log-likelihood lower bounds. We describe and evaluate three\ninstantiations of such models based on truncated rejection sampling,\nself-normalized importance sampling, and Hamiltonian importance sampling. These\nmodels outperform or perform comparably to the recently proposed Learned\nAccept/Reject Sampling algorithm and provide new insights on ranking Noise\nContrastive Estimation and Contrastive Predictive Coding. Moreover, EIMs allow\nus to generalize a recent connection between multi-sample variational lower\nbounds and auxiliary variable variational inference. We show how recent\nvariational bounds can be unified with EIMs as the variational family.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 05:31:18 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 18:10:38 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lawson", "Dieterich", ""], ["Tucker", "George", ""], ["Dai", "Bo", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "1910.14268", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Florian Kerschbaum", "title": "RIGA: Covert and Robust White-Box Watermarking of Deep Neural Networks", "comments": "WebConf'21 (Full Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking of deep neural networks (DNN) can enable their tracing once\nreleased by a data owner. In this paper, we generalize white-box watermarking\nalgorithms for DNNs, where the data owner needs white-box access to the model\nto extract the watermark. White-box watermarking algorithms have the advantage\nthat they do not impact the accuracy of the watermarked model. We propose\nRobust whIte-box GAn watermarking (RIGA), a novel white-box watermarking\nalgorithm that uses adversarial training. Our extensive experiments demonstrate\nthat the proposed watermarking algorithm not only does not impact accuracy, but\nalso significantly improves the covertness and robustness over the current\nstate-of-art.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 05:51:45 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 02:33:54 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 23:42:09 GMT"}, {"version": "v4", "created": "Sun, 14 Feb 2021 02:57:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Tianhao", ""], ["Kerschbaum", "Florian", ""]]}, {"id": "1910.14273", "submitter": "Xiaoxue Li", "authors": "Xiaoxue Li, Yanan Cao, Yanmin Shang, Yangxi Li, Yanbing Liu, Jianlong\n  Tan", "title": "RLINK: Deep Reinforcement Learning for User Identity Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User identity linkage is a task of recognizing the identities of the same\nuser across different social networks (SN). Previous works tackle this problem\nvia estimating the pairwise similarity between identities from different SN,\npredicting the label of identity pairs or selecting the most relevant identity\npair based on the similarity scores. However, most of these methods ignore the\nresults of previously matched identities, which could contribute to the linkage\nin following matching steps. To address this problem, we convert user identity\nlinkage into a sequence decision problem and propose a reinforcement learning\nmodel to optimize the linkage strategy from the global perspective. Our method\nmakes full use of both the social network structure and the history matched\nidentities, and explores the long-term influence of current matching on\nsubsequent decisions. We conduct experiments on different types of datasets,\nthe results show that our method achieves better performance than other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 06:21:33 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Li", "Xiaoxue", ""], ["Cao", "Yanan", ""], ["Shang", "Yanmin", ""], ["Li", "Yangxi", ""], ["Liu", "Yanbing", ""], ["Tan", "Jianlong", ""]]}, {"id": "1910.14280", "submitter": "Navjot Singh", "authors": "Navjot Singh, Deepesh Data, Jemin George, Suhas Diggavi", "title": "SPARQ-SGD: Event-Triggered and Compressed Communication in Decentralized\n  Stochastic Optimization", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze SPARQ-SGD, which is an event-triggered\nand compressed algorithm for decentralized training of large-scale machine\nlearning models. Each node can locally compute a condition (event) which\ntriggers a communication where quantized and sparsified local model parameters\nare sent. In SPARQ-SGD each node takes at least a fixed number ($H$) of local\ngradient steps and then checks if the model parameters have significantly\nchanged compared to its last update; it communicates further compressed model\nparameters only when there is a significant change, as specified by a (design)\ncriterion. We prove that the SPARQ-SGD converges as $O(\\frac{1}{nT})$ and\n$O(\\frac{1}{\\sqrt{nT}})$ in the strongly-convex and non-convex settings,\nrespectively, demonstrating that such aggressive compression, including\nevent-triggered communication, model sparsification and quantization does not\naffect the overall convergence rate as compared to uncompressed decentralized\ntraining; thereby theoretically yielding communication efficiency for \"free\".\nWe evaluate SPARQ-SGD over real datasets to demonstrate significant amount of\nsavings in communication over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 06:58:38 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 23:23:41 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Singh", "Navjot", ""], ["Data", "Deepesh", ""], ["George", "Jemin", ""], ["Diggavi", "Suhas", ""]]}, {"id": "1910.14315", "submitter": "Jiawei Shao", "authors": "Jiawei Shao, Jun Zhang", "title": "BottleNet++: An End-to-End Approach for Feature Compression in\n  Device-Edge Co-Inference Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of various intelligent mobile applications demands the\ndeployment of powerful deep learning models at resource-constrained mobile\ndevices. The device-edge co-inference framework provides a promising solution\nby splitting a neural network at a mobile device and an edge computing server.\nIn order to balance the on-device computation and the communication overhead,\nthe splitting point needs to be carefully picked, while the intermediate\nfeature needs to be compressed before transmission. Existing studies decoupled\nthe design of model splitting, feature compression, and communication, which\nmay lead to excessive resource consumption of the mobile device. In this paper,\nwe introduce an end-to-end architecture, named BottleNet++, that consists of an\nencoder, a non-trainable channel layer, and a decoder for more efficient\nfeature compression and transmission. The encoder and decoder essentially\nimplement joint source-channel coding via convolutional neural networks (CNNs),\nwhile explicitly considering the effect of channel noise. By exploiting the\nstrong sparsity and the fault-tolerant property of the intermediate feature in\na deep neural network (DNN), BottleNet++ achieves a much higher compression\nratio than existing methods. Furthermore, by providing the channel condition to\nthe encoder as an input, our method enjoys a strong generalization ability in\ndifferent channel conditions. Compared with merely transmitting intermediate\ndata without feature compression, BottleNet++ achieves up to 64x bandwidth\nreduction over the additive white Gaussian noise channel and up to 256x bit\ncompression ratio in the binary erasure channel, with less than 2% reduction in\naccuracy. With a higher compression ratio, BottleNet++ enables splitting a DNN\nat earlier layers, which leads to up to 3x reduction in on-device computation\ncompared with other compression methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 08:58:44 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 01:47:15 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 01:43:10 GMT"}, {"version": "v4", "created": "Sat, 18 Jan 2020 08:15:19 GMT"}, {"version": "v5", "created": "Fri, 5 Jun 2020 07:16:49 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Shao", "Jiawei", ""], ["Zhang", "Jun", ""]]}, {"id": "1910.14351", "submitter": "Haitao Xu", "authors": "Haitao Xu, Brendan McCane, Lech Szymanski", "title": "VASE: Variational Assorted Surprise Exploration for Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration in environments with continuous control and sparse rewards\nremains a key challenge in reinforcement learning (RL). Recently, surprise has\nbeen used as an intrinsic reward that encourages systematic and efficient\nexploration. We introduce a new definition of surprise and its RL\nimplementation named Variational Assorted Surprise Exploration (VASE). VASE\nuses a Bayesian neural network as a model of the environment dynamics and is\ntrained using variational inference, alternately updating the accuracy of the\nagent's model and policy. Our experiments show that in continuous control\nsparse reward environments VASE outperforms other surprise-based exploration\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:26:40 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Xu", "Haitao", ""], ["McCane", "Brendan", ""], ["Szymanski", "Lech", ""]]}, {"id": "1910.14354", "submitter": "Ciara Pike-Burke", "authors": "Ciara Pike-Burke and Steffen Gr\\\"unew\\\"alder", "title": "Recovering Bandits", "comments": "accepted to neurips 2019 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the recovering bandits problem, a variant of the stochastic\nmulti-armed bandit problem where the expected reward of each arm varies\naccording to some unknown function of the time since the arm was last played.\nWhile being a natural extension of the classical bandit problem that arises in\nmany real-world settings, this variation is accompanied by significant\ndifficulties. In particular, methods need to plan ahead and estimate many more\nquantities than in the classical bandit setting. In this work, we explore the\nuse of Gaussian processes to tackle the estimation and planing problem. We also\ndiscuss different regret definitions that let us quantify the performance of\nthe methods. To improve computational efficiency of the methods, we provide an\noptimistic planning approximation. We complement these discussions with regret\nbounds and empirical studies.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:34:37 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Pike-Burke", "Ciara", ""], ["Gr\u00fcnew\u00e4lder", "Steffen", ""]]}, {"id": "1910.14356", "submitter": "Aleksandar Bojchevski", "authors": "Aleksandar Bojchevski, Stephan G\\\"unnemann", "title": "Certifiable Robustness to Graph Perturbations", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the exploding interest in graph neural networks there has been little\neffort to verify and improve their robustness. This is even more alarming given\nrecent findings showing that they are extremely vulnerable to adversarial\nattacks on both the graph structure and the node attributes. We propose the\nfirst method for verifying certifiable (non-)robustness to graph perturbations\nfor a general class of models that includes graph neural networks and\nlabel/feature propagation. By exploiting connections to PageRank and Markov\ndecision processes our certificates can be efficiently (and under many threat\nmodels exactly) computed. Furthermore, we investigate robust training\nprocedures that increase the number of certifiably robust nodes while\nmaintaining or improving the clean predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:42:58 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 11:51:15 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bojchevski", "Aleksandar", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1910.14361", "submitter": "Victor Bapst", "authors": "Victor Bapst, Alvaro Sanchez-Gonzalez, Omar Shams, Kimberly\n  Stachenfeld, Peter W. Battaglia, Satinder Singh, Jessica B. Hamrick", "title": "Object-oriented state editing for HRL", "comments": "8 pages; accepted to the Perception as Generative Reasoning workshop\n  of the 33rd Conference on Neural InformationProcessing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce agents that use object-oriented reasoning to consider alternate\nstates of the world in order to more quickly find solutions to problems.\nSpecifically, a hierarchical controller directs a low-level agent to behave as\nif objects in the scene were added, deleted, or modified. The actions taken by\nthe controller are defined over a graph-based representation of the scene, with\nactions corresponding to adding, deleting, or editing the nodes of a graph. We\npresent preliminary results on three environments, demonstrating that our\napproach can achieve similar levels of reward as non-hierarchical agents, but\nwith better data efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:48:45 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Bapst", "Victor", ""], ["Sanchez-Gonzalez", "Alvaro", ""], ["Shams", "Omar", ""], ["Stachenfeld", "Kimberly", ""], ["Battaglia", "Peter W.", ""], ["Singh", "Satinder", ""], ["Hamrick", "Jessica B.", ""]]}, {"id": "1910.14380", "submitter": "Sarath Pattathil", "authors": "Weijie Liu, Aryan Mokhtari, Asuman Ozdaglar, Sarath Pattathil, Zebang\n  Shen, Nenggan Zheng", "title": "A Decentralized Proximal Point-type Method for Saddle Point Problems", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on solving a class of constrained non-convex\nnon-concave saddle point problems in a decentralized manner by a group of nodes\nin a network. Specifically, we assume that each node has access to a summand of\na global objective function and nodes are allowed to exchange information only\nwith their neighboring nodes. We propose a decentralized variant of the\nproximal point method for solving this problem. We show that when the objective\nfunction is $\\rho$-weakly convex-weakly concave the iterates converge to\napproximate stationarity with a rate of $\\mathcal{O}(1/\\sqrt{T})$ where the\napproximation error depends linearly on $\\sqrt{\\rho}$. We further show that\nwhen the objective function satisfies the Minty VI condition (which generalizes\nthe convex-concave case) we obtain convergence to stationarity with a rate of\n$\\mathcal{O}(1/\\sqrt{T})$. To the best of our knowledge, our proposed method is\nthe first decentralized algorithm with theoretical guarantees for solving a\nnon-convex non-concave decentralized saddle point problem. Our numerical\nresults for training a general adversarial network (GAN) in a decentralized\nmanner match our theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 11:24:21 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Liu", "Weijie", ""], ["Mokhtari", "Aryan", ""], ["Ozdaglar", "Asuman", ""], ["Pattathil", "Sarath", ""], ["Shen", "Zebang", ""], ["Zheng", "Nenggan", ""]]}, {"id": "1910.14388", "submitter": "Davide Belli", "authors": "Davide Belli and Thomas Kipf", "title": "Image-Conditioned Graph Generation for Road Network Extraction", "comments": "Presented at NeurIPS 2019 Workshop on Graph Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models for graphs have shown great promise in the area of\ndrug design, but have so far found little application beyond generating\ngraph-structured molecules. In this work, we demonstrate a proof of concept for\nthe challenging task of road network extraction from image data. This task can\nbe framed as image-conditioned graph generation, for which we develop the\nGenerative Graph Transformer (GGT), a deep autoregressive model that makes use\nof attention mechanisms for image conditioning and the recurrent generation of\ngraphs. We benchmark GGT on the application of road network extraction from\nsemantic segmentation data. For this, we introduce the Toulouse Road Network\ndataset, based on real-world publicly-available data. We further propose the\nStreetMover distance: a metric based on the Sinkhorn distance for effectively\nevaluating the quality of road network generation. The code and dataset are\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 11:38:13 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Belli", "Davide", ""], ["Kipf", "Thomas", ""]]}, {"id": "1910.14421", "submitter": "Amir Hossein Akhavan Rahnama", "authors": "Amir Hossein Akhavan Rahnama, Henrik Bostr\\\"om", "title": "A study of data and label shift in the LIME framework", "comments": "Accepted at the Neurip 2019 Workshop \"Human-Centric Machine Learning\"\n  (poster + spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIME is a popular approach for explaining a black-box prediction through an\ninterpretable model that is trained on instances in the vicinity of the\npredicted instance. To generate these instances, LIME randomly selects a subset\nof the non-zero features of the predicted instance. After that, the perturbed\ninstances are fed into the black-box model to obtain labels for these, which\nare then used for training the interpretable model. In this study, we present a\nsystematic evaluation of the interpretable models that are output by LIME on\nthe two use-cases that were considered in the original paper introducing the\napproach; text classification and object detection. The investigation shows\nthat the perturbation and labeling phases result in both data and label shift.\nIn addition, we study the correlation between the shift and the fidelity of the\ninterpretable model and show that in certain cases the shift negatively\ncorrelates with the fidelity. Based on these findings, it is argued that there\nis a need for a new sampling approach that mitigates the shift in the LIME's\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 12:39:03 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Rahnama", "Amir Hossein Akhavan", ""], ["Bostr\u00f6m", "Henrik", ""]]}, {"id": "1910.14425", "submitter": "Farzin Haddadpour", "authors": "Farzin Haddadpour, Mehrdad Mahdavi", "title": "On the Convergence of Local Descent Methods in Federated Learning", "comments": "47 pages, \"Updates from v1: A technical error in Lemma B3 is\n  corrected\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated distributed learning, the goal is to optimize a global training\nobjective defined over distributed devices, where the data shard at each device\nis sampled from a possibly different distribution (a.k.a., heterogeneous or non\ni.i.d. data samples). In this paper, we generalize the local stochastic and\nfull gradient descent with periodic averaging-- originally designed for\nhomogeneous distributed optimization, to solve nonconvex optimization problems\nin federated learning. Although scant research is available on the\neffectiveness of local SGD in reducing the number of communication rounds in\nhomogeneous setting, its convergence and communication complexity in\nheterogeneous setting is mostly demonstrated empirically and lacks through\ntheoretical understating. To bridge this gap, we demonstrate that by properly\nanalyzing the effect of unbiased gradients and sampling schema in federated\nsetting, under mild assumptions, the implicit variance reduction feature of\nlocal distributed methods generalize to heterogeneous data shards and exhibits\nthe best known convergence rates of homogeneous setting both in general\nnonconvex and under {\\pl}~ condition (generalization of strong-convexity). Our\ntheoretical results complement the recent empirical studies that demonstrate\nthe applicability of local GD/SGD to federated learning. We also specialize the\nproposed local method for networked distributed optimization. To the best of\nour knowledge, the obtained convergence rates are the sharpest known to date on\nthe convergence of local decant methods with periodic averaging for solving\nnonconvex federated optimization in both centralized and networked distributed\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 12:52:55 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 21:16:41 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "1910.14428", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Wittawat Jitkrittum, Krikamol Muandet, Bernhard\n  Sch\\\"olkopf", "title": "Kernel-Guided Training of Implicit Generative Models with Stability\n  Guarantees", "comments": "There was a misunderstanding in how an article should be updated on\n  arXiv. We have withdrawn this article from this link. The same article can be\n  found at arXiv:1901.09206", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern implicit generative models such as generative adversarial networks\n(GANs) are generally known to suffer from issues such as instability,\nuninterpretability, and difficulty in assessing their performance. If we see\nthese implicit models as dynamical systems, some of these issues are caused by\nbeing unable to control their behavior in a meaningful way during the course of\ntraining. In this work, we propose a theoretically grounded method to guide the\ntraining trajectories of GANs by augmenting the GAN loss function with a\nkernel-based regularization term that controls local and global discrepancies\nbetween the model and true distributions. This control signal allows us to\ninject prior knowledge into the model. We provide theoretical guarantees on the\nstability of the resulting dynamical system and demonstrate different aspects\nof it via a wide range of experiments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:02:32 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 17:06:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mehrjou", "Arash", ""], ["Jitkrittum", "Wittawat", ""], ["Muandet", "Krikamol", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1910.14460", "submitter": "Javier Cabrera", "authors": "J. Cabrera, D. Amaratunga, W. Kostis and J Kostis", "title": "Precision disease networks (PDN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for building patient-based networks that we call\nPrecision disease networks, and its uses for predicting medical outcomes. Our\nmethodology consists of building networks, one for each patient or case, that\ndescribes the dis-ease evolution of the patient (PDN) and store the networks as\na set of features in a data set of PDN's, one per observation. We cluster the\nPDN data and study the within and between cluster variability. In addition, we\ndevelop data visualization technics in order to display, compare and summarize\nthe network data. Finally, we analyze a dataset of heart diseases patients from\na New Jersey statewide data-base MIDAS (Myocardial Infarction Data Acquisition\nSystem, in order to show that the network data improve on the prediction of\nimportant patient outcomes such as death or cardiovascular death, when compared\nwith the standard statistical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:22:04 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Cabrera", "J.", ""], ["Amaratunga", "D.", ""], ["Kostis", "W.", ""], ["Kostis", "J", ""]]}, {"id": "1910.14472", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang and Zongqing Lu", "title": "Learning Fairness in Multi-Agent Systems", "comments": "NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is essential for human society, contributing to stability and\nproductivity. Similarly, fairness is also the key for many multi-agent systems.\nTaking fairness into multi-agent learning could help multi-agent systems become\nboth efficient and stable. However, learning efficiency and fairness\nsimultaneously is a complex, multi-objective, joint-policy optimization. To\ntackle these difficulties, we propose FEN, a novel hierarchical reinforcement\nlearning model. We first decompose fairness for each agent and propose\nfair-efficient reward that each agent learns its own policy to optimize. To\navoid multi-objective conflict, we design a hierarchy consisting of a\ncontroller and several sub-policies, where the controller maximizes the\nfair-efficient reward by switching among the sub-policies that provides diverse\nbehaviors to interact with the environment. FEN can be trained in a fully\ndecentralized way, making it easy to be deployed in real-world applications.\nEmpirically, we show that FEN easily learns both fairness and efficiency and\nsignificantly outperforms baselines in a variety of multi-agent scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 13:59:37 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Lu", "Zongqing", ""]]}, {"id": "1910.14479", "submitter": "Hui Guan", "authors": "Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan\n  Lim", "title": "In-Place Zero-Space Memory Protection for CNN", "comments": "Accepted in NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) are being actively explored for\nsafety-critical applications such as autonomous vehicles and aerospace, where\nit is essential to ensure the reliability of inference results in the presence\nof possible memory faults. Traditional methods such as error correction codes\n(ECC) and Triple Modular Redundancy (TMR) are CNN-oblivious and incur\nsubstantial memory overhead and energy cost. This paper introduces in-place\nzero-space ECC assisted with a new training scheme weight distribution-oriented\ntraining. The new method provides the first known zero space cost memory\nprotection for CNNs without compromising the reliability offered by traditional\nECC.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:15:11 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Guan", "Hui", ""], ["Ning", "Lin", ""], ["Lin", "Zhen", ""], ["Shen", "Xipeng", ""], ["Zhou", "Huiyang", ""], ["Lim", "Seung-Hwan", ""]]}, {"id": "1910.14481", "submitter": "Dushyant Rao", "authors": "Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan\n  Pascanu, Raia Hadsell", "title": "Continual Unsupervised Representation Learning", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to improve the ability of modern learning systems to\ndeal with non-stationary distributions, typically by attempting to learn a\nseries of tasks sequentially. Prior art in the field has largely considered\nsupervised or reinforcement learning tasks, and often assumes full knowledge of\ntask labels and boundaries. In this work, we propose an approach (CURL) to\ntackle a more general problem that we will refer to as unsupervised continual\nlearning. The focus is on learning representations without any knowledge about\ntask identity, and we explore scenarios when there are abrupt changes between\ntasks, smooth transitions from one task to another, or even when the data is\nshuffled. The proposed approach performs task inference directly within the\nmodel, is able to dynamically expand to capture new concepts over its lifetime,\nand incorporates additional rehearsal-based techniques to deal with\ncatastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised\nlearning setting with MNIST and Omniglot, where the lack of labels ensures no\ninformation is leaked about the task. Further, we demonstrate strong\nperformance compared to prior art in an i.i.d setting, or when adapting the\ntechnique to supervised tasks such as incremental class learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:18:45 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Rao", "Dushyant", ""], ["Visin", "Francesco", ""], ["Rusu", "Andrei A.", ""], ["Teh", "Yee Whye", ""], ["Pascanu", "Razvan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1910.14488", "submitter": "Mingkui Tan", "authors": "Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian Chen, Peilin Zhao,\n  Junzhou Huang", "title": "NAT: Neural Architecture Transformer for Accurate and Compact\n  Architectures", "comments": "This paper is accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective architectures is one of the key factors behind the\nsuccess of deep neural networks. Existing deep architectures are either\nmanually designed or automatically searched by some Neural Architecture Search\n(NAS) methods. However, even a well-searched architecture may still contain\nmany non-significant or redundant modules or operations (e.g., convolution or\npooling), which may not only incur substantial memory consumption and\ncomputation cost but also deteriorate the performance. Thus, it is necessary to\noptimize the operations inside an architecture to improve the performance\nwithout introducing extra computation cost. Unfortunately, such a constrained\noptimization problem is NP-hard. To make the problem feasible, we cast the\noptimization problem into a Markov decision process (MDP) and seek to learn a\nNeural Architecture Transformer (NAT) to replace the redundant operations with\nthe more computationally efficient ones (e.g., skip connection or directly\nremoving the connection). Based on MDP, we learn NAT by exploiting\nreinforcement learning to obtain the optimization policies w.r.t. different\narchitectures. To verify the effectiveness of the proposed strategies, we apply\nNAT on both hand-crafted architectures and NAS based architectures. Extensive\nexperiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate\nthat the transformed architecture by NAT significantly outperforms both its\noriginal form and those architectures optimized by existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:29:09 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 05:26:39 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 16:00:44 GMT"}, {"version": "v4", "created": "Thu, 2 Jan 2020 11:24:12 GMT"}, {"version": "v5", "created": "Mon, 13 Jan 2020 13:39:25 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Guo", "Yong", ""], ["Zheng", "Yin", ""], ["Tan", "Mingkui", ""], ["Chen", "Qi", ""], ["Chen", "Jian", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""]]}, {"id": "1910.14543", "submitter": "Dong Dong", "authors": "Wojciech Czaja, Dong Dong, Pierre-Emmanuel Jabin, Franck Olivier\n  Ndjakou Njeunje", "title": "Transport Model for Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new feature extraction method for complex and large datasets,\nbased on the concept of transport operators on graphs. The proposed approach\ngeneralizes and extends the many existing data representation methodologies\nbuilt upon diffusion processes, to a new domain where dynamical systems play a\nkey role. The main advantage of this approach comes from the ability to exploit\ndifferent relationships than those arising in the context of e.g., Graph\nLaplacians. Fundamental properties of the transport operators are proved. We\ndemonstrate the flexibility of the method by introducing several diverse\nexamples of transformations. We close the paper with a series of computational\nexperiments and applications to the problem of classification of hyperspectral\nsatellite imagery, to illustrate the practical implications of our algorithm\nand its ability to quantify new aspects of relationships within complicated\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:45:07 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Czaja", "Wojciech", ""], ["Dong", "Dong", ""], ["Jabin", "Pierre-Emmanuel", ""], ["Njeunje", "Franck Olivier Ndjakou", ""]]}, {"id": "1910.14576", "submitter": "Raimon Fabregat", "authors": "Raimon Fabregat, Nelly Pustelnik, Paulo Gon\\c{c}alves and Pierre\n  Borgnat", "title": "Solving NMF with smoothness and sparsity constraints using PALM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a problem of dimensionality reduction\nand source separation of data that has been widely used in many fields since it\nwas studied in depth in 1999 by Lee and Seung, including in compression of\ndata, document clustering, processing of audio spectrograms and astronomy. In\nthis work we have adapted a minimization scheme for convex functions with\nnon-differentiable constraints called PALM to solve the NMF problem with\nsolutions that can be smooth and/or sparse, two properties frequently desired.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:29:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 09:04:43 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fabregat", "Raimon", ""], ["Pustelnik", "Nelly", ""], ["Gon\u00e7alves", "Paulo", ""], ["Borgnat", "Pierre", ""]]}, {"id": "1910.14594", "submitter": "Vladimir Golkov", "authors": "Luca Della Libera, Vladimir Golkov, Yue Zhu, Arman Mielke, Daniel\n  Cremers", "title": "Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the reasons for the success of convolutional networks is their\nequivariance/invariance under translations. However, rotatable data such as\nmolecules, living cells, everyday objects, or galaxies require processing with\nequivariance/invariance under rotations in cases where the rotation of the\ncoordinate system does not affect the meaning of the data (e.g. object\nclassification). On the other hand, estimation/processing of rotations is\nnecessary in cases where rotations are important (e.g. motion estimation).\nThere has been recent progress in methods and theory in all these regards. Here\nwe provide an overview of existing methods, both for 2D and 3D rotations (and\ntranslations), and identify commonalities and links between them, in the hope\nthat our insights will be useful for choosing and perfecting the methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:47:46 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Della Libera", "Luca", ""], ["Golkov", "Vladimir", ""], ["Zhu", "Yue", ""], ["Mielke", "Arman", ""], ["Cremers", "Daniel", ""]]}, {"id": "1910.14613", "submitter": "Sharan Narang", "authors": "Arvind Neelakantan, Semih Yavuz, Sharan Narang, Vishaal Prasad, Ben\n  Goodrich, Daniel Duckworth, Chinnadhurai Sankar, Xifeng Yan", "title": "Neural Assistant: Joint Action Prediction, Response Generation, and\n  Latent Knowledge Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog presents a difficult challenge encompassing multiple\nproblems including multi-turn language understanding and generation, knowledge\nretrieval and reasoning, and action prediction. Modern dialog systems typically\nbegin by converting conversation history to a symbolic object referred to as\nbelief state by using supervised learning. The belief state is then used to\nreason on an external knowledge source whose result along with the conversation\nhistory is used in action prediction and response generation tasks\nindependently. Such a pipeline of individually optimized components not only\nmakes the development process cumbersome but also makes it non-trivial to\nleverage session-level user reinforcement signals. In this paper, we develop\nNeural Assistant: a single neural network model that takes conversation history\nand an external knowledge source as input and jointly produces both text\nresponse and action to be taken by the system as output. The model learns to\nreason on the provided knowledge source with weak supervision signal coming\nfrom the text generation and the action prediction tasks, hence removing the\nneed for belief state annotations. In the MultiWOZ dataset, we study the effect\nof distant supervision, and the size of knowledge base on model performance. We\nfind that the Neural Assistant without belief states is able to incorporate\nexternal knowledge information achieving higher factual accuracy scores\ncompared to Transformer. In settings comparable to reported baseline systems,\nNeural Assistant when provided with oracle belief state significantly improves\nlanguage generation performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:01:24 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Yavuz", "Semih", ""], ["Narang", "Sharan", ""], ["Prasad", "Vishaal", ""], ["Goodrich", "Ben", ""], ["Duckworth", "Daniel", ""], ["Sankar", "Chinnadhurai", ""], ["Yan", "Xifeng", ""]]}, {"id": "1910.14634", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Mahdi Soltanolkotabi", "title": "Denoising and Regularization via Exploiting the Structural Bias of\n  Convolutional Generators", "comments": "final ICRL version; simplifications in the proof", "journal-ref": "International Conference on Learning Representations (ICLR) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have emerged as highly successful tools\nfor image generation, recovery, and restoration. A major contributing factor to\nthis success is that convolutional networks impose strong prior assumptions\nabout natural images. A surprising experiment that highlights this\narchitectural bias towards natural images is that one can remove noise and\ncorruptions from a natural image without using any training data, by simply\nfitting (via gradient descent) a randomly initialized, over-parameterized\nconvolutional generator to the corrupted image. While this over-parameterized\nnetwork can fit the corrupted image perfectly, surprisingly after a few\niterations of gradient descent it generates an almost uncorrupted image. This\nintriguing phenomenon enables state-of-the-art CNN-based denoising and\nregularization of other inverse problems. In this paper, we attribute this\neffect to a particular architectural choice of convolutional networks, namely\nconvolutions with fixed interpolating filters. We then formally characterize\nthe dynamics of fitting a two-layer convolutional generator to a noisy signal\nand prove that early-stopped gradient descent denoises/regularizes. Our proof\nrelies on showing that convolutional generators fit the structured part of an\nimage significantly faster than the corrupted portion.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:22:00 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 01:49:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Heckel", "Reinhard", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1910.14655", "submitter": "Huan Zhang", "authors": "Huan Zhang, Minhao Cheng, Cho-Jui Hsieh", "title": "Enhancing Certifiable Robustness via a Deep Model Ensemble", "comments": "This is an extended version of ICLR 2019 Safe Machine Learning\n  Workshop (SafeML) paper, \"RobBoost: A provable approach to boost the\n  robustness of deep model ensemble\". May 6, 2019, New Orleans, LA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to enhance certified robustness of a deep model\nensemble by optimally weighting each base model. Unlike previous works on using\nensembles to empirically improve robustness, our algorithm is based on\noptimizing a guaranteed robustness certificate of neural networks. Our proposed\nensemble framework with certified robustness, RobBoost, formulates the optimal\nmodel selection and weighting task as an optimization problem on a lower bound\nof classification margin, which can be efficiently solved using coordinate\ndescent. Experiments show that our algorithm can form a more robust ensemble\nthan naively averaging all available models using robustly trained MNIST or\nCIFAR base models. Additionally, our ensemble typically has better accuracy on\nclean (unperturbed) data. RobBoost allows us to further improve certified\nrobustness and clean accuracy by creating an ensemble of already certified\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:48:33 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Huan", ""], ["Cheng", "Minhao", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1910.14659", "submitter": "Julian Salazar", "authors": "Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff", "title": "Masked Language Model Scoring", "comments": "ACL 2020 camera-ready (presented July 2020)", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics (2020), 2699-2712", "doi": "10.18653/v1/2020.acl-main.240", "report-no": null, "categories": "cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained masked language models (MLMs) require finetuning for most NLP\ntasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood\nscores (PLLs), which are computed by masking tokens one by one. We show that\nPLLs outperform scores from autoregressive language models like GPT-2 in a\nvariety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an\nend-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on\nstate-of-the-art baselines for low-resource translation pairs, with further\ngains from domain adaptation. We attribute this success to PLL's unsupervised\nexpression of linguistic acceptability without a left-to-right bias, greatly\nimproving on scores from GPT-2 (+10 points on island effects, NPI licensing in\nBLiMP). One can finetune MLMs to give scores without masking, enabling\ncomputation in a single inference pass. In all, PLLs and their associated\npseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of\npretrained MLMs; e.g., we use a single cross-lingual model to rescore\ntranslations in multiple languages. We release our library for language model\nscoring at https://github.com/awslabs/mlm-scoring.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:51:21 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:55:10 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 00:00:14 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Salazar", "Julian", ""], ["Liang", "Davis", ""], ["Nguyen", "Toan Q.", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "1910.14670", "submitter": "Colin Graber", "authors": "Colin Graber, Alexander Schwing", "title": "Graph Structured Prediction Energy Networks", "comments": "Appearing in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For joint inference over multiple variables, a variety of structured\nprediction techniques have been developed to model correlations among variables\nand thereby improve predictions. However, many classical approaches suffer from\none of two primary drawbacks: they either lack the ability to model high-order\ncorrelations among variables while maintaining computationally tractable\ninference, or they do not allow to explicitly model known correlations. To\naddress this shortcoming, we introduce `Graph Structured Prediction Energy\nNetworks,' for which we develop inference techniques that allow to both model\nexplicit local and implicit higher-order correlations while maintaining\ntractability of inference. We apply the proposed method to tasks from the\nnatural language processing and computer vision domain and demonstrate its\ngeneral utility.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:59:57 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 17:52:18 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Graber", "Colin", ""], ["Schwing", "Alexander", ""]]}, {"id": "1910.14673", "submitter": "Tiantian Fang", "authors": "Tiantian Fang and Alexander G. Schwing", "title": "Co-Generation with GANs using AIS based HMC", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the most likely configuration for a subset of variables of a joint\ndistribution given the remaining ones - which we refer to as co-generation - is\nan important challenge that is computationally demanding for all but the\nsimplest settings. This task has received a considerable amount of attention,\nparticularly for classical ways of modeling distributions like structured\nprediction. In contrast, almost nothing is known about this task when\nconsidering recently proposed techniques for modeling high-dimensional\ndistributions, particularly generative adversarial nets (GANs). Therefore, in\nthis paper, we study the occurring challenges for co-generation with GANs. To\naddress those challenges we develop an annealed importance sampling based\nHamiltonian Monte Carlo co-generation algorithm. The presented approach\nsignificantly outperforms classical gradient based methods on a synthetic and\non the CelebA and LSUN datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:59:59 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Fang", "Tiantian", ""], ["Schwing", "Alexander G.", ""]]}]