[{"id": "1104.0354", "submitter": "Yudong Chen", "authors": "Yudong Chen, Ali Jalali, Sujay Sanghavi, Constantine Caramanis", "title": "Low-rank Matrix Recovery from Errors and Erasures", "comments": "27 pages, 3 figures. Appeared in ISIT 2011", "journal-ref": "IEEE Transactions on Information Theory, vol. 59, no. 7,\n  4324-4337, 2013", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the recovery of a low-rank matrix from an observed\nversion that simultaneously contains both (a) erasures: most entries are not\nobserved, and (b) errors: values at a constant fraction of (unknown) locations\nare arbitrarily corrupted. We provide a new unified performance guarantee on\nwhen the natural convex relaxation of minimizing rank plus support succeeds in\nexact recovery. Our result allows for the simultaneous presence of random and\ndeterministic components in both the error and erasure patterns. On the one\nhand, corollaries obtained by specializing this one single result in different\nways recover (up to poly-log factors) all the existing works in matrix\ncompletion, and sparse and low-rank matrix recovery. On the other hand, our\nresults also provide the first guarantees for (a) recovery when we observe a\nvanishing fraction of entries of a corrupted matrix, and (b) deterministic\nmatrix completion.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2011 04:49:00 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2011 05:17:06 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Chen", "Yudong", ""], ["Jalali", "Ali", ""], ["Sanghavi", "Sujay", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1104.0455", "submitter": "Gonzalo Mateos", "authors": "Gonzalo Mateos and Georgios B. Giannakis", "title": "Robust Nonparametric Regression via Sparsity Control with Application to\n  Load Curve Data Cleansing", "comments": "30 pages", "journal-ref": null, "doi": "10.1109/TSP.2011.2181837", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric methods are widely applicable to statistical inference\nproblems, since they rely on a few modeling assumptions. In this context, the\nfresh look advocated here permeates benefits from variable selection and\ncompressive sampling, to robustify nonparametric regression against outliers -\nthat is, data markedly deviating from the postulated models. A variational\ncounterpart to least-trimmed squares regression is shown closely related to an\nL0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector\nexplicitly modeling the outliers. This connection suggests efficient solvers\nbased on convex relaxation, which lead naturally to a variational M-type\nestimator equivalent to the least-absolute shrinkage and selection operator\n(Lasso). Outliers are identified by judiciously tuning regularization\nparameters, which amounts to controlling the sparsity of the outlier vector\nalong the whole robustification path of Lasso solutions. Reduced bias and\nenhanced generalization capability are attractive features of an improved\nestimator obtained after replacing the L0-(pseudo)norm with a nonconvex\nsurrogate. The novel robust spline-based smoother is adopted to cleanse load\ncurve data, a key task aiding operational decisions in the envisioned smart\ngrid system. Computer simulations and tests on real load curve data corroborate\nthe effectiveness of the novel sparsity-controlling robust estimators.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 02:30:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1104.0729", "submitter": "Afshin Rostamizadeh", "authors": "Afshin Rostamizadeh, Alekh Agarwal, Peter Bartlett", "title": "Online and Batch Learning Algorithms for Data with Missing Features", "comments": null, "journal-ref": "27th Conference on Uncertainty in Artificial Intelligence (UAI\n  2011)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new online and batch algorithms that are robust to data with\nmissing features, a situation that arises in many practical applications. In\nthe online setup, we allow for the comparison hypothesis to change as a\nfunction of the subset of features that is observed on any given round,\nextending the standard setting where the comparison hypothesis is fixed\nthroughout. In the batch setup, we present a convex relation of a non-convex\nproblem to jointly estimate an imputation function, used to fill in the values\nof missing features, along with the classification hypothesis. We prove regret\nbounds in the online setting and Rademacher complexity bounds for the batch\ni.i.d. setting. The algorithms are tested on several UCI datasets, showing\nsuperior performance over baselines.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 04:28:51 GMT"}, {"version": "v2", "created": "Mon, 2 May 2011 05:24:55 GMT"}, {"version": "v3", "created": "Sat, 21 May 2011 18:17:23 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2011 15:40:28 GMT"}], "update_date": "2012-02-19", "authors_parsed": [["Rostamizadeh", "Afshin", ""], ["Agarwal", "Alekh", ""], ["Bartlett", "Peter", ""]]}, {"id": "1104.0861", "submitter": "Artin Armagan", "authors": "Artin Armagan, David Dunson, Jaeyong Lee", "title": "Generalized double Pareto shrinkage", "comments": null, "journal-ref": "Statistica Sinica 23 (2013), 119-143", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized double Pareto prior for Bayesian shrinkage\nestimation and inferences in linear models. The prior can be obtained via a\nscale mixture of Laplace or normal distributions, forming a bridge between the\nLaplace and Normal-Jeffreys' priors. While it has a spike at zero like the\nLaplace density, it also has a Student's $t$-like tail behavior. Bayesian\ncomputation is straightforward via a simple Gibbs sampling algorithm. We\ninvestigate the properties of the maximum a posteriori estimator, as sparse\nestimation plays an important role in many problems, reveal connections with\nsome well-established regularization procedures, and show some asymptotic\nresults. The performance of the prior is tested through simulations and an\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 14:42:50 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2011 12:36:36 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2012 18:32:11 GMT"}, {"version": "v4", "created": "Sat, 26 Jan 2013 19:47:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Armagan", "Artin", ""], ["Dunson", "David", ""], ["Lee", "Jaeyong", ""]]}, {"id": "1104.0896", "submitter": "Marco Scutari", "authors": "Marco Scutari and Radhakrishnan Nagarajan", "title": "On Identifying Significant Edges in Graphical Models of Molecular\n  Networks", "comments": "21 pages, 9 figures. Presented at the Conference for Artificial\n  Intelligence in Medicine (AIME '11), Workshop on Probabilistic Problem\n  Solving in Biomedicine", "journal-ref": "Artificial Intelligence in Medicine 2013, 57(3):207-217", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Modelling the associations from high-throughput experimental\nmolecular data has provided unprecedented insights into biological pathways and\nsignalling mechanisms. Graphical models and networks have especially proven to\nbe useful abstractions in this regard. Ad-hoc thresholds are often used in\nconjunction with structure learning algorithms to determine significant\nassociations. The present study overcomes this limitation by proposing a\nstatistically-motivated approach for identifying significant associations in a\nnetwork.\n  Methods and Materials: A new method that identifies significant associations\nin graphical models by estimating the threshold minimising the $L_{\\mathrm{1}}$\nnorm between the cumulative distribution function (CDF) of the observed edge\nconfidences and those of its asymptotic counterpart is proposed. The\neffectiveness of the proposed method is demonstrated on popular synthetic data\nsets as well as publicly available experimental molecular data corresponding to\ngene and protein expression profiles.\n  Results: The improved performance of the proposed approach is demonstrated\nacross the synthetic data sets using sensitivity, specificity and accuracy as\nperformance metrics. The results are also demonstrated across varying sample\nsizes and three different structure learning algorithms with widely varying\nassumptions. In all cases, the proposed approach has specificity and accuracy\nclose to 1, while sensitivity increases linearly in the logarithm of the sample\nsize. The estimated threshold systematically outperforms common ad-hoc ones in\nterms of sensitivity while maintaining comparable levels of specificity and\naccuracy. Networks from experimental data sets are reconstructed accurately\nwith respect to the results from the original papers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 17:08:50 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2011 17:03:45 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2011 10:38:27 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2012 10:02:27 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2013 11:27:58 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Scutari", "Marco", ""], ["Nagarajan", "Radhakrishnan", ""]]}, {"id": "1104.1041", "submitter": "Xiaodong Li", "authors": "Xiaodong Li", "title": "Compressed Sensing and Matrix Completion with Constant Proportion of\n  Corruptions", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve existing results in the field of compressed sensing and matrix\ncompletion when sampled data may be grossly corrupted. We introduce three new\ntheorems. 1) In compressed sensing, we show that if the m \\times n sensing\nmatrix has independent Gaussian entries, then one can recover a sparse signal x\nexactly by tractable \\ell1 minimimization even if a positive fraction of the\nmeasurements are arbitrarily corrupted, provided the number of nonzero entries\nin x is O(m/(log(n/m) + 1)). 2) In the very general sensing model introduced in\n\"A probabilistic and RIPless theory of compressed sensing\" by Candes and Plan,\nand assuming a positive fraction of corrupted measurements, exact recovery\nstill holds if the signal now has O(m/(log^2 n)) nonzero entries. 3) Finally,\nwe prove that one can recover an n \\times n low-rank matrix from m corrupted\nsampled entries by tractable optimization provided the rank is on the order of\nO(m/(n log^2 n)); again, this holds when there is a positive fraction of\ncorrupted samples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 09:20:48 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2012 07:00:50 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Li", "Xiaodong", ""]]}, {"id": "1104.1204", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Planar Cycle Covering Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new variational lower-bound on the minimum energy configuration\nof a planar binary Markov Random Field (MRF). Our method is based on adding\nauxiliary nodes to every face of a planar embedding of the graph in order to\ncapture the effect of unary potentials. A ground state of the resulting\napproximation can be computed efficiently by reduction to minimum-weight\nperfect matching. We show that optimization of variational parameters achieves\nthe same lower-bound as dual-decomposition into the set of all cycles of the\noriginal graph. We demonstrate that our variational optimization converges\nquickly and provides high-quality solutions to hard combinatorial problems\n10-100x faster than competing algorithms that optimize the same bound.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 22:12:14 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1104.1234", "submitter": "Chih Lee", "authors": "Chih Lee, Chun-Hsi Huang", "title": "Negative Example Aided Transcription Factor Binding Site Search", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational approaches to transcription factor binding site identification\nhave been actively researched for the past decade.\n  Negative examples have long been utilized in de novo motif discovery and have\nbeen shown useful in transcription factor binding site search as well.\n  However, understanding of the roles of negative examples in binding site\nsearch is still very limited.\n  We propose the 2-centroid and optimal discriminating vector methods, taking\ninto account negative examples. Cross-validation results on E. coli\ntranscription factors show that the proposed methods benefit from negative\nexamples, outperforming the centroid and position-specific scoring matrix\nmethods. We further show that our proposed methods perform better than a\nstate-of-the-art method. We characterize the proposed methods in the context of\nthe other compared methods and show that, coupled with motif subtype\nidentification, the proposed methods can be effectively applied to a wide range\nof transcription factors. Finally, we argue that the proposed methods are\nwell-suited for eukaryotic transcription factors as well.\n  Software tools are available at: http://biogrid.engr.uconn.edu/tfbs_search/.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 02:31:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lee", "Chih", ""], ["Huang", "Chun-Hsi", ""]]}, {"id": "1104.1436", "submitter": "Massimiliano Pontil", "authors": "Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil, Lixin\n  Shen, Yuesheng Xu", "title": "Efficient First Order Methods for Linear Composite Regularizers", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide class of regularization problems in machine learning and statistics\nemploy a regularization term which is obtained by composing a simple convex\nfunction \\omega with a linear transformation. This setting includes Group Lasso\nmethods, the Fused Lasso and other total variation methods, multi-task learning\nmethods and many more. In this paper, we present a general approach for\ncomputing the proximity operator of this class of regularizers, under the\nassumption that the proximity operator of the function \\omega is known in\nadvance. Our approach builds on a recent line of research on optimal first\norder optimization methods and uses fixed point iterations for numerically\ncomputing the proximity operator. It is more general than current approaches\nand, as we show with numerical simulations, computationally more efficient than\navailable first order methods which do not achieve the optimal rate. In\nparticular, our method outperforms state of the art O(1/T) methods for\noverlapping Group Lasso and matches optimal O(1/T^2) methods for the Fused\nLasso and tree structured Group Lasso.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 20:05:48 GMT"}], "update_date": "2011-04-11", "authors_parsed": [["Argyriou", "Andreas", ""], ["Micchelli", "Charles A.", ""], ["Pontil", "Massimiliano", ""], ["Shen", "Lixin", ""], ["Xu", "Yuesheng", ""]]}, {"id": "1104.1580", "submitter": "Michael Way", "authors": "Michael J. Way and Catherine Naud (NASA Goddard Institute for Space\n  Studies)", "title": "Proceedings of the 2011 New York Workshop on Computer, Earth and Space\n  Science", "comments": "Author lists modified. 82 pages. Workshop Proceedings from CESS 2011\n  in New York City, Goddard Institute for Space Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The purpose of the New York Workshop on Computer, Earth and Space Sciences is\nto bring together the New York area's finest Astronomers, Statisticians,\nComputer Scientists, Space and Earth Scientists to explore potential synergies\nbetween their respective fields. The 2011 edition (CESS2011) was a great\nsuccess, and we would like to thank all of the presenters and participants for\nattending. This year was also special as it included authors from the upcoming\nbook titled \"Advances in Machine Learning and Data Mining for Astronomy\". Over\ntwo days, the latest advanced techniques used to analyze the vast amounts of\ninformation now available for the understanding of our universe and our planet\nwere presented. These proceedings attempt to provide a small window into what\nthe current state of research is in this vast interdisciplinary field and we'd\nlike to thank the speakers who spent the time to contribute to this volume.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 14:08:47 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 13:01:56 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2011 18:57:12 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Way", "Michael J.", "", "NASA Goddard Institute for Space\n  Studies"], ["Naud", "Catherine", "", "NASA Goddard Institute for Space\n  Studies"]]}, {"id": "1104.1672", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade, Tong Zhang", "title": "Dimension-free tail inequalities for sums of random matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive exponential tail inequalities for sums of random matrices with no\ndependence on the explicit matrix dimensions. These are similar to the matrix\nversions of the Chernoff bound and Bernstein inequality except with the\nexplicit matrix dimensions replaced by a trace quantity that can be small even\nwhen the dimension is large or infinite. Some applications to principal\ncomponent analysis and approximate matrix multiplication are given to\nillustrate the utility of the new bounds.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 04:25:04 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2011 05:51:47 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2011 04:17:23 GMT"}], "update_date": "2011-05-16", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1104.1767", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Slicing: Nonsingular Estimation of High Dimensional Covariance Matrices\n  Using Multiway Kronecker Delta Covariance Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonsingular estimation of high dimensional covariance matrices is an\nimportant step in many statistical procedures like classification, clustering,\nvariable selection an future extraction. After a review of the essential\nbackground material, this paper introduces a technique we call slicing for\nobtaining a nonsingular covariance matrix of high dimensional data. Slicing is\nessentially assuming that the data has Kronecker delta covariance structure.\nFinally, we discuss the implications of the results in this paper and provide\nan example of classification for high dimensional gene expression data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2011 13:29:12 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 02:26:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1104.1872", "submitter": "Julien Mairal", "authors": "Julien Mairal, Rodolphe Jenatton (LIENS, INRIA Paris - Rocquencourt),\n  Guillaume Obozinski (LIENS, INRIA Paris - Rocquencourt), Francis Bach (LIENS,\n  INRIA Paris - Rocquencourt)", "title": "Convex and Network Flow Optimization for Structured Sparsity", "comments": "to appear in the Journal of Machine Learning Research (JMLR)", "journal-ref": "Journal of Machine Learning Research 12 (2011) 2681?2720", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of learning problems regularized by a structured\nsparsity-inducing norm defined as the sum of l_2- or l_infinity-norms over\ngroups of variables. Whereas much effort has been put in developing fast\noptimization techniques when the groups are disjoint or embedded in a\nhierarchy, we address here the case of general overlapping groups. To this end,\nwe present two different strategies: On the one hand, we show that the proximal\noperator associated with a sum of l_infinity-norms can be computed exactly in\npolynomial time by solving a quadratic min-cost flow problem, allowing the use\nof accelerated proximal gradient methods. On the other hand, we use proximal\nsplitting techniques, and address an equivalent formulation with\nnon-overlapping groups, but in higher dimension and with additional\nconstraints. We propose efficient and scalable algorithms exploiting these two\nstrategies, which are significantly faster than alternative approaches. We\nillustrate these methods with several problems such as CUR matrix\nfactorization, multi-task learning of tree-structured dictionaries, background\nsubtraction in video sequences, image denoising with wavelets, and topographic\ndictionary learning of natural image patches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 08:04:59 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2011 05:29:41 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2011 05:26:00 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS,\n  INRIA Paris - Rocquencourt"]]}, {"id": "1104.1990", "submitter": "Kevin Xu", "authors": "Kevin S. Xu, Mark Kliger, Alfred O. Hero III", "title": "Adaptive Evolutionary Clustering", "comments": "To appear in Data Mining and Knowledge Discovery, MATLAB toolbox\n  available at http://tbayes.eecs.umich.edu/xukevin/affect", "journal-ref": "Data Mining and Knowledge Discovery 28 (2014) 304-336", "doi": "10.1007/s10618-012-0302-x", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of clustering, the objects to be clustered\nevolve over time, and a clustering result is desired at each time step. In such\napplications, evolutionary clustering typically outperforms traditional static\nclustering by producing clustering results that reflect long-term trends while\nbeing robust to short-term variations. Several evolutionary clustering\nalgorithms have recently been proposed, often by adding a temporal smoothness\npenalty to the cost function of a static clustering method. In this paper, we\nintroduce a different approach to evolutionary clustering by accurately\ntracking the time-varying proximities between objects followed by static\nclustering. We present an evolutionary clustering framework that adaptively\nestimates the optimal smoothing parameter using shrinkage estimation, a\nstatistical approach that improves a naive estimate using additional\ninformation. The proposed framework can be used to extend a variety of static\nclustering algorithms, including hierarchical, k-means, and spectral\nclustering, into evolutionary clustering algorithms. Experiments on synthetic\nand real data sets indicate that the proposed framework outperforms static\nclustering and existing evolutionary clustering algorithms in many scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 16:38:50 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 22:35:53 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 16:17:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Xu", "Kevin S.", ""], ["Kliger", "Mark", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1104.1992", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa", "title": "Unified Treatment of Hidden Markov Switching Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems encountered in several disciplines deal with the\nmodeling of time-series containing different underlying dynamical regimes, for\nwhich probabilistic approaches are very often employed. In this paper we\ndescribe several such approaches in the common framework of graphical models.\nWe give a unified overview of models previously introduced in the literature,\nwhich is simpler and more comprehensive than previous descriptions and enables\nus to highlight commonalities and differences among models that were not\nobserved in the past. In addition, we present several new models and inference\nroutines, which are naturally derived within this unified viewpoint.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 16:45:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chiappa", "Silvia", ""]]}, {"id": "1104.2018", "submitter": "Varun Kanade", "authors": "Sham Kakade and Adam Tauman Kalai and Varun Kanade and Ohad Shamir", "title": "Efficient Learning of Generalized Linear and Single Index Models with\n  Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide\npowerful generalizations of linear regression, where the target variable is\nassumed to be a (possibly unknown) 1-dimensional function of a linear\npredictor. In general, these problems entail non-convex estimation procedures,\nand, in practice, iterative local search heuristics are often used. Kalai and\nSastry (2009) recently provided the first provably efficient method for\nlearning SIMs and GLMs, under the assumptions that the data are in fact\ngenerated under a GLM and under certain monotonicity and Lipschitz constraints.\nHowever, to obtain provable performance, the method requires a fresh sample\nevery iteration. In this paper, we provide algorithms for learning GLMs and\nSIMs, which are both computationally and statistically efficient. We also\nprovide an empirical study, demonstrating their feasibility in practice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 18:24:01 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Kakade", "Sham", ""], ["Kalai", "Adam Tauman", ""], ["Kanade", "Varun", ""], ["Shamir", "Ohad", ""]]}, {"id": "1104.2373", "submitter": "Mark Schmidt", "authors": "Michael P. Friedlander, Mark Schmidt", "title": "Hybrid Deterministic-Stochastic Methods for Data Fitting", "comments": "26 pages. Revised proofs of Theorems 2.6 and 3.1, results unchanged", "journal-ref": "SIAM Journal on Scientific Computing, 34(3):A1380-A1405, 2012", "doi": "10.1137/110830629", "report-no": null, "categories": "cs.NA cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many structured data-fitting applications require the solution of an\noptimization problem involving a sum over a potentially large number of\nmeasurements. Incremental gradient algorithms offer inexpensive iterations by\nsampling a subset of the terms in the sum. These methods can make great\nprogress initially, but often slow as they approach a solution. In contrast,\nfull-gradient methods achieve steady convergence at the expense of evaluating\nthe full objective and gradient on each iteration. We explore hybrid methods\nthat exhibit the benefits of both approaches. Rate-of-convergence analysis\nshows that by controlling the sample size in an incremental gradient algorithm,\nit is possible to maintain the steady convergence rates of full-gradient\nmethods. We detail a practical quasi-Newton implementation based on this\napproach. Numerical experiments illustrate its potential benefits.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 04:20:07 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2011 21:38:43 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2012 23:00:02 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2013 14:33:14 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Friedlander", "Michael P.", ""], ["Schmidt", "Mark", ""]]}, {"id": "1104.2930", "submitter": "Donghui Yan", "authors": "Donghui Yan, Aiyou Chen, Michael I. Jordan", "title": "Cluster Forests", "comments": "23 pages, 6 figures", "journal-ref": "Computational Statistics and Data Analysis 2013, Vol. 66, 178-192", "doi": "10.1016/j.csda.2013.04.010", "report-no": "COMSTA5571", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With inspiration from Random Forests (RF) in the context of classification, a\nnew clustering ensemble method---Cluster Forests (CF) is proposed.\nGeometrically, CF randomly probes a high-dimensional data cloud to obtain \"good\nlocal clusterings\" and then aggregates via spectral clustering to obtain\ncluster assignments for the whole dataset. The search for good local\nclusterings is guided by a cluster quality measure kappa. CF progressively\nimproves each local clustering in a fashion that resembles the tree growth in\nRF. Empirical studies on several real-world datasets under two different\nperformance metrics show that CF compares favorably to its competitors.\nTheoretical analysis reveals that the kappa measure makes it possible to grow\nthe local clustering in a desirable way---it is \"noise-resistant\". A\nclosed-form expression is obtained for the mis-clustering rate of spectral\nclustering under a perturbation model, which yields new insights into some\naspects of spectral clustering.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 21:29:10 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 05:06:04 GMT"}, {"version": "v3", "created": "Thu, 23 May 2013 21:17:26 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Yan", "Donghui", ""], ["Chen", "Aiyou", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1104.3476", "submitter": "Vincent Dubourg", "authors": "V. Dubourg and F. Deheeger and B. Sudret", "title": "Metamodel-based importance sampling for the simulation of rare events", "comments": "8 pages, 3 figures, 1 table. Preprint submitted to ICASP11\n  Mini-symposia entitled \"Meta-models/surrogate models for uncertainty\n  propagation, sensitivity and reliability analysis\"", "journal-ref": "Proceedings of the 11th International Conference on Applications\n  of Statistics and Probability in Civil Engineering (ICASP11). Zurich,\n  Switzerland, August 2011", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of structural reliability, the Monte-Carlo estimator is\nconsidered as the reference probability estimator. However, it is still\nuntractable for real engineering cases since it requires a high number of runs\nof the model. In order to reduce the number of computer experiments, many other\napproaches known as reliability methods have been proposed. A certain approach\nconsists in replacing the original experiment by a surrogate which is much\nfaster to evaluate. Nevertheless, it is often difficult (or even impossible) to\nquantify the error made by this substitution. In this paper an alternative\napproach is developed. It takes advantage of the kriging meta-modeling and\nimportance sampling techniques. The proposed alternative estimator is finally\napplied to a finite element based structural reliability analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2011 13:30:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Dubourg", "V.", ""], ["Deheeger", "F.", ""], ["Sudret", "B.", ""]]}, {"id": "1104.3621", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy and Anand Rangarajan", "title": "Distance Transform Gradient Density Estimation using the Stationary\n  Phase Approximation", "comments": "24 pages, 5 figures", "journal-ref": "SIAM J. Math. Anal., Vol. 44, No. 6, pp. 4250-4273, 2013", "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex wave representation (CWR) converts unsigned 2D distance\ntransforms into their corresponding wave functions. Here, the distance\ntransform S(X) appears as the phase of the wave function\n\\phi(X)---specifically, \\phi(X)=exp(iS(X)/\\tau where \\tau is a free parameter.\nIn this work, we prove a novel result using the higher-order stationary phase\napproximation: we show convergence of the normalized power spectrum (squared\nmagnitude of the Fourier transform) of the wave function to the density\nfunction of the distance transform gradients as the free parameter \\tau-->0. In\ncolloquial terms, spatial frequencies are gradient histogram bins. Since the\ndistance transform gradients have only orientation information (as their\nmagnitudes are identically equal to one almost everywhere), as \\tau-->0, the 2D\nFourier transform values mainly lie on the unit circle in the spatial frequency\ndomain. The proof of the result involves standard integration techniques and\nrequires proper ordering of limits. Our mathematical relation indicates that\nthe CWR of distance transforms is an intriguing, new representation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 01:41:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2011 18:24:24 GMT"}, {"version": "v3", "created": "Sun, 4 Dec 2011 17:45:34 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2013 15:08:38 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1104.3665", "submitter": "Remi Monasson", "authors": "Simona Cocco (LPS), Remi Monasson (LPTENS), Vitor Sessak (LPTENS)", "title": "High-Dimensional Inference with the generalized Hopfield Model:\n  Principal Component Analysis and Corrections", "comments": "Physical Review E: Statistical, Nonlinear, and Soft Matter Physics\n  (2011) to appear", "journal-ref": null, "doi": "10.1103/PhysRevE.83.051123", "report-no": null, "categories": "cond-mat.stat-mech q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the interactions between a set of N\nbinary variables from the knowledge of their frequencies and pairwise\ncorrelations. The inference framework is based on the Hopfield model, a special\ncase of the Ising model where the interaction matrix is defined through a set\nof patterns in the variable space, and is of rank much smaller than N. We show\nthat Maximum Lik elihood inference is deeply related to Principal Component\nAnalysis when the amp litude of the pattern components, xi, is negligible\ncompared to N^1/2. Using techniques from statistical mechanics, we calculate\nthe corrections to the patterns to the first order in xi/N^1/2. We stress that\nit is important to generalize the Hopfield model and include both attractive\nand repulsive patterns, to correctly infer networks with sparse and strong\ninteractions. We present a simple geometrical criterion to decide how many\nattractive and repulsive patterns should be considered as a function of the\nsampling noise. We moreover discuss how many sampled configurations are\nrequired for a good inference, as a function of the system size, N and of the\namplitude, xi. The inference approach is illustrated on synthetic and\nbiological data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 08:01:25 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Cocco", "Simona", "", "LPS"], ["Monasson", "Remi", "", "LPTENS"], ["Sessak", "Vitor", "", "LPTENS"]]}, {"id": "1104.3667", "submitter": "Vincent Dubourg", "authors": "V. Dubourg, B. Sudret and J.-M. Bourinet", "title": "Reliability-based design optimization using kriging surrogates and\n  subset simulation", "comments": "20 pages, 6 figures, 5 tables. Preprint submitted to Springer-Verlag", "journal-ref": "Structural Multisciplinary Optimization. 2011", "doi": "10.1007/s00158-011-0653-8", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present paper is to develop a strategy for solving\nreliability-based design optimization (RBDO) problems that remains applicable\nwhen the performance models are expensive to evaluate. Starting with the\npremise that simulation-based approaches are not affordable for such problems,\nand that the most-probable-failure-point-based approaches do not permit to\nquantify the error on the estimation of the failure probability, an approach\nbased on both metamodels and advanced simulation techniques is explored. The\nkriging metamodeling technique is chosen in order to surrogate the performance\nfunctions because it allows one to genuinely quantify the surrogate error. The\nsurrogate error onto the limit-state surfaces is propagated to the failure\nprobabilities estimates in order to provide an empirical error measure. This\nerror is then sequentially reduced by means of a population-based adaptive\nrefinement technique until the kriging surrogates are accurate enough for\nreliability analysis. This original refinement strategy makes it possible to\nadd several observations in the design of experiments at the same time.\nReliability and reliability sensitivity analyses are performed by means of the\nsubset simulation technique for the sake of numerical efficiency. The adaptive\nsurrogate-based strategy for reliability estimation is finally involved into a\nclassical gradient-based optimization algorithm in order to solve the RBDO\nproblem. The kriging surrogates are built in a so-called augmented reliability\nspace thus making them reusable from one nested RBDO iteration to the other.\nThe strategy is compared to other approaches available in the literature on\nthree academic examples in the field of structural mechanics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 08:07:38 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Dubourg", "V.", ""], ["Sudret", "B.", ""], ["Bourinet", "J. -M.", ""]]}, {"id": "1104.3770", "submitter": "Gilad Lerman", "authors": "Gilad Lerman, Teng Zhang", "title": "Robust recovery of multiple subspaces by geometric l_p minimization", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS914 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 5, 2686-2715", "doi": "10.1214/11-AOS914", "report-no": "IMS-AOS-AOS914", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume i.i.d. data sampled from a mixture distribution with K components\nalong fixed d-dimensional linear subspaces and an additional outlier component.\nFor p>0, we study the simultaneous recovery of the K fixed subspaces by\nminimizing the l_p-averaged distances of the sampled data points from any K\nsubspaces. Under some conditions, we show that if $0<p\\leq1$, then all\nunderlying subspaces can be precisely recovered by l_p minimization with\noverwhelming probability. On the other hand, if K>1 and p>1, then the\nunderlying subspaces cannot be recovered or even nearly recovered by l_p\nminimization. The results of this paper partially explain the successes and\nfailures of the basic approach of l_p energy minimization for modeling data by\nmultiple subspaces.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 15:13:29 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2012 11:50:56 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lerman", "Gilad", ""], ["Zhang", "Teng", ""]]}, {"id": "1104.3792", "submitter": "Yu-Ping Wang", "authors": "J. Duan, Charles Soussen, David Brie, Jerome Idier and Y.-P. Wang", "title": "A sufficient condition on monotonic increase of the number of nonzero\n  entry in the optimizer of L1 norm penalized least-square problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\ell$-1 norm based optimization is widely used in signal processing,\nespecially in recent compressed sensing theory. This paper studies the solution\npath of the $\\ell$-1 norm penalized least-square problem, whose constrained\nform is known as Least Absolute Shrinkage and Selection Operator (LASSO). A\nsolution path is the set of all the optimizers with respect to the evolution of\nthe hyperparameter (Lagrange multiplier). The study of the solution path is of\ngreat significance in viewing and understanding the profile of the tradeoff\nbetween the approximation and regularization terms. If the solution path of a\ngiven problem is known, it can help us to find the optimal hyperparameter under\na given criterion such as the Akaike Information Criterion. In this paper we\npresent a sufficient condition on $\\ell$-1 norm penalized least-square problem.\nUnder this sufficient condition, the number of nonzero entries in the optimizer\nor solution vector increases monotonically when the hyperparameter decreases.\nWe also generalize the result to the often used total variation case, where the\n$\\ell$-1 norm is taken over the first order derivative of the solution vector.\nWe prove that the proposed condition has intrinsic connections with the\ncondition given by Donoho, et al \\cite{Donoho08} and the positive cone\ncondition by Efron {\\it el al} \\cite{Efron04}. However, the proposed condition\ndoes not need to assume the sparsity level of the signal as required by Donoho\net al's condition, and is easier to verify than Efron, et al's positive cone\ncondition when being used for practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 16:19:03 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Duan", "J.", ""], ["Soussen", "Charles", ""], ["Brie", "David", ""], ["Idier", "Jerome", ""], ["Wang", "Y. -P.", ""]]}, {"id": "1104.3904", "submitter": "Lovro \\v{S}ubelj", "authors": "Lovro \\v{S}ubelj, \\v{S}tefan Furlan and Marko Bajec", "title": "An expert system for detecting automobile insurance fraud using social\n  network analysis", "comments": null, "journal-ref": "Expert Syst. Appl. 38(1), 1039-1052 (2011)", "doi": "10.1016/j.eswa.2010.07.143", "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article proposes an expert system for detection, and subsequent\ninvestigation, of groups of collaborating automobile insurance fraudsters. The\nsystem is described and examined in great detail, several technical\ndifficulties in detecting fraud are also considered, for it to be applicable in\npractice. Opposed to many other approaches, the system uses networks for\nrepresentation of data. Networks are the most natural representation of such a\nrelational domain, allowing formulation and analysis of complex relations\nbetween entities. Fraudulent entities are found by employing a novel assessment\nalgorithm, \\textit{Iterative Assessment Algorithm} (\\textit{IAA}), also\npresented in the article. Besides intrinsic attributes of entities, the\nalgorithm explores also the relations between entities. The prototype was\nevaluated and rigorously analyzed on real world data. Results show that\nautomobile insurance fraud can be efficiently detected with the proposed system\nand that appropriate data representation is vital.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 23:30:17 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["\u0160ubelj", "Lovro", ""], ["Furlan", "\u0160tefan", ""], ["Bajec", "Marko", ""]]}, {"id": "1104.4063", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Fast redshift clustering with the Baire (ultra) metric", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": "10.1142/9789814383295_0005", "report-no": null, "categories": "cs.IR astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. We apply the Baire distance to\nspectrometric and photometric redshifts from the Sloan Digital Sky Survey\nusing, in this work, about half a million astronomical objects. We want to know\nhow well the (more cos\\ tly to determine) spectrometric redshifts can predict\nthe (more easily obtained) photometric redshifts, i.e. we seek to regress the\nspectrometric on the photometric redshifts, and we develop a clusterwise\nnearest neighbor regression procedure for this.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 15:51:50 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1104.4302", "submitter": "Vincent Tan", "authors": "Vincent Y. F. Tan, Laura Balzano, Stark C. Draper", "title": "Rank Minimization over Finite Fields: Fundamental Limits and\n  Coding-Theoretic Interpretations", "comments": "Accepted to the IEEE Transactions on Information Theory; Presented at\n  IEEE International Symposium on Information Theory (ISIT) 2011", "journal-ref": null, "doi": "10.1109/TIT.2011.2178017", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes information-theoretic limits in estimating a finite\nfield low-rank matrix given random linear measurements of it. These linear\nmeasurements are obtained by taking inner products of the low-rank matrix with\nrandom sensing matrices. Necessary and sufficient conditions on the number of\nmeasurements required are provided. It is shown that these conditions are sharp\nand the minimum-rank decoder is asymptotically optimal. The reliability\nfunction of this decoder is also derived by appealing to de Caen's lower bound\non the probability of a union. The sufficient condition also holds when the\nsensing matrices are sparse - a scenario that may be amenable to efficient\ndecoding. More precisely, it is shown that if the n\\times n-sensing matrices\ncontain, on average, \\Omega(nlog n) entries, the number of measurements\nrequired is the same as that when the sensing matrices are dense and contain\nentries drawn uniformly at random from the field. Analogies are drawn between\nthe above results and rank-metric codes in the coding theory literature. In\nfact, we are also strongly motivated by understanding when minimum rank\ndistance decoding of random rank-metric codes succeeds. To this end, we derive\ndistance properties of equiprobable and sparse rank-metric codes. These\ndistance properties provide a precise geometric interpretation of the fact that\nthe sparse ensemble requires as few measurements as the dense one. Finally, we\nprovide a non-exhaustive procedure to search for the unknown low-rank matrix.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 16:15:02 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2011 04:35:28 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2011 20:16:45 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2011 20:31:50 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tan", "Vincent Y. F.", ""], ["Balzano", "Laura", ""], ["Draper", "Stark C.", ""]]}, {"id": "1104.4385", "submitter": "Nikhil Rao", "authors": "Nikhil S Rao, Robert D. Nowak, Stephen J. Wright and Nick G. Kingsbury", "title": "Convex Approaches to Model Wavelet Sparsity Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical dependencies among wavelet coefficients are commonly represented\nby graphical models such as hidden Markov trees(HMTs). However, in linear\ninverse problems such as deconvolution, tomography, and compressed sensing, the\npresence of a sensing or observation matrix produces a linear mixing of the\nsimple Markovian dependency structure. This leads to reconstruction problems\nthat are non-convex optimizations. Past work has dealt with this issue by\nresorting to greedy or suboptimal iterative reconstruction methods. In this\npaper, we propose new modeling approaches based on group-sparsity penalties\nthat leads to convex optimizations that can be solved exactly and efficiently.\nWe show that the methods we develop perform significantly better in\ndeconvolution and compressed sensing applications, while being as\ncomputationally efficient as standard coefficient-wise approaches such as\nlasso.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 04:35:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rao", "Nikhil S", ""], ["Nowak", "Robert D.", ""], ["Wright", "Stephen J.", ""], ["Kingsbury", "Nick G.", ""]]}, {"id": "1104.4512", "submitter": "Vassilis Kekatos", "authors": "Pedro A. Forero, Vassilis Kekatos, Georgios B. Giannakis", "title": "Robust Clustering Using Outlier-Sparsity Regularization", "comments": "Submitted to IEEE Trans. on PAMI", "journal-ref": null, "doi": "10.1109/TSP.2012.2196696", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notwithstanding the popularity of conventional clustering algorithms such as\nK-means and probabilistic clustering, their clustering results are sensitive to\nthe presence of outliers in the data. Even a few outliers can compromise the\nability of these algorithms to identify meaningful hidden structures rendering\ntheir outcome unreliable. This paper develops robust clustering algorithms that\nnot only aim to cluster the data, but also to identify the outliers. The novel\napproaches rely on the infrequent presence of outliers in the data which\ntranslates to sparsity in a judiciously chosen domain. Capitalizing on the\nsparsity in the outlier domain, outlier-aware robust K-means and probabilistic\nclustering approaches are proposed. Their novelty lies on identifying outliers\nwhile effecting sparsity in the outlier domain through carefully chosen\nregularization. A block coordinate descent approach is developed to obtain\niterative algorithms with convergence guarantees and small excess computational\ncomplexity with respect to their non-robust counterparts. Kernelized versions\nof the robust clustering algorithms are also developed to efficiently handle\nhigh-dimensional data, identify nonlinearly separable clusters, or even cluster\nobjects that are not represented by vectors. Numerical tests on both synthetic\nand real datasets validate the performance and applicability of the novel\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 22:01:14 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Forero", "Pedro A.", ""], ["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1104.4595", "submitter": "Tingni Sun", "authors": "Tingni Sun, Cun-Hui Zhang", "title": "Scaled Sparse Linear Regression", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaled sparse linear regression jointly estimates the regression coefficients\nand noise level in a linear model. It chooses an equilibrium with a sparse\nregression method by iteratively estimating the noise level via the mean\nresidual square and scaling the penalty in proportion to the estimated noise\nlevel. The iterative algorithm costs little beyond the computation of a path or\ngrid of the sparse regression estimator for penalty levels above a proper\nthreshold. For the scaled lasso, the algorithm is a gradient descent in a\nconvex minimization of a penalized joint loss function for the regression\ncoefficients and noise level. Under mild regularity conditions, we prove that\nthe scaled lasso simultaneously yields an estimator for the noise level and an\nestimated coefficient vector satisfying certain oracle inequalities for\nprediction, the estimation of the noise level and the regression coefficients.\nThese inequalities provide sufficient conditions for the consistency and\nasymptotic normality of the noise level estimator, including certain cases\nwhere the number of variables is of greater order than the sample size.\nParallel results are provided for the least squares estimation after model\nselection by the scaled lasso. Numerical results demonstrate the superior\nperformance of the proposed methods over an earlier proposal of joint convex\nminimization.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 00:03:25 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 01:34:25 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Sun", "Tingni", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1104.4605", "submitter": "Xiaoye Jiang", "authors": "Xiaoye Jiang and Yuan Yao and Han Liu and Leonidas Guibas", "title": "Compressive Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data acquisition routinely produces massive amounts of network data.\nThough many methods and models have been proposed to analyze such data, the\nresearch of network data is largely disconnected with the classical theory of\nstatistical learning and signal processing. In this paper, we present a new\nframework for modeling network data, which connects two seemingly different\nareas: network data analysis and compressed sensing. From a nonparametric\nperspective, we model an observed network using a large dictionary. In\nparticular, we consider the network clique detection problem and show\nconnections between our formulation with a new algebraic tool, namely Randon\nbasis pursuit in homogeneous spaces. Such a connection allows us to identify\nrigorous recovery conditions for clique detection problems. Though this paper\nis mainly conceptual, we also develop practical approximation algorithms for\nsolving empirical problems and demonstrate their usefulness on real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 06:06:12 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Jiang", "Xiaoye", ""], ["Yao", "Yuan", ""], ["Liu", "Han", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1104.4803", "submitter": "Yudong Chen", "authors": "Yudong Chen, Ali Jalali, Sujay Sanghavi and Huan Xu", "title": "Clustering Partially Observed Graphs via Convex Optimization", "comments": "This is the final version published in Journal of Machine Learning\n  Research (JMLR). Partial results appeared in International Conference on\n  Machine Learning (ICML) 2011", "journal-ref": "Journal of Machine Learning Research, vol. 15, pp. 2213-2238, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of clustering a partially observed\nunweighted graph---i.e., one where for some node pairs we know there is an edge\nbetween them, for some others we know there is no edge, and for the remaining\nwe do not know whether or not there is an edge. We want to organize the nodes\ninto disjoint clusters so that there is relatively dense (observed)\nconnectivity within clusters, and sparse across clusters.\n  We take a novel yet natural approach to this problem, by focusing on finding\nthe clustering that minimizes the number of \"disagreements\"---i.e., the sum of\nthe number of (observed) missing edges within clusters, and (observed) present\nedges across clusters. Our algorithm uses convex optimization; its basis is a\nreduction of disagreement minimization to the problem of recovering an\n(unknown) low-rank matrix and an (unknown) sparse matrix from their partially\nobserved sum. We evaluate the performance of our algorithm on the classical\nPlanted Partition/Stochastic Block Model. Our main theorem provides sufficient\nconditions for the success of our algorithm as a function of the minimum\ncluster size, edge density and observation probability; in particular, the\nresults characterize the tradeoff between the observation probability and the\nedge density gap. When there are a constant number of clusters of equal size,\nour results are optimal up to logarithmic factors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 20:33:55 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2011 19:08:49 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2013 21:27:20 GMT"}, {"version": "v4", "created": "Thu, 24 Jul 2014 00:44:05 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Chen", "Yudong", ""], ["Jalali", "Ali", ""], ["Sanghavi", "Sujay", ""], ["Xu", "Huan", ""]]}, {"id": "1104.4824", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Sahand N. Negahban and Martin J. Wainwright", "title": "Fast global convergence of gradient methods for high-dimensional\n  statistical recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical $M$-estimators are based on convex optimization problems\nformed by the combination of a data-dependent loss function with a norm-based\nregularizer. We analyze the convergence rates of projected gradient and\ncomposite gradient methods for solving such problems, working within a\nhigh-dimensional framework that allows the data dimension $\\pdim$ to grow with\n(and possibly exceed) the sample size $\\numobs$. This high-dimensional\nstructure precludes the usual global assumptions---namely, strong convexity and\nsmoothness conditions---that underlie much of classical optimization analysis.\nWe define appropriately restricted versions of these conditions, and show that\nthey are satisfied with high probability for various statistical models. Under\nthese conditions, our theory guarantees that projected gradient descent has a\nglobally geometric rate of convergence up to the \\emph{statistical precision}\nof the model, meaning the typical distance between the true unknown parameter\n$\\theta^*$ and an optimal solution $\\hat{\\theta}$. This result is substantially\nsharper than previous convergence results, which yielded sublinear convergence,\nor linear convergence only up to the noise level. Our analysis applies to a\nwide range of $M$-estimators and statistical models, including sparse linear\nregression using Lasso ($\\ell_1$-regularized regression); group Lasso for block\nsparsity; log-linear models with regularization; low-rank matrix recovery using\nnuclear norm regularization; and matrix decomposition. Overall, our analysis\nreveals interesting connections between statistical precision and computational\nefficiency in high-dimensional estimation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 23:53:45 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 02:25:56 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2012 19:13:52 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Agarwal", "Alekh", ""], ["Negahban", "Sahand N.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1104.5061", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula, Cynthia Rudin", "title": "On Combining Machine Learning with Decision Making", "comments": "35 pages, 16 figures, longer version of a paper appearing in\n  Algorithmic Decision Theory 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new application and covering number bound for the framework of\n\"Machine Learning with Operational Costs (MLOC),\" which is an exploratory form\nof decision theory. The MLOC framework incorporates knowledge about how a\npredictive model will be used for a subsequent task, thus combining machine\nlearning with the decision that is made afterwards. In this work, we use the\nMLOC framework to study a problem that has implications for power grid\nreliability and maintenance, called the Machine Learning and Traveling\nRepairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a\n\"repair crew,\" which repairs nodes on a graph. The repair crew aims to minimize\nthe cost of failures at the nodes, but as in many real situations, the failure\nprobabilities are not known and must be estimated. The MLOC framework allows us\nto understand how this uncertainty influences the repair route. We also present\nnew covering number generalization bounds for the MLOC framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 01:21:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 01:07:49 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1104.5070", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari", "title": "Online Learning: Stochastic and Constrained Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning theory has largely focused on two main learning scenarios. The first\nis the classical statistical setting where instances are drawn i.i.d. from a\nfixed distribution and the second scenario is the online learning, completely\nadversarial scenario where adversary at every time step picks the worst\ninstance to provide the learner with. It can be argued that in the real world\nneither of these assumptions are reasonable. It is therefore important to study\nproblems with a range of assumptions on data. Unfortunately, theoretical\nresults in this area are scarce, possibly due to absence of general tools for\nanalysis. Focusing on the regret formulation, we define the minimax value of a\ngame where the adversary is restricted in his moves. The framework captures\nstochastic and non-stochastic assumptions on data. Building on the sequential\nsymmetrization approach, we define a notion of distribution-dependent\nRademacher complexity for the spectrum of problems ranging from i.i.d. to\nworst-case. The bounds let us immediately deduce variation-type bounds. We then\nconsider the i.i.d. adversary and show equivalence of online and batch\nlearnability. In the supervised setting, we consider various hybrid assumptions\non the way that x and y variables are chosen. Finally, we consider smoothed\nlearning problems and show that half-spaces are online learnable in the\nsmoothed model. In fact, exponentially small noise added to adversary's\ndecisions turns this problem with infinite Littlestone's dimension into a\nlearnable problem.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 04:11:10 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1104.5186", "submitter": "Samet Oymak", "authors": "Samet Oymak, Babak Hassibi", "title": "Finding Dense Clusters via \"Low Rank + Sparse\" Decomposition", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding \"densely connected clusters\" in a graph is in general an important\nand well studied problem in the literature \\cite{Schaeffer}. It has various\napplications in pattern recognition, social networking and data mining\n\\cite{Duda,Mishra}. Recently, Ames and Vavasis have suggested a novel method\nfor finding cliques in a graph by using convex optimization over the adjacency\nmatrix of the graph \\cite{Ames, Ames2}. Also, there has been recent advances in\ndecomposing a given matrix into its \"low rank\" and \"sparse\" components\n\\cite{Candes, Chandra}. In this paper, inspired by these results, we view\n\"densely connected clusters\" as imperfect cliques, where imperfections\ncorrespond missing edges, which are relatively sparse. We analyze the problem\nin a probabilistic setting and aim to detect disjointly planted clusters. Our\nmain result basically suggests that, one can find \\emph{dense} clusters in a\ngraph, as long as the clusters are sufficiently large. We conclude by\ndiscussing possible extensions and future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 16:51:01 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Oymak", "Samet", ""], ["Hassibi", "Babak", ""]]}, {"id": "1104.5280", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao", "title": "Iterative Reweighted Algorithms for Sparse Signal Recovery with\n  Temporally Correlated Source Vectors", "comments": "Accepted by ICASSP 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative reweighted algorithms, as a class of algorithms for sparse signal\nrecovery, have been found to have better performance than their non-reweighted\ncounterparts. However, for solving the problem of multiple measurement vectors\n(MMVs), all the existing reweighted algorithms do not account for temporal\ncorrelation among source vectors and thus their performance degrades\nsignificantly in the presence of correlation. In this work we propose an\niterative reweighted sparse Bayesian learning (SBL) algorithm exploiting the\ntemporal correlation, and motivated by it, we propose a strategy to improve\nexisting reweighted $\\ell_2$ algorithms for the MMV problem, i.e. replacing\ntheir row norms with Mahalanobis distance measure. Simulations show that the\nproposed reweighted SBL algorithm has superior performance, and the proposed\nimprovement strategy is effective for existing reweighted $\\ell_2$ algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 02:48:08 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1104.5341", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu", "title": "Joint estimation of linear non-Gaussian acyclic models", "comments": "A revised version was accepted in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A linear non-Gaussian structural equation model called LiNGAM is an\nidentifiable model for exploratory causal analysis. Previous methods estimate a\ncausal ordering of variables and their connection strengths based on a single\ndataset. However, in many application domains, data are obtained under\ndifferent conditions, that is, multiple datasets are obtained rather than a\nsingle dataset. In this paper, we present a new method to jointly estimate\nmultiple LiNGAMs under the assumption that the models share a causal ordering\nbut may have different connection strengths and differently distributed\nvariables. In simulations, the new method estimates the models more accurately\nthan estimating them separately.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 10:02:17 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 07:47:30 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Shimizu", "Shohei", ""]]}, {"id": "1104.5466", "submitter": "Daniel Burfoot", "authors": "Daniel Burfoot", "title": "Notes on a New Philosophy of Empirical Science", "comments": "Draft Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book presents a methodology and philosophy of empirical science based on\nlarge scale lossless data compression. In this view a theory is scientific if\nit can be used to build a data compression program, and it is valuable if it\ncan compress a standard benchmark database to a small size, taking into account\nthe length of the compressor itself. This methodology therefore includes an\nOccam principle as well as a solution to the problem of demarcation. Because of\nthe fundamental difficulty of lossless compression, this type of research must\nbe empirical in nature: compression can only be achieved by discovering and\ncharacterizing empirical regularities in the data. Because of this, the\nphilosophy provides a way to reformulate fields such as computer vision and\ncomputational linguistics as empirical sciences: the former by attempting to\ncompress databases of natural images, the latter by attempting to compress\nlarge text databases. The book argues that the rigor and objectivity of the\ncompression principle should set the stage for systematic progress in these\nfields. The argument is especially strong in the context of computer vision,\nwhich is plagued by chronic problems of evaluation.\n  The book also considers the field of machine learning. Here the traditional\napproach requires that the models proposed to solve learning problems be\nextremely simple, in order to avoid overfitting. However, the world may contain\nintrinsically complex phenomena, which would require complex models to\nunderstand. The compression philosophy can justify complex models because of\nthe large quantity of data being modeled (if the target database is 100 Gb, it\nis easy to justify a 10 Mb model). The complex models and abstractions learned\non the basis of the raw data (images, language, etc) can then be reused to\nsolve any specific learning problem, such as face recognition or machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 18:46:06 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["Burfoot", "Daniel", ""]]}, {"id": "1104.5525", "submitter": "John Duchi", "authors": "Alekh Agarwal and John C. Duchi", "title": "Distributed Delayed Stochastic Optimization", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence of gradient-based optimization algorithms that\nbase their updates on delayed stochastic gradient information. The main\napplication of our results is to the development of gradient-based distributed\noptimization algorithms where a master node performs parameter updates while\nworker nodes compute stochastic gradients based on local information in\nparallel, which may give rise to delays due to asynchrony. We take motivation\nfrom statistical problems where the size of the data is so large that it cannot\nfit on one computer; with the advent of huge datasets in biology, astronomy,\nand the internet, such problems are now common. Our main contribution is to\nshow that for smooth stochastic problems, the delays are asymptotically\nnegligible and we can achieve order-optimal convergence results. In application\nto distributed optimization, we develop procedures that overcome communication\nbottlenecks and synchronization requirements. We show $n$-node architectures\nwhose optimization error in stochastic problems---in spite of asynchronous\ndelays---scales asymptotically as $\\order(1 / \\sqrt{nT})$ after $T$ iterations.\nThis rate is known to be optimal for a distributed system with $n$ nodes even\nin the absence of delays. We additionally complement our theoretical results\nwith numerical experiments on a statistical machine learning task.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 23:03:10 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Agarwal", "Alekh", ""], ["Duchi", "John C.", ""]]}, {"id": "1104.5667", "submitter": "Eduardo Mendes", "authors": "Eduardo F. Mendes", "title": "Model Selection Consistency for Cointegrating Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic properties of the adaptive Lasso in cointegration\nregressions in the case where all covariates are weakly exogenous. We assume\nthe number of candidate I(1) variables is sub-linear with respect to the sample\nsize (but possibly larger) and the number of candidate I(0) variables is\npolynomial with respect to the sample size. We show that, under classical\nconditions used in cointegration analysis, this estimator asymptotically\nchooses the correct subset of variables in the model and its asymptotic\ndistribution is the same as the distribution of the OLS estimate given the\nvariables in the model were known in beforehand (oracle property). We also\nderive an algorithm based on the local quadratic approximation and present a\nnumerical study to show the adequacy of the method in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 15:46:28 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 06:01:27 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2011 11:02:19 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Mendes", "Eduardo F.", ""]]}, {"id": "1104.5687", "submitter": "Christos Dimitrakakis", "authors": "Constantin Rothkopf and Christos Dimitrakakis", "title": "Preference elicitation and inverse reinforcement learning", "comments": "13 pages, 4 figures; ECML 2011", "journal-ref": null, "doi": null, "report-no": "EPFL-REPORT-165975", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We state the problem of inverse reinforcement learning in terms of preference\nelicitation, resulting in a principled (Bayesian) statistical formulation. This\ngeneralises previous work on Bayesian inverse reinforcement learning and allows\nus to obtain a posterior distribution on the agent's preferences, policy and\noptionally, the obtained reward sequence, from observations. We examine the\nrelation of the resulting approach to other statistical methods for inverse\nreinforcement learning via analysis and experimental results. We show that\npreferences can be determined accurately, even if the observed agent's policy\nis sub-optimal with respect to its own preferences. In that case, significantly\nimproved policies with respect to the agent's preferences are obtained,\ncompared to both other methods and to the performance of the demonstrated\npolicy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 17:45:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2011 14:06:42 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Rothkopf", "Constantin", ""], ["Dimitrakakis", "Christos", ""]]}]