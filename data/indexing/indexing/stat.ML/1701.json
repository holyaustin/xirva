[{"id": "1701.00138", "submitter": "Jun Suzuki", "authors": "Jun Suzuki, Masaaki Nagata", "title": "Cutting-off Redundant Repeating Generations for Neural Abstractive\n  Summarization", "comments": "7 pages, a draft version of EACL-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the reduction of redundant repeating generation that is\noften observed in RNN-based encoder-decoder models. Our basic idea is to\njointly estimate the upper-bound frequency of each target vocabulary in the\nencoder and control the output words based on the estimation in the decoder.\nOur method shows significant improvement over a strong RNN-based\nencoder-decoder baseline and achieved its best results on an abstractive\nsummarization benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 16:41:43 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 23:40:09 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Suzuki", "Jun", ""], ["Nagata", "Masaaki", ""]]}, {"id": "1701.00167", "submitter": "David Picard", "authors": "David Picard", "title": "Very Fast Kernel SVM under Budget Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we propose a fast online Kernel SVM algorithm under tight\nbudget constraints. We propose to split the input space using LVQ and train a\nKernel SVM in each cluster. To allow for online training, we propose to limit\nthe size of the support vector set of each cluster using different strategies.\nWe show in the experiment that our algorithm is able to achieve high accuracy\nwhile having a very high number of samples processed per second both in\ntraining and in the evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 21:17:08 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Picard", "David", ""]]}, {"id": "1701.00178", "submitter": "Jan-Peter Calliess", "authors": "Jan-Peter Calliess", "title": "Lazily Adapted Constant Kinky Inference for Nonparametric Regression and\n  Model-Reference Adaptive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques known as Nonlinear Set Membership prediction, Lipschitz\nInterpolation or Kinky Inference are approaches to machine learning that\nutilise presupposed Lipschitz properties to compute inferences over unobserved\nfunction values. Provided a bound on the true best Lipschitz constant of the\ntarget function is known a priori they offer convergence guarantees as well as\nbounds around the predictions. Considering a more general setting that builds\non Hoelder continuity relative to pseudo-metrics, we propose an online method\nfor estimating the Hoelder constant online from function value observations\nthat possibly are corrupted by bounded observational errors. Utilising this to\ncompute adaptive parameters within a kinky inference rule gives rise to a\nnonparametric machine learning method, for which we establish strong universal\napproximation guarantees. That is, we show that our prediction rule can learn\nany continuous function in the limit of increasingly dense data to within a\nworst-case error bound that depends on the level of observational uncertainty.\nWe apply our method in the context of nonparametric model-reference adaptive\ncontrol (MRAC). Across a range of simulated aircraft roll-dynamics and\nperformance metrics our approach outperforms recently proposed alternatives\nthat were based on Gaussian processes and RBF-neural networks. For\ndiscrete-time systems, we provide guarantees on the tracking success of our\nlearning-based controllers both for the batch and the online learning setting.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 23:25:59 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 15:36:08 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 20:14:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Calliess", "Jan-Peter", ""]]}, {"id": "1701.00251", "submitter": "Jiashi Feng", "authors": "Jiashi Feng, Huan Xu, Shie Mannor", "title": "Outlier Robust Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from noisy data in practical settings\nwhere the size of data is too large to store on a single machine. More\nchallenging, the data coming from the wild may contain malicious outliers. To\naddress the scalability and robustness issues, we present an online robust\nlearning (ORL) approach. ORL is simple to implement and has provable robustness\nguarantee -- in stark contrast to existing online learning approaches that are\ngenerally fragile to outliers. We specialize the ORL approach for two concrete\ncases: online robust principal component analysis and online linear regression.\nWe demonstrate the efficiency and robustness advantages of ORL through\ncomprehensive simulations and predicting image tags on a large-scale data set.\nWe also discuss extension of the ORL to distributed learning and provide\nexperimental evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 15:18:13 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1701.00285", "submitter": "Julio Castrillon PhD", "authors": "Julio E. Castrillon-Candas", "title": "High dimensional multilevel Kriging: A computational mathematics\n  approach", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of massive data sets much of the computational science and\nengineering communities have been moving toward data-driven approaches such as\nregression and classification. However, they present a significant challenge\ndue to the increasing size, complexity and dimensionality of the problems. In\nthis paper a multilevel Kriging method that scales well with the number of\nobservations and dimensions is developed. A multilevel basis is constructed\nthat is adapted to a kD-tree partitioning of the observations. Numerically\nunstable covariance matrices with large condition numbers are transformed into\nwell conditioned multilevel matrices without compromising accuracy. Moreover,\nit is shown that the multilevel prediction $exactly$ solves the Best Linear\nUnbiased Predictor (BLUP), but is numerically stable. The multilevel method is\ntested on numerically unstable problems of up 25 dimensions. Numerical results\nshow speedups of up to 42,050 for solving the BLUP problem but to the same\naccuracy than the traditional iterative approach.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 20:11:06 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 13:29:04 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Castrillon-Candas", "Julio E.", ""]]}, {"id": "1701.00294", "submitter": "Alejandro Frery", "authors": "Jos\\'e Naranjo-Torres, Juliana Gambini, and Alejandro C. Frery", "title": "The Geodesic Distance between $\\mathcal{G}_I^0$ Models and its\n  Application to Region Discrimination", "comments": "Accepted for publication in the IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing (J-STARS), 1 January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathcal{G}_I^0$ distribution is able to characterize different regions\nin monopolarized SAR imagery. It is indexed by three parameters: the number of\nlooks (which can be estimated in the whole image), a scale parameter and a\ntexture parameter. This paper presents a new proposal for feature extraction\nand region discrimination in SAR imagery, using the geodesic distance as a\nmeasure of dissimilarity between $\\mathcal{G}_I^0$ models. We derive geodesic\ndistances between models that describe several practical situations, assuming\nthe number of looks known, for same and different texture and for same and\ndifferent scale. We then apply this new tool to the problems of (i)~identifying\nedges between regions with different texture, and (ii)~quantify the\ndissimilarity between pairs of samples in actual SAR data. We analyze the\nadvantages of using the geodesic distance when compared to stochastic\ndistances.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 22:37:13 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Naranjo-Torres", "Jos\u00e9", ""], ["Gambini", "Juliana", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1701.00299", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Jia Deng", "title": "Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs\n  by Selective Execution", "comments": "fixed typos; updated CIFAR-10 results and added more details;\n  corrected the cascade D2NN configuration details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward\ndeep neural network that allows selective execution. Given an input, only a\nsubset of D2NN neurons are executed, and the particular subset is determined by\nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs\nprovide a way to improve computational efficiency. To achieve dynamic selective\nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic\ngraph of differentiable modules) with controller modules. Each controller\nmodule is a sub-network whose output is a decision that controls whether other\nmodules can execute. A D2NN is trained end to end. Both regular and controller\nmodules in a D2NN are learnable and are jointly trained to optimize both\naccuracy and efficiency. Such training is achieved by integrating\nbackpropagation with reinforcement learning. With extensive experiments of\nvarious D2NN architectures on image classification tasks, we demonstrate that\nD2NNs are general and flexible, and can effectively optimize\naccuracy-efficiency trade-offs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 00:09:14 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 02:22:10 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 02:03:00 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Liu", "Lanlan", ""], ["Deng", "Jia", ""]]}, {"id": "1701.00311", "submitter": "Yun Yang", "authors": "Yun Yang and Debdeep Pati", "title": "Bayesian model selection consistency and oracle inequality with\n  intractable marginal likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate large sample properties of model selection\nprocedures in a general Bayesian framework when a closed form expression of the\nmarginal likelihood function is not available or a local asymptotic quadratic\napproximation of the log-likelihood function does not exist. Under appropriate\nidentifiability assumptions on the true model, we provide sufficient conditions\nfor a Bayesian model selection procedure to be consistent and obey the Occam's\nrazor phenomenon, i.e., the probability of selecting the \"smallest\" model that\ncontains the truth tends to one as the sample size goes to infinity. In order\nto show that a Bayesian model selection procedure selects the smallest model\ncontaining the truth, we impose a prior anti-concentration condition, requiring\nthe prior mass assigned by large models to a neighborhood of the truth to be\nsufficiently small. In a more general setting where the strong model\nidentifiability assumption may not hold, we introduce the notion of local\nBayesian complexity and develop oracle inequalities for Bayesian model\nselection procedures. Our Bayesian oracle inequality characterizes a trade-off\nbetween the approximation error and a Bayesian characterization of the local\ncomplexity of the model, illustrating the adaptive nature of averaging-based\nBayesian procedures towards achieving an optimal rate of posterior convergence.\nSpecific applications of the model selection theory are discussed in the\ncontext of high-dimensional nonparametric regression and density regression\nwhere the regression function or the conditional density is assumed to depend\non a fixed subset of predictors. As a result of independent interest, we\npropose a general technique for obtaining upper bounds of certain small ball\nprobability of stationary Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 03:55:55 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 14:47:23 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Yang", "Yun", ""], ["Pati", "Debdeep", ""]]}, {"id": "1701.00322", "submitter": "Diogo R. Ferreira", "authors": "Francisco A. Matos, Diogo R. Ferreira, Pedro J. Carvalho, and JET\n  Contributors", "title": "Deep learning for plasma tomography using the bolometer system at JET", "comments": null, "journal-ref": "Fusion Engineering and Design, 114:18-25, January 2017", "doi": "10.1016/j.fusengdes.2016.11.006", "report-no": null, "categories": "stat.ML physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is having a profound impact in many fields, especially those\nthat involve some form of image processing. Deep neural networks excel in\nturning an input image into a set of high-level features. On the other hand,\ntomography deals with the inverse problem of recreating an image from a number\nof projections. In plasma diagnostics, tomography aims at reconstructing the\ncross-section of the plasma from radiation measurements. This reconstruction\ncan be computed with neural networks. However, previous attempts have focused\non learning a parametric model of the plasma profile. In this work, we use a\ndeep neural network to produce a full, pixel-by-pixel reconstruction of the\nplasma profile. For this purpose, we use the overview bolometer system at JET,\nand we introduce an up-convolutional network that has been trained and tested\non a large set of sample tomograms. We show that this network is able to\nreproduce existing reconstructions with a high level of accuracy, as measured\nby several metrics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 06:29:00 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Matos", "Francisco A.", ""], ["Ferreira", "Diogo R.", ""], ["Carvalho", "Pedro J.", ""], ["Contributors", "JET", ""]]}, {"id": "1701.00422", "submitter": "Nora Speicher", "authors": "Nora K. Speicher and Nico Pfeifer", "title": "Towards multiple kernel principal component analysis for integrative\n  analysis of tumor samples", "comments": "NIPS 2016 Workshop on Machine Learning for Health, Barcelona, Spain", "journal-ref": "Journal of Integrative Bioinformatics, 14(2), 2017", "doi": "10.1515/jib-2017-0019", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized treatment of patients based on tissue-specific cancer subtypes\nhas strongly increased the efficacy of the chosen therapies. Even though the\namount of data measured for cancer patients has increased over the last years,\nmost cancer subtypes are still diagnosed based on individual data sources (e.g.\ngene expression data). We propose an unsupervised data integration method based\non kernel principal component analysis. Principal component analysis is one of\nthe most widely used techniques in data analysis. Unfortunately, the\nstraight-forward multiple-kernel extension of this method leads to the use of\nonly one of the input matrices, which does not fit the goal of gaining\ninformation from all data sources. Therefore, we present a scoring function to\ndetermine the impact of each input matrix. The approach enables visualizing the\nintegrated data and subsequent clustering for cancer subtype identification.\nDue to the nature of the method, no free parameters have to be set. We apply\nthe methodology to five different cancer data sets and demonstrate its\nadvantages in terms of results and usability.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 15:37:46 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 08:30:38 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Speicher", "Nora K.", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1701.00481", "submitter": "Quanquan Gu", "authors": "Xiao Zhang and Lingxiao Wang and Quanquan Gu", "title": "Stochastic Variance-reduced Gradient Descent for Low-rank Matrix\n  Recovery from Linear Measurements", "comments": "24 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating low-rank matrices from linear measurements\n(a.k.a., matrix sensing) through nonconvex optimization. We propose an\nefficient stochastic variance reduced gradient descent algorithm to solve a\nnonconvex optimization problem of matrix sensing. Our algorithm is applicable\nto both noisy and noiseless settings. In the case with noisy observations, we\nprove that our algorithm converges to the unknown low-rank matrix at a linear\nrate up to the minimax optimal statistical error. And in the noiseless setting,\nour algorithm is guaranteed to linearly converge to the unknown low-rank matrix\nand achieves exact recovery with optimal sample complexity. Most notably, the\noverall computational complexity of our proposed algorithm, which is defined as\nthe iteration complexity times per iteration time complexity, is lower than the\nstate-of-the-art algorithms based on gradient descent. Experiments on synthetic\ndata corroborate the superiority of the proposed algorithm over the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 18:58:38 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 17:56:41 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Zhang", "Xiao", ""], ["Wang", "Lingxiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1701.00562", "submitter": "Shi-Xiong Zhang", "authors": "Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li and Yifan Gong", "title": "End-to-End Attention based Text-Dependent Speaker Verification", "comments": "@article{zhang2016End2End, title={End-to-End Attention based\n  Text-Dependent Speaker Verification}, author={Shi-Xiong Zhang, Zhuo\n  Chen$^{\\dag}$, Yong Zhao, Jinyu Li and Yifan Gong}, journal={IEEE Workshop on\n  Spoken Language Technology}, pages={171--178}, year={2016}, publisher={IEEE}\n  }", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of End-to-End system for text-dependent speaker verification is\npresented in this paper. Previously, using the phonetically\ndiscriminative/speaker discriminative DNNs as feature extractors for speaker\nverification has shown promising results. The extracted frame-level (DNN\nbottleneck, posterior or d-vector) features are equally weighted and aggregated\nto compute an utterance-level speaker representation (d-vector or i-vector). In\nthis work we use speaker discriminative CNNs to extract the noise-robust\nframe-level features. These features are smartly combined to form an\nutterance-level speaker vector through an attention mechanism. The proposed\nattention model takes the speaker discriminative information and the phonetic\ninformation to learn the weights. The whole system, including the CNN and\nattention model, is joint optimized using an end-to-end criterion. The training\nalgorithm imitates exactly the evaluation process --- directly mapping a test\nutterance and a few target speaker utterances into a single verification score.\nThe algorithm can automatically select the most similar impostor for each\ntarget speaker to train the network. We demonstrated the effectiveness of the\nproposed end-to-end system on Windows $10$ \"Hey Cortana\" speaker verification\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 01:15:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Zhang", "Shi-Xiong", ""], ["Chen", "Zhuo", ""], ["Zhao", "Yong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1701.00573", "submitter": "Gonzalo Otazu", "authors": "Gonzalo H Otazu", "title": "Robust method for finding sparse solutions to linear inverse problems\n  using an L2 regularization", "comments": "13 pages, 6 figures. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyzed the performance of a biologically inspired algorithm called the\nCorrected Projections Algorithm (CPA) when a sparseness constraint is required\nto unambiguously reconstruct an observed signal using atoms from an\novercomplete dictionary. By changing the geometry of the estimation problem,\nCPA gives an analytical expression for a binary variable that indicates the\npresence or absence of a dictionary atom using an L2 regularizer. The\nregularized solution can be implemented using an efficient real-time\nKalman-filter type of algorithm. The smoother L2 regularization of CPA makes it\nvery robust to noise, and CPA outperforms other methods in identifying known\natoms in the presence of strong novel atoms in the signal.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 03:31:03 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 05:38:58 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 22:19:57 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Otazu", "Gonzalo H", ""]]}, {"id": "1701.00652", "submitter": "David Gross", "authors": "Aditya Kela, Kai von Prillwitz, Johan Aberg, Rafael Chaves, David\n  Gross", "title": "Semidefinite tests for latent causal structures", "comments": "25 pages, 7 figures", "journal-ref": "IEEE Transactions on Information Theory 66, 339 (2019)", "doi": "10.1109/TIT.2019.2935755", "report-no": null, "categories": "stat.ML math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing whether a probability distribution is compatible with a given\nBayesian network is a fundamental task in the field of causal inference, where\nBayesian networks model causal relations. Here we consider the class of causal\nstructures where all correlations between observed quantities are solely due to\nthe influence from latent variables. We show that each model of this type\nimposes a certain signature on the observable covariance matrix in terms of a\nparticular decomposition into positive semidefinite components. This signature,\nand thus the underlying hypothetical latent structure, can be tested in a\ncomputationally efficient manner via semidefinite programming. This stands in\nstark contrast with the algebraic geometric tools required if the full\nobservable probability distribution is taken into account. The semidefinite\ntest is compared with tests based on entropic inequalities.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:12:18 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Kela", "Aditya", ""], ["von Prillwitz", "Kai", ""], ["Aberg", "Johan", ""], ["Chaves", "Rafael", ""], ["Gross", "David", ""]]}, {"id": "1701.00677", "submitter": "Ashkan Esmaeili", "authors": "Mohammad Amin Fakharian, Ashkan Esmaeili, and Farokh Marvasti", "title": "New Methods of Enhancing Prediction Accuracy in Linear Models with\n  Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, prediction for linear systems with missing information is\ninvestigated. New methods are introduced to improve the Mean Squared Error\n(MSE) on the test set in comparison to state-of-the-art methods, through\nappropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft\nWeighted Prediction (SWP) algorithm and its efficacy are depicted and compared\nto previous works for non-missing scenarios. The algorithm is then modified and\noptimized for missing scenarios. It is shown that controlled over-fitting by\nsuggested algorithms will improve prediction accuracy in various cases.\nSimulation results approve our heuristics in enhancing the prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 12:33:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Fakharian", "Mohammad Amin", ""], ["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1701.00757", "submitter": "Pedro Mercado", "authors": "Pedro Mercado, Francesco Tudisco and Matthias Hein", "title": "Clustering Signed Networks with the Geometric Mean of Laplacians", "comments": "14 pages, 5 figures. Accepted in Neural Information Processing\n  Systems (NIPS), 2016", "journal-ref": "Advances in Neural Information Processing Systems 29,\n  pp.4421--4429, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed networks allow to model positive and negative relationships. We\nanalyze existing extensions of spectral clustering to signed networks. It turns\nout that existing approaches do not recover the ground truth clustering in\nseveral situations where either the positive or the negative network structures\ncontain no noise. Our analysis shows that these problems arise as existing\napproaches take some form of arithmetic mean of the Laplacians of the positive\nand negative part. As a solution we propose to use the geometric mean of the\nLaplacians of positive and negative part and show that it outperforms the\nexisting approaches. While the geometric mean of matrices is computationally\nexpensive, we show that eigenvectors of the geometric mean can be computed\nefficiently, leading to a numerical scheme for sparse matrices which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 17:42:34 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Mercado", "Pedro", ""], ["Tudisco", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1701.00874", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "Neural Probabilistic Model for Non-projective MST Parsing", "comments": "To appear in IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:10:17 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 04:09:29 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 23:06:11 GMT"}, {"version": "v4", "created": "Sun, 3 Sep 2017 21:12:40 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1701.00903", "submitter": "Lakshmi Narasimhan Govindarajan", "authors": "Li Liu and Yongzhong Yang and Lakshmi Narasimhan Govindarajan and Shu\n  Wang and Bin Hu and Li Cheng and David S. Rosenblum", "title": "An Interval-Based Bayesian Generative Model for Human Complex Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex activity recognition is challenging due to the inherent uncertainty\nand diversity of performing a complex activity. Normally, each instance of a\ncomplex activity has its own configuration of atomic actions and their temporal\ndependencies. We propose in this paper an atomic action-based Bayesian model\nthat constructs Allen's interval relation networks to characterize complex\nactivities with structural varieties in a probabilistic generative way: By\nintroducing latent variables from the Chinese restaurant process, our approach\nis able to capture all possible styles of a particular complex activity as a\nunique set of distributions over atomic actions and relations. We also show\nthat local temporal dependencies can be retained and are globally consistent in\nthe resulting interval network. Moreover, network structure can be learned from\nempirical data. A new dataset of complex hand activities has been constructed\nand made publicly available, which is much larger in size than any existing\ndatasets. Empirical evaluations on benchmark datasets as well as our in-house\ndataset demonstrate the competitiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 05:53:46 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Liu", "Li", ""], ["Yang", "Yongzhong", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Wang", "Shu", ""], ["Hu", "Bin", ""], ["Cheng", "Li", ""], ["Rosenblum", "David S.", ""]]}, {"id": "1701.00939", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory is Robust to Adversarial Inputs", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 12, December 2018 p.3151-3167", "doi": "10.1162/neco_a_01143", "report-no": null, "categories": "cs.LG cs.CR cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 09:40:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1701.01037", "submitter": "Eric Lock", "authors": "Eric F. Lock", "title": "Tensor-on-tensor regression", "comments": "33 pages, 3 figures", "journal-ref": "Journal of Computational and Graphical Statistics 27 (3), 638-647,\n  2018", "doi": "10.1080/10618600.2017.1401544", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for the linear prediction of a multi-way array (i.e.,\na tensor) from another multi-way array of arbitrary dimension, using the\ncontracted tensor product. This framework generalizes several existing\napproaches, including methods to predict a scalar outcome from a tensor, a\nmatrix from a matrix, or a tensor from a scalar. We describe an approach that\nexploits the multiway structure of both the predictors and the outcomes by\nrestricting the coefficients to have reduced CP-rank. We propose a general and\nefficient algorithm for penalized least-squares estimation, which allows for a\nridge (L_2) penalty on the coefficients. The objective is shown to give the\nmode of a Bayesian posterior, which motivates a Gibbs sampling algorithm for\ninference. We illustrate the approach with an application to facial image data.\nAn R package is available at https://github.com/lockEF/MultiwayRegression .\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:59:35 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 21:09:45 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Lock", "Eric F.", ""]]}, {"id": "1701.01064", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas and C\\'edric Herzet", "title": "Optimal Low-Rank Dynamic Mode Decomposition", "comments": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASPP), New Orleans, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing\nthe dynamics of non-linear systems from experimental datasets. Recently,\nseveral attempts have extended DMD to the context of low-rank approximations.\nThis extension is of particular interest for reduced-order modeling in various\napplicative domains, e.g. for climate prediction, to study molecular dynamics\nor micro-electromechanical devices. This low-rank extension takes the form of a\nnon-convex optimization problem. To the best of our knowledge, only sub-optimal\nalgorithms have been proposed in the literature to compute the solution of this\nproblem. In this paper, we prove that there exists a closed-form optimal\nsolution to this problem and design an effective algorithm to compute it based\non Singular Value Decomposition (SVD). A toy-example illustrates the gain in\nperformance of the proposed algorithm compared to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 16:24:12 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 15:35:38 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 13:47:39 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1701.01093", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Kobbi Nissim, Hongxia Jin", "title": "Private Incremental Regression", "comments": "To appear in PODS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is continuously generated by modern data sources, and a recent challenge\nin machine learning has been to develop techniques that perform well in an\nincremental (streaming) setting. In this paper, we investigate the problem of\nprivate machine learning, where as common in practice, the data is not given at\nonce, but rather arrives incrementally over time.\n  We introduce the problems of private incremental ERM and private incremental\nregression where the general goal is to always maintain a good empirical risk\nminimizer for the history observed under differential privacy. Our first\ncontribution is a generic transformation of private batch ERM mechanisms into\nprivate incremental ERM mechanisms, based on a simple idea of invoking the\nprivate batch ERM procedure at some regular time intervals. We take this\nconstruction as a baseline for comparison. We then provide two mechanisms for\nthe private incremental regression problem. Our first mechanism is based on\nprivately constructing a noisy incremental gradient function, which is then\nused in a modified projected gradient procedure at every timestep. This\nmechanism has an excess empirical risk of $\\approx\\sqrt{d}$, where $d$ is the\ndimensionality of the data. While from the results of [Bassily et al. 2014]\nthis bound is tight in the worst-case, we show that certain geometric\nproperties of the input and constraint set can be used to derive significantly\nbetter results for certain interesting regression problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:18:07 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Nissim", "Kobbi", ""], ["Jin", "Hongxia", ""]]}, {"id": "1701.01095", "submitter": "Audrey Durand", "authors": "Audrey Durand, Christian Gagn\\'e", "title": "Estimating Quality in Multi-Objective Bandits Optimization", "comments": "Submitted to ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications are characterized by a number of conflicting\nperformance measures. As optimizing in a multi-objective setting leads to a set\nof non-dominated solutions, a preference function is required for selecting the\nsolution with the appropriate trade-off between the objectives. The question\nis: how good do estimations of these objectives have to be in order for the\nsolution maximizing the preference function to remain unchanged? In this paper,\nwe introduce the concept of preference radius to characterize the robustness of\nthe preference function and provide guidelines for controlling the quality of\nestimations in the multi-objective setting. More specifically, we provide a\ngeneral formulation of multi-objective optimization under the bandits setting.\nWe show how the preference radius relates to the optimal gap and we use this\nconcept to provide a theoretical analysis of the Thompson sampling algorithm\nfrom multivariate normal priors. We finally present experiments to support the\ntheoretical results and highlight the fact that one cannot simply scalarize\nmulti-objective problems into single-objective problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:20:47 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 21:23:44 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 20:37:39 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Durand", "Audrey", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1701.01096", "submitter": "Jianbo Ye", "authors": "Jianbo Ye, Jia Li, Michelle G. Newman, Reginald B. Adams Jr. and James\n  Z. Wang", "title": "Probabilistic Multigraph Modeling for Improving the Quality of\n  Crowdsourced Affective Data", "comments": "14 pages, 6 figures, 2 tables, meta data revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a probabilistic approach to joint modeling of participants'\nreliability and humans' regularity in crowdsourced affective studies.\nReliability measures how likely a subject will respond to a question seriously;\nand regularity measures how often a human will agree with other\nseriously-entered responses coming from a targeted population.\nCrowdsourcing-based studies or experiments, which rely on human self-reported\naffect, pose additional challenges as compared with typical crowdsourcing\nstudies that attempt to acquire concrete non-affective labels of objects. The\nreliability of participants has been massively pursued for typical\nnon-affective crowdsourcing studies, whereas the regularity of humans in an\naffective experiment in its own right has not been thoroughly considered. It\nhas been often observed that different individuals exhibit different feelings\non the same test question, which does not have a sole correct response in the\nfirst place. High reliability of responses from one individual thus cannot\nconclusively result in high consensus across individuals. Instead, globally\ntesting consensus of a population is of interest to investigators. Built upon\nthe agreement multigraph among tasks and workers, our probabilistic model\ndifferentiates subject regularity from population reliability. We demonstrate\nthe method's effectiveness for in-depth robust analysis of large-scale\ncrowdsourced affective data, including emotion and aesthetic assessments\ncollected by presenting visual stimuli to human subjects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:24:56 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 02:19:30 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Ye", "Jianbo", ""], ["Li", "Jia", ""], ["Newman", "Michelle G.", ""], ["Adams", "Reginald B.", "Jr."], ["Wang", "James Z.", ""]]}, {"id": "1701.01140", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich and Dean Eckles", "title": "Learning causal effects from many randomized experiments using\n  regularized instrumental variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific and business practices are increasingly resulting in large\ncollections of randomized experiments. Analyzed together, these collections can\ntell us things that individual experiments in the collection cannot. We study\nhow to learn causal relationships between variables from the kinds of\ncollections faced by modern data scientists: the number of experiments is\nlarge, many experiments have very small effects, and the analyst lacks metadata\n(e.g., descriptions of the interventions). Here we use experimental groups as\ninstrumental variables (IV) and show that a standard method (two-stage least\nsquares) is biased even when the number of experiments is infinite. We show how\na sparsity-inducing l0 regularization can --- in a reversal of the standard\nbias--variance tradeoff in regularization --- reduce bias (and thus error) of\ninterventional predictions. Because we are interested in interventional loss\nminimization we also propose a modified cross-validation procedure (IVCV) to\nfeasibly select the regularization parameter. We show, using a trick from Monte\nCarlo sampling, that IVCV can be done using summary statistics instead of raw\ndata. This makes our full procedure simple to use in many real-world\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 20:04:55 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 04:32:32 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 17:20:48 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Eckles", "Dean", ""]]}, {"id": "1701.01207", "submitter": "Yong Sheng Soh", "authors": "Yong Sheng Soh and Venkat Chandrasekaran", "title": "Learning Semidefinite Regularizers", "comments": "51 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization techniques are widely employed in optimization-based\napproaches for solving ill-posed inverse problems in data analysis and\nscientific computing. These methods are based on augmenting the objective with\na penalty function, which is specified based on prior domain-specific expertise\nto induce a desired structure in the solution. We consider the problem of\nlearning suitable regularization functions from data in settings in which\nprecise domain knowledge is not directly available. Previous work under the\ntitle of `dictionary learning' or `sparse coding' may be viewed as learning a\nregularization function that can be computed via linear programming. We\ndescribe generalizations of these methods to learn regularizers that can be\ncomputed and optimized via semidefinite programming. Our framework for learning\nsuch semidefinite regularizers is based on obtaining structured factorizations\nof data matrices, and our algorithmic approach for computing these\nfactorizations combines recent techniques for rank minimization problems along\nwith an operator analog of Sinkhorn scaling. Under suitable conditions on the\ninput data, our algorithm provides a locally linearly convergent method for\nidentifying the correct regularizer that promotes the type of structure\ncontained in the data. Our analysis is based on the stability properties of\nOperator Sinkhorn scaling and their relation to geometric aspects of\ndeterminantal varieties (in particular tangent spaces with respect to these\nvarieties). The regularizers obtained using our framework can be employed\neffectively in semidefinite programming relaxations for solving inverse\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 04:32:53 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 04:53:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Soh", "Yong Sheng", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1701.01231", "submitter": "Max Ren", "authors": "Max Yi Ren and Clayton Scott", "title": "Adaptive Questionnaires for Direct Identification of Optimal Product\n  Design", "comments": "submitted to Journal of Mechanical Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the most profitable product design\nfrom a finite set of candidates under unknown consumer preference. A standard\napproach to this problem follows a two-step strategy: First, estimate the\npreference of the consumer population, represented as a point in part-worth\nspace, using an adaptive discrete-choice questionnaire. Second, integrate the\nestimated part-worth vector with engineering feasibility and cost models to\ndetermine the optimal design. In this work, we (1) demonstrate that accurate\npreference estimation is neither necessary nor sufficient for identifying the\noptimal design, (2) introduce a novel adaptive questionnaire that leverages\nknowledge about engineering feasibility and manufacturing costs to directly\ndetermine the optimal design, and (3) interpret product design in terms of a\nnonlinear segmentation of part-worth space, and use this interpretation to\nilluminate the intrinsic difficulty of optimal design in the presence of noisy\nquestionnaire responses. We establish the superiority of the proposed approach\nusing a well-documented optimal product design task. This study demonstrates\nhow the identification of optimal product design can be accelerated by\nintegrating marketing and manufacturing knowledge into the adaptive\nquestionnaire.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 07:25:23 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Ren", "Max Yi", ""], ["Scott", "Clayton", ""]]}, {"id": "1701.01293", "submitter": "Giuseppe Casalicchio", "authors": "Giuseppe Casalicchio, Jakob Bossek, Michel Lang, Dominik Kirchhoff,\n  Pascal Kerschke, Benjamin Hofner, Heidi Seibold, Joaquin Vanschoren, Bernd\n  Bischl", "title": "OpenML: An R Package to Connect to the Machine Learning Platform OpenML", "comments": null, "journal-ref": "Computational Statistics, 2019, 34. Jg., Nr. 3, S. 977-991", "doi": "10.1007/s00180-017-0742-2", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenML is an online machine learning platform where researchers can easily\nshare data, machine learning tasks and experiments as well as organize them\nonline to work and collaborate more efficiently. In this paper, we present an R\npackage to interface with the OpenML platform and illustrate its usage in\ncombination with the machine learning R package mlr. We show how the OpenML\npackage allows R users to easily search, download and upload data sets and\nmachine learning tasks. Furthermore, we also show how to upload results of\nexperiments, share them with others and download results from other users.\nBeyond ensuring reproducibility of results, the OpenML platform automates much\nof the drudge work, speeds up research, facilitates collaboration and increases\nthe users' visibility online.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 12:33:19 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 07:03:28 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Casalicchio", "Giuseppe", ""], ["Bossek", "Jakob", ""], ["Lang", "Michel", ""], ["Kirchhoff", "Dominik", ""], ["Kerschke", "Pascal", ""], ["Hofner", "Benjamin", ""], ["Seibold", "Heidi", ""], ["Vanschoren", "Joaquin", ""], ["Bischl", "Bernd", ""]]}, {"id": "1701.01325", "submitter": "Ramakrishnan Kannan", "authors": "Ramakrishnan Kannan, Hyenkyun Woo, Charu C. Aggarwal, Haesun Park", "title": "Outlier Detection for Text Data : An Extended Version", "comments": "Accepted at 2017 SIAM Data Mining Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:14:52 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Kannan", "Ramakrishnan", ""], ["Woo", "Hyenkyun", ""], ["Aggarwal", "Charu C.", ""], ["Park", "Haesun", ""]]}, {"id": "1701.01329", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler, Thierry Kogej, Christian Tyrchan, Mark P. Waller", "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\n  Neural Networks", "comments": "17 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:28:34 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Segler", "Marwin H. S.", ""], ["Kogej", "Thierry", ""], ["Tyrchan", "Christian", ""], ["Waller", "Mark P.", ""]]}, {"id": "1701.01356", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Ond\\v{r}ej Straka", "title": "Gaussian Process Quadrature Moment Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of moments of transformed random variables is a problem appearing\nin many engineering applications. The current methods for moment transformation\nare mostly based on the classical quadrature rules which cannot account for the\napproximation errors. Our aim is to design a method for moment transformation\nfor Gaussian random variables which accounts for the error in the numerically\ncomputed mean. We employ an instance of Bayesian quadrature, called Gaussian\nprocess quadrature (GPQ), which allows us to treat the integral itself as a\nrandom variable, where the integral variance informs about the incurred\nintegration error. Experiments on the coordinate transformation and nonlinear\nfiltering examples show that the proposed GPQ moment transform performs better\nthan the classical transforms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 15:39:17 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Straka", "Ond\u0159ej", ""]]}, {"id": "1701.01394", "submitter": "Andrew Knyazev", "authors": "Andrew V. Knyazev", "title": "On spectral partitioning of signed graphs", "comments": "12 pages, 10 figures. Rev 2 to appear in proceedings of the SIAM\n  Workshop on Combinatorial Scientific Computing 2018 (CSC18)", "journal-ref": null, "doi": null, "report-no": "Rev. 1 MERL TR2017-001", "categories": "cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 17:31:16 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:51:22 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Knyazev", "Andrew V.", ""]]}, {"id": "1701.01437", "submitter": "Leila Wehbe", "authors": "Leila Wehbe, Anwar Nunez-Elizalde, Marcel van Gerven, Irina Rish,\n  Brian Murphy, Moritz Grosse-Wentrup, Georg Langs, Guillermo Cecchi", "title": "NIPS 2016 Workshop on Representation Learning in Artificial and\n  Biological Neural Networks (MLINI 2016)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This workshop explores the interface between cognitive neuroscience and\nrecent advances in AI fields that aim to reproduce human performance such as\nnatural language processing and computer vision, and specifically deep learning\napproaches to such problems.\n  When studying the cognitive capabilities of the brain, scientists follow a\nsystem identification approach in which they present different stimuli to the\nsubjects and try to model the response that different brain areas have of that\nstimulus. The goal is to understand the brain by trying to find the function\nthat expresses the activity of brain areas in terms of different properties of\nthe stimulus. Experimental stimuli are becoming increasingly complex with more\nand more people being interested in studying real life phenomena such as the\nperception of natural images or natural sentences. There is therefore a need\nfor a rich and adequate vector representation of the properties of the\nstimulus, that we can obtain using advances in machine learning.\n  In parallel, new ML approaches, many of which in deep learning, are inspired\nto a certain extent by human behavior or biological principles. Neural networks\nfor example were originally inspired by biological neurons. More recently,\nprocesses such as attention are being used which have are inspired by human\nbehavior. However, the large bulk of these methods are independent of findings\nabout brain function, and it is unclear whether it is at all beneficial for\nmachine learning to try to emulate brain function in order to achieve the same\ntasks that the brain achieves.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 14:58:34 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:22:30 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Wehbe", "Leila", ""], ["Nunez-Elizalde", "Anwar", ""], ["van Gerven", "Marcel", ""], ["Rish", "Irina", ""], ["Murphy", "Brian", ""], ["Grosse-Wentrup", "Moritz", ""], ["Langs", "Georg", ""], ["Cecchi", "Guillermo", ""]]}, {"id": "1701.01470", "submitter": "Sriram Somanchi", "authors": "Sriram Somanchi and Daniel B. Neill", "title": "Graph Structure Learning from Unlabeled Data for Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processes such as disease propagation and information diffusion often spread\nover some latent network structure which must be learned from observation.\nGiven a set of unlabeled training examples representing occurrences of an event\ntype of interest (e.g., a disease outbreak), our goal is to learn a graph\nstructure that can be used to accurately detect future events of that type.\nMotivated by new theoretical results on the consistency of constrained and\nunconstrained subset scans, we propose a novel framework for learning graph\nstructure from unlabeled data by comparing the most anomalous subsets detected\nwith and without the graph constraints. Our framework uses the mean normalized\nlog-likelihood ratio score to measure the quality of a graph structure, and\nefficiently searches for the highest-scoring graph structure. Using simulated\ndisease outbreaks injected into real-world Emergency Department data from\nAllegheny County, we show that our method learns a structure similar to the\ntrue underlying graph, but enables faster and more accurate detection.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 20:34:57 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Somanchi", "Sriram", ""], ["Neill", "Daniel B.", ""]]}, {"id": "1701.01582", "submitter": "Song Liu Dr.", "authors": "Song Liu, Kenji Fukumizu, Taiji Suzuki", "title": "Learning Sparse Structural Changes in High-dimensional Markov Networks:\n  A Review on Methodologies and Theories", "comments": "Fixed a few typos in Section 4.4: \\theta should be \\delta", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an increasing popularity of learning the sparse\n\\emph{changes} in Markov Networks. Changes in the structure of Markov Networks\nreflect alternations of interactions between random variables under different\nregimes and provide insights into the underlying system. While each individual\nnetwork structure can be complicated and difficult to learn, the overall change\nfrom one network to another can be simple. This intuition gave birth to an\napproach that \\emph{directly} learns the sparse changes without modelling and\nlearning the individual (possibly dense) networks. In this paper, we review\nsuch a direct learning method with some latest developments along this line of\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 09:30:22 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 10:02:02 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Liu", "Song", ""], ["Fukumizu", "Kenji", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1701.01672", "submitter": "Paul Fearnhead", "authors": "Robert Maidstone, Paul Fearnhead and Adam Letchford", "title": "Detecting changes in slope with an $L_0$ penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are many approaches to detecting changes in mean for a\nunivariate time-series, the problem of detecting multiple changes in slope has\ncomparatively been ignored. Part of the reason for this is that detecting\nchanges in slope is much more challenging. For example, simple binary\nsegmentation procedures do not work for this problem, whilst efficient dynamic\nprogramming methods that work well for the change in mean problem cannot be\ndirectly used for detecting changes in slope. We present a novel dynamic\nprogramming approach, CPOP, for finding the \"best\" continuous piecewise-linear\nfit to data. We define best based on a criterion that measures fit to data\nusing the residual sum of squares, but penalises complexity based on an $L_0$\npenalty on changes in slope. We show that using such a criterion is more\nreliable at estimating changepoint locations than approaches that penalise\ncomplexity using an $L_1$ penalty. Empirically CPOP has good computational\nproperties, and can analyse a time-series with over 10,000 observations and\nover 100 changes in a few minutes. Our method is used to analyse data on the\nmotion of bacteria, and provides fits to the data that both have substantially\nsmaller residual sum of squares and are more parsimonious than two competing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 15:52:45 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 10:26:34 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Maidstone", "Robert", ""], ["Fearnhead", "Paul", ""], ["Letchford", "Adam", ""]]}, {"id": "1701.01722", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 18:43:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 18:18:21 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 01:04:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1701.01772", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou and Nesreen K. Ahmed", "title": "Estimation of Graphlet Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphlets are induced subgraphs of a large network and are important for\nunderstanding and modeling complex networks. Despite their practical\nimportance, graphlets have been severely limited to applications and domains\nwith relatively small graphs. Most previous work has focused on exact\nalgorithms, however, it is often too expensive to compute graphlets exactly in\nmassive networks with billions of edges, and finding an approximate count is\nusually sufficient for many applications. In this work, we propose an unbiased\ngraphlet estimation framework that is (a) fast with significant speedups\ncompared to the state-of-the-art, (b) parallel with nearly linear-speedups, (c)\naccurate with <1% relative error, (d) scalable and space-efficient for massive\nnetworks with billions of edges, and (e) flexible for a variety of real-world\nsettings, as well as estimating macro and micro-level graphlet statistics\n(e.g., counts) of both connected and disconnected graphlets. In addition, an\nadaptive approach is introduced that finds the smallest sample size required to\nobtain estimates within a given user-defined error bound. On 300 networks from\n20 domains, we obtain <1% relative error for all graphlets. This is\nsignificantly more accurate than existing methods while using less data.\nMoreover, it takes a few seconds on billion edge graphs (as opposed to\ndays/weeks). These are by far the largest graphlet computations to date.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 22:37:59 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 16:18:56 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1701.02046", "submitter": "Ping Li", "authors": "Ping Li", "title": "Tunable GMM Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed \"generalized min-max\" (GMM) kernel can be efficiently\nlinearized, with direct applications in large-scale statistical learning and\nfast near neighbor search. The linearized GMM kernel was extensively compared\nin with linearized radial basis function (RBF) kernel. On a large number of\nclassification tasks, the tuning-free GMM kernel performs (surprisingly) well\ncompared to the best-tuned RBF kernel. Nevertheless, one would naturally expect\nthat the GMM kernel ought to be further improved if we introduce tuning\nparameters.\n  In this paper, we study three simple constructions of tunable GMM kernels:\n(i) the exponentiated-GMM (or eGMM) kernel, (ii) the powered-GMM (or pGMM)\nkernel, and (iii) the exponentiated-powered-GMM (epGMM) kernel. The pGMM kernel\ncan still be efficiently linearized by modifying the original hashing procedure\nfor the GMM kernel. On about 60 publicly available classification datasets, we\nverify that the proposed tunable GMM kernels typically improve over the\noriginal GMM kernel. On some datasets, the improvements can be astonishingly\nsignificant.\n  For example, on 11 popular datasets which were used for testing deep learning\nalgorithms and tree methods, our experiments show that the proposed tunable GMM\nkernels are strong competitors to trees and deep nets. The previous studies\ndeveloped tree methods including \"abc-robust-logitboost\" and demonstrated the\nexcellent performance on those 11 datasets (and other datasets), by\nestablishing the second-order tree-split formula and new derivatives for\nmulti-class logistic loss. Compared to tree methods like\n\"abc-robust-logitboost\" (which are slow and need substantial model sizes), the\ntunable GMM kernels produce largely comparable results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 01:20:55 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 17:25:16 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1701.02058", "submitter": "Mehmet Basbug", "authors": "Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Coupled Compound Poisson Factorization", "comments": "Under review at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a general framework, the coupled compound Poisson factorization\n(CCPF), to capture the missing-data mechanism in extremely sparse data sets by\ncoupling a hierarchical Poisson factorization with an arbitrary data-generating\nmodel. We derive a stochastic variational inference algorithm for the resulting\nmodel and, as examples of our framework, implement three different\ndata-generating models---a mixture model, linear regression, and factor\nanalysis---to robustly model non-random missing data in the context of\nclustering, prediction, and matrix factorization. In all three cases, we test\nour framework against models that ignore the missing-data mechanism on large\nscale studies with non-random missing data, and we show that explicitly\nmodeling the missing-data mechanism substantially improves the quality of the\nresults, as measured using data log likelihood on a held-out test set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 03:49:26 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1701.02071", "submitter": "Valeriy Kalyagin", "authors": "Valery A. Kalyagin, Alexander P. Koldanov, Petr A. Koldanov, Panos M.\n  Pardalos", "title": "Optimal statistical decision for Gaussian graphical model selection", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical model is a graphical representation of the dependence\nstructure for a Gaussian random vector. It is recognized as a powerful tool in\ndifferent applied fields such as bioinformatics, error-control codes, speech\nlanguage, information retrieval and others. Gaussian graphical model selection\nis a statistical problem to identify the Gaussian graphical model from a sample\nof a given size. Different approaches for Gaussian graphical model selection\nare suggested in the literature. One of them is based on considering the family\nof individual conditional independence tests. The application of this approach\nleads to the construction of a variety of multiple testing statistical\nprocedures for Gaussian graphical model selection. An important characteristic\nof these procedures is its error rate for a given sample size. In existing\nliterature great attention is paid to the control of error rates for incorrect\nedge inclusion (Type I error). However, in graphical model selection it is also\nimportant to take into account error rates for incorrect edge exclusion (Type\nII error). To deal with this issue we consider the graphical model selection\nproblem in the framework of the multiple decision theory. The quality of\nstatistical procedures is measured by a risk function with additive losses.\nAdditive losses allow both types of errors to be taken into account. We\nconstruct the tests of a Neyman structure for individual hypotheses and combine\nthem to obtain a multiple decision statistical procedure. We show that the\nobtained procedure is optimal in the sense that it minimizes the linear\ncombination of expected numbers of Type I and Type II errors in the class of\nunbiased multiple decision procedures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 06:34:00 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kalyagin", "Valery A.", ""], ["Koldanov", "Alexander P.", ""], ["Koldanov", "Petr A.", ""], ["Pardalos", "Panos M.", ""]]}, {"id": "1701.02110", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn and Achim Zeileis", "title": "Transformation Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regression models for supervised learning problems with a continuous target\nare commonly understood as models for the conditional mean of the target given\npredictors. This notion is simple and therefore appealing for interpretation\nand visualisation. Information about the whole underlying conditional\ndistribution is, however, not available from these models. A more general\nunderstanding of regression models as models for conditional distributions\nallows much broader inference from such models, for example the computation of\nprediction intervals. Several random forest-type algorithms aim at estimating\nconditional distributions, most prominently quantile regression forests\n(Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric\nfamily of distributions characterised by their transformation function. A\ndedicated novel \"transformation tree\" algorithm able to detect distributional\nchanges is developed. Based on these transformation trees, we introduce\n\"transformation forests\" as an adaptive local likelihood estimator of\nconditional distribution functions. The resulting models are fully parametric\nyet very general and allow broad inference procedures, such as the model-based\nbootstrap, to be applied in a straightforward way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 09:52:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 10:08:16 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hothorn", "Torsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1701.02133", "submitter": "Michele Svanera", "authors": "Michele Svanera, Sergio Benini, Gal Raz, Talma Hendler, Rainer Goebel,\n  and Giancarlo Valente", "title": "Deep driven fMRI decoding of visual categories", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/01", "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been developed drawing inspiration from the brain\nvisual pathway, implementing an end-to-end approach: from image data to video\nobject classes. However building an fMRI decoder with the typical structure of\nConvolutional Neural Network (CNN), i.e. learning multiple level of\nrepresentations, seems impractical due to lack of brain data. As a possible\nsolution, this work presents the first hybrid fMRI and deep features decoding\napproach: collected fMRI and deep learnt representations of video object\nclasses are linked together by means of Kernel Canonical Correlation Analysis.\nIn decoding, this allows exploiting the discriminatory power of CNN by relating\nthe fMRI representation to the last layer of CNN (fc7). We show the\neffectiveness of embedding fMRI data onto a subspace related to deep features\nin distinguishing semantic visual categories based solely on brain imaging\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:06:39 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Svanera", "Michele", ""], ["Benini", "Sergio", ""], ["Raz", "Gal", ""], ["Hendler", "Talma", ""], ["Goebel", "Rainer", ""], ["Valente", "Giancarlo", ""]]}, {"id": "1701.02265", "submitter": "Xingye Qiao", "authors": "Chong Zhang, Wenbo Wang, and Xingye Qiao", "title": "On Reject and Refine Options in Multicategory Classification", "comments": "A revised version of this paper was accepted for publication in the\n  Journal of the American Statistical Association Theory and Methods Section.\n  52 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real applications of statistical learning, a decision made from\nmisclassification can be too costly to afford; in this case, a reject option,\nwhich defers the decision until further investigation is conducted, is often\npreferred. In recent years, there has been much development for binary\nclassification with a reject option. Yet, little progress has been made for the\nmulticategory case. In this article, we propose margin-based multicategory\nclassification methods with a reject option. In addition, and more importantly,\nwe introduce a new and unique refine option for the multicategory problem,\nwhere the class of an observation is predicted to be from a set of class\nlabels, whose cardinality is not necessarily one. The main advantage of both\noptions lies in their capacity of identifying error-prone observations.\nMoreover, the refine option can provide more constructive information for\nclassification by effectively ruling out implausible classes. Efficient\nimplementations have been developed for the proposed methods. On the\ntheoretical side, we offer a novel statistical learning theory and show a fast\nconvergence rate of the excess $\\ell$-risk of our methods with emphasis on\ndiverging dimensionality and number of classes. The results can be further\nimproved under a low noise assumption. A set of comprehensive simulation and\nreal data studies has shown the usefulness of the new learning tools compared\nto regular multicategory classifiers. Detailed proofs of theorems and extended\nnumerical results are included in the supplemental materials available online.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 17:19:45 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhang", "Chong", ""], ["Wang", "Wenbo", ""], ["Qiao", "Xingye", ""]]}, {"id": "1701.02291", "submitter": "Tapabrata Ghosh", "authors": "Tapabrata Ghosh", "title": "QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures", "comments": "Updated once", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QuickNet, a fast and accurate network architecture that is both\nfaster and significantly more accurate than other fast deep architectures like\nSqueezeNet. Furthermore, it uses less parameters than previous networks, making\nit more memory efficient. We do this by making two major modifications to the\nreference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable\nconvolutions and 2) The use of parametric rectified linear units. We make the\nobservation that parametric rectified linear units are computationally\nequivalent to leaky rectified linear units at test time and the observation\nthat separable convolutions can be interpreted as a compressed Inception\nnetwork (Chollet, 2016). Using these observations, we derive a network\narchitecture, which we call QuickNet, that is both faster and more accurate\nthan previous models. Our architecture provides at least four major advantages:\n(1) A smaller model size, which is more tenable on memory constrained systems;\n(2) A significantly faster network which is more tenable on computationally\nconstrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10\nDataset which outperforms all but one result published so far, although we note\nthat our works are orthogonal approaches and can be combined (4) Orthogonality\nto previous model compression approaches allowing for further speed gains to be\nrealized.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:29:07 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 07:44:17 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ghosh", "Tapabrata", ""]]}, {"id": "1701.02301", "submitter": "Quanquan Gu", "authors": "Lingxiao Wang and Xiao Zhang and Quanquan Gu", "title": "A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank\n  Matrix Recovery", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic framework based on a new stochastic variance-reduced\ngradient descent algorithm for accelerating nonconvex low-rank matrix recovery.\nStarting from an appropriate initial estimator, our proposed algorithm performs\nprojected gradient descent based on a novel semi-stochastic gradient\nspecifically designed for low-rank matrix recovery. Based upon the mild\nrestricted strong convexity and smoothness conditions, we derive a projected\nnotion of the restricted Lipschitz continuous gradient property, and prove that\nour algorithm enjoys linear convergence rate to the unknown low-rank matrix\nwith an improved computational complexity. Moreover, our algorithm can be\nemployed to both noiseless and noisy observations, where the optimal sample\ncomplexity and the minimax optimal statistical rate can be attained\nrespectively. We further illustrate the superiority of our generic framework\nthrough several specific examples, both theoretically and experimentally.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:56:56 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 16:48:19 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Wang", "Lingxiao", ""], ["Zhang", "Xiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1701.02343", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Erdem Yoruk, Rene Vidal, Laurent Younes, Donald Geman", "title": "Information Pursuit: A Bayesian Framework for Sequential Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite enormous progress in object detection and classification, the problem\nof incorporating expected contextual relationships among object instances into\nmodern recognition systems remains a key challenge. In this work we propose\nInformation Pursuit, a Bayesian framework for scene parsing that combines prior\nmodels for the geometry of the scene and the spatial arrangement of objects\ninstances with a data model for the output of high-level image classifiers\ntrained to answer specific questions about the scene. In the proposed\nframework, the scene interpretation is progressively refined as evidence\naccumulates from the answers to a sequence of questions. At each step, we\nchoose the question to maximize the mutual information between the new answer\nand the full interpretation given the current evidence obtained from previous\ninquiries. We also propose a method for learning the parameters of the model\nfrom synthesized, annotated scenes obtained by top-down sampling from an\neasy-to-learn generative scene model. Finally, we introduce a database of\nannotated indoor scenes of dining room tables, which we use to evaluate the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 20:39:12 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yoruk", "Erdem", ""], ["Vidal", "Rene", ""], ["Younes", "Laurent", ""], ["Geman", "Donald", ""]]}, {"id": "1701.02349", "submitter": "Benjamin Brown", "authors": "Benjamin Brown, Timothy Weaver, Julian Wolfson", "title": "MEBoost: Variable Selection in the Presence of Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for variable selection in regression models when\ncovariates are measured with error. The iterative algorithm we propose,\nMEBoost, follows a path defined by estimating equations that correct for\ncovariate measurement error. Via simulation, we evaluated our method and\ncompare its performance to the recently-proposed Convex Conditioned Lasso\n(CoCoLasso) and to the \"naive\" Lasso which does not correct for measurement\nerror. Increasing the degree of measurement error increased prediction error\nand decreased the probability of accurate covariate selection, but this loss of\naccuracy was least pronounced when using MEBoost. We illustrate the use of\nMEBoost in practice by analyzing data from the Box Lunch Study, a clinical\ntrial in nutrition where several variables are based on self-report and hence\nmeasured with error.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:00:46 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 03:34:06 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 13:41:25 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Brown", "Benjamin", ""], ["Weaver", "Timothy", ""], ["Wolfson", "Julian", ""]]}, {"id": "1701.02359", "submitter": "Tapio Pahikkala", "authors": "Markus Viljanen, Antti Airola, Jukka Heikkonen, Tapio Pahikkala", "title": "Playtime Measurement with Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing product use is a central goal of many businesses, which makes\nretention and monetization two central analytics metrics in games. Player\nretention may refer to various duration variables quantifying product use:\ntotal playtime or session playtime are popular research targets, and active\nplaytime is well-suited for subscription games. Such research often has the\ngoal of increasing player retention or conversely decreasing player churn.\nSurvival analysis is a framework of powerful tools well suited for retention\ntype data. This paper contributes new methods to game analytics on how playtime\ncan be analyzed using survival analysis without covariates. Survival and hazard\nestimates provide both a visual and an analytic interpretation of the playtime\nphenomena as a funnel type nonparametric estimate. Metrics based on the\nsurvival curve can be used to aggregate this playtime information into a single\nstatistic. Comparison of survival curves between cohorts provides a scientific\nAB-test. All these methods work on censored data and enable computation of\nconfidence intervals. This is especially important in time and sample limited\ndata which occurs during game development. Throughout this paper, we illustrate\nthe application of these methods to real world game development problems on the\nHipster Sheep mobile game.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 10:25:04 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Viljanen", "Markus", ""], ["Airola", "Antti", ""], ["Heikkonen", "Jukka", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1701.02386", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann\n  Simon-Gabriel and Bernhard Sch\\\"olkopf", "title": "AdaGAN: Boosting Generative Models", "comments": "Updated with MNIST pictures and discussions + Unrolled GAN\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an\neffective method for training generative models of complex data such as natural\nimages. However, they are notoriously hard to train and can suffer from the\nproblem of missing modes where the model is not able to produce examples in\ncertain regions of the space. We propose an iterative procedure, called AdaGAN,\nwhere at every step we add a new component into a mixture model by running a\nGAN algorithm on a reweighted sample. This is inspired by boosting algorithms,\nwhere many potentially weak individual predictors are greedily aggregated to\nform a strong composite predictor. We prove that such an incremental procedure\nleads to convergence to the true distribution in a finite number of steps if\neach step is optimal, and convergence at an exponential rate otherwise. We also\nillustrate experimentally that this procedure addresses the problem of missing\nmodes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 23:19:28 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 11:45:00 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Gelly", "Sylvain", ""], ["Bousquet", "Olivier", ""], ["Simon-Gabriel", "Carl-Johann", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1701.02440", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Em. Karniadakis", "title": "Machine Learning of Linear Differential Equations using Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.07.050", "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work leverages recent advances in probabilistic machine learning to\ndiscover conservation laws expressed by parametric linear equations. Such\nequations involve, but are not limited to, ordinary and partial differential,\nintegro-differential, and fractional order operators. Here, Gaussian process\npriors are modified according to the particular form of such operators and are\nemployed to infer parameters of the linear equations from scarce and possibly\nnoisy observations. Such observations may come from experiments or \"black-box\"\ncomputer simulations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 05:14:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George Em.", ""]]}, {"id": "1701.02511", "submitter": "Feng Liu", "authors": "Feng Liu, Guanquan Zhang, Jie Lu", "title": "Heterogeneous domain adaptation: An unsupervised approach", "comments": "This paper has been accepted by IEEE transactions on neural networks\n  and learning systems (TNNLS)", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2973293", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation leverages the knowledge in one domain - the source domain -\nto improve learning efficiency in another domain - the target domain. Existing\nheterogeneous domain adaptation research is relatively well-progressed, but\nonly in situations where the target domain contains at least a few labeled\ninstances. In contrast, heterogeneous domain adaptation with an unlabeled\ntarget domain has not been well-studied. To contribute to the research in this\nemerging field, this paper presents: (1) an unsupervised knowledge transfer\ntheorem that guarantees the correctness of transferring knowledge; and (2) a\nprincipal angle-based metric to measure the distance between two pairs of\ndomains: one pair comprises the original source and target domains and the\nother pair comprises two homogeneous representations of two domains. The\ntheorem and the metric have been implemented in an innovative transfer model,\ncalled a Grassmann-Linear monotonic maps-geodesic flow kernel (GLG), that is\nspecifically designed for heterogeneous unsupervised domain adaptation (HeUDA).\nThe linear monotonic maps meet the conditions of the theorem and are used to\nconstruct homogeneous representations of the heterogeneous domains. The metric\nshows the extent to which the homogeneous representations have preserved the\ninformation in the original source and target domains. By minimizing the\nproposed metric, the GLG model learns the homogeneous representations of\nheterogeneous domains and transfers knowledge through these learned\nrepresentations via a geodesic flow kernel. To evaluate the model, five public\ndatasets were reorganized into ten HeUDA tasks across three applications:\ncancer detection, credit assessment, and text classification. The experiments\ndemonstrate that the proposed model delivers superior performance over the\nexisting baselines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 10:42:25 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:22:17 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 23:04:57 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2018 23:54:18 GMT"}, {"version": "v5", "created": "Mon, 10 Feb 2020 01:31:57 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Feng", ""], ["Zhang", "Guanquan", ""], ["Lu", "Jie", ""]]}, {"id": "1701.02720", "submitter": "Mohammad Pezeshki", "authors": "Ying Zhang, Mohammad Pezeshki, Philemon Brakel, Saizheng Zhang, Cesar\n  Laurent Yoshua Bengio, Aaron Courville", "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models/Gaussian Mixture Models\n(HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 18:30:11 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Zhang", "Ying", ""], ["Pezeshki", "Mohammad", ""], ["Brakel", "Philemon", ""], ["Zhang", "Saizheng", ""], ["Bengio", "Cesar Laurent Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1701.02776", "submitter": "Ravi Kiran Raman", "authors": "Ravi Kiran Raman and Lav R. Varshney", "title": "Universal Joint Image Clustering and Registration using Partition\n  Information", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of universal joint clustering and registration of\nimages and define algorithms using multivariate information functionals. We\nfirst study registering two images using maximum mutual information and prove\nits asymptotic optimality. We then show the shortcomings of pairwise\nregistration in multi-image registration, and design an asymptotically optimal\nalgorithm based on multiinformation. Further, we define a novel multivariate\ninformation functional to perform joint clustering and registration of images,\nand prove consistency of the algorithm. Finally, we consider registration and\nclustering of numerous limited-resolution images, defining algorithms that are\norder-optimal in scaling of number of pixels in each image with the number of\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 20:20:24 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 20:18:10 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Raman", "Ravi Kiran", ""], ["Varshney", "Lav R.", ""]]}, {"id": "1701.02789", "submitter": "Rajat Sen", "authors": "Rajat Sen, Karthikeyan Shanmugam, Alexandros G. Dimakis, and Sanjay\n  Shakkottai", "title": "Identifying Best Interventions through Online Importance Sampling", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in computational advertising and systems biology,\nwe consider the problem of identifying the best out of several possible soft\ninterventions at a source node $V$ in an acyclic causal directed graph, to\nmaximize the expected value of a target node $Y$ (located downstream of $V$).\nOur setting imposes a fixed total budget for sampling under various\ninterventions, along with cost constraints on different types of interventions.\nWe pose this as a best arm identification bandit problem with $K$ arms where\neach arm is a soft intervention at $V,$ and leverage the information leakage\namong the arms to provide the first gap dependent error and simple regret\nbounds for this problem. Our results are a significant improvement over the\ntraditional best arm identification results. We empirically show that our\nalgorithms outperform the state of the art in the Flow Cytometry data-set, and\nalso apply our algorithm for model interpretation of the Inception-v3 deep net\nthat classifies images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 21:26:03 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:19:33 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 22:50:38 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Dimakis", "Alexandros G.", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1701.02804", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Stephen Kelley, Brandon Oselio, Alfred O. Hero\n  III", "title": "Similarity Function Tracking using Pairwise Comparisons", "comments": "submitted to IEEE transactions on signal processing. arXiv admin\n  note: substantial text overlap with arXiv:1610.03090, arXiv:1603.03678", "journal-ref": null, "doi": "10.1109/TSP.2017.2739100", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, and demonstrate\nparameter-free RICE-OCELAD metric learning on both synthetic data and a highly\nnonstationary Twitter dataset. We show significant performance improvements and\nincreased robustness to nonstationary effects relative to previously proposed\nbatch and online distance metric learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 03:44:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Kelley", "Stephen", ""], ["Oselio", "Brandon", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1701.02815", "submitter": "Bo Dai", "authors": "Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song", "title": "Stochastic Generative Hashing", "comments": "21 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 00:23:34 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 21:36:09 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dai", "Bo", ""], ["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["He", "Niao", ""], ["Song", "Le", ""]]}, {"id": "1701.02856", "submitter": "Tracy Holsclaw", "authors": "Tracy Holsclaw, Arthur M. Greene, Andrew W. Robertson, Padhraic Smyth", "title": "Bayesian Non-Homogeneous Markov Models via Polya-Gamma Data Augmentation\n  with Applications to Rainfall Modeling", "comments": "40 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time hidden Markov models are a broadly useful class of\nlatent-variable models with applications in areas such as speech recognition,\nbioinformatics, and climate data analysis. It is common in practice to\nintroduce temporal non-homogeneity into such models by making the transition\nprobabilities dependent on time-varying exogenous input variables via a\nmultinomial logistic parametrization. We extend such models to introduce\nadditional non-homogeneity into the emission distribution using a generalized\nlinear model (GLM), with data augmentation for sampling-based inference.\nHowever, the presence of the logistic function in the state transition model\nsignificantly complicates parameter inference for the overall model,\nparticularly in a Bayesian context. To address this we extend the\nrecently-proposed Polya-Gamma data augmentation approach to handle\nnon-homogeneous hidden Markov models (NHMMs), allowing the development of an\nefficient Markov chain Monte Carlo (MCMC) sampling scheme. We apply our model\nand inference scheme to 30 years of daily rainfall in India, leading to a\nnumber of insights into rainfall-related phenomena in the region. Our proposed\napproach allows for fully Bayesian analysis of relatively complex NHMMs on a\nscale that was not possible with previous methods. Software implementing the\nmethods described in the paper is available via the R package NHMM.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 06:07:55 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 02:47:42 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Holsclaw", "Tracy", ""], ["Greene", "Arthur M.", ""], ["Robertson", "Andrew W.", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1701.02892", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Chi Xu and Yu Zhang and Tingshao Zhu and Li Cheng", "title": "Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:52:53 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Xu", "Chi", ""], ["Zhang", "Yu", ""], ["Zhu", "Tingshao", ""], ["Cheng", "Li", ""]]}, {"id": "1701.02960", "submitter": "Johan Jonasson", "authors": "Johan Jonasson", "title": "Fast mixing for Latent Dirichlet allocation", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability\ntheory in general and in machine learning in particular. A Markov chain is\ndevised so that its stationary distribution is some probability distribution of\ninterest. Then one samples from the given distribution by running the Markov\nchain for a \"long time\" until it appears to be stationary and then collects the\nsample. However these chains are often very complex and there are no\ntheoretical guarantees that stationarity is actually reached. In this paper we\nstudy the Gibbs sampler of the posterior distribution of a very simple case of\nLatent Dirichlet Allocation, the arguably most well known Bayesian unsupervised\nlearning model for text generation and text classification. It is shown that\nwhen the corpus consists of two long documents of equal length $m$ and the\nvocabulary consists of only two different words, the mixing time is at most of\norder $m^2\\log m$ (which corresponds to $m\\log m$ rounds over the corpus). It\nwill be apparent from our analysis that it seems very likely that the mixing\ntime is not much worse in the more relevant case when the number of documents\nand the size of the vocabulary are also large as long as each word is\nrepresented a large number in each document, even though the computations\ninvolved may be intractable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:08:52 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 14:01:34 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Jonasson", "Johan", ""]]}, {"id": "1701.02967", "submitter": "Zhenyu Liao", "authors": "Zhenyu Liao, Romain Couillet", "title": "A Large Dimensional Analysis of Least Squares Support Vector Machines", "comments": "14 pages, 13 figures, 1 table, partially presented at ICASSP 2017", "journal-ref": "IEEE Transactions on Signal Processing (Volume: 67, Issue: 4,\n  Feb.15 2019)", "doi": "10.1109/TSP.2018.2889954", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a large dimensional performance analysis of kernel least\nsquares support vector machines (LS-SVMs) is provided under the assumption of a\ntwo-class Gaussian mixture model for the input data. Building upon recent\nadvances in random matrix theory, we show, when the dimension of data $p$ and\ntheir number $n$ are both large, that the LS-SVM decision function can be well\napproximated by a normally distributed random variable, the mean and variance\nof which depend explicitly on a local behavior of the kernel function. This\ntheoretical result is then applied to the MNIST and Fashion-MNIST datasets\nwhich, despite their non-Gaussianity, exhibit a convincingly close behavior.\nMost importantly, our analysis provides a deeper understanding of the mechanism\ninto play in SVM-type methods and in particular of the impact on the choice of\nthe kernel function as well as some of their theoretical limits in separating\nhigh dimensional Gaussian vectors.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:36:34 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 19:54:25 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liao", "Zhenyu", ""], ["Couillet", "Romain", ""]]}, {"id": "1701.03006", "submitter": "Xin Yuan", "authors": "Xin Yuan, Yunchen Pu, Lawrence Carin", "title": "Compressive Sensing via Convolutional Factor Analysis", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the compressive sensing problem via convolutional factor analysis,\nwhere the convolutional dictionaries are learned {\\em in situ} from the\ncompressed measurements. An alternating direction method of multipliers (ADMM)\nparadigm for compressive sensing inversion based on convolutional factor\nanalysis is developed. The proposed algorithm provides reconstructed images as\nwell as features, which can be directly used for recognition ($e.g.$,\nclassification) tasks. When a deep (multilayer) model is constructed, a\nstochastic unpooling process is employed to build a generative model. During\nreconstruction and testing, we project the upper layer dictionary to the data\nlevel and only a single layer deconvolution is required. We demonstrate that\nusing $\\sim30\\%$ (relative to pixel numbers) compressed measurements, the\nproposed model achieves the classification accuracy comparable to the original\ndata on MNIST. We also observe that when the compressed measurements are very\nlimited ($e.g.$, $<10\\%$), the upper layer dictionary can provide better\nreconstruction results than the bottom layer.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 15:18:18 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Yuan", "Xin", ""], ["Pu", "Yunchen", ""], ["Carin", "Lawrence", ""]]}, {"id": "1701.03041", "submitter": "Matthew Veres", "authors": "Matthew Veres, Medhat Moussa, Graham W. Taylor", "title": "Modeling Grasp Motor Imagery through Deep Conditional Generative Models", "comments": "Accepted for publication in Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping is a complex process involving knowledge of the object, the\nsurroundings, and of oneself. While humans are able to integrate and process\nall of the sensory information required for performing this task, equipping\nmachines with this capability is an extremely challenging endeavor. In this\npaper, we investigate how deep learning techniques can allow us to translate\nhigh-level concepts such as motor imagery to the problem of robotic grasp\nsynthesis. We explore a paradigm based on generative models for learning\nintegrated object-action representations, and demonstrate its capacity for\ncapturing and generating multimodal, multi-finger grasp configurations on a\nsimulated grasping dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:20:39 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Veres", "Matthew", ""], ["Moussa", "Medhat", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1701.03077", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A General and Adaptive Robust Loss Function", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the Cauchy/Lorentzian, Geman-McClure,\nWelsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2\nloss functions. By introducing robustness as a continuous parameter, our loss\nfunction allows algorithms built around robust loss minimization to be\ngeneralized, which improves performance on basic vision tasks such as\nregistration and clustering. Interpreting our loss as the negative log of a\nunivariate density yields a general probability distribution that includes\nnormal and Cauchy distributions as special cases. This probabilistic\ninterpretation enables the training of neural networks in which the robustness\nof the loss automatically adapts itself during training, which improves\nperformance on learning-based tasks such as generative image synthesis and\nunsupervised monocular depth estimation, without requiring any manual parameter\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:39:14 GMT"}, {"version": "v10", "created": "Thu, 4 Apr 2019 20:05:33 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 21:16:44 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 18:11:17 GMT"}, {"version": "v4", "created": "Fri, 26 Jan 2018 23:48:44 GMT"}, {"version": "v5", "created": "Sun, 11 Feb 2018 17:33:16 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2018 17:36:46 GMT"}, {"version": "v7", "created": "Tue, 11 Dec 2018 17:53:58 GMT"}, {"version": "v8", "created": "Sat, 9 Feb 2019 20:56:30 GMT"}, {"version": "v9", "created": "Sat, 30 Mar 2019 00:48:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "1701.03102", "submitter": "Xiang Xiang", "authors": "Xiang Xiang, Trac D. Tran", "title": "Linear Disentangled Representation Learning for Facial Actions", "comments": "Codes available at https://github.com/eglxiang/icassp15_emotion and\n  https://github.com/eglxiang/FacialAU. arXiv admin note: text overlap with\n  arXiv:1410.1606", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited annotated data available for the recognition of facial expression and\naction units embarrasses the training of deep networks, which can learn\ndisentangled invariant features. However, a linear model with just several\nparameters normally is not demanding in terms of training data. In this paper,\nwe propose an elegant linear model to untangle confounding factors in\nchallenging realistic multichannel signals such as 2D face videos. The simple\nyet powerful model does not rely on huge training data and is natural for\nrecognizing facial actions without explicitly disentangling the identity. Base\non well-understood intuitive linear models such as Sparse Representation based\nClassification (SRC), previous attempts require a prepossessing of explicit\ndecoupling which is practically inexact. Instead, we exploit the low-rank\nproperty across frames to subtract the underlying neutral faces which are\nmodeled jointly with sparse representation on the action components with group\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\nautomatic method on raw face videos performs as competitive as SRC applied on\nmanually prepared action components and performs even better than SRC in terms\nof true positive rate. We apply the model to the even more challenging task of\nfacial action unit recognition, verified on the MPI Face Video Database\n(MPI-VDB) achieving a decent performance. All the programs and data have been\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:34:29 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1701.03212", "submitter": "Wei Guo", "authors": "Wei Guo, Krithika Manohar, Steven L. Brunton and Ashis G. Banerjee", "title": "Sparse-TDA: Sparse Realization of Topological Data Analysis for\n  Multi-Way Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis (TDA) has emerged as one of the most promising\ntechniques to reconstruct the unknown shapes of high-dimensional spaces from\nobserved data samples. TDA, thus, yields key shape descriptors in the form of\npersistent topological features that can be used for any supervised or\nunsupervised learning task, including multi-way classification. Sparse\nsampling, on the other hand, provides a highly efficient technique to\nreconstruct signals in the spatial-temporal domain from just a few\ncarefully-chosen samples. Here, we present a new method, referred to as the\nSparse-TDA algorithm, that combines favorable aspects of the two techniques.\nThis combination is realized by selecting an optimal set of sparse pixel\nsamples from the persistent features generated by a vector-based TDA algorithm.\nThese sparse samples are selected from a low-rank matrix representation of\npersistent features using QR pivoting. We show that the Sparse-TDA method\ndemonstrates promising performance on three benchmark problems related to human\nposture recognition and image texture classification.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 02:22:45 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 23:49:37 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 08:13:59 GMT"}, {"version": "v4", "created": "Mon, 13 Nov 2017 01:31:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Guo", "Wei", ""], ["Manohar", "Krithika", ""], ["Brunton", "Steven L.", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "1701.03268", "submitter": "Hideyuki Miyahara", "authors": "Hideyuki Miyahara, Koji Tsumura, and Yuki Sughiyama", "title": "Relaxation of the EM Algorithm via Quantum Annealing for Gaussian\n  Mixture Models", "comments": "Presented at IEEE CDC 2016, the 2016 IEEE 55th Conference on Decision\n  and Control (CDC)", "journal-ref": null, "doi": "10.1109/CDC.2016.7798981", "report-no": null, "categories": "stat.ML cond-mat.stat-mech quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modified expectation-maximization algorithm by introducing the\nconcept of quantum annealing, which we call the deterministic quantum annealing\nexpectation-maximization (DQAEM) algorithm. The expectation-maximization (EM)\nalgorithm is an established algorithm to compute maximum likelihood estimates\nand applied to many practical applications. However, it is known that EM\nheavily depends on initial values and its estimates are sometimes trapped by\nlocal optima. To solve such a problem, quantum annealing (QA) was proposed as a\nnovel optimization approach motivated by quantum mechanics. By employing QA, we\nthen formulate DQAEM and present a theorem that supports its stability.\nFinally, we demonstrate numerical simulations to confirm its efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 08:48:03 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Miyahara", "Hideyuki", ""], ["Tsumura", "Koji", ""], ["Sughiyama", "Yuki", ""]]}, {"id": "1701.03436", "submitter": "Gregor Verbic", "authors": "Ruidong Liu, Gregor Verbic, Jin Ma", "title": "Fast Stability Scanning for Future Grid Scenario Analysis", "comments": "10 pages, 7 figures, 2 tables. Submitted for publicatiob to IEEE\n  Transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future grid scenario analysis requires a major departure from conventional\npower system planning, where only a handful of most critical conditions is\ntypically analyzed. To capture the inter-seasonal variations in renewable\ngeneration of a future grid scenario necessitates the use of computationally\nintensive time-series analysis. In this paper, we propose a planning framework\nfor fast stability scanning of future grid scenarios using a novel feature\nselection algorithm and a novel self-adaptive PSO-k-means clustering algorithm.\nTo achieve the computational speed-up, the stability analysis is performed only\non small number of representative cluster centroids instead of on the full set\nof operating conditions. As a case study, we perform small-signal stability and\nsteady-state voltage stability scanning of a simplified model of the Australian\nNational Electricity Market with significant penetration of renewable\ngeneration. The simulation results show the effectiveness of the proposed\napproach. Compared to an exhaustive time series scanning, the proposed\nframework reduced the computational burden up to ten times, with an acceptable\nlevel of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 00:27:18 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Liu", "Ruidong", ""], ["Verbic", "Gregor", ""], ["Ma", "Jin", ""]]}, {"id": "1701.03441", "submitter": "Fathi Salem", "authors": "Yuzhen Lu and Fathi M. Salem", "title": "Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural\n  Networks", "comments": "5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text\n  overlap with arXiv:1612.03707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard LSTM recurrent neural networks while very powerful in long-range\ndependency sequence applications have highly complex structure and relatively\nlarge (adaptive) parameters. In this work, we present empirical comparison\nbetween the standard LSTM recurrent neural network architecture and three new\nparameter-reduced variants obtained by eliminating combinations of the input\nsignal, bias, and hidden unit signals from individual gating signals. The\nexperiments on two sequence datasets show that the three new variants, called\nsimply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the\nstandard LSTM model with less (adaptive) parameters.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:12:05 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Lu", "Yuzhen", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.03449", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence and Carl Henrik Ek", "title": "Manifold Alignment Determination: finding correspondences across\n  different data views", "comments": "NIPS workshop on Multi-Modal Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Manifold Alignment Determination (MAD), an algorithm for learning\nalignments between data points from multiple views or modalities. The approach\nis capable of learning correspondences between views as well as correspondences\nbetween individual data-points. The proposed method requires only a few aligned\nexamples from which it is capable to recover a global alignment through a\nprobabilistic model. The strong, yet flexible regularization provided by the\ngenerative model is sufficient to align the views. We provide experiments on\nboth synthetic and real data to highlight the benefit of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:36:47 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1701.03452", "submitter": "Fathi Salem", "authors": "Joel Heck and Fathi M. Salem", "title": "Simplified Minimal Gated Unit Variations for Recurrent Neural Networks", "comments": "5 pages, 3 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks with various types of hidden units have been used\nto solve a diverse range of problems involving sequence data. Two of the most\nrecent proposals, gated recurrent units (GRU) and minimal gated units (MGU),\nhave shown comparable promising results on example public datasets. In this\npaper, we introduce three model variants of the minimal gated unit (MGU) which\nfurther simplify that design by reducing the number of parameters in the\nforget-gate dynamic equation. These three model variants, referred to simply as\nMGU1, MGU2, and MGU3, were tested on sequences generated from the MNIST dataset\nand from the Reuters Newswire Topics (RNT) dataset. The new models have shown\nsimilar accuracy to the MGU model while using fewer parameters and thus\nlowering training expense. One model variant, namely MGU2, performed better\nthan MGU on the datasets considered, and thus may be used as an alternate to\nMGU or GRU in recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:52:31 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Heck", "Joel", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.03504", "submitter": "Gabriel Loaiza-Ganem", "authors": "Gabriel Loaiza-Ganem, Yuanjun Gao, John P. Cunningham", "title": "Maximum Entropy Flow Networks", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum entropy modeling is a flexible and popular framework for formulating\nstatistical models given partial knowledge. In this paper, rather than the\ntraditional method of optimizing over the continuous density directly, we learn\na smooth and invertible transformation that maps a simple distribution to the\ndesired maximum entropy distribution. Doing so is nontrivial in that the\nobjective being maximized (entropy) is a function of the density itself. By\nexploiting recent developments in normalizing flow networks, we cast the\nmaximum entropy problem into a finite-dimensional constrained optimization, and\nsolve the problem by combining stochastic optimization with the augmented\nLagrangian method. Simulation results demonstrate the effectiveness of our\nmethod, and applications to finance and computer vision show the flexibility\nand accuracy of using maximum entropy flow networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 21:00:30 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 17:13:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Loaiza-Ganem", "Gabriel", ""], ["Gao", "Yuanjun", ""], ["Cunningham", "John P.", ""]]}, {"id": "1701.03537", "submitter": "Adel Javanmard", "authors": "Adel Javanmard", "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models", "comments": "30 pages, 2 figures; accepted for publication in the Journal of\n  Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a firm that sells a large number of products to its customers in\nan online fashion. Each product is described by a high dimensional feature\nvector, and the market value of a product is assumed to be linear in the values\nof its features. Parameters of the valuation model are unknown and can change\nover time. The firm sequentially observes a product's features and can use the\nhistorical sales data (binary sale/no sale feedbacks) to set the price of\ncurrent product, with the objective of maximizing the collected revenue. We\nmeasure the performance of a dynamic pricing policy via regret, which is the\nexpected revenue loss compared to a clairvoyant that knows the sequence of\nmodel parameters in advance.\n  We propose a pricing policy based on projected stochastic gradient descent\n(PSGD) and characterize its regret in terms of time $T$, features dimension\n$d$, and the temporal variability in the model parameters, $\\delta_t$. We\nconsider two settings. In the first one, feature vectors are chosen\nantagonistically by nature and we prove that the regret of PSGD pricing policy\nis of order $O(\\sqrt{T} + \\sum_{t=1}^T \\sqrt{t}\\delta_t)$. In the second\nsetting (referred to as stochastic features model), the feature vectors are\ndrawn independently from an unknown distribution. We show that in this case,\nthe regret of PSGD pricing policy is of order $O(d^2 \\log T + \\sum_{t=1}^T\nt\\delta_t/d)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 01:08:35 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 21:53:21 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Javanmard", "Adel", ""]]}, {"id": "1701.03550", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck and Hui Li", "title": "Bayesian System Identification based on Hierarchical Sparse Bayesian\n  Learning and Gibbs Sampling with Application to Structural Damage Assessment", "comments": "12 figures", "journal-ref": null, "doi": "10.1016/j.cma.2017.01.030", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus in this paper is Bayesian system identification based on noisy\nincomplete modal data where we can impose spatially-sparse stiffness changes\nwhen updating a structural model. To this end, based on a similar hierarchical\nsparse Bayesian learning model from our previous work, we propose two Gibbs\nsampling algorithms. The algorithms differ in their strategies to deal with the\nposterior uncertainty of the equation-error precision parameter, but both\nsample from the conditional posterior probability density functions (PDFs) for\nthe structural stiffness parameters and system modal parameters. The effective\ndimension for the Gibbs sampling is low because iterative sampling is done from\nonly three conditional posterior PDFs that correspond to three parameter\ngroups, along with sampling of the equation-error precision parameter from\nanother conditional posterior PDF in one of the algorithms where it is not\nintegrated out as a \"nuisance\" parameter. A nice feature from a computational\nperspective is that it is not necessary to solve a nonlinear eigenvalue problem\nof a structural model. The effectiveness and robustness of the proposed\nalgorithms are illustrated by applying them to the IASE-ASCE Phase II simulated\nand experimental benchmark studies. The goal is to use incomplete modal data\nidentified before and after possible damage to detect and assess\nspatially-sparse stiffness reductions induced by any damage. Our past and\ncurrent focus on meeting challenges arising from Bayesian inference of\nstructural stiffness serve to strengthen the capability of vibration-based\nstructural system identification but our methods also have much broader\napplicability for inverse problems in science and technology where system\nmatrices are to be inferred from noisy partial information about their\neigenquantities.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 02:51:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Li", "Hui", ""]]}, {"id": "1701.03577", "submitter": "Avner May", "authors": "Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu,\n  Aur\\'elien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury,\n  Michael Picheny, Fei Sha", "title": "Kernel Approximation Methods for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 07:24:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["May", "Avner", ""], ["Garakani", "Alireza Bagheri", ""], ["Lu", "Zhiyun", ""], ["Guo", "Dong", ""], ["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Hsu", "Daniel", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1701.03619", "submitter": "Ori Katz", "authors": "Ori Katz, Ronen Talmon, Yu-Lun Lo and Hau-Tieng Wu", "title": "Diffusion-based nonlinear filtering for multimodal data fusion with\n  application to sleep stage assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of information fusion from multiple data-sets acquired by\nmultimodal sensors has drawn significant research attention over the years. In\nthis paper, we focus on a particular problem setting consisting of a physical\nphenomenon or a system of interest observed by multiple sensors. We assume that\nall sensors measure some aspects of the system of interest with additional\nsensor-specific and irrelevant components. Our goal is to recover the variables\nrelevant to the observed system and to filter out the nuisance effects of the\nsensor-specific variables. We propose an approach based on manifold learning,\nwhich is particularly suitable for problems with multiple modalities, since it\naims to capture the intrinsic structure of the data and relies on minimal prior\nmodel knowledge. Specifically, we propose a nonlinear filtering scheme, which\nextracts the hidden sources of variability captured by two or more sensors,\nthat are independent of the sensor-specific components. In addition to\npresenting a theoretical analysis, we demonstrate our technique on real\nmeasured data for the purpose of sleep stage assessment based on multiple,\nmultimodal sensor measurements. We show that without prior knowledge on the\ndifferent modalities and on the measured system, our method gives rise to a\ndata-driven representation that is well correlated with the underlying sleep\nprocess and is robust to noise and sensor-specific effects.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 10:45:01 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:39:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Katz", "Ori", ""], ["Talmon", "Ronen", ""], ["Lo", "Yu-Lun", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1701.03655", "submitter": "Karin Schnass", "authors": "Valeriya Naumova and Karin Schnass", "title": "Dictionary Learning from Incomplete Data", "comments": "22 pages, 9 figures, (this version with bug fix for wksvd)", "journal-ref": null, "doi": "10.1186/s13634-018-0533-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 13:06:47 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 13:37:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Naumova", "Valeriya", ""], ["Schnass", "Karin", ""]]}, {"id": "1701.03743", "submitter": "Arnim Bleier", "authors": "Arnim Bleier", "title": "Truncation-free Hybrid Inference for DPMM", "comments": "NIPS 2016 Workshop: Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian\nnon-parametrics. While these models free from choosing the number of components\na-priori, computationally attractive variational inference often reintroduces\nthe need to do so, via a truncation on the variational distribution. In this\npaper we present a truncation-free hybrid inference for DPMM, combining the\nadvantages of sampling-based MCMC and variational methods. The proposed\nhybridization enables more efficient variational updates, while increasing\nmodel complexity only if needed. We evaluate the properties of the hybrid\nupdates and their empirical performance in single- as well as mixed-membership\nmodels. Our method is easy to implement and performs favorably compared to\nexisting schemas.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:28:09 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Bleier", "Arnim", ""]]}, {"id": "1701.03755", "submitter": "Matthias Gall\\'e", "authors": "Matthias Gall\\'e", "title": "What Can I Do Now? Guiding Users in a World of Automated Decisions", "comments": "presented at BigIA 2016 workshop: http://bigia2016.irisa.fr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more processes governing our lives use in some part an automatic\ndecision step, where -- based on a feature vector derived from an applicant --\nan algorithm has the decision power over the final outcome. Here we present a\nsimple idea which gives some of the power back to the applicant by providing\nher with alternatives which would make the decision algorithm decide\ndifferently. It is based on a formalization reminiscent of methods used for\nevasion attacks, and consists in enumerating the subspaces where the\nclassifiers decides the desired output. This has been implemented for the\nspecific case of decision forests (ensemble methods based on decision trees),\nmapping the problem to an iterative version of enumerating $k$-cliques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:49:47 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Gall\u00e9", "Matthias", ""]]}, {"id": "1701.03757", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin\n  Murphy, David M. Blei", "title": "Deep Probabilistic Programming", "comments": "Appears in International Conference on Learning Representations,\n  2017. A companion webpage for this paper is available at\n  http://edwardlib.org/iclr2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:52:07 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:41:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""], ["Brevdo", "Eugene", ""], ["Murphy", "Kevin", ""], ["Blei", "David M.", ""]]}, {"id": "1701.03891", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Richard G. Baraniuk", "title": "Learning to Invert: Signal Recovery via Deep Convolutional Networks", "comments": "Accepted at The 42nd IEEE International Conference on Acoustics,\n  Speech and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promise of compressive sensing (CS) has been offset by two significant\nchallenges. First, real-world data is not exactly sparse in a fixed basis.\nSecond, current high-performance recovery algorithms are slow to converge,\nwhich limits CS to either non-real-time applications or scenarios where massive\nback-end computing is available. In this paper, we attack both of these\nchallenges head-on by developing a new signal recovery framework we call {\\em\nDeepInverse} that learns the inverse transformation from measurement vectors to\nsignals using a {\\em deep convolutional network}. When trained on a set of\nrepresentative images, the network learns both a representation for the signals\n(addressing challenge one) and an inverse map approximating a greedy or convex\nrecovery algorithm (addressing challenge two). Our experiments indicate that\nthe DeepInverse network closely approximates the solution produced by\nstate-of-the-art CS recovery algorithms yet is hundreds of times faster in run\ntime. The tradeoff for the ultrafast run time is a computationally intensive,\noff-line training procedure typical to deep networks. However, the training\nneeds to be completed only once, which makes the approach attractive for a host\nof sparse recovery problems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 08:42:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Mousavi", "Ali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1701.03918", "submitter": "Yongqing Wang", "authors": "Yongqing Wang, Shenghua Liu, Huawei Shen, Xueqi Cheng", "title": "Marked Temporal Dynamics Modeling based on Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are now witnessing the increasing availability of event stream data, i.e.,\na sequence of events with each event typically being denoted by the time it\noccurs and its mark information (e.g., event type). A fundamental problem is to\nmodel and predict such kind of marked temporal dynamics, i.e., when the next\nevent will take place and what its mark will be. Existing methods either\npredict only the mark or the time of the next event, or predict both of them,\nyet separately. Indeed, in marked temporal dynamics, the time and the mark of\nthe next event are highly dependent on each other, requiring a method that\ncould simultaneously predict both of them. To tackle this problem, in this\npaper, we propose to model marked temporal dynamics by using a mark-specific\nintensity function to explicitly capture the dependency between the mark and\nthe time of the next event. Extensive experiments on two datasets demonstrate\nthat the proposed method outperforms state-of-the-art methods at predicting\nmarked temporal dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 13:26:39 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Wang", "Yongqing", ""], ["Liu", "Shenghua", ""], ["Shen", "Huawei", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1701.03974", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Qing Ling, Georgios B. Giannakis", "title": "An Online Convex Optimization Approach to Dynamic Network Resource\n  Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2750109", "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to online convex optimization (OCO) make sequential\none-slot-ahead decisions, which lead to (possibly adversarial) losses that\ndrive subsequent decision iterates. Their performance is evaluated by the\nso-called regret that measures the difference of losses between the online\nsolution and the best yet fixed overall solution in hindsight. The present\npaper deals with online convex optimization involving adversarial loss\nfunctions and adversarial constraints, where the constraints are revealed after\nmaking decisions, and can be tolerable to instantaneous violations but must be\nsatisfied in the long term. Performance of an online algorithm in this setting\nis assessed by: i) the difference of its losses relative to the best dynamic\nsolution with one-slot-ahead information of the loss function and the\nconstraint (that is here termed dynamic regret); and, ii) the accumulated\namount of constraint violations (that is here termed dynamic fit). In this\ncontext, a modified online saddle-point (MOSP) scheme is developed, and proved\nto simultaneously yield sub-linear dynamic regret and fit, provided that the\naccumulated variations of per-slot minimizers and constraints are sub-linearly\ngrowing with time. MOSP is also applied to the dynamic network resource\nallocation task, and it is compared with the well-known stochastic dual\ngradient method. Under various scenarios, numerical experiments demonstrate the\nperformance gain of MOSP relative to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 23:28:21 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 05:33:29 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Tianyi", ""], ["Ling", "Qing", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1701.03980", "submitter": "Graham Neubig", "authors": "Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed\n  Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel\n  Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette,\n  Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya\n  Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha\n  Swayamdipta, Pengcheng Yin", "title": "DyNet: The Dynamic Neural Network Toolkit", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe DyNet, a toolkit for implementing neural network models based on\ndynamic declaration of network structure. In the static declaration strategy\nthat is used in toolkits like Theano, CNTK, and TensorFlow, the user first\ndefines a computation graph (a symbolic representation of the computation), and\nthen examples are fed into an engine that executes this computation and\ncomputes its derivatives. In DyNet's dynamic declaration strategy, computation\ngraph construction is mostly transparent, being implicitly constructed by\nexecuting procedural code that computes the network outputs, and the user is\nfree to use different network structures for each input. Dynamic declaration\nthus facilitates the implementation of more complicated network architectures,\nand DyNet is specifically designed to allow users to implement their models in\na way that is idiomatic in their preferred programming language (C++ or\nPython). One challenge with dynamic declaration is that because the symbolic\ncomputation graph is defined anew for every training example, its construction\nmust have low overhead. To achieve this, DyNet has an optimized C++ backend and\nlightweight graph representation. Experiments show that DyNet's speeds are\nfaster than or comparable with static declaration toolkits, and significantly\nfaster than Chainer, another dynamic declaration toolkit. DyNet is released\nopen-source under the Apache 2.0 license and available at\nhttp://github.com/clab/dynet.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 01:53:23 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Neubig", "Graham", ""], ["Dyer", "Chris", ""], ["Goldberg", "Yoav", ""], ["Matthews", "Austin", ""], ["Ammar", "Waleed", ""], ["Anastasopoulos", "Antonios", ""], ["Ballesteros", "Miguel", ""], ["Chiang", "David", ""], ["Clothiaux", "Daniel", ""], ["Cohn", "Trevor", ""], ["Duh", "Kevin", ""], ["Faruqui", "Manaal", ""], ["Gan", "Cynthia", ""], ["Garrette", "Dan", ""], ["Ji", "Yangfeng", ""], ["Kong", "Lingpeng", ""], ["Kuncoro", "Adhiguna", ""], ["Kumar", "Gaurav", ""], ["Malaviya", "Chaitanya", ""], ["Michel", "Paul", ""], ["Oda", "Yusuke", ""], ["Richardson", "Matthew", ""], ["Saphra", "Naomi", ""], ["Swayamdipta", "Swabha", ""], ["Yin", "Pengcheng", ""]]}, {"id": "1701.04112", "submitter": "Gabor Lugosi", "authors": "G\\'abor Lugosi and Shahar Mendelson", "title": "Regularization, sparse recovery, and median-of-means tournaments", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regularized risk minimization procedure for regression function estimation\nis introduced that achieves near optimal accuracy and confidence under general\nconditions, including heavy-tailed predictor and response variables. The\nprocedure is based on median-of-means tournaments, introduced by the authors in\n[8]. It is shown that the new procedure outperforms standard regularized\nempirical risk minimization procedures such as lasso or slope in heavy-tailed\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 21:14:22 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 15:09:54 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Lugosi", "G\u00e1bor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1701.04207", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Delin Chu and Li-Zhi Liao and Michael K. Ng", "title": "Sparse Kernel Canonical Correlation Analysis via $\\ell_1$-regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a multivariate statistical technique\nfor finding the linear relationship between two sets of variables. The kernel\ngeneralization of CCA named kernel CCA has been proposed to find nonlinear\nrelations between datasets. Despite their wide usage, they have one common\nlimitation that is the lack of sparsity in their solution. In this paper, we\nconsider sparse kernel CCA and propose a novel sparse kernel CCA algorithm\n(SKCCA). Our algorithm is based on a relationship between kernel CCA and least\nsquares. Sparsity of the dual transformations is introduced by penalizing the\n$\\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not\nonly performs well in computing sparse dual transformations but also can\nalleviate the over-fitting problem of kernel CCA.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 09:15:12 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Chu", "Delin", ""], ["Liao", "Li-Zhi", ""], ["Ng", "Michael K.", ""]]}, {"id": "1701.04245", "submitter": "Xiaolei Ma", "authors": "Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Na, Yong Wang and Yunpeng\n  Wang", "title": "Learning Traffic as Images: A Deep Convolutional Neural Network for\n  Large-Scale Transportation Network Speed Prediction", "comments": null, "journal-ref": "Sensors,2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a convolutional neural network (CNN)-based method that\nlearns traffic as images and predicts large-scale, network-wide traffic speed\nwith a high accuracy. Spatiotemporal traffic dynamics are converted to images\ndescribing the time and space relations of traffic flow via a two-dimensional\ntime-space matrix. A CNN is applied to the image following two consecutive\nsteps: abstract traffic feature extraction and network-wide traffic speed\nprediction. The effectiveness of the proposed method is evaluated by taking two\nreal-world transportation networks, the second ring road and north-east\ntransportation network in Beijing, as examples, and comparing the method with\nfour prevailing algorithms, namely, ordinary least squares, k-nearest\nneighbors, artificial neural network, and random forest, and three deep\nlearning architectures, namely, stacked autoencoder, recurrent neural network,\nand long-short-term memory network. The results show that the proposed method\noutperforms other algorithms by an average accuracy improvement of 42.91%\nwithin an acceptable execution time. The CNN can train the model in a\nreasonable time and, thus, is suitable for large-scale transportation networks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:22:38 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 10:41:03 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 00:55:34 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 06:25:18 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Ma", "Xiaolei", ""], ["Dai", "Zhuang", ""], ["He", "Zhengbing", ""], ["Na", "Jihui", ""], ["Wang", "Yong", ""], ["Wang", "Yunpeng", ""]]}, {"id": "1701.04342", "submitter": "Wolfgang Doneit", "authors": "Wolfgang Doneit, Ralf Mikut, Markus Reischl", "title": "Datenqualit\\\"at in Regressionsproblemen", "comments": "7 pages, in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are increasingly built using datasets which do not follow a\ndesign of experiment. Instead, the data is e.g. gathered by an automated\nmonitoring of a technical system. As a consequence, already the input data\nrepresents phenomena of the system and violates statistical assumptions of\ndistributions. The input data can show correlations, clusters or other\npatterns. Further, the distribution of input data influences the reliability of\nregression models. We propose criteria to quantify typical phenomena of input\ndata for regression and show their suitability with simulated benchmark\ndatasets.\n  -----\n  Regressionen werden zunehmend auf Datens\\\"atzen angewendet, deren\nEingangsvektoren nicht durch eine statistische Versuchsplanung festgelegt\nwurden. Stattdessen werden die Daten beispielsweise durch die passive\nBeobachtung technischer Systeme gesammelt. Damit bilden bereits die\nEingangsdaten Ph\\\"anomene des Systems ab und widersprechen statistischen\nVerteilungsannahmen. Die Verteilung der Eingangsdaten hat Einfluss auf die\nZuverl\\\"assigkeit eines Regressionsmodells. Wir stellen deshalb\nBewertungskriterien f\\\"ur einige typische Ph\\\"anomene in Eingangsdaten von\nRegressionen vor und zeigen ihre Funktionalit\\\"at anhand simulierter\nBenchmarkdatens\\\"atze.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 16:03:14 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Doneit", "Wolfgang", ""], ["Mikut", "Ralf", ""], ["Reischl", "Markus", ""]]}, {"id": "1701.04355", "submitter": "Hadrien Bertrand", "authors": "Hadrien Bertrand, Matthieu Perrot, Roberto Ardon, Isabelle Bloch", "title": "Classification of MRI data using Deep Learning and Gaussian\n  Process-based Model Selection", "comments": "Accepted at ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of MRI images according to the anatomical field of view is\na necessary task to solve when faced with the increasing quantity of medical\nimages. In parallel, advances in deep learning makes it a suitable tool for\ncomputer vision problems. Using a common architecture (such as AlexNet)\nprovides quite good results, but not sufficient for clinical use. Improving the\nmodel is not an easy task, due to the large number of hyper-parameters\ngoverning both the architecture and the training of the network, and to the\nlimited understanding of their relevance. Since an exhaustive search is not\ntractable, we propose to optimize the network first by random search, and then\nby an adaptive search based on Gaussian Processes and Probability of\nImprovement. Applying this method on a large and varied MRI dataset, we show a\nsubstantial improvement between the baseline network and the final one (up to\n20\\% for the most difficult classes).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 17:02:31 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Bertrand", "Hadrien", ""], ["Perrot", "Matthieu", ""], ["Ardon", "Roberto", ""], ["Bloch", "Isabelle", ""]]}, {"id": "1701.04389", "submitter": "Gregory Ledva", "authors": "Gregory S. Ledva, Laura Balzano, Johanna L. Mathieu", "title": "Real-Time Energy Disaggregation of a Distribution Feeder's Demand Using\n  Online Learning", "comments": "14 pages, article in press, 2018, IEEE Transactions on Power Systems", "journal-ref": null, "doi": "10.1109/TPWRS.2018.2800535", "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though distribution system operators have been adding more sensors to their\nnetworks, they still often lack an accurate real-time picture of the behavior\nof distributed energy resources such as demand responsive electric loads and\nresidential solar generation. Such information could improve system\nreliability, economic efficiency, and environmental impact. Rather than\ninstalling additional, costly sensing and communication infrastructure to\nobtain additional real-time information, it may be possible to use existing\nsensing capabilities and leverage knowledge about the system to reduce the need\nfor new infrastructure. In this paper, we disaggregate a distribution feeder's\ndemand measurements into: 1) the demand of a population of air conditioners,\nand 2) the demand of the remaining loads connected to the feeder. We use an\nonline learning algorithm, Dynamic Fixed Share (DFS), that uses the real-time\ndistribution feeder measurements as well as models generated from historical\nbuilding- and device-level data. We develop two implementations of the\nalgorithm and conduct case studies using real demand data from households and\ncommercial buildings to investigate the effectiveness of the algorithm. The\ncase studies demonstrate that DFS can effectively perform online disaggregation\nand the choice and construction of models included in the algorithm affects its\naccuracy, which is comparable to that of a set of Kalman filters.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:32:36 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 17:36:54 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 19:07:51 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Ledva", "Gregory S.", ""], ["Balzano", "Laura", ""], ["Mathieu", "Johanna L.", ""]]}, {"id": "1701.04398", "submitter": "Takashi Nakamura", "authors": "Takashi Nakamura, Valentin Goverdovsky, Mary J. Morrell, Danilo P.\n  Mandic", "title": "Automatic sleep monitoring using ear-EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring of sleep patterns without patient's inconvenience or\ninvolvement of a medical specialist is a clinical question of significant\nimportance. To this end, we propose an automatic sleep stage monitoring system\nbased on an affordable, unobtrusive, discreet, and long-term wearable in-ear\nsensor for recording the Electroencephalogram (ear-EEG). The selected features\nfor sleep pattern classification from a single ear-EEG channel include the\nspectral edge frequency (SEF) and multi- scale fuzzy entropy (MSFE), a\nstructural complexity feature. In this preliminary study, the manually scored\nhypnograms from simultaneous scalp-EEG and ear-EEG recordings of four subjects\nare used as labels for two analysis scenarios: 1) classification of ear-EEG\nhypnogram labels from ear-EEG recordings and 2) prediction of scalp-EEG\nhypnogram labels from ear-EEG recordings. We consider both 2-class and 4-class\nsleep scoring, with the achieved accuracies ranging from 78.5 % to 95.2 % for\near-EEG labels predicted from ear-EEG, and 76.8 % to 91.8 % for scalp-EEG\nlabels predicted from ear-EEG. The corresponding kappa coefficients, which\nrange from 0.64 to 0.83 for Scenario 1 and from 0.65 to 0.80 for Scenario 2,\nindicate a Substantial to Almost Perfect agreement, thus proving the\nfeasibility of in-ear sensing for sleep monitoring in the community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:58:25 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Nakamura", "Takashi", ""], ["Goverdovsky", "Valentin", ""], ["Morrell", "Mary J.", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1701.04455", "submitter": "David Gamarnik", "authors": "David Gamarnik and Ilias Zadik", "title": "High-Dimensional Regression with Binary Coefficients. Estimating Squared\n  Error and a Phase Transition", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse linear regression model Y=X\\beta^{*}+W where X has a\nGaussian entries, W is the noise vector with mean zero Gaussian entries, and\n\\beta^{*} is a binary vector with support size (sparsity) k. Using a novel\nconditional second moment method we obtain a tight up to a multiplicative\nconstant approximation of the optimal squared error\n\\min_{\\beta}\\|Y-X\\beta\\|_{2}, where the minimization is over all k-sparse\nbinary vectors \\beta. The approximation reveals interesting structural\nproperties of the underlying regression problem. In particular, a) We establish\nthat n^*=2k\\log p/\\log (2k/\\sigma^{2}+1) is a phase transition point with the\nfollowing \"all-or-nothing\" property. When n exceeds n^{*},\n(2k)^{-1}\\|\\beta_{2}-\\beta^*\\|_0\\approx 0, and when n is below n^{*},\n(2k)^{-1}\\|\\beta_{2}-\\beta^*\\|_0\\approx 1, where \\beta_2 is the optimal\nsolution achieving the smallest squared error. With this we prove that n^{*} is\nthe asymptotic threshold for recovering \\beta^* information theoretically. b)\nWe compute the squared error for an intermediate problem\n\\min_{\\beta}\\|Y-X\\beta\\|_{2} where minimization is restricted to vectors \\beta\nwith \\|\\beta-\\beta^{*}\\|_0=2k \\zeta, for \\zeta\\in [0,1]. We show that a lower\nbound part \\Gamma(\\zeta) of the estimate, which corresponds to the estimate\nbased on the first moment method, undergoes a phase transition at three\ndifferent thresholds, namely n_{\\text{inf,1}}=\\sigma^2\\log p, which is\ninformation theoretic bound for recovering \\beta^* when k=1 and \\sigma is\nlarge, then at n^{*} and finally at n_{\\text{LASSO/CS}}. c) We establish a\ncertain Overlap Gap Property (OGP) on the space of all binary vectors \\beta\nwhen n\\le ck\\log p for sufficiently small constant c. We conjecture that OGP is\nthe source of algorithmic hardness of solving the minimization problem\n\\min_{\\beta}\\|Y-X\\beta\\|_{2} in the regime n<n_{\\text{LASSO/CS}}.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 20:44:36 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 15:35:54 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 06:08:51 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Gamarnik", "David", ""], ["Zadik", "Ilias", ""]]}, {"id": "1701.04489", "submitter": "Tapabrata Ghosh", "authors": "Tapabrata Ghosh", "title": "Towards a New Interpretation of Separable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 23:57:33 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Ghosh", "Tapabrata", ""]]}, {"id": "1701.04503", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu", "title": "Deep Learning for Computational Chemistry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CE cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 01:15:14 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Goh", "Garrett B.", ""], ["Hodas", "Nathan O.", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1701.04516", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Aruna Tiwari and Qian Leng", "title": "On The Construction of Extreme Learning Machine for Online and Offline\n  One-Class Classification - An Expanded Toolbox", "comments": "This paper has been accepted in Neurocomputing Journal (Elsevier)\n  with Manuscript id: NEUCOM-D-15-02856", "journal-ref": null, "doi": "10.1016/j.neucom.2016.04.070", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-Class Classification (OCC) has been prime concern for researchers and\neffectively employed in various disciplines. But, traditional methods based\none-class classifiers are very time consuming due to its iterative process and\nvarious parameters tuning. In this paper, we present six OCC methods based on\nextreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed\nclassifiers mainly lie in two categories: reconstruction based and boundary\nbased, which supports both types of learning viz., online and offline learning.\nOut of various proposed methods, four are offline and remaining two are online\nmethods. Out of four offline methods, two methods perform random feature\nmapping and two methods perform kernel feature mapping. Kernel feature mapping\nbased approaches have been tested with RBF kernel and online version of\none-class classifiers are tested with both types of nodes viz., additive and\nRBF. It is well known fact that threshold decision is a crucial factor in case\nof OCC, so, three different threshold deciding criteria have been employed so\nfar and analyses the effectiveness of one threshold deciding criteria over\nanother. Further, these methods are tested on two artificial datasets to check\nthere boundary construction capability and on eight benchmark datasets from\ndifferent discipline to evaluate the performance of the classifiers. Our\nproposed classifiers exhibit better performance compared to ten traditional\none-class classifiers and ELM based two one-class classifiers. Through proposed\none-class classifiers, we intend to expand the functionality of the most used\ntoolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with\nall the present features of the toolbox.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 02:55:51 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Gautam", "Chandan", ""], ["Tiwari", "Aruna", ""], ["Leng", "Qian", ""]]}, {"id": "1701.04532", "submitter": "Shiliang Sun", "authors": "Qiuyang Liu, Shiliang Sun", "title": "Multi-view Regularized Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) have been proven to be powerful tools in various\nareas of machine learning. However, there are very few applications of GPs in\nthe scenario of multi-view learning. In this paper, we present a new GP model\nfor multi-view learning. Unlike existing methods, it combines multiple views by\nregularizing marginal likelihood with the consistency among the posterior\ndistributions of latent functions from different views. Moreover, we give a\ngeneral point selection scheme for multi-view learning and improve the proposed\nmodel by this criterion. Experimental results on multiple real world data sets\nhave verified the effectiveness of the proposed model and witnessed the\nperformance improvement through employing this novel point selection scheme.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 05:20:38 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Liu", "Qiuyang", ""], ["Sun", "Shiliang", ""]]}, {"id": "1701.04724", "submitter": "Alexander Jung", "authors": "Nguyen Q. Tran and Oleksii Abramenko and Alexander Jung", "title": "On the Sample Complexity of Graphical Model Selection for Non-Stationary\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the sample size required for accurate graphical model\nselection from non-stationary samples. The observed data is modeled as a\nvector-valued zero-mean Gaussian random process whose samples are uncorrelated\nbut have different covariance matrices. This model contains as special cases\nthe standard setting of i.i.d. samples as well as the case of samples forming a\nstationary or underspread (non-stationary) processes. More generally, our model\napplies to any process model for which an efficient decorrelation can be\nobtained. By analyzing a particular model selection method, we derive a\nsufficient condition on the required sample size for accurate graphical model\nselection based on non-stationary data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:19:44 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 08:14:21 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 10:37:23 GMT"}, {"version": "v4", "created": "Sun, 26 May 2019 14:47:03 GMT"}, {"version": "v5", "created": "Thu, 27 Jun 2019 13:10:36 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Tran", "Nguyen Q.", ""], ["Abramenko", "Oleksii", ""], ["Jung", "Alexander", ""]]}, {"id": "1701.04831", "submitter": "Jing Chen", "authors": "Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xiang", "title": "Equivalence of restricted Boltzmann machines and tensor network states", "comments": "18 pages, 12 figures + 2 appendices; Code implementations at\n  https://github.com/yzcj105/rbm2mps", "journal-ref": "Phys. Rev. B 97, 085104 (2018)", "doi": "10.1103/PhysRevB.97.085104", "report-no": null, "categories": "cond-mat.str-el quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted Boltzmann machine (RBM) is one of the fundamental building\nblocks of deep learning. RBM finds wide applications in dimensional reduction,\nfeature extraction, and recommender systems via modeling the probability\ndistributions of a variety of input data including natural images, speech\nsignals, and customer ratings, etc. We build a bridge between RBM and tensor\nnetwork states (TNS) widely used in quantum many-body physics research. We\ndevise efficient algorithms to translate an RBM into the commonly used TNS.\nConversely, we give sufficient and necessary conditions to determine whether a\nTNS can be transformed into an RBM of given architectures. Revealing these\ngeneral and constructive connections can cross-fertilize both deep learning and\nquantum many-body physics. Notably, by exploiting the entanglement entropy\nbound of TNS, we can rigorously quantify the expressive power of RBM on complex\ndata sets. Insights into TNS and its entanglement capacity can guide the design\nof more powerful deep learning architectures. On the other hand, RBM can\nrepresent quantum many-body states with fewer parameters compared to TNS, which\nmay allow more efficient classical simulations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 19:00:07 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 22:56:47 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Chen", "Jing", ""], ["Cheng", "Song", ""], ["Xie", "Haidong", ""], ["Wang", "Lei", ""], ["Xiang", "Tao", ""]]}, {"id": "1701.04851", "submitter": "Forrester Cole", "authors": "Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar\n  Mosseri, William T. Freeman", "title": "Synthesizing Normalized Faces from Facial Identity Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for synthesizing a frontal, neutral-expression image of a\nperson's face given an input face photograph. This is achieved by learning to\ngenerate facial landmarks and textures from features extracted from a\nfacial-recognition network. Unlike previous approaches, our encoding feature\nvector is largely invariant to lighting, pose, and facial expression.\nExploiting this invariance, we train our decoder network using only frontal,\nneutral-expression photographs. Since these photographs are well aligned, we\ncan decompose them into a sparse set of landmark points and aligned texture\nmaps. The decoder then predicts landmarks and textures independently and\ncombines them using a differentiable image warping operation. The resulting\nimages can be used for a number of applications, such as analyzing facial\nattributes, exposure and white balance adjustment, or creating a 3-D avatar.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 20:03:46 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 12:52:59 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 14:27:34 GMT"}, {"version": "v4", "created": "Tue, 17 Oct 2017 15:27:21 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Cole", "Forrester", ""], ["Belanger", "David", ""], ["Krishnan", "Dilip", ""], ["Sarna", "Aaron", ""], ["Mosseri", "Inbar", ""], ["Freeman", "William T.", ""]]}, {"id": "1701.04862", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, L\\'eon Bottou", "title": "Towards Principled Methods for Training Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 20:46:21 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Arjovsky", "Martin", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1701.04869", "submitter": "Samuel Kadoury", "authors": "Samuel Kadoury, William Mandel, Marjolaine Roy-Beaudry, Marie-Lyne\n  Nault, Stefan Parent", "title": "3D Morphology Prediction of Progressive Spinal Deformities from\n  Probabilistic Modeling of Discriminant Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for predicting the progression of adolescent\nidiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray\nimages. Recent progress in machine learning have allowed to improve\nclassification and prognosis rates, but lack a probabilistic framework to\nmeasure uncertainty in the data. We propose a discriminative probabilistic\nmanifold embedding where locally linear mappings transform data points from\nhigh-dimensional space to corresponding low-dimensional coordinates. A\ndiscriminant adjacency matrix is constructed to maximize the separation between\nprogressive and non-progressive groups of patients diagnosed with scoliosis,\nwhile minimizing the distance in latent variables belonging to the same class.\nTo predict the evolution of deformation, a baseline reconstruction is projected\nonto the manifold, from which a spatiotemporal regression model is built from\nparallel transport curves inferred from neighboring exemplars. Rate of\nprogression is modulated from the spine flexibility and curve magnitude of the\n3D spine deformation. The method was tested on 745 reconstructions from 133\nsubjects using longitudinal 3D reconstructions of the spine, with results\ndemonstrating the discriminatory framework can identify between progressive and\nnon-progressive of scoliotic patients with a classification rate of 81% and\nprediction differences of 2.1$^{o}$ in main curve angulation, outperforming\nother manifold learning methods. Our method achieved a higher prediction\naccuracy and improved the modeling of spatiotemporal morphological changes in\nhighly deformed spines compared to other learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 21:15:56 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 12:37:29 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Kadoury", "Samuel", ""], ["Mandel", "William", ""], ["Roy-Beaudry", "Marjolaine", ""], ["Nault", "Marie-Lyne", ""], ["Parent", "Stefan", ""]]}, {"id": "1701.04889", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty and Tianxi Cai", "title": "Efficient and Adaptive Linear Regression in Semi-Supervised Settings", "comments": "51 pages; Revised version - to appear in The Annals of Statistics", "journal-ref": "The Annals of Statistics 2018, Vol. 46, No. 4, 1541-1572", "doi": "10.1214/17-AOS1594", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression problem under semi-supervised settings\nwherein the available data typically consists of: (i) a small or moderate sized\n'labeled' data, and (ii) a much larger sized 'unlabeled' data. Such data arises\nnaturally from settings where the outcome, unlike the covariates, is expensive\nto obtain, a frequent scenario in modern studies involving large databases like\nelectronic medical records (EMR). Supervised estimators like the ordinary least\nsquares (OLS) estimator utilize only the labeled data. It is often of interest\nto investigate if and when the unlabeled data can be exploited to improve\nestimation of the regression parameter in the adopted linear model.\n  In this paper, we propose a class of 'Efficient and Adaptive Semi-Supervised\nEstimators' (EASE) to improve estimation efficiency. The EASE are two-step\nestimators adaptive to model mis-specification, leading to improved (optimal in\nsome cases) efficiency under model mis-specification, and equal (optimal)\nefficiency under a linear model. This adaptive property, often unaddressed in\nthe existing literature, is crucial for advocating 'safe' use of the unlabeled\ndata. The construction of EASE primarily involves a flexible\n'semi-non-parametric' imputation, including a smoothing step that works well\neven when the number of covariates is not small; and a follow up 'refitting'\nstep along with a cross-validation (CV) strategy both of which have useful\npractical as well as theoretical implications towards addressing two important\nissues: under-smoothing and over-fitting. We establish asymptotic results\nincluding consistency, asymptotic normality and the adaptive properties of\nEASE. We also provide influence function expansions and a 'double' CV strategy\nfor inference. The results are further validated through extensive simulations,\nfollowed by application to an EMR study on auto-immunity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 22:29:22 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 18:15:39 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Cai", "Tianxi", ""]]}, {"id": "1701.04895", "submitter": "Samuel Albanie", "authors": "Samuel Albanie, Hillary Shakespeare and Tom Gunter", "title": "Unknowable Manipulators: Social Network Curator Algorithms", "comments": "NIPS Symposium 2016: Machine Learning and the Law", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a social networking service to acquire and retain users, it must find\nways to keep them engaged. By accurately gauging their preferences, it is able\nto serve them with the subset of available content that maximises revenue for\nthe site. Without the constraints of an appropriate regulatory framework, we\nargue that a sufficiently sophisticated curator algorithm tasked with\nperforming this process may choose to explore curation strategies that are\ndetrimental to users. In particular, we suggest that such an algorithm is\ncapable of learning to manipulate its users, for several qualitative reasons:\n1. Access to vast quantities of user data combined with ongoing breakthroughs\nin the field of machine learning are leading to powerful but uninterpretable\nstrategies for decision making at scale. 2. The availability of an effective\nfeedback mechanism for assessing the short and long term user responses to\ncuration strategies. 3. Techniques from reinforcement learning have allowed\nmachines to learn automated and highly successful strategies at an abstract\nlevel, often resulting in non-intuitive yet nonetheless highly appropriate\naction selection. In this work, we consider the form that these strategies for\nuser manipulation might take and scrutinise the role that regulation should\nplay in the design of such systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 22:52:24 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Albanie", "Samuel", ""], ["Shakespeare", "Hillary", ""], ["Gunter", "Tom", ""]]}, {"id": "1701.04944", "submitter": "Hemant Ishwaran", "authors": "Min Lu and Hemant Ishwaran", "title": "A Machine Learning Alternative to P-values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an alternative approach to p-values in regression\nsettings. This approach, whose origins can be traced to machine learning, is\nbased on the leave-one-out bootstrap for prediction error. In machine learning\nthis is called the out-of-bag (OOB) error. To obtain the OOB error for a model,\none draws a bootstrap sample and fits the model to the in-sample data. The\nout-of-sample prediction error for the model is obtained by calculating the\nprediction error for the model using the out-of-sample data. Repeating and\naveraging yields the OOB error, which represents a robust cross-validated\nestimate of the accuracy of the underlying model. By a simple modification to\nthe bootstrap data involving \"noising up\" a variable, the OOB method yields a\nvariable importance (VIMP) index, which directly measures how much a specific\nvariable contributes to the prediction precision of a model. VIMP provides a\nscientifically interpretable measure of the effect size of a variable, we call\nthe \"predictive effect size\", that holds whether the researcher's model is\ncorrect or not, unlike the p-value whose calculation is based on the assumed\ncorrectness of the model. We also discuss a marginal VIMP index, also easily\ncalculated, which measures the marginal effect of a variable, or what we call\n\"the discovery effect\". The OOB procedure can be applied to both parametric and\nnonparametric regression models and requires only that the researcher can\nrepeatedly fit their model to bootstrap and modified bootstrap data. We\nillustrate this approach on a survival data set involving patients with\nsystolic heart failure and to a simulated survival data set where the model is\nincorrectly specified to illustrate its robustness to model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 05:07:03 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 01:59:00 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 15:09:41 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 00:01:13 GMT"}, {"version": "v5", "created": "Mon, 20 Feb 2017 19:12:33 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Lu", "Min", ""], ["Ishwaran", "Hemant", ""]]}, {"id": "1701.04968", "submitter": "Peng Zhao", "authors": "Zhao Peng", "title": "Multilayer Perceptron Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 06:49:03 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Peng", "Zhao", ""]]}, {"id": "1701.05004", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Thomas R. Shultz", "title": "Converting Cascade-Correlation Neural Nets into Probabilistic Generative\n  Models", "comments": null, "journal-ref": "Proceedings of the 39th Annual Conference of the Cognitive Science\n  Society (2017) (pp. 1029-1034). Austin, TX: Cognitive Science Society", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are not only adept in recognizing what class an input instance belongs\nto (i.e., classification task), but perhaps more remarkably, they can imagine\n(i.e., generate) plausible instances of a desired class with ease, when\nprompted. Inspired by this, we propose a framework which allows transforming\nCascade-Correlation Neural Networks (CCNNs) into probabilistic generative\nmodels, thereby enabling CCNNs to generate samples from a category of interest.\nCCNNs are a well-known class of deterministic, discriminative NNs, which\nautonomously construct their topology, and have been successful in giving\naccounts for a variety of psychological phenomena. Our proposed framework is\nbased on a Markov Chain Monte Carlo (MCMC) method, called the\nMetropolis-adjusted Langevin algorithm, which capitalizes on the gradient\ninformation of the target distribution to direct its explorations towards\nregions of high probability, thereby achieving good mixing properties. Through\nextensive simulations, we demonstrate the efficacy of our proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 10:51:58 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1701.05130", "submitter": "Rendani Mbuvha", "authors": "Ludvig Ericson, Rendani Mbuvha", "title": "On the Performance of Network Parallel Training in Artificial Neural\n  Networks", "comments": "4 Pages, 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) have received increasing attention in\nrecent years with applications that span a wide range of disciplines including\nvital domains such as medicine, network security and autonomous transportation.\nHowever, neural network architectures are becoming increasingly complex and\nwith an increasing need to obtain real-time results from such models, it has\nbecome pivotal to use parallelization as a mechanism for speeding up network\ntraining and deployment. In this work we propose an implementation of Network\nParallel Training through Cannon's Algorithm for matrix multiplication. We show\nthat increasing the number of processes speeds up training until the point\nwhere process communication costs become prohibitive; this point varies by\nnetwork complexity. We also show through empirical efficiency calculations that\nthe speedup obtained is superlinear.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:17:35 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Ericson", "Ludvig", ""], ["Mbuvha", "Rendani", ""]]}, {"id": "1701.05131", "submitter": "Lucas Lamata", "authors": "Lucas Lamata", "title": "Basic protocols in quantum reinforcement learning with superconducting\n  circuits", "comments": "Published version", "journal-ref": "Scientific Reports 7, 1609 (2017)", "doi": "10.1038/s41598-017-01711-6", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cond-mat.supr-con cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superconducting circuit technologies have recently achieved quantum protocols\ninvolving closed feedback loops. Quantum artificial intelligence and quantum\nmachine learning are emerging fields inside quantum technologies which may\nenable quantum devices to acquire information from the outer world and improve\nthemselves via a learning process. Here we propose the implementation of basic\nprotocols in quantum reinforcement learning, with superconducting circuits\nemploying feedback-loop control. We introduce diverse scenarios for\nproof-of-principle experiments with state-of-the-art superconducting circuit\ntechnologies and analyze their feasibility in presence of imperfections. The\nfield of quantum artificial intelligence implemented with superconducting\ncircuits paves the way for enhanced quantum control and quantum computation\nprotocols.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:18:22 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 07:48:33 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 09:50:14 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lamata", "Lucas", ""]]}, {"id": "1701.05228", "submitter": "Konstantina Christakopoulou", "authors": "Konstantina Christakopoulou, Jaya Kawale, Arindam Banerjee", "title": "Recommendation under Capacity Constraints", "comments": "Extended methods section and experimental section to include bayesian\n  personalized ranking objective as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the common scenario where every candidate item\nfor recommendation is characterized by a maximum capacity, i.e., number of\nseats in a Point-of-Interest (POI) or size of an item's inventory. Despite the\nprevalence of the task of recommending items under capacity constraints in a\nvariety of settings, to the best of our knowledge, none of the known\nrecommender methods is designed to respect capacity constraints. To close this\ngap, we extend three state-of-the art latent factor recommendation approaches:\nprobabilistic matrix factorization (PMF), geographical matrix factorization\n(GeoMF), and bayesian personalized ranking (BPR), to optimize for both\nrecommendation accuracy and expected item usage that respects the capacity\nconstraints. We introduce the useful concepts of user propensity to listen and\nitem capacity. Our experimental results in real-world datasets, both for the\ndomain of item recommendation and POI recommendation, highlight the benefit of\nour method for the setting of recommendation under capacity constraints.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:45:57 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 23:33:18 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Christakopoulou", "Konstantina", ""], ["Kawale", "Jaya", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1701.05230", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Matey Neykov, Raymond Carroll and Tianxi Cai", "title": "Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index\n  Models for Binary Outcomes", "comments": "50 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recovery of regression coefficients, denoted by\n$\\boldsymbol{\\beta}_0$, for a single index model (SIM) relating a binary\noutcome $Y$ to a set of possibly high dimensional covariates $\\boldsymbol{X}$,\nbased on a large but 'unlabeled' dataset $\\mathcal{U}$, with $Y$ never\nobserved. On $\\mathcal{U}$, we fully observe $\\boldsymbol{X}$ and additionally,\na surrogate $S$ which, while not being strongly predictive of $Y$ throughout\nthe entirety of its support, can forecast it with high accuracy when it assumes\nextreme values. Such datasets arise naturally in modern studies involving large\ndatabases such as electronic medical records (EMR) where $Y$, unlike\n$(\\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,\nan example of $Y$ and $S$ would be the true disease phenotype and the count of\nthe associated diagnostic codes respectively. Assuming another SIM for $S$\ngiven $\\boldsymbol{X}$, we show that under sparsity assumptions, we can recover\n$\\boldsymbol{\\beta}_0$ proportionally by simply fitting a least squares LASSO\nestimator to the subset of the observed data on $(\\boldsymbol{X}, S)$\nrestricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of\n$S$. We obtain sharp finite sample performance bounds for our estimator,\nincluding deterministic deviation bounds and probabilistic guarantees. We\ndemonstrate the effectiveness of our approach through multiple simulation\nstudies, as well as by application to real data from an EMR study conducted at\nthe Partners HealthCare Systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:51:27 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 17:59:30 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 01:09:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Neykov", "Matey", ""], ["Carroll", "Raymond", ""], ["Cai", "Tianxi", ""]]}, {"id": "1701.05265", "submitter": "Wilson Hsu", "authors": "Wilson Hsu, Agastya Kalra, Pascal Poupart", "title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-product networks have recently emerged as an attractive representation\ndue to their dual view as a special type of deep neural network with clear\nsemantics and a special type of probabilistic graphical model for which\ninference is always tractable. Those properties follow from some conditions\n(i.e., completeness and decomposability) that must be respected by the\nstructure of the network. As a result, it is not easy to specify a valid\nsum-product network by hand and therefore structure learning techniques are\ntypically used in practice. This paper describes the first online structure\nlearning technique for continuous SPNs with Gaussian leaves. We also introduce\nan accompanying new parameter learning technique.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 00:42:01 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Hsu", "Wilson", ""], ["Kalra", "Agastya", ""], ["Poupart", "Pascal", ""]]}, {"id": "1701.05305", "submitter": "Hemant Ishwaran", "authors": "Fei Tang and Hemant Ishwaran", "title": "Random Forest Missing Data Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) missing data algorithms are an attractive approach for\ndealing with missing data. They have the desirable properties of being able to\nhandle mixed types of missing data, they are adaptive to interactions and\nnonlinearity, and they have the potential to scale to big data settings.\nCurrently there are many different RF imputation algorithms but relatively\nlittle guidance about their efficacy, which motivated us to study their\nperformance. Using a large, diverse collection of data sets, performance of\nvarious RF algorithms was assessed under different missing data mechanisms.\nAlgorithms included proximity imputation, on the fly imputation, and imputation\nutilizing multivariate unsupervised and supervised splitting---the latter class\nrepresenting a generalization of a new promising imputation algorithm called\nmissForest. Performance of algorithms was assessed by ability to impute data\naccurately. Our findings reveal RF imputation to be generally robust with\nperformance improving with increasing correlation. Performance was good under\nmoderate to high missingness, and even (in certain cases) when data was missing\nnot at random.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 05:58:05 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 01:32:56 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tang", "Fei", ""], ["Ishwaran", "Hemant", ""]]}, {"id": "1701.05306", "submitter": "Hemant Ishwaran", "authors": "Min Lu, Saad Sadiq, Daniel J. Feaster, Hemant Ishwaran", "title": "Estimating Individual Treatment Effect in Observational Data Using\n  Random Forest Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of individual treatment effect in observational data is\ncomplicated due to the challenges of confounding and selection bias. A useful\ninferential framework to address this is the counterfactual (potential\noutcomes) model which takes the hypothetical stance of asking what if an\nindividual had received both treatments. Making use of random forests (RF)\nwithin the counterfactual framework we estimate individual treatment effects by\ndirectly modeling the response. We find accurate estimation of individual\ntreatment effects is possible even in complex heterogeneous settings but that\nthe type of RF approach plays an important role in accuracy. Methods designed\nto be adaptive to confounding, when used in parallel with out-of-sample\nestimation, do best. One method found to be especially promising is\ncounterfactual synthetic forests. We illustrate this new methodology by\napplying it to a large comparative effectiveness trial, Project Aware, in order\nto explore the role drug use plays in sexual risk. The analysis reveals\nimportant connections between risky behavior, drug usage, and sexual risk.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 06:11:14 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 12:11:28 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Lu", "Min", ""], ["Sadiq", "Saad", ""], ["Feaster", "Daniel J.", ""], ["Ishwaran", "Hemant", ""]]}, {"id": "1701.05335", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Validity of Clusters Produced By kernel-$k$-means With Kernel-Trick", "comments": "27 pages", "journal-ref": "an extension of the paper in Foundations of Intelligent Systems.,\n  LNCS vol 10352. Springer, Cham, pp. 97-104 (2017)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper corrects the proof of the Theorem 2 from the Gower's paper\n\\cite[page 5]{Gower:1982} as well as corrects the Theorem 7 from Gower's paper\n\\cite{Gower:1986}. The first correction is needed in order to establish the\nexistence of the kernel function used commonly in the kernel trick e.g. for\n$k$-means clustering algorithm, on the grounds of distance matrix. The\ncorrection encompasses the missing if-part proof and dropping unnecessary\nconditions. The second correction deals with transformation of the kernel\nmatrix into a one embeddable in Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 08:55:20 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 11:59:52 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 12:00:27 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1701.05363", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth), Bertrand\n  Thirion (PARIETAL, NEUROSPIN), Gael Varoquaux (NEUROSPIN, PARIETAL)", "title": "Stochastic Subsampling for Factorizing Huge Matrices", "comments": "IEEE Transactions on Signal Processing, Institute of Electrical and\n  Electronics Engineers, A Para\\^itre", "journal-ref": "IEEE Transactions on Signal Processing, 2018, 66 (1), pp 113-128", "doi": "10.1109/TSP.2017.2752697", "report-no": null, "categories": "stat.ML cs.LG math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:35:01 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:29:34 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:24:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL, NEUROSPIN"], ["Mairal", "Julien", "", "Thoth"], ["Thirion", "Bertrand", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Gael", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1701.05369", "submitter": "Dmitry Molchanov", "authors": "Dmitry Molchanov, Arsenii Ashukha and Dmitry Vetrov", "title": "Variational Dropout Sparsifies Deep Neural Networks", "comments": "Published in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a recently proposed Variational Dropout technique that provided an\nelegant Bayesian interpretation to Gaussian Dropout. We extend Variational\nDropout to the case when dropout rates are unbounded, propose a way to reduce\nthe variance of the gradient estimator and report first experimental results\nwith individual dropout rates per weight. Interestingly, it leads to extremely\nsparse solutions both in fully-connected and convolutional layers. This effect\nis similar to automatic relevance determination effect in empirical Bayes but\nhas a number of advantages. We reduce the number of parameters up to 280 times\non LeNet architectures and up to 68 times on VGG-like networks with a\nnegligible decrease of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:44:55 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 20:43:27 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 11:01:55 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Molchanov", "Dmitry", ""], ["Ashukha", "Arsenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1701.05458", "submitter": "Rym Worms", "authors": "Julien Worms (LM-Versailles), Rym Worms (LAMA)", "title": "Extreme value statistics for censored data with heavy tails under\n  competing risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating, in the presence of random\ncensoring as well as competing risks, the extreme value index of the\n(sub)-distribution function associated to one particular cause, in the\nheavy-tail case. Asymptotic normality of the proposed estimator (which has the\nform of an Aalen-Johansen integral, and is the first estimator proposed in this\ncontext) is established. A small simulation study exhibits its performances for\nfinite samples. Estimation of extreme quantiles of the cumulative incidence\nfunction is also addressed.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:08:20 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Worms", "Julien", "", "LM-Versailles"], ["Worms", "Rym", "", "LAMA"]]}, {"id": "1701.05512", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Fisher consistency for prior probability shift", "comments": "28 pages, 2 figures, 8 tables, introduction extended", "journal-ref": "Journal of Machine Learning Research 18, 3338-3369, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:07:21 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 20:04:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1701.05517", "submitter": "Tim Salimans", "authors": "Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma", "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture\n  Likelihood and Other Modifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PixelCNNs are a recently proposed class of powerful generative models with\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\nmake available at https://github.com/openai/pixel-cnn. Our implementation\ncontains a number of modifications to the original model that both simplify its\nstructure and improve its performance. 1) We use a discretized logistic mixture\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\nup training. 2) We condition on whole pixels, rather than R/G/B sub-pixels,\nsimplifying the model structure. 3) We use downsampling to efficiently capture\nstructure at multiple resolutions. 4) We introduce additional short-cut\nconnections to further speed up optimization. 5) We regularize the model using\ndropout. Finally, we present state-of-the-art log likelihood results on\nCIFAR-10 to demonstrate the usefulness of these modifications.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:29:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Salimans", "Tim", ""], ["Karpathy", "Andrej", ""], ["Chen", "Xi", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "1701.05573", "submitter": "Aaron Schein", "authors": "Aaron Schein, Mingyuan Zhou, Hanna Wallach", "title": "Poisson--Gamma Dynamical Systems", "comments": "Appeared in the Proceedings of the 29th Advances in Neural\n  Information Processing Systems (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dynamical system for sequentially observed multivariate\ncount data. This model is based on the gamma--Poisson construction---a natural\nchoice for count data---and relies on a novel Bayesian nonparametric prior that\nties and shrinks the model parameters, thus avoiding overfitting. We present an\nefficient MCMC inference algorithm that advances recent work on augmentation\nschemes for inference in negative binomial models. Finally, we demonstrate the\nmodel's inductive bias using a variety of real-world data sets, showing that it\nexhibits superior predictive performance over other models and infers highly\ninterpretable latent structure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 19:28:37 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Schein", "Aaron", ""], ["Zhou", "Mingyuan", ""], ["Wallach", "Hanna", ""]]}, {"id": "1701.05593", "submitter": "Peyman Tavallali", "authors": "Peyman Tavallali, Marianne Razavi, Sean Brady", "title": "Parameter Selection Algorithm For Continuous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new algorithm for supervised learning methods,\nby which one can both capture the non-linearity in data and also find the best\nsubset model. To produce an enhanced subset of the original variables, an ideal\nselection method should have the potential of adding a supplementary level of\nregression analysis that would capture complex relationships in the data via\nmathematical transformation of the predictors and exploration of synergistic\neffects of combined variables. The method that we present here has the\npotential to produce an optimal subset of variables, rendering the overall\nprocess of model selection to be more efficient. The core objective of this\npaper is to introduce a new estimation technique for the classical least square\nregression framework. This new automatic variable transformation and model\nselection method could offer an optimal and stable model that minimizes the\nmean square error and variability, while combining all possible subset\nselection methodology and including variable transformations and interaction.\nMoreover, this novel method controls multicollinearity, leading to an optimal\nset of explanatory variables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:35:31 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tavallali", "Peyman", ""], ["Razavi", "Marianne", ""], ["Brady", "Sean", ""]]}, {"id": "1701.05632", "submitter": "Simon Angus", "authors": "Klaus Ackermann, Simon D Angus, Paul A Raschky", "title": "The Internet as Quantitative Social Science Platform: Insights from a\n  Trillion Observations", "comments": "40 pages, including 4 main figures, and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC cs.CY cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large-scale penetration of the internet, for the first time,\nhumanity has become linked by a single, open, communications platform.\nHarnessing this fact, we report insights arising from a unified internet\nactivity and location dataset of an unparalleled scope and accuracy drawn from\nover a trillion (1.5$\\times 10^{12}$) observations of end-user internet\nconnections, with temporal resolution of just 15min over 2006-2012. We first\napply this dataset to the expansion of the internet itself over 1,647 urban\nagglomerations globally. We find that unique IP per capita counts reach\nsaturation at approximately one IP per three people, and take, on average, 16.1\nyears to achieve; eclipsing the estimated 100- and 60- year saturation times\nfor steam-power and electrification respectively. Next, we use intra-diurnal\ninternet activity features to up-scale traditional over-night sleep\nobservations, producing the first global estimate of over-night sleep duration\nin 645 cities over 7 years. We find statistically significant variation between\ncontinental, national and regional sleep durations including some evidence of\nglobal sleep duration convergence. Finally, we estimate the relationship\nbetween internet concentration and economic outcomes in 411 OECD regions and\nfind that the internet's expansion is associated with negative or positive\nproductivity gains, depending strongly on sectoral considerations. To our\nknowledge, our study is the first of its kind to use online/offline activity of\nthe entire internet to infer social science insights, demonstrating the\nunparalleled potential of the internet as a social data-science platform.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 22:35:46 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Ackermann", "Klaus", ""], ["Angus", "Simon D", ""], ["Raschky", "Paul A", ""]]}, {"id": "1701.05644", "submitter": "Yunlong Wang", "authors": "Yong Cai, Yunlong Wang, Dong Dai", "title": "Rare Disease Physician Targeting: A Factor Graph Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In rare disease physician targeting, a major challenge is how to identify\nphysicians who are treating diagnosed or underdiagnosed rare diseases patients.\nRare diseases have extremely low incidence rate. For a specified rare disease,\nonly a small number of patients are affected and a fractional of physicians are\ninvolved. The existing targeting methodologies, such as segmentation and\nprofiling, are developed under mass market assumption. They are not suitable\nfor rare disease market where the target classes are extremely imbalanced. The\nauthors propose a graphical model approach to predict targets by jointly\nmodeling physician and patient features from different data spaces and\nutilizing the extra relational information. Through an empirical example with\nmedical claim and prescription data, the proposed approach demonstrates better\naccuracy in finding target physicians. The graph representation also provides\nvisual interpretability of relationship among physicians and patients. The\nmodel can be extended to incorporate more complex dependency structures. This\narticle contributes to the literature of exploring the benefit of utilizing\nrelational dependencies among entities in healthcare industry.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 23:43:54 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Cai", "Yong", ""], ["Wang", "Yunlong", ""], ["Dai", "Dong", ""]]}, {"id": "1701.05654", "submitter": "Young Woong Park", "authors": "Young Woong Park, Diego Klabjan", "title": "Bayesian Network Learning via Topological Order", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(99) 1-32, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixed integer programming (MIP) model and iterative algorithms\nbased on topological orders to solve optimization problems with acyclic\nconstraints on a directed graph. The proposed MIP model has a significantly\nlower number of constraints compared to popular MIP models based on cycle\nelimination constraints and triangular inequalities. The proposed iterative\nalgorithms use gradient descent and iterative reordering approaches,\nrespectively, for searching topological orders. A computational experiment is\npresented for the Gaussian Bayesian network learning problem, an optimization\nproblem minimizing the sum of squared errors of regression models with L1\npenalty over a feature network with application of gene network inference in\nbioinformatics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 01:58:33 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 21:19:02 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1701.05672", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Guang Cheng and Yufeng Liu", "title": "Stability Enhanced Large-Margin Classifier Selection", "comments": "38 pages. To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is an important aspect of a classification procedure because\nunstable predictions can potentially reduce users' trust in a classification\nsystem and also harm the reproducibility of scientific conclusions. The major\ngoal of our work is to introduce a novel concept of classification instability,\ni.e., decision boundary instability (DBI), and incorporate it with the\ngeneralization error (GE) as a standard for selecting the most accurate and\nstable classifier. Specifically, we implement a two-stage algorithm: (i)\ninitially select a subset of classifiers whose estimated GEs are not\nsignificantly different from the minimal estimated GE among all the candidate\nclassifiers; (ii) the optimal classifier is chosen as the one achieving the\nminimal DBI among the subset selected in stage (i). This general selection\nprinciple applies to both linear and nonlinear classifiers. Large-margin\nclassifiers are used as a prototypical example to illustrate the above idea.\nOur selection method is shown to be consistent in the sense that the optimal\nclassifier simultaneously achieves the minimal GE and the minimal DBI. Various\nsimulations and real examples further demonstrate the advantage of our method\nover several alternative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 03:38:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Sun", "Will Wei", ""], ["Cheng", "Guang", ""], ["Liu", "Yufeng", ""]]}, {"id": "1701.05763", "submitter": "Jussi Korpela", "authors": "Jussi Korpela and Emilia Oikarinen and Kai Puolam\\\"aki and Antti\n  Ukkonen", "title": "Multivariate Confidence Intervals", "comments": "A short version of this paper appeared in the 2017 SIAM International\n  Conference on Data Mining, SDM'17. This extended version contains proofs of\n  theorems in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals are a popular way to visualize and analyze data\ndistributions. Unlike p-values, they can convey information both about\nstatistical significance as well as effect size. However, very little work\nexists on applying confidence intervals to multivariate data. In this paper we\ndefine confidence intervals for multivariate data that extend the\none-dimensional definition in a natural way. In our definition every variable\nis associated with its own confidence interval as usual, but a data vector can\nbe outside of a few of these, and still be considered to be within the\nconfidence area. We analyze the problem and show that the resulting confidence\nareas retain the good qualities of their one-dimensional counterparts: they are\ninformative and easy to interpret. Furthermore, we show that the problem of\nfinding multivariate confidence intervals is hard, but provide efficient\napproximate algorithms to solve the problem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 11:19:50 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Korpela", "Jussi", ""], ["Oikarinen", "Emilia", ""], ["Puolam\u00e4ki", "Kai", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1701.05804", "submitter": "Daniele Tantari", "authors": "Paolo Barucca, Fabrizio Lillo, Piero Mazzarisi, Daniele Tantari", "title": "Disentangling group and link persistence in Dynamic Stochastic Block\n  models", "comments": "13 pages, 8 figures; Final Section added; figures updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 14:33:45 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:40:17 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 17:52:52 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 17:53:42 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Barucca", "Paolo", ""], ["Lillo", "Fabrizio", ""], ["Mazzarisi", "Piero", ""], ["Tantari", "Daniele", ""]]}, {"id": "1701.05923", "submitter": "Fathi Salem", "authors": "Rahul Dey and Fathi M. Salem", "title": "Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks", "comments": "5 pages, 8 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in\nrecurrent neural networks (RNN) by reducing parameters in the update and reset\ngates. We evaluate the three variant GRU models on MNIST and IMDB datasets and\nshow that these GRU-RNN variant models perform as well as the original GRU RNN\nmodel while reducing the computational expense.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 20:53:51 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Dey", "Rahul", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.05927", "submitter": "Luke de Oliveira", "authors": "Luke de Oliveira, Michela Paganini, and Benjamin Nachman", "title": "Learning Particle Physics by Example: Location-Aware Generative\n  Adversarial Networks for Physics Synthesis", "comments": "23 pages, 23 figures, 1 table, and appendix; Added new validation\n  metric, acknowledgements, minor corrections", "journal-ref": "Comput Softw Big Sci (2017) 1: 4", "doi": "10.1007/s41781-017-0004-6", "report-no": null, "categories": "stat.ML hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a bridge between generative modeling in the Machine Learning\ncommunity and simulated physical processes in High Energy Particle Physics by\napplying a novel Generative Adversarial Network (GAN) architecture to the\nproduction of jet images -- 2D representations of energy depositions from\nparticles interacting with a calorimeter. We propose a simple architecture, the\nLocation-Aware Generative Adversarial Network, that learns to produce realistic\nradiation patterns from simulated high energy particle collisions. The pixel\nintensities of GAN-generated images faithfully span over many orders of\nmagnitude and exhibit the desired low-dimensional physical properties (i.e.,\njet mass, n-subjettiness, etc.). We shed light on limitations, and provide a\nnovel empirical validation of image quality and validity of GAN-produced\nsimulations of the natural world. This work provides a base for further\nexplorations of GANs for use in faster simulation in High Energy Particle\nPhysics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 21:09:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:50:04 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["de Oliveira", "Luke", ""], ["Paganini", "Michela", ""], ["Nachman", "Benjamin", ""]]}, {"id": "1701.05936", "submitter": "Yaohui Zeng", "authors": "Yaohui Zeng, Patrick Breheny", "title": "The biglasso Package: A Memory- and Computation-Efficient Solver for\n  Lasso Model Fitting with Big Data in R", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression models such as the lasso have been extensively applied\nto analyzing high-dimensional data sets. However, due to memory limitations,\nexisting R packages like glmnet and ncvreg are not capable of fitting\nlasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are\nincreasingly seen in many areas such as genetics, genomics, biomedical imaging,\nand high-frequency finance. In this research, we implement an R package called\nbiglasso that tackles this challenge. biglasso utilizes memory-mapped files to\nstore the massive data on the disk, only reading data into memory when\nnecessary during model fitting, and is thus able to handle out-of-core\ncomputation seamlessly. Moreover, it's equipped with newly proposed, more\nefficient feature screening rules, which substantially accelerate the\ncomputation. Benchmarking experiments show that our biglasso package, as\ncompared to existing popular ones like glmnet, is much more memory- and\ncomputation-efficient. We further analyze a 31 GB real data set on a laptop\nwith only 16 GB RAM to demonstrate the out-of-core computation capability of\nbiglasso in analyzing massive data sets that cannot be accommodated by existing\nR packages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 22:08:49 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 23:29:42 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zeng", "Yaohui", ""], ["Breheny", "Patrick", ""]]}, {"id": "1701.05954", "submitter": "Ioannis Paschalidis", "authors": "Manjesh K. Hanawal, Hao Liu, Henghui Zhu, Ioannis Ch. Paschalidis", "title": "Learning Policies for Markov Decision Processes from Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a policy for a Markov decision process\nconsistent with data captured on the state-actions pairs followed by the\npolicy. We assume that the policy belongs to a class of parameterized policies\nwhich are defined using features associated with the state-action pairs. The\nfeatures are known a priori, however, only an unknown subset of them could be\nrelevant. The policy parameters that correspond to an observed target policy\nare recovered using $\\ell_1$-regularized logistic regression that best fits the\nobserved state-action samples. We establish bounds on the difference between\nthe average reward of the estimated and the original policy (regret) in terms\nof the generalization error and the ergodic coefficient of the underlying\nMarkov chain. To that end, we combine sample complexity theory and sensitivity\nanalysis of the stationary distribution of Markov chains. Our analysis suggests\nthat to achieve regret within order $O(\\sqrt{\\epsilon})$, it suffices to use\ntraining sample size on the order of $\\Omega(\\log n \\cdot poly(1/\\epsilon))$,\nwhere $n$ is the number of the features. We demonstrate the effectiveness of\nour method on a synthetic robot navigation example.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 00:11:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hanawal", "Manjesh K.", ""], ["Liu", "Hao", ""], ["Zhu", "Henghui", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1701.06084", "submitter": "Yuheng Bu", "authors": "Yuheng Bu, Shaofeng Zou, Venugopal V. Veeravalli", "title": "Linear-Complexity Exponentially-Consistent Tests for Universal Outlying\n  Sequence Detection", "comments": "Double-column 12-page version sent to IEEE. Transaction on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of universal outlying sequence detection is studied, where the\ngoal is to detect outlying sequences among $M$ sequences of samples. A sequence\nis considered as outlying if the observations therein are generated by a\ndistribution different from those generating the observations in the majority\nof the sequences. In the universal setting, we are interested in identifying\nall the outlying sequences without knowing the underlying generating\ndistributions. In this paper, a class of tests based on distribution clustering\nis proposed. These tests are shown to be exponentially consistent with linear\ntime complexity in $M$. Numerical results demonstrate that our clustering-based\ntests achieve similar performance to existing tests, while being considerably\nmore computationally efficient.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 20:52:43 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 18:00:06 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 21:28:57 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 18:54:22 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bu", "Yuheng", ""], ["Zou", "Shaofeng", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1701.06106", "submitter": "Sahil Garg", "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "title": "Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 00:35:24 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 08:15:55 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Garg", "Sahil", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1701.06120", "submitter": "Zhongnan Zhang", "authors": "Tingxi Wen, Zhongnan Zhang", "title": "Effective and Extensible Feature Extraction Method Using Genetic\n  Algorithm-Based Frequency-Domain Feature Search for Epileptic EEG\n  Multi-classification", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a genetic algorithm-based frequency-domain feature search\n(GAFDS) method is proposed for the electroencephalogram (EEG) analysis of\nepilepsy. In this method, frequency-domain features are first searched and then\ncombined with nonlinear features. Subsequently, these features are selected and\noptimized to classify EEG signals. The extracted features are analyzed\nexperimentally. The features extracted by GAFDS show remarkable independence,\nand they are superior to the nonlinear features in terms of the ratio of\ninter-class distance and intra-class distance. Moreover, the proposed feature\nsearch method can additionally search for features of instantaneous frequency\nin a signal after Hilbert transformation. The classification results achieved\nusing these features are reasonable, thus, GAFDS exhibits good extensibility.\nMultiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant\nanalysis, decision tree, AdaBoost, multilayer perceptron, and Na\\\"ive Bayes)\nachieve good results by using the features generated by GAFDS method and the\noptimized selection. Specifically, the accuracies for the two-classification\nand three-classification problems may reach up to 99% and 97%, respectively.\nResults of several cross-validation experiments illustrate that GAFDS is\neffective in feature extraction for EEG classification. Therefore, the proposed\nfeature selection and optimization model can improve classification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 04:20:52 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wen", "Tingxi", ""], ["Zhang", "Zhongnan", ""]]}, {"id": "1701.06225", "submitter": "Omar Montasser", "authors": "Omar Montasser and Daniel Kifer", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged\n  Tweets", "comments": "6 pages, AAAI-17 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of predicting demographics of\ngeographic units given geotagged Tweets that are composed within these units.\nTraditional survey methods that offer demographics estimates are usually\nlimited in terms of geographic resolution, geographic boundaries, and time\nintervals. Thus, it would be highly useful to develop computational methods\nthat can complement traditional survey methods by offering demographics\nestimates at finer geographic resolutions, with flexible geographic boundaries\n(i.e. not confined to administrative boundaries), and at different time\nintervals. While prior work has focused on predicting demographics and health\nstatistics at relatively coarse geographic resolutions such as the county-level\nor state-level, we introduce an approach to predict demographics at finer\ngeographic resolutions such as the blockgroup-level. For the task of predicting\ngender and race/ethnicity counts at the blockgroup-level, an approach adapted\nfrom prior work to our problem achieves an average correlation of 0.389\n(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms\nthis prior approach with an average correlation of 0.671 (gender) and 0.692\n(race).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 22:16:46 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Montasser", "Omar", ""], ["Kifer", "Daniel", ""]]}, {"id": "1701.06279", "submitter": "Patrick Ng", "authors": "Patrick Ng", "title": "dna2vec: Consistent vector representations of variable-length k-mers", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 07:21:43 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ng", "Patrick", ""]]}, {"id": "1701.06421", "submitter": "Emilie Poisson Caillault", "authors": "Thi-Thu-Hong Phan (LISIC), Emilie Poisson Caillault (LISIC), Andr\\'e\n  Bigand (LISIC)", "title": "Comparative study on supervised learning methods for identifying\n  phytoplankton species", "comments": null, "journal-ref": "2016 IEEE Sixth International Conference on Communications and\n  Electronics (ICCE), Jul 2016, Ha Long, Vietnam. pp.283 - 288, 2016", "doi": "10.1109/CCE.2016.7562650", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phytoplankton plays an important role in marine ecosystem. It is defined as a\nbiological factor to assess marine quality. The identification of phytoplankton\nspecies has a high potential for monitoring environmental, climate changes and\nfor evaluating water quality. However, phytoplankton species identification is\nnot an easy task owing to their variability and ambiguity due to thousands of\nmicro and pico-plankton species. Therefore, the aim of this paper is to build a\nframework for identifying phytoplankton species and to perform a comparison on\ndifferent features types and classifiers. We propose a new features type\nextracted from raw signals of phytoplankton species. We then analyze the\nperformance of various classifiers on the proposed features type as well as two\nother features types for finding the robust one. Through experiments, it is\nfound that Random Forest using the proposed features gives the best\nclassification results with average accuracy up to 98.24%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 14:45:20 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Phan", "Thi-Thu-Hong", "", "LISIC"], ["Caillault", "Emilie Poisson", "", "LISIC"], ["Bigand", "Andr\u00e9", "", "LISIC"]]}, {"id": "1701.06450", "submitter": "Andrea Baisero", "authors": "Andrea Baisero, Stefan Otte, Peter Englert and Marc Toussaint", "title": "Identification of Unmodeled Objects from Symbolic Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful human-robot cooperation hinges on each agent's ability to process\nand exchange information about the shared environment and the task at hand.\nHuman communication is primarily based on symbolic abstractions of object\nproperties, rather than precise quantitative measures. A comprehensive robotic\nframework thus requires an integrated communication module which is able to\nestablish a link and convert between perceptual and abstract information.\n  The ability to interpret composite symbolic descriptions enables an\nautonomous agent to a) operate in unstructured and cluttered environments, in\ntasks which involve unmodeled or never seen before objects; and b) exploit the\naggregation of multiple symbolic properties as an instance of ensemble\nlearning, to improve identification performance even when the individual\npredicates encode generic information or are imprecisely grounded.\n  We propose a discriminative probabilistic model which interprets symbolic\ndescriptions to identify the referent object contextually w.r.t.\\ the structure\nof the environment and other objects. The model is trained using a collected\ndataset of identifications, and its performance is evaluated by quantitative\nmeasures and a live demo developed on the PR2 robot platform, which integrates\nelements of perception, object extraction, object identification and grasping.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:26:01 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Baisero", "Andrea", ""], ["Otte", "Stefan", ""], ["Englert", "Peter", ""], ["Toussaint", "Marc", ""]]}, {"id": "1701.06452", "submitter": "Giovanni Montana", "authors": "Petros-Pavlos Ypsilantis and Giovanni Montana", "title": "Learning what to look in chest X-rays with a recurrent visual attention\n  model", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:29:47 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ypsilantis", "Petros-Pavlos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1701.06508", "submitter": "Alexander Gates", "authors": "Alexander J Gates and Yong-Yeol Ahn", "title": "The Impact of Random Models on Clustering Similarity", "comments": "28 pages, 5 figures", "journal-ref": "Journal of Machine Learning Research 18 (2017) 87,1-28", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a central approach for unsupervised learning. After clustering\nis applied, the most fundamental analysis is to quantitatively compare\nclusterings. Such comparisons are crucial for the evaluation of clustering\nmethods as well as other tasks such as consensus clustering. It is often argued\nthat, in order to establish a baseline, clustering similarity should be\nassessed in the context of a random ensemble of clusterings. The prevailing\nassumption for the random clustering ensemble is the permutation model in which\nthe number and sizes of clusters are fixed. However, this assumption does not\nnecessarily hold in practice; for example, multiple runs of K-means clustering\nreturns clusterings with a fixed number of clusters, while the cluster size\ndistribution varies greatly. Here, we derive corrected variants of two\nclustering similarity measures (the Rand index and Mutual Information) in the\ncontext of two random clustering ensembles in which the number and sizes of\nclusters vary. In addition, we study the impact of one-sided comparisons in the\nscenario with a reference clustering. The consequences of different random\nmodels are illustrated using synthetic examples, handwriting recognition, and\ngene expression data. We demonstrate that the choice of random model can have a\ndrastic impact on the ranking of similar clustering pairs, and the evaluation\nof a clustering method with respect to a random baseline; thus, the choice of\nrandom clustering model should be carefully justified.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:09:34 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 15:05:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Gates", "Alexander J", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "1701.06511", "submitter": "Bikash Joshi", "authors": "Bikash Joshi, Massih-Reza Amini, Ioannis Partalas, Franck Iutzeler,\n  Yury Maximov", "title": "Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:14:02 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 22:02:17 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 09:34:40 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Joshi", "Bikash", ""], ["Amini", "Massih-Reza", ""], ["Partalas", "Ioannis", ""], ["Iutzeler", "Franck", ""], ["Maximov", "Yury", ""]]}, {"id": "1701.06538", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\n  Le, Geoffrey Hinton, Jeff Dean", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shazeer", "Noam", ""], ["Mirhoseini", "Azalia", ""], ["Maziarz", "Krzysztof", ""], ["Davis", "Andy", ""], ["Le", "Quoc", ""], ["Hinton", "Geoffrey", ""], ["Dean", "Jeff", ""]]}, {"id": "1701.06597", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani, Chinmay Hegde", "title": "Iterative Thresholding for Demixing Structured Superpositions in High\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the demixing problem of two (or more) high-dimensional vectors\nfrom nonlinear observations when the number of such observations is far less\nthan the ambient dimension of the underlying vectors. Specifically, we\ndemonstrate an algorithm that stably estimate the underlying components under\ngeneral \\emph{structured sparsity} assumptions on these components.\nSpecifically, we show that for certain types of structured superposition\nmodels, our method provably recovers the components given merely $n =\n\\mathcal{O}(s)$ samples where $s$ denotes the number of nonzero entries in the\nunderlying components. Moreover, our method achieves a fast (linear)\nconvergence rate, and also exhibits fast (near-linear) per-iteration complexity\nfor certain types of structured models. We also provide a range of simulations\nto illustrate the performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 19:28:30 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1701.06607", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani, Chinmay Hegde", "title": "Stable Recovery Of Sparse Vectors From Random Sinusoidal Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sinusoidal features are a popular approach for speeding up\nkernel-based inference in large datasets. Prior to the inference stage, the\napproach suggests performing dimensionality reduction by first multiplying each\ndata vector by a random Gaussian matrix, and then computing an element-wise\nsinusoid. Theoretical analysis shows that collecting a sufficient number of\nsuch features can be reliably used for subsequent inference in kernel\nclassification and regression.\n  In this work, we demonstrate that with a mild increase in the dimension of\nthe embedding, it is also possible to reconstruct the data vector from such\nrandom sinusoidal features, provided that the underlying data is sparse enough.\nIn particular, we propose a numerically stable algorithm for reconstructing the\ndata vector given the nonlinear features, and analyze its sample complexity.\nOur algorithm can be extended to other types of structured inverse problems,\nsuch as demixing a pair of sparse (but incoherent) vectors. We support the\nefficacy of our approach via numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 19:55:50 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:10:40 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1701.06649", "submitter": "David Yaron", "authors": "Christopher R. Collins, Geoffrey J. Gordon, O. Anatole von Lilienfeld,\n  David J. Yaron", "title": "Constant Size Molecular Descriptors For Use With Machine Learning", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of molecular descriptors whose length is independent of molecular size\nis developed for machine learning models that target thermodynamic and\nelectronic properties of molecules. These features are evaluated by monitoring\nperformance of kernel ridge regression models on well-studied data sets of\nsmall organic molecules. The features include connectivity counts, which\nrequire only the bonding pattern of the molecule, and encoded distances, which\nsummarize distances between both bonded and non-bonded atoms and so require the\nfull molecular geometry. In addition to having constant size, these features\nsummarize information regarding the local environment of atoms and bonds, such\nthat models can take advantage of similarities resulting from the presence of\nsimilar chemical fragments across molecules. Combining these two types of\nfeatures leads to models whose performance is comparable to or better than the\ncurrent state of the art. The features introduced here have the advantage of\nleading to models that may be trained on smaller molecules and then used\nsuccessfully on larger molecules.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 22:03:08 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Collins", "Christopher R.", ""], ["Gordon", "Geoffrey J.", ""], ["von Lilienfeld", "O. Anatole", ""], ["Yaron", "David J.", ""]]}, {"id": "1701.06655", "submitter": "Chiwoo Park", "authors": "Chiwoo Park and Daniel Apley", "title": "Patchwork Kriging for Large-scale Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for Gaussian process (GP) regression for\nlarge datasets. The approach involves partitioning the regression input domain\ninto multiple local regions with a different local GP model fitted in each\nregion. Unlike existing local partitioned GP approaches, we introduce a\ntechnique for patching together the local GP models nearly seamlessly to ensure\nthat the local GP models for two neighboring regions produce nearly the same\nresponse prediction and prediction error variance on the boundary between the\ntwo regions. This largely mitigates the well-known discontinuity problem that\ndegrades the boundary accuracy of existing local partitioned GP methods. Our\nmain innovation is to represent the continuity conditions as additional\npseudo-observations that the differences between neighboring GP responses are\nidentically zero at an appropriately chosen set of boundary input locations. To\npredict the response at any input location, we simply augment the actual\nresponse observations with the pseudo-observations and apply standard GP\nprediction methods to the augmented data. In contrast to heuristic continuity\nadjustments, this has an advantage of working within a formal GP framework, so\nthat the GP-based predictive uncertainty quantification remains valid. Our\napproach also inherits a sparse block-like structure for the sample covariance\nmatrix, which results in computationally efficient closed-form expressions for\nthe predictive mean and variance. In addition, we provide a new spatial\npartitioning scheme based on a recursive space partitioning along local\nprincipal component directions, which makes the proposed approach applicable\nfor regression domains having more than two dimensions. Using three spatial\ndatasets and three higher dimensional datasets, we investigate the numerical\nperformance of the approach and compare it to several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 22:20:47 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 19:40:22 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 19:17:16 GMT"}, {"version": "v4", "created": "Sat, 7 Jul 2018 16:55:04 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Park", "Chiwoo", ""], ["Apley", "Daniel", ""]]}, {"id": "1701.06675", "submitter": "Melissa Aczon", "authors": "M Aczon, D Ledbetter, L Ho, A Gunny, A Flynn, J Williams, R Wetzel", "title": "Dynamic Mortality Risk Predictions in Pediatric Critical Care Using\n  Recurrent Neural Networks", "comments": "18 pages (5 of which in appendix), 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE math.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewing the trajectory of a patient as a dynamical system, a recurrent neural\nnetwork was developed to learn the course of patient encounters in the\nPediatric Intensive Care Unit (PICU) of a major tertiary care center. Data\nextracted from Electronic Medical Records (EMR) of about 12000 patients who\nwere admitted to the PICU over a period of more than 10 years were leveraged.\nThe RNN model ingests a sequence of measurements which include physiologic\nobservations, laboratory results, administered drugs and interventions, and\ngenerates temporally dynamic predictions for in-ICU mortality at user-specified\ntimes. The RNN's ICU mortality predictions offer significant improvements over\nthose from two clinically-used scores and static machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 23:32:10 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Aczon", "M", ""], ["Ledbetter", "D", ""], ["Ho", "L", ""], ["Gunny", "A", ""], ["Flynn", "A", ""], ["Williams", "J", ""], ["Wetzel", "R", ""]]}, {"id": "1701.06731", "submitter": "Sze Zheng Yong", "authors": "Sze Zheng Yong, Lingyun Gao, Necmiye Ozay", "title": "Weak Adaptive Submodularity and Group-Based Active Diagnosis with\n  Applications to State Estimation with Persistent Sensor Faults", "comments": "To appear in 2017 IEEE American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider adaptive decision-making problems for stochastic\nstate estimation with partial observations. First, we introduce the concept of\nweak adaptive submodularity, a generalization of adaptive submodularity, which\nhas found great success in solving challenging adaptive state estimation\nproblems. Then, for the problem of active diagnosis, i.e., discrete state\nestimation via active sensing, we show that an adaptive greedy policy has a\nnear-optimal performance guarantee when the reward function possesses this\nproperty. We further show that the reward function for group-based active\ndiagnosis, which arises in applications such as medical diagnosis and state\nestimation with persistent sensor faults, is also weakly adaptive submodular.\nFinally, in experiments of state estimation for an aircraft electrical system\nwith persistent sensor faults, we observe that an adaptive greedy policy\nperforms equally well as an exhaustive search.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 04:56:40 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 23:58:47 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Yong", "Sze Zheng", ""], ["Gao", "Lingyun", ""], ["Ozay", "Necmiye", ""]]}, {"id": "1701.06749", "submitter": "Adel Mohammadpour", "authors": "Mahdi Teimouri, Saeid Rezakhah and Adel Mohammdpour", "title": "Robust mixture modelling using sub-Gaussian stable distribution", "comments": "14 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions are widely used in robust mixture modelling due to\npossessing thick tails. As a computationally tractable subclass of the stable\ndistributions, sub-Gaussian $\\alpha$-stable distribution received much interest\nin the literature. Here, we introduce a type of expectation maximization\nalgorithm that estimates parameters of a mixture of sub-Gaussian stable\ndistributions. A comparative study, in the presence of some well-known mixture\nmodels, is performed to show the robustness and performance of the mixture of\nsub-Gaussian $\\alpha$-stable distributions for modelling, simulated, synthetic,\nand real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 06:59:26 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Teimouri", "Mahdi", ""], ["Rezakhah", "Saeid", ""], ["Mohammdpour", "Adel", ""]]}, {"id": "1701.06852", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong, Nikos Deligiannis, Jurgen Seiler, Soren Forchhammer,\n  and Andre Kaup", "title": "Incorporating Prior Information in Compressive Online Robust Principal\n  Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online version of the robust Principle Component Analysis\n(PCA), which arises naturally in time-varying source separations such as video\nforeground-background separation. This paper proposes a compressive online\nrobust PCA with prior information for recursively separating a sequences of\nframes into sparse and low-rank components from a small set of measurements. In\ncontrast to conventional batch-based PCA, which processes all the frames\ndirectly, the proposed method processes measurements taken from each frame.\nMoreover, this method can efficiently incorporate multiple prior information,\nnamely previous reconstructed frames, to improve the separation and thereafter,\nupdate the prior information for the next frame. We utilize multiple prior\ninformation by solving $n\\text{-}\\ell_{1}$ minimization for incorporating the\nprevious sparse components and using incremental singular value decomposition\n($\\mathrm{SVD}$) for exploiting the previous low-rank components. We also\nestablish theoretical bounds on the number of measurements required to\nguarantee successful separation under assumptions of static or slowly-changing\nlow-rank components. Using numerical experiments, we evaluate our bounds and\nthe performance of the proposed algorithm. In addition, we apply the proposed\nalgorithm to online video foreground and background separation from compressive\nmeasurements. Experimental results show that the proposed method outperforms\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 13:02:27 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 14:36:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Van Luong", "Huynh", ""], ["Deligiannis", "Nikos", ""], ["Seiler", "Jurgen", ""], ["Forchhammer", "Soren", ""], ["Kaup", "Andre", ""]]}, {"id": "1701.06981", "submitter": "Andre Manoel", "authors": "Andre Manoel, Florent Krzakala, Marc M\\'ezard and Lenka Zdeborov\\'a", "title": "Multi-Layer Generalized Linear Estimation", "comments": "5 pages, 1 figure", "journal-ref": "IEEE International Symposium on Information Theory (ISIT), pages\n  2098-2102 (2017)", "doi": "10.1109/ISIT.2017.8006899", "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a signal from multi-layered\n(possibly) non-linear measurements. Using non-rigorous but standard methods\nfrom statistical physics we present the Multi-Layer Approximate Message Passing\n(ML-AMP) algorithm for computing marginal probabilities of the corresponding\nestimation problem and derive the associated state evolution equations to\nanalyze its performance. We also give the expression of the asymptotic free\nenergy and the minimal information-theoretically achievable reconstruction\nerror. Finally, we present some applications of this measurement model for\ncompressed sensing and perceptron learning with structured matrices/patterns,\nand for a simple model of estimation of latent variables in an auto-encoder.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 17:01:48 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Manoel", "Andre", ""], ["Krzakala", "Florent", ""], ["M\u00e9zard", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1701.07194", "submitter": "Dacheng Tao", "authors": "Shan You, Chang Xu, Yunhe Wang, Chao Xu, Dacheng Tao", "title": "Privileged Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents privileged multi-label learning (PrML) to explore and\nexploit the relationship between labels in multi-label learning problems. We\nsuggest that for each individual label, it cannot only be implicitly connected\nwith other labels via the low-rank constraint over label predictors, but also\nits performance on examples can receive the explicit comments from other labels\ntogether acting as an \\emph{Oracle teacher}. We generate privileged label\nfeature for each example and its individual label, and then integrate it into\nthe framework of low-rank based multi-label learning. The proposed algorithm\ncan therefore comprehensively explore and exploit label relationships by\ninheriting all the merits of privileged information and low-rank constraints.\nWe show that PrML can be efficiently solved by dual coordinate descent\nalgorithm using iterative optimization strategy with cheap updates. Experiments\non benchmark datasets show that through privileged label features, the\nperformance can be significantly improved and PrML is superior to several\ncompeting methods in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 07:43:13 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["You", "Shan", ""], ["Xu", "Chang", ""], ["Wang", "Yunhe", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1701.07213", "submitter": "David H\\\"ubner", "authors": "D H\\\"ubner, T Verhoeven, K Schmid, K-R M\\\"uller, M Tangermann, P-J\n  Kindermans", "title": "Learning from Label Proportions in Brain-Computer Interfaces: Online\n  Unsupervised Learning with Guarantees", "comments": "The EEG data of 13 subjects is freely available online at:\n  http://doi.org/10.5281/zenodo.192684", "journal-ref": null, "doi": "10.1371/journal.pone.0175856", "report-no": null, "categories": "stat.ML cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Using traditional approaches, a Brain-Computer Interface (BCI)\nrequires the collection of calibration data for new subjects prior to online\nuse. Calibration time can be reduced or eliminated e.g.~by transfer of a\npre-trained classifier or unsupervised adaptive classification methods which\nlearn from scratch and adapt over time. While such heuristics work well in\npractice, none of them can provide theoretical guarantees. Our objective is to\nmodify an event-related potential (ERP) paradigm to work in unison with the\nmachine learning decoder to achieve a reliable calibration-less decoding with a\nguarantee to recover the true class means.\n  Method: We introduce learning from label proportions (LLP) to the BCI\ncommunity as a new unsupervised, and easy-to-implement classification approach\nfor ERP-based BCIs. The LLP estimates the mean target and non-target responses\nbased on known proportions of these two classes in different groups of the\ndata. We modified a visual ERP speller to meet the requirements of the LLP. For\nevaluation, we ran simulations on artificially created data sets and conducted\nan online BCI study with N=13 subjects performing a copy-spelling task.\n  Results: Theoretical considerations show that LLP is guaranteed to minimize\nthe loss function similarly to a corresponding supervised classifier. It\nperformed well in simulations and in the online application, where 84.5% of\ncharacters were spelled correctly on average without prior calibration.\n  Significance: The continuously adapting LLP classifier is the first\nunsupervised decoder for ERP BCIs guaranteed to find the true class means. This\nmakes it an ideal solution to avoid a tedious calibration and to tackle\nnon-stationarities in the data. Additionally, LLP works on complementary\nprinciples compared to existing unsupervised methods, allowing for their\nfurther enhancement when combined with LLP.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 09:09:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["H\u00fcbner", "D", ""], ["Verhoeven", "T", ""], ["Schmid", "K", ""], ["M\u00fcller", "K-R", ""], ["Tangermann", "M", ""], ["Kindermans", "P-J", ""]]}, {"id": "1701.07266", "submitter": "Kfir Levy Yehuda", "authors": "Oren Anava, Kfir Y. Levy", "title": "k*-Nearest Neighbors: From Global to Local", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted k-nearest neighbors algorithm is one of the most fundamental\nnon-parametric methods in pattern recognition and machine learning. The\nquestion of setting the optimal number of neighbors as well as the optimal\nweights has received much attention throughout the years, nevertheless this\nproblem seems to have remained unsettled. In this paper we offer a simple\napproach to locally weighted regression/classification, where we make the\nbias-variance tradeoff explicit. Our formulation enables us to phrase a notion\nof optimal weights, and to efficiently find these weights as well as the\noptimal number of neighbors efficiently and adaptively, for each data point\nwhose value we wish to estimate. The applicability of our approach is\ndemonstrated on several datasets, showing superior performance over standard\nlocally weighted methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 11:18:18 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Anava", "Oren", ""], ["Levy", "Kfir Y.", ""]]}, {"id": "1701.07275", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Universal representations:The missing link between faces, text,\n  planktons, and cat breeds", "comments": "10 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of large labelled datasets and high-capacity models, the\nperformance of machine vision systems has been improving rapidly. However, the\ntechnology has still major limitations, starting from the fact that different\nvision problems are still solved by different models, trained from scratch or\nfine-tuned on the target data. The human visual system, in stark contrast,\nlearns a universal representation for vision in the early life of an\nindividual. This representation works well for an enormous variety of vision\nproblems, with little or no change, with the major advantage of requiring\nlittle training data to solve any of them.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 12:07:15 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1701.07422", "submitter": "Amirhossein Javaheri", "authors": "Amirhossein Javaheri, Hadi Zayyani and Farokh Marvasti", "title": "A Convex Similarity Index for Sparse Recovery of Missing Image Samples", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of recovering missing samples using\nmethods based on sparse representation adapted especially for image signals.\nInstead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality\nmeasure is used as the similarity criterion between the original and the\nreconstructed images. The proposed criterion called Convex SIMilarity (CSIM)\nindex is a modified version of the Structural SIMilarity (SSIM) index, which\ndespite its predecessor, is convex and uni-modal. We derive mathematical\nproperties for the proposed index and show how to optimally choose the\nparameters of the proposed criterion, investigating the Restricted Isometry\n(RIP) and error-sensitivity properties. We also propose an iterative sparse\nrecovery method based on a constrained $l_1$-norm minimization problem,\nincorporating CSIM as the fidelity criterion. The resulting convex optimization\nproblem is solved via an algorithm based on Alternating Direction Method of\nMultipliers (ADMM). Taking advantage of the convexity of the CSIM index, we\nalso prove the convergence of the algorithm to the globally optimal solution of\nthe proposed optimization problem, starting from any arbitrary point.\nSimulation results confirm the performance of the new similarity index as well\nas the proposed algorithm for missing sample recovery of image patch signals.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 18:49:45 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 18:59:41 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 15:17:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Javaheri", "Amirhossein", ""], ["Zayyani", "Hadi", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1701.07429", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707,\n  arXiv:1612.06879", "journal-ref": "Neural Networks 79: 20-36 (2016)", "doi": "10.1016/j.neunet.2016.03.002", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification, and clustering. For regression and cluster\nanalyses of continuous data, MoE usually use normal experts following the\nGaussian distribution. However, for a set of data containing a group or groups\nof observations with heavy tails or atypical observations, the use of normal\nexperts is unsuitable and can unduly affect the fit of the MoE model. We\nintroduce a robust MoE modeling using the $t$ distribution. The proposed $t$\nMoE (TMoE) deals with these issues regarding heavy-tailed and noisy data. We\ndevelop a dedicated expectation-maximization (EM) algorithm to estimate the\nparameters of the proposed model by monotonically maximizing the observed data\nlog-likelihood. We describe how the presented model can be used in prediction\nand in model-based clustering of regression data. The proposed model is\nvalidated on numerical experiments carried out on simulated data, which show\nthe effectiveness and the robustness of the proposed model in terms of modeling\nnon-linear regression functions as well as in model-based clustering. Then, it\nis applied to the real-world data of tone perception for musical data analysis,\nand the one of temperature anomalies for the analysis of climate change data.\nThe obtained results show the usefulness of the TMoE model for practical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 14:42:40 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1701.07474", "submitter": "Zhengping Che", "authors": "Zhengping Che, Yu Cheng, Zhaonan Sun, Yan Liu", "title": "Exploiting Convolutional Neural Network for Risk Prediction with Medical\n  Feature Embedding", "comments": "NIPS 2016 Workshop on Machine Learning for Health (ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread availability of electronic health records (EHRs) promises to\nusher in the era of personalized medicine. However, the problem of extracting\nuseful clinical representations from longitudinal EHR data remains challenging.\nIn this paper, we explore deep neural network models with learned medical\nfeature embedding to deal with the problems of high dimensionality and\ntemporality. Specifically, we use a multi-layer convolutional neural network\n(CNN) to parameterize the model and is thus able to capture complex non-linear\nlongitudinal evolution of EHRs. Our model can effectively capture local/short\ntemporal dependency in EHRs, which is beneficial for risk prediction. To\naccount for high dimensionality, we use the embedding medical features in the\nCNN model which hold the natural medical concepts. Our initial experiments\nproduce promising results and demonstrate the effectiveness of both the medical\nfeature embedding and the proposed convolutional neural network in risk\nprediction on cohorts of congestive heart failure and diabetes patients\ncompared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:25:29 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Che", "Zhengping", ""], ["Cheng", "Yu", ""], ["Sun", "Zhaonan", ""], ["Liu", "Yan", ""]]}, {"id": "1701.07483", "submitter": "Ashwin Venkataraman", "authors": "Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin\n  Venkataraman", "title": "A Model-based Projection Technique for Segmenting Customers", "comments": "51 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:47:40 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jagabathula", "Srikanth", ""], ["Subramanian", "Lakshminarayanan", ""], ["Venkataraman", "Ashwin", ""]]}, {"id": "1701.07681", "submitter": "Patrick Sch\\\"afer", "authors": "Patrick Sch\\\"afer and Ulf Leser", "title": "Fast and Accurate Time Series Classification with WEASEL", "comments": null, "journal-ref": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (CIKM '17). ACM, 637-646", "doi": "10.1145/3132847.3132980", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series (TS) occur in many scientific and commercial applications,\nranging from earth surveillance to industry automation to the smart grids. An\nimportant type of TS analysis is classification, which can, for instance,\nimprove energy load forecasting in smart grids by detecting the types of\nelectronic devices based on their energy consumption profiles recorded by\nautomatic sensors. Such sensor-driven applications are very often characterized\nby (a) very long TS and (b) very large TS datasets needing classification.\nHowever, current methods to time series classification (TSC) cannot cope with\nsuch data volumes at acceptable accuracy; they are either scalable but offer\nonly inferior classification quality, or they achieve state-of-the-art\nclassification quality but cannot scale to large data volumes.\n  In this paper, we present WEASEL (Word ExtrAction for time SEries\ncLassification), a novel TSC method which is both scalable and accurate. Like\nother state-of-the-art TSC methods, WEASEL transforms time series into feature\nvectors, using a sliding-window approach, which are then analyzed through a\nmachine learning classifier. The novelty of WEASEL lies in its specific method\nfor deriving features, resulting in a much smaller yet much more discriminative\nfeature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more\naccurate than the best current non-ensemble algorithms at orders-of-magnitude\nlower classification and training times, and it is almost as accurate as\nensemble classifiers, whose computational complexity makes them inapplicable\neven for mid-size datasets. The outstanding robustness of WEASEL is also\nconfirmed by experiments on two real smart grid datasets, where it\nout-of-the-box achieves almost the same accuracy as highly tuned,\ndomain-specific methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 13:09:48 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Sch\u00e4fer", "Patrick", ""], ["Leser", "Ulf", ""]]}, {"id": "1701.07756", "submitter": "Siwar Jendoubi", "authors": "Siwar Jendoubi, Arnaud Martin, Ludovic Li\\'etard, Boutheina Ben\n  Yaghlane, Hend Ben Hadji", "title": "Dynamic time warping distance for message propagation classification in\n  Twitter", "comments": "10 pages, 1 figure ECSQARU 2015, Proceedings of the 13th European\n  Conferences on Symbolic and Quantitative Approaches to Reasoning with\n  Uncertainty, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-20807-7_38", "report-no": null, "categories": "cs.AI cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social messages classification is a research domain that has attracted the\nattention of many researchers in these last years. Indeed, the social message\nis different from ordinary text because it has some special characteristics\nlike its shortness. Then the development of new approaches for the processing\nof the social message is now essential to make its classification more\nefficient. In this paper, we are mainly interested in the classification of\nsocial messages based on their spreading on online social networks (OSN). We\nproposed a new distance metric based on the Dynamic Time Warping distance and\nwe use it with the probabilistic and the evidential k Nearest Neighbors (k-NN)\nclassifiers to classify propagation networks (PrNets) of messages. The\npropagation network is a directed acyclic graph (DAG) that is used to record\npropagation traces of the message, the traversed links and their types. We\ntested the proposed metric with the chosen k-NN classifiers on real world\npropagation traces that were collected from Twitter social network and we got\ngood classification accuracies.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:14:40 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jendoubi", "Siwar", ""], ["Martin", "Arnaud", ""], ["Li\u00e9tard", "Ludovic", ""], ["Yaghlane", "Boutheina Ben", ""], ["Hadji", "Hend Ben", ""]]}, {"id": "1701.07761", "submitter": "M. Ros\\'ario Oliveira", "authors": "Francisco Macedo and M. Ros\\'ario Oliveira and Ant\\'onio Pacheco and\n  Rui Valadas", "title": "Theoretical Foundations of Forward Feature Selection Methods based on\n  Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection problems arise in a variety of applications, such as\nmicroarray analysis, clinical prediction, text categorization, image\nclassification and face recognition, multi-label learning, and classification\nof internet traffic. Among the various classes of methods, forward feature\nselection methods based on mutual information have become very popular and are\nwidely used in practice. However, comparative evaluations of these methods have\nbeen limited by being based on specific datasets and classifiers. In this\npaper, we develop a theoretical framework that allows evaluating the methods\nbased on their theoretical properties. Our framework is grounded on the\nproperties of the target objective function that the methods try to\napproximate, and on a novel categorization of features, according to their\ncontribution to the explanation of the class; we derive upper and lower bounds\nfor the target objective function and relate these bounds with the feature\ntypes. Then, we characterize the types of approximations taken by the methods,\nand analyze how these approximations cope with the good properties of the\ntarget objective function. Additionally, we develop a distributional setting\ndesigned to illustrate the various deficiencies of the methods, and provide\nseveral examples of wrong feature selections. Based on our work, we identify\nclearly the methods that should be avoided, and the methods that currently have\nthe best performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:23:39 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 16:19:23 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Macedo", "Francisco", ""], ["Oliveira", "M. Ros\u00e1rio", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1701.07767", "submitter": "Konstantinos Slavakis", "authors": "Konstantinos Slavakis and Shiva Salsabilian and David S. Wack and\n  Sarah F. Muldoon and Henry E. Baidoo-Williams and Jean M. Vettel and Matthew\n  Cieslak and Scott T. Grafton", "title": "Riemannian-geometry-based modeling and clustering of network-wide\n  non-stationary time series: The brain-network case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates Riemannian multi-manifold modeling in the context of\nnetwork-wide non-stationary time-series analysis. Time-series data, collected\nsequentially over time and across a network, yield features which are viewed as\npoints in or close to a union of multiple submanifolds of a Riemannian\nmanifold, and distinguishing disparate time series amounts to clustering\nmultiple Riemannian submanifolds. To support the claim that exploiting the\nlatent Riemannian geometry behind many statistical features of time series is\nbeneficial to learning from network data, this paper focuses on brain networks\nand puts forth two feature-generation schemes for network-wide dynamic time\nseries. The first is motivated by Granger-causality arguments and uses an\nauto-regressive moving average model to map low-rank linear vector subspaces,\nspanned by column vectors of appropriately defined observability matrices, to\npoints into the Grassmann manifold. The second utilizes (non-linear)\ndependencies among network nodes by introducing kernel-based partial\ncorrelations to generate points in the manifold of positive-definite matrices.\nCapitilizing on recently developed research on clustering Riemannian\nsubmanifolds, an algorithm is provided for distinguishing time series based on\ntheir geometrical properties, revealed within Riemannian feature spaces.\nExtensive numerical tests demonstrate that the proposed framework outperforms\nclassical and state-of-the-art techniques in clustering brain-network\nstates/structures hidden beneath synthetic fMRI time series and brain-activity\nsignals generated from real brain-network structural connectivity matrices.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:38:00 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Slavakis", "Konstantinos", ""], ["Salsabilian", "Shiva", ""], ["Wack", "David S.", ""], ["Muldoon", "Sarah F.", ""], ["Baidoo-Williams", "Henry E.", ""], ["Vettel", "Jean M.", ""], ["Cieslak", "Matthew", ""], ["Grafton", "Scott T.", ""]]}, {"id": "1701.07808", "submitter": "Chao Qu", "authors": "Chao Qu, Huan Xu", "title": "Linear convergence of SDCA in statistical estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider stochastic dual coordinate (SDCA) {\\em without}\nstrongly convex assumption or convex assumption. We show that SDCA converges\nlinearly under mild conditions termed restricted strong convexity. This covers\na wide array of popular statistical models including Lasso, group Lasso, and\nlogistic regression with $\\ell_1$ regularization, corrected Lasso and linear\nregression with SCAD regularizer. This significantly improves previous\nconvergence results on SDCA for problems that are not strongly convex. As a by\nproduct, we derive a dual free form of SDCA that can handle general\nregularization term, which is of interest by itself.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 18:37:34 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:54:55 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 22:04:09 GMT"}, {"version": "v4", "created": "Sun, 2 Apr 2017 18:43:11 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qu", "Chao", ""], ["Xu", "Huan", ""]]}, {"id": "1701.07875", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, Soumith Chintala, L\\'eon Bottou", "title": "Wasserstein GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:10:29 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 23:12:26 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 20:01:54 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Arjovsky", "Martin", ""], ["Chintala", "Soumith", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1701.07895", "submitter": "Adarsh Barik", "authors": "Adarsh Barik, Jean Honorio, Mohit Tawarmalani", "title": "Information Theoretic Limits for Linear Prediction with Graph-Structured\n  Sparsity", "comments": null, "journal-ref": "2017 IEEE International Symposium on Information Theory (ISIT)", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the necessary number of samples for sparse vector recovery in a\nnoisy linear prediction setup. This model includes problems such as linear\nregression and classification. We focus on structured graph models. In\nparticular, we prove that sufficient number of samples for the weighted graph\nmodel proposed by Hegde and others is also necessary. We use the Fano's\ninequality on well constructed ensembles as our main tool in establishing\ninformation theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 22:43:20 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 23:11:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Barik", "Adarsh", ""], ["Honorio", "Jean", ""], ["Tawarmalani", "Mohit", ""]]}, {"id": "1701.07899", "submitter": "Emilie Devijver", "authors": "Emilie Devijver, M\\'elina Gallopin and Emeline Perthame", "title": "Nonlinear network-based quantitative trait prediction from\n  transcriptomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitatively predicting phenotype variables by the expression changes in a\nset of candidate genes is of great interest in molecular biology but it is also\na challenging task for several reasons. First, the collected biological\nobservations might be heterogeneous and correspond to different biological\nmechanisms. Secondly, the gene expression variables used to predict the\nphenotype are potentially highly correlated since genes interact though unknown\nregulatory networks. In this paper, we present a novel approach designed to\npredict quantitative trait from transcriptomic data, taking into account the\nheterogeneity in biological samples and the hidden gene regulatory networks\nunderlying different biological mechanisms. The proposed model performs well on\nprediction but it is also fully parametric, which facilitates the downstream\nbiological interpretation. The model provides clusters of individuals based on\nthe relation between gene expression data and the phenotype, and also leads to\ninfer a gene regulatory network specific for each cluster of individuals. We\nperform numerical simulations to demonstrate that our model is competitive with\nother prediction models, and we demonstrate the predictive performance and the\ninterpretability of our model to predict alcohol sensitivity from\ntranscriptomic data on real data from Drosophila Melanogaster Genetic Reference\nPanel (DGRP).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 23:05:40 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:53:25 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 13:14:53 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 11:23:30 GMT"}, {"version": "v5", "created": "Thu, 20 Jul 2017 16:45:07 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Devijver", "Emilie", ""], ["Gallopin", "M\u00e9lina", ""], ["Perthame", "Emeline", ""]]}, {"id": "1701.07920", "submitter": "Young Woong Park", "authors": "Young Woong Park, Diego Klabjan", "title": "Subset Selection for Multiple Linear Regression via Optimization", "comments": null, "journal-ref": "Journal of Global Optimization 77-3(2020): 543-574", "doi": "10.1007/s10898-020-00876-1", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection in multiple linear regression aims to choose a subset of\ncandidate explanatory variables that tradeoff fitting error (explanatory power)\nand model complexity (number of variables selected). We build mathematical\nprogramming models for regression subset selection based on mean square and\nabsolute errors, and minimal-redundancy-maximal-relevance criteria. The\nproposed models are tested using a linear-program-based branch-and-bound\nalgorithm with tailored valid inequalities and big M values and are compared\nagainst the algorithms in the literature. For high dimensional cases, an\niterative heuristic algorithm is proposed based on the mathematical programming\nmodels and a core set concept, and a randomized version of the algorithm is\nderived to guarantee convergence to the global optimum. From the computational\nexperiments, we find that our models quickly find a quality solution while the\nrest of the time is spent to prove optimality; the iterative algorithms find\nsolutions in a relatively short time and are competitive compared to\nstate-of-the-art algorithms; using ad-hoc big M values is not recommended.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 01:43:10 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 16:59:04 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1701.07926", "submitter": "Donald Lee", "authors": "Donald K.K. Lee, Ningyuan Chen, Hemant Ishwaran", "title": "Boosted nonparametric hazards with time-dependent covariates", "comments": "33 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given functional data from a survival process with time-dependent covariates,\nwe derive a smooth convex representation for its nonparametric log-likelihood\nfunctional and obtain its functional gradient. From this we devise a generic\ngradient boosting procedure for estimating the hazard function\nnonparametrically. An illustrative implementation of the procedure using\nregression trees is described to show how to recover the unknown hazard. We\nshow that the generic estimator is consistent if the model is correctly\nspecified; alternatively an oracle inequality can be demonstrated for\ntree-based models. To avoid overfitting, boosting employs several\nregularization devices. One of them is step-size restriction, but the rationale\nfor this is somewhat mysterious from the viewpoint of consistency. Our work\nbrings some clarity to this issue by revealing that step-size restriction is a\nmechanism for preventing the curvature of the risk from derailing convergence.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 02:45:58 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 02:02:56 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 06:14:50 GMT"}, {"version": "v4", "created": "Thu, 8 Nov 2018 23:03:10 GMT"}, {"version": "v5", "created": "Sun, 23 Jun 2019 21:11:55 GMT"}, {"version": "v6", "created": "Wed, 20 Nov 2019 01:03:22 GMT"}, {"version": "v7", "created": "Mon, 29 Jun 2020 06:00:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "Donald K. K.", ""], ["Chen", "Ningyuan", ""], ["Ishwaran", "Hemant", ""]]}, {"id": "1701.07953", "submitter": "Karan Singh", "authors": "Naman Agarwal and Karan Singh", "title": "The Price of Differential Privacy For Online Learning", "comments": "To appear in the Proceedings of the 34th International Conference on\n  Machine Learning (ICML), Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design differentially private algorithms for the problem of online linear\noptimization in the full information and bandit settings with optimal\n$\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our\nresults demonstrate that $\\epsilon$-differential privacy may be ensured for\nfree -- in particular, the regret bounds scale as\n$O(\\sqrt{T})+\\tilde{O}\\left(\\frac{1}{\\epsilon}\\right)$. For bandit linear\noptimization, and as a special case, for non-stochastic multi-armed bandits,\nthe proposed algorithm achieves a regret of\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}\\sqrt{T}\\right)$, while the previously known\nbest regret bound was\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}T^{\\frac{2}{3}}\\right)$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 06:17:14 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 21:25:12 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Agarwal", "Naman", ""], ["Singh", "Karan", ""]]}, {"id": "1701.08027", "submitter": "Cl\\'audia Soares", "authors": "Cl\\'audia Soares, Jo\\~ao Gomes, Beatriz Ferreira, Jo\\~ao Paulo\n  Costeira", "title": "LocDyn: Robust Distributed Localization for Mobile Underwater Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to self-localize large teams of underwater nodes using only noisy range\nmeasurements? How to do it in a distributed way, and incorporating dynamics\ninto the problem? How to reject outliers and produce trustworthy position\nestimates? The stringent acoustic communication channel and the accuracy needs\nof our geophysical survey application demand faster and more accurate\nlocalization methods. We approach dynamic localization as a MAP estimation\nproblem where the prior encodes dynamics, and we devise a convex relaxation\nmethod that takes advantage of previous estimates at each measurement\nacquisition step; The algorithm converges at an optimal rate for first order\nmethods. LocDyn is distributed: there is no fusion center responsible for\nprocessing acquired data and the same simple computations are performed for\neach node. LocDyn is accurate: experiments attest to a smaller positioning\nerror than a comparable Kalman filter. LocDyn is robust: it rejects outlier\nnoise, while the comparing methods succumb in terms of positioning error.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:25:19 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Soares", "Cl\u00e1udia", ""], ["Gomes", "Jo\u00e3o", ""], ["Ferreira", "Beatriz", ""], ["Costeira", "Jo\u00e3o Paulo", ""]]}, {"id": "1701.08055", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Zhaozhi Qian", "title": "Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:01:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Qian", "Zhaozhi", ""]]}, {"id": "1701.08100", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos", "title": "The Causal Frame Problem: An Algorithmic Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frame Problem (FP) is a puzzle in philosophy of mind and epistemology,\narticulated by the Stanford Encyclopedia of Philosophy as follows: \"How do we\naccount for our apparent ability to make decisions on the basis only of what is\nrelevant to an ongoing situation without having explicitly to consider all that\nis not relevant?\" In this work, we focus on the causal variant of the FP, the\nCausal Frame Problem (CFP). Assuming that a reasoner's mental causal model can\nbe (implicitly) represented by a causal Bayes net, we first introduce a notion\ncalled Potential Level (PL). PL, in essence, encodes the relative position of a\nnode with respect to its neighbors in a causal Bayes net. Drawing on the\npsychological literature on causal judgment, we substantiate the claim that PL\nmay bear on how time is encoded in the mind. Using PL, we propose an inference\nframework, called the PL-based Inference Framework (PLIF), which permits a\nboundedly-rational approach to the CFP to be formally articulated at Marr's\nalgorithmic level of analysis. We show that our proposed framework, PLIF, is\nconsistent with a wide range of findings in causal judgment literature, and\nthat PL and PLIF make a number of predictions, some of which are already\nsupported by existing findings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:42:29 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1701.08140", "submitter": "Jesus Daniel Arroyo Relion", "authors": "Jes\\'us D. Arroyo-Reli\\'on, Daniel Kessler, Elizaveta Levina, Stephan\n  F. Taylor", "title": "Network classification with applications to brain connectomics", "comments": null, "journal-ref": null, "doi": "10.1214/19-AOAS1252", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:26:38 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 06:39:26 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 20:13:32 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Arroyo-Reli\u00f3n", "Jes\u00fas D.", ""], ["Kessler", "Daniel", ""], ["Levina", "Elizaveta", ""], ["Taylor", "Stephan F.", ""]]}, {"id": "1701.08142", "submitter": "Clara Grazian", "authors": "Clara Grazian, Fabrizio Leisen, Brunero Liseo", "title": "Modelling Preference Data with the Wallenius Distribution", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wallenius distribution is a generalisation of the Hypergeometric\ndistribution where weights are assigned to balls of different colours. This\nnaturally defines a model for ranking categories which can be used for\nclassification purposes. Since, in general, the resulting likelihood is not\nanalytically available, we adopt an approximate Bayesian computational (ABC)\napproach for estimating the importance of the categories. We illustrate the\nperformance of the estimation procedure on simulated datasets. Finally, we use\nthe new model for analysing two datasets about movies ratings and Italian\nacademic statisticians' journal preferences. The latter is a novel dataset\ncollected by the authors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:30:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:45:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:54:59 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 12:03:47 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 14:46:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Grazian", "Clara", ""], ["Leisen", "Fabrizio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1701.08254", "submitter": "Murat Kocaoglu", "authors": "Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, Babak\n  Hassibi", "title": "Entropic Causality and Greedy Minimum Entropy Coupling", "comments": "Submitted to ISIT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying the causal relationship between two\ndiscrete random variables from observational data. We recently proposed a novel\nframework called entropic causality that works in a very general functional\nmodel but makes the assumption that the unobserved exogenous variable has small\nentropy in the true causal direction.\n  This framework requires the solution of a minimum entropy coupling problem:\nGiven marginal distributions of m discrete random variables, each on n states,\nfind the joint distribution with minimum entropy, that respects the given\nmarginals. This corresponds to minimizing a concave function of nm variables\nover a convex polytope defined by nm linear constraints, called a\ntransportation polytope. Unfortunately, it was recently shown that this minimum\nentropy coupling problem is NP-hard, even for 2 variables with n states. Even\nrepresenting points (joint distributions) over this space can require\nexponential complexity (in n, m) if done naively.\n  In our recent work we introduced an efficient greedy algorithm to find an\napproximate solution for this problem. In this paper we analyze this algorithm\nand establish two results: that our algorithm always finds a local minimum and\nalso is within an additive approximation error from the unknown global optimum.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 05:17:25 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""], ["Hassibi", "Babak", ""]]}, {"id": "1701.08302", "submitter": "A Mani", "authors": "Mani A and Rebeka Mukherjee", "title": "A Study of FOSS'2013 Survey Data Using Clustering Techniques", "comments": "IEEE Women in Engineering Conference Paper: WIECON-ECE'2016\n  (Scheduled to appear in IEEE Xplore )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FOSS is an acronym for Free and Open Source Software. The FOSS 2013 survey\nprimarily targets FOSS contributors and relevant anonymized dataset is publicly\navailable under CC by SA license. In this study, the dataset is analyzed from a\ncritical perspective using statistical and clustering techniques (especially\nmultiple correspondence analysis) with a strong focus on women contributors\ntowards discovering hidden trends and facts. Important inferences are drawn\nabout development practices and other facets of the free software and OSS\nworlds.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 16:52:13 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 17:18:01 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["A", "Mani", ""], ["Mukherjee", "Rebeka", ""]]}, {"id": "1701.08305", "submitter": "Pan Li", "authors": "Pan Li and Olgica Milenkovic", "title": "Multiclass MinMax Rank Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of minmax rank aggregation problems under two\ndistance measures, the Kendall {\\tau} and the Spearman footrule. As the\nproblems are NP-hard, we proceed to describe a number of constant-approximation\nalgorithms for solving them. We conclude with illustrative applications of the\naggregation methods on the Mallows model and genomic data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 17:45:58 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1701.08318", "submitter": "Xueliang (Leon) Liu", "authors": "Xueliang Liu", "title": "Deep Recurrent Neural Network for Protein Function Prediction from\n  Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As high-throughput biological sequencing becomes faster and cheaper, the need\nto extract useful information from sequencing becomes ever more paramount,\noften limited by low-throughput experimental characterizations. For proteins,\naccurate prediction of their functions directly from their primary amino-acid\nsequences has been a long standing challenge. Here, machine learning using\nartificial recurrent neural networks (RNN) was applied towards classification\nof protein function directly from primary sequence without sequence alignment,\nheuristic scoring or feature engineering. The RNN models containing\nlong-short-term-memory (LSTM) units trained on public, annotated datasets from\nUniProt achieved high performance for in-class prediction of four important\nprotein functions tested, particularly compared to other machine learning\nalgorithms using sequence-derived protein features. RNN models were used also\nfor out-of-class predictions of phylogenetically distinct protein families with\nsimilar functions, including proteins of the CRISPR-associated nuclease,\nferritin-like iron storage and cytochrome P450 families. Applying the trained\nRNN models on the partially unannotated UniRef100 database predicted not only\ncandidates validated by existing annotations but also currently unannotated\nsequences. Some RNN predictions for the ferritin-like iron sequestering\nfunction were experimentally validated, even though their sequences differ\nsignificantly from known, characterized proteins and from each other and cannot\nbe easily predicted using popular bioinformatics methods. As sequencing and\nexperimental characterization data increases rapidly, the machine-learning\napproach based on RNN could be useful for discovery and prediction of\nhomologues for a wide range of protein functions.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 19:33:59 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Liu", "Xueliang", ""]]}, {"id": "1701.08381", "submitter": "Dimosthenis Tsagkrasoulis", "authors": "Dimosthenis Tsagkrasoulis and Giovanni Montana", "title": "Random Forest regression for manifold-valued responses", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing array of biomedical and computer vision applications requires\nthe predictive modeling of complex data, for example images and shapes. The\nmain challenge when predicting such objects lies in the fact that they do not\ncomply to the assumptions of Euclidean geometry. Rather, they occupy non-linear\nspaces, a.k.a. manifolds, where it is difficult to define concepts such as\ncoordinates, vectors and expected values. In this work, we construct a\nnon-parametric predictive methodology for manifold-valued objects, based on a\ndistance modification of the Random Forest algorithm. Our method is versatile\nand can be applied both in cases where the response space is a well-defined\nmanifold, but also when such knowledge is not available. Model fitting and\nprediction phases only require the definition of a suitable distance function\nfor the observed responses. We validate our methodology using simulations and\napply it on a series of illustrative image completion applications, showcasing\nsuperior predictive performance, compared to various established regression\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:47:38 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 22:07:32 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Tsagkrasoulis", "Dimosthenis", ""], ["Montana", "Giovanni", ""]]}, {"id": "1701.08473", "submitter": "Quang N. Tran", "authors": "Ba-Ngu Vo, Quang N. Tran, Dinh Phung, Ba-Tuong Vo", "title": "Model-based Classification and Novelty Detection For Point Pattern Data", "comments": "Prepint: 23rd Int. Conf. Pattern Recognition (ICPR). Cancun, Mexico,\n  December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point patterns are sets or multi-sets of unordered elements that can be found\nin numerous data sources. However, in data analysis tasks such as\nclassification and novelty detection, appropriate statistical models for point\npattern data have not received much attention. This paper proposes the\nmodelling of point pattern data via random finite sets (RFS). In particular, we\npropose appropriate likelihood functions, and a maximum likelihood estimator\nfor learning a tractable family of RFS models. In novelty detection, we propose\nnovel ranking functions based on RFS models, which substantially improve\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:47:44 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 03:44:39 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Vo", "Ba-Ngu", ""], ["Tran", "Quang N.", ""], ["Phung", "Dinh", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1701.08528", "submitter": "Martin J\\\"anicke", "authors": "David Bannach, Martin J\\\"anicke, Vitor F. Rey, Sven Tomforde, Bernhard\n  Sick, Paul Lukowicz", "title": "Self-Adaptation of Activity Recognition Systems to New Sensors", "comments": "26 pages, very descriptive figures, comprehensive evaluation on\n  real-life datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 10:01:38 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Bannach", "David", ""], ["J\u00e4nicke", "Martin", ""], ["Rey", "Vitor F.", ""], ["Tomforde", "Sven", ""], ["Sick", "Bernhard", ""], ["Lukowicz", "Paul", ""]]}, {"id": "1701.08588", "submitter": "Erik Schlicht", "authors": "Erik J. Schlicht and Nichole L. Morris", "title": "Estimating the risk associated with transportation technology using\n  multifidelity simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a quantitative method for estimating the risk associated\nwith candidate transportation technology, before it is developed and deployed.\nThe proposed solution extends previous methods that rely exclusively on\nlow-fidelity human-in-the-loop experimental data, or high-fidelity traffic\ndata, by adopting a multifidelity approach that leverages data from both low-\nand high-fidelity sources. The multifidelity method overcomes limitations\ninherent to existing approaches by allowing a model to be trained\ninexpensively, while still assuring that its predictions generalize to the\nreal-world. This allows for candidate technologies to be evaluated at the stage\nof conception, and enables a mechanism for only the safest and most effective\ntechnology to be developed and released.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 13:38:56 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 21:48:50 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Schlicht", "Erik J.", ""], ["Morris", "Nichole L.", ""]]}, {"id": "1701.08687", "submitter": "Christian Hansen", "authors": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo,\n  Christian Hansen, and Whitney Newey", "title": "Double/Debiased/Neyman Machine Learning of Treatment Effects", "comments": "Conference paper, forthcoming in American Economic Review, Papers and\n  Proceedings, 2017. arXiv admin note: text overlap with arXiv:1608.00060", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) provide a\ngeneric double/de-biased machine learning (DML) approach for obtaining valid\ninferential statements about focal parameters, using Neyman-orthogonal scores\nand cross-fitting, in settings where nuisance parameters are estimated using a\nnew generation of nonparametric fitting methods for high-dimensional data,\ncalled machine learning methods. In this note, we illustrate the application of\nthis method in the context of estimating average treatment effects (ATE) and\naverage treatment effects on the treated (ATTE) using observational data. A\nmore general discussion and references to the existing literature are available\nin Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016).\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:31:44 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Demirer", "Mert", ""], ["Duflo", "Esther", ""], ["Hansen", "Christian", ""], ["Newey", "Whitney", ""]]}, {"id": "1701.08701", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar and Giuseppe Caire", "title": "Signal Recovery from Unlabeled Samples", "comments": "18 pages, 11 figures. Submitted to IEEE Trans. on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2786276", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the recovery of a signal from a set of noisy linear\nprojections (measurements), when such projections are unlabeled, that is, the\ncorrespondence between the measurements and the set of projection vectors\n(i.e., the rows of the measurement matrix) is not known a priori. We consider a\nspecial case of unlabeled sensing referred to as Unlabeled Ordered Sampling\n(UOS) where the ordering of the measurements is preserved. We identify a\nnatural duality between this problem and classical Compressed Sensing (CS),\nwhere we show that the unknown support (location of nonzero elements) of a\nsparse signal in CS corresponds to the unknown indices of the measurements in\nUOS. While in CS it is possible to recover a sparse signal from an\nunder-determined set of linear equations (less equations than the signal\ndimension), successful recovery in UOS requires taking more samples than the\ndimension of the signal. Motivated by this duality, we develop a Restricted\nIsometry Property (RIP) similar to that in CS. We also design a low-complexity\nAlternating Minimization algorithm that achieves a stable signal recovery under\nthe established RIP. We analyze our proposed algorithm for different signal\ndimensions and number of measurements theoretically and investigate its\nperformance empirically via simulations. The results are reminiscent of\nphase-transition similar to that occurring in CS.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:48:53 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 11:33:16 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 12:11:42 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 18:15:34 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Haghighatshoar", "Saeid", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1701.08711", "submitter": "Yan Chi Vinci Chow", "authors": "Vinci Chow", "title": "Predicting Auction Price of Vehicle License Plate with Deep Recurrent\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2019.113008", "report-no": null, "categories": "cs.CL cs.LG q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:14:25 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 17:41:35 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 16:36:38 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 08:34:32 GMT"}, {"version": "v5", "created": "Tue, 8 Oct 2019 16:25:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chow", "Vinci", ""]]}, {"id": "1701.08718", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sarath Chandar, Yoshua Bengio", "title": "Memory Augmented Neural Networks with Wormhole Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical results on long-term dependency tasks have shown that neural\nnetworks augmented with an external memory can learn the long-term dependency\ntasks more easily and achieve better generalization than vanilla recurrent\nneural networks (RNN). We suggest that memory augmented neural networks can\nreduce the effects of vanishing gradients by creating shortcut (or wormhole)\nconnections. Based on this observation, we propose a novel memory augmented\nneural network model called TARDIS (Temporal Automatic Relation Discovery in\nSequences). The controller of TARDIS can store a selective set of embeddings of\nits own previous hidden states into an external memory and revisit them as and\nwhen needed. For TARDIS, memory acts as a storage for wormhole connections to\nthe past to propagate the gradients more effectively and it helps to learn the\ntemporal dependencies. The memory structure of TARDIS has similarities to both\nNeural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but\nboth read and write operations of TARDIS are simpler and more efficient. We use\ndiscrete addressing for read/write operations which helps to substantially to\nreduce the vanishing gradient problem with very long sequences. Read and write\noperations in TARDIS are tied with a heuristic once the memory becomes full,\nand this makes the learning problem simpler when compared to NTM or D-NTM type\nof architectures. We provide a detailed analysis on the gradient propagation in\ngeneral for MANNs. We evaluate our models on different long-term dependency\ntasks and report competitive results in all of them.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:34:51 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Chandar", "Sarath", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1701.08757", "submitter": "Dmitry Ignatov", "authors": "Mikhail V. Goubko and Sergey O. Kuznetsov and Alexey A. Neznanov and\n  Dmitry I. Ignatov", "title": "Bayesian Learning of Consumer Preferences for Residential Demand\n  Response", "comments": null, "journal-ref": "IFAC-PapersOnLine, 49(32), 2016, p. 24-29, ISSN 2405-8963", "doi": "10.1016/j.ifacol.2016.12.184", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In coming years residential consumers will face real-time electricity tariffs\nwith energy prices varying day to day, and effective energy saving will require\nautomation - a recommender system, which learns consumer's preferences from her\nactions. A consumer chooses a scenario of home appliance use to balance her\ncomfort level and the energy bill. We propose a Bayesian learning algorithm to\nestimate the comfort level function from the history of appliance use. In\nnumeric experiments with datasets generated from a simulation model of a\nconsumer interacting with small home appliances the algorithm outperforms\npopular regression analysis tools. Our approach can be extended to control an\nair heating and conditioning system, which is responsible for up to half of a\nhousehold's energy bill.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 20:45:31 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Goubko", "Mikhail V.", ""], ["Kuznetsov", "Sergey O.", ""], ["Neznanov", "Alexey A.", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1701.08789", "submitter": "Akash Malhotra", "authors": "Akash Malhotra, Mayank Maloo", "title": "Understanding food inflation in India: A Machine Learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the stellar growth of Indian economy has been\nchallenged by persistently high levels of inflation, particularly in food\nprices. The primary reason behind this stubborn food inflation is mismatch in\nsupply-demand, as domestic agricultural production has failed to keep up with\nrising demand owing to a number of proximate factors. The relative significance\nof these factors in determining the change in food prices have been analysed\nusing gradient boosted regression trees (BRT), a machine learning technique.\nThe results from BRT indicates all predictor variables to be fairly significant\nin explaining the change in food prices, with MSP and farm wages being\nrelatively more important than others. International food prices were found to\nhave limited relevance in explaining the variation in domestic food prices. The\nchallenge of ensuring food and nutritional security for growing Indian\npopulation with rising incomes needs to be addressed through resolute policy\nreforms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 19:22:01 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Malhotra", "Akash", ""], ["Maloo", "Mayank", ""]]}, {"id": "1701.08795", "submitter": "Karan Singh", "authors": "Angela Zhou, Irineo Cabreros, Karan Singh", "title": "Dynamic Task Allocation for Crowdsourcing Settings", "comments": "Presented at the Data Efficient Machine Learning Workshop at\n  International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal budget allocation for crowdsourcing\nproblems, allocating users to tasks to maximize our final confidence in the\ncrowdsourced answers. Such an optimized worker assignment method allows us to\nboost the efficacy of any popular crowdsourcing estimation algorithm. We\nconsider a mutual information interpretation of the crowdsourcing problem,\nwhich leads to a stochastic subset selection problem with a submodular\nobjective function. We present experimental simulation results which\ndemonstrate the effectiveness of our dynamic task allocation method for\nachieving higher accuracy, possibly requiring fewer labels, as well as\nimproving upon a previous method which is sensitive to the proportion of users\nto questions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 19:37:21 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 07:02:37 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zhou", "Angela", ""], ["Cabreros", "Irineo", ""], ["Singh", "Karan", ""]]}, {"id": "1701.08810", "submitter": "Romain Laroche", "authors": "Romain Laroche and Raphael Feraud", "title": "Reinforcement Learning Algorithm Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 20:13:17 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 19:20:40 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 21:08:17 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Laroche", "Romain", ""], ["Feraud", "Raphael", ""]]}, {"id": "1701.08840", "submitter": "Andre Goncalves", "authors": "Andr\\'e R. Gon\\c{c}alves, Arindam Banerjee, Fernando J. Von Zuben", "title": "Spatial Projection of Multiple Climate Variables using Hierarchical\n  Multitask Learning", "comments": "Accepted for the 31st AAAI Conference on Artificial Intelligence\n  (AAAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future projection of climate is typically obtained by combining outputs from\nmultiple Earth System Models (ESMs) for several climate variables such as\ntemperature and precipitation. While IPCC has traditionally used a simple model\noutput average, recent work has illustrated potential advantages of using a\nmultitask learning (MTL) framework for projections of individual climate\nvariables. In this paper we introduce a framework for hierarchical multitask\nlearning (HMTL) with two levels of tasks such that each super-task, i.e., task\nat the top level, is itself a multitask learning problem over sub-tasks. For\nclimate projections, each super-task focuses on projections of specific climate\nvariables spatially using an MTL formulation. For the proposed HMTL approach, a\ngroup lasso regularization is added to couple parameters across the\nsuper-tasks, which in the climate context helps exploit relationships among the\nbehavior of different climate variables at a given spatial location. We show\nthat some recent works on MTL based on learning task dependency structures can\nbe viewed as special cases of HMTL. Experiments on synthetic and real climate\ndata show that HMTL produces better results than decoupled MTL methods applied\nseparately on the super-tasks and HMTL significantly outperforms baselines for\nclimate projection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 21:56:18 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Gon\u00e7alves", "Andr\u00e9 R.", ""], ["Banerjee", "Arindam", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1701.08916", "submitter": "Chenyue Wu", "authors": "Chenyue Wu and Esteban G. Tabak", "title": "Prototypal Analysis and Prototypal Regression", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototypal analysis is introduced to overcome two shortcomings of archetypal\nanalysis: its sensitivity to outliers and its non-locality, which reduces its\napplicability as a learning tool. Same as archetypal analysis, prototypal\nanalysis finds prototypes through convex combination of the data points and\napproximates the data through convex combination of the archetypes, but it adds\na penalty for using prototypes distant from the data points for their\nreconstruction. Prototypal analysis can be extended---via kernel embedding---to\nprobability distributions, since the convexity of the prototypes makes them\ninterpretable as mixtures. Finally, prototypal regression is developed, a\nrobust supervised procedure which allows the use of distributions as either\nfeatures or labels.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:04:55 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 01:41:19 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Wu", "Chenyue", ""], ["Tabak", "Esteban G.", ""]]}, {"id": "1701.08946", "submitter": "Abdelghafour Talibi", "authors": "Abdelghafour Talibi and Boujem\\^aa Achchab and Rafik Lasri", "title": "Variable selection for clustering with Gaussian mixture models: state of\n  the art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture models have become widely used in clustering, given its\nprobabilistic framework in which its based, however, for modern databases that\nare characterized by their large size, these models behave disappointingly in\nsetting out the model, making essential the selection of relevant variables for\nthis type of clustering. After recalling the basics of clustering based on a\nmodel, this article will examine the variable selection methods for model-based\nclustering, as well as presenting opportunities for improvement of these\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 08:51:59 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Talibi", "Abdelghafour", ""], ["Achchab", "Boujem\u00e2a", ""], ["Lasri", "Rafik", ""]]}, {"id": "1701.08974", "submitter": "Pedro Costa", "authors": "Pedro Costa, Adrian Galdran, Maria In\\^es Meyer, Michael David\n  Abr\\`amoff, Meindert Niemeijer, Ana Maria Mendon\\c{c}a, Aur\\'elio Campilho", "title": "Towards Adversarial Retinal Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:17:13 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Costa", "Pedro", ""], ["Galdran", "Adrian", ""], ["Meyer", "Maria In\u00eas", ""], ["Abr\u00e0moff", "Michael David", ""], ["Niemeijer", "Meindert", ""], ["Mendon\u00e7a", "Ana Maria", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1701.09055", "submitter": "Nil Venet", "authors": "Fran\\c{c}ois Bachoc (GdR MASCOT-NUM, IMT), Fabrice Gamboa (IMT),\n  Jean-Michel Loubes (IMT), Nil Venet (CEA, IMT)", "title": "A Gaussian Process Regression Model for Distribution Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monge-Kantorovich distances, otherwise known as Wasserstein distances, have\nreceived a growing attention in statistics and machine learning as a powerful\ndiscrepancy measure for probability distributions. In this paper, we focus on\nforecasting a Gaussian process indexed by probability distributions. For this,\nwe provide a family of positive definite kernels built using transportation\nbased distances. We provide a probabilistic understanding of these kernels and\ncharacterize the corresponding stochastic processes. We prove that the Gaussian\nprocesses indexed by distributions corresponding to these kernels can be\nefficiently forecast, opening new perspectives in Gaussian process modeling.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 14:24:28 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 09:19:35 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "GdR MASCOT-NUM, IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Venet", "Nil", "", "CEA, IMT"]]}, {"id": "1701.09177", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Hongyuan Zha", "title": "A Dirichlet Mixture Model of Hawkes Processes for Event Sequence\n  Clustering", "comments": null, "journal-ref": "31st Conference on Neural Information Processing Systems (NIPS\n  2017), Long Beach, CA, USA", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective method to solve the event sequence clustering\nproblems based on a novel Dirichlet mixture model of a special but significant\ntype of point processes --- Hawkes process. In this model, each event sequence\nbelonging to a cluster is generated via the same Hawkes process with specific\nparameters, and different clusters correspond to different Hawkes processes.\nThe prior distribution of the Hawkes processes is controlled via a Dirichlet\ndistribution. We learn the model via a maximum likelihood estimator (MLE) and\npropose an effective variational Bayesian inference algorithm. We specifically\nanalyze the resulting EM-type algorithm in the context of inner-outer\niterations and discuss several inner iteration allocation strategies. The\nidentifiability of our model, the convergence of our learning method, and its\nsample complexity are analyzed in both theoretical and empirical ways, which\ndemonstrate the superiority of our method to other competitors. The proposed\nmethod learns the number of clusters automatically and is robust to model\nmisspecification. Experiments on both synthetic and real-world data show that\nour method can learn diverse triggering patterns hidden in asynchronous event\nsequences and achieve encouraging performance on clustering purity and\nconsistency.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 18:42:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 16:28:30 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 03:12:55 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 22:38:34 GMT"}, {"version": "v5", "created": "Thu, 21 Sep 2017 15:47:34 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Xu", "Hongteng", ""], ["Zha", "Hongyuan", ""]]}]