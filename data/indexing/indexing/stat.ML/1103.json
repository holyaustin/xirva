[{"id": "1103.0102", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou and Dacheng Tao", "title": "Multi-label Learning via Structured Decomposition and Group Sparsity", "comments": "13 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In multi-label learning, each sample is associated with several labels.\nExisting works indicate that exploring correlations between labels improve the\nprediction performance. However, embedding the label correlations into the\ntraining process significantly increases the problem size. Moreover, the\nmapping of the label structure in the feature space is not clear. In this\npaper, we propose a novel multi-label learning method \"Structured Decomposition\n+ Group Sparsity (SDGS)\". In SDGS, we learn a feature subspace for each label\nfrom the structured decomposition of the training data, and predict the labels\nof a new sample from its group sparse representation on the multi-subspace\nobtained from the structured decomposition. In particular, in the training\nstage, we decompose the data matrix $X\\in R^{n\\times p}$ as\n$X=\\sum_{i=1}^kL^i+S$, wherein the rows of $L^i$ associated with samples that\nbelong to label $i$ are nonzero and consist a low-rank matrix, while the other\nrows are all-zeros, the residual $S$ is a sparse matrix. The row space of $L_i$\nis the feature subspace corresponding to label $i$. This decomposition can be\nefficiently obtained via randomized optimization. In the prediction stage, we\nestimate the group sparse representation of a new sample on the multi-subspace\nvia group \\emph{lasso}. The nonzero representation coefficients tend to\nconcentrate on the subspaces of labels that the sample belongs to, and thus an\neffective prediction can be obtained. We evaluate SDGS on several real datasets\nand compare it with popular methods. Results verify the effectiveness and\nefficiency of SDGS.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 08:15:28 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2011 00:00:13 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1103.0431", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki, Ryota Tomioka, Masashi Sugiyama", "title": "Fast Convergence Rate of Multiple Kernel Learning with Elastic-net\n  Regularization", "comments": "21 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the learning rate of multiple kernel leaning (MKL) with\nelastic-net regularization, which consists of an $\\ell_1$-regularizer for\ninducing the sparsity and an $\\ell_2$-regularizer for controlling the\nsmoothness. We focus on a sparse setting where the total number of kernels is\nlarge but the number of non-zero components of the ground truth is relatively\nsmall, and prove that elastic-net MKL achieves the minimax learning rate on the\n$\\ell_2$-mixed-norm ball. Our bound is sharper than the convergence rates ever\nshown, and has a property that the smoother the truth is, the faster the\nconvergence rate is.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 13:59:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 13:24:28 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Suzuki", "Taiji", ""], ["Tomioka", "Ryota", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1103.0769", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos and Georgios B. Giannakis", "title": "Sparse Volterra and Polynomial Regression Models: Recoverability and\n  Estimation", "comments": "20 pages, to appear in IEEE Trans. on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2011.2165952", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volterra and polynomial regression models play a major role in nonlinear\nsystem identification and inference tasks. Exciting applications ranging from\nneuroscience to genome-wide association analysis build on these models with the\nadditional requirement of parsimony. This requirement has high interpretative\nvalue, but unfortunately cannot be met by least-squares based or kernel\nregression methods. To this end, compressed sampling (CS) approaches, already\nsuccessful in linear regression settings, can offer a viable alternative. The\nviability of CS for sparse Volterra and polynomial models is the core theme of\nthis work. A common sparse regression task is initially posed for the two\nmodels. Building on (weighted) Lasso-based schemes, an adaptive RLS-type\nalgorithm is developed for sparse polynomial regressions. The identifiability\nof polynomial models is critically challenged by dimensionality. However,\nfollowing the CS principle, when these models are sparse, they could be\nrecovered by far fewer measurements. To quantify the sufficient number of\nmeasurements for a given level of sparsity, restricted isometry properties\n(RIP) are investigated in commonly met polynomial regression settings,\ngeneralizing known results for their linear counterparts. The merits of the\nnovel (weighted) adaptive CS algorithms to sparse polynomial modeling are\nverified through synthetic as well as real data tests for genotype-phenotype\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 20:21:28 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 00:03:45 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1103.0790", "submitter": "Marius Kloft", "authors": "Marius Kloft and Gilles Blanchard", "title": "The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an upper bound on the local Rademacher complexity of $\\ell_p$-norm\nmultiple kernel learning, which yields a tighter excess risk bound than global\napproaches. Previous local approaches aimed at analyzed the case $p=1$ only\nwhile our analysis covers all cases $1\\leq p\\leq\\infty$, assuming the different\nfeature mappings corresponding to the different kernels to be uncorrelated. We\nalso show a lower bound that shows that the bound is tight, and derive\nconsequences regarding excess loss, namely fast convergence rates of the order\n$O(n^{-\\frac{\\alpha}{1+\\alpha}})$, where $\\alpha$ is the minimum eigenvalue\ndecay rate of the individual kernels.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 21:29:00 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Kloft", "Marius", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1103.0897", "submitter": "Hannes Nickisch", "authors": "Hannes Nickisch and Matthias Seeger", "title": "Multiple Kernel Learning: A Unifying Probabilistic Viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a probabilistic viewpoint to multiple kernel learning unifying\nwell-known regularised risk approaches and recent advances in approximate\nBayesian inference relaxations. The framework proposes a general objective\nfunction suitable for regression, robust regression and classification that is\nlower bound of the marginal likelihood and contains many regularised risk\napproaches as special cases. Furthermore, we derive an efficient and provably\nconvergent optimisation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 13:32:28 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 05:16:58 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2012 17:54:06 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Nickisch", "Hannes", ""], ["Seeger", "Matthias", ""]]}, {"id": "1103.0941", "submitter": "Cosma Rohilla Shalizi", "authors": "Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish (Carnegie\n  Mellon University)", "title": "Estimating $\\beta$-mixing coefficients", "comments": "9 pages, accepted by AIStats. CMU Statistics Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on statistical learning for time series assumes the asymptotic\nindependence or ``mixing' of the data-generating process. These mixing\nassumptions are never tested, nor are there methods for estimating mixing rates\nfrom data. We give an estimator for the $\\beta$-mixing rate based on a single\nstationary sample path and show it is $L_1$-risk consistent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 16:29:04 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["McDonald", "Daniel J.", "", "Carnegie\n  Mellon University"], ["Shalizi", "Cosma Rohilla", "", "Carnegie\n  Mellon University"], ["Schervish", "Mark", "", "Carnegie\n  Mellon University"]]}, {"id": "1103.0942", "submitter": "Daniel McDonald", "authors": "Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish (Carnegie\n  Mellon University)", "title": "Generalization error bounds for stationary autoregressive models", "comments": "10 pages, 3 figures. CMU Statistics Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive generalization error bounds for stationary univariate\nautoregressive (AR) models. We show that imposing stationarity is enough to\ncontrol the Gaussian complexity without further regularization. This lets us\nuse structural risk minimization for model selection. We demonstrate our\nmethods by predicting interest rate movements.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 16:38:55 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2011 19:08:19 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["McDonald", "Daniel J.", "", "Carnegie\n  Mellon University"], ["Shalizi", "Cosma Rohilla", "", "Carnegie\n  Mellon University"], ["Schervish", "Mark", "", "Carnegie\n  Mellon University"]]}, {"id": "1103.0949", "submitter": "Cosma Rohilla Shalizi", "authors": "Cosma Rohilla Shalizi, Abigail Z. Jacobs, Kristina Lisa Klinkner,\n  Aaron Clauset", "title": "Adapting to Non-stationarity with Growing Expert Ensembles", "comments": "9 pages, 1 figure; CMU Statistics Technical Report. v2: Added\n  empirical example, revised discussion of related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with time series with complex non-stationarities, low\nretrospective regret on individual realizations is a more appropriate goal than\nlow prospective risk in expectation. Online learning algorithms provide\npowerful guarantees of this form, and have often been proposed for use with\nnon-stationary processes because of their ability to switch between different\nforecasters or ``experts''. However, existing methods assume that the set of\nexperts whose forecasts are to be combined are all given at the start, which is\nnot plausible when dealing with a genuinely historical or evolutionary system.\nWe show how to modify the ``fixed shares'' algorithm for tracking the best\nexpert to cope with a steadily growing set of experts, obtained by fitting new\nmodels to new data as it becomes available, and obtain regret bounds for the\ngrowing ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 17:04:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 23:25:41 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""], ["Jacobs", "Abigail Z.", ""], ["Klinkner", "Kristina Lisa", ""], ["Clauset", "Aaron", ""]]}, {"id": "1103.1689", "submitter": "Morteza Ibrahimi", "authors": "Jos\\'e Bento and Morteza Ibrahimi and Andrea Montanari", "title": "Information Theoretic Limits on Learning Stochastic Differential\n  Equations", "comments": "6 pages, 2 figures, conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST q-fin.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of learning the drift coefficient of a stochastic\ndifferential equation from a sample path. In this paper, we assume that the\ndrift is parametrized by a high dimensional vector. We address the question of\nhow long the system needs to be observed in order to learn this vector of\nparameters. We prove a general lower bound on this time complexity by using a\ncharacterization of mutual information as time integral of conditional\nvariance, due to Kadota, Zakai, and Ziv. This general lower bound is applied to\nspecific classes of linear and non-linear stochastic differential equations. In\nthe linear case, the problem under consideration is the one of learning a\nmatrix of interaction coefficients. We evaluate our lower bound for ensembles\nof sparse and dense random matrices. The resulting estimates match the\nqualitative behavior of upper bounds achieved by computationally efficient\nprocedures.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 02:03:17 GMT"}], "update_date": "2011-03-10", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Ibrahimi", "Morteza", ""], ["Montanari", "Andrea", ""]]}, {"id": "1103.1761", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar and Simon Lacoste-Julien", "title": "A Kernel Approach to Tractable Bayesian Nonparametrics", "comments": "acknowledgements added to previous version, content otherwise\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in popular nonparametric Bayesian models typically relies on\nsampling or other approximations. This paper presents a general methodology for\nconstructing novel tractable nonparametric Bayesian methods by applying the\nkernel trick to inference in a parametric Bayesian model. For example, Gaussian\nprocess regression can be derived this way from Bayesian linear regression.\nDespite the success of the Gaussian process framework, the kernel trick is\nrarely explicitly considered in the Bayesian literature. In this paper, we aim\nto fill this gap and demonstrate the potential of applying the kernel trick to\ntractable Bayesian parametric models in a wider context than just regression.\nAs an example, we present an intuitive Bayesian kernel machine for density\nestimation that is obtained by applying the kernel trick to a Gaussian\ngenerative model in feature space.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 11:39:21 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 17:05:24 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2011 10:56:21 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1103.2068", "submitter": "Tamara Kolda", "authors": "Justin D. Basilico and M. Arthur Munson and Tamara G. Kolda and Kevin\n  R. Dixon and W. Philip Kegelmeyer", "title": "COMET: A Recipe for Learning and Using Large Ensembles on Massive Data", "comments": null, "journal-ref": "ICDM 2011: Proceedings of the 2011 IEEE International Conference\n  on Data Mining, pp. 41-50, 2011", "doi": "10.1109/ICDM.2011.39", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COMET is a single-pass MapReduce algorithm for learning on large-scale data.\nIt builds multiple random forest ensembles on distributed blocks of data and\nmerges them into a mega-ensemble. This approach is appropriate when learning\nfrom massive-scale data that is too large to fit on a single machine. To get\nthe best accuracy, IVoting should be used instead of bagging to generate the\ntraining subset for each decision tree in the random forest. Experiments with\ntwo large datasets (5GB and 50GB compressed) show that COMET compares favorably\n(in both accuracy and training time) to learning on a subsample of data using a\nserial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble\nevaluation which dynamically decides how many ensemble members to evaluate per\ndata point; this can reduce evaluation cost by 100X or more.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 16:15:42 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2011 16:20:45 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Basilico", "Justin D.", ""], ["Munson", "M. Arthur", ""], ["Kolda", "Tamara G.", ""], ["Dixon", "Kevin R.", ""], ["Kegelmeyer", "W. Philip", ""]]}, {"id": "1103.2670", "submitter": "Iead Rezek", "authors": "Iead Rezek", "title": "Constrained Mixture Models for Asset Returns Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of asset return distributions is crucial for determining\noptimal trading strategies. In this paper we describe the constrained mixture\nmodel, based on a mixture of Gamma and Gaussian distributions, to provide an\naccurate description of price trends as being clearly positive, negative or\nranging while accounting for heavy tails and high kurtosis. The model is\nestimated in the Expectation Maximisation framework and model order estimation\nalso respects the model's constraints.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 14:03:30 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Rezek", "Iead", ""]]}, {"id": "1103.2756", "submitter": "Xinmei Tian", "authors": "Xinmei Tian and Dacheng Tao and Yong Rui", "title": "Sparse Transfer Learning for Interactive Video Search Reranking", "comments": "17 pages", "journal-ref": null, "doi": "10.1145/0000000.0000000", "report-no": null, "categories": "cs.IR cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual reranking is effective to improve the performance of the text-based\nvideo search. However, existing reranking algorithms can only achieve limited\nimprovement because of the well-known semantic gap between low level visual\nfeatures and high level semantic concepts. In this paper, we adopt interactive\nvideo search reranking to bridge the semantic gap by introducing user's\nlabeling effort. We propose a novel dimension reduction tool, termed sparse\ntransfer learning (STL), to effectively and efficiently encode user's labeling\ninformation. STL is particularly designed for interactive video search\nreranking. Technically, it a) considers the pair-wise discriminative\ninformation to maximally separate labeled query relevant samples from labeled\nquery irrelevant ones, b) achieves a sparse representation for the subspace to\nencodes user's intention by applying the elastic net penalty, and c) propagates\nuser's labeling information from labeled samples to unlabeled samples by using\nthe data distribution knowledge. We conducted extensive experiments on the\nTRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular\ndimension reduction algorithms. We report superior performance by using the\nproposed STL based interactive video search reranking.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 19:48:20 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 03:49:33 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2011 00:12:42 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Tian", "Xinmei", ""], ["Tao", "Dacheng", ""], ["Rui", "Yong", ""]]}, {"id": "1103.2816", "submitter": "Yi-Kai Liu", "authors": "Yi-Kai Liu", "title": "Universal low-rank matrix recovery from Pauli measurements", "comments": "v2: corrected typos, added proof details, 9+8 pages, to appear in\n  NIPS 2011", "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 24,\n  pp.1638-1646 (2011)", "doi": null, "report-no": null, "categories": "quant-ph cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reconstructing an unknown matrix M of rank r and\ndimension d using O(rd poly log d) Pauli measurements. This has applications in\nquantum state tomography, and is a non-commutative analogue of a well-known\nproblem in compressed sensing: recovering a sparse vector from a few of its\nFourier coefficients.\n  We show that almost all sets of O(rd log^6 d) Pauli measurements satisfy the\nrank-r restricted isometry property (RIP). This implies that M can be recovered\nfrom a fixed (\"universal\") set of Pauli measurements, using nuclear-norm\nminimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error.\nA similar result holds for any class of measurements that use an orthonormal\noperator basis whose elements have small operator norm. Our proof uses Dudley's\ninequality for Gaussian processes, together with bounds on covering numbers\nobtained via entropy duality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 23:51:32 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2011 02:08:52 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Liu", "Yi-Kai", ""]]}, {"id": "1103.3095", "submitter": "Satyaki Mahalanabis", "authors": "Satyaki Mahalanabis", "title": "A note on active learning for smooth problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the disagreement coefficient of certain smooth hypothesis\nclasses is $O(m)$, where $m$ is the dimension of the hypothesis space, thereby\nanswering a question posed in \\cite{friedman09}.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 04:54:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mahalanabis", "Satyaki", ""]]}, {"id": "1103.3300", "submitter": "Georg M Goerg", "authors": "Georg M. Goerg", "title": "A Nonparametric Frequency Domain EM Algorithm for Time Series\n  Classification with Applications to Spike Sorting and Macro-Economics", "comments": "Winner of the JSM 2011 student paper competition in \"Statistical\n  Learning and Data Mining (SAM)\"; 34 pages. Accepted for publication in SAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a frequency domain adaptation of the Expectation Maximization (EM)\nalgorithm to group a family of time series in classes of similar dynamic\nstructure. It does this by viewing the magnitude of the discrete Fourier\ntransform (DFT) of each signal (or power spectrum) as a probability\ndensity/mass function (pdf/pmf) on the unit circle: signals with similar\ndynamics have similar pdfs; distinct patterns have distinct pdfs. An advantage\nof this approach is that it does not rely on any parametric form of the dynamic\nstructure, but can be used for non-parametric, robust and model-free\nclassification. This new method works for non-stationary signals of similar\nshape as well as stationary signals with similar auto-correlation structure.\nApplications to neural spike sorting (non-stationary) and pattern-recognition\nin socio-economic time series (stationary) demonstrate the usefulness and wide\napplicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 21:31:17 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 03:41:54 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 19:07:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1103.3681", "submitter": "Sergei Krivov", "authors": "Sergei V. Krivov", "title": "Optimal Dimensionality Reduction of Complex Dynamics: The Chess Game as\n  Diffusion on a Free Energy Landscape", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.84.011135", "report-no": null, "categories": "physics.data-an nlin.AO physics.chem-ph physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is ubiquitous in analysis of complex dynamics. The\nconventional dimensionality reduction techniques, however, focus on reproducing\nthe underlying configuration space, rather than the dynamics itself. The\nconstructed low-dimensional space does not provide complete and accurate\ndescription of the dynamics. Here I describe how to perform dimensionality\nreduction while preserving the essential properties of the dynamics. The\napproach is illustrated by analyzing the chess game - the archetype of complex\ndynamics. A variable that provides complete and accurate description of chess\ndynamics is constructed. Winning probability is predicted by describing the\ngame as a random walk on the free energy landscape associated with the\nvariable. The approach suggests a possible way of obtaining a simple yet\naccurate description of many important complex phenomena. The analysis of the\nchess game shows that the approach can quantitatively describe the dynamics of\nprocesses where human decision-making plays a central role, e.g., financial and\nsocial dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2011 18:21:42 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Krivov", "Sergei V.", ""]]}, {"id": "1103.4023", "submitter": "Nicolas Durrande", "authors": "Nicolas Durrande (CROCUS-ENSMSE), David Ginsbourger, Olivier Roustant\n  (CROCUS-ENSMSE, - M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques)", "title": "Additive Kernels for Gaussian Process Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process (GP) models are often used as mathematical approximations of\ncomputationally expensive experiments. Provided that its kernel is suitably\nchosen and that enough data is available to obtain a reasonable fit of the\nsimulator, a GP model can beneficially be used for tasks such as prediction,\noptimization, or Monte-Carlo-based quantification of uncertainty. However, the\nformer conditions become unrealistic when using classical GPs as the dimension\nof input increases. One popular alternative is then to turn to Generalized\nAdditive Models (GAMs), relying on the assumption that the simulator's response\ncan approximately be decomposed as a sum of univariate functions. If such an\napproach has been successfully applied in approximation, it is nevertheless not\ncompletely compatible with the GP framework and its versatile applications. The\nambition of the present work is to give an insight into the use of GPs for\nadditive models by integrating additivity within the kernel, and proposing a\nparsimonious numerical method for data-driven parameter estimation. The first\npart of this article deals with the kernels naturally associated to additive\nprocesses and the properties of the GP models based on such kernels. The second\npart is dedicated to a numerical procedure based on relaxation for additive\nkernel parameter estimation. Finally, the efficiency of the proposed method is\nillustrated and compared to other approaches on Sobol's g-function.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 14:16:55 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Durrande", "Nicolas", "", "CROCUS-ENSMSE"], ["Ginsbourger", "David", "", "CROCUS-ENSMSE, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"], ["Roustant", "Olivier", "", "CROCUS-ENSMSE, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"]]}, {"id": "1103.4296", "submitter": "John Duchi", "authors": "John C. Duchi and Peter L. Bartlett and Martin J. Wainwright", "title": "Randomized Smoothing for Stochastic Optimization", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze convergence rates of stochastic optimization procedures for\nnon-smooth convex optimization problems. By combining randomized smoothing\ntechniques with accelerated gradient methods, we obtain convergence rates of\nstochastic optimization procedures, both in expectation and with high\nprobability, that have optimal dependence on the variance of the gradient\nestimates. To the best of our knowledge, these are the first variance-based\nrates for non-smooth optimization. We give several applications of our results\nto statistical estimation problems, and provide experimental results that\ndemonstrate the effectiveness of the proposed algorithms. We also describe how\na combination of our algorithm with recent work on decentralized optimization\nyields a distributed stochastic optimization algorithm that is order-optimal.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 16:03:01 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2012 08:06:49 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Duchi", "John C.", ""], ["Bartlett", "Peter L.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1103.4480", "submitter": "Kishor Barman", "authors": "Kishor Barman, Onkar Dabeer", "title": "Clustered regression with unknown clusters", "comments": "9 pages, Submitted to KDD 2011, San Diego", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a collection of prediction experiments, which are clustered in\nthe sense that groups of experiments ex- hibit similar relationship between the\npredictor and response variables. The experiment clusters as well as the\nregres- sion relationships are unknown. The regression relation- ships define\nthe experiment clusters, and in general, the predictor and response variables\nmay not exhibit any clus- tering. We call this prediction problem clustered\nregres- sion with unknown clusters (CRUC) and in this paper we focus on linear\nregression. We study and compare several methods for CRUC, demonstrate their\napplicability to the Yahoo Learning-to-rank Challenge (YLRC) dataset, and in-\nvestigate an associated mathematical model. CRUC is at the crossroads of many\nprior works and we study several prediction algorithms with diverse origins: an\nadaptation of the expectation-maximization algorithm, an approach in- spired by\nK-means clustering, the singular value threshold- ing approach to matrix rank\nminimization under quadratic constraints, an adaptation of the Curds and Whey\nmethod in multiple regression, and a local regression (LoR) scheme reminiscent\nof neighborhood methods in collaborative filter- ing. Based on empirical\nevaluation on the YLRC dataset as well as simulated data, we identify the LoR\nmethod as a good practical choice: it yields best or near-best prediction\nperformance at a reasonable computational load, and it is less sensitive to the\nchoice of the algorithm parameter. We also provide some analysis of the LoR\nmethod for an asso- ciated mathematical model, which sheds light on optimal\nparameter choice and prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 10:20:14 GMT"}], "update_date": "2011-03-24", "authors_parsed": [["Barman", "Kishor", ""], ["Dabeer", "Onkar", ""]]}, {"id": "1103.4601", "submitter": "Lihong Li", "authors": "Miroslav Dudik and John Langford and Lihong Li", "title": "Doubly Robust Policy Evaluation and Learning", "comments": "Published at ICML 2011, 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study decision making in environments where the reward is only partially\nobserved, but can be modeled as a function of an action and an observed\ncontext. This setting, known as contextual bandits, encompasses a wide variety\nof applications including health-care policy and Internet advertising. A\ncentral task is evaluation of a new policy given historic data consisting of\ncontexts, actions and received rewards. The key challenge is that the past data\ntypically does not faithfully represent proportions of actions taken by a new\npolicy. Previous approaches rely either on models of rewards or models of the\npast policy. The former are plagued by a large bias whereas the latter have a\nlarge variance.\n  In this work, we leverage the strength and overcome the weaknesses of the two\napproaches by applying the doubly robust technique to the problems of policy\nevaluation and optimization. We prove that this approach yields accurate value\nestimates when we have either a good (but not necessarily consistent) model of\nrewards or a good (but not necessarily consistent) model of past policy.\nExtensive empirical comparison demonstrates that the doubly robust approach\nuniformly improves over existing techniques, achieving both lower variance in\nvalue estimation and better policies. As such, we expect the doubly robust\napproach to become common practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 19:37:45 GMT"}, {"version": "v2", "created": "Fri, 6 May 2011 02:38:18 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["Dudik", "Miroslav", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1103.4614", "submitter": "Daniel Percival", "authors": "Daniel Percival", "title": "Theoretical Properties of the Overlapping Groups Lasso", "comments": "20 pages, submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two sets of theoretical results on the grouped lasso with overlap\nof Jacob, Obozinski and Vert (2009) in the linear regression setting. This\nmethod allows for joint selection of predictors in sparse regression, allowing\nfor complex structured sparsity over the predictors encoded as a set of groups.\nThis flexible framework suggests that arbitrarily complex structures can be\nencoded with an intricate set of groups. Our results show that this strategy\nresults in unexpected theoretical consequences for the procedure. In\nparticular, we give two sets of results: (1) finite sample bounds on prediction\nand estimation, and (2) asymptotic distribution and selection. Both sets of\nresults give insight into the consequences of choosing an increasingly complex\nset of groups for the procedure, as well as what happens when the set of groups\ncannot recover the true sparsity pattern. Additionally, these results\ndemonstrate the differences and similarities between the the grouped lasso\nprocedure with and without overlapping groups. Our analysis shows the set of\ngroups must be chosen with caution - an overly complex set of groups will\ndamage the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 20:01:04 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2011 23:10:16 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Percival", "Daniel", ""]]}, {"id": "1103.4789", "submitter": "John Paisley", "authors": "John Paisley, Chong Wang and David Blei", "title": "The Discrete Infinite Logistic Normal Distribution", "comments": "This paper will appear in Bayesian Analysis. A shorter version of\n  this paper appeared at AISTATS 2011, Fort Lauderdale, FL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discrete infinite logistic normal distribution (DILN), a\nBayesian nonparametric prior for mixed membership models. DILN is a\ngeneralization of the hierarchical Dirichlet process (HDP) that models\ncorrelation structure between the weights of the atoms at the group level. We\nderive a representation of DILN as a normalized collection of gamma-distributed\nrandom variables, and study its statistical properties. We consider\napplications to topic modeling and derive a variational inference algorithm for\napproximate posterior inference. We study the empirical performance of the DILN\ntopic model on four corpora, comparing performance with the HDP and the\ncorrelated topic model (CTM). To deal with large-scale data sets, we also\ndevelop an online inference algorithm for DILN and compare with online HDP and\nonline LDA on the Nature magazine, which contains approximately 350,000\narticles.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 15:31:47 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 18:52:51 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2012 13:20:10 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Paisley", "John", ""], ["Wang", "Chong", ""], ["Blei", "David", ""]]}, {"id": "1103.4896", "submitter": "Hugo Larochelle", "authors": "J\\'er\\^ome Louradour and Hugo Larochelle", "title": "Classification of Sets using Restricted Boltzmann Machines", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification when inputs correspond to sets of\nvectors. This setting occurs in many problems such as the classification of\npieces of mail containing several pages, of web sites with several sections or\nof images that have been pre-segmented into smaller regions. We propose\ngeneralizations of the restricted Boltzmann machine (RBM) that are appropriate\nin this context and explore how to incorporate different assumptions about the\nrelationship between the input sets and the target class within the RBM. In\nexperiments on standard multiple-instance learning datasets, we demonstrate the\ncompetitiveness of approaches based on RBMs and apply the proposed variants to\nthe problem of incoming mail classification.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 02:33:27 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Louradour", "J\u00e9r\u00f4me", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1103.4998", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Gang Niu, Jun Takagi, Masashi Sugiyama", "title": "Sufficient Component Analysis for Supervised Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of sufficient dimension reduction (SDR) is to find the\nlow-dimensional subspace of input features that is sufficient for predicting\noutput values. In this paper, we propose a novel distribution-free SDR method\ncalled sufficient component analysis (SCA), which is computationally more\nefficient than existing methods. In our method, a solution is computed by\niteratively performing dependence estimation and maximization: Dependence\nestimation is analytically carried out by recently-proposed least-squares\nmutual information (LSMI), and dependence maximization is also analytically\ncarried out by utilizing the Epanechnikov kernel. Through large-scale\nexperiments on real-world image classification and audio tagging problems, the\nproposed method is shown to compare favorably with existing dimension reduction\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 15:35:16 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Yamada", "Makoto", ""], ["Niu", "Gang", ""], ["Takagi", "Jun", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1103.5201", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki, Ryota Tomioka, Masashi Sugiyama", "title": "Sharp Convergence Rate and Support Consistency of Multiple Kernel\n  Learning with Sparse and Dense Regularization", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically investigate the convergence rate and support consistency\n(i.e., correctly identifying the subset of non-zero coefficients in the large\nsample limit) of multiple kernel learning (MKL). We focus on MKL with block-l1\nregularization (inducing sparse kernel combination), block-l2 regularization\n(inducing uniform kernel combination), and elastic-net regularization\n(including both block-l1 and block-l2 regularization). For the case where the\ntrue kernel combination is sparse, we show a sharper convergence rate of the\nblock-l1 and elastic-net MKL methods than the existing rate for block-l1 MKL.\nWe further show that elastic-net MKL requires a milder condition for being\nconsistent than block-l1 MKL. For the case where the optimal kernel combination\nis not exactly sparse, we prove that elastic-net MKL can achieve a faster\nconvergence rate than the block-l1 and block-l2 MKL methods by carefully\ncontrolling the balance between the block-l1and block-l2 regularizers. Thus,\nour theoretical results overall suggest the use of elastic-net regularization\nin MKL.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2011 11:41:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2011 10:25:48 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Suzuki", "Taiji", ""], ["Tomioka", "Ryota", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1103.5202", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Fast Learning Rate of lp-MKL and its Minimax Optimality", "comments": "16 pages, submitted to COLT2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a new sharp generalization bound of lp-MKL which is a\ngeneralized framework of multiple kernel learning (MKL) and imposes\nlp-mixed-norm regularization instead of l1-mixed-norm regularization. We\nutilize localization techniques to obtain the sharp learning rate. The bound is\ncharacterized by the decay rate of the eigenvalues of the associated kernels. A\nlarger decay rate gives a faster convergence rate. Furthermore, we give the\nminimax learning rate on the ball characterized by lp-mixed-norm in the product\nspace. Then we show that our derived learning rate of lp-MKL achieves the\nminimax optimal rate on the lp-mixed-norm ball.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2011 11:49:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 13:22:29 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1103.5537", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Masashi Sugiyama, Jun Sese", "title": "Least-Squares Independence Regression for Non-Linear Causal Inference\n  under Non-Gaussian Noise", "comments": "submitted to Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of non-linear causal relationship under additive non-Gaussian\nnoise models has attracted considerable attention recently because of their\nhigh flexibility. In this paper, we propose a novel causal inference algorithm\ncalled least-squares independence regression (LSIR). LSIR learns the additive\nnoise model through the minimization of an estimator of the squared-loss mutual\ninformation between inputs and residuals. A notable advantage of LSIR over\nexisting approaches is that tuning parameters such as the kernel width and the\nregularization parameter can be naturally optimized by cross-validation,\nallowing us to avoid overfitting in a data-dependent fashion. Through\nexperiments with real-world datasets, we show that LSIR compares favorably with\na state-of-the-art causal inference method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 03:15:32 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 02:26:13 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Yamada", "Makoto", ""], ["Sugiyama", "Masashi", ""], ["Sese", "Jun", ""]]}, {"id": "1103.5708", "submitter": "Yi Sun", "authors": "Yi Sun, Faustino Gomez, Juergen Schmidhuber", "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  To maximize its success, an AGI typically needs to explore its initially\nunknown world. Is there an optimal way of doing so? Here we derive an\naffirmative answer for a broad class of environments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 17:02:35 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Sun", "Yi", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1103.6119", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Serge Iovleff", "title": "Auto-associative models, nonlinear Principal component analysis,\n  manifolds and projection pursuit", "comments": null, "journal-ref": "S. Girard & S. Iovleff. \"Auto-associative models, nonlinear\n  Principal component analysis, manifolds and projection pursuit\", LNCSE,\n  volume 28, p. 205-222, Springer-Verlag, 2007", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, auto-associative models are proposed as candidates to the\ngeneralization of Principal Component Analysis. We show that these models are\ndedicated to the approximation of the dataset by a manifold. Here, the word\n\"manifold\" refers to the topology properties of the structure. The\napproximating manifold is built by a projection pursuit algorithm. At each step\nof the algorithm, the dimension of the manifold is incremented. Some\ntheoretical properties are provided. In particular, we can show that, at each\nstep of the algorithm, the mean residuals norm is not increased. Moreover, it\nis also established that the algorithm converges in a finite number of steps.\nSome particular auto-associative models are exhibited and compared to the\nclassical PCA and some neural networks models. Implementation aspects are\ndiscussed. We show that, in numerous cases, no optimization procedure is\nrequired. Some illustrations on simulated and real data are presented.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 08:03:54 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Iovleff", "Serge", ""]]}]