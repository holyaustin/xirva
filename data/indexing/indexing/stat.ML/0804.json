[{"id": "0804.1026", "submitter": "Harchaoui Zaid", "authors": "Zaid Harchaoui (LTCI), Francis Bach (INRIA Rocquencourt), Eric\n  Moulines (LTCI)", "title": "Testing for Homogeneity with Kernel Fisher Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to investigate test statistics for testing homogeneity in\nreproducing kernel Hilbert spaces. Asymptotic null distributions under null\nhypothesis are derived, and consistency against fixed and local alternatives is\nassessed. Finally, experimental evidence of the performance of the proposed\napproach on both artificial data and a speaker verification task is provided.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2008 13:46:27 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Harchaoui", "Zaid", "", "LTCI"], ["Bach", "Francis", "", "INRIA Rocquencourt"], ["Moulines", "Eric", "", "LTCI"]]}, {"id": "0804.1302", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Bolasso: model consistent Lasso estimation through the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-square linear regression problem with regularization by\nthe l1-norm, a problem usually referred to as the Lasso. In this paper, we\npresent a detailed asymptotic analysis of model consistency of the Lasso. For\nvarious decays of the regularization parameter, we compute asymptotic\nequivalents of the probability of correct model selection (i.e., variable\nselection). For a specific rate decay, we show that the Lasso selects all the\nvariables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection algorithm, referred to as the Bolasso, is\ncompared favorably to other linear regression methods on synthetic data and\ndatasets from the UCI machine learning repository.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2008 15:40:03 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0804.1325", "submitter": "Hugh Chipman", "authors": "Wanhua Su, Hugh Chipman, Mu Zhu", "title": "On the underestimation of model uncertainty by Bayesian K-nearest\n  neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using the K-nearest neighbors method, one often ignores uncertainty in\nthe choice of K. To account for such uncertainty, Holmes and Adams (2002)\nproposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN\n(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain\nMonte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams\n(2002) focused on the performance of BKNN in terms of misclassification error\nbut did not assess its ability to quantify uncertainty. We present some\nevidence to show that BKNN still significantly underestimates model\nuncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2008 16:58:11 GMT"}], "update_date": "2008-04-09", "authors_parsed": [["Su", "Wanhua", ""], ["Chipman", "Hugh", ""], ["Zhu", "Mu", ""]]}, {"id": "0804.2138", "submitter": "Alexey Koloydenko", "authors": "J. Lember, A. Koloydenko", "title": "A constructive proof of the existence of Viterbi processes", "comments": "Submitted to the IEEE Transactions on Information Theory, focuses on\n  the proofs of the results presented in arXiv:0709.2317, and arXiv:0803.2394", "journal-ref": "IEEE Transactions on Information Theory, volume 56, issue 4, 2010,\n  pages 2017 - 2033", "doi": "10.1109/TIT.2010.2040897", "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early days of digital communication, hidden Markov models (HMMs)\nhave now been also routinely used in speech recognition, processing of natural\nlanguages, images, and in bioinformatics. In an HMM $(X_i,Y_i)_{i\\ge 1}$,\nobservations $X_1,X_2,...$ are assumed to be conditionally independent given an\n``explanatory'' Markov process $Y_1,Y_2,...$, which itself is not observed;\nmoreover, the conditional distribution of $X_i$ depends solely on $Y_i$.\nCentral to the theory and applications of HMM is the Viterbi algorithm to find\n{\\em a maximum a posteriori} (MAP) estimate $q_{1:n}=(q_1,q_2,...,q_n)$ of\n$Y_{1:n}$ given observed data $x_{1:n}$. Maximum {\\em a posteriori} paths are\nalso known as Viterbi paths or alignments. Recently, attempts have been made to\nstudy the behavior of Viterbi alignments when $n\\to \\infty$. Thus, it has been\nshown that in some special cases a well-defined limiting Viterbi alignment\nexists. While innovative, these attempts have relied on rather strong\nassumptions and involved proofs which are existential. This work proves the\nexistence of infinite Viterbi alignments in a more constructive manner and for\na very general class of HMMs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2008 18:31:18 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Lember", "J.", ""], ["Koloydenko", "A.", ""]]}, {"id": "0804.2247", "submitter": "Marie Chavent", "authors": "Marie Chavent (IMB), J\\'er\\^ome Saracco (GREThA)", "title": "On central tendency and dispersion measures for intervals and hypercubes", "comments": null, "journal-ref": "Communications in Statistics - Theory and Methods 37, 9 (2008)\n  1471 - 1482", "doi": "10.1080/03610920701678984", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uncertainty or the variability of the data may be treated by considering,\nrather than a single value for each data, the interval of values in which it\nmay fall. This paper studies the derivation of basic description statistics for\ninterval-valued datasets. We propose a geometrical approach in the\ndetermination of summary statistics (central tendency and dispersion measures)\nfor interval-valued variables.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2008 19:25:53 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Chavent", "Marie", "", "IMB"], ["Saracco", "J\u00e9r\u00f4me", "", "GREThA"]]}, {"id": "0804.2848", "submitter": "Kevin Carter", "authors": "Kevin M. Carter, Raviv Raich, William G. Finn, Alfred O. Hero III", "title": "Information Preserving Component Analysis: Data Projections for Flow\n  Cytometry Analysis", "comments": "26 pages", "journal-ref": null, "doi": "10.1109/JSTSP.2008.2011112", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry is often used to characterize the malignant cells in leukemia\nand lymphoma patients, traced to the level of the individual cell. Typically,\nflow cytometric data analysis is performed through a series of 2-dimensional\nprojections onto the axes of the data set. Through the years, clinicians have\ndetermined combinations of different fluorescent markers which generate\nrelatively known expression patterns for specific subtypes of leukemia and\nlymphoma -- cancers of the hematopoietic system. By only viewing a series of\n2-dimensional projections, the high-dimensional nature of the data is rarely\nexploited. In this paper we present a means of determining a low-dimensional\nprojection which maintains the high-dimensional relationships (i.e.\ninformation) between differing oncological data sets. By using machine learning\ntechniques, we allow clinicians to visualize data in a low dimension defined by\na linear combination of all of the available markers, rather than just 2 at a\ntime. This provides an aid in diagnosing similar forms of cancer, as well as a\nmeans for variable selection in exploratory flow cytometric research. We refer\nto our method as Information Preserving Component Analysis (IPCA).\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2008 16:25:48 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Carter", "Kevin M.", ""], ["Raich", "Raviv", ""], ["Finn", "William G.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "0804.2937", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot, Peter L. Bartlett", "title": "Margin-adaptive model selection in statistical learning", "comments": "Published in at http://dx.doi.org/10.3150/10-BEJ288 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 17, 2 (2011) 687-713", "doi": "10.3150/10-BEJ288", "report-no": "IMS-BEJ-BEJ288", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical condition for fast learning rates is the margin condition, first\nintroduced by Mammen and Tsybakov. We tackle in this paper the problem of\nadaptivity to this condition in the context of model selection, in a general\nlearning framework. Actually, we consider a weaker version of this condition\nthat allows one to take into account that learning within a small model can be\nmuch easier than within a large one. Requiring this \"strong margin adaptivity\"\nmakes the model selection problem more challenging. We first prove, in a\ngeneral framework, that some penalization procedures (including local\nRademacher complexities) exhibit this adaptivity when the models are nested.\nContrary to previous results, this holds with penalties that only depend on the\ndata. Our second main result is that strong margin adaptivity is not always\npossible when the models are not nested: for every model selection procedure\n(even a randomized one), there is a problem for which it does not demonstrate\nstrong margin adaptivity.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 04:24:42 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2010 07:57:36 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2011 07:32:12 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Arlot", "Sylvain", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "0804.3678", "submitter": "Dominik Janzing", "authors": "Dominik Janzing and Bernhard Schoelkopf", "title": "Causal inference using the algorithmic Markov condition", "comments": "16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the causal structure that links n observables is usually based upon\ndetecting statistical dependences and choosing simple graphs that make the\njoint measure Markovian. Here we argue why causal inference is also possible\nwhen only single observations are present.\n  We develop a theory how to generate causal graphs explaining similarities\nbetween single objects. To this end, we replace the notion of conditional\nstochastic independence in the causal Markov condition with the vanishing of\nconditional algorithmic mutual information and describe the corresponding\ncausal inference rules.\n  We explain why a consistent reformulation of causal inference in terms of\nalgorithmic complexity implies a new inference principle that takes into\naccount also the complexity of conditional probability densities, making it\npossible to select among Markov equivalent causal graphs. This insight provides\na theoretical foundation of a heuristic principle proposed in earlier work.\n  We also discuss how to replace Kolmogorov complexity with decidable\ncomplexity criteria. This can be seen as an algorithmic analog of replacing the\nempirically undecidable question of statistical independence with practical\nindependence tests that are based on implicit or explicit assumptions on the\nunderlying distribution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2008 10:39:41 GMT"}], "update_date": "2008-04-24", "authors_parsed": [["Janzing", "Dominik", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "0804.3835", "submitter": "Jin Yu", "authors": "Jin Yu, S.V.N. Vishwanathan, Simon Guenter, and Nicol N. Schraudolph", "title": "A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in\n  Machine Learning", "comments": null, "journal-ref": "Journal of Machine Learning Research 11(Mar):1145-1200, 2010", "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the well-known BFGS quasi-Newton method and its memory-limited\nvariant LBFGS to the optimization of nonsmooth convex objectives. This is done\nin a rigorous fashion by generalizing three components of BFGS to\nsubdifferentials: the local quadratic model, the identification of a descent\ndirection, and the Wolfe line search conditions. We prove that under some\ntechnical conditions, the resulting subBFGS algorithm is globally convergent in\nobjective function value. We apply its memory-limited variant (subLBFGS) to\nL_2-regularized risk minimization with the binary hinge loss. To extend our\nalgorithm to the multiclass and multilabel settings, we develop a new,\nefficient, exact line search algorithm. We prove its worst-case time complexity\nbounds, and show that our line search can also be used to extend a recently\ndeveloped bundle method to the multiclass and multilabel settings. We also\napply the direction-finding component of our algorithm to L_1-regularized risk\nminimization with logistic loss. In all these contexts our methods perform\ncomparable to or better than specialized state-of-the-art solvers on a number\nof publicly available datasets. An open source implementation of our algorithms\nis freely available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2008 04:38:54 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2008 09:08:14 GMT"}, {"version": "v3", "created": "Sun, 11 May 2008 02:54:28 GMT"}, {"version": "v4", "created": "Sat, 29 Nov 2008 05:07:29 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2010 08:53:56 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Yu", "Jin", ""], ["Vishwanathan", "S. V. N.", ""], ["Guenter", "Simon", ""], ["Schraudolph", "Nicol N.", ""]]}, {"id": "0804.4685", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Gaussian Processes and Limiting Linear Models", "comments": "31 pages, 10 figures, 4 tables, accepted by CSDA, earlier version in\n  JSM06 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes retain the linear model either as a special case, or in\nthe limit. We show how this relationship can be exploited when the data are at\nleast partially linear. However from the perspective of the Bayesian posterior,\nthe Gaussian processes which encode the linear model either have probability of\nnearly zero or are otherwise unattainable without the explicit construction of\na prior with the limiting linear model in mind. We develop such a prior, and\nshow that its practical benefits extend well beyond the computational and\nconceptual simplicity of the linear model. For example, linearity can be\nextracted on a per-dimension basis, or can be combined with treed partition\nmodels to yield a highly efficient nonstationary model. Our approach is\ndemonstrated on synthetic and real datasets of varying linearity and\ndimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2008 19:43:43 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2008 20:02:26 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2008 09:43:32 GMT"}, {"version": "v4", "created": "Sun, 13 Jul 2008 09:19:09 GMT"}], "update_date": "2008-07-13", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}]