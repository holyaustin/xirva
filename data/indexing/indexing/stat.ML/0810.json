[{"id": "0810.0901", "submitter": "Matthias Seeger", "authors": "Matthias W. Seeger, Hannes Nickisch", "title": "Large Scale Variational Inference and Experimental Design for Sparse\n  Generalized Linear Models", "comments": "34 pages, 6 figures, technical report (submitted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems of low-level computer vision and image processing, such as\ndenoising, deconvolution, tomographic reconstruction or super-resolution, can\nbe addressed by maximizing the posterior distribution of a sparse linear model\n(SLM). We show how higher-order Bayesian decision-making problems, such as\noptimizing image acquisition in magnetic resonance scanners, can be addressed\nby querying the SLM posterior covariance, unrelated to the density's mode. We\npropose a scalable algorithmic framework, with which SLM posteriors over full,\nhigh-resolution images can be approximated for the first time, solving a\nvariational optimization problem which is convex iff posterior mode finding is\nconvex. These methods successfully drive the optimization of sampling\ntrajectories for real-world magnetic resonance imaging through Bayesian\nexperimental design, which has not been attempted before. Our methodology\nprovides new insight into similarities and differences between sparse\nreconstruction and approximate Bayesian inference, and has important\nimplications for compressive sensing of real-world images.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2008 07:59:45 GMT"}, {"version": "v2", "created": "Thu, 12 Aug 2010 20:29:23 GMT"}], "update_date": "2010-08-16", "authors_parsed": [["Seeger", "Matthias W.", ""], ["Nickisch", "Hannes", ""]]}, {"id": "0810.3724", "submitter": "Guangliang Chen", "authors": "Guangliang Chen, Gilad Lerman", "title": "Foundations of a Multi-way Spectral Clustering Framework for Hybrid\n  Linear Modeling", "comments": "40 pages. Minor changes to the previous version (mainly revised\n  Sections 2.2 & 2.3, and added references). Accepted to the Journal of\n  Foundations of Computational Mathematics", "journal-ref": "Found Comput Math (2009) 9(5): 517-558", "doi": "10.1007/s10208-009-9043-7", "report-no": "arXiv:0810.3724v2", "categories": "stat.ML math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Hybrid Linear Modeling (HLM) is to model and segment data\nusing a mixture of affine subspaces. Different strategies have been proposed to\nsolve this problem, however, rigorous analysis justifying their performance is\nmissing. This paper suggests the Theoretical Spectral Curvature Clustering\n(TSCC) algorithm for solving the HLM problem, and provides careful analysis to\njustify it. The TSCC algorithm is practically a combination of Govindu's\nmulti-way spectral clustering framework (CVPR 2005) and Ng et al.'s spectral\nclustering algorithm (NIPS 2001). The main result of this paper states that if\nthe given data is sampled from a mixture of distributions concentrated around\naffine subspaces, then with high sampling probability the TSCC algorithm\nsegments well the different underlying clusters. The goodness of clustering\ndepends on the within-cluster errors, the between-clusters interaction, and a\ntuning parameter applied by TSCC. The proof also provides new insights for the\nanalysis of Ng et al. (NIPS 2001).\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2008 01:54:44 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2009 03:15:36 GMT"}], "update_date": "2009-08-27", "authors_parsed": [["Chen", "Guangliang", ""], ["Lerman", "Gilad", ""]]}, {"id": "0810.4401", "submitter": "Nic Schraudolph", "authors": "Nicol N. Schraudolph and Dmitry Kamenetsky", "title": "Efficient Exact Inference in Planar Ising Models", "comments": "Fixed a number of bugs in v1; added 10 pages of additional figures,\n  explanations, proofs, and experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give polynomial-time algorithms for the exact computation of lowest-energy\n(ground) states, worst margin violators, log partition functions, and marginal\nedge probabilities in certain binary undirected graphical models. Our approach\nprovides an interesting alternative to the well-known graph cut paradigm in\nthat it does not impose any submodularity constraints; instead we require\nplanarity to establish a correspondence with perfect matchings (dimer\ncoverings) in an expanded dual graph. We implement a unified framework while\ndelegating complex but well-understood subproblems (planar embedding,\nmaximum-weight perfect matching) to established algorithms for which efficient\nimplementations are freely available. Unlike graph cut methods, we can perform\npenalized maximum-likelihood as well as maximum-margin parameter estimation in\nthe associated conditional random fields (CRFs), and employ marginal posterior\nprobabilities as well as maximum a posteriori (MAP) states for prediction.\nMaximum-margin CRF parameter estimation on image denoising and segmentation\nproblems shows our approach to be efficient and effective. A C++ implementation\nis available from http://nic.schraudolph.org/isinf/\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2008 08:49:09 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2008 06:47:01 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Schraudolph", "Nicol N.", ""], ["Kamenetsky", "Dmitry", ""]]}, {"id": "0810.4553", "submitter": "Raphael Pelossof", "authors": "Raphael Pelossof, Michael Jones, Ilia Vovsha, Cynthia Rudin", "title": "Online Coordinate Boosting", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new online boosting algorithm for adapting the weights of a\nboosted classifier, which yields a closer approximation to Freund and\nSchapire's AdaBoost algorithm than previous online boosting algorithms. We also\ncontribute a new way of deriving the online algorithm that ties together\nprevious online boosting work. We assume that the weak hypotheses were selected\nbeforehand, and only their weights are updated during online boosting. The\nupdate rule is derived by minimizing AdaBoost's loss when viewed in an\nincremental form. The equations show that optimization is computationally\nexpensive. However, a fast online approximation is possible. We compare\napproximation error to batch AdaBoost on synthetic datasets and generalization\nerror on face datasets and the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2008 21:05:16 GMT"}], "update_date": "2008-10-28", "authors_parsed": [["Pelossof", "Raphael", ""], ["Jones", "Michael", ""], ["Vovsha", "Ilia", ""], ["Rudin", "Cynthia", ""]]}, {"id": "0810.4752", "submitter": "Ulrike von Luxburg", "authors": "Ulrike von Luxburg, Bernhard Schoelkopf", "title": "Statistical Learning Theory: Models, Concepts, and Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning theory provides the theoretical basis for many of\ntoday's machine learning algorithms. In this article we attempt to give a\ngentle, non-technical overview over the key ideas and insights of statistical\nlearning theory. We target at a broad audience, not necessarily machine\nlearning researchers. This paper can serve as a starting point for people who\nwant to get an overview on the field before diving into technical details.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2008 08:25:20 GMT"}], "update_date": "2008-10-28", "authors_parsed": [["von Luxburg", "Ulrike", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "0810.5117", "submitter": "Anil Raj", "authors": "Anil Raj and Chris H. Wiggins", "title": "A non-negative expansion for small Jensen-Shannon Divergences", "comments": "4 page technical report, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we derive a non-negative series expansion for the\nJensen-Shannon divergence (JSD) between two probability distributions. This\nseries expansion is shown to be useful for numerical calculations of the JSD,\nwhen the probability distributions are nearly equal, and for which,\nconsequently, small numerical errors dominate evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2008 19:42:15 GMT"}], "update_date": "2008-10-29", "authors_parsed": [["Raj", "Anil", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "0810.5276", "submitter": "Byeong U. Park", "authors": "Peter Hall, Byeong U. Park, Richard J. Samworth", "title": "Choice of neighbor order in nearest-neighbor classification", "comments": "Published in at http://dx.doi.org/10.1214/07-AOS537 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2008, Vol. 36, No. 5, 2135-2152", "doi": "10.1214/07-AOS537", "report-no": "IMS-AOS-AOS537", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$th-nearest neighbor rule is arguably the simplest and most intuitively\nappealing nonparametric classification procedure. However, application of this\nmethod is inhibited by lack of knowledge about its properties, in particular,\nabout the manner in which it is influenced by the value of $k$; and by the\nabsence of techniques for empirical choice of $k$. In the present paper we\ndetail the way in which the value of $k$ determines the misclassification\nerror. We consider two models, Poisson and Binomial, for the training samples.\nUnder the first model, data are recorded in a Poisson stream and are \"assigned\"\nto one or other of the two populations in accordance with the prior\nprobabilities. In particular, the total number of data in both training samples\nis a Poisson-distributed random variable. Under the Binomial model, however,\nthe total number of data in the training samples is fixed, although again each\ndata value is assigned in a random way. Although the values of risk and regret\nassociated with the Poisson and Binomial models are different, they are\nasymptotically equivalent to first order, and also to the risks associated with\nkernel-based classifiers that are tailored to the case of two derivatives.\nThese properties motivate new methods for choosing the value of $k$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2008 13:41:01 GMT"}], "update_date": "2008-10-30", "authors_parsed": [["Hall", "Peter", ""], ["Park", "Byeong U.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "0810.5655", "submitter": "Wenxin Jiang", "authors": "Wenxin Jiang, Martin A. Tanner", "title": "Gibbs posterior for variable selection in high-dimensional\n  classification and data mining", "comments": "Published in at http://dx.doi.org/10.1214/07-AOS547 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2008, Vol. 36, No. 5, 2207-2231", "doi": "10.1214/07-AOS547", "report-no": "IMS-AOS-AOS547", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the popular approach of \"Bayesian variable selection\" (BVS), one uses\nprior and posterior distributions to select a subset of candidate variables to\nenter the model. A completely new direction will be considered here to study\nBVS with a Gibbs posterior originating in statistical mechanics. The Gibbs\nposterior is constructed from a risk function of practical interest (such as\nthe classification error) and aims at minimizing a risk function without\nmodeling the data probabilistically. This can improve the performance over the\nusual Bayesian approach, which depends on a probability model which may be\nmisspecified. Conditions will be provided to achieve good risk performance,\neven in the presence of high dimensionality, when the number of candidate\nvariables \"$K$\" can be much larger than the sample size \"$n$.\" In addition, we\ndevelop a convenient Markov chain Monte Carlo algorithm to implement BVS with\nthe Gibbs posterior.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2008 10:38:41 GMT"}], "update_date": "2008-11-03", "authors_parsed": [["Jiang", "Wenxin", ""], ["Tanner", "Martin A.", ""]]}]