[{"id": "1605.00031", "submitter": "Thomas Wiatowski", "authors": "Philipp Grohs, Thomas Wiatowski, Helmut B\\\"olcskei", "title": "Deep Convolutional Neural Networks on Cartoon Functions", "comments": "This is a slightly updated version of the paper published in the ISIT\n  proceedings. Specifically, we corrected errors in the arguments on the volume\n  of tubes. Note that this correction does not affect the main statements of\n  the paper", "journal-ref": "Proc. of IEEE International Symposium on Information Theory\n  (ISIT), Barcelona, Spain, pp. 1163-1167, July 2016", "doi": "10.1109/ISIT.2016.7541482", "report-no": null, "categories": "cs.LG cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wiatowski and B\\\"olcskei, 2015, proved that deformation stability and\nvertical translation invariance of deep convolutional neural network-based\nfeature extractors are guaranteed by the network structure per se rather than\nthe specific convolution kernels and non-linearities. While the translation\ninvariance result applies to square-integrable functions, the deformation\nstability bound holds for band-limited functions only. Many signals of\npractical relevance (such as natural images) exhibit, however, sharp and curved\ndiscontinuities and are, hence, not band-limited. The main contribution of this\npaper is a deformation stability result that takes these structural properties\ninto account. Specifically, we establish deformation stability bounds for the\nclass of cartoon functions introduced by Donoho, 2001.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 21:40:16 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 13:47:49 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Grohs", "Philipp", ""], ["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1605.00042", "submitter": "Ankit Parekh", "authors": "Ankit Parekh and Ivan W. Selesnick", "title": "Improved Sparse Low-Rank Matrix Estimation", "comments": "10 pages, 10 figures", "journal-ref": "Signal Processing, Apr. 2017", "doi": "10.1016/j.sigpro.2017.04.011", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating a sparse low-rank matrix from its noisy\nobservation. We propose an objective function consisting of a data-fidelity\nterm and two parameterized non-convex penalty functions. Further, we show how\nto set the parameters of the non-convex penalty functions, in order to ensure\nthat the objective function is strictly convex. The proposed objective function\nbetter estimates sparse low-rank matrices than a convex method which utilizes\nthe sum of the nuclear norm and the $\\ell_1$ norm. We derive an algorithm (as\nan instance of ADMM) to solve the proposed problem, and guarantee its\nconvergence provided the scalar augmented Lagrangian parameter is set\nappropriately. We demonstrate the proposed method for denoising an audio signal\nand an adjacency matrix representing protein interactions in the `Escherichia\ncoli' bacteria.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 23:36:39 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 16:42:20 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Parekh", "Ankit", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1605.00155", "submitter": "Chad Hazlett", "authors": "Chad Hazlett", "title": "Kernel Balancing: A flexible non-parametric weighting procedure for\n  estimating causal effects", "comments": "Work originally included in PhD Thesis, May 2014, MIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of unobserved confounders, matching and weighting methods are\nwidely used to estimate causal quantities including the Average Treatment\nEffect on the Treated (ATT). Unfortunately, these methods do not necessarily\nachieve their goal of making the multivariate distribution of covariates for\nthe control group identical to that of the treated, leaving some (potentially\nmultivariate) functions of the covariates with different means between the two\ngroups. When these \"imbalanced\" functions influence the non-treatment potential\noutcome, the conditioning on observed covariates fails, and ATT estimates may\nbe biased. Kernel balancing, introduced here, targets a weaker requirement for\nunbiased ATT estimation, specifically, that the expected non-treatment\npotential outcome for the treatment and control groups are equal. The\nconditional expectation of the non-treatment potential outcome is assumed to\nfall in the space of functions associated with a choice of kernel, implying a\nset of basis functions in which this regression surface is linear. Weights are\nthen chosen on the control units such that the treated and control group have\nequal means on these basis functions. As a result, the expectation of the\nnon-treatment potential outcome must also be equal for the treated and control\ngroups after weighting, allowing unbiased ATT estimation by subsequent\ndifference in means or an outcome model using these weights. Moreover, the\nweights produced are (1) precisely those that equalize a particular\nkernel-based approximation of the multivariate distribution of covariates for\nthe treated and control, and (2) equivalent to a form of stabilized inverse\npropensity score weighting, though it does not require assuming any model of\nthe treatment assignment mechanism. An R package, KBAL, is provided to\nimplement this approach.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 19:49:20 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Hazlett", "Chad", ""]]}, {"id": "1605.00201", "submitter": "Ting Kei Pong", "authors": "Tianxiang Liu and Ting Kei Pong", "title": "Further properties of the forward-backward envelope with applications to\n  difference-of-convex programming", "comments": "Theorem 3.3 is added. Included numerical tests on oversampled DCT\n  matrix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we further study the forward-backward envelope first\nintroduced in [28] and [30] for problems whose objective is the sum of a proper\nclosed convex function and a twice continuously differentiable possibly\nnonconvex function with Lipschitz continuous gradient. We derive sufficient\nconditions on the original problem for the corresponding forward-backward\nenvelope to be a level-bounded and Kurdyka-{\\L}ojasiewicz function with an\nexponent of $\\frac12$; these results are important for the efficient\nminimization of the forward-backward envelope by classical optimization\nalgorithms. In addition, we demonstrate how to minimize some\ndifference-of-convex regularized least squares problems by minimizing a\nsuitably constructed forward-backward envelope. Our preliminary numerical\nresults on randomly generated instances of large-scale $\\ell_{1-2}$ regularized\nleast squares problems [37] illustrate that an implementation of this approach\nwith a limited-memory BFGS scheme usually outperforms standard first-order\nmethods such as the nonmonotone proximal gradient method in [35].\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 03:36:58 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 11:19:21 GMT"}, {"version": "v3", "created": "Sat, 27 Aug 2016 12:42:11 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 14:02:49 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Liu", "Tianxiang", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1605.00223", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos\n  Anagnostopoulos and Giovanni Montana", "title": "Text-mining the NeuroSynth corpus using Deep Boltzmann Machines", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale automated meta-analysis of neuroimaging data has recently\nestablished itself as an important tool in advancing our understanding of human\nbrain function. This research has been pioneered by NeuroSynth, a database\ncollecting both brain activation coordinates and associated text across a large\ncohort of neuroimaging research papers. One of the fundamental aspects of such\nmeta-analysis is text-mining. To date, word counts and more sophisticated\nmethods such as Latent Dirichlet Allocation have been proposed. In this work we\npresent an unsupervised study of the NeuroSynth text corpus using Deep\nBoltzmann Machines (DBMs). The use of DBMs yields several advantages over the\naforementioned methods, principal among which is the fact that it yields both\nword and document embeddings in a high-dimensional vector space. Such\nembeddings serve to facilitate the use of traditional machine learning\ntechniques on the text corpus. The proposed DBM model is shown to learn\nembeddings with a clear semantic structure.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 09:01:13 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Lorenz", "Romy", ""], ["Leech", "Robert", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1605.00251", "submitter": "Andreas Maurer", "authors": "Andreas Maurer", "title": "A vector-contraction inequality for Rademacher complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contraction inequality for Rademacher averages is extended to Lipschitz\nfunctions with vector-valued domains, and it is also shown that in the bounding\nexpression the Rademacher variables can be replaced by arbitrary iid symmetric\nand sub-gaussian variables. Example applications are given for multi-category\nlearning, K-means clustering and learning-to-learn.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 13:19:57 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Maurer", "Andreas", ""]]}, {"id": "1605.00252", "submitter": "Nishant Mehta", "authors": "Peter D. Gr\\\"unwald and Nishant A. Mehta", "title": "Fast Rates for General Unbounded Loss Functions: from ERM to Generalized\n  Bayes", "comments": "accepted to JMLR pending minor final modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new excess risk bounds for general unbounded loss functions\nincluding log loss and squared loss, where the distribution of the losses may\nbe heavy-tailed. The bounds hold for general estimators, but they are optimized\nwhen applied to $\\eta$-generalized Bayesian, MDL, and empirical risk\nminimization estimators. In the case of log loss, the bounds imply convergence\nrates for generalized Bayesian inference under misspecification in terms of a\ngeneralization of the Hellinger metric as long as the learning rate $\\eta$ is\nset correctly. For general loss functions, our bounds rely on two separate\nconditions: the $v$-GRIP (generalized reversed information projection)\nconditions, which control the lower tail of the excess loss; and the newly\nintroduced witness condition, which controls the upper tail. The parameter $v$\nin the $v$-GRIP conditions determines the achievable rate and is akin to the\nexponent in the Tsybakov margin condition and the Bernstein condition for\nbounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in\ncombination with small model complexity leads to $\\tilde{O}(1/n)$ rates. The\nwitness condition allows us to connect the excess risk to an \"annealed\" version\nthereof, by which we generalize several previous results connecting Hellinger\nand R\\'enyi divergence to KL divergence.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 13:35:15 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 07:39:14 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 16:26:22 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 18:36:02 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Gr\u00fcnwald", "Peter D.", ""], ["Mehta", "Nishant A.", ""]]}, {"id": "1605.00316", "submitter": "Suvrit Sra", "authors": "Suvrit Sra", "title": "Directional Statistics in Machine Learning: a Brief Review", "comments": "12 pages, slightly modified version of submitted book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern data analyst must cope with data encoded in various forms,\nvectors, matrices, strings, graphs, or more. Consequently, statistical and\nmachine learning models tailored to different data encodings are important. We\nfocus on data encoded as normalized vectors, so that their \"direction\" is more\nimportant than their magnitude. Specifically, we consider high-dimensional\nvectors that lie either on the surface of the unit hypersphere or on the real\nprojective plane. For such data, we briefly review common mathematical models\nprevalent in machine learning, while also outlining some technical aspects,\nsoftware, applications, and open mathematical challenges.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 22:37:24 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Sra", "Suvrit", ""]]}, {"id": "1605.00355", "submitter": "Abhinav Maurya", "authors": "Abhinav Maurya, Mark Cheung", "title": "Contrastive Structured Anomaly Detection for Gaussian Graphical Models", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGMs) are probabilistic tools of choice for\nanalyzing conditional dependencies between variables in complex systems.\nFinding changepoints in the structural evolution of a GGM is therefore\nessential to detecting anomalies in the underlying system modeled by the GGM.\nIn order to detect structural anomalies in a GGM, we consider the problem of\nestimating changes in the precision matrix of the corresponding Gaussian\ndistribution. We take a two-step approach to solving this problem:- (i)\nestimating a background precision matrix using system observations from the\npast without any anomalies, and (ii) estimating a foreground precision matrix\nusing a sliding temporal window during anomaly monitoring. Our primary\ncontribution is in estimating the foreground precision using a novel\ncontrastive inverse covariance estimation procedure. In order to accurately\nlearn only the structural changes to the GGM, we maximize a penalized\nlog-likelihood where the penalty is the $l_1$ norm of difference between the\nforeground precision being estimated and the already learned background\nprecision. We modify the alternating direction method of multipliers (ADMM)\nalgorithm for sparse inverse covariance estimation to perform contrastive\nestimation of the foreground precision matrix. Our results on simulated GGM\ndata show significant improvement in precision and recall for detecting\nstructural changes to the GGM, compared to a non-contrastive sliding window\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 05:42:10 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Maurya", "Abhinav", ""], ["Cheung", "Mark", ""]]}, {"id": "1605.00388", "submitter": "Anat Reiner-Benaim", "authors": "Anat Reiner-Benaim, Anna Grabarnick, Edi Shmueli", "title": "Highly Accurate Prediction of Jobs Runtime Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating the short jobs from the long is a known technique to improve\nscheduling performance. In this paper we describe a method we developed for\naccurately predicting the runtimes classes of the jobs to enable this\nseparation. Our method uses the fact that the runtimes can be represented as a\nmixture of overlapping Gaussian distributions, in order to train a CART\nclassifier to provide the prediction. The threshold that separates the short\njobs from the long jobs is determined during the evaluation of the classifier\nto maximize prediction accuracy. Our results indicate overall accuracy of 90%\nfor the data set used in our study, with sensitivity and specificity both above\n90%.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 08:31:48 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 11:39:16 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Reiner-Benaim", "Anat", ""], ["Grabarnick", "Anna", ""], ["Shmueli", "Edi", ""]]}, {"id": "1605.00391", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Arthur Gretton, Bernhard Sch\\\"olkopf, Moritz\n  Grosse-Wentrup", "title": "Recovery of non-linear cause-effect relationships from linearly mixed\n  neuroimaging data", "comments": "arXiv admin note: text overlap with arXiv:1512.01255", "journal-ref": "Pattern Recognition in Neuroimaging (PRNI), International Workshop\n  on, 1-4, 2016", "doi": "10.1109/PRNI.2016.7552331", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables. However, often only linear combinations of variables\nconstitute meaningful causal variables. For example, recovering the signal of a\ncortical source from electroencephalography requires a well-tuned combination\nof signals recorded at multiple electrodes. We recently introduced the MERLiN\n(Mixture Effect Recovery in Linear Networks) algorithm that is able to recover,\nfrom an observed linear mixture, a causal variable that is a linear effect of\nanother given variable. Here we relax the assumption of this cause-effect\nrelationship being linear and present an extended algorithm that can pick up\nnon-linear cause-effect relationships. Thus, the main contribution is an\nalgorithm (and ready to use code) that has broader applicability and allows for\na richer model class. Furthermore, a comparative analysis indicates that the\nassumption of linear cause-effect relationships is not restrictive in analysing\nelectroencephalographic data.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 08:45:59 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 20:01:00 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1605.00513", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino, Francisco De Carvalho, Rosanna Verde", "title": "Fuzzy clustering of distribution-valued data using adaptive L2\n  Wasserstein distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional (or distribution-valued) data are a new type of data arising\nfrom several sources and are considered as realizations of distributional\nvariables. A new set of fuzzy c-means algorithms for data described by\ndistributional variables is proposed.\n  The algorithms use the $L2$ Wasserstein distance between distributions as\ndissimilarity measures. Beside the extension of the fuzzy c-means algorithm for\ndistributional data, and considering a decomposition of the squared $L2$\nWasserstein distance, we propose a set of algorithms using different automatic\nway to compute the weights associated with the variables as well as with their\ncomponents, globally or cluster-wise. The relevance weights are computed in the\nclustering process introducing product-to-one constraints.\n  The relevance weights induce adaptive distances expressing the importance of\neach variable or of each component in the clustering process, acting also as a\nvariable selection method in clustering. We have tested the proposed algorithms\non artificial and real-world data. Results confirm that the proposed methods\nare able to better take into account the cluster structure of the data with\nrespect to the standard fuzzy c-means, with non-adaptive distances.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 14:56:18 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Irpino", "Antonio", ""], ["De Carvalho", "Francisco", ""], ["Verde", "Rosanna", ""]]}, {"id": "1605.00519", "submitter": "Mario Lucic", "authors": "Mario Lucic, Olivier Bachem, Andreas Krause", "title": "Linear-time Outlier Detection via Sensitivity", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outliers are ubiquitous in modern data sets. Distance-based techniques are a\npopular non-parametric approach to outlier detection as they require no prior\nassumptions on the data generating distribution and are simple to implement.\nScaling these techniques to massive data sets without sacrificing accuracy is a\nchallenging task. We propose a novel algorithm based on the intuition that\noutliers have a significant influence on the quality of divergence-based\nclustering solutions. We propose sensitivity - the worst-case impact of a data\npoint on the clustering objective - as a measure of outlierness. We then prove\nthat influence, a (non-trivial) upper-bound on the sensitivity, can be computed\nby a simple linear time algorithm. To scale beyond a single machine, we propose\na communication efficient distributed algorithm. In an extensive experimental\nevaluation, we demonstrate the effectiveness and establish the statistical\nsignificance of the proposed approach. In particular, it outperforms the most\npopular distance-based approaches while being several orders of magnitude\nfaster.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 15:17:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lucic", "Mario", ""], ["Bachem", "Olivier", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.00529", "submitter": "Mario Lucic", "authors": "Mario Lucic, Mesrob I. Ohannessian, Amin Karbasi, Andreas Krause", "title": "Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning", "comments": "Conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with massive data, is it possible to trade off (statistical) risk, and\n(computational) space and time? This challenge lies at the heart of large-scale\nmachine learning. Using k-means clustering as a prototypical unsupervised\nlearning problem, we show how we can strategically summarize the data (control\nspace) in order to trade off risk and time when data is generated by a\nprobabilistic model. Our summarization is based on coreset constructions from\ncomputational geometry. We also develop an algorithm, TRAM, to navigate the\nspace/time/data/risk tradeoff in practice. In particular, we show that for a\nfixed risk (or data size), as the data size increases (resp. risk increases)\nthe running time of TRAM decreases. Our extensive experiments on real data sets\ndemonstrate the existence and practical utility of such tradeoffs, not only for\nk-means but also for Gaussian Mixture Models.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 15:32:14 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lucic", "Mario", ""], ["Ohannessian", "Mesrob I.", ""], ["Karbasi", "Amin", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.00596", "submitter": "Shuai Li", "authors": "Shuai Li and Claudio Gentile and Alexandros Karatzoglou", "title": "Graph Clustering Bandits for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an efficient context-dependent clustering technique for\nrecommender systems based on exploration-exploitation strategies through\nmulti-armed bandits over multiple users. Our algorithm dynamically groups users\nbased on their observed behavioral similarity during a sequence of logged\nactivities. In doing so, the algorithm reacts to the currently served user by\nshaping clusters around him/her but, at the same time, it explores the\ngeneration of clusters over users which are not currently engaged. We motivate\nthe effectiveness of this clustering policy, and provide an extensive empirical\nanalysis on real-world datasets, showing scalability and improved prediction\nperformance over state-of-the-art methods for sequential clustering of users in\nmulti-armed bandit scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 18:13:04 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Li", "Shuai", ""], ["Gentile", "Claudio", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1605.00609", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi, Anastasios Kyrillidis, Bernd G\\\"artner, Andreas Krause", "title": "Algorithms for Learning Sparse Additive Models with Interactions in High\n  Dimensions", "comments": "To appear in Information and Inference: A Journal of the IMA. Made\n  following changes after review process: (a) Corrected typos throughout the\n  text. (b) Corrected choice of sampling distribution in Section 5, see eqs.\n  (5.2), (5.3). (c) More detailed comparison with existing work in Section 8.\n  (d) Added Section B in appendix on roots of cubic equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a Sparse Additive\nModel (SPAM), if it is of the form $f(\\mathbf{x}) = \\sum_{l \\in\n\\mathcal{S}}\\phi_{l}(x_l)$ where $\\mathcal{S} \\subset [d]$, $|\\mathcal{S}| \\ll\nd$. Assuming $\\phi$'s, $\\mathcal{S}$ to be unknown, there exists extensive work\nfor estimating $f$ from its samples. In this work, we consider a generalized\nversion of SPAMs, that also allows for the presence of a sparse number of\nsecond order interaction terms. For some $\\mathcal{S}_1 \\subset [d],\n\\mathcal{S}_2 \\subset {[d] \\choose 2}$, with $|\\mathcal{S}_1| \\ll d,\n|\\mathcal{S}_2| \\ll d^2$, the function $f$ is now assumed to be of the form:\n$\\sum_{p \\in \\mathcal{S}_1}\\phi_{p} (x_p) + \\sum_{(l,l^{\\prime}) \\in\n\\mathcal{S}_2}\\phi_{(l,l^{\\prime})} (x_l,x_{l^{\\prime}})$. Assuming we have the\nfreedom to query $f$ anywhere in its domain, we derive efficient algorithms\nthat provably recover $\\mathcal{S}_1,\\mathcal{S}_2$ with finite sample bounds.\nOur analysis covers the noiseless setting where exact samples of $f$ are\nobtained, and also extends to the noisy setting where the queries are corrupted\nwith noise. For the noisy setting in particular, we consider two noise models\nnamely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methods\nfor identification of $\\mathcal{S}_2$ essentially rely on estimation of sparse\nHessian matrices, for which we provide two novel compressed sensing based\nschemes. Once $\\mathcal{S}_1, \\mathcal{S}_2$ are known, we show how the\nindividual components $\\phi_p$, $\\phi_{(l,l^{\\prime})}$ can be estimated via\nadditional queries of $f$, with uniform error bounds. Lastly, we provide\nsimulation results on synthetic data that validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 18:32:19 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 14:47:25 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 15:44:45 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tyagi", "Hemant", ""], ["Kyrillidis", "Anastasios", ""], ["G\u00e4rtner", "Bernd", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.00758", "submitter": "Elizabeth Hou", "authors": "Jes\\'us Arroyo, Elizabeth Hou", "title": "Efficient Distributed Estimation of Inverse Covariance Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/SSP.2016.7551705", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed systems, communication is a major concern due to issues such\nas its vulnerability or efficiency. In this paper, we are interested in\nestimating sparse inverse covariance matrices when samples are distributed into\ndifferent machines. We address communication efficiency by proposing a method\nwhere, in a single round of communication, each machine transfers a small\nsubset of the entries of the inverse covariance matrix. We show that, with this\nefficient distributed method, the error rates can be comparable with estimation\nin a non-distributed setting, and correct model selection is still possible.\nPractical performance is shown through simulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 05:51:21 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Hou", "Elizabeth", ""]]}, {"id": "1605.00779", "submitter": "Sipan Aslan", "authors": "Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun", "title": "Temporal Clustering of Time Series via Threshold Autoregressive Models:\n  Application to Commodity Prices", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aimed to find temporal clusters for several commodity prices using\nthe threshold non-linear autoregressive model. It is expected that the process\nof determining the commodity groups that are time-dependent will advance the\ncurrent knowledge about the dynamics of co-moving and coherent prices, and can\nserve as a basis for multivariate time series analyses. The clustering of\ncommodity prices was examined using the proposed clustering approach based on\ntime series models to incorporate the time varying properties of price series\ninto the clustering scheme. Accordingly, the primary aim in this study was\ngrouping time series according to the similarity between their Data Generating\nMechanisms (DGMs) rather than comparing pattern similarities in the time series\ntraces. The approximation to the DGM of each series was accomplished using\nthreshold autoregressive models, which are recognized for their ability to\nrepresent nonlinear features in time series, such as abrupt changes,\ntime-irreversibility and regime-shifting behavior. Through the use of the\nproposed approach, one can determine and monitor the set of co-moving time\nseries variables across the time dimension. Furthermore, generating a time\nvarying commodity price index and sub-indexes can become possible.\nConsequently, we conducted a simulation study to assess the effectiveness of\nthe proposed clustering approach and the results are presented for both the\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 08:13:58 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Aslan", "Sipan", ""], ["Yozgatligil", "Ceylan", ""], ["Iyigun", "Cem", ""]]}, {"id": "1605.00937", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Julien Mairal (LEAR), Bertrand Thirion\n  (PARIETAL), Ga\\\"el Varoquaux (PARIETAL)", "title": "Dictionary Learning for Massive Matrix Factorization", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2016, pp 1737-1746", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix factorization is a popular tool to obtain interpretable data\ndecompositions, which are also effective to perform data completion or\ndenoising. Its applicability to large datasets has been addressed with online\nand randomized methods, that reduce the complexity in one of the matrix\ndimension, but not in both of them. In this paper, we tackle very large\nmatrices in both dimensions. We propose a new factoriza-tion method that scales\ngracefully to terabyte-scale datasets, that could not be processed by previous\nalgorithms in a reasonable amount of time. We demonstrate the efficiency of our\napproach on massive functional Magnetic Resonance Imaging (fMRI) data, and on\nmatrix completion problems for recommender systems, where we obtain significant\nspeed-ups compared to state-of-the art coordinate descent methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:05:32 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 06:33:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Mairal", "Julien", "", "LEAR"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1605.00959", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar", "title": "Personalized Risk Scoring for Critical Care Patients using Mixtures of\n  Gaussian Process Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a personalized real time risk scoring algorithm that provides\ntimely and granular assessments for the clinical acuity of ward patients based\non their (temporal) lab tests and vital signs. Heterogeneity of the patients\npopulation is captured via a hierarchical latent class model. The proposed\nalgorithm aims to discover the number of latent classes in the patients\npopulation, and train a mixture of Gaussian Process (GP) experts, where each\nexpert models the physiological data streams associated with a specific class.\nSelf-taught transfer learning is used to transfer the knowledge of latent\nclasses learned from the domain of clinically stable patients to the domain of\nclinically deteriorating patients. For new patients, the posterior beliefs of\nall GP experts about the patient's clinical status given her physiological data\nstream are computed, and a personalized risk score is evaluated as a weighted\naverage of those beliefs, where the weights are learned from the patient's\nhospital admission information. Experiments on a heterogeneous cohort of 6,313\npatients admitted to Ronald Regan UCLA medical center show that our risk score\noutperforms the currently deployed risk scores, such as MEWS and Rothman\nscores.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:54:33 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Yoon", "Jinsung", ""], ["Hu", "Scott", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1605.01107", "submitter": "Alec Koppel", "authors": "Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro", "title": "Decentralized Dynamic Discriminative Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider discriminative dictionary learning in a distributed online\nsetting, where a network of agents aims to learn a common set of dictionary\nelements of a feature space and model parameters while sequentially receiving\nobservations. We formulate this problem as a distributed stochastic program\nwith a non-convex objective and present a block variant of the Arrow-Hurwicz\nsaddle point algorithm to solve it. Using Lagrange multipliers to penalize the\ndiscrepancy between them, only neighboring nodes exchange model information. We\nshow that decisions made with this saddle point algorithm asymptotically\nachieve a first-order stationarity condition on average.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 22:37:05 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Koppel", "Alec", ""], ["Warnell", "Garrett", ""], ["Stump", "Ethan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1605.01116", "submitter": "Truyen Tran", "authors": "Thuong Nguyen, Truyen Tran, Shivapratap Gopakumar, Dinh Phung, Svetha\n  Venkatesh", "title": "An evaluation of randomized machine learning methods for redundant data:\n  Predicting short and medium-term suicide risk from administrative records and\n  risk assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of suicide risk in mental health patients remains an open\nproblem. Existing methods including clinician judgments have acceptable\nsensitivity, but yield many false positives. Exploiting administrative data has\na great potential, but the data has high dimensionality and redundancies in the\nrecording processes. We investigate the efficacy of three most effective\nrandomized machine learning techniques random forests, gradient boosting\nmachines, and deep neural nets with dropout in predicting suicide risk. Using a\ncohort of mental health patients from a regional Australian hospital, we\ncompare the predictive performance with popular traditional approaches\nclinician judgments based on a checklist, sparse logistic regression and\ndecision trees. The randomized methods demonstrated robustness against data\nredundancies and superior predictive performance on AUC and F-measure.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 23:46:48 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Nguyen", "Thuong", ""], ["Tran", "Truyen", ""], ["Gopakumar", "Shivapratap", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1605.01185", "submitter": "Nandan Sudarsanam", "authors": "Nandan Sudarsanam and Balaraman Ravindran", "title": "Linear Bandit algorithms using the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents two new algorithms for solving linear stochastic bandit\nproblems. The proposed methods use an approach from non-parametric statistics\ncalled bootstrapping to create confidence bounds. This is achieved without\nmaking any assumptions about the distribution of noise in the underlying\nsystem. We present the X-Random and X-Fixed bootstrap bandits which correspond\nto the two well-known approaches for conducting bootstraps on models, in the\nliterature. The proposed methods are compared to other popular solutions for\nlinear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling.\nThe comparisons are carried out using a simulation study on a hierarchical\nprobability meta-model, built from published data of experiments, which are run\non real systems. The model representing the response surfaces is conceptualized\nas a Bayesian Network which is presented with varying degrees of noise for the\nsimulations. One of the proposed methods, X-Random bootstrap, performs better\nthan the baselines in-terms of cumulative regret across various degrees of\nnoise and different number of trials. In certain settings the cumulative regret\nof this method is less than half of the best baseline. The X-Fixed bootstrap\nperforms comparably in most situations and particularly well when the number of\ntrials is low. The study concludes that these algorithms could be a preferred\nalternative for solving linear bandit problems, especially when the\ndistribution of the noise in the system is unknown.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 08:52:10 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Sudarsanam", "Nandan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1605.01194", "submitter": "Sharmistha Jat", "authors": "Lavanya Sita Tekumalla and Sharmistha", "title": "IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based\n  Multiple Chunk Aligner", "comments": "SEMEVAL Workshop @ NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable semantic textual similarity (iSTS) task adds a crucial\nexplanatory layer to pairwise sentence similarity. We address various\ncomponents of this task: chunk level semantic alignment along with assignment\nof similarity type and score for aligned chunks with a novel system presented\nin this paper. We propose an algorithm, iMATCH, for the alignment of multiple\nnon-contiguous chunks based on Integer Linear Programming (ILP). Similarity\ntype and score assignment for pairs of chunks is done using a supervised\nmulticlass classification technique based on Random Forrest Classifier. Results\nshow that our algorithm iMATCH has low execution time and outperforms most\nother participating systems in terms of alignment score. Of the three datasets,\nwe are top ranked for answer- students dataset in terms of overall score and\nhave top alignment score for headlines dataset in the gold chunks track.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 09:36:49 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Tekumalla", "Lavanya Sita", ""], ["Sharmistha", "", ""]]}, {"id": "1605.01278", "submitter": "Adrian \\v{S}o\\v{s}i\\'c", "authors": "Adrian \\v{S}o\\v{s}i\\'c, Abdelhak M. Zoubir, Heinz Koeppl", "title": "A Bayesian Approach to Policy Recognition and State Representation\n  Learning", "comments": "17 pages, 8 figures; ### Version 4 ### to appear in IEEE Transactions\n  on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2711024", "report-no": null, "categories": "stat.ML cs.LG cs.SY math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from demonstration (LfD) is the process of building behavioral\nmodels of a task from demonstrations provided by an expert. These models can be\nused e.g. for system control by generalizing the expert demonstrations to\npreviously unencountered situations. Most LfD methods, however, make strong\nassumptions about the expert behavior, e.g. they assume the existence of a\ndeterministic optimal ground truth policy or require direct monitoring of the\nexpert's controls, which limits their practical use as part of a general system\nidentification framework. In this work, we consider the LfD problem in a more\ngeneral setting where we allow for arbitrary stochastic expert policies,\nwithout reasoning about the optimality of the demonstrations. Following a\nBayesian methodology, we model the full posterior distribution of possible\nexpert controllers that explain the provided demonstration data. Moreover, we\nshow that our methodology can be applied in a nonparametric context to infer\nthe complexity of the state representation used by the expert, and to learn\ntask-appropriate partitionings of the system state space.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 13:44:53 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 15:05:59 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 14:13:55 GMT"}, {"version": "v4", "created": "Fri, 4 Aug 2017 12:50:01 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["\u0160o\u0161i\u0107", "Adrian", ""], ["Zoubir", "Abdelhak M.", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1605.01384", "submitter": "Konstantinos Zygalakis", "authors": "Michael B. Giles and Mateusz B. Majka and Lukasz Szpruch and Sebastian\n  Vollmer and Konstantinos Zygalakis", "title": "Multilevel Monte Carlo methods for the approximation of invariant\n  measures of stochastic differential equations", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework that allows the use of the multi-level Monte Carlo\n(MLMC) methodology (Giles2015) to calculate expectations with respect to the\ninvariant measure of an ergodic SDE. In that context, we study the\n(over-damped) Langevin equations with a strongly concave potential. We show\nthat, when appropriate contracting couplings for the numerical integrators are\navailable, one can obtain a uniform in time estimate of the MLMC variance in\ncontrast to the majority of the results in the MLMC literature. As a\nconsequence, a root mean square error of $\\mathcal{O}(\\varepsilon)$ is achieved\nwith $\\mathcal{O}(\\varepsilon^{-2})$ complexity on par with Markov Chain Monte\nCarlo (MCMC) methods, which however can be computationally intensive when\napplied to large data sets. Finally, we present a multi-level version of the\nrecently introduced Stochastic Gradient Langevin Dynamics (SGLD) method\n(Welling and Teh, 2011) built for large datasets applications. We show that\nthis is the first stochastic gradient MCMC method with complexity\n$\\mathcal{O}(\\varepsilon^{-2}|\\log {\\varepsilon}|^{3})$, in contrast to the\ncomplexity $\\mathcal{O}(\\varepsilon^{-3})$ of currently available methods.\nNumerical experiments confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:12:13 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 08:57:39 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 11:07:26 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 14:07:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Giles", "Michael B.", ""], ["Majka", "Mateusz B.", ""], ["Szpruch", "Lukasz", ""], ["Vollmer", "Sebastian", ""], ["Zygalakis", "Konstantinos", ""]]}, {"id": "1605.01436", "submitter": "Behtash Babadi", "authors": "Abbas Kazemipour, Sina Miran, Piya Pal, Behtash Babadi, and Min Wu", "title": "Sampling Requirements for Stable Autoregressive Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2656848", "report-no": null, "categories": "cs.IT cs.DM math.IT math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of a linear univariate\nautoregressive model with sub-Gaussian innovations from a limited sequence of\nconsecutive observations. Assuming that the parameters are compressible, we\nanalyze the performance of the $\\ell_1$-regularized least squares as well as a\ngreedy estimator of the parameters and characterize the sampling trade-offs\nrequired for stable recovery in the non-asymptotic regime. In particular, we\nshow that for a fixed sparsity level, stable recovery of AR parameters is\npossible when the number of samples scale sub-linearly with the AR order. Our\nresults improve over existing sampling complexity requirements in AR estimation\nusing the LASSO, when the sparsity level scales faster than the square root of\nthe model order. We further derive sufficient conditions on the sparsity level\nthat guarantee the minimax optimality of the $\\ell_1$-regularized least squares\nestimate. Applying these techniques to simulated data as well as real-world\ndatasets from crude oil prices and traffic speed data confirm our predicted\ntheoretical performance gains in terms of estimation accuracy and model\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:07:04 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 19:22:02 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Miran", "Sina", ""], ["Pal", "Piya", ""], ["Babadi", "Behtash", ""], ["Wu", "Min", ""]]}, {"id": "1605.01559", "submitter": "Alain Durmus", "authors": "Alain Durmus and Eric Moulines", "title": "High-dimensional Bayesian inference via the Unadjusted Langevin\n  Algorithm", "comments": "Supplementary material available at\n  https://hal.inria.fr/hal-01176084/. arXiv admin note: substantial text\n  overlap with arXiv:1507.05021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the problem of sampling a high-dimensional\nprobability distribution $\\pi$ having a density with respect to the Lebesgue\nmeasure on $\\mathbb{R}^d$, known up to a normalization constant $x \\mapsto\n\\pi(x)= \\mathrm{e}^{-U(x)}/\\int_{\\mathbb{R}^d} \\mathrm{e}^{-U(y)} \\mathrm{d}\ny$. Such problem naturally occurs for example in Bayesian inference and machine\nlearning. Under the assumption that $U$ is continuously differentiable, $\\nabla\nU$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic\nbounds for the convergence to stationarity in Wasserstein distance of order $2$\nand total variation distance of the sampling method based on the Euler\ndiscretization of the Langevin stochastic differential equation, for both\nconstant and decreasing step sizes. The dependence on the dimension of the\nstate space of these bounds is explicit. The convergence of an appropriately\nweighted empirical measure is also investigated and bounds for the mean square\nerror and exponential deviation inequality are reported for functions which are\nmeasurable and bounded. An illustration to Bayesian inference for binary\nregression is presented to support our claims.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 11:42:35 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 02:19:57 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 08:55:10 GMT"}, {"version": "v4", "created": "Sun, 15 Jul 2018 09:47:23 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "1605.01573", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Observational-Interventional Priors for Dose-Response Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled interventions provide the most direct source of information for\nlearning causal effects. In particular, a dose-response curve can be learned by\nvarying the treatment level and observing the corresponding outcomes. However,\ninterventions can be expensive and time-consuming. Observational data, where\nthe treatment is not controlled by a known mechanism, is sometimes available.\nUnder some strong assumptions, observational data allows for the estimation of\ndose-response curves. Estimating such curves nonparametrically is hard: sample\nsizes for controlled interventions may be small, while in the observational\ncase a large number of measured confounders may need to be marginalized. In\nthis paper, we introduce a hierarchical Gaussian process prior that constructs\na distribution over the dose-response curve by learning from observational\ndata, and reshapes the distribution with a nonparametric affine transform\nlearned from controlled interventions. This function composition from different\nsources is shown to speed-up learning, which we demonstrate with a thorough\nsensitivity analysis and an application to modeling the effect of therapy on\ncognitive skills of premature infants.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 12:50:44 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1605.01635", "submitter": "Omid Sadjadi", "authors": "Seyed Omid Sadjadi, Jason Pelecanos, Sriram Ganapathy", "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis", "comments": "submitted to INTERSPEECH 2016. arXiv admin note: substantial text\n  overlap with arXiv:1602.07291", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the recent advances along with an error analysis of the IBM\nspeaker recognition system for conversational speech. Some of the key\nadvancements that contribute to our system include: a nearest-neighbor\ndiscriminant analysis (NDA) approach (as opposed to LDA) for intersession\nvariability compensation in the i-vector space, the application of speaker and\nchannel-adapted features derived from an automatic speech recognition (ASR)\nsystem for speaker recognition, and the use of a DNN acoustic model with a very\nlarge number of output units (~10k senones) to compute the frame-level soft\nalignments required in the i-vector estimation process. We evaluate these\ntechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as\nthe 10sec-10sec condition. To our knowledge, results achieved by our system\nrepresent the best performances published to date on these conditions. For\nexample, on the extended tel-tel condition (C5) the system achieves an EER of\n0.59%. To garner further understanding of the remaining errors (on C5), we\nexamine the recordings associated with the low scoring target trials, where\nvarious issues are identified for the problematic recordings/trials.\nInterestingly, it is observed that correcting the pathological recordings not\nonly improves the scores for the target trials but also for the nontarget\ntrials.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 15:57:21 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Sadjadi", "Seyed Omid", ""], ["Pelecanos", "Jason", ""], ["Ganapathy", "Sriram", ""]]}, {"id": "1605.01643", "submitter": "Jonathan Bates", "authors": "Jonathan Bates", "title": "The embedding dimension of Laplacian eigenfunction maps", "comments": "16 pages, 2 figures, 3 theorems, and a torus in a pear tree", "journal-ref": "Appl. Comput. Harmon. Anal. 37 (3) (2014) 516-530", "doi": "10.1016/j.acha.2014.03.002", "report-no": null, "categories": "stat.ML cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any closed, connected Riemannian manifold $M$ can be smoothly embedded by its\nLaplacian eigenfunction maps into $\\mathbb{R}^m$ for some $m$. We call the\nsmallest such $m$ the maximal embedding dimension of $M$. We show that the\nmaximal embedding dimension of $M$ is bounded from above by a constant\ndepending only on the dimension of $M$, a lower bound for injectivity radius, a\nlower bound for Ricci curvature, and a volume bound. We interpret this result\nfor the case of surfaces isometrically immersed in $\\mathbb{R}^3$, showing that\nthe maximal embedding dimension only depends on bounds for the Gaussian\ncurvature, mean curvature, and surface area. Furthermore, we consider the\nrelevance of these results for shape registration.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:18:51 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Bates", "Jonathan", ""]]}, {"id": "1605.01656", "submitter": "Jie Shen", "authors": "Jie Shen and Ping Li", "title": "A Tight Bound of Hard Thresholding", "comments": "V1 was submitted to COLT 2016. V2 fixes minor flaws, adds extra\n  experiments and discusses time complexity, V3 has been accepted to JMLR", "journal-ref": "Journal of Machine Learning Research 18(208): 1-42, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NA math.IT math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the hard thresholding operator which sets all\nbut the $k$ largest absolute elements of a vector to zero. We establish a {\\em\ntight} bound to quantitatively characterize the deviation of the thresholded\nsolution from a given signal. Our theoretical result is universal in the sense\nthat it holds for all choices of parameters, and the underlying analysis\ndepends only on fundamental arguments in mathematical optimization. We discuss\nthe implications for two domains:\n  Compressed Sensing. On account of the crucial estimate, we bridge the\nconnection between the restricted isometry property (RIP) and the sparsity\nparameter for a vast volume of hard thresholding based algorithms, which\nrenders an improvement on the RIP condition especially when the true sparsity\nis unknown. This suggests that in essence, many more kinds of sensing matrices\nor fewer measurements are admissible for the data acquisition procedure.\n  Machine Learning. In terms of large-scale machine learning, a significant yet\nchallenging problem is learning accurate sparse models in an efficient manner.\nIn stark contrast to prior work that attempted the $\\ell_1$-relaxation for\npromoting sparsity, we present a novel stochastic algorithm which performs hard\nthresholding in each iteration, hence ensuring such parsimonious solutions.\nEquipped with the developed bound, we prove the {\\em global linear convergence}\nfor a number of prevalent statistical models under mild assumptions, even\nthough the problem turns out to be non-convex.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 17:10:34 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 03:04:09 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 17:58:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shen", "Jie", ""], ["Li", "Ping", ""]]}, {"id": "1605.01677", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hiroshi Nakagawa", "title": "Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm,\n  and Computationally Efficient Algorithm", "comments": "To appear in ICML2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the K-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. The hardness of recommending Copeland winners, the arms that\nbeat the greatest number of other arms, is characterized by deriving an\nasymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical\nDivergence (CW-RMED) and derive an asymptotically optimal regret bound for it.\nHowever, it is not known whether the algorithm can be efficiently computed or\nnot. To address this issue, we devise an efficient version (ECW-RMED) and\nderive its asymptotic regret bound. Experimental comparisons of dueling bandit\nalgorithms show that ECW-RMED significantly outperforms existing ones.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 18:08:13 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 12:42:15 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1605.01703", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite and Nikolaj Tatti", "title": "A note on adjusting $R^2$ for using with cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to adjust the coefficient of determination ($R^2$) when used for\nmeasuring predictive accuracy via leave-one-out cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:34:08 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Zliobaite", "Indre", ""], ["Tatti", "Nikolaj", ""]]}, {"id": "1605.01749", "submitter": "Paul Bertens", "authors": "Paul Bertens", "title": "Rank Ordered Autoencoders", "comments": "Personal project, 14 pages, 9 figures. For source code see:\n  https://github.com/paulbertens", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for the unsupervised learning of sparse representations using\nautoencoders is proposed and implemented by ordering the output of the hidden\nunits by their activation value and progressively reconstructing the input in\nthis order. This can be done efficiently in parallel with the use of cumulative\nsums and sorting only slightly increasing the computational costs. Minimizing\nthe difference of this progressive reconstruction with respect to the input can\nbe seen as minimizing the number of active output units required for the\nreconstruction of the input. The model thus learns to reconstruct optimally\nusing the least number of active output units. This leads to high sparsity\nwithout the need for extra hyperparameters, the amount of sparsity is instead\nimplicitly learned by minimizing this progressive reconstruction error. Results\nof the trained model are given for patches of the CIFAR10 dataset, showing\nrapid convergence of features and extremely sparse output activations while\nmaintaining a minimal reconstruction error and showing extreme robustness to\noverfitting. Additionally the reconstruction as function of number of active\nunits is presented which shows the autoencoder learns a rank order code over\nthe input where the highest ranked units correspond to the highest decrease in\nreconstruction error.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 20:33:38 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Bertens", "Paul", ""]]}, {"id": "1605.01779", "submitter": "Matt Barnes", "authors": "Matt Barnes, Artur Dubrawski", "title": "Clustering on the Edge: Learning Structure in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent popularity of graphical clustering methods, there has been an\nincreased focus on the information between samples. We show how learning\ncluster structure using edge features naturally and simultaneously determines\nthe most likely number of clusters and addresses data scale issues. These\nresults are particularly useful in instances where (a) there are a large number\nof clusters and (b) we have some labeled edges. Applications in this domain\ninclude image segmentation, community discovery and entity resolution. Our\nmodel is an extension of the planted partition model and our solution uses\nresults of correlation clustering, which achieves a partition O(log(n))-close\nto the log-likelihood of the true clustering.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 22:23:21 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Barnes", "Matt", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1605.01939", "submitter": "Elena Mocanu", "authors": "Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu", "title": "Energy Disaggregation for Real-Time Building Flexibility Detection", "comments": "To appear in IEEE PES General Meeting, 2016, Boston, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is a limited resource which has to be managed wisely, taking into\naccount both supply-demand matching and capacity constraints in the\ndistribution grid. One aspect of the smart energy management at the building\nlevel is given by the problem of real-time detection of flexible demand\navailable. In this paper we propose the use of energy disaggregation techniques\nto perform this task. Firstly, we investigate the use of existing\nclassification methods to perform energy disaggregation. A comparison is\nperformed between four classifiers, namely Naive Bayes, k-Nearest Neighbors,\nSupport Vector Machine and AdaBoost. Secondly, we propose the use of Restricted\nBoltzmann Machine to automatically perform feature extraction. The extracted\nfeatures are then used as inputs to the four classifiers and consequently shown\nto improve their accuracy. The efficiency of our approach is demonstrated on a\nreal database consisting of detailed appliance-level measurements with high\ntemporal resolution, which has been used for energy disaggregation in previous\nstudies, namely the REDD. The results show robustness and good generalization\ncapabilities to newly presented buildings with at least 96% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 13:52:45 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Mocanu", "Elena", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""]]}, {"id": "1605.02105", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c and Alex Olshevsky and C\\'esar Uribe", "title": "Distributed Learning with Infinitely Many Hypotheses", "comments": "Submitted to CDC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed learning setup where a network of agents\nsequentially access realizations of a set of random variables with unknown\ndistributions. The network objective is to find a parametrized distribution\nthat best describes their joint observations in the sense of the\nKullback-Leibler divergence. Apart from recent efforts in the literature, we\nanalyze the case of countably many hypotheses and the case of a continuum of\nhypotheses. We provide non-asymptotic bounds for the concentration rate of the\nagents' beliefs around the correct hypothesis in terms of the number of agents,\nthe network parameters, and the learning abilities of the agents. Additionally,\nwe provide a novel motivation for a general set of distributed Non-Bayesian\nupdate rules as instances of the distributed stochastic mirror descent\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 21:47:36 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar", ""]]}, {"id": "1605.02113", "submitter": "Radu V. Craiu", "authors": "Reihaneh Entezari, Radu V. Craiu, and Jeffrey S. Rosenthal", "title": "Likelihood Inflating Sampling Algorithm", "comments": "32 pages, 3 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution\ncorresponding to a massive data set can be computationally prohibitive since\nproducing one sample requires a number of operations that is linear in the data\nsize. In this paper, we introduce a new communication-free parallel method, the\nLikelihood Inflating Sampling Algorithm (LISA), that significantly reduces\ncomputational costs by randomly splitting the dataset into smaller subsets and\nrunning MCMC methods independently in parallel on each subset using different\nprocessors. Each processor will be used to run an MCMC chain that samples\nsub-posterior distributions which are defined using an \"inflated\" likelihood\nfunction. We develop a strategy for combining the draws from different\nsub-posteriors to study the full posterior of the Bayesian Additive Regression\nTrees (BART) model. The performance of the method is tested using both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 22:43:15 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 06:19:11 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 17:57:32 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Entezari", "Reihaneh", ""], ["Craiu", "Radu V.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1605.02190", "submitter": "Giulio Caravagna", "authors": "Giulio Caravagna, Luca Bortolussi, Guido Sanguinetti", "title": "Matching models across abstraction levels with Gaussian Processes", "comments": "LNCS format", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological systems are often modelled at different levels of abstraction\ndepending on the particular aims/resources of a study. Such different models\noften provide qualitatively concordant predictions over specific\nparametrisations, but it is generally unclear whether model predictions are\nquantitatively in agreement, and whether such agreement holds for different\nparametrisations. Here we present a generally applicable statistical machine\nlearning methodology to automatically reconcile the predictions of different\nmodels across abstraction levels. Our approach is based on defining a\ncorrection map, a random function which modifies the output of a model in order\nto match the statistics of the output of a different model of the same system.\nWe use two biological examples to give a proof-of-principle demonstration of\nthe methodology, and discuss its advantages and potential further applications.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 13:00:05 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Caravagna", "Giulio", ""], ["Bortolussi", "Luca", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1605.02234", "submitter": "Farouk Nathoo", "authors": "Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance and\n  Farouk S. Nathoo", "title": "A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recent advances in technology for brain imaging and\nhigh-throughput genotyping have motivated studies examining the influence of\ngenetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have\ndeveloped an approach for the analysis of imaging genomic studies using\npenalized multi-task regression with regularization based on a novel group\n$l_{2,1}$-norm penalty which encourages structured sparsity at both the gene\nlevel and SNP level. While incorporating a number of useful features, the\nproposed method only furnishes a point estimate of the regression coefficients;\ntechniques for conducting statistical inference are not provided. A new\nBayesian method is proposed here to overcome this limitation.\n  Results: We develop a Bayesian hierarchical modeling formulation where the\nposterior mode corresponds to the estimator proposed by Wang et al.\n(Bioinformatics, 2012), and an approach that allows for full posterior\ninference including the construction of interval estimates for the regression\nparameters. We show that the proposed hierarchical model can be expressed as a\nthree-level Gaussian scale mixture and this representation facilitates the use\nof a Gibbs sampling algorithm for posterior simulation. Simulation studies\ndemonstrate that the interval estimates obtained using our approach achieve\nadequate coverage probabilities that outperform those obtained from the\nnonparametric bootstrap. Our proposed methodology is applied to the analysis of\nneuroimaging and genetic data collected as part of the Alzheimer's Disease\nNeuroimaging Initiative (ADNI), and this analysis of the ADNI cohort\ndemonstrates clearly the value added of incorporating interval estimation\nbeyond only point estimation when relating SNPs to brain imaging\nendophenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 19:16:53 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:46:27 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Greenlaw", "Keelin", ""], ["Szefer", "Elena", ""], ["Graham", "Jinko", ""], ["Lesperance", "Mary", ""], ["Nathoo", "Farouk S.", ""]]}, {"id": "1605.02268", "submitter": "Ahmad Beirami", "authors": "Matthew Nokleby, Ahmad Beirami, and Robert Calderbank", "title": "Rate-Distortion Bounds on Bayes Risk in Supervised Learning", "comments": "Revised submission to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an information-theoretic framework for bounding the number of\nlabeled samples needed to train a classifier in a parametric Bayesian setting.\nWe derive bounds on the average $L_p$ distance between the learned classifier\nand the true maximum a posteriori classifier, which are well-established\nsurrogates for the excess classification error due to imperfect learning. We\nprovide lower and upper bounds on the rate-distortion function, using $L_p$\nloss as the distortion measure, of a maximum a priori classifier in terms of\nthe differential entropy of the posterior distribution and a quantity called\nthe interpolation dimension, which characterizes the complexity of the\nparametric distribution family. In addition to expressing the information\ncontent of a classifier in terms of lossy compression, the rate-distortion\nfunction also expresses the minimum number of bits a learning machine needs to\nextract from training data to learn a classifier to within a specified $L_p$\ntolerance. We use results from universal source coding to express the\ninformation content in the training data in terms of the Fisher information of\nthe parametric family and the number of training samples available. The result\nis a framework for computing lower bounds on the Bayes $L_p$ risk. This\nframework complements the well-known probably approximately correct (PAC)\nframework, which provides minimax risk bounds involving the Vapnik-Chervonenkis\ndimension or Rademacher complexity. Whereas the PAC framework provides upper\nbounds the risk for the worst-case data distribution, the proposed\nrate-distortion framework lower bounds the risk averaged over the data\ndistribution. We evaluate the bounds for a variety of data models, including\ncategorical, multinomial, and Gaussian models. In each case the bounds are\nprovably tight orderwise, and in two cases we prove that the bounds are tight\nup to multiplicative constants.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 03:54:34 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 17:58:36 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Nokleby", "Matthew", ""], ["Beirami", "Ahmad", ""], ["Calderbank", "Robert", ""]]}, {"id": "1605.02277", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Jing Lei and Stephen E. Fienberg", "title": "On-Average KL-Privacy and its equivalence to Generalization for\n  Max-Entropy Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define On-Average KL-Privacy and present its properties and connections to\ndifferential privacy, generalization and information-theoretic quantities\nincluding max-information and mutual information. The new definition\nsignificantly weakens differential privacy, while preserving its minimalistic\ndesign features such as composition over small group and multiple queries as\nwell as closeness to post-processing. Moreover, we show that On-Average\nKL-Privacy is **equivalent** to generalization for a large class of\ncommonly-used tools in statistics and machine learning that samples from Gibbs\ndistributions---a class of distributions that arises naturally from the maximum\nentropy principle. In addition, a byproduct of our analysis yields a lower\nbound for generalization error in terms of mutual information which reveals an\ninteresting interplay with known upper bounds that use the same quantity.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 05:13:33 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lei", "Jing", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1605.02315", "submitter": "Vince Lyzinski", "authors": "Vince Lyzinski", "title": "Information Recovery in Shuffled Graphs via Graph Matching", "comments": "55 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many multiple graph inference methodologies operate under the implicit\nassumption that an explicit vertex correspondence is known across the vertex\nsets of the graphs, in practice these correspondences may only be partially or\nerrorfully known. Herein, we provide an information theoretic foundation for\nunderstanding the practical impact that errorfully observed vertex\ncorrespondences can have on subsequent inference, and the capacity of graph\nmatching methods to recover the lost vertex alignment and inferential\nperformance. Working in the correlated stochastic blockmodel setting, we\nestablish a duality between the loss of mutual information due to an errorfully\nobserved vertex correspondence and the ability of graph matching algorithms to\nrecover the true correspondence across graphs. In the process, we establish a\nphase transition for graph matchability in terms of the correlation across\ngraphs, and we conjecture the analogous phase transition for the relative\ninformation loss due to shuffling vertex labels. We demonstrate the practical\neffect that graph shuffling---and matching---can have on subsequent inference,\nwith examples from two sample graph hypothesis testing and joint spectral graph\nclustering.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 12:35:33 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 21:14:32 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Lyzinski", "Vince", ""]]}, {"id": "1605.02408", "submitter": "Shiqian Ma", "authors": "Bo Jiang, Tianyi Lin, Shiqian Ma, Shuzhong Zhang", "title": "Structured Nonconvex and Nonsmooth Optimization: Algorithms and\n  Iteration Complexity Analysis", "comments": "Section 4.1 is updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex and nonsmooth optimization problems are frequently encountered in\nmuch of statistics, business, science and engineering, but they are not yet\nwidely recognized as a technology in the sense of scalability. A reason for\nthis relatively low degree of popularity is the lack of a well developed system\nof theory and algorithms to support the applications, as is the case for its\nconvex counterpart. This paper aims to take one step in the direction of\ndisciplined nonconvex and nonsmooth optimization. In particular, we consider in\nthis paper some constrained nonconvex optimization models in block decision\nvariables, with or without coupled affine constraints. In the case of without\ncoupled constraints, we show a sublinear rate of convergence to an\n$\\epsilon$-stationary solution in the form of variational inequality for a\ngeneralized conditional gradient method, where the convergence rate is shown to\nbe dependent on the H\\\"olderian continuity of the gradient of the smooth part\nof the objective. For the model with coupled affine constraints, we introduce\ncorresponding $\\epsilon$-stationarity conditions, and apply two proximal-type\nvariants of the ADMM to solve such a model, assuming the proximal ADMM updates\ncan be implemented for all the block variables except for the last block, for\nwhich either a gradient step or a majorization-minimization step is\nimplemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to\nreach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show\nthat the same iteration complexity of a proximal BCD method follows\nimmediately. Numerical results are provided to illustrate the efficacy of the\nproposed algorithms for tensor robust PCA.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 03:39:49 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 05:50:42 GMT"}, {"version": "v3", "created": "Fri, 22 Sep 2017 17:17:58 GMT"}, {"version": "v4", "created": "Wed, 15 Nov 2017 00:04:35 GMT"}, {"version": "v5", "created": "Wed, 17 Jan 2018 22:57:33 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Jiang", "Bo", ""], ["Lin", "Tianyi", ""], ["Ma", "Shiqian", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1605.02470", "submitter": "Nikhil Karamchandani", "authors": "Vivek S. Borkar, Nikhil Karamchandani, Sharad Mirani", "title": "Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of inferring the overall ranking among entities in the\nframework of Bradley-Terry-Luce (BTL) model, based on available empirical data\non pairwise preferences. By a simple transformation, we can cast the problem as\nthat of solving a noisy linear system, for which a ready algorithm is available\nin the form of the randomized Kaczmarz method. This scheme is provably\nconvergent, has excellent empirical performance, and is amenable to on-line,\ndistributed and asynchronous variants. Convergence, convergence rate, and error\nanalysis of the proposed algorithm are presented and several numerical\nexperiments are conducted whose results validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 08:36:55 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Borkar", "Vivek S.", ""], ["Karamchandani", "Nikhil", ""], ["Mirani", "Sharad", ""]]}, {"id": "1605.02531", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba and Shie Mannor", "title": "Clustering Time Series and the Surprising Robustness of HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given a time series where consecutive samples are\nbelieved to come from a probabilistic source, that the source changes from time\nto time and that the total number of sources is fixed. Our objective is to\nestimate the distributions of the sources. A standard approach to this problem\nis to model the data as a hidden Markov model (HMM). However, since the data\noften lacks the Markov or the stationarity properties of an HMM, one can ask\nwhether this approach is still suitable or perhaps another approach is\nrequired. In this paper we show that a maximum likelihood HMM estimator can be\nused to approximate the source distributions in a much larger class of models\nthan HMMs. Specifically, we propose a natural and fairly general non-stationary\nmodel of the data, where the only restriction is that the sources do not change\ntoo often. Our main result shows that for this model, a maximum-likelihood HMM\nestimator produces the correct second moment of the data, and the results can\nbe extended to higher moments.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:24:19 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 13:49:37 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "1605.02536", "submitter": "Romain Brault Rb", "authors": "Romain Brault, Florence d'Alch\\'e-Buc, Markus Heinonen", "title": "Random Fourier Features for Operator-Valued Kernels", "comments": "32 pages, 6 figures", "journal-ref": "ACML, Hamilton, New-Zealand, JMLR Workshop and Conference\n  Proceedings, November 2016, vol. 63, pp. 110-125", "doi": null, "report-no": "PMLR 63:110-125", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devoted to multi-task learning and structured output learning,\noperator-valued kernels provide a flexible tool to build vector-valued\nfunctions in the context of Reproducing Kernel Hilbert Spaces. To scale up\nthese methods, we extend the celebrated Random Fourier Feature methodology to\nget an approximation of operator-valued kernels. We propose a general principle\nfor Operator-valued Random Fourier Feature construction relying on a\ngeneralization of Bochner's theorem for translation-invariant operator-valued\nMercer kernels. We prove the uniform convergence of the kernel approximation\nfor bounded and unbounded operator random Fourier features using appropriate\nBernstein matrix concentration inequality. An experimental proof-of-concept\nshows the quality of the approximation and the efficiency of the corresponding\nlinear models on example datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:36:40 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 16:35:41 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 12:59:58 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Brault", "Romain", ""], ["d'Alch\u00e9-Buc", "Florence", ""], ["Heinonen", "Markus", ""]]}, {"id": "1605.02540", "submitter": "Fabrice Rossi", "authors": "Marco Corneli (SAMM), Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Exact ICL maximization in a non-stationary temporal extension of the\n  stochastic block model for dynamic networks", "comments": null, "journal-ref": "Neurocomputing, Elsevier, 2016, Advances in artificial neural\n  networks, machine learning and computational intelligence - Selected papers\n  from the 23rd European Symposium on Artificial Neural Networks (ESANN 2015),\n  192, pp.81-91", "doi": "10.1016/j.neucom.2016.02.031", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a flexible probabilistic tool that can be\nused to model interactions between clusters of nodes in a network. However, it\ndoes not account for interactions of time varying intensity between clusters.\nThe extension of the SBM developed in this paper addresses this shortcoming\nthrough a temporal partition: assuming interactions between nodes are recorded\non fixed-length time intervals, the inference procedure associated with the\nmodel we propose allows to cluster simultaneously the nodes of the network and\nthe time intervals. The number of clusters of nodes and of time intervals, as\nwell as the memberships to clusters, are obtained by maximizing an exact\nintegrated complete-data likelihood, relying on a greedy search approach.\nExperiments on simulated and real data are carried out in order to assess the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:44:24 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 09:37:27 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Corneli", "Marco", "", "SAMM"], ["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1605.02541", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (Viadeo, SAMM), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Mean Absolute Percentage Error for regression models", "comments": null, "journal-ref": "Neurocomputing, Elsevier, 2016, Advances in artificial neural\n  networks, machine learning and computational intelligence - Selected papers\n  from the 23rd European Symposium on Artificial Neural Networks (ESANN 2015),\n  192, pp.38 - 48", "doi": "10.1016/j.neucom.2015.12.114", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We prove the\nexistence of an optimal MAPE model and we show the universal consistency of\nEmpirical Risk Minimization based on the MAPE. We also show that finding the\nbest model under the MAPE is equivalent to doing weighted Mean Absolute Error\n(MAE) regression, and we apply this weighting strategy to kernel regression.\nThe behavior of the MAPE kernel regression is illustrated on simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:46:26 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 09:36:23 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["De Myttenaere", "Arnaud", "", "Viadeo, SAMM"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1605.02609", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni, Marcel A. J. van Gerven and Eric Maris", "title": "Dynamic Decomposition of Spatiotemporal Neural Signals", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005540", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural signals are characterized by rich temporal and spatiotemporal dynamics\nthat reflect the organization of cortical networks. Theoretical research has\nshown how neural networks can operate at different dynamic ranges that\ncorrespond to specific types of information processing. Here we present a data\nanalysis framework that uses a linearized model of these dynamic states in\norder to decompose the measured neural signal into a series of components that\ncapture both rhythmic and non-rhythmic neural activity. The method is based on\nstochastic differential equations and Gaussian process regression. Through\ncomputer simulations and analysis of magnetoencephalographic data, we\ndemonstrate the efficacy of the method in identifying meaningful modulations of\noscillatory signals corrupted by structured temporal and spatiotemporal noise.\nThese results suggest that the method is particularly suitable for the analysis\nand interpretation of complex temporal and spatiotemporal neural signals.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 14:47:53 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Ambrogioni", "Luca", ""], ["van Gerven", "Marcel A. J.", ""], ["Maris", "Eric", ""]]}, {"id": "1605.02633", "submitter": "Chong You", "authors": "Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal", "title": "Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace\n  Clustering", "comments": "15 pages, 6 figures, accepted to CVPR 2016 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art subspace clustering methods are based on expressing each\ndata point as a linear combination of other data points while regularizing the\nmatrix of coefficients with $\\ell_1$, $\\ell_2$ or nuclear norms. $\\ell_1$\nregularization is guaranteed to give a subspace-preserving affinity (i.e.,\nthere are no connections between points from different subspaces) under broad\ntheoretical conditions, but the clusters may not be connected. $\\ell_2$ and\nnuclear norm regularization often improve connectivity, but give a\nsubspace-preserving affinity only for independent subspaces. Mixed $\\ell_1$,\n$\\ell_2$ and nuclear norm regularizations offer a balance between the\nsubspace-preserving and connectedness properties, but this comes at the cost of\nincreased computational complexity. This paper studies the geometry of the\nelastic net regularizer (a mixture of the $\\ell_1$ and $\\ell_2$ norms) and uses\nit to derive a provably correct and scalable active set method for finding the\noptimal coefficients. Our geometric analysis also provides a theoretical\njustification and a geometric interpretation for the balance between the\nconnectedness (due to $\\ell_2$ regularization) and subspace-preserving (due to\n$\\ell_1$ regularization) properties for elastic net subspace clustering. Our\nexperiments show that the proposed active set method not only achieves\nstate-of-the-art clustering performance, but also efficiently handles\nlarge-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 15:49:36 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["You", "Chong", ""], ["Li", "Chun-Guang", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "1605.02654", "submitter": "Alexander Vervuurt", "authors": "Yves-Laurent Kom Samo, Alexander Vervuurt", "title": "Stochastic Portfolio Theory: A Machine Learning Perspective", "comments": "9 pages, UAI 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel application of Gaussian processes (GPs) to\nfinancial asset allocation. Our approach is deeply rooted in Stochastic\nPortfolio Theory (SPT), a stochastic analysis framework introduced by Robert\nFernholz that aims at flexibly analysing the performance of certain investment\nstrategies in stock markets relative to benchmark indices. In particular, SPT\nhas exhibited some investment strategies based on company sizes that, under\nrealistic assumptions, outperform benchmark indices with probability 1 over\ncertain time horizons. Galvanised by this result, we consider the inverse\nproblem that consists of learning (from historical data) an optimal investment\nstrategy based on any given set of trading characteristics, and using a\nuser-specified optimality criterion that may go beyond outperforming a\nbenchmark index. Although this inverse problem is of the utmost interest to\ninvestment management practitioners, it can hardly be tackled using the SPT\nframework. We show that our machine learning approach learns investment\nstrategies that considerably outperform existing SPT strategies in the US stock\nmarket.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 16:53:27 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Samo", "Yves-Laurent Kom", ""], ["Vervuurt", "Alexander", ""]]}, {"id": "1605.02674", "submitter": "Sergio Mu\\~noz-Romero", "authors": "Sergio Mu\\~noz-Romero, Vanessa G\\'omez-Verdejo, Jer\\'onimo\n  Arenas-Garc\\'ia", "title": "Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate\n  Analysis", "comments": "9 pages; added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Analysis (MVA) comprises a family of well-known methods for\nfeature extraction that exploit correlations among input variables of the data\nrepresentation. One important property that is enjoyed by most such methods is\nuncorrelation among the extracted features. Recently, regularized versions of\nMVA methods have appeared in the literature, mainly with the goal to gain\ninterpretability of the solution. In these cases, the solutions can no longer\nbe obtained in a closed manner, and it is frequent to recur to the iteration of\ntwo steps, one of them being an orthogonal Procrustes problem. This letter\nshows that the Procrustes solution is not optimal from the perspective of the\noverall MVA method, and proposes an alternative approach based on the solution\nof an eigenvalue problem. Our method ensures the preservation of several\nproperties of the original methods, most notably the uncorrelation of the\nextracted features, as demonstrated theoretically and through a collection of\nselected experiments.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:07:06 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 23:14:21 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Mu\u00f1oz-Romero", "Sergio", ""], ["G\u00f3mez-Verdejo", "Vanessa", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""]]}, {"id": "1605.02693", "submitter": "Eric Hall", "authors": "Eric C. Hall and Garvesh Raskutti and Rebecca Willett", "title": "Inference of High-dimensional Autoregressive Generalized Linear Models", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregressive models characterize a variety of time series in which\nlinear combinations of current and past observations can be used to accurately\npredict future observations. For instance, each element of an observation\nvector could correspond to a different node in a network, and the parameters of\nan autoregressive model would correspond to the impact of the network structure\non the time series evolution. Often these models are used successfully in\npractice to learn the structure of social, epidemiological, financial, or\nbiological neural networks. However, little is known about statistical\nguarantees on estimates of such models in non-Gaussian settings. This paper\naddresses the inference of the autoregressive parameters and associated network\nstructure within a generalized linear model framework that includes Poisson and\nBernoulli autoregressive processes. At the heart of this analysis is a\nsparsity-regularized maximum likelihood estimator. While\nsparsity-regularization is well-studied in the statistics and machine learning\ncommunities, those analysis methods cannot be applied to autoregressive\ngeneralized linear models because of the correlations and potential\nheteroscedasticity inherent in the observations. Sample complexity bounds are\nderived using a combination of martingale concentration inequalities and modern\nempirical process techniques for dependent random variables. These bounds,\nwhich are supported by several simulation studies, characterize the impact of\nvarious network parameters on estimator performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:54:17 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 18:20:18 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Hall", "Eric C.", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""]]}, {"id": "1605.02699", "submitter": "Saikat Basu", "authors": "Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay,\n  Sangram Ganguly, Ramakrishna Nemani and Shreekant Gayaka", "title": "A Theoretical Analysis of Deep Neural Networks for Texture\n  Classification", "comments": "Accepted in International Joint Conference on Neural Networks, IJCNN\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Deep Neural Networks for the classification of\nimage datasets where texture features are important for generating\nclass-conditional discriminative representations. To this end, we first derive\nthe size of the feature space for some standard textural features extracted\nfrom the input dataset and then use the theory of Vapnik-Chervonenkis dimension\nto show that hand-crafted feature extraction creates low-dimensional\nrepresentations which help in reducing the overall excess error rate. As a\ncorollary to this analysis, we derive for the first time upper bounds on the VC\ndimension of Convolutional Neural Network as well as Dropout and Dropconnect\nnetworks and the relation between excess error rate of Dropout and Dropconnect\nnetworks. The concept of intrinsic dimension is used to validate the intuition\nthat texture-based datasets are inherently higher dimensional as compared to\nhandwritten digits or other object recognition datasets and hence more\ndifficult to be shattered by neural networks. We then derive the mean distance\nfrom the centroid to the nearest and farthest sampling points in an\nn-dimensional manifold and show that the Relative Contrast of the sample data\nvanishes as dimensionality of the underlying vector space tends to infinity.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:11:22 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 19:32:06 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Basu", "Saikat", ""], ["Karki", "Manohar", ""], ["DiBiano", "Robert", ""], ["Mukhopadhyay", "Supratik", ""], ["Ganguly", "Sangram", ""], ["Nemani", "Ramakrishna", ""], ["Gayaka", "Shreekant", ""]]}, {"id": "1605.02711", "submitter": "Xingguo Li", "authors": "Xingguo Li, Raman Arora, Han Liu, Jarvis Haupt, Tuo Zhao", "title": "Nonconvex Sparse Learning via Stochastic Optimization with Progressive\n  Variance Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic variance reduced optimization algorithm for solving\nsparse learning problems with cardinality constraints. Sufficient conditions\nare provided, under which the proposed algorithm enjoys strong linear\nconvergence guarantees and optimal estimation accuracy in high dimensions. We\nfurther extend the proposed algorithm to an asynchronous parallel variant with\na near linear speedup. Numerical experiments demonstrate the efficiency of our\nalgorithm in terms of both parameter estimation and computational performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:44:17 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 20:20:12 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 20:50:18 GMT"}, {"version": "v4", "created": "Mon, 13 Mar 2017 17:50:18 GMT"}, {"version": "v5", "created": "Sat, 23 Dec 2017 17:54:03 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Li", "Xingguo", ""], ["Arora", "Raman", ""], ["Liu", "Han", ""], ["Haupt", "Jarvis", ""], ["Zhao", "Tuo", ""]]}, {"id": "1605.02784", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Identification of refugee influx patterns in Greece via model-theoretic\n  analysis of daily arrivals", "comments": "21 pages, 26 figures, 1 table, 23 equations, 72 references", "journal-ref": null, "doi": null, "report-no": "HG/GT.0507.01v1", "categories": "stat.ML cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The refugee crisis is perhaps the single most challenging problem for Europe\ntoday. Hundreds of thousands of people have already traveled across dangerous\nsea passages from Turkish shores to Greek islands, resulting in thousands of\ndead and missing, despite the best rescue efforts from both sides. One of the\nmain reasons is the total lack of any early warning-alerting system, which\ncould provide some preparation time for the prompt and effective deployment of\nresources at the hot zones. This work is such an attempt for a systemic\nanalysis of the refugee influx in Greece, aiming at (a) the statistical and\nsignal-level characterization of the smuggling networks and (b) the formulation\nand preliminary assessment of such models for predictive purposes, i.e., as the\nbasis of such an early warning-alerting protocol. To our knowledge, this is the\nfirst-ever attempt to design such a system, since this refugee crisis itself\nand its geographical properties are unique (intense event handling, little or\nno warning). The analysis employs a wide range of statistical, signal-based and\nmatrix factorization (decomposition) techniques, including linear &\nlinear-cosine regression, spectral analysis, ARMA, SVD, Probabilistic PCA, ICA,\nK-SVD for Dictionary Learning, as well as fractal dimension analysis. It is\nestablished that the behavioral patterns of the smuggling networks closely\nmatch (as expected) the regular burst and pause periods of store-and-forward\nnetworks in digital communications. There are also major periodic trends in the\nrange of 6.2-6.5 days and strong correlations in lags of four or more days,\nwith distinct preference in the Sunday-Monday 48-hour time frame. These results\nshow that such models can be used successfully for short-term forecasting of\nthe influx intensity, producing an invaluable operational asset for planners,\ndecision-makers and first-responders.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 21:08:12 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1605.02832", "submitter": "Sho Sonoda Dr", "authors": "Sho Sonoda, Noboru Murata", "title": "Transport Analysis of Infinitely Deep Neural Network", "comments": null, "journal-ref": "Journal of Machine Learning Research 20(2):1-52, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the feature map inside deep neural networks (DNNs) by\ntracking the transport map. We are interested in the role of depth (why do DNNs\nperform better than shallow models?) and the interpretation of DNNs (what do\nintermediate layers do?) Despite the rapid development in their application,\nDNNs remain analytically unexplained because the hidden layers are nested and\nthe parameters are not faithful. Inspired by the integral representation of\nshallow NNs, which is the continuum limit of the width, or the hidden unit\nnumber, we developed the flow representation and transport analysis of DNNs.\nThe flow representation is the continuum limit of the depth or the hidden layer\nnumber, and it is specified by an ordinary differential equation with a vector\nfield. We interpret an ordinary DNN as a transport map or a Euler broken line\napproximation of the flow. Technically speaking, a dynamical system is a\nnatural model for the nested feature maps. In addition, it opens a new way to\nthe coordinate-free treatment of DNNs by avoiding the redundant parametrization\nof DNNs. Following Wasserstein geometry, we analyze a flow in three aspects:\ndynamical system, continuity equation, and Wasserstein gradient flow. A key\nfinding is that we specified a series of transport maps of the denoising\nautoencoder (DAE). Starting from the shallow DAE, this paper develops three\ntopics: the transport map of the deep DAE, the equivalence between the stacked\nDAE and the composition of DAEs, and the development of the double continuum\nlimit or the integral representation of the flow representation. As partial\nanswers to the research questions, we found that deeper DAEs converge faster\nand the extracted features are better; in addition, a deep Gaussian DAE\ntransports mass to decrease the Shannon entropy of the data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 03:06:23 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 17:53:12 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sonoda", "Sho", ""], ["Murata", "Noboru", ""]]}, {"id": "1605.02869", "submitter": "Xiaoli Wu", "authors": "Qi She, Xiaoli Wu, Beth Jelfs, Adam S. Charles, Rosa H.M.Chan", "title": "An Efficient and Flexible Spike Train Model via Empirical Bayes", "comments": "16 pages, 20 figures, 3 tables", "journal-ref": "IEEE Trans. Signal Processing 69 (2021) 3236-3251", "doi": "10.1109/TSP.2021.3076885", "report-no": null, "categories": "q-bio.QM eess.SP q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Accurate statistical models of neural spike responses can characterize the\ninformation carried by neural populations. But the limited samples of spike\ncounts during recording usually result in model overfitting. Besides, current\nmodels assume spike counts to be Poisson-distributed, which ignores the fact\nthat many neurons demonstrate over-dispersed spiking behaviour. Although the\nNegative Binomial Generalized Linear Model (NB-GLM) provides a powerful tool\nfor modeling over-dispersed spike counts, the maximum likelihood-based standard\nNB-GLM leads to highly variable and inaccurate parameter estimates. Thus, we\npropose a hierarchical parametric empirical Bayes method to estimate the neural\nspike responses among neuronal population. Our method integrates both\nGeneralized Linear Models (GLMs) and empirical Bayes theory, which aims to (1)\nimprove the accuracy and reliability of parameter estimation, compared to the\nmaximum likelihood-based method for NB-GLM and Poisson-GLM; (2) effectively\ncapture the over-dispersion nature of spike counts from both simulated data and\nexperimental data; and (3) provide insight into both neural interactions and\nspiking behaviours of the neuronal populations. We apply our approach to study\nboth simulated data and experimental neural data. The estimation of simulation\ndata indicates that the new framework can accurately predict mean spike counts\nsimulated from different models and recover the connectivity weights among\nneural populations. The estimation based on retinal neurons demonstrate the\nproposed method outperforms both NB-GLM and Poisson-GLM in terms of the\npredictive log-likelihood of held-out data. Codes are available in\nhttps://doi.org/10.5281/zenodo.4704423\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 06:57:16 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 07:39:29 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 01:34:32 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 13:20:57 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 13:14:48 GMT"}, {"version": "v6", "created": "Tue, 27 Apr 2021 05:07:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["She", "Qi", ""], ["Wu", "Xiaoli", ""], ["Jelfs", "Beth", ""], ["Charles", "Adam S.", ""], ["Chan", "Rosa H. M.", ""]]}, {"id": "1605.02887", "submitter": "Yunlong Feng", "authors": "Hanyuan Hang, Yunlong Feng, Ingo Steinwart, and Johan A.K. Suykens", "title": "Learning theory estimates with observations from general stationary\n  stochastic processes", "comments": "arXiv admin note: text overlap with arXiv:1501.03059", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the supervised learning problem with observations\ndrawn from certain general stationary stochastic processes. Here by\n\\emph{general}, we mean that many stationary stochastic processes can be\nincluded. We show that when the stochastic processes satisfy a generalized\nBernstein-type inequality, a unified treatment on analyzing the learning\nschemes with various mixing processes can be conducted and a sharp oracle\ninequality for generic regularized empirical risk minimization schemes can be\nestablished. The obtained oracle inequality is then applied to derive\nconvergence rates for several learning schemes such as empirical risk\nminimization (ERM), least squares support vector machines (LS-SVMs) using given\ngeneric kernels, and SVMs using Gaussian kernels for both least squares and\nquantile regression. It turns out that for i.i.d.~processes, our learning rates\nfor ERM recover the optimal rates. On the other hand, for non-i.i.d.~processes\nincluding geometrically $\\alpha$-mixing Markov processes, geometrically\n$\\alpha$-mixing processes with restricted decay, $\\phi$-mixing processes, and\n(time-reversed) geometrically $\\mathcal{C}$-mixing processes, our learning\nrates for SVMs with Gaussian kernels match, up to some arbitrarily small extra\nterm in the exponent, the optimal rates. For the remaining cases, our rates are\nat least close to the optimal rates. As a by-product, the assumed generalized\nBernstein-type inequality also provides an interpretation of the so-called\n\"effective number of observations\" for various mixing processes.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 08:18:24 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Hang", "Hanyuan", ""], ["Feng", "Yunlong", ""], ["Steinwart", "Ingo", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1605.02989", "submitter": "Marco Capo MSc", "authors": "Marco Cap\\'o, Aritz P\\'erez, Jos\\'e Antonio Lozano", "title": "An efficient K-means algorithm for Massive Data", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the progressive growth of the amount of data available in a wide\nvariety of scientific fields, it has become more difficult to ma- nipulate and\nanalyze such information. Even though datasets have grown in size, the K-means\nalgorithm remains as one of the most popular clustering methods, in spite of\nits dependency on the initial settings and high computational cost, especially\nin terms of distance computations. In this work, we propose an efficient\napproximation to the K-means problem intended for massive data. Our approach\nrecursively partitions the entire dataset into a small number of sub- sets,\neach of which is characterized by its representative (center of mass) and\nweight (cardinality), afterwards a weighted version of the K-means algorithm is\napplied over such local representation, which can drastically reduce the number\nof distances computed. In addition to some theoretical properties, experimental\nresults indicate that our method outperforms well-known approaches, such as the\nK-means++ and the minibatch K-means, in terms of the relation between number of\ndistance computations and the quality of the approximation.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 13:01:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Cap\u00f3", "Marco", ""], ["P\u00e9rez", "Aritz", ""], ["Lozano", "Jos\u00e9 Antonio", ""]]}, {"id": "1605.03027", "submitter": "Brendan Guillouet", "authors": "Philippe C. Besse, Brendan Guillouet, Jean-Michel Loubes, and Francois\n  Royer", "title": "Destination Prediction by Trajectory Distribution Based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method to predict the final destination of\nvehicle trips based on their initial partial trajectories. We first review how\nwe obtained clustering of trajectories that describes user behaviour. Then, we\nexplain how we model main traffic flow patterns by a mixture of 2d Gaussian\ndistributions. This yielded a density based clustering of locations, which\nproduces a data driven grid of similar points within each pattern. We present\nhow this model can be used to predict the final destination of a new trajectory\nbased on their first locations using a two step procedure: We first assign the\nnew trajectory to the clusters it mot likely belongs. Secondly, we use\ncharacteristics from trajectories inside these clusters to predict the final\ndestination. Finally, we present experimental results of our methods for\nclassification of trajectories and final destination prediction on datasets of\ntimestamped GPS-Location of taxi trips. We test our methods on two different\ndatasets, to assess the capacity of our method to adapt automatically to\ndifferent subsets.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 14:22:45 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Besse", "Philippe C.", ""], ["Guillouet", "Brendan", ""], ["Loubes", "Jean-Michel", ""], ["Royer", "Francois", ""]]}, {"id": "1605.03040", "submitter": "Tianxi Li", "authors": "Tianxi Li", "title": "A note on the statistical view of matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple interpretation of matrix completion problem is introduced based\non statistical models. Combined with the well-known results from missing data\nanalysis, such interpretation indicates that matrix completion is still a valid\nand principled estimation procedure even without the missing completely at\nrandom (MCAR) assumption, which almost all of the current theoretical studies\nof matrix completion assume.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 14:55:46 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Li", "Tianxi", ""]]}, {"id": "1605.03122", "submitter": "Yanning Shen", "authors": "Yanning Shen, Brian Baingana, and Georgios B. Giannakis", "title": "Kernel-Based Structural Equation Models for Topology Identification of\n  Directed Networks", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2017.2664039", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models (SEMs) have been widely adopted for inference of\ncausal interactions in complex networks. Recent examples include unveiling\ntopologies of hidden causal networks over which processes such as spreading\ndiseases, or rumors propagate. The appeal of SEMs in these settings stems from\ntheir simplicity and tractability, since they typically assume linear\ndependencies among observable variables. Acknowledging the limitations inherent\nto adopting linear models, the present paper advocates nonlinear SEMs, which\naccount for (possible) nonlinear dependencies among network nodes. The\nadvocated approach leverages kernels as a powerful encompassing framework for\nnonlinear modeling, and an efficient estimator with affordable tradeoffs is put\nforth. Interestingly, pursuit of the novel kernel-based approach yields a\nconvex regularized estimator that promotes edge sparsity, and is amenable to\nproximal-splitting optimization methods. To this end, solvers with\ncomplementary merits are developed by leveraging the alternating direction\nmethod of multipliers, and proximal gradient iterations. Experiments conducted\non simulated data demonstrate that the novel approach outperforms linear SEMs\nwith respect to edge detection errors. Furthermore, tests on a real gene\nexpression dataset unveil interesting new edges that were not revealed by\nlinear SEMs, which could shed more light on regulatory behavior of human genes.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 17:40:20 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Shen", "Yanning", ""], ["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1605.03267", "submitter": "Sam Davanloo", "authors": "Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique del Castillo", "title": "Generalized Sparse Precision Matrix Selection for Fitting Multivariate\n  Gaussian Random Fields to Large Data Sets", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202017.0091", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for estimating multivariate, second-order stationary\nGaussian Random Field (GRF) models based on the Sparse Precision matrix\nSelection (SPS) algorithm, proposed by Davanloo et al. (2015) for estimating\nscalar GRF models. Theoretical convergence rates for the estimated\nbetween-response covariance matrix and for the estimated parameters of the\nunderlying spatial correlation function are established. Numerical tests using\nsimulated and real datasets validate our theoretical findings. Data\nsegmentation is used to handle large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 03:10:20 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 21:44:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tajbakhsh", "Sam Davanloo", ""], ["Aybat", "Necdet Serhat", ""], ["del Castillo", "Enrique", ""]]}, {"id": "1605.03306", "submitter": "Jinchi Lv", "authors": "Zemin Zheng and Yingying Fan and Jinchi Lv", "title": "High dimensional thresholded regression and shrinkage effect", "comments": "23 pages, 3 figures, 5 tables", "journal-ref": "Journal of the Royal Statistical Society Series B 76, 627-649", "doi": "10.1111/rssb.12037", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse modeling via regularization provides a powerful tool\nfor analyzing large-scale data sets and obtaining meaningful, interpretable\nmodels. The use of nonconvex penalty functions shows advantage in selecting\nimportant features in high dimensions, but the global optimality of such\nmethods still demands more understanding. In this paper, we consider sparse\nregression with hard-thresholding penalty, which we show to give rise to\nthresholded regression. This approach is motivated by its close connection with\nthe $L_0$-regularization, which can be unrealistic to implement in practice but\nof appealing sampling properties, and its computational advantage. Under some\nmild regularity conditions allowing possibly exponentially growing\ndimensionality, we establish the oracle inequalities of the resulting\nregularized estimator, as the global minimizer, under various prediction and\nvariable selection losses, as well as the oracle risk inequalities of the\nhard-thresholded estimator followed by a further $L_2$-regularization. The risk\nproperties exhibit interesting shrinkage effects under both estimation and\nprediction losses. We identify the optimal choice of the ridge parameter, which\nis shown to have simultaneous advantages to both the $L_2$-loss and prediction\nloss. These new results and phenomena are evidenced by simulation and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:12:07 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Zheng", "Zemin", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03310", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Asymptotic equivalence of regularization methods in thresholded\n  parameter space", "comments": "39 pages, 3 figures", "journal-ref": "Journal of the American Statistical Association 108, 1044-1061", "doi": "10.1080/01621459.2013.803972", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data analysis has motivated a spectrum of regularization\nmethods for variable selection and sparse modeling, with two popular classes of\nconvex ones and concave ones. A long debate has been on whether one class\ndominates the other, an important question both in theory and to practitioners.\nIn this paper, we characterize the asymptotic equivalence of regularization\nmethods, with general penalty functions, in a thresholded parameter space under\nthe generalized linear model setting, where the dimensionality can grow up to\nexponentially with the sample size. To assess their performance, we establish\nthe oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the global\nminimizer for these methods under various prediction and variable selection\nlosses. These results reveal an interesting phase transition phenomenon. For\npolynomially growing dimensionality, the $L_1$-regularization method of Lasso\nand concave methods are asymptotically equivalent, having the same convergence\nrates in the oracle inequalities. For exponentially growing dimensionality,\nconcave methods are asymptotically equivalent but have faster convergence rates\nthan the Lasso. We also establish a stronger property of the oracle risk\ninequalities of the regularization methods, as well as the sampling properties\nof computable solutions. Our new theoretical results are illustrated and\njustified by simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:39:16 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03311", "submitter": "Jinchi Lv", "authors": "Yinfei Kong and Zemin Zheng and Jinchi Lv", "title": "The constrained Dantzig selector with enhanced consistency", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dantzig selector has received popularity for many applications such as\ncompressed sensing and sparse modeling, thanks to its computational efficiency\nas a linear programming problem and its nice sampling properties. Existing\nresults show that it can recover sparse signals mimicking the accuracy of the\nideal procedure, up to a logarithmic factor of the dimensionality. Such a\nfactor has been shown to hold for many regularization methods. An important\nquestion is whether this factor can be reduced to a logarithmic factor of the\nsample size in ultra-high dimensions under mild regularity conditions. To\nprovide an affirmative answer, in this paper we suggest the constrained Dantzig\nselector, which has more flexible constraints and parameter space. We prove\nthat the suggested method can achieve convergence rates within a logarithmic\nfactor of the sample size of the oracle rates and improved sparsity, under a\nfairly weak assumption on the signal strength. Such improvement is significant\nin ultra-high dimensions. This method can be implemented efficiently through\nsequential linear programming. Numerical studies confirm that the sample size\nneeded for a certain level of accuracy in these problems can be much reduced.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:41:50 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Kong", "Yinfei", ""], ["Zheng", "Zemin", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03313", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Innovated scalable efficient estimation in ultra-large Gaussian\n  graphical models", "comments": "to appear, The Annals of Statistics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale precision matrix estimation is of fundamental importance yet\nchallenging in many contemporary applications for recovering Gaussian graphical\nmodels. In this paper, we suggest a new approach of innovated scalable\nefficient estimation (ISEE) for estimating large precision matrix. Motivated by\nthe innovated transformation, we convert the original problem into that of\nlarge covariance matrix estimation. The suggested method combines the strengths\nof recent advances in high-dimensional sparse modeling and large covariance\nmatrix estimation. Compared to existing approaches, our method is scalable and\ncan deal with much larger precision matrices with simple tuning. Under mild\nregularity conditions, we establish that this procedure can recover the\nunderlying graphical structure with significant probability and provide\nefficient estimation of link strengths. Both computational and theoretical\nadvantages of the procedure are evidenced through simulation and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:50:21 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03315", "submitter": "Jinchi Lv", "authors": "Yinfei Kong and Daoji Li and Yingying Fan and Jinchi Lv", "title": "Interaction pursuit in high-dimensional multi-response regression via\n  distance correlation", "comments": "to appear in The Annals of Statistics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature interactions can contribute to a large proportion of variation in\nmany prediction models. In the era of big data, the coexistence of high\ndimensionality in both responses and covariates poses unprecedented challenges\nin identifying important interactions. In this paper, we suggest a two-stage\ninteraction identification method, called the interaction pursuit via distance\ncorrelation (IPDC), in the setting of high-dimensional multi-response\ninteraction models that exploits feature screening applied to transformed\nvariables with distance correlation followed by feature selection. Such a\nprocedure is computationally efficient, generally applicable beyond the\nheredity assumption, and effective even when the number of responses diverges\nwith the sample size. Under mild regularity conditions, we show that this\nmethod enjoys nice theoretical properties including the sure screening\nproperty, support union recovery, and oracle inequalities in prediction and\nestimation for both interactions and main effects. The advantages of our method\nare supported by several simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:03:14 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Kong", "Yinfei", ""], ["Li", "Daoji", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03321", "submitter": "Yingying Fan", "authors": "Yingying Fan and Cheng Yong Tang", "title": "Tuning parameter selection in high dimensional penalized likelihood", "comments": "38 pages", "journal-ref": "Journal of the Royal Statistical Society Series B 75, 531-552\n  (2013)", "doi": "10.1111/rssb.12001", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining how to appropriately select the tuning parameter is essential in\npenalized likelihood methods for high-dimensional data analysis. We examine\nthis problem in the setting of penalized likelihood methods for generalized\nlinear models, where the dimensionality of covariates p is allowed to increase\nexponentially with the sample size n. We propose to select the tuning parameter\nby optimizing the generalized information criterion (GIC) with an appropriate\nmodel complexity penalty. To ensure that we consistently identify the true\nmodel, a range for the model complexity penalty is identified in GIC. We find\nthat this model complexity penalty should diverge at the rate of some power of\n$\\log p$ depending on the tail probability behavior of the response variables.\nThis reveals that using the AIC or BIC to select the tuning parameter may not\nbe adequate for consistently identifying the true model. Based on our\ntheoretical study, we propose a uniform choice of the model complexity penalty\nand show that the proposed approach consistently identifies the true model\namong candidate models with asymptotic probability one. We justify the\nperformance of the proposed procedure by numerical simulations and a gene\nexpression data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:17:58 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "1605.03324", "submitter": "Ozan Sener", "authors": "Ozan Sener and Amir Roshan Zamir and Chenxia Wu and Silvio Savarese\n  and Ashutosh Saxena", "title": "Unsupervised Semantic Action Discovery from Video Collections", "comments": "First version of this paper arXiv:1506.08438 appeared in ICCV 2015.\n  This extended version has more details on the learning algorithm and\n  hierarchical clustering with full derivation, additional analysis on the\n  robustness to the subtitle noise, and a novel application on robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication takes many forms, including speech, text and\ninstructional videos. It typically has an underlying structure, with a starting\npoint, ending, and certain objective steps between them. In this paper, we\nconsider instructional videos where there are tens of millions of them on the\nInternet.\n  We propose a method for parsing a video into such semantic steps in an\nunsupervised way. Our method is capable of providing a semantic \"storyline\" of\nthe video composed of its objective steps. We accomplish this using both visual\nand language cues in a joint generative model. Our method can also provide a\ntextual description for each of the identified semantic steps and video\nsegments. We evaluate our method on a large number of complex YouTube videos\nand show that our method discovers semantically correct instructions for a\nvariety of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:22:06 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Sener", "Ozan", ""], ["Zamir", "Amir Roshan", ""], ["Wu", "Chenxia", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1605.03335", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Asymptotic properties for combined $L_1$ and concave regularization", "comments": "16 pages", "journal-ref": "Biometrika 101, 57-70", "doi": "10.1093/biomet/ast047", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two important goals of high-dimensional modeling are prediction and variable\nselection. In this article, we consider regularization with combined $L_1$ and\nconcave penalties, and study the sampling properties of the global optimum of\nthe suggested method in ultra-high dimensional settings. The $L_1$-penalty\nprovides the minimum regularization needed for removing noise variables in\norder to achieve oracle prediction risk, while concave penalty imposes\nadditional regularization to control model sparsity. In the linear model\nsetting, we prove that the global optimum of our method enjoys the same oracle\ninequalities as the lasso estimator and admits an explicit bound on the false\nsign rate, which can be asymptotically vanishing. Moreover, we establish oracle\nrisk inequalities for the method and the sampling properties of computable\nsolutions. Numerical studies suggest that our method yields more stable\nestimates than using a concave penalty alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:51:05 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03364", "submitter": "Hans Kersting", "authors": "Hans Kersting, Philipp Hennig", "title": "Active Uncertainty Calibration in Bayesian ODE Solvers", "comments": "10 pages, 3 figures, published at UAI 2016. Changes for Version 3:\n  fixed minor index mistake in equation (14) (q-1-i instead of q+1-i on top of\n  the product)", "journal-ref": "Proceedings of the Thirty-Second Conference on Uncertainty in\n  Artificial Intelligence (UAI2016) 309--3018", "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is resurging interest, in statistics and machine learning, in solvers\nfor ordinary differential equations (ODEs) that return probability measures\ninstead of point estimates. Recently, Conrad et al. introduced a sampling-based\nclass of methods that are 'well-calibrated' in a specific sense. But the\ncomputational cost of these methods is significantly above that of classic\nmethods. On the other hand, Schober et al. pointed out a precise connection\nbetween classic Runge-Kutta ODE solvers and Gaussian filters, which gives only\na rough probabilistic calibration, but at negligible cost overhead. By\nformulating the solution of ODEs as approximate inference in linear Gaussian\nSDEs, we investigate a range of probabilistic ODE solvers, that bridge the\ntrade-off between computational cost and probabilistic calibration, and\nidentify the inaccurate gradient measurement as the crucial source of\nuncertainty. We propose the novel filtering-based method Bayesian Quadrature\nfiltering (BQF) which uses Bayesian quadrature to actively learn the\nimprecision in the gradient measurement by collecting multiple gradient\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 10:24:04 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 13:56:57 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 09:39:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kersting", "Hans", ""], ["Hennig", "Philipp", ""]]}, {"id": "1605.03391", "submitter": "Marvin N Wright", "authors": "Marvin N. Wright, Theresa Dankowski and Andreas Ziegler", "title": "Unbiased split variable selection for random survival forests using\n  maximally selected rank statistics", "comments": null, "journal-ref": "Wright, M. N., Dankowski, T. & Ziegler, A. (2017). Unbiased split\n  variable selection for random survival forests using maximally selected rank\n  statistics. Statistics in Medicine 36:1272-1284", "doi": "10.1002/sim.7212", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular approach for analyzing survival data is the Cox regression\nmodel. The Cox model may, however, be misspecified, and its proportionality\nassumption may not always be fulfilled. An alternative approach for survival\nprediction is random forests for survival outcomes. The standard split\ncriterion for random survival forests is the log-rank test statistics, which\nfavors splitting variables with many possible split points. Conditional\ninference forests avoid this split variable selection bias. However, linear\nrank statistics are utilized by default in conditional inference forests to\nselect the optimal splitting variable, which cannot detect non-linear effects\nin the independent variables. An alternative is to use maximally selected rank\nstatistics for the split point selection. As in conditional inference forests,\nsplitting variables are compared on the p-value scale. However, instead of the\nconditional Monte-Carlo approach used in conditional inference forests, p-value\napproximations are employed. We describe several p-value approximations and the\nimplementation of the proposed random forest approach. A simulation study\ndemonstrates that unbiased split variable selection is possible. However, there\nis a trade-off between unbiased split variable selection and runtime. In\nbenchmark studies of prediction performance on simulated and real datasets the\nnew method performs better than random survival forests if informative\ndichotomous variables are combined with uninformative variables with more\ncategories and better than conditional inference forests if non-linear\ncovariate effects are included. In a runtime comparison the method proves to be\ncomputationally faster than both alternatives, if a simple p-value\napproximation is used.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 11:48:05 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 12:24:46 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Wright", "Marvin N.", ""], ["Dankowski", "Theresa", ""], ["Ziegler", "Andreas", ""]]}, {"id": "1605.03468", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang, Ritambhara Singh and Yanjun Qi", "title": "A constrained L1 minimization approach for estimating multiple Sparse\n  Gaussian or Nonparanormal Graphical Models", "comments": "Extended Journal Version / Previously @ ICML 2016 comp. bio workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying context-specific entity networks from aggregated data is an\nimportant task, arising often in bioinformatics and neuroimaging.\nComputationally, this task can be formulated as jointly estimating multiple\ndifferent, but related, sparse Undirected Graphical Models (UGM) from\naggregated samples across several contexts. Previous joint-UGM studies have\nmostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify\ncontext-specific edge patterns directly. We, therefore, propose a novel\napproach, SIMULE (detecting Shared and Individual parts of MULtiple graphs\nExplicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE\nautomatically infers both specific edge patterns that are unique to each\ncontext and shared interactions preserved among all the contexts. Through the\nL1 constrained formulation, this problem is cast as multiple independent\nsubtasks of linear programming that can be solved efficiently in parallel. In\naddition to Gaussian data, SIMULE can also handle multivariate Nonparanormal\ndata that greatly relaxes the normality assumption that many real-world\napplications do not follow. We provide a novel theoretical proof showing that\nSIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple\nsynthetic datasets and two biomedical datasets, SIMULE shows significant\nimprovement over state-of-the-art multi-sGGM and single-UGM baselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 14:54:44 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 11:05:24 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 21:40:33 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 18:11:38 GMT"}, {"version": "v5", "created": "Wed, 8 Mar 2017 20:34:04 GMT"}, {"version": "v6", "created": "Mon, 18 Sep 2017 11:29:14 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wang", "Beilun", ""], ["Singh", "Ritambhara", ""], ["Qi", "Yanjun", ""]]}, {"id": "1605.03631", "submitter": "Bo Tang", "authors": "Bo Tang, Steven Kay, Haibo He, and Paul M. Baggenstoss", "title": "EEF: Exponentially Embedded Families with Class-Specific Features for\n  Classification", "comments": "9 pages, 3 figures, to be published in IEEE Signal Processing Letter.\n  IEEE Signal Processing Letter, 2016", "journal-ref": null, "doi": "10.1109/LSP.2016.2574327", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we present a novel exponentially embedded families (EEF)\nbased classification method, in which the probability density function (PDF) on\nraw data is estimated from the PDF on features. With the PDF construction, we\nshow that class-specific features can be used in the proposed classification\nmethod, instead of a common feature subset for all classes as used in\nconventional approaches. We apply the proposed EEF classifier for text\ncategorization as a case study and derive an optimal Bayesian classification\nrule with class-specific feature selection based on the Information Gain (IG)\nscore. The promising performance on real-life data sets demonstrates the\neffectiveness of the proposed approach and indicates its wide potential\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 22:25:07 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 19:01:31 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Tang", "Bo", ""], ["Kay", "Steven", ""], ["He", "Haibo", ""], ["Baggenstoss", "Paul M.", ""]]}, {"id": "1605.03661", "submitter": "Fredrik D. Johansson", "authors": "Fredrik D. Johansson, Uri Shalit and David Sontag", "title": "Learning Representations for Counterfactual Inference", "comments": "Appeared in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational studies are rising in importance due to the widespread\naccumulation of data in fields such as healthcare, education, employment and\necology. We consider the task of answering counterfactual questions such as,\n\"Would this patient have lower blood sugar had she received a different\nmedication?\". We propose a new algorithmic framework for counterfactual\ninference which brings together ideas from domain adaptation and representation\nlearning. In addition to a theoretical justification, we perform an empirical\ncomparison with previous approaches to causal inference from observational\ndata. Our deep learning algorithm significantly outperforms the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 02:59:40 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 17:04:07 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 13:00:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Johansson", "Fredrik D.", ""], ["Shalit", "Uri", ""], ["Sontag", "David", ""]]}, {"id": "1605.03662", "submitter": "Zhuang Ma", "authors": "Zhuang Ma, Xiaodong Li", "title": "Subspace Perspective on Canonical Correlation Analysis: Dimension\n  Reduction and Minimax Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a fundamental statistical tool for\nexploring the correlation structure between two sets of random variables. In\nthis paper, motivated by recent success of applying CCA to learn low\ndimensional representations of high dimensional objects, we propose to quantify\nthe estimation loss of CCA by the excess prediction loss defined through a\nprediction-after-dimension-reduction framework. Such framework suggests viewing\nCCA estimation as estimating the subspaces spanned by the canonical variates.\nInterestedly, the proposed error metrics derived from the excess prediction\nloss turn out to be closely related to the principal angles between the\nsubspaces spanned by the population and sample canonical variates respectively.\n  We characterize the non-asymptotic minimax rates under the proposed metrics,\nespecially the dependency of the minimax rates on the key quantities including\nthe dimensions, the condition number of the covariance matrices, the canonical\ncorrelations and the eigen-gap, with minimal assumptions on the joint\ncovariance matrix. To the best of our knowledge, this is the first finite\nsample result that captures the effect of the canonical correlations on the\nminimax rates.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 03:09:28 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 03:53:44 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ma", "Zhuang", ""], ["Li", "Xiaodong", ""]]}, {"id": "1605.03795", "submitter": "Alexander Novikov", "authors": "Alexander Novikov, Mikhail Trofimov, Ivan Oseledets", "title": "Exponential Machines", "comments": "ICLR-2017 workshop track paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling interactions between features improves the performance of machine\nlearning solutions in many domains (e.g. recommender systems or sentiment\nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor\nthat models all interactions of every order. The key idea is to represent an\nexponentially large tensor of parameters in a factorized format called Tensor\nTrain (TT). The Tensor Train format regularizes the model and lets you control\nthe number of underlying parameters. To train the model, we develop a\nstochastic Riemannian optimization procedure, which allows us to fit tensors\nwith 2^160 entries. We show that the model achieves state-of-the-art\nperformance on synthetic data with high-order interactions and that it works on\npar with high-order factorization machines on a recommender system dataset\nMovieLens 100K.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 13:08:11 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 10:24:08 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 08:17:58 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Novikov", "Alexander", ""], ["Trofimov", "Mikhail", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1605.03805", "submitter": "Richard Neuberg", "authors": "Richard Neuberg and Yixin Shi", "title": "Detecting Relative Anomaly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System states that are anomalous from the perspective of a domain expert\noccur frequently in some anomaly detection problems. The performance of\ncommonly used unsupervised anomaly detection methods may suffer in that\nsetting, because they use frequency as a proxy for anomaly. We propose a novel\nconcept for anomaly detection, called relative anomaly detection. It is\ntailored to be robust towards anomalies that occur frequently, by taking into\naccount their location relative to the most typical observations. The\napproaches we develop are computationally feasible even for large data sets,\nand they allow real-time detection. We illustrate using data sets of potential\nscraping attempts and Wi-Fi channel utilization, both from Google, Inc.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 13:29:45 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 15:29:52 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Neuberg", "Richard", ""], ["Shi", "Yixin", ""]]}, {"id": "1605.03835", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in conditional recurrent language modelling have mainly\nfocused on network architectures (e.g., attention mechanism), learning\nalgorithms (e.g., scheduled sampling and sequence-level training) and novel\napplications (e.g., image/video description generation, speech recognition,\netc.) On the other hand, we notice that decoding algorithms/strategies have not\nbeen investigated as much, and it has become standard to use greedy or beam\nsearch. In this paper, we propose a novel decoding strategy motivated by an\nearlier observation that nonlinear hidden layers of a deep neural network\nstretch the data manifold. The proposed strategy is embarrassingly\nparallelizable without any communication overhead, while improving an existing\ndecoding algorithm. We extensively evaluate it with attention-based neural\nmachine translation on the task of En->Cz translation.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:39:50 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1605.03843", "submitter": "Dmitry Rokhlin B.", "authors": "Dmitry B. Rokhlin", "title": "Asymptotic sequential Rademacher complexity of a finite function class", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite function class we describe the large sample limit of the\nsequential Rademacher complexity in terms of the viscosity solution of a\n$G$-heat equation. In the language of Peng's sublinear expectation theory, the\nsame quantity equals to the expected value of the largest order statistics of a\nmultidimensional $G$-normal random variable. We illustrate this result by\nderiving upper and lower bounds for the asymptotic sequential Rademacher\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 19:49:57 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Rokhlin", "Dmitry B.", ""]]}, {"id": "1605.03848", "submitter": "Gilles Louppe", "authors": "Antonio Sutera, Gilles Louppe, V\\^an Anh Huynh-Thu, Louis Wehenkel,\n  Pierre Geurts", "title": "Context-dependent feature analysis with random forests", "comments": "Accepted for presentation at UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, feature selection is often more complicated than identifying a\nsingle subset of input variables that would together explain the output. There\nmay be interactions that depend on contextual information, i.e., variables that\nreveal to be relevant only in some specific circumstances. In this setting, the\ncontribution of this paper is to extend the random forest variable importances\nframework in order (i) to identify variables whose relevance is\ncontext-dependent and (ii) to characterize as precisely as possible the effect\nof contextual information on these variables. The usage and the relevance of\nour framework for highlighting context-dependent variables is illustrated on\nboth artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:59:42 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Sutera", "Antonio", ""], ["Louppe", "Gilles", ""], ["Huynh-Thu", "V\u00e2n Anh", ""], ["Wehenkel", "Louis", ""], ["Geurts", "Pierre", ""]]}, {"id": "1605.03884", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "An Empirical-Bayes Score for Discrete Bayesian Networks", "comments": "12 pages, PGM 2016", "journal-ref": "Journal of Machine Learning Research (52, Proceedings Track, PGM\n  2016), 438-448", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is often performed in a Bayesian setting,\nby evaluating candidate structures using their posterior probabilities for a\ngiven data set. Score-based algorithms then use those posterior probabilities\nas an objective function and return the maximum a posteriori network as the\nlearned model. For discrete Bayesian networks, the canonical choice for a\nposterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal\nlikelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its\nfavourable theoretical properties descend from assuming a uniform prior both on\nthe space of the network structures and on the space of the parameters of the\nnetwork. In this paper, we revisit the limitations of these assumptions; and we\nintroduce an alternative set of assumptions and the resulting score: the\nBayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a\nmarginal uniform (MU) graph prior. We evaluate its performance in an extensive\nsimulation study, showing that MU+BDs is more accurate than U+BDeu both in\nlearning the structure of the network and in predicting new observations, while\nnot being computationally more complex to estimate.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:44:05 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 11:24:40 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 16:21:54 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1605.03933", "submitter": "Jieming Mao", "authors": "Xi Chen, Sivakanth Gopi, Jieming Mao, Jon Schneider", "title": "Competitive analysis of the top-K ranking problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in recommender systems, web search, social choice\nand crowdsourcing, we consider the problem of identifying the set of top $K$\nitems from noisy pairwise comparisons. In our setting, we are non-actively\ngiven $r$ pairwise comparisons between each pair of $n$ items, where each\ncomparison has noise constrained by a very general noise model called the\nstrong stochastic transitivity (SST) model. We analyze the competitive ratio of\nalgorithms for the top-$K$ problem. In particular, we present a linear time\nalgorithm for the top-$K$ problem which has a competitive ratio of\n$\\tilde{O}(\\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithm\nneeds at most $\\tilde{O}(\\sqrt{n})$ times as many samples needed as the best\npossible algorithm for that instance (in contrast, all previous known\nalgorithms for the top-$K$ problem have competitive ratios of\n$\\tilde{\\Omega}(n)$ or worse). We further show that this is tight: any\nalgorithm for the top-$K$ problem has competitive ratio at least\n$\\tilde{\\Omega}(\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:07:31 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Chen", "Xi", ""], ["Gopi", "Sivakanth", ""], ["Mao", "Jieming", ""], ["Schneider", "Jon", ""]]}, {"id": "1605.04034", "submitter": "Joey Tianyi Zhou Dr", "authors": "Joey Tianyi Zhou, Xinxing Xu, Sinno Jialin Pan, Ivor W. Tsang, Zheng\n  Qin and Rick Siow Mong Goh", "title": "Transfer Hashing with Privileged Information", "comments": "Accepted by IJCAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing learning to hash methods assume that there are sufficient data,\neither labeled or unlabeled, on the domain of interest (i.e., the target\ndomain) for training. However, this assumption cannot be satisfied in some\nreal-world applications. To address this data sparsity issue in hashing,\ninspired by transfer learning, we propose a new framework named Transfer\nHashing with Privileged Information (THPI). Specifically, we extend the\nstandard learning to hash method, Iterative Quantization (ITQ), in a transfer\nlearning manner, namely ITQ+. In ITQ+, a new slack function is learned from\nauxiliary data to approximate the quantization error in ITQ. We developed an\nalternating optimization approach to solve the resultant optimization problem\nfor ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure\namong the auxiliary data for learning more precise binary codes in the target\ndomain. Extensive experiments on several benchmark datasets verify the\neffectiveness of our proposed approaches through comparisons with several\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 02:49:43 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Zhou", "Joey Tianyi", ""], ["Xu", "Xinxing", ""], ["Pan", "Sinno Jialin", ""], ["Tsang", "Ivor W.", ""], ["Qin", "Zheng", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "1605.04039", "submitter": "Guangrun Wang", "authors": "Liang Lin, Guangrun Wang, Wangmeng Zuo, Xiangchu Feng, and Lei Zhang", "title": "Cross-Domain Visual Matching via Generalized Similarity Measure and\n  Feature Learning", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI), 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2567386", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain visual data matching is one of the fundamental problems in many\nreal-world vision tasks, e.g., matching persons across ID photos and\nsurveillance videos. Conventional approaches to this problem usually involves\ntwo steps: i) projecting samples from different domains into a common space,\nand ii) computing (dis-)similarity in this space based on a certain distance.\nIn this paper, we present a novel pairwise similarity measure that advances\nexisting models by i) expanding traditional linear projections into affine\ntransformations and ii) fusing affine Mahalanobis distance and Cosine\nsimilarity by a data-driven combination. Moreover, we unify our similarity\nmeasure with feature representation learning via deep convolutional neural\nnetworks. Specifically, we incorporate the similarity measure matrix into the\ndeep architecture, enabling an end-to-end way of model optimization. We\nextensively evaluate our generalized similarity model in several challenging\ncross-domain matching tasks: person re-identification under different views and\nface verification over different modalities (i.e., faces from still images and\nvideos, older and younger faces, and sketch and photo portraits). The\nexperimental results demonstrate superior performance of our model over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 03:35:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Guangrun", ""], ["Zuo", "Wangmeng", ""], ["Feng", "Xiangchu", ""], ["Zhang", "Lei", ""]]}, {"id": "1605.04074", "submitter": "Muhammad Yousefnezhad", "authors": "Hosein Alizadeh, Muhammad Yousefnezhad and Behrouz Minaei Bidgoli", "title": "Wisdom of Crowds cluster ensemble", "comments": "Intelligent Data Analysis (IDA), IOS Press", "journal-ref": null, "doi": "10.3233/IDA-150728", "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wisdom of Crowds is a phenomenon described in social science that\nsuggests four criteria applicable to groups of people. It is claimed that, if\nthese criteria are satisfied, then the aggregate decisions made by a group will\noften be better than those of its individual members. Inspired by this concept,\nwe present a novel feedback framework for the cluster ensemble problem, which\nwe call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional\ncluster ensemble methods focusing on diversity have recently been proposed,\nWOCCE analyzes the conditions necessary for a crowd to exhibit this collective\nwisdom. These include decentralization criteria for generating primary results,\nindependence criteria for the base algorithms, and diversity criteria for the\nensemble members. We suggest appropriate procedures for evaluating these\nmeasures, and propose a new measure to assess the diversity. We evaluate the\nperformance of WOCCE against some other traditional base algorithms as well as\nstate-of-the-art ensemble methods. The results demonstrate the efficiency of\nWOCCE's aggregate decision-making compared to other algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 07:50:50 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Alizadeh", "Hosein", ""], ["Yousefnezhad", "Muhammad", ""], ["Bidgoli", "Behrouz Minaei", ""]]}, {"id": "1605.04131", "submitter": "Conghui Tan", "authors": "Conghui Tan, Shiqian Ma, Yu-Hong Dai, Yuqiu Qian", "title": "Barzilai-Borwein Step Size for Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major issues in stochastic gradient descent (SGD) methods is how\nto choose an appropriate step size while running the algorithm. Since the\ntraditional line search technique does not apply for stochastic optimization\nalgorithms, the common practice in SGD is either to use a diminishing step\nsize, or to tune a fixed step size by hand, which can be time consuming in\npractice. In this paper, we propose to use the Barzilai-Borwein (BB) method to\nautomatically compute step sizes for SGD and its variant: stochastic variance\nreduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and\nSVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective\nfunctions. As a by-product, we prove the linear convergence result of SVRG with\nOption I proposed in [10], whose convergence result is missing in the\nliterature. Numerical experiments on standard data sets show that the\nperformance of SGD-BB and SVRG-BB is comparable to and sometimes even better\nthan SGD and SVRG with best-tuned step sizes, and is superior to some advanced\nSGD variants.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 11:08:50 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 02:51:08 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Tan", "Conghui", ""], ["Ma", "Shiqian", ""], ["Dai", "Yu-Hong", ""], ["Qian", "Yuqiu", ""]]}, {"id": "1605.04135", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Shuai Li and Harikrishna Narasimhan and Sanjay\n  Chawla and Fabrizio Sebastiani", "title": "Online Optimization Methods for the Quantification Problem", "comments": "26 pages, 6 figures. A short version of this manuscript will appear\n  in the proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining, KDD 2016", "journal-ref": null, "doi": "10.1145/2939672.2939832", "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of class prevalence, i.e., the fraction of a population that\nbelongs to a certain class, is a very useful tool in data analytics and\nlearning, and finds applications in many domains such as sentiment analysis,\nepidemiology, etc. For example, in sentiment analysis, the objective is often\nnot to estimate whether a specific text conveys a positive or a negative\nsentiment, but rather estimate the overall distribution of positive and\nnegative sentiments during an event window. A popular way of performing the\nabove task, often dubbed quantification, is to use supervised learning to train\na prevalence estimator from labeled data.\n  Contemporary literature cites several performance measures used to measure\nthe success of such prevalence estimators. In this paper we propose the first\nonline stochastic algorithms for directly optimizing these\nquantification-specific performance measures. We also provide algorithms that\noptimize hybrid performance measures that seek to balance quantification and\nclassification performance. Our algorithms present a significant advancement in\nthe theory of multivariate optimization and we show, by a rigorous theoretical\nanalysis, that they exhibit optimal convergence. We also report extensive\nexperiments on benchmark and real data sets which demonstrate that our methods\nsignificantly outperform existing optimization techniques used for these\nperformance measures.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 11:14:58 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 04:29:47 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 18:11:54 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kar", "Purushottam", ""], ["Li", "Shuai", ""], ["Narasimhan", "Harikrishna", ""], ["Chawla", "Sanjay", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1605.04262", "submitter": "Derek Feng", "authors": "Derek Feng, Xiaofei Wang", "title": "ABtree: An Algorithm for Subgroup-Based Treatment Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two possible treatments, there may exist subgroups who benefit greater\nfrom one treatment than the other. This problem is relevant to the field of\nmarketing, where treatments may correspond to different ways of selling a\nproduct. It is similarly relevant to the field of public policy, where\ntreatments may correspond to specific government programs. And finally,\npersonalized medicine is a field wholly devoted to understanding which\nsubgroups of individuals will benefit from particular medical treatments. We\npresent a computationally fast tree-based method, ABtree, for treatment effect\ndifferentiation. Unlike other methods, ABtree specifically produces decision\nrules for optimal treatment assignment on a per-individual basis. The treatment\nchoices are selected for maximizing the overall occurrence of a desired binary\noutcome, conditional on a set of covariates. In this poster, we present the\nmethodology on tree growth and pruning, and show performance results when\napplied to simulated data as well as real data.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:27:55 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Feng", "Derek", ""], ["Wang", "Xiaofei", ""]]}, {"id": "1605.04337", "submitter": "Harikrishna Narasimhan", "authors": "Harikrishna Narasimhan, Shivani Agarwal", "title": "Support Vector Algorithms for Optimizing the Partial Area Under the ROC\n  Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the ROC curve (AUC) is a widely used performance measure in\nmachine learning. Increasingly, however, in several applications, ranging from\nranking to biometric screening to medicine, performance is measured not in\nterms of the full area under the ROC curve, but in terms of the \\emph{partial}\narea under the ROC curve between two false positive rates. In this paper, we\ndevelop support vector algorithms for directly optimizing the partial AUC\nbetween any two false positive rates. Our methods are based on minimizing a\nsuitable proxy or surrogate objective for the partial AUC error. In the case of\nthe full AUC, one can readily construct and optimize convex surrogates by\nexpressing the performance measure as a summation of pairwise terms. The\npartial AUC, on the other hand, does not admit such a simple decomposable\nstructure, making it more challenging to design and optimize (tight) convex\nsurrogates for this measure.\n  Our approach builds on the structural SVM framework of Joachims (2005) to\ndesign convex surrogates for partial AUC, and solves the resulting optimization\nproblem using a cutting plane solver. Unlike the full AUC, where the\ncombinatorial optimization needed in each iteration of the cutting plane solver\ncan be decomposed and solved efficiently, the corresponding problem for the\npartial AUC is harder to decompose. One of our main contributions is a\npolynomial time algorithm for solving the combinatorial optimization problem\nassociated with partial AUC. We also develop an approach for optimizing a\ntighter non-convex hinge loss based surrogate for the partial AUC using\ndifference-of-convex programming. Our experiments on a variety of real-world\nand benchmark tasks confirm the efficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 21:33:45 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Narasimhan", "Harikrishna", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1605.04435", "submitter": "Irina Rish", "authors": "I. Rish, L. Wehbe, G. Langs, M. Grosse-Wentrup, B. Murphy, G. Cecchi", "title": "Proceedings of the 5th Workshop on Machine Learning and Interpretation\n  in Neuroimaging (MLINI) at NIPS 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume is a collection of contributions from the 5th Workshop on Machine\nLearning and Interpretation in Neuroimaging (MLINI) at the Neural Information\nProcessing Systems (NIPS 2015) conference. Modern multivariate statistical\nmethods developed in the rapidly growing field of machine learning are being\nincreasingly applied to various problems in neuroimaging, from cognitive state\ndetection to clinical diagnosis and prognosis. Multivariate pattern analysis\nmethods are designed to examine complex relationships between high-dimensional\nsignals, such as brain images, and outcomes of interest, such as the category\nof a stimulus, a type of a mental state of a subject, or a specific mental\ndisorder. Such techniques are in contrast with the traditional mass-univariate\napproaches that dominated neuroimaging in the past and treated each individual\nimaging measurement in isolation.\n  We believe that machine learning has a prominent role in shaping how\nquestions in neuroscience are framed, and that the machine-learning mind set is\nnow entering modern psychology and behavioral studies. It is also equally\nimportant that practical applications in these fields motivate a rapidly\nevolving line or research in the machine learning community. In parallel, there\nis an intense interest in learning more about brain function in the context of\nrich naturalistic environments and scenes. Efforts to go beyond highly specific\nparadigms that pinpoint a single function, towards schemes for measuring the\ninteraction with natural and more varied scene are made. The goal of the\nworkshop is to pinpoint the most pressing issues and common challenges across\nthe neuroscience, neuroimaging, psychology and machine learning fields, and to\nsketch future directions and open questions in the light of novel methodology.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 16:37:54 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Rish", "I.", ""], ["Wehbe", "L.", ""], ["Langs", "G.", ""], ["Grosse-Wentrup", "M.", ""], ["Murphy", "B.", ""], ["Cecchi", "G.", ""]]}, {"id": "1605.04465", "submitter": "Avradeep Bhowmik", "authors": "Avradeep Bhowmik, Joydeep Ghosh", "title": "Monotone Retargeting for Unsupervised Rank Aggregation with Object\n  Features", "comments": "15 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the true ordering between objects by aggregating a set of expert\nopinion rank order lists is an important and ubiquitous problem in many\napplications ranging from social choice theory to natural language processing\nand search aggregation. We study the problem of unsupervised rank aggregation\nwhere no ground truth ordering information in available, neither about the true\npreference ordering between any set of objects nor about the quality of\nindividual rank lists. Aggregating the often inconsistent and poor quality rank\nlists in such an unsupervised manner is a highly challenging problem, and\nstandard consensus-based methods are often ill-defined, and difficult to solve.\nIn this manuscript we propose a novel framework to bypass these issues by using\nobject attributes to augment the standard rank aggregation framework. We design\nalgorithms that learn joint models on both rank lists and object features to\nobtain an aggregated rank ordering that is more accurate and robust, and also\nhelps weed out rank lists of dubious validity. We validate our techniques on\nsynthetic datasets where our algorithm is able to estimate the true rank\nordering even when the rank lists are corrupted. Experiments on three real\ndatasets, MQ2008, MQ2008 and OHSUMED, show that using object features can\nresult in significant improvement in performance over existing rank aggregation\nmethods that do not use object information. Furthermore, when at least some of\nthe rank lists are of high quality, our methods are able to effectively exploit\ntheir high expertise to output an aggregated rank ordering of great accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 20:35:20 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Bhowmik", "Avradeep", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1605.04466", "submitter": "Avradeep Bhowmik", "authors": "Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo", "title": "Generalized Linear Models for Aggregated Data", "comments": "AISTATS 2015, 9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases in domains such as healthcare are routinely released to the public\nin aggregated form. Unfortunately, naive modeling with aggregated data may\nsignificantly diminish the accuracy of inferences at the individual level. This\npaper addresses the scenario where features are provided at the individual\nlevel, but the target variables are only available as histogram aggregates or\norder statistics. We consider a limiting case of generalized linear modeling\nwhen the target variables are only known up to permutation, and explore how\nthis relates to permutation testing; a standard technique for assessing\nstatistical dependency. Based on this relationship, we propose a simple\nalgorithm to estimate the model parameters and individual level inferences via\nalternating imputation and standard generalized linear model fitting. Our\nresults suggest the effectiveness of the proposed approach when, in the\noriginal data, permutation testing accurately ascertains the veracity of the\nlinear relationship. The framework is extended to general histogram data with\nlarger bins - with order statistics such as the median as a limiting case. Our\nexperimental results on simulated data and aggregated healthcare data suggest a\ndiminishing returns property with respect to the granularity of the histogram -\nwhen a linear relationship holds in the original data, the targets can be\npredicted accurately given relatively coarse histograms.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 21:09:10 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Bhowmik", "Avradeep", ""], ["Ghosh", "Joydeep", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1605.04638", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Rong Jin, Jinfeng Yi", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online\n  Learning with True and Noisy Gradient", "comments": "Accepted by the 33rd International Conference on Machine Learning\n  (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on dynamic regret of online convex optimization that\ncompares the performance of online learning to a clairvoyant who knows the\nsequence of loss functions in advance and hence selects the minimizer of the\nloss function at each step. By assuming that the clairvoyant moves slowly\n(i.e., the minimizers change slowly), we present several improved\nvariation-based upper bounds of the dynamic regret under the true and noisy\ngradient feedback, which are {\\it optimal} in light of the presented lower\nbounds. The key to our analysis is to explore a regularity metric that measures\nthe temporal changes in the clairvoyant's minimizers, to which we refer as {\\it\npath variation}. Firstly, we present a general lower bound in terms of the path\nvariation, and then show that under full information or gradient feedback we\nare able to achieve an optimal dynamic regret. Secondly, we present a lower\nbound with noisy gradient feedback and then show that we can achieve optimal\ndynamic regrets under a stochastic gradient feedback and two-point bandit\nfeedback. Moreover, for a sequence of smooth loss functions that admit a small\nvariation in the gradients, our dynamic regret under the two-point bandit\nfeedback matches what is achieved with full information.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 03:01:41 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""], ["Yi", "Jinfeng", ""]]}, {"id": "1605.04639", "submitter": "Akira Imakura", "authors": "Tetsuya Sakurai, Akira Imakura, Yuto Inoue and Yasunori Futamura", "title": "Alternating optimization method based on nonnegative matrix\n  factorizations for deep neural networks", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation algorithm for calculating gradients has been widely used\nin computation of weights for deep neural networks (DNNs). This method requires\nderivatives of objective functions and has some difficulties finding\nappropriate parameters such as learning rate. In this paper, we propose a novel\napproach for computing weight matrices of fully-connected DNNs by using two\ntypes of semi-nonnegative matrix factorizations (semi-NMFs). In this method,\noptimization processes are performed by calculating weight matrices\nalternately, and backpropagation (BP) is not used. We also present a method to\ncalculate stacked autoencoder using a NMF. The output results of the\nautoencoder are used as pre-training data for DNNs. The experimental results\nshow that our method using three types of NMFs attains similar error rates to\nthe conventional DNNs with BP.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 03:11:17 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Sakurai", "Tetsuya", ""], ["Imakura", "Akira", ""], ["Inoue", "Yuto", ""], ["Futamura", "Yasunori", ""]]}, {"id": "1605.04654", "submitter": "Matthew Hirn", "authors": "Matthew Hirn, St\\'ephane Mallat and Nicolas Poilvert", "title": "Wavelet Scattering Regression of Quantum Chemical Energies", "comments": "Replaces arXiv:1502.02077. v2: Minor clarifications, additions, and\n  typo corrections. v3: Minor edits. Software to reproduce the numerical\n  results is available at: https://github.com/matthew-hirn/ScatNet-QM-2D", "journal-ref": "Multiscale Modeling and Simulation, volume 15, issue 2, 827-863,\n  2017", "doi": "10.1137/16M1075454", "report-no": null, "categories": "math.CA physics.chem-ph quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multiscale invariant dictionaries to estimate quantum chemical\nenergies of organic molecules, from training databases. Molecular energies are\ninvariant to isometric atomic displacements, and are Lipschitz continuous to\nmolecular deformations. Similarly to density functional theory (DFT), the\nmolecule is represented by an electronic density function. A multiscale\ninvariant dictionary is calculated with wavelet scattering invariants. It\ncascades a first wavelet transform which separates scales, with a second\nwavelet transform which computes interactions across scales. Sparse scattering\nregressions give state of the art results over two databases of organic planar\nmolecules. On these databases, the regression error is of the order of the\nerror produced by DFT codes, but at a fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 05:39:31 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 20:34:26 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 20:36:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hirn", "Matthew", ""], ["Mallat", "St\u00e9phane", ""], ["Poilvert", "Nicolas", ""]]}, {"id": "1605.04657", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta", "title": "Solve-Select-Scale: A Three Step Process For Sparse Signal Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the theory of compressed sensing (CS), the sparsity $\\|x\\|_0$ of the\nunknown signal $\\mathbf{x} \\in \\mathcal{R}^n$ is of prime importance and the\nfocus of reconstruction algorithms has mainly been either $\\|x\\|_0$ or its\nconvex relaxation (via $\\|x\\|_1$). However, it is typically unknown in practice\nand has remained a challenge when nothing about the size of the support is\nknown. As pointed recently, $\\|x\\|_0$ might not be the best metric to minimize\ndirectly, both due to its inherent complexity as well as its noise performance.\nRecently a novel stable measure of sparsity $s(\\mathbf{x}) :=\n\\|\\mathbf{x}\\|_1^2/\\|\\mathbf{x}\\|_2^2$ has been investigated by Lopes\n\\cite{Lopes2012}, which is a sharp lower bound on $\\|\\mathbf{x}\\|_0$. The\nestimation procedure for this measure uses only a small number of linear\nmeasurements, does not rely on any sparsity assumptions, and requires very\nlittle computation. The usage of the quantity $s(\\mathbf{x})$ in sparse signal\nestimation problems has not received much importance yet. We develop the idea\nof incorporating $s(\\mathbf{x})$ into the signal estimation framework. We also\nprovide a three step algorithm to solve problems of the form $\\mathbf{Ax=b}$\nwith no additional assumptions on the original signal $\\mathbf{x}$.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 06:10:44 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Gupta", "Mithun Das", ""]]}, {"id": "1605.04672", "submitter": "Pushpendre Rastogi", "authors": "Pushpendre Rastogi, Benjamin Van Durme", "title": "A Critical Examination of RESCAL for Completion of Knowledge Bases with\n  Transitive Relations", "comments": "Four and a half page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction in large knowledge graphs has received a lot of attention\nrecently because of its importance for inferring missing relations and for\ncompleting and improving noisily extracted knowledge graphs. Over the years a\nnumber of machine learning researchers have presented various models for\npredicting the presence of missing relations in a knowledge base. Although all\nthe previous methods are presented with empirical results that show high\nperformance on select datasets, there is almost no previous work on\nunderstanding the connection between properties of a knowledge base and the\nperformance of a model. In this paper we analyze the RESCAL method and prove\nthat it can not encode asymmetric transitive relations in knowledge bases.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 07:43:28 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Rastogi", "Pushpendre", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1605.04764", "submitter": "Avradeep Bhowmik", "authors": "Avradeep Bhowmik, Nathan Liu, Erheng Zhong, Badri Narayan Bhaskar,\n  Suju Rajan", "title": "Geometry Aware Mappings for High Dimensional Sparse Factors", "comments": "AISTATS 2016, 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While matrix factorisation models are ubiquitous in large scale\nrecommendation and search, real time application of such models requires inner\nproduct computations over an intractably large set of item factors. In this\nmanuscript we present a novel framework that uses the inverted index\nrepresentation to exploit structural properties of sparse vectors to\nsignificantly reduce the run time computational cost of factorisation models.\nWe develop techniques that use geometry aware permutation maps on a tessellated\nunit sphere to obtain high dimensional sparse embeddings for latent factors\nwith sparsity patterns related to angular closeness of the original latent\nfactors. We also design several efficient and deterministic realisations within\nthis framework and demonstrate with experiments that our techniques lead to\nfaster run time operation with minimal loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:21:15 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Bhowmik", "Avradeep", ""], ["Liu", "Nathan", ""], ["Zhong", "Erheng", ""], ["Bhaskar", "Badri Narayan", ""], ["Rajan", "Suju", ""]]}, {"id": "1605.04812", "submitter": "Adith Swaminathan", "authors": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav\n  Dud\\'ik, John Langford, Damien Jose, Imed Zitouni", "title": "Off-policy evaluation for slate recommendation", "comments": "31 pages (9 main paper, 20 supplementary), 12 figures (2 main paper,\n  10 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the evaluation of policies that recommend an ordered set\nof items (e.g., a ranking) based on some context---a common scenario in web\nsearch, ads, and recommendation. We build on techniques from combinatorial\nbandits to introduce a new practical estimator that uses logged data to\nestimate a policy's performance. A thorough empirical evaluation on real-world\ndata reveals that our estimator is accurate in a variety of settings, including\nas a subroutine in a learning-to-rank task, where it achieves competitive\nperformance. We derive conditions under which our estimator is unbiased---these\nconditions are weaker than prior heuristics for slate evaluation---and\nexperimentally demonstrate a smaller bias than parametric approaches, even when\nthese conditions are violated. Finally, our theory and experiments also show\nexponential savings in the amount of required data compared with general\nunbiased estimators.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 15:47:21 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 10:34:21 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 22:55:48 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Swaminathan", "Adith", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Dud\u00edk", "Miroslav", ""], ["Langford", "John", ""], ["Jose", "Damien", ""], ["Zitouni", "Imed", ""]]}, {"id": "1605.04932", "submitter": "Frosti Palsson", "authors": "Magnus O. Ulfarsson, Frosti Palsson, Jakob Sigurdsson and Johannes R.\n  Sveinsson", "title": "Classification of Big Data with Application to Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data applications, such as medical imaging and genetics, typically\ngenerate datasets that consist of few observations n on many more variables p,\na scenario that we denote as p>>n. Traditional data processing methods are\noften insufficient for extracting information out of big data. This calls for\nthe development of new algorithms that can deal with the size, complexity, and\nthe special structure of such datasets. In this paper, we consider the problem\nof classifying p>>n data and propose a classification method based on linear\ndiscriminant analysis (LDA). Traditional LDA depends on the covariance estimate\nof the data, but when p>>n the sample covariance estimate is singular. The\nproposed method estimates the covariance by using a sparse version of noisy\nprincipal component analysis (nPCA). The use of sparsity in this setting aims\nat automatically selecting variables that are relevant for classification. In\nexperiments, the new method is compared to state-of-the art methods for big\ndata problems using both simulated datasets and imaging genetics datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 20:16:29 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Ulfarsson", "Magnus O.", ""], ["Palsson", "Frosti", ""], ["Sigurdsson", "Jakob", ""], ["Sveinsson", "Johannes R.", ""]]}, {"id": "1605.04955", "submitter": "Diego Hernan Diaz Martinez", "authors": "Diego Hern\\'an D\\'iaz Mart\\'inez, Christine H. Lee, Peter T. Kim,\n  Washington Mio", "title": "Probing the Geometry of Data with Diffusion Fr\\'echet Functions", "comments": "26 pages, 8 figures. Lemma 1b and Theorem 2 have been revised, as\n  well as the results derived from them", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex ecosystems, such as those formed by multiple microbial taxa,\ninvolve intricate interactions amongst various sub-communities. The most basic\nrelationships are frequently modeled as co-occurrence networks in which the\nnodes represent the various players in the community and the weighted edges\nencode levels of interaction. In this setting, the composition of a community\nmay be viewed as a probability distribution on the nodes of the network. This\npaper develops methods for modeling the organization of such data, as well as\ntheir Euclidean counterparts, across spatial scales. Using the notion of\ndiffusion distance, we introduce diffusion Frechet functions and diffusion\nFrechet vectors associated with probability distributions on Euclidean space\nand the vertex set of a weighted network, respectively. We prove that these\nfunctional statistics are stable with respect to the Wasserstein distance\nbetween probability measures, thus yielding robust descriptors of their shapes.\nWe apply the methodology to investigate bacterial communities in the human gut,\nseeking to characterize divergence from intestinal homeostasis in patients with\nClostridium difficile infection (CDI) and the effects of fecal microbiota\ntransplantation, a treatment used in CDI patients that has proven to be\nsignificantly more effective than traditional treatment with antibiotics. The\nproposed method proves useful in deriving a biomarker that might help elucidate\nthe mechanisms that drive these processes.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 21:11:08 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 02:36:28 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Mart\u00ednez", "Diego Hern\u00e1n D\u00edaz", ""], ["Lee", "Christine H.", ""], ["Kim", "Peter T.", ""], ["Mio", "Washington", ""]]}, {"id": "1605.05045", "submitter": "Raffaello Camoriano", "authors": "Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale,\n  Lorenzo Rosasco, Giorgio Metta", "title": "Incremental Robot Learning of New Objects with Fixed Update Time", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider object recognition in the context of lifelong learning, where a\nrobotic agent learns to discriminate between a growing number of object classes\nas it accumulates experience about the environment. We propose an incremental\nvariant of the Regularized Least Squares for Classification (RLSC) algorithm,\nand exploit its structure to seamlessly add new classes to the learned model.\nThe presented algorithm addresses the problem of having an unbalanced\nproportion of training examples per class, which occurs when new objects are\npresented to the system for the first time.\n  We evaluate our algorithm on both a machine learning benchmark dataset and\ntwo challenging object recognition tasks in a robotic setting. Empirical\nevidence shows that our approach achieves comparable or higher classification\nperformance than its batch counterpart when classes are unbalanced, while being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 07:50:58 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 20:50:38 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 16:53:19 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Camoriano", "Raffaello", ""], ["Pasquale", "Giulia", ""], ["Ciliberto", "Carlo", ""], ["Natale", "Lorenzo", ""], ["Rosasco", "Lorenzo", ""], ["Metta", "Giorgio", ""]]}, {"id": "1605.05239", "submitter": "Benjamin Migliori", "authors": "Benjamin Migliori, Riley Zeller-Townson, Daniel Grady, Daniel Gebhardt", "title": "Biologically Inspired Radio Signal Feature Extraction with Sparse\n  Denoising Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic modulation classification (AMC) is an important task for modern\ncommunication systems; however, it is a challenging problem when signal\nfeatures and precise models for generating each modulation may be unknown. We\npresent a new biologically-inspired AMC method without the need for models or\nmanually specified features --- thus removing the requirement for expert prior\nknowledge. We accomplish this task using regularized stacked sparse denoising\nautoencoders (SSDAs). Our method selects efficient classification features\ndirectly from raw in-phase/quadrature (I/Q) radio signals in an unsupervised\nmanner. These features are then used to construct higher-complexity abstract\nfeatures which can be used for automatic modulation classification. We\ndemonstrate this process using a dataset generated with a software defined\nradio, consisting of random input bits encoded in 100-sample segments of\nvarious common digital radio modulations. Our results show correct\nclassification rates of > 99% at 7.5 dB signal-to-noise ratio (SNR) and > 92%\nat 0 dB SNR in a 6-way classification test. Our experiments demonstrate a\ndramatically new and broadly applicable mechanism for performing AMC and\nrelated tasks without the need for expert-defined or modulation-specific signal\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 17:03:02 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Migliori", "Benjamin", ""], ["Zeller-Townson", "Riley", ""], ["Grady", "Daniel", ""], ["Gebhardt", "Daniel", ""]]}, {"id": "1605.05273", "submitter": "Mathias Niepert", "authors": "Mathias Niepert and Mohamed Ahmed and Konstantin Kutzkov", "title": "Learning Convolutional Neural Networks for Graphs", "comments": "To be presented at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous important problems can be framed as learning from graph data. We\npropose a framework for learning convolutional neural networks for arbitrary\ngraphs. These graphs may be undirected, directed, and with both discrete and\ncontinuous node and edge attributes. Analogous to image-based convolutional\nnetworks that operate on locally connected regions of the input, we present a\ngeneral approach to extracting locally connected regions from graphs. Using\nestablished benchmark data sets, we demonstrate that the learned feature\nrepresentations are competitive with state of the art graph kernels and that\ntheir computation is highly efficient.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 18:13:13 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 15:38:30 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 13:33:38 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 11:40:13 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Niepert", "Mathias", ""], ["Ahmed", "Mohamed", ""], ["Kutzkov", "Konstantin", ""]]}, {"id": "1605.05278", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski and Donald B. Percival", "title": "Exact Simulation of Noncircular or Improper Complex-Valued Stationary\n  Gaussian Processes using Circulant Embedding", "comments": "Link to published version:\n  http://ieeexplore.ieee.org/document/7738840/", "journal-ref": "2016 IEEE 26th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": "10.1109/MLSP.2016.7738840", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an algorithm for simulating improper (or noncircular)\ncomplex-valued stationary Gaussian processes. The technique utilizes recently\ndeveloped methods for multivariate Gaussian processes from the circulant\nembedding literature. The method can be performed in $\\mathcal{O}(n\\log_2 n)$\noperations, where $n$ is the length of the desired sequence. The method is\nexact, except when eigenvalues of prescribed circulant matrices are negative.\nWe evaluate the performance of the algorithm empirically, and provide a\npractical example where the method is guaranteed to be exact for all $n$, with\nan improper fractional Gaussian noise process.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 18:22:20 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 17:06:15 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Percival", "Donald B.", ""]]}, {"id": "1605.05284", "submitter": "Zahra Shakeri", "authors": "Zahra Shakeri, Waheed U. Bajwa, Anand D. Sarwate", "title": "Minimax Lower Bounds for Kronecker-Structured Dictionary Learning", "comments": "5 pages, 1 figure. To appear in 2016 IEEE International Symposium on\n  Information Theory", "journal-ref": "Proc. IEEE Intl. Symp. Information Theory, Barcelona, Spain, Jul.\n  10-15, 2016, pp. 1148-1152", "doi": "10.1109/ISIT.2016.7541479", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is the problem of estimating the collection of atomic\nelements that provide a sparse representation of measured/collected signals or\ndata. This paper finds fundamental limits on the sample complexity of\nestimating dictionaries for tensor data by proving a lower bound on the minimax\nrisk. This lower bound depends on the dimensions of the tensor and parameters\nof the generative model. The focus of this paper is on second-order tensor\ndata, with the underlying dictionaries constructed by taking the Kronecker\nproduct of two smaller dictionaries and the observed data generated by sparse\nlinear combinations of dictionary atoms observed through white Gaussian noise.\nIn this regard, the paper provides a general lower bound on the minimax risk\nand also adapts the proof techniques for equivalent results using sparse and\nGaussian coefficient models. The reported results suggest that the sample\ncomplexity of dictionary learning for tensor data can be significantly lower\nthan that for unstructured data.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 18:42:31 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Shakeri", "Zahra", ""], ["Bajwa", "Waheed U.", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "1605.05349", "submitter": "Subhadeep Paul", "authors": "Subhadeep Paul and Yuguo Chen", "title": "Orthogonal symmetric non-negative matrix factorization under the\n  stochastic block model", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method based on the orthogonal symmetric non-negative matrix\ntri-factorization of the normalized Laplacian matrix for community detection in\ncomplex networks. While the exact factorization of a given order may not exist\nand is NP hard to compute, we obtain an approximate factorization by solving an\noptimization problem. We establish the connection of the factors obtained\nthrough the factorization to a non-negative basis of an invariant subspace of\nthe estimated matrix, drawing parallel with the spectral clustering. Using such\nfactorization for clustering in networks is motivated by analyzing a\nblock-diagonal Laplacian matrix with the blocks representing the connected\ncomponents of a graph. The method is shown to be consistent for community\ndetection in graphs generated from the stochastic block model and the degree\ncorrected stochastic block model. Simulation results and real data analysis\nshow the effectiveness of these methods under a wide variety of situations,\nincluding sparse and highly heterogeneous graphs where the usual spectral\nclustering is known to fail. Our method also performs better than the state of\nthe art in popular benchmark network datasets, e.g., the political web blogs\nand the karate club data.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 20:22:12 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Paul", "Subhadeep", ""], ["Chen", "Yuguo", ""]]}, {"id": "1605.05422", "submitter": "Shinji Ito", "authors": "Shinji Ito and Ryohei Fujimaki", "title": "Optimization Beyond Prediction: Prescriptive Price Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a novel data science problem, prescriptive price\noptimization, which derives the optimal price strategy to maximize future\nprofit/revenue on the basis of massive predictive formulas produced by machine\nlearning. The prescriptive price optimization first builds sales forecast\nformulas of multiple products, on the basis of historical data, which reveal\ncomplex relationships between sales and prices, such as price elasticity of\ndemand and cannibalization. Then, it constructs a mathematical optimization\nproblem on the basis of those predictive formulas. We present that the\noptimization problem can be formulated as an instance of binary quadratic\nprogramming (BQP). Although BQP problems are NP-hard in general and\ncomputationally intractable, we propose a fast approximation algorithm using a\nsemi-definite programming (SDP) relaxation, which is closely related to the\nGoemans-Williamson's Max-Cut approximation. Our experiments on simulation and\nreal retail datasets show that our prescriptive price optimization\nsimultaneously derives the optimal prices of tens/hundreds products with\npractical computational time, that potentially improve 8.2% of gross profit of\nthose products.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 02:46:14 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 06:38:18 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Ito", "Shinji", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1605.05509", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Michele Scarpiniti, Danilo Comminiello, Aurelio\n  Uncini", "title": "Learning activation functions from data using cubic spline interpolation", "comments": "Submitted to the 27th Italian Workshop on Neural Networks (WIRN 2017)", "journal-ref": "Neural Advances in Processing Nonlinear Dynamic Signals, 2017", "doi": "10.1007/978-3-319-95098-3_7", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks require a careful design in order to perform properly on a\ngiven task. In particular, selecting a good activation function (possibly in a\ndata-dependent fashion) is a crucial step, which remains an open problem in the\nresearch community. Despite a large amount of investigations, most current\nimplementations simply select one fixed function from a small set of\ncandidates, which is not adapted during training, and is shared among all\nneurons throughout the different layers. However, neither two of these\nassumptions can be supposed optimal in practice. In this paper, we present a\nprincipled way to have data-dependent adaptation of the activation functions,\nwhich is performed independently for each neuron. This is achieved by\nleveraging over past and present advances on cubic spline interpolation,\nallowing for local adaptation of the functions around their regions of use. The\nresulting algorithm is relatively cheap to implement, and overfitting is\ncounterbalanced by the inclusion of a novel damping criterion, which penalizes\nunwanted oscillations from a predefined shape. Experimental results validate\nthe proposal over two well-known benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 10:46:01 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 07:28:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Scardapane", "Simone", ""], ["Scarpiniti", "Michele", ""], ["Comminiello", "Danilo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1605.05537", "submitter": "Jean-Michel Marin", "authors": "Louis Raynal, Jean-Michel Marin, Pierre Pudlo, Mathieu Ribatet,\n  Christian P. Robert and Arnaud Estoup", "title": "ABC random forests for Bayesian parameter inference", "comments": "Main text: 24 pages, 6 figures Supplementary Information: 14 pages, 5\n  figures", "journal-ref": null, "doi": "10.24072/pci.evolbiol.100036", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This preprint has been reviewed and recommended by Peer Community In\nEvolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036).\nApproximate Bayesian computation (ABC) has grown into a standard methodology\nthat manages Bayesian inference for models associated with intractable\nlikelihood functions. Most ABC implementations require the preliminary\nselection of a vector of informative statistics summarizing raw data.\nFurthermore, in almost all existing implementations, the tolerance level that\nseparates acceptance from rejection of simulated parameter values needs to be\ncalibrated. We propose to conduct likelihood-free Bayesian inferences about\nparameters with no prior selection of the relevant components of the summary\nstatistics and bypassing the derivation of the associated tolerance level. The\napproach relies on the random forest methodology of Breiman (2001) applied in a\n(non parametric) regression setting. We advocate the derivation of a new random\nforest for each component of the parameter vector of interest. When compared\nwith earlier ABC solutions, this method offers significant gains in terms of\nrobustness to the choice of the summary statistics, does not depend on any type\nof tolerance level, and is a good trade-off in term of quality of point\nestimator precision and credible interval estimations for a given computing\ntime. We illustrate the performance of our methodological proposal and compare\nit with earlier ABC methods on a Normal toy example and a population genetics\nexample dealing with human population evolution. All methods designed here have\nbeen incorporated in the R package abcrf (version 1.7) available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:04:38 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 16:28:19 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 14:36:45 GMT"}, {"version": "v4", "created": "Tue, 14 Nov 2017 15:15:23 GMT"}, {"version": "v5", "created": "Fri, 2 Nov 2018 14:17:46 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Raynal", "Louis", ""], ["Marin", "Jean-Michel", ""], ["Pudlo", "Pierre", ""], ["Ribatet", "Mathieu", ""], ["Robert", "Christian P.", ""], ["Estoup", "Arnaud", ""]]}, {"id": "1605.05588", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi", "title": "A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire\n  Systems", "comments": "It had to be noted that the assumption was made that all sensors have\n  access to all observations and state estimate vectors. In addition, the\n  summations in the DAQKF Algorithm are on all sensors, not just the\n  neighbouring sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of automated flight control and management systems have made\npossible aircraft designs that sacrifice arodynamic stability in order to\nincorporate stealth technology intro their shape, operate more efficiently, and\nare highly maneuverable. Therefore, modern flight management systems are\nreliant on multiple redundant sensors to monitor and control the rotations of\nthe aircraft. To this end, a novel distributed quaternion Kalman filtering\nalgorithm is developed for tracking the rotation and orientation of an aircraft\nin the three-dimensional space. The algorithm is developed to distribute\ncomputation among the sensors in a manner that forces them to consent to a\nunique solution while being robust to sensor and link failure, a desirable\ncharacteristic for flight management systems. In addition, the underlying\nquaternion-valued state space model allows to avoid problems associated with\ngimbal lock. The performance of the developed algorithm is verified through\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 19:48:53 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 08:06:46 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Talebi", "Sayed Pouria", ""]]}, {"id": "1605.05622", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan and David J. Nott", "title": "Gaussian variational approximation with sparse precision matrices", "comments": "18 pages, 9 figures", "journal-ref": "Statistics and Computing 28 (2018) 259-275", "doi": "10.1007/s11222-017-9729-7", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a Gaussian variational approximation to\nthe posterior distribution for a high-dimensional parameter, where we impose\nsparsity in the precision matrix to reflect appropriate conditional\nindependence structure in the model. Incorporating sparsity in the precision\nmatrix allows the Gaussian variational distribution to be both flexible and\nparsimonious, and the sparsity is achieved through parameterization in terms of\nthe Cholesky factor. Efficient stochastic gradient methods which make\nappropriate use of gradient information for the target distribution are\ndeveloped for the optimization. We consider alternative estimators of the\nstochastic gradients which have lower variation and are more stable. Our\napproach is illustrated using generalized linear mixed models and state space\nmodels for time series.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 15:38:16 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 05:52:36 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 03:29:26 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Nott", "David J.", ""]]}, {"id": "1605.05697", "submitter": "Carlos Gomez Uribe", "authors": "Carlos Alberto Gomez-Uribe", "title": "Online Algorithms For Parameter Mean And Variance Estimation In Dynamic\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a regression model from\na set of observations, each consisting of a response and a predictor. The\nresponse is assumed to be related to the predictor via a regression model of\nunknown parameters. Often, in such models the parameters to be estimated are\nassumed to be constant. Here we consider the more general scenario where the\nparameters are allowed to evolve over time, a more natural assumption for many\napplications. We model these dynamics via a linear update equation with\nadditive noise that is often used in a wide range of engineering applications,\nparticularly in the well-known and widely used Kalman filter (where the system\nstate it seeks to estimate maps to the parameter values here). We derive an\napproximate algorithm to estimate both the mean and the variance of the\nparameter estimates in an online fashion for a generic regression model. This\nalgorithm turns out to be equivalent to the extended Kalman filter. We\nspecialize our algorithm to the multivariate exponential family distribution to\nobtain a generalization of the generalized linear model (GLM). Because the\ncommon regression models encountered in practice such as logistic, exponential\nand multinomial all have observations modeled through an exponential family\ndistribution, our results are used to easily obtain algorithms for online mean\nand variance parameter estimation for all these regression models in the\ncontext of time-dependent parameters. Lastly, we propose to use these\nalgorithms in the contextual multi-armed bandit scenario, where so far model\nparameters are assumed static and observations univariate and Gaussian or\nBernoulli. Both of these restrictions can be relaxed using the algorithms\ndescribed here, which we combine with Thompson sampling to show the resulting\nperformance on a simulation.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 19:00:39 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Gomez-Uribe", "Carlos Alberto", ""]]}, {"id": "1605.05721", "submitter": "Ping Li", "authors": "Ping Li", "title": "Linearized GMM Kernels and Normalized Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of \"random Fourier features (RFF)\" has become a popular tool for\napproximating the \"radial basis function (RBF)\" kernel. The variance of RFF is\nactually large. Interestingly, the variance can be substantially reduced by a\nsimple normalization step as we theoretically demonstrate. We name the improved\nscheme as the \"normalized RFF (NRFF)\".\n  We also propose the \"generalized min-max (GMM)\" kernel as a measure of data\nsimilarity. GMM is positive definite as there is an associated hashing method\nnamed \"generalized consistent weighted sampling (GCWS)\" which linearizes this\nnonlinear kernel. We provide an extensive empirical evaluation of the RBF\nkernel and the GMM kernel on more than 50 publicly available datasets. For a\nmajority of the datasets, the (tuning-free) GMM kernel outperforms the\nbest-tuned RBF kernel.\n  We conduct extensive experiments for comparing the linearized RBF kernel\nusing NRFF with the linearized GMM kernel using GCWS. We observe that, to reach\na comparable classification accuracy, GCWS typically requires substantially\nfewer samples than NRFF, even on datasets where the original RBF kernel\noutperforms the original GMM kernel. The empirical success of GCWS (compared to\nNRFF) can also be explained from a theoretical perspective. Firstly, the\nrelative variance (normalized by the squared expectation) of GCWS is\nsubstantially smaller than that of NRFF, except for the very high similarity\nregion (where the variances of both methods are close to zero). Secondly, if we\nmake a model assumption on the data, we can show analytically that GCWS\nexhibits much smaller variance than NRFF for estimating the same object (e.g.,\nthe RBF kernel), except for the very high similarity region.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 19:54:22 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 19:51:39 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 18:42:09 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 17:11:48 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1605.05775", "submitter": "E.M. Stoudenmire", "authors": "E. Miles Stoudenmire and David J. Schwab", "title": "Supervised Learning with Quantum-Inspired Tensor Networks", "comments": "11 pages, 15 figures; updated version includes corrections, links to\n  sample codes, expanded discussion, and additional references", "journal-ref": "Advances in Neural Information Processing Systems 29, 4799 (2016)", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.str-el cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks are efficient representations of high-dimensional tensors\nwhich have been very successful for physics and mathematics applications. We\ndemonstrate how algorithms for optimizing such networks can be adapted to\nsupervised learning tasks by using matrix product states (tensor trains) to\nparameterize models for classifying images. For the MNIST data set we obtain\nless than 1% test set classification error. We discuss how the tensor network\nform imparts additional structure to the learned model and suggest a possible\ngenerative interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 22:20:35 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:03:45 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stoudenmire", "E. Miles", ""], ["Schwab", "David J.", ""]]}, {"id": "1605.05776", "submitter": "Navid Tafaghodi Khajavi", "authors": "Navid Tafaghodi Khajavi and Anthony Kuh", "title": "The Quality of the Covariance Selection Through Detection Problem and\n  AUC Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quantifying the quality of a model selection\nproblem for a graphical model. We discuss this by formulating the problem as a\ndetection problem. Model selection problems usually minimize a distance between\nthe original distribution and the model distribution. For the special case of\nGaussian distributions, the model selection problem simplifies to the\ncovariance selection problem which is widely discussed in literature by\nDempster [2] where the likelihood criterion is maximized or equivalently the\nKullback-Leibler (KL) divergence is minimized to compute the model covariance\nmatrix. While this solution is optimal for Gaussian distributions in the sense\nof the KL divergence, it is not optimal when compared with other information\ndivergences and criteria such as Area Under the Curve (AUC).\n  In this paper, we analytically compute upper and lower bounds for the AUC and\ndiscuss the quality of model selection problem using the AUC and its bounds as\nan accuracy measure in detection problem. We define the correlation\napproximation matrix (CAM) and show that analytical computation of the KL\ndivergence, the AUC and its bounds only depend on the eigenvalues of CAM. We\nalso show the relationship between the AUC, the KL divergence and the ROC curve\nby optimizing with respect to the ROC curve. In the examples provided, we pick\ntree structures as the simplest graphical models. We perform simulations on\nfully-connected graphs and compute the tree structured models by applying the\nwidely used Chow-Liu algorithm [3]. Examples show that the quality of tree\napproximation models are not good in general based on information divergences,\nthe AUC and its bounds when the number of nodes in the graphical model is\nlarge. We show both analytically and by simulations that the 1-AUC for the tree\napproximation model decays exponentially as the dimension of graphical model\nincreases.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 22:22:57 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 05:13:12 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 03:31:54 GMT"}, {"version": "v4", "created": "Wed, 18 Oct 2017 11:21:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Khajavi", "Navid Tafaghodi", ""], ["Kuh", "Anthony", ""]]}, {"id": "1605.05785", "submitter": "Shashank Singh", "authors": "Shashank Singh, Simon S. Du, Barnab\\'as P\\'oczos", "title": "Efficient Nonparametric Smoothness Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sobolev quantities (norms, inner products, and distances) of probability\ndensity functions are important in the theory of nonparametric statistics, but\nhave rarely been used in practice, partly due to a lack of practical\nestimators. They also include, as special cases, $L^2$ quantities which are\nused in many applications. We propose and analyze a family of estimators for\nSobolev quantities of unknown probability density functions. We bound the bias\nand variance of our estimators over finite samples, finding that they are\ngenerally minimax rate-optimal. Our estimators are significantly more\ncomputationally tractable than previous estimators, and exhibit a\nstatistical/computational trade-off allowing them to adapt to computational\nconstraints. We also draw theoretical connections to recent work on fast\ntwo-sample testing. Finally, we empirically validate our estimators on\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 00:29:38 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 02:47:02 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Singh", "Shashank", ""], ["Du", "Simon S.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1605.05799", "submitter": "Joseph Makin", "authors": "Joseph G. Makin, Benjamin K. Dichter, Philip N. Sabes", "title": "Recurrent Exponential-Family Harmoniums without Backprop-Through-Time", "comments": "28 pages, 6 figures. Under review at JMLR since January 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential-family harmoniums (EFHs), which extend restricted Boltzmann\nmachines (RBMs) from Bernoulli random variables to other exponential families\n(Welling et al., 2005), are generative models that can be trained with\nunsupervised-learning techniques, like contrastive divergence (Hinton et al.\n2006; Hinton, 2002), as density estimators for static data. Methods for\nextending RBMs--and likewise EFHs--to data with temporal dependencies have been\nproposed previously (Sutskever and Hinton, 2007; Sutskever et al., 2009), the\nlearning procedure being validated by qualitative assessment of the generative\nmodel. Here we propose and justify, from a very different perspective, an\nalternative training procedure, proving sufficient conditions for optimal\ninference under that procedure. The resulting algorithm can be learned with\nonly forward passes through the data--backprop-through-time is not required, as\nin previous approaches. The proof exploits a recent result about information\nretention in density estimators (Makin and Sabes, 2015), and applies it to a\n\"recurrent EFH\" (rEFH) by induction. Finally, we demonstrate optimality by\nsimulation, testing the rEFH: (1) as a filter on training data generated with a\nlinear dynamical system, the position of which is noisily reported by a\npopulation of \"neurons\" with Poisson-distributed spike counts; and (2) with the\nqualitative experiments proposed by Sutskever et al. (2009).\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 03:19:31 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Makin", "Joseph G.", ""], ["Dichter", "Benjamin K.", ""], ["Sabes", "Philip N.", ""]]}, {"id": "1605.05860", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, Yuan Yao", "title": "False Discovery Rate Control and Statistical Quality Assessment of\n  Annotators in Crowdsourced Ranking", "comments": "ICML 2016 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of crowdsourcing platforms it has become easy and\nrelatively inexpensive to collect a dataset labeled by multiple annotators in a\nshort time. However due to the lack of control over the quality of the\nannotators, some abnormal annotators may be affected by position bias which can\npotentially degrade the quality of the final consensus labels. In this paper we\nintroduce a statistical framework to model and detect annotator's position bias\nin order to control the false discovery rate (FDR) without a prior knowledge on\nthe amount of biased annotators - the expected fraction of false discoveries\namong all discoveries being not too high, in order to assure that most of the\ndiscoveries are indeed true and replicable. The key technical development\nrelies on some new knockoff filters adapted to our problem and new algorithms\nbased on the Inverse Scale Space dynamics whose discretization is potentially\nsuitable for large scale crowdsourcing data analysis. Our studies are supported\nby experiments with both simulated examples and real-world data. The proposed\nframework provides us a useful tool for quantitatively studying annotator's\nabnormal behavior in crowdsourcing data arising from machine learning,\nsociology, computer vision, multimedia, etc.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 09:14:39 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 10:23:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 15:30:43 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Yao", "Yuan", ""]]}, {"id": "1605.05918", "submitter": "Pierre-Alexandre Mattei", "authors": "Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei", "title": "Bayesian Variable Selection for Globally Sparse Probabilistic PCA", "comments": "An earlier version of this paper appeared in the Proceedings of the\n  19th International Conference on Artificial Intelligence and Statistics\n  (AISTATS 2016)", "journal-ref": null, "doi": "10.1214/18-EJS1450", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse versions of principal component analysis (PCA) have imposed themselves\nas simple, yet powerful ways of selecting relevant features of high-dimensional\ndata in an unsupervised manner. However, when several sparse principal\ncomponents are computed, the interpretation of the selected variables is\ndifficult since each axis has its own sparsity pattern and has to be\ninterpreted separately. To overcome this drawback, we propose a Bayesian\nprocedure called globally sparse probabilistic PCA (GSPPCA) that allows to\nobtain several sparse components with the same sparsity pattern. This allows\nthe practitioner to identify the original variables which are relevant to\ndescribe the data. To this end, using Roweis' probabilistic interpretation of\nPCA and a Gaussian prior on the loading matrix, we provide the first exact\ncomputation of the marginal likelihood of a Bayesian PCA model. To avoid the\ndrawbacks of discrete model selection, a simple relaxation of this framework is\npresented. It allows to find a path of models using a variational\nexpectation-maximization algorithm. The exact marginal likelihood is then\nmaximized over this path. This approach is illustrated on real and synthetic\ndata sets. In particular, using unlabeled microarray data, GSPPCA infers much\nmore relevant gene subsets than traditional sparse PCA algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:34:34 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:15:21 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bouveyron", "Charles", ""], ["Latouche", "Pierre", ""], ["Mattei", "Pierre-Alexandre", ""]]}, {"id": "1605.05969", "submitter": "Yangyang Xu", "authors": "Xiang Gao, Yangyang Xu, Shuzhong Zhang", "title": "Randomized Primal-Dual Proximal Block Coordinate Updates", "comments": "convergence rate results are presented in a more explicit way;\n  numerical results are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a randomized primal-dual proximal block coordinate\nupdating framework for a general multi-block convex optimization model with\ncoupled objective function and linear constraints. Assuming mere convexity, we\nestablish its $O(1/t)$ convergence rate in terms of the objective value and\nfeasibility measure. The framework includes several existing algorithms as\nspecial cases such as a primal-dual method for bilinear saddle-point problems\n(PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of the\nADMM method for multi-block convex optimization. Our analysis recovers and/or\nstrengthens the convergence properties of several existing algorithms. For\nexample, for PD-S our result leads to the same order of convergence rate\nwithout the previously assumed boundedness condition on the constraint sets,\nand for Prox-JADMM the new result provides convergence rate in terms of the\nobjective value and the feasibility violation. It is well known that the\noriginal ADMM may fail to converge when the number of blocks exceeds two. Our\nresult shows that if an appropriate randomization procedure is invoked to\nselect the updating blocks, then a sublinear rate of convergence in expectation\ncan be guaranteed for multi-block ADMM, without assuming any strong convexity.\nThe new approach is also extended to solve problems where only a stochastic\napproximation of the (sub-)gradient of the objective is available, and we\nestablish an $O(1/\\sqrt{t})$ convergence rate of the extended approach for\nsolving stochastic programming.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 14:20:20 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 03:57:50 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 15:43:48 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Gao", "Xiang", ""], ["Xu", "Yangyang", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1605.06049", "submitter": "Albert Berahas", "authors": "Albert S. Berahas, Jorge Nocedal, Martin Tak\\'a\\v{c}", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "comments": "NIPS 2016. 31 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to parallelize the stochastic gradient descent (SGD)\nmethod has received much attention in the literature. In this paper, we focus\ninstead on batch methods that use a sizeable fraction of the training set at\neach iteration to facilitate parallelism, and that employ second-order\ninformation. In order to improve the learning process, we follow a multi-batch\napproach in which the batch changes at each iteration. This can cause\ndifficulties because L-BFGS employs gradient differences to update the Hessian\napproximations, and when these gradients are computed using different data\npoints the process can be unstable. This paper shows how to perform stable\nquasi-Newton updating in the multi-batch setting, illustrates the behavior of\nthe algorithm in a distributed computing platform, and studies its convergence\nproperties for both the convex and nonconvex cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 16:53:50 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 22:48:01 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Berahas", "Albert S.", ""], ["Nocedal", "Jorge", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1605.06181", "submitter": "Yusheng Xie", "authors": "Yusheng Xie, Nan Du, Wei Fan, Jing Zhai, Weicheng Zhu", "title": "Variational hybridization and transformation for large inaccurate\n  noisy-or networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference provides approximations to the computationally\nintractable posterior distribution in Bayesian networks. A prominent medical\napplication of noisy-or Bayesian network is to infer potential diseases given\nobserved symptoms. Previous studies focus on approximating a handful of\ncomplicated pathological cases using variational transformation. Our goal is to\nuse variational transformation as part of a novel hybridized inference for\nserving reliable and real time diagnosis at web scale. We propose a hybridized\ninference that allows variational parameters to be estimated without disease\nposteriors or priors, making the inference faster and much of its computation\nrecyclable. In addition, we propose a transformation ranking algorithm that is\nvery stable to large variances in network prior probabilities, a common issue\nthat arises in medical applications of Bayesian networks. In experiments, we\nperform comparative study on a large real life medical network and scalability\nstudy on a much larger (36,000x) synthesized network.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 00:31:07 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Xie", "Yusheng", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Zhai", "Jing", ""], ["Zhu", "Weicheng", ""]]}, {"id": "1605.06197", "submitter": "Eric Nalisnick", "authors": "Eric Nalisnick and Padhraic Smyth", "title": "Stick-Breaking Variational Autoencoders", "comments": "ICLR 2017, Conference Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 02:20:13 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 03:16:25 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 22:36:33 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Nalisnick", "Eric", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1605.06201", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Kent Quanrud, Amirhossein Taghvaei", "title": "Adversarial Delays in Online Strongly-Convex Optimization", "comments": "We discovered mistakes in the proof of proof of Theorem 3.1. The\n  overall is no longer correct, although the claim is still true", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of strongly-convex online optimization in presence of\nadversarial delays; in a T-iteration online game, the feedback of the player's\nquery at time t is arbitrarily delayed by an adversary for d_t rounds and\ndelivered before the game ends, at iteration t+d_t-1. Specifically for\n\\algo{online-gradient-descent} algorithm we show it has a simple regret bound\nof \\Oh{\\sum_{t=1}^T \\log (1+ \\frac{d_t}{t})}. This gives a clear and simple\nbound without resorting any distributional and limiting assumptions on the\ndelays. We further show how this result encompasses and generalizes several of\nthe existing known results in the literature. Specifically it matches the\ncelebrated logarithmic regret \\Oh{\\log T} when there are no delays (i.e. d_t =\n1) and regret bound of \\Oh{\\tau \\log T} for constant delays d_t = \\tau.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 02:55:59 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 18:41:57 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 18:34:56 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 04:53:49 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Khashabi", "Daniel", ""], ["Quanrud", "Kent", ""], ["Taghvaei", "Amirhossein", ""]]}, {"id": "1605.06220", "submitter": "Bai Jiang", "authors": "Bai Jiang, Tung-yu Wu, Wing H. Wong", "title": "Convergence of Contrastive Divergence with Annealed Learning Rate in\n  Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our recent paper, we showed that in exponential family, contrastive\ndivergence (CD) with fixed learning rate will give asymptotically consistent\nestimates \\cite{wu2016convergence}. In this paper, we establish consistency and\nconvergence rate of CD with annealed learning rate $\\eta_t$. Specifically,\nsuppose CD-$m$ generates the sequence of parameters $\\{\\theta_t\\}_{t \\ge 0}$\nusing an i.i.d. data sample $\\mathbf{X}_1^n \\sim p_{\\theta^*}$ of size $n$,\nthen $\\delta_n(\\mathbf{X}_1^n) = \\limsup_{t \\to \\infty} \\Vert \\sum_{s=t_0}^t\n\\eta_s \\theta_s / \\sum_{s=t_0}^t \\eta_s - \\theta^* \\Vert$ converges in\nprobability to 0 at a rate of $1/\\sqrt[3]{n}$. The number ($m$) of MCMC\ntransitions in CD only affects the coefficient factor of convergence rate. Our\nproof is not a simple extension of the one in \\cite{wu2016convergence}. which\ndepends critically on the fact that $\\{\\theta_t\\}_{t \\ge 0}$ is a homogeneous\nMarkov chain conditional on the observed sample $\\mathbf{X}_1^n$. Under\nannealed learning rate, the homogeneous Markov property is not available and we\nhave to develop an alternative approach based on super-martingales. Experiment\nresults of CD on a fully-visible $2\\times 2$ Boltzmann Machine are provided to\ndemonstrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 06:26:38 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Jiang", "Bai", ""], ["Wu", "Tung-yu", ""], ["Wong", "Wing H.", ""]]}, {"id": "1605.06265", "submitter": "Julien Mairal", "authors": "Julien Mairal", "title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "comments": "to appear in Advances in Neural Information Processing Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new image representation based on a multilayer\nkernel machine. Unlike traditional kernel methods where data representation is\ndecoupled from the prediction task, we learn how to shape the kernel with\nsupervision. We proceed by first proposing improvements of the\nrecently-introduced convolutional kernel networks (CKNs) in the context of\nunsupervised learning; then, we derive backpropagation rules to take advantage\nof labeled training data. The resulting model is a new type of convolutional\nneural network, where optimizing the filters at each layer is equivalent to\nlearning a linear subspace in a reproducing kernel Hilbert space (RKHS). We\nshow that our method achieves reasonably competitive performance for image\nclassification on some standard \"deep learning\" datasets such as CIFAR-10 and\nSVHN, and also for image super-resolution, demonstrating the applicability of\nour approach to a large variety of image-related tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 09:52:14 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 12:52:50 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Mairal", "Julien", ""]]}, {"id": "1605.06276", "submitter": "Alexander Gorban", "authors": "A.N. Gorban, E.M. Mirkes, A. Zinovyev", "title": "Piece-wise quadratic approximations of arbitrary error functions for\n  fast and robust machine learning", "comments": "Edited and extended version with algortihms of regularized regression", "journal-ref": "Neural Networks, Volume 84, December 2016, 28-38", "doi": "10.1016/j.neunet.2016.08.007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of machine learning approaches have stemmed from the application of\nminimizing the mean squared distance principle, based on the computationally\nefficient quadratic optimization methods. However, when faced with\nhigh-dimensional and noisy data, the quadratic error functionals demonstrated\nmany weaknesses including high sensitivity to contaminating factors and\ndimensionality curse. Therefore, a lot of recent applications in machine\nlearning exploited properties of non-quadratic error functionals based on $L_1$\nnorm or even sub-linear potentials corresponding to quasinorms $L_p$ ($0<p<1$).\nThe back side of these approaches is increase in computational cost for\noptimization. Till so far, no approaches have been suggested to deal with {\\it\narbitrary} error functionals, in a flexible and computationally efficient\nframework. In this paper, we develop a theory and basic universal data\napproximation algorithms ($k$-means, principal components, principal manifolds\nand graphs, regularized and sparse regression), based on piece-wise quadratic\nerror potentials of subquadratic growth (PQSQ potentials). We develop a new and\nuniversal framework to minimize {\\it arbitrary sub-quadratic error potentials}\nusing an algorithm with guaranteed fast convergence to the local or global\nerror minimum. The theory of PQSQ potentials is based on the notion of the cone\nof minorant functions, and represents a natural approximation formalism based\non the application of min-plus algebra. The approach can be applied in most of\nexisting machine learning methods, including methods of data approximation and\nregularized and sparse regression, leading to the improvement in the\ncomputational cost/accuracy trade-off. We demonstrate that on synthetic and\nreal-life datasets PQSQ-based machine learning methods achieve orders of\nmagnitude faster computational performance than the corresponding\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 10:25:47 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 12:44:25 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Gorban", "A. N.", ""], ["Mirkes", "E. M.", ""], ["Zinovyev", "A.", ""]]}, {"id": "1605.06336", "submitter": "Aapo Hyvarinen", "authors": "Aapo Hyvarinen and Hiroshi Morioka", "title": "Unsupervised Feature Extraction by Time-Contrastive Learning and\n  Nonlinear ICA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear independent component analysis (ICA) provides an appealing\nframework for unsupervised feature learning, but the models proposed so far are\nnot identifiable. Here, we first propose a new intuitive principle of\nunsupervised deep learning from time series which uses the nonstationary\nstructure of the data. Our learning principle, time-contrastive learning (TCL),\nfinds a representation which allows optimal discrimination of time segments\n(windows). Surprisingly, we show how TCL can be related to a nonlinear ICA\nmodel, when ICA is redefined to include temporal nonstationarities. In\nparticular, we show that TCL combined with linear ICA estimates the nonlinear\nICA model up to point-wise transformations of the sources, and this solution is\nunique --- thus providing the first identifiability result for nonlinear ICA\nwhich is rigorous, constructive, as well as very general.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 12:59:22 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Hyvarinen", "Aapo", ""], ["Morioka", "Hiroshi", ""]]}, {"id": "1605.06359", "submitter": "Eugene Belilovsky", "authors": "Eugene Belilovsky (CVN, GALEN), Kyle Kastner, Ga\\\"el Varoquaux\n  (NEUROSPIN, PARIETAL), Matthew Blaschko", "title": "Learning to Discover Sparse Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider structure discovery of undirected graphical models from\nobservational data. Inferring likely structures from few examples is a complex\ntask often requiring the formulation of priors and sophisticated inference\nprocedures. Popular methods rely on estimating a penalized maximum likelihood\nof the precision matrix. However, in these approaches structure recovery is an\nindirect consequence of the data-fit term, the penalty can be difficult to\nadapt for domain-specific knowledge, and the inference is computationally\ndemanding. By contrast, it may be easier to generate training samples of data\nthat arise from graphs with the desired structure properties. We propose here\nto leverage this latter source of information as training data to learn a\nfunction, parametrized by a neural network that maps empirical covariance\nmatrices to estimated graph structures. Learning this function brings two\nbenefits: it implicitly models the desired structure or sparsity properties to\nform suitable priors, and it can be tailored to the specific problem of edge\nstructure discovery, rather than maximizing data likelihood. Applying this\nframework, we find our learnable graph-discovery method trained on synthetic\ndata generalizes well: identifying relevant edges in both synthetic and real\ndata, completely unknown at training time. We find that on genetics, brain\nimaging, and simulation data we obtain performance generally superior to\nanalytical methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 13:58:21 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 06:32:14 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 09:09:36 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Belilovsky", "Eugene", "", "CVN, GALEN"], ["Kastner", "Kyle", "", "NEUROSPIN, PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "NEUROSPIN, PARIETAL"], ["Blaschko", "Matthew", ""]]}, {"id": "1605.06376", "submitter": "George Papamakarios", "authors": "George Papamakarios, Iain Murray", "title": "Fast $\\epsilon$-free Inference of Simulation Models with Bayesian\n  Conditional Density Estimation", "comments": "Appeared at NIPS 2016. Fixed typo in Eq (37)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical models can be simulated forwards but have intractable\nlikelihoods. Approximate Bayesian Computation (ABC) methods are used to infer\nproperties of these models from data. Traditionally these methods approximate\nthe posterior over parameters by conditioning on data being inside an\n$\\epsilon$-ball around the observed data, which is only correct in the limit\n$\\epsilon\\!\\rightarrow\\!0$. Monte Carlo methods can then draw samples from the\napproximate posterior to approximate predictions or error bars on parameters.\nThese algorithms critically slow down as $\\epsilon\\!\\rightarrow\\!0$, and in\npractice draw samples from a broader distribution than the posterior. We\npropose a new approach to likelihood-free inference based on Bayesian\nconditional density estimation. Preliminary inferences based on limited\nsimulation data are used to guide later simulations. In some cases, learning an\naccurate parametric representation of the entire true posterior distribution\nrequires fewer model simulations than Monte Carlo ABC methods need to produce a\nsingle sample from an approximate posterior.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 14:34:38 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 14:55:38 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 17:11:50 GMT"}, {"version": "v4", "created": "Mon, 2 Apr 2018 16:05:09 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Papamakarios", "George", ""], ["Murray", "Iain", ""]]}, {"id": "1605.06416", "submitter": "Jisu Kim", "authors": "Jisu Kim, Yen-Chi Chen, Sivaraman Balakrishnan, Alessandro Rinaldo,\n  Larry Wasserman", "title": "Statistical Inference for Cluster Trees", "comments": "20 pages, 6 figures, accepted in Neural Information Processing\n  Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cluster tree provides a highly-interpretable summary of a density function\nby representing the hierarchy of its high-density clusters. It is estimated\nusing the empirical tree, which is the cluster tree constructed from a density\nestimator. This paper addresses the basic question of quantifying our\nuncertainty by assessing the statistical significance of topological features\nof an empirical cluster tree. We first study a variety of metrics that can be\nused to compare different trees, analyze their properties and assess their\nsuitability for inference. We then propose methods to construct and summarize\nconfidence sets for the unknown true cluster tree. We introduce a partial\nordering on cluster trees which we use to prune some of the statistically\ninsignificant features of the empirical tree, yielding interpretable and\nparsimonious cluster trees. Finally, we illustrate the proposed methods on a\nvariety of synthetic examples and furthermore demonstrate their utility in the\nanalysis of a Graft-versus-Host Disease (GvHD) data set.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:04:01 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 03:00:06 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 17:12:00 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kim", "Jisu", ""], ["Chen", "Yen-Chi", ""], ["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1605.06420", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, James Zou", "title": "Quantifying the accuracy of approximate diffusions and Markov chains", "comments": "In Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chains and diffusion processes are indispensable tools in machine\nlearning and statistics that are used for inference, sampling, and modeling.\nWith the growth of large-scale datasets, the computational cost associated with\nsimulating these stochastic processes can be considerable, and many algorithms\nhave been proposed to approximate the underlying Markov chain or diffusion. A\nfundamental question is how the computational savings trade off against the\nstatistical error incurred due to approximations. This paper develops general\nresults that address this question. We bound the Wasserstein distance between\nthe equilibrium distributions of two diffusions as a function of their mixing\nrates and the deviation in their drifts. We show that this error bound is tight\nin simple Gaussian settings. Our general result on continuous diffusions can be\ndiscretized to provide insights into the computational-statistical trade-off of\nMarkov chains. As an illustration, we apply our framework to derive\nfinite-sample error bounds of approximate unadjusted Langevin dynamics. We\ncharacterize computation-constrained settings where, by using fast-to-compute\napproximate gradients in the Langevin dynamics, we obtain more accurate samples\ncompared to using the exact gradients. Finally, as an additional application of\nour approach, we quantify the accuracy of approximate zig-zag sampling. Our\ntheoretical analyses are supported by simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:17:22 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 17:30:56 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 15:06:21 GMT"}, {"version": "v4", "created": "Wed, 30 Aug 2017 14:50:01 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Zou", "James", ""]]}, {"id": "1605.06422", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala, Marc Lelarge and Lenka Zdeborov\\'a", "title": "Fast Randomized Semi-Supervised Clustering", "comments": null, "journal-ref": "Journal of Physics: Conf. Series 1036 (2018) 012015", "doi": "10.1088/1742-6596/1036/1/012015", "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering partially labeled data from a minimal\nnumber of randomly chosen pairwise comparisons between the items. We introduce\nan efficient local algorithm based on a power iteration of the non-backtracking\noperator and study its performance on a simple model. For the case of two\nclusters, we give bounds on the classification error and show that a small\nerror can be achieved from $O(n)$ randomly chosen measurements, where $n$ is\nthe number of items in the dataset. Our algorithm is therefore efficient both\nin terms of time and space complexities. We also investigate numerically the\nperformance of the algorithm on synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:21:13 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 16:26:16 GMT"}, {"version": "v3", "created": "Sun, 9 Oct 2016 07:45:16 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Lelarge", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1605.06423", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Tamara Broderick", "title": "Coresets for Scalable Bayesian Logistic Regression", "comments": "In Proceedings of Advances in Neural Information Processing Systems\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Bayesian methods in large-scale data settings is attractive\nbecause of the rich hierarchical models, uncertainty quantification, and prior\nspecification they provide. Standard Bayesian inference algorithms are\ncomputationally expensive, however, making their direct application to large\ndatasets difficult or infeasible. Recent work on scaling Bayesian inference has\nfocused on modifying the underlying algorithms to, for example, use only a\nrandom data subsample at each iteration. We leverage the insight that data is\noften redundant to instead obtain a weighted subset of the data (called a\ncoreset) that is much smaller than the original dataset. We can then use this\nsmall coreset in any number of existing posterior inference algorithms without\nmodification. In this paper, we develop an efficient coreset construction\nalgorithm for Bayesian logistic regression models. We provide theoretical\nguarantees on the size and approximation quality of the coreset -- both for\nfixed, known datasets, and in expectation for a wide class of data generative\nmodels. Crucially, the proposed approach also permits efficient construction of\nthe coreset in both streaming and parallel settings, with minimal additional\neffort. We demonstrate the efficacy of our approach on a number of synthetic\nand real-world datasets, and find that, in practice, the size of the coreset is\nindependent of the original dataset size. Furthermore, constructing the coreset\ntakes a negligible amount of time compared to that required to run MCMC on it.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:26:45 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 14:12:19 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 15:11:30 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1605.06432", "submitter": "Maximilian Soelch", "authors": "Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der\n  Smagt", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space\n  Models from Raw Data", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Variational Bayes Filters (DVBF), a new method for\nunsupervised learning and identification of latent Markovian state space\nmodels. Leveraging recent advances in Stochastic Gradient Variational Bayes,\nDVBF can overcome intractable inference distributions via variational\ninference. Thus, it can handle highly nonlinear input data with temporal and\nspatial dependencies such as image sequences without domain knowledge. Our\nexperiments show that enabling backpropagation through transitions enforces\nstate space assumptions and significantly improves information content of the\nlatent embedding. This also enables realistic long-term prediction.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:52:22 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 07:33:14 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 18:12:53 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Karl", "Maximilian", ""], ["Soelch", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1605.06443", "submitter": "Scott Yang", "authors": "Corinna Cortes, Mehryar Mohri, Vitaly Kuznetsov, Scott Yang", "title": "Structured Prediction Theory Based on Factor Graph Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theoretical analysis of structured prediction with a\nseries of new results. We give new data-dependent margin guarantees for\nstructured prediction for a very wide family of loss functions and a general\nfamily of hypotheses, with an arbitrary factor graph decomposition. These are\nthe tightest margin bounds known for both standard multi-class and general\nstructured prediction problems. Our guarantees are expressed in terms of a\ndata-dependent complexity measure, factor graph complexity, which we show can\nbe estimated from data and bounded in terms of familiar quantities. We further\nextend our theory by leveraging the principle of Voted Risk Minimization (VRM)\nand show that learning is possible even with complex factor graphs. We present\nnew learning bounds for this advanced setting, which we use to design two new\nalgorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting\n(StructBoost). These algorithms can make use of complex features and factor\ngraphs and yet benefit from favorable learning guarantees. We also report the\nresults of experiments with VCRF on several datasets to validate our theory.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 17:21:17 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 17:02:48 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Kuznetsov", "Vitaly", ""], ["Yang", "Scott", ""]]}, {"id": "1605.06444", "submitter": "Carlo Baldassi", "authors": "Carlo Baldassi, Christian Borgs, Jennifer Chayes, Alessandro Ingrosso,\n  Carlo Lucibello, Luca Saglietti and Riccardo Zecchina", "title": "Unreasonable Effectiveness of Learning Neural Networks: From Accessible\n  States and Robust Ensembles to Basic Algorithmic Schemes", "comments": "31 pages (14 main text, 18 appendix), 12 figures (6 main text, 6\n  appendix)", "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 113(48):E7655-E7662, 2016", "doi": "10.1073/pnas.1608103113", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In artificial neural networks, learning from data is a computationally\ndemanding task in which a large number of connection weights are iteratively\ntuned through stochastic-gradient-based heuristic processes over a\ncost-function. It is not well understood how learning occurs in these systems,\nin particular how they avoid getting trapped in configurations with poor\ncomputational performance. Here we study the difficult case of networks with\ndiscrete weights, where the optimization landscape is very rough even for\nsimple architectures, and provide theoretical and numerical evidence of the\nexistence of rare - but extremely dense and accessible - regions of\nconfigurations in the network weight space. We define a novel measure, which we\ncall the \"robust ensemble\" (RE), which suppresses trapping by isolated\nconfigurations and amplifies the role of these dense regions. We analytically\ncompute the RE in some exactly solvable models, and also provide a general\nalgorithmic scheme which is straightforward to implement: define a\ncost-function given by a sum of a finite number of replicas of the original\ncost-function, with a constraint centering the replicas around a driving\nassignment. To illustrate this, we derive several powerful new algorithms,\nranging from Markov Chains to message passing to gradient descent processes,\nwhere the algorithms target the robust dense states, resulting in substantial\nimprovements in performance. The weak dependence on the number of precision\nbits of the weights leads us to conjecture that very similar reasoning applies\nto more conventional neural networks. Analogous algorithmic schemes can also be\napplied to other optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 17:27:18 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 15:34:46 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 19:05:31 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Baldassi", "Carlo", ""], ["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Ingrosso", "Alessandro", ""], ["Lucibello", "Carlo", ""], ["Saglietti", "Luca", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1605.06451", "submitter": "Christian Knoll", "authors": "Christian Knoll, Franz Pernkopf, Dhagash Mehta, Tianran Chen", "title": "Fixed Points of Belief Propagation -- An Analysis via Polynomial\n  Homotopy Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation (BP) is an iterative method to perform approximate\ninference on arbitrary graphical models. Whether BP converges and if the\nsolution is a unique fixed point depends on both the structure and the\nparametrization of the model. To understand this dependence it is interesting\nto find \\emph{all} fixed points. In this work, we formulate a set of polynomial\nequations, the solutions of which correspond to BP fixed points. To solve such\na nonlinear system we present the numerical polynomial-homotopy-continuation\n(NPHC) method. Experiments on binary Ising models and on error-correcting codes\nshow how our method is capable of obtaining all BP fixed points. On Ising\nmodels with fixed parameters we show how the structure influences both the\nnumber of fixed points and the convergence properties. We further asses the\naccuracy of the marginals and weighted combinations thereof. Weighting\nmarginals with their respective partition function increases the accuracy in\nall experiments. Contrary to the conjecture that uniqueness of BP fixed points\nimplies convergence, we find graphs for which BP fails to converge, even though\na unique fixed point exists. Moreover, we show that this fixed point gives a\ngood approximation, and the NPHC method is able to obtain this fixed point.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 17:42:03 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 15:19:00 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 14:05:56 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Knoll", "Christian", ""], ["Pernkopf", "Franz", ""], ["Mehta", "Dhagash", ""], ["Chen", "Tianran", ""]]}, {"id": "1605.06457", "submitter": "Adrien Gaidon", "authors": "Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig", "title": "Virtual Worlds as Proxy for Multi-Object Tracking Analysis", "comments": "CVPR 2016, Virtual KITTI dataset download at\n  http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision algorithms typically require expensive data\nacquisition and accurate manual labeling. In this work, we instead leverage the\nrecent progress in computer graphics to generate fully labeled, dynamic, and\nphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtual\nworld cloning method, and validate our approach by building and publicly\nreleasing a new video dataset, called Virtual KITTI (see\nhttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),\nautomatically labeled with accurate ground truth for object detection,\ntracking, scene and instance segmentation, depth, and optical flow. We provide\nquantitative experimental evidence suggesting that (i) modern deep learning\nalgorithms pre-trained on real data behave similarly in real and virtual\nworlds, and (ii) pre-training on virtual data improves performance. As the gap\nbetween real and virtual worlds is small, virtual worlds enable measuring the\nimpact of various weather and imaging conditions on recognition performance,\nall other things being equal. We show these factors may affect drastically\notherwise high-performing deep models for tracking.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 18:03:07 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Gaidon", "Adrien", ""], ["Wang", "Qiao", ""], ["Cabon", "Yohann", ""], ["Vig", "Eleonora", ""]]}, {"id": "1605.06593", "submitter": "Zheng Wen", "authors": "Zheng Wen, Branislav Kveton, Michal Valko, Sharan Vaswani", "title": "Online Influence Maximization under Independent Cascade Model with\n  Semi-Bandit Feedback", "comments": "Compared with the previous version, this version has fixed a mistake.\n  This version is also consistent with the NIPS camera-ready version", "journal-ref": "Z. Wen, B. Kveton, M. Valko, and S. Vaswani, \"Online Influence\n  Maximization under Independent Cascade Model with Semi-Bandit Feedback\",\n  Advances in Neural Information Processing Systems 30 Proceedings, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online influence maximization problem in social networks under\nthe independent cascade model. Specifically, we aim to learn the set of \"best\ninfluencers\" in a social network online while repeatedly interacting with it.\nWe address the challenges of (i) combinatorial action space, since the number\nof feasible influencer sets grows exponentially with the maximum number of\ninfluencers, and (ii) limited feedback, since only the influenced portion of\nthe network is observed. Under a stochastic semi-bandit feedback, we propose\nand analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our\nbounds on the cumulative regret are polynomial in all quantities of interest,\nachieve near-optimal dependence on the number of interactions and reflect the\ntopology of the network and the activation probabilities of its edges, thereby\ngiving insights on the problem complexity. To the best of our knowledge, these\nare the first such results. Our experiments show that in several representative\ngraph topologies, the regret of IMLinUCB scales as suggested by our upper\nbounds. IMLinUCB permits linear generalization and thus is both statistically\nand computationally suitable for large-scale problems. Our experiments also\nshow that IMLinUCB with linear generalization can lead to low regret in\nreal-world online influence maximization.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 06:07:53 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 23:36:42 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 05:51:52 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Wen", "Zheng", ""], ["Kveton", "Branislav", ""], ["Valko", "Michal", ""], ["Vaswani", "Sharan", ""]]}, {"id": "1605.06619", "submitter": "Yitan Li", "authors": "Yitan Li, Linli Xu, Xiaowei Zhong, Qing Ling", "title": "Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic\n  Gradient Descent", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel optimization algorithms for solving large-scale machine\nlearning problems have drawn significant attention from academia to industry\nrecently. This paper proposes a novel algorithm, decoupled asynchronous\nproximal stochastic gradient descent (DAP-SGD), to minimize an objective\nfunction that is the composite of the average of multiple empirical losses and\na regularization term. Unlike the traditional asynchronous proximal stochastic\ngradient descent (TAP-SGD) in which the master carries much of the computation\nload, the proposed algorithm off-loads the majority of computation tasks from\nthe master to workers, and leaves the master to conduct simple addition\noperations. This strategy yields an easy-to-parallelize algorithm, whose\nperformance is justified by theoretical convergence analyses. To be specific,\nDAP-SGD achieves an $O(\\log T/T)$ rate when the step-size is diminishing and an\nergodic $O(1/\\sqrt{T})$ rate when the step-size is constant, where $T$ is the\nnumber of total iterations.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 10:27:50 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Li", "Yitan", ""], ["Xu", "Linli", ""], ["Zhong", "Xiaowei", ""], ["Ling", "Qing", ""]]}, {"id": "1605.06636", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan", "title": "Deep Transfer Learning with Joint Adaptation Networks", "comments": "34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have been successfully applied to learn transferable features\nfor adapting models from a source domain to a different target domain. In this\npaper, we present joint adaptation networks (JAN), which learn a transfer\nnetwork by aligning the joint distributions of multiple domain-specific layers\nacross domains based on a joint maximum mean discrepancy (JMMD) criterion.\nAdversarial training strategy is adopted to maximize JMMD such that the\ndistributions of the source and target domains are made more distinguishable.\nLearning can be performed by stochastic gradient descent with the gradients\ncomputed by back-propagation in linear-time. Experiments testify that our model\nyields state of the art results on standard datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 12:56:14 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 07:35:59 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Long", "Mingsheng", ""], ["Zhu", "Han", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1605.06650", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Tengfei Liu, Leonard K.M. Poon, Zhourong\n  Chen and Farhan Khawar", "title": "Latent Tree Models for Hierarchical Topic Detection", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for hierarchical topic detection where topics are\nobtained by clustering documents in multiple ways. Specifically, we model\ndocument collections using a class of graphical models called hierarchical\nlatent tree models (HLTMs). The variables at the bottom level of an HLTM are\nobserved binary variables that represent the presence/absence of words in a\ndocument. The variables at other levels are binary latent variables, with those\nat the lowest latent level representing word co-occurrence patterns and those\nat higher levels representing co-occurrence of patterns at the level below.\nEach latent variable gives a soft partition of the documents, and document\nclusters in the partitions are interpreted as topics. Latent variables at high\nlevels of the hierarchy capture long-range word co-occurrence patterns and\nhence give thematically more general topics, while those at low levels of the\nhierarchy capture short-range word co-occurrence patterns and give thematically\nmore specific topics. Unlike LDA-based topic models, HLTMs do not refer to a\ndocument generation process and use word variables instead of token variables.\nThey use a tree structure to model the relationships between topics and words,\nwhich is conducive to the discovery of meaningful topics and topic hierarchies.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 14:36:33 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 08:59:14 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Liu", "Tengfei", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""], ["Khawar", "Farhan", ""]]}, {"id": "1605.06711", "submitter": "Bo Yang", "authors": "Bo Yang, Xiao Fu and Nicholas D. Sidiropoulos", "title": "Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2614491", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction techniques play an essential role in data analytics,\nsignal processing and machine learning. Dimensionality reduction is usually\nperformed in a preprocessing stage that is separate from subsequent data\nanalysis, such as clustering or classification. Finding reduced-dimension\nrepresentations that are well-suited for the intended task is more appealing.\nThis paper proposes a joint factor analysis and latent clustering framework,\nwhich aims at learning cluster-aware low-dimensional representations of matrix\nand tensor data. The proposed approach leverages matrix and tensor\nfactorization models that produce essentially unique latent representations of\nthe data to unravel latent cluster structure -- which is otherwise obscured\nbecause of the freedom to apply an oblique transformation in latent space. At\nthe same time, latent cluster structure is used as prior information to enhance\nthe performance of factorization. Specific contributions include several\ncustom-built problem formulations, corresponding algorithms, and discussion of\nassociated convergence properties. Besides extensive simulations, real-world\ndatasets such as Reuters document data and MNIST image data are also employed\nto showcase the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 23:51:02 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yang", "Bo", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1605.06715", "submitter": "Jiaming Song", "authors": "Jiaming Song, Zhe Gan, Lawrence Carin", "title": "Factored Temporal Sigmoid Belief Networks for Sequence Learning", "comments": "to appear in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep conditional generative models are developed to simultaneously learn the\ntemporal dependencies of multiple sequences. The model is designed by\nintroducing a three-way weight tensor to capture the multiplicative\ninteractions between side information and sequences. The proposed model builds\non the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid\nBelief Networks (SBNs). The transition matrices are further factored to reduce\nthe number of parameters and improve generalization. When side information is\nnot available, a general framework for semi-supervised learning based on the\nproposed model is constituted, allowing robust sequence classification.\nExperimental results show that the proposed approach achieves state-of-the-art\npredictive and classification performance on sequential data, and has the\ncapacity to synthesize sequences, with controlled style transitioning and\nblending.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 00:17:31 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Song", "Jiaming", ""], ["Gan", "Zhe", ""], ["Carin", "Lawrence", ""]]}, {"id": "1605.06718", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Arthur P. Guillaumin, Jonathan M.\n  Lilly, Jeffrey J. Early", "title": "The De-Biased Whittle Likelihood", "comments": "To appear shortly in Biometrika. Full published version includes\n  extensions of theory to non-Gaussian processes, and new simulation examples\n  with an AR(4) and non-Gaussian process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Whittle likelihood is a widely used and computationally efficient\npseudo-likelihood. However, it is known to produce biased parameter estimates\nfor large classes of models. We propose a method for de-biasing Whittle\nestimates for second-order stationary stochastic processes. The de-biased\nWhittle likelihood can be computed in the same $\\mathcal{O}(n\\log n)$\noperations as the standard approach. We demonstrate the superior performance of\nthe method in simulation studies and in application to a large-scale\noceanographic dataset, where in both cases the de-biased approach reduces bias\nby up to two orders of magnitude, achieving estimates that are close to exact\nmaximum likelihood, at a fraction of the computational cost. We prove that the\nmethod yields estimates that are consistent at an optimal convergence rate of\n$n^{-1/2}$, under weaker assumptions than standard theory, where we do not\nrequire that the power spectral density is continuous in frequency. We describe\nhow the method can be easily combined with standard methods of bias reduction,\nsuch as tapering and differencing, to further reduce bias in parameter\nestimates.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 00:47:52 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 23:38:15 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 15:39:43 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Guillaumin", "Arthur P.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1605.06742", "submitter": "Wenshuo Wang", "authors": "Wenshuo Wang and Junqiang Xi", "title": "A Rapid Pattern-Recognition Method for Driving Types Using\n  Clustering-Based Support Vector Machines", "comments": "6 pages, 9 figures, 2 tables. To be appear in 2016 American Control\n  Conference, Boston, MA, USA, 2016", "journal-ref": "2017 American Control Conference", "doi": "10.1109/ACC.2016.7526495", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapid pattern-recognition approach to characterize driver's\ncurve-negotiating behavior is proposed. To shorten the recognition time and\nimprove the recognition of driving styles, a k-means clustering-based support\nvector machine ( kMC-SVM) method is developed and used for classifying drivers\ninto two types: aggressive and moderate. First, vehicle speed and throttle\nopening are treated as the feature parameters to reflect the driving styles.\nSecond, to discriminate driver curve-negotiating behaviors and reduce the\nnumber of support vectors, the k-means clustering method is used to extract and\ngather the two types of driving data and shorten the recognition time. Then,\nbased on the clustering results, a support vector machine approach is utilized\nto generate the hyperplane for judging and predicting to which types the human\ndriver are subject. Lastly, to verify the validity of the kMC-SVM method, a\ncross-validation experiment is designed and conducted. The research results\nshow that the $ k $MC-SVM is an effective method to classify driving styles\nwith a short time, compared with SVM method.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 06:15:11 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Wang", "Wenshuo", ""], ["Xi", "Junqiang", ""]]}, {"id": "1605.06796", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Zoltan Szabo, Kacper Chwialkowski, Arthur Gretton", "title": "Interpretable Distribution Features with Maximum Testing Power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two semimetrics on probability distributions are proposed, given as the sum\nof differences of expectations of analytic functions evaluated at spatial or\nfrequency locations (i.e, features). The features are chosen so as to maximize\nthe distinguishability of the distributions, by optimizing a lower bound on\ntest power for a statistical test using these features. The result is a\nparsimonious and interpretable indication of how and where two distributions\ndiffer locally. An empirical estimate of the test power criterion converges\nwith increasing sample size, ensuring the quality of the returned features. In\nreal-world benchmarks on high-dimensional text and image data, linear-time\ntests using the proposed semimetrics achieve comparable performance to the\nstate-of-the-art quadratic-time maximum mean discrepancy test, while returning\nhuman-interpretable features that explain the test results.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 14:10:13 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 10:48:05 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Szabo", "Zoltan", ""], ["Chwialkowski", "Kacper", ""], ["Gretton", "Arthur", ""]]}, {"id": "1605.06838", "submitter": "Ridho Rahmadi", "authors": "Ridho Rahmadi, Perry Groot, Marieke HC van Rijn, Jan AJG van den\n  Brand, Marianne Heins, Hans Knoop, Tom Heskes (the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium)", "title": "Causality on Longitudinal Data: Stable Specification Search in\n  Constrained Structural Equation Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical problem in causal modeling is the instability of model structure\nlearning, i.e., small changes in finite data can result in completely different\noptimal models. The present work introduces a novel causal modeling algorithm\nfor longitudinal data, that is robust for finite samples based on recent\nadvances in stability selection using subsampling and selection algorithms. Our\napproach uses exploratory search but allows incorporation of prior knowledge,\ne.g., the absence of a particular causal relationship between two specific\nvariables. We represent causal relationships using structural equation models.\nModels are scored along two objectives: the model fit and the model complexity.\nSince both objectives are often conflicting we apply a multi-objective\nevolutionary algorithm to search for Pareto optimal models. To handle the\ninstability of small finite data samples, we repeatedly subsample the data and\nselect those substructures (from the optimal models) that are both stable and\nparsimonious. These substructures can be visualized through a causal graph. Our\nmore exploratory approach achieves at least comparable performance as, but\noften a significant improvement over state-of-the-art alternative approaches on\na simulated data set with a known ground truth. We also present the results of\nour method on three real-world longitudinal data sets on chronic fatigue\nsyndrome, Alzheimer disease, and chronic kidney disease. The findings obtained\nwith our approach are generally in line with results from more\nhypothesis-driven analyses in earlier studies and suggest some novel\nrelationships that deserve further research.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 19:28:25 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:32:02 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 12:46:28 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Rahmadi", "Ridho", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["Groot", "Perry", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["van Rijn", "Marieke HC", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["Brand", "Jan AJG van den", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["Heins", "Marianne", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["Knoop", "Hans", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"], ["Heskes", "Tom", "", "the Alzheimer's Disease\n  Neuroimaging Initiatives, the MASTERPLAN Study Group, the OPTIMISTIC\n  Consortium"]]}, {"id": "1605.06855", "submitter": "Manuel Gomez Rodriguez", "authors": "Mohammad Reza Karimi and Erfan Tavakoli and Mehrdad Farajtabar and Le\n  Song and Manuel Gomez-Rodriguez", "title": "Smart broadcasting: Do you want to be seen?", "comments": "To appear in Proceedings of the 22nd ACM SIGKDD International\n  Conference on Knowledge Discovery and Data Mining (KDD), San Francisco (CA,\n  USA), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many users in online social networks are constantly trying to gain attention\nfrom their followers by broadcasting posts to them. These broadcasters are\nlikely to gain greater attention if their posts can remain visible for a longer\nperiod of time among their followers' most recent feeds. Then when to post? In\nthis paper, we study the problem of smart broadcasting using the framework of\ntemporal point processes, where we model users feeds and posts as discrete\nevents occurring in continuous time. Based on such continuous-time model, then\nchoosing a broadcasting strategy for a user becomes a problem of designing the\nconditional intensity of her posting events. We derive a novel formula which\nlinks this conditional intensity with the visibility of the user in her\nfollowers' feeds. Furthermore, by exploiting this formula, we develop an\nefficient convex optimization framework for the when-to-post problem. Our\nmethod can find broadcasting strategies that reach a desired visibility level\nwith provable guarantees. We experimented with data gathered from Twitter, and\nshow that our framework can consistently make broadcasters' post more visible\nthan alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 21:18:19 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Karimi", "Mohammad Reza", ""], ["Tavakoli", "Erfan", ""], ["Farajtabar", "Mehrdad", ""], ["Song", "Le", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1605.06886", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Bin Li, Yi Wang, Yang Wang, Fang Chen", "title": "Stochastic Patching Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic partition models tailor a product space into a number of\nrectangular regions such that the data within each region exhibit certain types\nof homogeneity. Due to constraints of partition strategy, existing models may\ncause unnecessary dissections in sparse regions when fitting data in dense\nregions. To alleviate this limitation, we propose a parsimonious partition\nmodel, named Stochastic Patching Process (SPP), to deal with multi-dimensional\narrays. SPP adopts an \"enclosing\" strategy to attach rectangular patches to\ndense regions. SPP is self-consistent such that it can be extended to infinite\narrays. We apply SPP to relational modeling and the experimental results\nvalidate its merit compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 03:43:01 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:26:56 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Fan", "Xuhui", ""], ["Li", "Bin", ""], ["Wang", "Yi", ""], ["Wang", "Yang", ""], ["Chen", "Fang", ""]]}, {"id": "1605.06892", "submitter": "Khanh Hien Le", "authors": "Le Thi Khanh Hien, Cuong V. Nguyen, Huan Xu, Canyi Lu, Jiashi Feng", "title": "Accelerated Randomized Mirror Descent Algorithms For Composite\n  Non-strongly Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the sum of an average function of a\nlarge number of smooth convex components and a general, possibly\nnon-differentiable, convex function. Although many methods have been proposed\nto solve this problem with the assumption that the sum is strongly convex, few\nmethods support the non-strongly convex case. Adding a small quadratic\nregularization is a common devise used to tackle non-strongly convex problems;\nhowever, it may cause loss of sparsity of solutions or weaken the performance\nof the algorithms. Avoiding this devise, we propose an accelerated randomized\nmirror descent method for solving this problem without the strongly convex\nassumption. Our method extends the deterministic accelerated proximal gradient\nmethods of Paul Tseng and can be applied even when proximal points are computed\ninexactly. We also propose a scheme for solving the problem when the component\nfunctions are non-smooth.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 04:21:07 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 02:15:13 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 00:55:24 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 10:23:14 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 16:52:04 GMT"}, {"version": "v6", "created": "Mon, 31 Dec 2018 18:48:15 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Hien", "Le Thi Khanh", ""], ["Nguyen", "Cuong V.", ""], ["Xu", "Huan", ""], ["Lu", "Canyi", ""], ["Feng", "Jiashi", ""]]}, {"id": "1605.06900", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Fast Stochastic Methods for Nonsmooth Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic algorithms for optimizing nonconvex, nonsmooth\nfinite-sum problems, where the nonconvex part is smooth and the nonsmooth part\nis convex. Surprisingly, unlike the smooth case, our knowledge of this\nfundamental problem is very limited. For example, it is not known whether the\nproximal stochastic gradient method with constant minibatch converges to a\nstationary point. To tackle this issue, we develop fast stochastic algorithms\nthat provably converge to a stationary point for constant minibatches.\nFurthermore, using a variant of these algorithms, we show provably faster\nconvergence than batch proximal gradient descent. Finally, we prove global\nlinear convergence rate for an interesting subclass of nonsmooth nonconvex\nfunctions, that subsumes several recent works. This paper builds upon our\nrecent series of papers on fast stochastic methods for smooth nonconvex\noptimization [22, 23], with a novel analysis for nonconvex and nonsmooth\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 05:49:00 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1605.06931", "submitter": "Oliver Cliff", "authors": "Oliver M. Cliff, Mikhail Prokopenko and Robert Fitch", "title": "An Information Criterion for Inferring Coupling in Distributed Dynamical\n  Systems", "comments": null, "journal-ref": "Front. Robot. AI 3(71), 2016", "doi": "10.3389/frobt.2016.00071", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behaviour of many real-world phenomena can be modelled by nonlinear\ndynamical systems whereby a latent system state is observed through a filter.\nWe are interested in interacting subsystems of this form, which we model by a\nset of coupled maps as a synchronous update graph dynamical systems.\nSpecifically, we study the structure learning problem for spatially distributed\ndynamical systems coupled via a directed acyclic graph. Unlike established\nstructure learning procedures that find locally maximum posterior probabilities\nof a network structure containing latent variables, our work exploits the\nproperties of dynamical systems to compute globally optimal approximations of\nthese distributions. We arrive at this result by the use of time delay\nembedding theorems. Taking an information-theoretic perspective, we show that\nthe log-likelihood has an intuitive interpretation in terms of information\ntransfer.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 08:40:58 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 02:45:24 GMT"}, {"version": "v3", "created": "Fri, 11 Nov 2016 06:45:08 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Cliff", "Oliver M.", ""], ["Prokopenko", "Mikhail", ""], ["Fitch", "Robert", ""]]}, {"id": "1605.06950", "submitter": "James Newling", "authors": "James Newling, Fran\\c{c}ois Fleuret", "title": "A Sub-Quadratic Exact Medoid Algorithm", "comments": "Version 2: Added acknowledgements, Version 3: Post-acceptance at\n  AISTATS 2017, Version 4: N-1 -> N denominator correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm, trimed, for obtaining the medoid of a set, that\nis the element of the set which minimises the mean distance to all other\nelements. The algorithm is shown to have, under certain assumptions, expected\nrun time O(N^(3/2)) in R^d where N is the set size, making it the first\nsub-quadratic exact medoid algorithm for d>1. Experiments show that it performs\nvery well on spatial network data, frequently requiring two orders of magnitude\nfewer distance calculations than state-of-the-art approximate algorithms. As an\napplication, we show how trimed can be used as a component in an accelerated\nK-medoids algorithm, and then how it can be relaxed to obtain further\ncomputational gains with only a minor loss in cluster quality.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 09:24:59 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 07:44:29 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 07:55:33 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 18:25:34 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1605.06995", "submitter": "Mijung Park", "authors": "Mijung Park, Jimmy Foulds, Kamalika Chaudhuri, Max Welling", "title": "DP-EM: Differentially Private Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iterative nature of the expectation maximization (EM) algorithm presents\na challenge for privacy-preserving estimation, as each iteration increases the\namount of noise needed. We propose a practical private EM algorithm that\novercomes this challenge using two innovations: (1) a novel moment perturbation\nformulation for differentially private EM (DP-EM), and (2) the use of two\nrecently developed composition methods to bound the privacy \"cost\" of multiple\nEM iterations: the moments accountant (MA) and zero-mean concentrated\ndifferential privacy (zCDP). Both MA and zCDP bound the moment generating\nfunction of the privacy loss random variable and achieve a refined tail bound,\nwhich effectively decrease the amount of additive noise. We present empirical\nresults showing the benefits of our approach, as well as similar performance\nbetween these two composition methods in the DP-EM setting for Gaussian mixture\nmodels. Our approach can be readily extended to many iterative learning\nalgorithms, opening up various exciting future directions.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 12:36:55 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:17:01 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Park", "Mijung", ""], ["Foulds", "Jimmy", ""], ["Chaudhuri", "Kamalika", ""], ["Welling", "Max", ""]]}, {"id": "1605.07018", "submitter": "Alon Cohen", "authors": "Alon Cohen, Tamir Hazan, Tomer Koren", "title": "Online Learning with Feedback Graphs Without the Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online learning framework introduced by Mannor and Shamir (2011)\nin which the feedback is specified by a graph, in a setting where the graph may\nvary from round to round and is \\emph{never fully revealed} to the learner. We\nshow a large gap between the adversarial and the stochastic cases. In the\nadversarial case, we prove that even for dense feedback graphs, the learner\ncannot improve upon a trivial regret bound obtained by ignoring any additional\nfeedback besides her own loss. In contrast, in the stochastic case we give an\nalgorithm that achieves $\\widetilde \\Theta(\\sqrt{\\alpha T})$ regret over $T$\nrounds, provided that the independence numbers of the hidden feedback graphs\nare at most $\\alpha$. We also extend our results to a more general feedback\nmodel, in which the learner does not necessarily observe her own loss, and show\nthat, even in simple cases, concealing the feedback graphs might render a\nlearnable problem unlearnable.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 14:07:43 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Cohen", "Alon", ""], ["Hazan", "Tamir", ""], ["Koren", "Tomer", ""]]}, {"id": "1605.07025", "submitter": "Hyunjik Kim", "authors": "Hyunjik Kim, Xiaoyu Lu, Seth Flaxman, Yee Whye Teh", "title": "Collaborative Filtering with Side Information: a Gaussian Process\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of collaborative filtering (CF) with side information,\nthrough the lens of Gaussian Process (GP) regression. Driven by the idea of\nusing the kernel to explicitly model user-item similarities, we formulate the\nGP in a way that allows the incorporation of low-rank matrix factorisation,\narriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP\ngeneralises classical Bayesian matrix factorisation models, and goes beyond\nthem to give a natural and elegant method for incorporating side information,\ngiving enhanced predictive performance for CF problems. Moreover we show that\nit is a novel model for regression, especially well-suited to grid-structured\ndata and problems where the dependence on covariates is close to being\nseparable.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 14:19:02 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 16:19:46 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 11:18:56 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Kim", "Hyunjik", ""], ["Lu", "Xiaoyu", ""], ["Flaxman", "Seth", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1605.07051", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, John Lafferty", "title": "Convergence Analysis for Rectangular Matrix Completion Using\n  Burer-Monteiro Factorization and Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the rectangular matrix completion problem by lifting the unknown\nmatrix to a positive semidefinite matrix in higher dimension, and optimizing a\nnonconvex objective over the semidefinite factor using a simple gradient\ndescent scheme. With $O( \\mu r^2 \\kappa^2 n \\max(\\mu, \\log n))$ random\nobservations of a $n_1 \\times n_2$ $\\mu$-incoherent matrix of rank $r$ and\ncondition number $\\kappa$, where $n = \\max(n_1, n_2)$, the algorithm linearly\nconverges to the global optimum with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 15:14:09 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 03:37:12 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Zheng", "Qinqing", ""], ["Lafferty", "John", ""]]}, {"id": "1605.07057", "submitter": "Xiaoran Yan", "authors": "Xiaoran Yan", "title": "Bayesian Model Selection of Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in analyzing networks is partitioning them into modules or\ncommunities. One of the best tools for this is the stochastic block model,\nwhich clusters vertices into blocks with statistically homogeneous pattern of\nlinks. Despite its flexibility and popularity, there has been a lack of\nprincipled statistical model selection criteria for the stochastic block model.\nHere we propose a Bayesian framework for choosing the number of blocks as well\nas comparing it to the more elaborate degree- corrected block models,\nultimately leading to a universal model selection framework capable of\ncomparing multiple modeling combinations. We will also investigate its\nconnection to the minimum description length principle.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 15:16:51 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Yan", "Xiaoran", ""]]}, {"id": "1605.07066", "submitter": "Thang Bui", "authors": "Thang D. Bui, Josiah Yan, Richard E. Turner", "title": "A Unifying Framework for Gaussian Process Pseudo-Point Approximations\n  using Power Expectation Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are flexible distributions over functions that\nenable high-level assumptions about unknown functions to be encoded in a\nparsimonious, flexible and general way. Although elegant, the application of\nGPs is limited by computational and analytical intractabilities that arise when\ndata are sufficiently numerous or when employing non-Gaussian models.\nConsequently, a wealth of GP approximation schemes have been developed over the\nlast 15 years to address these key limitations. Many of these schemes employ a\nsmall set of pseudo data points to summarise the actual data. In this paper, we\ndevelop a new pseudo-point approximation framework using Power Expectation\nPropagation (Power EP) that unifies a large number of these pseudo-point\napproximations. Unlike much of the previous venerable work in this area, the\nnew framework is built on standard methods for approximate inference\n(variational free-energy, EP and Power EP methods) rather than employing\napproximations to the probabilistic generative model itself. In this way, all\nof approximation is performed at `inference time' rather than at `modelling\ntime' resolving awkward philosophical and empirical questions that trouble\nprevious approaches. Crucially, we demonstrate that the new framework includes\nnew pseudo-point approximation methods that outperform current approaches on\nregression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 15:53:51 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 16:26:54 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 14:58:34 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Bui", "Thang D.", ""], ["Yan", "Josiah", ""], ["Turner", "Richard E.", ""]]}, {"id": "1605.07078", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti", "title": "Learning Sensor Multiplexing Design through Back-propagation", "comments": "NIPS 2016. Project page at http://www.ttic.edu/chakrabarti/learncfa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on many imaging and vision tasks has been driven by the use\nof deep feed-forward neural networks, which are trained by propagating\ngradients of a loss defined on the final output, back through the network up to\nthe first layer that operates directly on the image. We propose\nback-propagating one step further---to learn camera sensor designs jointly with\nnetworks that carry out inference on the images they capture. In this paper, we\nspecifically consider the design and inference problems in a typical color\ncamera---where the sensor is able to measure only one color channel at each\npixel location, and computational inference is required to reconstruct a full\ncolor image. We learn the camera sensor's color multiplexing pattern by\nencoding it as layer whose learnable weights determine which color channel,\nfrom among a fixed set, will be measured at each location. These weights are\njointly trained with those of a reconstruction network that operates on the\ncorresponding sensor measurements to produce a full color image. Our network\nachieves significant improvements in accuracy over the traditional Bayer\npattern used in most color cameras. It automatically learns to employ a sparse\ncolor measurement approach similar to that of a recent design, and moreover,\nimproves upon that design by learning an optimal layout for these measurements.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:26:59 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 18:30:57 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Chakrabarti", "Ayan", ""]]}, {"id": "1605.07079", "submitter": "Aaron Klein", "authors": "Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank\n  Hutter", "title": "Fast Bayesian Optimization of Machine Learning Hyperparameters on Large\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has become a successful tool for hyperparameter\noptimization of machine learning algorithms, such as support vector machines or\ndeep neural networks. Despite its success, for large datasets, training and\nvalidating a single configuration often takes hours, days, or even weeks, which\nlimits the achievable performance. To accelerate hyperparameter optimization,\nwe propose a generative model for the validation error as a function of\ntraining set size, which is learned during the optimization process and allows\nexploration of preliminary configurations on small subsets, by extrapolating to\nthe full dataset. We construct a Bayesian optimization procedure, dubbed\nFabolas, which models loss and training time as a function of dataset size and\nautomatically trades off high information gain about the global optimum against\ncomputational cost. Experiments optimizing support vector machines and deep\nneural networks show that Fabolas often finds high-quality solutions 10 to 100\ntimes faster than other state-of-the-art Bayesian optimization methods or the\nrecently proposed bandit strategy Hyperband.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:29:51 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 14:48:54 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Bartels", "Simon", ""], ["Hennig", "Philipp", ""], ["Hutter", "Frank", ""]]}, {"id": "1605.07094", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Tatiana Fomina, Bernhard Sch\\\"olkopf, Moritz\n  Grosse-Wentrup", "title": "A note on the expected minimum error probability in equientropic\n  channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the channel capacity reflects a theoretical upper bound on the\nachievable information transmission rate in the limit of infinitely many bits,\nit does not characterise the information transfer of a given encoding routine\nwith finitely many bits. In this note, we characterise the quality of a code\n(i. e. a given encoding routine) by an upper bound on the expected minimum\nerror probability that can be achieved when using this code. We show that for\nequientropic channels this upper bound is minimal for codes with maximal\nmarginal entropy. As an instructive example we show for the additive white\nGaussian noise (AWGN) channel that random coding---also a capacity achieving\ncode---indeed maximises the marginal entropy in the limit of infinite messages.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 17:04:57 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 16:55:42 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Fomina", "Tatiana", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1605.07110", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi", "title": "Deep Learning without Poor Local Minima", "comments": "In NIPS 2016. Selected for NIPS oral presentation (top 2%\n  submissions). ---- The final NIPS 2016 version: the results remain the same", "journal-ref": null, "doi": null, "report-no": "Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2016-005", "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove a conjecture published in 1989 and also partially\naddress an open problem announced at the Conference on Learning Theory (COLT)\n2015. With no unrealistic assumption, we first prove the following statements\nfor the squared loss function of deep linear neural networks with any depth and\nany widths: 1) the function is non-convex and non-concave, 2) every local\nminimum is a global minimum, 3) every critical point that is not a global\nminimum is a saddle point, and 4) there exist \"bad\" saddle points (where the\nHessian has no negative eigenvalue) for the deeper networks (with more than\nthree layers), whereas there is no bad saddle point for the shallow networks\n(with three layers). Moreover, for deep nonlinear neural networks, we prove the\nsame four statements via a reduction to a deep linear model under the\nindependence assumption adopted from recent work. As a result, we present an\ninstance, for which we can answer the following question: how difficult is it\nto directly train a deep model in theory? It is more difficult than the\nclassical machine learning models (because of the non-convexity), but not too\ndifficult (because of the nonexistence of poor local minima). Furthermore, the\nmathematically proven existence of bad saddle points for deeper models would\nsuggest a possible open problem. We note that even though we have advanced the\ntheoretical foundations of deep learning and non-convex optimization, there is\nstill a gap between theory and practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 17:34:20 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 14:26:22 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 22:47:50 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Kawaguchi", "Kenji", ""]]}, {"id": "1605.07127", "submitter": "Stefan Depeweg", "authors": "Stefan Depeweg, Jos\\'e Miguel Hern\\'andez-Lobato, Finale Doshi-Velez,\n  Steffen Udluft", "title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for model-based reinforcement learning that combines\nBayesian neural networks (BNNs) with random roll-outs and stochastic\noptimization for policy learning. The BNNs are trained by minimizing\n$\\alpha$-divergences, allowing us to capture complicated statistical patterns\nin the transition dynamics, e.g. multi-modality and heteroskedasticity, which\nare usually missed by other common modeling approaches. We illustrate the\nperformance of our method by solving a challenging benchmark where model-based\napproaches usually fail and by obtaining promising results in a real-world\nscenario for controlling a gas turbine.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:28:15 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 07:23:20 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 01:07:15 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Depeweg", "Stefan", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Doshi-Velez", "Finale", ""], ["Udluft", "Steffen", ""]]}, {"id": "1605.07129", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker", "title": "Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed\n  entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix has attracted a lot of attention of the\nstatistical research community over the years, partially due to important\napplications such as Principal Component Analysis. However, frequently used\nempirical covariance estimator (and its modifications) is very sensitive to\noutliers in the data. As P. J. Huber wrote in 1964, \"...This raises a question\nwhich could have been asked already by Gauss, but which was, as far as I know,\nonly raised a few years ago (notably by Tukey): what happens if the true\ndistribution deviates slightly from the assumed normal one? As is now well\nknown, the sample mean then may have a catastrophically bad performance...\"\nMotivated by this question, we develop a new estimator of the (element-wise)\nmean of a random matrix, which includes covariance estimation problem as a\nspecial case. Assuming that the entries of a matrix possess only finite second\nmoment, this new estimator admits sub-Gaussian or sub-exponential concentration\naround the unknown mean in the operator norm. We will explain the key ideas\nbehind our construction, as well as applications to covariance estimation and\nmatrix completion problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:36:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:52:56 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 16:53:04 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 21:32:07 GMT"}, {"version": "v5", "created": "Sun, 17 Jun 2018 22:26:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Minsker", "Stanislav", ""]]}, {"id": "1605.07139", "submitter": "Matthew Joseph", "authors": "Matthew Joseph and Michael Kearns and Jamie Morgenstern and Aaron Roth", "title": "Fairness in Learning: Classic and Contextual Bandits", "comments": "A condensed version of this work appears in the 30th Annual\n  Conference on Neural Information Processing Systems (NIPS), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the study of fairness in multi-armed bandit problems. Our\nfairness definition can be interpreted as demanding that given a pool of\napplicants (say, for college admission or mortgages), a worse applicant is\nnever favored over a better one, despite a learning algorithm's uncertainty\nover the true payoffs. We prove results of two types.\n  First, in the important special case of the classic stochastic bandits\nproblem (i.e., in which there are no contexts), we provide a provably fair\nalgorithm based on \"chained\" confidence intervals, and provide a cumulative\nregret bound with a cubic dependence on the number of arms. We further show\nthat any fair algorithm must have such a dependence. When combined with regret\nbounds for standard non-fair algorithms such as UCB, this proves a strong\nseparation between fair and unfair learning, which extends to the general\ncontextual case.\n  In the general contextual case, we prove a tight connection between fairness\nand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class\nof functions can be transformed into a provably fair contextual bandit\nalgorithm, and conversely any fair contextual bandit algorithm can be\ntransformed into a KWIK learning algorithm. This tight connection allows us to\nprovide a provably fair algorithm for the linear contextual bandit problem with\na polynomial dependence on the dimension, and to show (for a different class of\nfunctions) a worst-case exponential gap in regret between fair and non-fair\nlearning algorithms\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:58:24 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:49:05 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Joseph", "Matthew", ""], ["Kearns", "Michael", ""], ["Morgenstern", "Jamie", ""], ["Roth", "Aaron", ""]]}, {"id": "1605.07144", "submitter": "Sebastian Tschiatschek", "authors": "Adish Singla, Sebastian Tschiatschek, Andreas Krause", "title": "Actively Learning Hemimetrics with Applications to Eliciting User\n  Preferences", "comments": "Extended version of ICML'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application of eliciting users' preferences, we investigate\nthe problem of learning hemimetrics, i.e., pairwise distances among a set of\n$n$ items that satisfy triangle inequalities and non-negativity constraints. In\nour application, the (asymmetric) distances quantify private costs a user\nincurs when substituting one item by another. We aim to learn these distances\n(costs) by asking the users whether they are willing to switch from one item to\nanother for a given incentive offer. Without exploiting structural constraints\nof the hemimetric polytope, learning the distances between each pair of items\nrequires $\\Theta(n^2)$ queries. We propose an active learning algorithm that\nsubstantially reduces this sample complexity by exploiting the structural\nconstraints on the version space of hemimetrics. Our proposed algorithm\nachieves provably-optimal sample complexity for various instances of the task.\nFor example, when the items are embedded into $K$ tight clusters, the sample\ncomplexity of our algorithm reduces to $O(n K)$. Extensive experiments on a\nrestaurant recommendation data set support the conclusions of our theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:21:35 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 17:45:26 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Singla", "Adish", ""], ["Tschiatschek", "Sebastian", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.07145", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Hung Q. Ngo, Nils Napp, Venu Govindaraju", "title": "On Optimality Conditions for Auto-Encoder Signal Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Encoders are unsupervised models that aim to learn patterns from\nobserved data by minimizing a reconstruction cost. The useful representations\nlearned are often found to be sparse and distributed. On the other hand,\ncompressed sensing and sparse coding assume a data generating process, where\nthe observed data is generated from some true latent signal source, and try to\nrecover the corresponding signal from measurements. Looking at auto-encoders\nfrom this \\textit{signal recovery perspective} enables us to have a more\ncoherent view of these techniques. In this paper, in particular, we show that\nthe \\textit{true} hidden representation can be approximately recovered if the\nweight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the\nbias vectors takes the value (approximately) equal to the negative of the data\nmean. The recovery also becomes more and more accurate as the sparsity in\nhidden signals increases. Additionally, we empirically demonstrate that\nauto-encoders are capable of recovering the data generating dictionary when\nonly data samples are given.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:21:53 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 16:25:15 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Ngo", "Hung Q.", ""], ["Napp", "Nils", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1605.07156", "submitter": "Sasha Targ", "authors": "Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye", "title": "Genetic Architect: Discovering Genomic Structure with Learned Neural\n  Architectures", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each human genome is a 3 billion base pair set of encoding instructions.\nDecoding the genome using deep learning fundamentally differs from most tasks,\nas we do not know the full structure of the data and therefore cannot design\narchitectures to suit it. As such, architectures that fit the structure of\ngenomics should be learned not prescribed. Here, we develop a novel search\nalgorithm, applicable across domains, that discovers an optimal architecture\nwhich simultaneously learns general genomic patterns and identifies the most\nimportant sequence motifs in predicting functional genomic outcomes. The\narchitectures we find using this algorithm succeed at using only RNA expression\ndata to predict gene regulatory structure, learn human-interpretable\nvisualizations of key sequence motifs, and surpass state-of-the-art results on\nbenchmark genomics challenges.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:43:08 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Deming", "Laura", ""], ["Targ", "Sasha", ""], ["Sauder", "Nate", ""], ["Almeida", "Diogo", ""], ["Ye", "Chun Jimmie", ""]]}, {"id": "1605.07174", "submitter": "Daniel Romero", "authors": "Daniel Romero, Meng Ma, Georgios B. Giannakis", "title": "Kernel-based Reconstruction of Graph Signals", "comments": "Submitted May 2016", "journal-ref": null, "doi": "10.1109/TSP.2016.2620116", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of applications in engineering, social sciences, physics, and\nbiology involve inference over networks. In this context, graph signals are\nwidely encountered as descriptors of vertex attributes or features in\ngraph-structured data. Estimating such signals in all vertices given noisy\nobservations of their values on a subset of vertices has been extensively\nanalyzed in the literature of signal processing on graphs (SPoG). This paper\nadvocates kernel regression as a framework generalizing popular SPoG modeling\nand reconstruction and expanding their capabilities. Formulating signal\nreconstruction as a regression task on reproducing kernel Hilbert spaces of\ngraph signals permeates benefits from statistical learning, offers fresh\ninsights, and allows for estimators to leverage richer forms of prior\ninformation than existing alternatives. A number of SPoG notions such as\nbandlimitedness, graph filters, and the graph Fourier transform are naturally\naccommodated in the kernel framework. Additionally, this paper capitalizes on\nthe so-called representer theorem to devise simpler versions of existing\nThikhonov regularized estimators, and offers a novel probabilistic\ninterpretation of kernel methods on graphs based on graphical models. Motivated\nby the challenges of selecting the bandwidth parameter in SPoG estimators or\nthe kernel map in kernel-based methods, the present paper further proposes two\nmulti-kernel approaches with complementary strengths. Whereas the first enables\nestimation of the unknown bandwidth of bandlimited signals, the second allows\nfor efficient graph filter selection. Numerical tests with synthetic as well as\nreal data demonstrate the merits of the proposed methods relative to\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:47:18 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Romero", "Daniel", ""], ["Ma", "Meng", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1605.07221", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Behnam Neyshabur, Nathan Srebro", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there are no spurious local minima in the non-convex factorized\nparametrization of low-rank matrix recovery from incoherent linear\nmeasurements. With noisy measurements we show all local minima are very close\nto a global optimum. Together with a curvature bound at saddle points, this\nyields a polynomial time global convergence guarantee for stochastic gradient\ndescent {\\em from random initialization}.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 22:05:42 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 00:54:17 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1605.07252", "submitter": "Marc Vuffray", "authors": "Marc Vuffray, Sidhant Misra, Andrey Y. Lokhov and Michael Chertkov", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising\n  Models", "comments": "To be published in Advances in Neural Information Processing Systems\n  30", "journal-ref": "Advances in Neural Information Processing Systems, 2595--2603,\n  2016", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the underlying graph of an unknown Ising\nmodel on p spins from a collection of i.i.d. samples generated from the model.\nWe suggest a new estimator that is computationally efficient and requires a\nnumber of samples that is near-optimal with respect to previously established\ninformation-theoretic lower-bound. Our statistical estimator has a physical\ninterpretation in terms of \"interaction screening\". The estimator is consistent\nand is efficiently implemented using convex optimization. We prove that with\nappropriate regularization, the estimator recovers the underlying graph using a\nnumber of samples that is logarithmic in the system size p and exponential in\nthe maximum coupling-intensity and maximum node-degree.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 01:36:48 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 03:00:29 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 13:32:25 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""], ["Lokhov", "Andrey Y.", ""], ["Chertkov", "Michael", ""]]}, {"id": "1605.07254", "submitter": "Motonobu Kanagawa", "authors": "Motonobu Kanagawa, Bharath K. Sriperumbudur, Kenji Fukumizu", "title": "Convergence guarantees for kernel-based quadrature rules in misspecified\n  settings", "comments": "To appear at NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based quadrature rules are becoming important in machine learning and\nstatistics, as they achieve super-$\\sqrt{n}$ convergence rates in numerical\nintegration, and thus provide alternatives to Monte Carlo integration in\nchallenging settings where integrands are expensive to evaluate or where\nintegrands are high dimensional. These rules are based on the assumption that\nthe integrand has a certain degree of smoothness, which is expressed as that\nthe integrand belongs to a certain reproducing kernel Hilbert space (RKHS).\nHowever, this assumption can be violated in practice (e.g., when the integrand\nis a black box function), and no general theory has been established for the\nconvergence of kernel quadratures in such misspecified settings. Our\ncontribution is in proving that kernel quadratures can be consistent even when\nthe integrand does not belong to the assumed RKHS, i.e., when the integrand is\nless smooth than assumed. Specifically, we derive convergence rates that depend\non the (unknown) lesser smoothness of the integrand, where the degree of\nsmoothness is expressed via powers of RKHSs or via Sobolev spaces.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 01:41:25 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 06:37:58 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Kanagawa", "Motonobu", ""], ["Sriperumbudur", "Bharath K.", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1605.07272", "submitter": "Tengyu Ma", "authors": "Rong Ge, Jason D. Lee, Tengyu Ma", "title": "Matrix Completion has No Spurious Local Minimum", "comments": "NIPS'16 best student paper. fixed Theorem 2.3 in preliminary section\n  in the previous version. The results are not affected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a basic machine learning problem that has wide\napplications, especially in collaborative filtering and recommender systems.\nSimple non-convex optimization algorithms are popular and effective in\npractice. Despite recent progress in proving various non-convex algorithms\nconverge from a good initial point, it remains unclear why random or arbitrary\ninitialization suffices in practice. We prove that the commonly used non-convex\nobjective function for \\textit{positive semidefinite} matrix completion has no\nspurious local minima --- all local minima must also be global. Therefore, many\npopular optimization algorithms such as (stochastic) gradient descent can\nprovably solve positive semidefinite matrix completion with \\textit{arbitrary}\ninitialization in polynomial time. The result can be generalized to the setting\nwhen the observed entries contain noise. We believe that our main proof\nstrategy can be useful for understanding geometric properties of other\nstatistical problems involving partial or noisy observations.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:53:27 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 19:58:48 GMT"}, {"version": "v3", "created": "Sun, 29 Jan 2017 18:45:48 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 05:20:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Jason D.", ""], ["Ma", "Tengyu", ""]]}, {"id": "1605.07332", "submitter": "Matthew Chalk", "authors": "Matthew Chalk, Olivier Marre, Gasper Tkacik", "title": "Relevant sparse codes with variational information bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is desirable to extract only the relevant aspects of\ndata. A principled way to do this is the information bottleneck (IB) method,\nwhere one seeks a code that maximizes information about a 'relevance' variable,\nY, while constraining the information encoded about the original data, X.\nUnfortunately however, the IB method is computationally demanding when data are\nhigh-dimensional and/or non-gaussian. Here we propose an approximate\nvariational scheme for maximizing a lower bound on the IB objective, analogous\nto variational EM. Using this method, we derive an IB algorithm to recover\nfeatures that are both relevant and sparse. Finally, we demonstrate how\nkernelized versions of the algorithm can be used to address a broad range of\nproblems with non-linear relation between X and Y.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 08:16:54 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 08:59:02 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Chalk", "Matthew", ""], ["Marre", "Olivier", ""], ["Tkacik", "Gasper", ""]]}, {"id": "1605.07358", "submitter": "Xing Sun", "authors": "Xing Sun, Nelson H.C. Yung, Edmund Y. Lam and Hayden K.-H. So", "title": "Consistency Analysis for the Doubly Stochastic Dirichlet Process", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report proves components consistency for the Doubly Stochastic\nDirichlet Process with exponential convergence of posterior probability. We\nalso present the fundamental properties for DSDP as well as inference\nalgorithms. Simulation toy experiment and real-world experiment results for\nsingle and multi-cluster also support the consistency proof. This report is\nalso a support document for the paper \"Computationally Efficient Hyperspectral\nData Learning Based on the Doubly Stochastic Dirichlet Process\".\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:13:19 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Sun", "Xing", ""], ["Yung", "Nelson H. C.", ""], ["Lam", "Edmund Y.", ""], ["So", "Hayden K. -H.", ""]]}, {"id": "1605.07367", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra", "title": "Riemannian stochastic variance reduced gradient on Grassmann manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large, but finite, number of loss functions. In\nthis paper, we propose a novel Riemannian extension of the Euclidean stochastic\nvariance reduced gradient algorithm (R-SVRG) to a compact manifold search\nspace. To this end, we show the developments on the Grassmann manifold. The key\nchallenges of averaging, addition, and subtraction of multiple gradients are\naddressed with notions like logarithm mapping and parallel translation of\nvectors on the Grassmann manifold. We present a global convergence analysis of\nthe proposed algorithm with decay step-sizes and a local convergence rate\nanalysis under fixed step-size with some natural assumptions. The proposed\nalgorithm is applied on a number of problems on the Grassmann manifold like\nprincipal components analysis, low-rank matrix completion, and the Karcher mean\ncomputation. In all these cases, the proposed algorithm outperforms the\nstandard Riemannian stochastic gradient descent algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:36:32 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 12:55:11 GMT"}, {"version": "v3", "created": "Sun, 9 Apr 2017 06:17:52 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Sato", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1605.07371", "submitter": "Jan Humplik", "authors": "Jan Humplik, Ga\\v{s}per Tka\\v{c}ik", "title": "Semiparametric energy-based probabilistic models", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models can be defined by an energy function, where the\nprobability of each state is proportional to the exponential of the state's\nnegative energy. This paper considers a generalization of energy-based models\nin which the probability of a state is proportional to an arbitrary positive,\nstrictly decreasing, and twice differentiable function of the state's energy.\nThe precise shape of the nonlinear map from energies to unnormalized\nprobabilities has to be learned from data together with the parameters of the\nenergy function. As a case study we show that the above generalization of a\nfully visible Boltzmann machine yields an accurate model of neural activity of\nretinal ganglion cells. We attribute this success to the model's ability to\neasily capture distributions whose probabilities span a large dynamic range, a\npossible consequence of latent variables that globally couple the system.\nSimilar features have recently been observed in many datasets, suggesting that\nour new method has wide applicability.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:51:13 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Humplik", "Jan", ""], ["Tka\u010dik", "Ga\u0161per", ""]]}, {"id": "1605.07416", "submitter": "Sebastien Gerchinovitz", "authors": "S\\'ebastien Gerchinovitz (IMT, AOC), Tor Lattimore", "title": "Refined Lower Bounds for Adversarial Bandits", "comments": null, "journal-ref": "D. D. Lee; M. Sugiyama; U. V. Luxburg; I. Guyon; R. Garnett. NIPS\n  2016, Dec 2016, Barcelona, Spain. Curran Associates, Inc., pp.1198--1206,\n  Advances in Neural Information Processing Systems 29", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new lower bounds on the regret that must be suffered by\nadversarial bandit algorithms. The new results show that recent upper bounds\nthat either (a) hold with high-probability or (b) depend on the total lossof\nthe best arm or (c) depend on the quadratic variation of the losses, are close\nto tight. Besides this we prove two impossibility results. First, the existence\nof a single arm that is optimal in every round cannot improve the regret in the\nworst case. Second, the regret cannot scale with the effective range of the\nlosses. In contrast, both results are possible in the full-information setting.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:36:47 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 13:48:10 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gerchinovitz", "S\u00e9bastien", "", "IMT, AOC"], ["Lattimore", "Tor", ""]]}, {"id": "1605.07422", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman, Carsten Eickhoff and Maarten de Rijke", "title": "Computing Web-scale Topic Models using an Asynchronous Parameter Server", "comments": "To appear in SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3084135", "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:40:29 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 08:43:56 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 22:37:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Jagerman", "Rolf", ""], ["Eickhoff", "Carsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1605.07427", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald\n  Tesauro, Yoshua Bengio", "title": "Hierarchical Memory Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory networks are neural networks with an explicit memory component that\ncan be both read and written to by the network. The memory is often addressed\nin a soft way using a softmax function, making end-to-end training with\nbackpropagation possible. However, this is not computationally scalable for\napplications which require the network to read from extremely large memories.\nOn the other hand, it is well known that hard attention mechanisms based on\nreinforcement learning are challenging to train successfully. In this paper, we\nexplore a form of hierarchical memory network, which can be considered as a\nhybrid between hard and soft attention memory networks. The memory is organized\nin a hierarchical structure such that reading from it is done with less\ncomputation than soft attention over a flat memory, while also being easier to\ntrain than hard attention over a flat memory. Specifically, we propose to\nincorporate Maximum Inner Product Search (MIPS) in the training and inference\nprocedures for our hierarchical memory network. We explore the use of various\nstate-of-the art approximate MIPS techniques and report results on\nSimpleQuestions, a challenging large scale factoid question answering task.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:48:19 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Chandar", "Sarath", ""], ["Ahn", "Sungjin", ""], ["Larochelle", "Hugo", ""], ["Vincent", "Pascal", ""], ["Tesauro", "Gerald", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1605.07496", "submitter": "Supratik Paul", "authors": "Supratik Paul, Konstantinos Chatzilygeroudis, Kamil Ciosek,\n  Jean-Baptiste Mouret, Michael A. Osborne, Shimon Whiteson", "title": "Alternating Optimisation and Quadrature for Robust Control", "comments": "To appear in AAAI 2018. Video of policy learnt in simulation deployed\n  on a real hexapod see https://youtu.be/ME90xtIPsKk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation has been successfully applied to a variety of\nreinforcement learning problems. However, the traditional approach for learning\noptimal policies in simulators does not utilise the opportunity to improve\nlearning by adjusting certain environment variables: state features that are\nunobservable and randomly determined by the environment in a physical setting\nbut are controllable in a simulator. This paper considers the problem of\nfinding a robust policy while taking into account the impact of environment\nvariables. We present Alternating Optimisation and Quadrature (ALOQ), which\nuses Bayesian optimisation and Bayesian quadrature to address such settings.\nALOQ is robust to the presence of significant rare events, which may not be\nobservable under random sampling, but play a substantial role in determining\nthe optimal policy. Experimental results across different domains show that\nALOQ can learn more efficiently and robustly than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 15:15:57 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 12:39:46 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 10:12:32 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Paul", "Supratik", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Ciosek", "Kamil", ""], ["Mouret", "Jean-Baptiste", ""], ["Osborne", "Michael A.", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1605.07511", "submitter": "Mijung Park", "authors": "Mijung Park, Max Welling", "title": "A note on privacy preserving iteratively reweighted least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iteratively reweighted least squares (IRLS) is a widely-used method in\nmachine learning to estimate the parameters in the generalised linear models.\nIn particular, IRLS for L1 minimisation under the linear model provides a\nclosed-form solution in each step, which is a simple multiplication between the\ninverse of the weighted second moment matrix and the weighted first moment\nvector. When dealing with privacy sensitive data, however, developing a privacy\npreserving IRLS algorithm faces two challenges. First, due to the inversion of\nthe second moment matrix, the usual sensitivity analysis in differential\nprivacy incorporating a single datapoint perturbation gets complicated and\noften requires unrealistic assumptions. Second, due to its iterative nature, a\nsignificant cumulative privacy loss occurs. However, adding a high level of\nnoise to compensate for the privacy loss hinders from getting accurate\nestimates. Here, we develop a practical algorithm that overcomes these\nchallenges and outputs privatised and accurate IRLS solutions. In our method,\nwe analyse the sensitivity of each moments separately and treat the matrix\ninversion and multiplication as a post-processing step, which simplifies the\nsensitivity analysis. Furthermore, we apply the {\\it{concentrated differential\nprivacy}} formalism, a more relaxed version of differential privacy, which\nrequires adding a significantly less amount of noise for the same level of\nprivacy guarantee, compared to the conventional and advanced compositions of\ndifferentially private mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 15:50:26 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Park", "Mijung", ""], ["Welling", "Max", ""]]}, {"id": "1605.07541", "submitter": "Peter Wittek", "authors": "Alex Monr\\`as, Gael Sent\\'is, Peter Wittek", "title": "Inductive supervised quantum learning", "comments": "6+10 pages", "journal-ref": "Phys. Rev. Lett. 118, 190503 (2017)", "doi": "10.1103/PhysRevLett.118.190503", "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning, an inductive learning algorithm extracts general\nrules from observed training instances, then the rules are applied to test\ninstances. We show that this splitting of training and application arises\nnaturally, in the classical setting, from a simple independence requirement\nwith a physical interpretation of being non-signalling. Thus, two seemingly\ndifferent definitions of inductive learning happen to coincide. This follows\nfrom the properties of classical information that break down in the quantum\nsetup. We prove a quantum de Finetti theorem for quantum channels, which shows\nthat in the quantum case, the equivalence holds in the asymptotic setting, that\nis, for large number of test instances. This reveals a natural analogy between\nclassical learning protocols and their quantum counterparts, justifying a\nsimilar treatment, and allowing to inquire about standard elements in\ncomputational learning theory, such as structural risk minimization and sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 16:56:46 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 10:48:23 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Monr\u00e0s", "Alex", ""], ["Sent\u00eds", "Gael", ""], ["Wittek", "Peter", ""]]}, {"id": "1605.07571", "submitter": "Marco Fraccaro", "authors": "Marco Fraccaro, S{\\o}ren Kaae S{\\o}nderby, Ulrich Paquet, Ole Winther", "title": "Sequential Neural Models with Stochastic Layers", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we efficiently propagate uncertainty in a latent state representation\nwith recurrent neural networks? This paper introduces stochastic recurrent\nneural networks which glue a deterministic recurrent neural network and a state\nspace model together to form a stochastic and sequential neural generative\nmodel. The clear separation of deterministic and stochastic layers allows a\nstructured variational inference network to track the factorization of the\nmodel's posterior distribution. By retaining both the nonlinear recursive\nstructure of a recurrent neural network and averaging over the uncertainty in a\nlatent path, like a state space model, we improve the state of the art results\non the Blizzard and TIMIT speech modeling data sets by a large margin, while\nachieving comparable performances to competing methods on polyphonic music\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 18:23:58 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 18:04:41 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fraccaro", "Marco", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Paquet", "Ulrich", ""], ["Winther", "Ole", ""]]}, {"id": "1605.07583", "submitter": "Cameron Musco", "authors": "Cameron Musco and Christopher Musco", "title": "Recursive Sampling for the Nystr\\\"om Method", "comments": "To appear, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for kernel Nystr\\\"om approximation that runs in\n*linear time in the number of training points* and is provably accurate for all\nkernel matrices, without dependence on regularity or incoherence conditions.\nThe algorithm projects the kernel onto a set of $s$ landmark points sampled by\ntheir *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and\n$O(ns^2)$ additional runtime. While leverage score sampling has long been known\nto give strong theoretical guarantees for Nystr\\\"om approximation, by employing\na fast recursive sampling scheme, our algorithm is the first to make the\napproach scalable. Empirically we show that it finds more accurate, lower rank\nkernel approximations in less time than popular techniques such as uniformly\nsampled Nystr\\\"om approximation and the random Fourier features method.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 18:56:57 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 19:48:44 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 16:37:17 GMT"}, {"version": "v4", "created": "Thu, 16 Mar 2017 17:58:14 GMT"}, {"version": "v5", "created": "Fri, 3 Nov 2017 14:40:15 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1605.07588", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco", "title": "A Consistent Regularization Approach for Structured Prediction", "comments": "39 pages, 2 Tables, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a regularization approach for structured prediction\nproblems. We characterize a large class of loss functions that allows to\nnaturally embed structured outputs in a linear space. We exploit this fact to\ndesign learning algorithms using a surrogate loss approach and regularization\ntechniques. We prove universal consistency and finite sample bounds\ncharacterizing the generalization properties of the proposed methods.\nExperimental results are provided to demonstrate the practical usefulness of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 19:06:43 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:55:37 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 09:36:05 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1605.07596", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu, Sabyasachi Chatterjee, John Duchi and John Lafferty", "title": "Local Minimax Complexity of Stochastic Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the traditional worst-case, minimax analysis of stochastic convex\noptimization by introducing a localized form of minimax complexity for\nindividual functions. Our main result gives function-specific lower and upper\nbounds on the number of stochastic subgradient evaluations needed to optimize\neither the function or its \"hardest local alternative\" to a given numerical\nprecision. The bounds are expressed in terms of a localized and computational\nanalogue of the modulus of continuity that is central to statistical minimax\nanalysis. We show how the computational modulus of continuity can be explicitly\ncalculated in concrete cases, and relates to the curvature of the function at\nthe optimum. We also prove a superefficiency result that demonstrates it is a\nmeaningful benchmark, acting as a computational analogue of the Fisher\ninformation in statistical estimation. The nature and practical implications of\nthe results are demonstrated in simulations.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 19:37:41 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 15:43:36 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 16:07:37 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Chatterjee", "Sabyasachi", ""], ["Duchi", "John", ""], ["Lafferty", "John", ""]]}, {"id": "1605.07604", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, David M. Blei", "title": "Posterior Dispersion Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is cyclical: we specify a model, infer its posterior,\nand evaluate its performance. Evaluation drives the cycle, as we revise our\nmodel based on how it performs. This requires a metric. Traditionally,\npredictive accuracy prevails. Yet, predictive accuracy does not tell the whole\nstory. We propose to evaluate a model through posterior dispersion. The idea is\nto analyze how each datapoint fares in relation to posterior uncertainty around\nthe hidden structure. We propose a family of posterior dispersion indices (PDI)\nthat capture this idea. A PDI identifies rich patterns of model mismatch in\nthree real data examples: voting preferences, supermarket shopping, and\npopulation genetics.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 19:58:02 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Blei", "David M.", ""]]}, {"id": "1605.07689", "submitter": "Jason Lee", "authors": "Michael I. Jordan, Jason D. Lee, Yun Yang", "title": "Communication-Efficient Distributed Statistical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Communication-efficient Surrogate Likelihood (CSL) framework for\nsolving distributed statistical inference problems. CSL provides a\ncommunication-efficient surrogate to the global likelihood that can be used for\nlow-dimensional estimation, high-dimensional regularized estimation and\nBayesian inference. For low-dimensional estimation, CSL provably improves upon\nnaive averaging schemes and facilitates the construction of confidence\nintervals. For high-dimensional regularized estimation, CSL leads to a\nminimax-optimal estimator with controlled communication cost. For Bayesian\ninference, CSL can be used to form a communication-efficient quasi-posterior\ndistribution that converges to the true posterior. This quasi-posterior\nprocedure significantly improves the computational efficiency of MCMC\nalgorithms even in a non-distributed setting. We present both theoretical\nanalysis and experiments to explore the properties of the CSL approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 00:12:06 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 05:31:41 GMT"}, {"version": "v3", "created": "Sun, 6 Nov 2016 00:37:39 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Jordan", "Michael I.", ""], ["Lee", "Jason D.", ""], ["Yang", "Yun", ""]]}, {"id": "1605.07696", "submitter": "Yu Lu", "authors": "Chao Gao, Yu Lu, Dengyong Zhou", "title": "Exact Exponent in Optimal Rates for Crowdsourcing", "comments": "To appear in the Proceedings of the 33rd International Conference on\n  Machine Learning, New York, NY, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, crowdsourcing has become the primary\nmeans for label collection. In this paper, we study the optimal error rate for\naggregating labels provided by a set of non-expert workers. Under the classic\nDawid-Skene model, we establish matching upper and lower bounds with an exact\nexponent $mI(\\pi)$ in which $m$ is the number of workers and $I(\\pi)$ the\naverage Chernoff information that characterizes the workers' collective\nability. Such an exact characterization of the error exponent allows us to\nstate a precise sample size requirement\n$m>\\frac{1}{I(\\pi)}\\log\\frac{1}{\\epsilon}$ in order to achieve an $\\epsilon$\nmisclassification error. In addition, our results imply the optimality of\nvarious EM algorithms for crowdsourcing initialized by consistent estimators.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 01:16:06 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:43:49 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Gao", "Chao", ""], ["Lu", "Yu", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1605.07717", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Yu Cheng, Weining Lu, Zhongfei Zhang", "title": "Deep Structured Energy Based Models for Anomaly Detection", "comments": "To appear in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attack the anomaly detection problem by directly modeling\nthe data distribution with deep architectures. We propose deep structured\nenergy based models (DSEBMs), where the energy function is the output of a\ndeterministic deep neural network with structure. We develop novel model\narchitectures to integrate EBMs with different types of data such as static\ndata, sequential data, and spatial data, and apply appropriate model\narchitectures to adapt to the data structure. Our training algorithm is built\nupon the recent development of score matching \\cite{sm}, which connects an EBM\nwith a regularized autoencoder, eliminating the need for complicated sampling\nmethod. Statistically sound decision criterion can be derived for anomaly\ndetection purpose from the perspective of the energy landscape of the data\ndistribution. We investigate two decision criteria for performing anomaly\ndetection: the energy score and the reconstruction error. Extensive empirical\nstudies on benchmark tasks demonstrate that our proposed model consistently\nmatches or outperforms all the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 03:40:18 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 02:36:10 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Cheng", "Yu", ""], ["Lu", "Weining", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1605.07719", "submitter": "Huishuai Zhang", "authors": "Huishuai Zhang, Yi Zhou, Yingbin Liang, Yuejie Chi", "title": "Reshaped Wirtinger Flow and Incremental Algorithm for Solving Quadratic\n  System of Equations", "comments": "Part of this draft is accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the phase retrieval problem, which solves quadratic system of\nequations, i.e., recovers a vector $\\boldsymbol{x}\\in \\mathbb{R}^n$ from its\nmagnitude measurements $y_i=|\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\\rangle|,\ni=1,..., m$. We develop a gradient-like algorithm (referred to as RWF\nrepresenting reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss\nfunction. In comparison with existing nonconvex Wirtinger flow (WF) algorithm\n\\cite{candes2015phase}, although the loss function becomes nonsmooth, it\ninvolves only the second power of variable and hence reduces the complexity. We\nshow that for random Gaussian measurements, RWF enjoys geometric convergence to\na global optimal point as long as the number $m$ of measurements is on the\norder of $n$, the dimension of the unknown $\\boldsymbol{x}$. This improves the\nsample complexity of WF, and achieves the same sample complexity as truncated\nWirtinger flow (TWF) \\cite{chen2015solving}, but without truncation in gradient\nloop. Furthermore, RWF costs less computationally than WF, and runs faster\nnumerically than both WF and TWF. We further develop the incremental\n(stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges\nlinearly to the true signal. We further establish performance guarantee of an\nexisting Kaczmarz method for the phase retrieval problem based on its\nconnection to IRWF. We also empirically demonstrate that IRWF outperforms\nexisting ITWF algorithm (stochastic version of TWF) as well as other batch\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 03:45:44 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 01:44:00 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zhang", "Huishuai", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Chi", "Yuejie", ""]]}, {"id": "1605.07723", "submitter": "Alexander Ratner", "authors": "Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam,\n  Christopher R\\'e", "title": "Data Programming: Creating Large Training Sets, Quickly", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 29, 2016,\n  3567--3575", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large labeled training sets are the critical building blocks of supervised\nlearning methods and are key enablers of deep learning techniques. For some\napplications, creating labeled training sets is the most time-consuming and\nexpensive part of applying machine learning. We therefore propose a paradigm\nfor the programmatic creation of training sets called data programming in which\nusers express weak supervision strategies or domain heuristics as labeling\nfunctions, which are programs that label subsets of the data, but that are\nnoisy and may conflict. We show that by explicitly representing this training\nset labeling process as a generative model, we can \"denoise\" the generated\ntraining set, and establish theoretically that we can recover the parameters of\nthese generative models in a handful of settings. We then show how to modify a\ndiscriminative loss function to make it noise-aware, and demonstrate our method\nover a range of discriminative models including logistic regression and LSTMs.\nExperimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data\nprogramming would have led to a new winning score, and also show that applying\ndata programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points\nover a state-of-the-art LSTM baseline (and into second place in the\ncompetition). Additionally, in initial user studies we observed that data\nprogramming may be an easier way for non-experts to create machine learning\nmodels when training data is limited or unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:14:59 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 20:03:26 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 19:48:53 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Ratner", "Alexander", ""], ["De Sa", "Christopher", ""], ["Wu", "Sen", ""], ["Selsam", "Daniel", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1605.07725", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Andrew M. Dai, Ian Goodfellow", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training provides a means of regularizing supervised learning\nalgorithms while virtual adversarial training is able to extend supervised\nlearning algorithms to the semi-supervised setting. However, both methods\nrequire making small perturbations to numerous entries of the input vector,\nwhich is inappropriate for sparse high-dimensional inputs such as one-hot word\nrepresentations. We extend adversarial and virtual adversarial training to the\ntext domain by applying perturbations to the word embeddings in a recurrent\nneural network rather than to the original input itself. The proposed method\nachieves state of the art results on multiple benchmark semi-supervised and\npurely supervised tasks. We provide visualizations and analysis showing that\nthe learned word embeddings have improved in quality and that while training,\nthe model is less prone to overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:25:45 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:59:37 GMT"}, {"version": "v3", "created": "Sat, 6 May 2017 20:16:03 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Miyato", "Takeru", ""], ["Dai", "Andrew M.", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1605.07747", "submitter": "Davood Hajinezhad", "authors": "Davood Hajinezhad, Mingyi Hong, Tuo Zhao, Zhaoran Wang", "title": "NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and\n  Stochastic Optimization", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a stochastic and distributed algorithm for nonconvex problems whose\nobjective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a\nnonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT)\nalgorithm splits the problem into $N$ subproblems, and utilizes an augmented\nLagrangian based primal-dual scheme to solve it in a distributed and stochastic\nmanner. With a special non-uniform sampling, a version of NESTT achieves\n$\\epsilon$-stationary solution using\n$\\mathcal{O}((\\sum_{i=1}^N\\sqrt{L_i/N})^2/\\epsilon)$ gradient evaluations,\nwhich can be up to $\\mathcal{O}(N)$ times better than the (proximal) gradient\ndescent methods. It also achieves Q-linear convergence rate for nonconvex\n$\\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we\nreveal a fundamental connection between primal-dual based methods and a few\nprimal only methods such as IAG/SAG/SAGA.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 06:42:51 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 23:06:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Hajinezhad", "Davood", ""], ["Hong", "Mingyi", ""], ["Zhao", "Tuo", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1605.07784", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Dohyung Park, Yudong Chen, Constantine Caramanis", "title": "Fast Algorithms for Robust PCA via Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Robust PCA in the fully and partially observed\nsettings. Without corruptions, this is the well-known matrix completion\nproblem. From a statistical standpoint this problem has been recently\nwell-studied, and conditions on when recovery is possible (how many\nobservations do we need, how many corruptions can we tolerate) via\npolynomial-time algorithms is by now understood. This paper presents and\nanalyzes a non-convex optimization approach that greatly reduces the\ncomputational complexity of the above problems, compared to the best available\nalgorithms. In particular, in the fully observed case, with $r$ denoting rank\nand $d$ dimension, we reduce the complexity from\n$\\mathcal{O}(r^2d^2\\log(1/\\varepsilon))$ to\n$\\mathcal{O}(rd^2\\log(1/\\varepsilon))$ -- a big savings when the rank is big.\nFor the partially observed case, we show the complexity of our algorithm is no\nmore than $\\mathcal{O}(r^4d \\log d \\log(1/\\varepsilon))$. Not only is this the\nbest-known run-time for a provable algorithm under partial observation, but in\nthe setting where $r$ is small compared to $d$, it also allows for\nnear-linear-in-$d$ run-time that can be exploited in the fully-observed case as\nwell, by simply running our algorithm on a subset of the observations.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 09:10:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 17:28:25 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Yi", "Xinyang", ""], ["Park", "Dohyung", ""], ["Chen", "Yudong", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1605.07826", "submitter": "Matthew Graham", "authors": "Matthew M. Graham and Amos J. Storkey", "title": "Asymptotically exact inference in differentiable generative models", "comments": "14 pages, 5 figures. Accepted for AISTATS 2017, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many generative models can be expressed as a differentiable function of\nrandom inputs drawn from some simple probability density. This framework\nincludes both deep generative architectures such as Variational Autoencoders\nand a large class of procedurally defined simulator models. We present a method\nfor performing efficient MCMC inference in such models when conditioning on\nobservations of the model output. For some models this offers an asymptotically\nexact inference method where Approximate Bayesian Computation might otherwise\nbe employed. We use the intuition that inference corresponds to integrating a\ndensity across the manifold corresponding to the set of inputs consistent with\nthe observed outputs. This motivates the use of a constrained variant of\nHamiltonian Monte Carlo which leverages the smooth geometry of the manifold to\ncoherently move between inputs exactly consistent with observations. We\nvalidate the method by performing inference tasks in a diverse set of models.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 11:10:36 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 20:39:35 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2016 21:31:22 GMT"}, {"version": "v4", "created": "Thu, 2 Mar 2017 22:07:33 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Graham", "Matthew M.", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1605.07870", "submitter": "Simeng Qu", "authors": "Simeng Qu, Xiao Wang", "title": "Simultaneous Sparse Dictionary Learning and Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a cutting-edge area in imaging processing, that has\nrecently led to state-of-the-art results in many signal processing tasks. The\nidea is to conduct a linear decomposition of a signal using a few atoms of a\nlearned and usually over-completed dictionary instead of a pre-defined basis.\nDetermining a proper size of the to-be-learned dictionary is crucial for both\nprecision and efficiency of the process, while most of the existing dictionary\nlearning algorithms choose the size quite arbitrarily. In this paper, a novel\nregularization method called the Grouped Smoothly Clipped Absolute Deviation\n(GSCAD) is employed for learning the dictionary. The proposed method can\nsimultaneously learn a sparse dictionary and select the appropriate dictionary\nsize. Efficient algorithm is designed based on the alternative direction method\nof multipliers (ADMM) which decomposes the joint non-convex problem with the\nnon-convex penalty into two convex optimization problems. Several examples are\npresented for image denoising and the experimental results are compared with\nother state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 13:24:39 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Qu", "Simeng", ""], ["Wang", "Xiao", ""]]}, {"id": "1605.07906", "submitter": "Zexun Chen", "authors": "Zexun Chen and Bo Wang", "title": "How priors of initial hyperparameters affect Gaussian process regression\n  models", "comments": null, "journal-ref": "Neurocomputing 275 (2018): 1702-1710", "doi": "10.1016/j.neucom.2017.10.028", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hyperparameters in Gaussian process regression (GPR) model with a\nspecified kernel are often estimated from the data via the maximum marginal\nlikelihood. Due to the non-convexity of marginal likelihood with respect to the\nhyperparameters, the optimization may not converge to the global maxima. A\ncommon approach to tackle this issue is to use multiple starting points\nrandomly selected from a specific prior distribution. As a result the choice of\nprior distribution may play a vital role in the predictability of this\napproach. However, there exists little research in the literature to study the\nimpact of the prior distributions on the hyperparameter estimation and the\nperformance of GPR. In this paper, we provide the first empirical study on this\nproblem using simulated and real data experiments. We consider different types\nof priors for the initial values of hyperparameters for some commonly used\nkernels and investigate the influence of the priors on the predictability of\nGPR models. The results reveal that, once a kernel is chosen, different priors\nfor the initial hyperparameters have no significant impact on the performance\nof GPR prediction, despite that the estimates of the hyperparameters are very\ndifferent to the true values in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:45:26 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 14:36:55 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Chen", "Zexun", ""], ["Wang", "Bo", ""]]}, {"id": "1605.07913", "submitter": "Marianna Pensky", "authors": "Pawan Gupta and Marianna Pensky", "title": "Solution of linear ill-posed problems using random dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. In the context of\nregression problems, there has been enormous amount of effort to recover an\nunknown function using such dictionaries. One of the most popular methods,\nlasso and its versions, is based on minimizing empirical likelihood and\nunfortunately, requires stringent assumptions on the dictionary, the, so\ncalled, compatibility conditions. Though compatibility conditions are hard to\nsatisfy, it is well known that this can be accomplished by using random\ndictionaries. In the present paper, we show how one can apply random\ndictionaries to solution of ill-posed linear inverse problems. We put a\ntheoretical foundation under the suggested methodology and study its\nperformance via simulations.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:51:50 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 19:42:16 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:35:05 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Gupta", "Pawan", ""], ["Pensky", "Marianna", ""]]}, {"id": "1605.07950", "submitter": "Haoming Jiang", "authors": "Xingguo Li, Haoming Jiang, Jarvis Haupt, Raman Arora, Han Liu, Mingyi\n  Hong, and Tuo Zhao", "title": "On Fast Convergence of Proximal Algorithms for SQRT-Lasso Optimization:\n  Don't Worry About Its Nonsmooth Loss Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning techniques sacrifice convenient computational\nstructures to gain estimation robustness and modeling flexibility. However, by\nexploring the modeling structures, we find these \"sacrifices\" do not always\nrequire more computational efforts. To shed light on such a \"free-lunch\"\nphenomenon, we study the square-root-Lasso (SQRT-Lasso) type regression\nproblem. Specifically, we show that the nonsmooth loss functions of SQRT-Lasso\ntype regression ease tuning effort and gain adaptivity to inhomogeneous noise,\nbut is not necessarily more challenging than Lasso in computation. We can\ndirectly apply proximal algorithms (e.g. proximal gradient descent, proximal\nNewton, and proximal Quasi-Newton algorithms) without worrying the\nnonsmoothness of the loss function. Theoretically, we prove that the proximal\nalgorithms combined with the pathwise optimization scheme enjoy fast\nconvergence guarantees with high probability. Numerical results are provided to\nsupport our theory.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 16:08:08 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 02:10:41 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 18:32:20 GMT"}, {"version": "v4", "created": "Sat, 3 Feb 2018 05:50:46 GMT"}, {"version": "v5", "created": "Wed, 14 Feb 2018 16:06:58 GMT"}, {"version": "v6", "created": "Sun, 14 Apr 2019 01:00:57 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Li", "Xingguo", ""], ["Jiang", "Haoming", ""], ["Haupt", "Jarvis", ""], ["Arora", "Raman", ""], ["Liu", "Han", ""], ["Hong", "Mingyi", ""], ["Zhao", "Tuo", ""]]}, {"id": "1605.07991", "submitter": "Jialei Wang", "authors": "Jialei Wang, Mladen Kolar, Nathan Srebro, Tong Zhang", "title": "Efficient Distributed Learning with Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, efficient approach for distributed sparse learning in\nhigh-dimensions, where observations are randomly partitioned across machines.\nComputationally, at each round our method only requires the master machine to\nsolve a shifted ell_1 regularized M-estimation problem, and other workers to\ncompute the gradient. In respect of communication, the proposed approach\nprovably matches the estimation error bound of centralized methods within\nconstant rounds of communications (ignoring logarithmic factors). We conduct\nextensive experiments on both simulated and real world datasets, and\ndemonstrate encouraging performances on high-dimensional regression and\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 18:15:43 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wang", "Jialei", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""], ["Zhang", "Tong", ""]]}, {"id": "1605.07999", "submitter": "Baxter Eaves Jr", "authors": "Baxter S. Eaves Jr and Patrick Shafto", "title": "Toward a general, scaleable framework for Bayesian teaching with\n  applications to topic models", "comments": "7 Pages, 5 Figures, submitted to IJCAI 2016 workshop on Interactive\n  Machine Learning: Connecting Humans and Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machines, not humans, are the world's dominant knowledge accumulators but\nhumans remain the dominant decision makers. Interpreting and disseminating the\nknowledge accumulated by machines requires expertise, time, and is prone to\nfailure. The problem of how best to convey accumulated knowledge from computers\nto humans is a critical bottleneck in the broader application of machine\nlearning. We propose an approach based on human teaching where the problem is\nformalized as selecting a small subset of the data that will, with high\nprobability, lead the human user to the correct inference. This approach,\nthough successful for modeling human learning in simple laboratory experiments,\nhas failed to achieve broader relevance due to challenges in formulating\ngeneral and scalable algorithms. We propose general-purpose teaching via\npseudo-marginal sampling and demonstrate the algorithm by teaching topic\nmodels. Simulation results show our sampling-based approach: effectively\napproximates the probability where ground-truth is possible via enumeration,\nresults in data that are markedly different from those expected by random\nsampling, and speeds learning especially for small amounts of data. Application\nto movie synopsis data illustrates differences between teaching and random\nsampling for teaching distributions and specific topics, and demonstrates gains\nin scalability and applicability to real-world problems.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 18:33:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Eaves", "Baxter S.", "Jr"], ["Shafto", "Patrick", ""]]}, {"id": "1605.08003", "submitter": "Blake Woodworth", "authors": "Blake Woodworth and Nathan Srebro", "title": "Tight Complexity Bounds for Optimizing Composite Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide tight upper and lower bounds on the complexity of minimizing the\naverage of $m$ convex functions using gradient and prox oracles of the\ncomponent functions. We show a significant gap between the complexity of\ndeterministic vs randomized optimization. For smooth functions, we show that\naccelerated gradient descent (AGD) and an accelerated variant of SVRG are\noptimal in the deterministic and randomized settings respectively, and that a\ngradient oracle is sufficient for the optimal rate. For non-smooth functions,\nhaving access to prox oracles reduces the complexity and we present optimal\nmethods based on smoothing that improve over methods using just gradient\naccesses.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 18:44:54 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 18:32:55 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 17:14:54 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Woodworth", "Blake", ""], ["Srebro", "Nathan", ""]]}, {"id": "1605.08062", "submitter": "Zhaohan Guo", "authors": "Zhaohan Daniel Guo, Shayan Doroudi, Emma Brunskill", "title": "A PAC RL Algorithm for Episodic POMDPs", "comments": null, "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics, pp. 510-518, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting real world domains involve reinforcement learning (RL) in\npartially observable environments. Efficient learning in such domains is\nimportant, but existing sample complexity bounds for partially observable RL\nare at least exponential in the episode length. We give, to our knowledge, the\nfirst partially observable RL algorithm with a polynomial bound on the number\nof episodes on which the algorithm may not achieve near-optimal performance.\nOur algorithm is suitable for an important class of episodic POMDPs. Our\napproach builds on recent advances in method of moments for latent variable\nmodel estimation.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 20:15:38 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 18:23:04 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Guo", "Zhaohan Daniel", ""], ["Doroudi", "Shayan", ""], ["Brunskill", "Emma", ""]]}, {"id": "1605.08108", "submitter": "Xiang Cheng", "authors": "Xiang Cheng, Farbod Roosta-Khorasani, Stefan Palombo, Peter L.\n  Bartlett and Michael W. Mahoney", "title": "FLAG n' FLARE: Fast Linearly-Coupled Adaptive Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider first order gradient methods for effectively optimizing a\ncomposite objective in the form of a sum of smooth and, potentially, non-smooth\nfunctions. We present accelerated and adaptive gradient methods, called FLAG\nand FLARE, which can offer the best of both worlds. They can achieve the\noptimal convergence rate by attaining the optimal first-order oracle complexity\nfor smooth convex optimization. Additionally, they can adaptively and\nnon-uniformly re-scale the gradient direction to adapt to the limited curvature\navailable and conform to the geometry of the domain. We show theoretically and\nempirically that, through the compounding effects of acceleration and\nadaptivity, FLAG and FLARE can be highly effective for many data fitting and\nmachine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 00:24:37 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 20:20:17 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cheng", "Xiang", ""], ["Roosta-Khorasani", "Farbod", ""], ["Palombo", "Stefan", ""], ["Bartlett", "Peter L.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1605.08174", "submitter": "Hyeryung Jang", "authors": "Hyeryung Jang, Hyungwon Choi, Yung Yi, Jinwoo Shin", "title": "Adiabatic Persistent Contrastive Divergence Learning", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of parameter learning in probabilistic\ngraphical models having latent variables, where the standard approach is the\nexpectation maximization algorithm alternating expectation (E) and maximization\n(M) steps. However, both E and M steps are computationally intractable for high\ndimensional data, while the substitution of one step to a faster surrogate for\ncombating against intractability can often cause failure in convergence. We\npropose a new learning algorithm which is computationally efficient and\nprovably ensures convergence to a correct optimum. Its key idea is to run only\na few cycles of Markov Chains (MC) in both E and M steps. Such an idea of\nrunning incomplete MC has been well studied only for M step in the literature,\ncalled Contrastive Divergence (CD) learning. While such known CD-based schemes\nfind approximated gradients of the log-likelihood via the mean-field approach\nin E step, our proposed algorithm does exact ones via MC algorithms in both\nsteps due to the multi-time-scale stochastic approximation theory. Despite its\ntheoretical guarantee in convergence, the proposed scheme might suffer from the\nslow mixing of MC in E step. To tackle it, we also propose a hybrid approach\napplying both mean-field and MC approximation in E step, where the hybrid\napproach outperforms the bare mean-field CD scheme in our experiments on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 07:26:25 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 10:52:07 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Jang", "Hyeryung", ""], ["Choi", "Hyungwon", ""], ["Yi", "Yung", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1605.08179", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard\n  Sch\\\"olkopf, L\\'eon Bottou", "title": "Discovering Causal Signals in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the existence of observable footprints that reveal the\n\"causal dispositions\" of the object categories appearing in collections of\nimages. We achieve this goal in two steps. First, we take a learning approach\nto observational causal discovery, and build a classifier that achieves\nstate-of-the-art performance on finding the causal direction between pairs of\nrandom variables, given samples from their joint distribution. Second, we use\nour causal direction classifier to effectively distinguish between features of\nobjects and features of their contexts in collections of static images. Our\nexperiments demonstrate the existence of a relation between the direction of\ncausality and the difference between objects and their contexts, and by the\nsame token, the existence of observable signals that reveal the causal\ndispositions of objects.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 07:36:56 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 11:14:18 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lopez-Paz", "David", ""], ["Nishihara", "Robert", ""], ["Chintala", "Soumith", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1605.08201", "submitter": "Steffen Limmer", "authors": "Steffen Limmer and S{\\l}awomir Sta\\'nczak", "title": "Towards optimal nonlinearities for sparse recovery using higher-order\n  statistics", "comments": "6 pages, 5 figures, accepted for publication at MLSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine learning techniques to develop low-latency approximate\nsolutions to a class of inverse problems. More precisely, we use a\nprobabilistic approach for the problem of recovering sparse stochastic signals\nthat are members of the $\\ell_p$-balls. In this context, we analyze the\nBayesian mean-square-error (MSE) for two types of estimators: (i) a linear\nestimator and (ii) a structured estimator composed of a linear operator\nfollowed by a Cartesian product of univariate nonlinear mappings. By\nconstruction, the complexity of the proposed nonlinear estimator is comparable\nto that of its linear counterpart since the nonlinear mapping can be\nimplemented efficiently in hardware by means of look-up tables (LUTs). The\nproposed structure lends itself to neural networks and iterative\nshrinkage/thresholding-type algorithms restricted to a single iterate (e.g. due\nto imposed hardware or latency constraints). By resorting to an alternating\nminimization technique, we obtain a sequence of optimized linear operators and\nnonlinear mappings that converge in the MSE objective. The result is attractive\nfor real-time applications where general iterative and convex optimization\nmethods are infeasible.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 09:17:05 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 07:36:58 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Limmer", "Steffen", ""], ["Sta\u0144czak", "S\u0142awomir", ""]]}, {"id": "1605.08228", "submitter": "Angelo Valleriani", "authors": "Marco Rusconi, Angelo Valleriani", "title": "Predict or classify: The deceptive role of time-locking in brain signal\n  classification", "comments": "23 pages, 5 figures", "journal-ref": "Scientific Reports 6, 28236 (2016)", "doi": "10.1038/srep28236", "report-no": null, "categories": "q-bio.NC physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several experimental studies claim to be able to predict the outcome of\nsimple decisions from brain signals measured before subjects are aware of their\ndecision. Often, these studies use multivariate pattern recognition methods\nwith the underlying assumption that the ability to classify the brain signal is\nequivalent to predict the decision itself. Here we show instead that it is\npossible to correctly classify a signal even if it does not contain any\npredictive information about the decision. We first define a simple stochastic\nmodel that mimics the random decision process between two equivalent\nalternatives, and generate a large number of independent trials that contain no\nchoice-predictive information. The trials are first time-locked to the time\npoint of the final event and then classified using standard machine-learning\ntechniques. The resulting classification accuracy is above chance level long\nbefore the time point of time-locking. We then analyze the same trials using\ninformation theory. We demonstrate that the high classification accuracy is a\nconsequence of time-locking and that its time behavior is simply related to the\nlarge relaxation time of the process. We conclude that when time-locking is a\ncrucial step in the analysis of neural activity patterns, both the emergence\nand the timing of the classification accuracy are affected by structural\nproperties of the network that generates the signal.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 11:17:41 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 12:23:34 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Rusconi", "Marco", ""], ["Valleriani", "Angelo", ""]]}, {"id": "1605.08233", "submitter": "Zhiqiang Xu", "authors": "Zhiqiang Xu and Yiping Ke", "title": "Stochastic Variance Reduced Riemannian Eigensolver", "comments": "Under review. Supplementary material included in the paper as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic Riemannian gradient algorithm for matrix\neigen-decomposition. The state-of-the-art stochastic Riemannian algorithm\nrequires the learning rate to decay to zero and thus suffers from slow\nconvergence and sub-optimal solutions. In this paper, we address this issue by\ndeploying the variance reduction (VR) technique of stochastic gradient descent\n(SGD). The technique was originally developed to solve convex problems in the\nEuclidean space. We generalize it to Riemannian manifolds and realize it to\nsolve the non-convex eigen-decomposition problem. We are the first to propose\nand analyze the generalization of SVRG to Riemannian manifolds. Specifically,\nwe propose the general variance reduction form, SVRRG, in the framework of the\nstochastic Riemannian gradient optimization. It's then specialized to the\nproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a\nnovel and elegant theoretical analysis on this algorithm. The theory shows that\na fixed learning rate can be used in the Riemannian setting with an exponential\nglobal convergence rate guaranteed. The theoretical results make a significant\nimprovement over existing studies, with the effectiveness empirically verified.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 11:30:45 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 13:29:37 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Xu", "Zhiqiang", ""], ["Ke", "Yiping", ""]]}, {"id": "1605.08254", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues", "title": "Robust Large Margin Deep Neural Networks", "comments": "accepted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2708039", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization error of deep neural networks via their classification\nmargin is studied in this work. Our approach is based on the Jacobian matrix of\na deep neural network and can be applied to networks with arbitrary\nnon-linearities and pooling layers, and to networks with different\narchitectures such as feed forward networks and residual networks. Our analysis\nleads to the conclusion that a bounded spectral norm of the network's Jacobian\nmatrix in the neighbourhood of the training samples is crucial for a deep\nneural network of arbitrary depth and width to generalize well. This is a\nsignificant improvement over the current bounds in the literature, which imply\nthat the generalization error grows with either the width or the depth of the\nnetwork. Moreover, it shows that the recently proposed batch normalization and\nweight normalization re-parametrizations enjoy good generalization properties,\nand leads to a novel network regularizer based on the network's Jacobian\nmatrix. The analysis is supported with experimental results on the MNIST,\nCIFAR-10, LaRED and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 12:19:09 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 15:54:33 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 11:45:31 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1605.08257", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Bamdev Mishra", "title": "Low-rank tensor completion: a Riemannian manifold preconditioning\n  approach", "comments": "The 33rd International Conference on Machine Learning (ICML 2016).\n  arXiv admin note: substantial text overlap with arXiv:1506.02159", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Riemannian manifold preconditioning approach for the\ntensor completion problem with rank constraint. A novel Riemannian metric or\ninner product is proposed that exploits the least-squares structure of the cost\nfunction and takes into account the structured symmetry that exists in Tucker\ndecomposition. The specific metric allows to use the versatile framework of\nRiemannian optimization on quotient manifolds to develop preconditioned\nnonlinear conjugate gradient and stochastic gradient descent algorithms for\nbatch and online setups, respectively. Concrete matrix representations of\nvarious optimization-related ingredients are listed. Numerical comparisons\nsuggest that our proposed algorithms robustly outperform state-of-the-art\nalgorithms across different synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 12:55:02 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1605.08283", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Michael Tschannen and Aleksandar Stani\\'c and\n  Philipp Grohs and Helmut B\\\"olcskei", "title": "Discrete Deep Feature Extraction: A Theory and New Architectures", "comments": "Proc. of International Conference on Machine Learning (ICML), New\n  York, USA, June 2016, to appear", "journal-ref": "Proc. of International Conference on Machine Learning (ICML), New\n  York, USA, pp. 2149-2158, June 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First steps towards a mathematical theory of deep convolutional neural\nnetworks for feature extraction were made---for the continuous-time case---in\nMallat, 2012, and Wiatowski and B\\\"olcskei, 2015. This paper considers the\ndiscrete case, introduces new convolutional neural network architectures, and\nproposes a mathematical framework for their analysis. Specifically, we\nestablish deformation and translation sensitivity results of local and global\nnature, and we investigate how certain structural properties of the input\nsignal are reflected in the corresponding feature vectors. Our theory applies\nto general filters and general Lipschitz-continuous non-linearities and pooling\noperators. Experiments on handwritten digit classification and facial landmark\ndetection---including feature importance evaluation---complement the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 13:55:07 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Tschannen", "Michael", ""], ["Stani\u0107", "Aleksandar", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1605.08285", "submitter": "Gang Wang", "authors": "Gang Wang and Georgios B. Giannakis and Yonina C. Eldar", "title": "Solving Systems of Random Quadratic Equations via Truncated Amplitude\n  Flow", "comments": "37 Pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm, termed \\emph{truncated amplitude flow}\n(TAF), to recover an unknown vector $\\bm{x}$ from a system of quadratic\nequations of the form $y_i=|\\langle\\bm{a}_i,\\bm{x}\\rangle|^2$, where\n$\\bm{a}_i$'s are given random measurement vectors. This problem is known to be\n\\emph{NP-hard} in general. We prove that as soon as the number of equations is\non the order of the number of unknowns, TAF recovers the solution exactly (up\nto a global unimodular constant) with high probability and complexity growing\nlinearly with both the number of unknowns and the number of equations. Our TAF\napproach adopts the \\emph{amplitude-based} empirical loss function, and\nproceeds in two stages. In the first stage, we introduce an\n\\emph{orthogonality-promoting} initialization that can be obtained with a few\npower iterations. Stage two refines the initial estimate by successive updates\nof scalable \\emph{truncated generalized gradient iterations}, which are able to\nhandle the rather challenging nonconvex and nonsmooth amplitude-based objective\nfunction. In particular, when vectors $\\bm{x}$ and $\\bm{a}_i$'s are\nreal-valued, our gradient truncation rule provably eliminates erroneously\nestimated signs with high probability to markedly improve upon its untruncated\nversion. Numerical tests using synthetic data and real images demonstrate that\nour initialization returns more accurate and robust estimates relative to\nspectral initializations. Furthermore, even under the same initialization, the\nproposed amplitude-based refinement outperforms existing Wirtinger flow\nvariants, corroborating the superior performance of TAF over state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 13:57:49 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 20:19:19 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 23:40:18 GMT"}, {"version": "v4", "created": "Sun, 16 Oct 2016 19:02:18 GMT"}, {"version": "v5", "created": "Sun, 20 Aug 2017 20:55:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1605.08299", "submitter": "Aleksandr Aravkin", "authors": "Eunho Yang, Aurelie Lozano, and Aleksandr Aravkin", "title": "A General Family of Trimmed Estimators for Robust High-dimensional Data\n  Analysis", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robustifying high-dimensional structured\nestimation. Robust techniques are key in real-world applications which often\ninvolve outliers and data corruption. We focus on trimmed versions of\nstructurally regularized M-estimators in the high-dimensional setting,\nincluding the popular Least Trimmed Squares estimator, as well as analogous\nestimators for generalized linear models and graphical models, using possibly\nnon-convex loss functions. We present a general analysis of their statistical\nconvergence rates and consistency, and then take a closer look at the trimmed\nversions of the Lasso and Graphical Lasso estimators as special cases. On the\noptimization side, we show how to extend algorithms for M-estimators to fit\ntrimmed variants and provide guarantees on their numerical convergence. The\ngenerality and competitive performance of high-dimensional trimmed estimators\nare illustrated numerically on both simulated and real-world genomics data.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 14:26:20 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 12:50:26 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Yang", "Eunho", ""], ["Lozano", "Aurelie", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1605.08301", "submitter": "Markus Sch\\\"oberl", "authors": "Markus Sch\\\"oberl and Nicholas Zabaras and Phaedon-Stelios\n  Koutsourelakis", "title": "Predictive Coarse-Graining", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.10.073", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven, coarse-graining formulation in the context of\nequilibrium statistical mechanics. In contrast to existing techniques which are\nbased on a fine-to-coarse map, we adopt the opposite strategy by prescribing a\nprobabilistic coarse-to-fine map. This corresponds to a directed probabilistic\nmodel where the coarse variables play the role of latent generators of the fine\nscale (all-atom) data. From an information-theoretic perspective, the framework\nproposed provides an improvement upon the relative entropy method and is\ncapable of quantifying the uncertainty due to the information loss that\nunavoidably takes place during the CG process. Furthermore, it can be readily\nextended to a fully Bayesian model where various sources of uncertainties are\nreflected in the posterior of the model parameters. The latter can be used to\nproduce not only point estimates of fine-scale reconstructions or macroscopic\nobservables, but more importantly, predictive posterior distributions on these\nquantities. Predictive posterior distributions reflect the confidence of the\nmodel as a function of the amount of data and the level of coarse-graining. The\nissues of model complexity and model selection are seamlessly addressed by\nemploying a hierarchical prior that favors the discovery of sparse solutions,\nrevealing the most prominent features in the coarse-grained model. A flexible\nand parallelizable Monte Carlo - Expectation-Maximization (MC-EM) scheme is\nproposed for carrying out inference and learning tasks. A comparative\nassessment of the proposed methodology is presented for a lattice spin system\nand the SPC/E water model.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 14:37:12 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 09:27:20 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Sch\u00f6berl", "Markus", ""], ["Zabaras", "Nicholas", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1605.08346", "submitter": "Adam Charles", "authors": "Adam Charles, Dong Yin, Christopher Rozell", "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent\n  Networks", "comments": "37 pages, 3 figures", "journal-ref": "Journal of Machine Learning Research, 18:1-37 Jan. 2017", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have drawn interest from machine learning\nresearchers because of their effectiveness at preserving past inputs for\ntime-varying data processing tasks. To understand the success and limitations\nof RNNs, it is critical that we advance our analysis of their fundamental\nmemory properties. We focus on echo state networks (ESNs), which are RNNs with\nsimple memoryless nodes and random connectivity. In most existing analyses, the\nshort-term memory (STM) capacity results conclude that the ESN network size\nmust scale linearly with the input size for unstructured inputs. The main\ncontribution of this paper is to provide general results characterizing the STM\ncapacity for linear ESNs with multidimensional input streams when the inputs\nhave common low-dimensional structure: sparsity in a basis or significant\nstatistical dependence between inputs. In both cases, we show that the number\nof nodes in the network must scale linearly with the information rate and\npoly-logarithmically with the ambient input dimension. The analysis relies on\nadvanced applications of random matrix theory and results in explicit\nnon-asymptotic bounds on the recovery error. Taken together, this analysis\nprovides a significant step forward in our understanding of the STM properties\nin RNNs.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 16:02:26 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:14:31 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 15:03:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Charles", "Adam", ""], ["Yin", "Dong", ""], ["Rozell", "Christopher", ""]]}, {"id": "1605.08361", "submitter": "Daniel Soudry", "authors": "Daniel Soudry, Yair Carmon", "title": "No bad local minima: Data independent training error guarantees for\n  multilayer neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use smoothed analysis techniques to provide guarantees on the training\nloss of Multilayer Neural Networks (MNNs) at differentiable local minima.\nSpecifically, we examine MNNs with piecewise linear activation functions,\nquadratic loss and a single output, under mild over-parametrization. We prove\nthat for a MNN with one hidden layer, the training error is zero at every\ndifferentiable local minimum, for almost every dataset and dropout-like noise\nrealization. We then extend these results to the case of more than one hidden\nlayer. Our theoretical guarantees assume essentially nothing on the training\ndata, and are verified numerically. These results suggest why the highly\nnon-convex loss of such MNNs can be easily optimized using local updates (e.g.,\nstochastic gradient descent), as observed empirically.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 16:51:05 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 04:33:39 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Soudry", "Daniel", ""], ["Carmon", "Yair", ""]]}, {"id": "1605.08370", "submitter": "Chi Jin", "authors": "Chi Jin, Sham M. Kakade, Praneeth Netrapalli", "title": "Provable Efficient Online Matrix Completion via Non-convex Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion, where we wish to recover a low rank matrix by observing a\nfew entries from it, is a widely studied problem in both theory and practice\nwith wide applications. Most of the provable algorithms so far on this problem\nhave been restricted to the offline setting where they provide an estimate of\nthe unknown matrix using all observations simultaneously. However, in many\napplications, the online version, where we observe one entry at a time and\ndynamically update our estimate, is more appealing. While existing algorithms\nare efficient for the offline setting, they could be highly inefficient for the\nonline setting.\n  In this paper, we propose the first provable, efficient online algorithm for\nmatrix completion. Our algorithm starts from an initial estimate of the matrix\nand then performs non-convex stochastic gradient descent (SGD). After every\nobservation, it performs a fast update involving only one row of two tall\nmatrices, giving near linear total runtime. Our algorithm can be naturally used\nin the offline setting as well, where it gives competitive sample complexity\nand runtime to state of the art algorithms. Our proofs introduce a general\nframework to show that SGD updates tend to stay away from saddle surfaces and\ncould be of broader interests for other non-convex problems to prove tight\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:26:18 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1605.08374", "submitter": "Zelda Mariet", "authors": "Zelda Mariet and Suvrit Sra", "title": "Kronecker Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Determinantal Point Processes (DPPs) are probabilistic models over all\nsubsets a ground set of $N$ items. They have recently gained prominence in\nseveral applications that rely on \"diverse\" subsets. However, their\napplicability to large problems is still limited due to the $\\mathcal O(N^3)$\ncomplexity of core tasks such as sampling and learning. We enable efficient\nsampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel\nmatrix decomposes as a tensor product of multiple smaller kernel matrices. This\ndecomposition immediately enables fast exact sampling. But contrary to what one\nmay expect, leveraging the Kronecker product structure for speeding up DPP\nlearning turns out to be more difficult. We overcome this challenge, and derive\nbatch and stochastic optimization algorithms for efficiently learning the\nparameters of a KronDPP.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:33:31 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Mariet", "Zelda", ""], ["Sra", "Suvrit", ""]]}, {"id": "1605.08375", "submitter": "Junhong Lin", "authors": "Junhong Lin, Raffaello Camoriano, Lorenzo Rosasco", "title": "Generalization Properties and Implicit Regularization for Multiple\n  Passes SGM", "comments": "26 pages, 4 figures. To appear in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization properties of stochastic gradient methods for\nlearning with convex loss functions and linearly parameterized functions. We\nshow that, in the absence of penalizations or constraints, the stability and\napproximation properties of the algorithm can be controlled by tuning either\nthe step-size or the number of passes over the data. In this view, these\nparameters can be seen to control a form of implicit regularization. Numerical\nresults complement the theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:37:51 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Lin", "Junhong", ""], ["Camoriano", "Raffaello", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1605.08400", "submitter": "Veeranjaneyulu Sadhanala", "authors": "Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Ryan Tibshirani", "title": "Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of\n  Linear Smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a function defined over $n$ locations\non a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$). When\nthe function is constrained to have discrete total variation bounded by $C_n$,\nwe derive the minimax optimal (squared) $\\ell_2$ estimation error rate,\nparametrized by $n$ and $C_n$. Total variation denoising, also known as the\nfused lasso, is seen to be rate optimal. Several simpler estimators exist, such\nas Laplacian smoothing and Laplacian eigenmaps. A natural question is: can\nthese simpler estimators perform just as well? We prove that these estimators,\nand more broadly all estimators given by linear transformations of the input\ndata, are suboptimal over the class of functions with bounded variation. This\nextends fundamental findings of Donoho and Johnstone [1998] on 1-dimensional\ntotal variation spaces to higher dimensions. The implication is that the\ncomputationally simpler methods cannot be used for such sophisticated denoising\ntasks, without sacrificing statistical accuracy. We also derive minimax rates\nfor discrete Sobolev spaces over $d$-dimensional grids, which are, in some\nsense, smaller than the total variation function spaces. Indeed, these are\nsmall enough spaces that linear estimators can be optimal---and a few\nwell-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we\nshow. Lastly, we investigate the problem of adaptivity of the total variation\ndenoiser to these smaller Sobolev function spaces.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 18:38:38 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Sadhanala", "Veeranjaneyulu", ""], ["Wang", "Yu-Xiang", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1605.08454", "submitter": "Yuanjun Gao", "authors": "Yuanjun Gao, Evan Archer, Liam Paninski, John P. Cunningham", "title": "Linear dynamical neural population models through nonlinear embeddings", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A body of recent work in modeling neural activity focuses on recovering\nlow-dimensional latent features that capture the statistical structure of\nlarge-scale neural populations. Most such approaches have focused on linear\ngenerative models, where inference is computationally tractable. Here, we\npropose fLDS, a general class of nonlinear generative models that permits the\nfiring rate of each neuron to vary as an arbitrary smooth function of a latent,\nlinear dynamical state. This extra flexibility allows the model to capture a\nricher set of neural variability than a purely linear model, but retains an\neasily visualizable low-dimensional latent space. To fit this class of\nnon-conjugate models we propose a variational inference scheme, along with a\nnovel approximate posterior capable of capturing rich temporal correlations\nacross time. We show that our techniques permit inference in a wide class of\ngenerative models.We also show in application to two neural datasets that,\ncompared to state-of-the-art neural population models, fLDS captures a much\nlarger proportion of neural variability with a small number of latent\ndimensions, providing superior predictive performance and interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 21:25:26 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 19:44:03 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Gao", "Yuanjun", ""], ["Archer", "Evan", ""], ["Paninski", "Liam", ""], ["Cunningham", "John P.", ""]]}, {"id": "1605.08455", "submitter": "Artur Dubrawski", "authors": "P. Tandon (1), P. Huggins (1), A. Dubrawski (1), S. Labov (2), K.\n  Nelson (2) ((1) Auton Lab, Carnegie Mellon University, (2) Lawrence Livermore\n  National Laboratory)", "title": "Suppressing Background Radiation Using Poisson Principal Component\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of nuclear threat detection systems based on gamma-ray\nspectrometry often strongly depends on the ability to identify the part of\nmeasured signal that can be attributed to background radiation. We have\nsuccessfully applied a method based on Principal Component Analysis (PCA) to\nobtain a compact null-space model of background spectra using PCA projection\nresiduals to derive a source detection score. We have shown the method's\nutility in a threat detection system using mobile spectrometers in urban scenes\n(Tandon et al 2012). While it is commonly assumed that measured photon counts\nfollow a Poisson process, standard PCA makes a Gaussian assumption about the\ndata distribution, which may be a poor approximation when photon counts are\nlow. This paper studies whether and in what conditions PCA with a Poisson-based\nloss function (Poisson PCA) can outperform standard Gaussian PCA in modeling\nbackground radiation to enable more sensitive and specific nuclear threat\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 21:27:11 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Tandon", "P.", ""], ["Huggins", "P.", ""], ["Dubrawski", "A.", ""], ["Labov", "S.", ""], ["Nelson", "K.", ""]]}, {"id": "1605.08491", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Frederic Koehler, Tengyu Ma, Ankur Moitra", "title": "Provable Algorithms for Inference in Topic Models", "comments": "to appear at ICML'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been considerable progress on designing algorithms with\nprovable guarantees -- typically using linear algebraic methods -- for\nparameter learning in latent variable models. But designing provable algorithms\nfor inference has proven to be more challenging. Here we take a first step\ntowards provable inference in topic models. We leverage a property of topic\nmodels that enables us to construct simple linear estimators for the unknown\ntopic proportions that have small variance, and consequently can work with\nshort documents. Our estimators also correspond to finding an estimate around\nwhich the posterior is well-concentrated. We show lower bounds that for shorter\ndocuments it can be information theoretically impossible to find the hidden\ntopics. Finally, we give empirical results that demonstrate that our algorithm\nworks on realistic topic models. It yields good solutions on synthetic data and\nruns in time comparable to a {\\em single} iteration of Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 02:18:43 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Koehler", "Frederic", ""], ["Ma", "Tengyu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1605.08501", "submitter": "Yao Chen", "authors": "Yao Chen, Xiao Wang, Linglong Kong and Hongtu Zhu", "title": "Local Region Sparse Learning for Image-on-Scalar Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of regions of interest (ROI) associated with certain disease\nhas a great impact on public health. Imposing sparsity of pixel values and\nextracting active regions simultaneously greatly complicate the image analysis.\nWe address these challenges by introducing a novel region-selection penalty in\nthe framework of image-on-scalar regression. Our penalty combines the Smoothly\nClipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the\nSCAD of total variation (TV) regularization, enforcing spatial contiguity, into\none group, which segments contiguous spatial regions against zero-valued\nbackground. Efficient algorithm is based on the alternative direction method of\nmultipliers (ADMM) which decomposes the non-convex problem into two iterative\noptimization problems with explicit solutions. Another virtue of the proposed\nmethod is that a divide and conquer learning algorithm is developed, thereby\nallowing scaling to large images. Several examples are presented and the\nexperimental results are compared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 03:28:41 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Chen", "Yao", ""], ["Wang", "Xiao", ""], ["Kong", "Linglong", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1605.08576", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth and Chris Sherlock", "title": "Merging MCMC Subposteriors through Gaussian-Process Approximations", "comments": "Accepted to Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms have become powerful tools for\nBayesian inference. However, they do not scale well to large-data problems.\nDivide-and-conquer strategies, which split the data into batches and, for each\nbatch, run independent MCMC algorithms targeting the corresponding\nsubposterior, can spread the computational burden across a number of separate\nworkers. The challenge with such strategies is in recombining the subposteriors\nto approximate the full posterior. By creating a Gaussian-process approximation\nfor each log-subposterior density we create a tractable approximation for the\nfull posterior. This approximation is exploited through three methodologies:\nfirstly a Hamiltonian Monte Carlo algorithm targeting the expectation of the\nposterior density provides a sample from an approximation to the posterior;\nsecondly, evaluating the true posterior at the sampled points leads to an\nimportance sampler that, asymptotically, targets the true posterior\nexpectations; finally, an alternative importance sampler uses the full\nGaussian-process distribution of the approximation to the log-posterior density\nto re-weight any initial sample and provide both an estimate of the posterior\nexpectation and a measure of the uncertainty in it.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 10:51:48 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:35:44 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Nemeth", "Christopher", ""], ["Sherlock", "Chris", ""]]}, {"id": "1605.08618", "submitter": "Christian Gruhl", "authors": "Christian Gruhl, Bernhard Sick", "title": "Variational Bayesian Inference for Hidden Markov Models With\n  Multivariate Gaussian Output Distributions", "comments": "Preliminary version. Contains all necessary equations for\n  implementation. Ongoing research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMM) have been used for several years in many time\nseries analysis or pattern recognitions tasks. HMM are often trained by means\nof the Baum-Welch algorithm which can be seen as a special variant of an\nexpectation maximization (EM) algorithm. Second-order training techniques such\nas Variational Bayesian Inference (VI) for probabilistic models regard the\nparameters of the probabilistic models as random variables and define\ndistributions over these distribution parameters, hence the name of this\ntechnique. VI can also bee regarded as a special case of an EM algorithm. In\nthis article, we bring both together and train HMM with multivariate Gaussian\noutput distributions with VI. The article defines the new training technique\nfor HMM. An evaluation based on some case studies and a comparison to related\napproaches is part of our ongoing work.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 13:00:31 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Gruhl", "Christian", ""], ["Sick", "Bernhard", ""]]}, {"id": "1605.08636", "submitter": "Pascal Germain", "authors": "Pascal Germain (INRIA Paris), Francis Bach (INRIA Paris), Alexandre\n  Lacoste (Google), Simon Lacoste-Julien (INRIA Paris)", "title": "PAC-Bayesian Theory Meets Bayesian Inference", "comments": "Published at NIPS 2015\n  (http://papers.nips.cc/paper/6569-pac-bayesian-theory-meets-bayesian-inference)", "journal-ref": "Advances in Neural Information Processing Systems 29 (NIPS 2016),\n  p. 1884-1892", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the\nBayesian marginal likelihood. That is, for the negative log-likelihood loss\nfunction, we show that the minimization of PAC-Bayesian generalization risk\nbounds maximizes the Bayesian marginal likelihood. This provides an alternative\nexplanation to the Bayesian Occam's razor criteria, under the assumption that\nthe data is generated by an i.i.d distribution. Moreover, as the negative\nlog-likelihood is an unbounded loss function, we motivate and propose a\nPAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that\nour approach is sound on classical Bayesian linear regression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 13:41:33 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 14:49:05 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 22:48:15 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 17:14:52 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Germain", "Pascal", "", "INRIA Paris"], ["Bach", "Francis", "", "INRIA Paris"], ["Lacoste", "Alexandre", "", "Google"], ["Lacoste-Julien", "Simon", "", "INRIA Paris"]]}, {"id": "1605.08671", "submitter": "Andrea Locatelli", "authors": "Andrea Locatelli, Maurilio Gutzeit, and Alexandra Carpentier", "title": "An optimal algorithm for the Thresholding Bandit Problem", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a specific \\textit{combinatorial pure exploration stochastic bandit\nproblem} where the learner aims at finding the set of arms whose means are\nabove a given threshold, up to a given precision, and \\textit{for a fixed time\nhorizon}. We propose a parameter-free algorithm based on an original heuristic,\nand prove that it is optimal for this problem by deriving matching upper and\nlower bounds. To the best of our knowledge, this is the first non-trivial pure\nexploration setting with \\textit{fixed budget} for which optimal strategies are\nconstructed.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 14:35:29 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Locatelli", "Andrea", ""], ["Gutzeit", "Maurilio", ""], ["Carpentier", "Alexandra", ""]]}, {"id": "1605.08798", "submitter": "Jamshid Sourati", "authors": "Jamshid Sourati, Murat Akcakaya, Todd K. Leen, Deniz Erdogmus,\n  Jennifer G. Dy", "title": "Asymptotic Analysis of Objectives based on Fisher Information in Active\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining labels can be costly and time-consuming. Active learning allows a\nlearning algorithm to intelligently query samples to be labeled for efficient\nlearning. Fisher information ratio (FIR) has been used as an objective for\nselecting queries in active learning. However, little is known about the theory\nbehind the use of FIR for active learning. There is a gap between the\nunderlying theory and the motivation of its usage in practice. In this paper,\nwe attempt to fill this gap and provide a rigorous framework for analyzing\nexisting FIR-based active learning methods. In particular, we show that FIR can\nbe asymptotically viewed as an upper bound of the expected variance of the\nlog-likelihood ratio. Additionally, our analysis suggests a unifying framework\nthat not only enables us to make theoretical comparisons among the existing\nquerying methods based on FIR, but also allows us to give insight into the\ndevelopment of new active learning approaches based on this objective.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 20:44:28 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 21:09:35 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sourati", "Jamshid", ""], ["Akcakaya", "Murat", ""], ["Leen", "Todd K.", ""], ["Erdogmus", "Deniz", ""], ["Dy", "Jennifer G.", ""]]}, {"id": "1605.08803", "submitter": "Laurent Dinh", "authors": "Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio", "title": "Density estimation using Real NVP", "comments": "10 pages of main content, 3 pages of bibliography, 18 pages of\n  appendix. Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of probabilistic models is a central yet challenging\nproblem in machine learning. Specifically, designing models with tractable\nlearning, sampling, inference and evaluation is crucial in solving this task.\nWe extend the space of such models using real-valued non-volume preserving\n(real NVP) transformations, a set of powerful invertible and learnable\ntransformations, resulting in an unsupervised learning algorithm with exact\nlog-likelihood computation, exact sampling, exact inference of latent\nvariables, and an interpretable latent space. We demonstrate its ability to\nmodel natural images on four datasets through sampling, log-likelihood\nevaluation and latent variable manipulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 21:24:32 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 21:37:10 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 23:21:10 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dinh", "Laurent", ""], ["Sohl-Dickstein", "Jascha", ""], ["Bengio", "Samy", ""]]}, {"id": "1605.08833", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "Muffled Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a novel approach to semi-supervised learning. This approach is\ncontrary to the common approach in that the unlabeled examples serve to\n\"muffle,\" rather than enhance, the guidance provided by the labeled examples.\nWe provide several variants of the basic algorithm and show experimentally that\nthey can achieve significantly higher AUC than boosted trees, random forests\nand logistic regression when unlabeled examples are available.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 02:39:24 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1605.08882", "submitter": "Junhong Lin", "authors": "Junhong Lin, Lorenzo Rosasco", "title": "Optimal Rates for Multi-pass Stochastic Gradient Methods", "comments": "Fixed a typo in Eq (66)", "journal-ref": "Journal of Machine Learning Research, 18:1-47, 2017", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the learning properties of the stochastic gradient method when\nmultiple passes over the data and mini-batches are allowed. We study how\nregularization properties are controlled by the step-size, the number of passes\nand the mini-batch size. In particular, we consider the square loss and show\nthat for a universal step-size choice, the number of passes acts as a\nregularization parameter, and optimal finite sample bounds can be achieved by\nearly-stopping. Moreover, we show that larger step-sizes are allowed when\nconsidering mini-batches. Our analysis is based on a unifying approach,\nencompassing both batch and stochastic gradient methods as special cases. As a\nbyproduct, we derive optimal convergence results for batch gradient methods\n(even in the non-attainable cases).\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 12:11:22 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 22:55:48 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 02:07:20 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lin", "Junhong", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1605.08933", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Yinfei Kong and Daoji Li and Jinchi Lv", "title": "Interaction Pursuit with Feature Screening and Selection", "comments": "34 pages for the main text including 7 figures, 53 pages for the\n  Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how features interact with each other is of paramount\nimportance in many scientific discoveries and contemporary applications. Yet\ninteraction identification becomes challenging even for a moderate number of\ncovariates. In this paper, we suggest an efficient and flexible procedure,\ncalled the interaction pursuit (IP), for interaction identification in\nultra-high dimensions. The suggested method first reduces the number of\ninteractions and main effects to a moderate scale by a new feature screening\napproach, and then selects important interactions and main effects in the\nreduced feature space using regularization methods. Compared to existing\napproaches, our method screens interactions separately from main effects and\nthus can be more effective in interaction screening. Under a fairly general\nframework, we establish that for both interactions and main effects, the method\nenjoys the sure screening property in screening and oracle inequalities in\nselection. Our method and theoretical results are supported by several\nsimulation and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 20:59:46 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Fan", "Yingying", ""], ["Kong", "Yinfei", ""], ["Li", "Daoji", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.08961", "submitter": "Anastasios Kyrillidis", "authors": "Megasthenis Asteris, Anastasios Kyrillidis, Oluwasanmi Koyejo, Russell\n  Poldrack", "title": "A simple and provable algorithm for sparse diagonal CCA", "comments": "To appear at ICML 2016, 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of variables, derived from a common set of samples, sparse\nCanonical Correlation Analysis (CCA) seeks linear combinations of a small\nnumber of variables in each set, such that the induced canonical variables are\nmaximally correlated. Sparse CCA is NP-hard.\n  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,\nsparse CCA under the additional assumption that variables within each set are\nstandardized and uncorrelated. Our algorithm operates on a low rank\napproximation of the input data and its computational complexity scales\nlinearly with the number of input variables. It is simple to implement, and\nparallelizable. In contrast to most existing approaches, our algorithm\nadministers precise control on the sparsity of the extracted canonical vectors,\nand comes with theoretical data-dependent global approximation guarantees, that\nhinge on the spectrum of the input data. Finally, it can be straightforwardly\nadapted to other constrained variants of CCA enforcing structure beyond\nsparsity.\n  We empirically evaluate the proposed scheme and apply it on a real\nneuroimaging dataset to investigate associations between brain activity and\nbehavior measurements.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 03:56:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell", ""]]}, {"id": "1605.09004", "submitter": "Andrea Locatelli", "authors": "Alexandra Carpentier and Andrea Locatelli", "title": "Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit\n  Problem", "comments": "COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of \\textit{best arm identification} with a\n\\textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with\narms distribution defined on $[0,1]$. We prove that any bandit strategy, for at\nleast one bandit problem characterized by a complexity $H$, will misidentify\nthe best arm with probability lower bounded by\n$$\\exp\\Big(-\\frac{T}{\\log(K)H}\\Big),$$ where $H$ is the sum for all sub-optimal\narms of the inverse of the squared gaps. Our result disproves formally the\ngeneral belief - coming from results in the fixed confidence setting - that\nthere must exist an algorithm for this problem whose probability of error is\nupper bounded by $\\exp(-T/H)$. This also proves that some existing strategies\nbased on the Successive Rejection of the arms are optimal - closing therefore\nthe current gap between upper and lower bounds for the fixed budget best arm\nidentification problem.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 13:59:48 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Locatelli", "Andrea", ""]]}, {"id": "1605.09026", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva and Juan-Pablo Ortega", "title": "Singular ridge regression with homoscedastic residuals: generalization\n  error with estimated parameters", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes the conditional distribution properties of the\nfinite sample ridge regression estimator and uses that result to evaluate total\nregression and generalization errors that incorporate the inaccuracies\ncommitted at the time of parameter estimation. The paper provides explicit\nformulas for those errors. Unlike other classical references in this setup, our\nresults take place in a fully singular setup that does not assume the existence\nof a solution for the non-regularized regression problem. In exchange, we\ninvoke a conditional homoscedasticity hypothesis on the regularized regression\nresiduals that is crucial in our developments.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 16:45:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1605.09042", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn, Michael Chertkov, Jinwoo Shin", "title": "MCMC assisted by Belief Propagation", "comments": "Fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most\npopular algorithms for computational inference in Graphical Models (GM). In\nprinciple, MCMC is an exact probabilistic method which, however, often suffers\nfrom exponentially slow mixing. In contrast, BP is a deterministic method,\nwhich is typically fast, empirically very successful, however in general\nlacking control of accuracy over loopy graphs. In this paper, we introduce MCMC\nalgorithms correcting the approximation error of BP, i.e., we provide a way to\ncompensate for BP errors via a consecutive BP-aware MCMC. Our framework is\nbased on the Loop Calculus (LC) approach which allows expressing the BP error\nas a sum of weighted generalized loops. Although the full series is\ncomputationally intractable, it is known that a truncated series, summing up\nall 2-regular loops, is computable in polynomial-time for planar pair-wise\nbinary GMs and it also provides a highly accurate approximation empirically.\nMotivated by this, we first propose a polynomial-time approximation MCMC scheme\nfor the truncated series of general (non-planar) pair-wise binary models. Our\nmain idea here is to use the Worm algorithm, known to provide fast mixing in\nother (related) problems, and then design an appropriate rejection scheme to\nsample 2-regular loops. Furthermore, we also design an efficient rejection-free\nMCMC scheme for approximating the full series. The main novelty underlying our\ndesign is in utilizing the concept of cycle basis, which provides an efficient\ndecomposition of the generalized loops. In essence, the proposed MCMC schemes\nrun on transformed GM built upon the non-trivial BP solution, and our\nexperiments show that this synthesis of BP and MCMC outperforms both direct\nMCMC and bare BP schemes.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 18:24:45 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 05:48:45 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 02:58:38 GMT"}, {"version": "v4", "created": "Wed, 9 Nov 2016 14:55:22 GMT"}, {"version": "v5", "created": "Mon, 21 Nov 2016 01:32:03 GMT"}, {"version": "v6", "created": "Mon, 11 May 2020 03:20:05 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1605.09046", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne\n  Morvan, Tamas Sarlos, Jamal Atif", "title": "TripleSpin - a generic compact paradigm for fast machine learning\n  computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 19:07:09 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 15:05:31 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Fagan", "Francois", ""], ["Gouy-Pailler", "Cedric", ""], ["Morvan", "Anne", ""], ["Sarlos", "Tamas", ""], ["Atif", "Jamal", ""]]}, {"id": "1605.09049", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski, Vikas Sindhwani", "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme for recycling Gaussian random vectors into structured\nmatrices to approximate various kernel functions in sublinear time via random\nembeddings. Our framework includes the Fastfood construction as a special case,\nbut also extends to Circulant, Toeplitz and Hankel matrices, and the broader\nfamily of structured matrices that are characterized by the concept of\nlow-displacement rank. We introduce notions of coherence and graph-theoretic\nstructural constants that control the approximation quality, and prove\nunbiasedness and low-variance properties of random feature maps that arise\nwithin our framework. For the case of low-displacement matrices, we show how\nthe degree of structure and randomness can be controlled to reduce statistical\nvariance at the cost of increased computation and storage requirements.\nEmpirical results strongly support our theory and justify the use of a broader\nfamily of structured matrices for scaling up kernel methods using random\nfeatures.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 19:21:22 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1605.09068", "submitter": "Michael Lash", "authors": "Michael T. Lash, Qihang Lin, W. Nick Street and Jennifer G. Robinson", "title": "A budget-constrained inverse classification framework for smooth\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse classification is the process of manipulating an instance such that\nit is more likely to conform to a specific class. Past methods that address\nsuch a problem have shortcomings. Greedy methods make changes that are overly\nradical, often relying on data that is strictly discrete. Other methods rely on\ncertain data points, the presence of which cannot be guaranteed. In this paper\nwe propose a general framework and method that overcomes these and other\nlimitations. The formulation of our method can use any differentiable\nclassification function. We demonstrate the method by using logistic regression\nand Gaussian kernel SVMs. We constrain the inverse classification to occur on\nfeatures that can actually be changed, each of which incurs an individual cost.\nWe further subject such changes to fall within a certain level of cumulative\nchange (budget). Our framework can also accommodate the estimation of\n(indirectly changeable) features whose values change as a consequence of\nactions taken. Furthermore, we propose two methods for specifying feature-value\nranges that result in different algorithmic behavior. We apply our method, and\na proposed sensitivity analysis-based benchmark method, to two freely available\ndatasets: Student Performance from the UCI Machine Learning Repository and a\nreal world cardiovascular disease dataset. The results obtained demonstrate the\nvalidity and benefits of our framework and method.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 21:50:25 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 22:30:53 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 18:27:39 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Lash", "Michael T.", ""], ["Lin", "Qihang", ""], ["Street", "W. Nick", ""], ["Robinson", "Jennifer G.", ""]]}, {"id": "1605.09080", "submitter": "Forough Arabshahi", "authors": "Forough Arabshahi, Animashree Anandkumar", "title": "Spectral Methods for Correlated Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose guaranteed spectral methods for learning a broad\nrange of topic models, which generalize the popular Latent Dirichlet Allocation\n(LDA). We overcome the limitation of LDA to incorporate arbitrary topic\ncorrelations, by assuming that the hidden topic proportions are drawn from a\nflexible class of Normalized Infinitely Divisible (NID) distributions. NID\ndistributions are generated through the process of normalizing a family of\nindependent Infinitely Divisible (ID) random variables. The Dirichlet\ndistribution is a special case obtained by normalizing a set of Gamma random\nvariables. We prove that this flexible topic model class can be learned via\nspectral methods using only moments up to the third order, with (low order)\npolynomial sample and computational complexity. The proof is based on a key new\ntechnique derived here that allows us to diagonalize the moments of the NID\ndistribution through an efficient procedure that requires evaluating only\nunivariate integrals, despite the fact that we are handling high dimensional\nmultivariate moments. In order to assess the performance of our proposed Latent\nNID topic model, we use two real datasets of articles collected from New York\nTimes and Pubmed. Our experiments yield improved perplexity on both datasets\ncompared with the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 00:32:11 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 14:30:11 GMT"}, {"version": "v3", "created": "Sun, 5 Jun 2016 08:27:34 GMT"}, {"version": "v4", "created": "Sat, 20 Aug 2016 01:44:30 GMT"}, {"version": "v5", "created": "Sun, 13 Nov 2016 20:24:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Arabshahi", "Forough", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1605.09085", "submitter": "Matthew Blaschko", "authors": "Amal Rannen Triki and Matthew B. Blaschko", "title": "Stochastic Function Norm Regularization of Deep Networks", "comments": "arXiv admin note: text overlap with arXiv:1710.06703", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have had an enormous impact on image analysis.\nState-of-the-art training methods, based on weight decay and DropOut, result in\nimpressive performance when a very large training set is available. However,\nthey tend to have large problems overfitting to small data sets. Indeed, the\navailable regularization methods deal with the complexity of the network\nfunction only indirectly. In this paper, we study the feasibility of directly\nusing the $L_2$ function norm for regularization. Two methods to integrate this\nnew regularization in the stochastic backpropagation are proposed. Moreover,\nthe convergence of these new algorithms is studied. We finally show that they\noutperform the state-of-the-art methods in the low sample regime on benchmark\ndatasets (MNIST and CIFAR10). The obtained results demonstrate very clear\nimprovement, especially in the context of small sample regimes with data laying\nin a low dimensional manifold. Source code of the method can be found at\n\\url{https://github.com/AmalRT/DNN_Reg}.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 01:49:18 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 14:14:30 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 14:38:32 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Triki", "Amal Rannen", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1605.09114", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Mehdi Alizadeh", "title": "ParMAC: distributed optimisation of nested functions, with application\n  to learning binary autoencoders", "comments": "40 pages, 13 figures. The abstract appearing here is slightly shorter\n  than the one in the PDF file because of the arXiv's limitation of the\n  abstract field to 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many powerful machine learning models are based on the composition of\nmultiple processing layers, such as deep nets, which gives rise to nonconvex\nobjective functions. A general, recent approach to optimise such \"nested\"\nfunctions is the method of auxiliary coordinates (MAC). MAC introduces an\nauxiliary coordinate for each data point in order to decouple the nested model\ninto independent submodels. This decomposes the optimisation into steps that\nalternate between training single layers and updating the coordinates. It has\nthe advantage that it reuses existing single-layer algorithms, introduces\nparallelism, and does not need to use chain-rule gradients, so it works with\nnondifferentiable layers. With large-scale problems, or when distributing the\ncomputation is necessary for faster training, the dataset may not fit in a\nsingle machine. It is then essential to limit the amount of communication\nbetween machines so it does not obliterate the benefit of parallelism. We\ndescribe a general way to achieve this, ParMAC. ParMAC works on a cluster of\nprocessing machines with a circular topology and alternates two steps until\nconvergence: one step trains the submodels in parallel using stochastic\nupdates, and the other trains the coordinates in parallel. Only submodel\nparameters, no data or coordinates, are ever communicated between machines.\nParMAC exhibits high parallelism, low communication overhead, and facilitates\ndata shuffling, load balancing, fault tolerance and streaming data processing.\nWe study the convergence of ParMAC and propose a theoretical model of its\nruntime and parallel speedup. We develop ParMAC to learn binary autoencoders\nfor fast, approximate image retrieval. We implement it in MPI in a distributed\nsystem and demonstrate nearly perfect speedups in a 128-processor cluster with\na training set of 100 million high-dimensional points.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 06:31:14 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Alizadeh", "Mehdi", ""]]}, {"id": "1605.09136", "submitter": "Gianni Franchi Gianni Franchi", "authors": "Gianni Franchi, Jesus Angulo, and Dino Sejdinovic", "title": "Hyperspectral Image Classification with Support Vector Machines on\n  Kernel Distribution Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for pixel classification in hyperspectral images,\nleveraging on both the spatial and spectral information in the data. The\nintroduced method relies on a recently proposed framework for learning on\ndistributions -- by representing them with mean elements in reproducing kernel\nHilbert spaces (RKHS) and formulating a classification algorithm therein. In\nparticular, we associate each pixel to an empirical distribution of its\nneighbouring pixels, a judicious representation of which in an RKHS, in\nconjunction with the spectral information contained in the pixel itself, give a\nnew explicit set of features that can be fed into a suite of standard\nclassification techniques -- we opt for a well-established framework of support\nvector machines (SVM). Furthermore, the computational complexity is reduced via\nrandom Fourier features formalism. We study the consistency and the convergence\nrates of the proposed method and the experiments demonstrate strong performance\non hyperspectral data with gains in comparison to the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 08:26:28 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Franchi", "Gianni", ""], ["Angulo", "Jesus", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1605.09196", "submitter": "S{\\o}ren Havelund Welling", "authors": "Soeren H. Welling, Hanne H.F. Refsgaard, Per B. Brockhoff, Line H.\n  Clemmensen", "title": "Forest Floor Visualizations of Random Forests", "comments": "25 pages, 12 figures, supplementary materials. v2->v3: minor\n  proofing, moderated comments on ICE-plots, replaced \\psi-operator with the\n  subset named H in equation 13 and 14 to improve simplicity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology, forest floor, to visualize and interpret\nrandom forest (RF) models. RF is a popular and useful tool for non-linear\nmulti-variate classification and regression, which yields a good trade-off\nbetween robustness (low variance) and adaptiveness (low bias). Direct\ninterpretation of a RF model is difficult, as the explicit ensemble model of\nhundreds of deep trees is complex. Nonetheless, it is possible to visualize a\nRF model fit by its mapping from feature space to prediction space. Hereby the\nuser is first presented with the overall geometrical shape of the model\nstructure, and when needed one can zoom in on local details. Dimensional\nreduction by projection is used to visualize high dimensional shapes. The\ntraditional method to visualize RF model structure, partial dependence plots,\nachieve this by averaging multiple parallel projections. We suggest to first\nuse feature contributions, a method to decompose trees by splitting features,\nand then subsequently perform projections. The advantages of forest floor over\npartial dependence plots is that interactions are not masked by averaging. As a\nconsequence, it is possible to locate interactions, which are not visualized in\na given projection. Furthermore, we introduce: a goodness-of-visualization\nmeasure, use of colour gradients to identify interactions and an out-of-bag\ncross validated variant of feature contributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 12:24:08 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 13:06:46 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 12:53:27 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Welling", "Soeren H.", ""], ["Refsgaard", "Hanne H. F.", ""], ["Brockhoff", "Per B.", ""], ["Clemmensen", "Line H.", ""]]}, {"id": "1605.09232", "submitter": "Raja Giryes", "authors": "Raja Giryes and Yonina C. Eldar and Alex M. Bronstein and Guillermo\n  Sapiro", "title": "Tradeoffs between Convergence Speed and Reconstruction Accuracy in\n  Inverse Problems", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems with iterative algorithms is popular, especially for\nlarge data. Due to time constraints, the number of possible iterations is\nusually limited, potentially affecting the achievable accuracy. Given an error\none is willing to tolerate, an important question is whether it is possible to\nmodify the original iterations to obtain faster convergence to a minimizer\nachieving the allowed error without increasing the computational cost of each\niteration considerably. Relying on recent recovery techniques developed for\nsettings in which the desired signal belongs to some low-dimensional set, we\nshow that using a coarse estimate of this set may lead to faster convergence at\nthe cost of an additional reconstruction error related to the accuracy of the\nset approximation. Our theory ties to recent advances in sparse recovery,\ncompressed sensing, and deep learning. Particularly, it may provide a possible\nexplanation to the successful approximation of the l1-minimization solution by\nneural networks with layers representing iterations, as practiced in the\nlearned iterative shrinkage-thresholding algorithm (LISTA).\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 13:43:59 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 17:43:20 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 10:53:57 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Giryes", "Raja", ""], ["Eldar", "Yonina C.", ""], ["Bronstein", "Alex M.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1605.09346", "submitter": "Jean-Baptiste Alayrac", "authors": "Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet K.\n  Dokania, Simon Lacoste-Julien", "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs", "comments": "Appears in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016). 31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose several improvements on the block-coordinate\nFrank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to\noptimize the structured support vector machine (SSVM) objective in the context\nof structured prediction, though it has wider applications. The key intuition\nbehind our improvements is that the estimates of block gaps maintained by BCFW\nreveal the block suboptimality that can be used as an adaptive criterion.\nFirst, we sample objects at each iteration of BCFW in an adaptive non-uniform\nway via gapbased sampling. Second, we incorporate pairwise and away-step\nvariants of Frank-Wolfe into the block-coordinate setting. Third, we cache\noracle calls with a cache-hit criterion based on the block gaps. Fourth, we\nprovide the first method to compute an approximate regularization path for\nSSVM. Finally, we provide an exhaustive empirical evaluation of all our methods\non four structured prediction datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 18:15:30 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Osokin", "Anton", ""], ["Alayrac", "Jean-Baptiste", ""], ["Lukasewitz", "Isabella", ""], ["Dokania", "Puneet K.", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1605.09370", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Tobias Bischoff, Pietro Perona, Frederick\n  Eberhardt", "title": "Unsupervised Discovery of El Nino Using Causal Feature Learning on\n  Microlevel Climate Data", "comments": "Accepted for plenary presentation at UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the climate phenomena of El Nino and La Nina arise naturally as\nstates of macro-variables when our recent causal feature learning framework\n(Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind\n(ZW) and sea surface temperatures (SST) taken over the equatorial band of the\nPacific Ocean. The method identifies these unusual climate states on the basis\nof the relation between ZW and SST patterns without any input about past\noccurrences of El Nino or La Nina. The simpler alternatives of (i) clustering\nthe SST fields while disregarding their relationship with ZW patterns, or (ii)\nclustering the joint ZW-SST patterns, do not discover El Nino. We discuss the\ndegree to which our method supports a causal interpretation and use a\nlow-dimensional toy example to explain its success over other clustering\napproaches. Finally, we propose a new robust and scalable alternative to our\noriginal algorithm (Chalupka 2016), which circumvents the need for\nhigh-dimensional density learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 19:57:56 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Bischoff", "Tobias", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1605.09459", "submitter": "Xiao Fu", "authors": "Xiao Fu, Kejun Huang, Mingyi Hong, Nicholas D. Sidiropoulos, Anthony\n  Man-Cho So", "title": "Scalable and Flexible Multiview MAX-VAR Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2698365", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized canonical correlation analysis (GCCA) aims at finding latent\nlow-dimensional common structure from multiple views (feature vectors in\ndifferent domains) of the same entities. Unlike principal component analysis\n(PCA) that handles a single view, (G)CCA is able to integrate information from\ndifferent feature spaces. Here we focus on MAX-VAR GCCA, a popular formulation\nwhich has recently gained renewed interest in multilingual processing and\nspeech modeling. The classic MAX-VAR GCCA problem can be solved optimally via\neigen-decomposition of a matrix that compounds the (whitened) correlation\nmatrices of the views; but this solution has serious scalability issues, and is\nnot directly amenable to incorporating pertinent structural constraints such as\nnon-negativity and sparsity on the canonical components. We posit regularized\nMAX-VAR GCCA as a non-convex optimization problem and propose an alternating\noptimization (AO)-based algorithm to handle it. Our algorithm alternates\nbetween {\\em inexact} solutions of a regularized least squares subproblem and a\nmanifold-constrained non-convex subproblem, thereby achieving substantial\nmemory and computational savings. An important benefit of our design is that it\ncan easily handle structure-promoting regularization. We show that the\nalgorithm globally converges to a critical point at a sublinear rate, and\napproaches a global optimal solution at a linear rate when no regularization is\nconsidered. Judiciously designed simulations and large-scale word embedding\ntasks are employed to showcase the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 01:01:52 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 16:20:21 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 14:56:25 GMT"}, {"version": "v4", "created": "Thu, 4 May 2017 21:19:48 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Hong", "Mingyi", ""], ["Sidiropoulos", "Nicholas D.", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1605.09466", "submitter": "Robert B. Gramacy", "authors": "Victor Picheny, Robert B. Gramacy, Stefan M. Wild, Sebastien Le\n  Digabel", "title": "Bayesian optimization under mixed constraints with a slack-variable\n  augmented Lagrangian", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An augmented Lagrangian (AL) can convert a constrained optimization problem\ninto a sequence of simpler (e.g., unconstrained) problems, which are then\nusually solved with local solvers. Recently, surrogate-based Bayesian\noptimization (BO) sub-solvers have been successfully deployed in the AL\nframework for a more global search in the presence of inequality constraints;\nhowever, a drawback was that expected improvement (EI) evaluations relied on\nMonte Carlo. Here we introduce an alternative slack variable AL, and show that\nin this formulation the EI may be evaluated with library routines. The slack\nvariables furthermore facilitate equality as well as inequality constraints,\nand mixtures thereof. We show how our new slack \"ALBO\" compares favorably to\nthe original. Its superiority over conventional alternatives is reinforced on\nseveral mixed constraint examples.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 02:14:18 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Picheny", "Victor", ""], ["Gramacy", "Robert B.", ""], ["Wild", "Stefan M.", ""], ["Digabel", "Sebastien Le", ""]]}, {"id": "1605.09477", "submitter": "Yin Zheng", "authors": "Yin Zheng, Bangsheng Tang, Wenkui Ding, Hanning Zhou", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "comments": "Accepted by ICML2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes CF-NADE, a neural autoregressive architecture for\ncollaborative filtering (CF) tasks, which is inspired by the Restricted\nBoltzmann Machine (RBM) based CF model and the Neural Autoregressive\nDistribution Estimator (NADE). We first describe the basic CF-NADE model for CF\ntasks. Then we propose to improve the model by sharing parameters between\ndifferent ratings. A factored version of CF-NADE is also proposed for better\nscalability. Furthermore, we take the ordinal nature of the preferences into\nconsideration and propose an ordinal cost to optimize CF-NADE, which shows\nsuperior performance. Finally, CF-NADE can be extended to a deep model, with\nonly moderately increased computational complexity. Experimental results show\nthat CF-NADE with a single hidden layer beats all previous state-of-the-art\nmethods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more\nhidden layers can further improve the performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 03:07:06 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Zheng", "Yin", ""], ["Tang", "Bangsheng", ""], ["Ding", "Wenkui", ""], ["Zhou", "Hanning", ""]]}, {"id": "1605.09499", "submitter": "Parameswaran Raman", "authors": "Jiong Zhang, Parameswaran Raman, Shihao Ji, Hsiang-Fu Yu, S.V.N.\n  Vishwanathan, Inderjit S. Dhillon", "title": "Extreme Stochastic Variational Inference: Distributed and Asynchronous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI), the state-of-the-art algorithm for\nscaling variational inference to large-datasets, is inherently serial.\nMoreover, it requires the parameters to fit in the memory of a single\nprocessor; this is problematic when the number of parameters is in billions. In\nthis paper, we propose extreme stochastic variational inference (ESVI), an\nasynchronous and lock-free algorithm to perform variational inference for\nmixture models on massive real world datasets. ESVI overcomes the limitations\nof SVI by requiring that each processor only access a subset of the data and a\nsubset of the parameters, thus providing data and model parallelism\nsimultaneously. We demonstrate the effectiveness of ESVI by running Latent\nDirichlet Allocation (LDA) on UMBC-3B, a dataset that has a vocabulary of 3\nmillion and a token size of 3 billion. In our experiments, we found that ESVI\nnot only outperforms VI and SVI in wallclock-time, but also achieves a better\nquality solution. In addition, we propose a strategy to speed up computation\nand save memory when fitting large number of topics.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 05:10:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 05:55:09 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 20:50:37 GMT"}, {"version": "v4", "created": "Fri, 21 Apr 2017 17:37:39 GMT"}, {"version": "v5", "created": "Tue, 23 May 2017 08:14:27 GMT"}, {"version": "v6", "created": "Thu, 15 Feb 2018 01:02:49 GMT"}, {"version": "v7", "created": "Wed, 18 Apr 2018 01:10:33 GMT"}, {"version": "v8", "created": "Mon, 21 May 2018 23:39:58 GMT"}, {"version": "v9", "created": "Fri, 3 Aug 2018 22:05:49 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhang", "Jiong", ""], ["Raman", "Parameswaran", ""], ["Ji", "Shihao", ""], ["Yu", "Hsiang-Fu", ""], ["Vishwanathan", "S. V. N.", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1605.09522", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard\n  Sch\\\"olkopf", "title": "Kernel Mean Embedding of Distributions: A Review and Beyond", "comments": "147 pages; this is the final version", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 10: No. 1-2, pp\n  1-141 (2017)", "doi": "10.1561/2200000060", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hilbert space embedding of a distribution---in short, a kernel mean\nembedding---has recently emerged as a powerful tool for machine learning and\ninference. The basic idea behind this framework is to map distributions into a\nreproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel\nmethods can be extended to probability measures. It can be viewed as a\ngeneralization of the original \"feature map\" common to support vector machines\n(SVMs) and other kernel methods. While initially closely associated with the\nlatter, it has meanwhile found application in fields ranging from kernel\nmachines and probabilistic modeling to statistical inference, causal discovery,\nand deep learning. The goal of this survey is to give a comprehensive review of\nexisting work and recent advances in this research area, and to discuss the\nmost challenging issues and open problems that could lead to new research\ndirections. The survey begins with a brief introduction to the RKHS and\npositive definite kernels which forms the backbone of this survey, followed by\na thorough discussion of the Hilbert space embedding of marginal distributions,\ntheoretical guarantees, and a review of its applications. The embedding of\ndistributions enables us to apply RKHS methods to probability measures which\nprompts a wide range of applications such as kernel two-sample testing,\nindependent testing, and learning on distributional data. Next, we discuss the\nHilbert space embedding for conditional distributions, give theoretical\ninsights, and review some applications. The conditional mean embedding enables\nus to perform sum, product, and Bayes' rules---which are ubiquitous in\ngraphical model, probabilistic inference, and reinforcement learning---in a\nnon-parametric way. We then discuss relationships between this framework and\nother related areas. Lastly, we give some suggestions on future research\ndirections.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 08:23:33 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 04:25:09 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 15:05:47 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 12:45:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Sriperumbudur", "Bharath", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1605.09593", "submitter": "Yasutoshi Ida", "authors": "Yasutoshi Ida, Yasuhiro Fujiwara, Sotetsu Iwamura", "title": "Adaptive Learning Rate via Covariance Matrix Based Preconditioning for\n  Deep Neural Networks", "comments": "Accepted at IJCAI 2017", "journal-ref": null, "doi": "10.24963/ijcai.2017/267", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning rate algorithms such as RMSProp are widely used for\ntraining deep neural networks. RMSProp offers efficient training since it uses\nfirst order gradients to approximate Hessian-based preconditioning. However,\nsince the first order gradients include noise caused by stochastic\noptimization, the approximation may be inaccurate. In this paper, we propose a\nnovel adaptive learning rate algorithm called SDProp. Its key idea is effective\nhandling of the noise by preconditioning based on covariance matrix. For\nvarious neural networks, our approach is more efficient and effective than\nRMSProp and its variant.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 12:11:51 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 10:24:50 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Ida", "Yasutoshi", ""], ["Fujiwara", "Yasuhiro", ""], ["Iwamura", "Sotetsu", ""]]}, {"id": "1605.09619", "submitter": "Mario Lucic", "authors": "Mario Lucic and Olivier Bachem and Morteza Zadimoghaddam and Andreas\n  Krause", "title": "Horizontally Scalable Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of large-scale machine learning problems can be cast as instances\nof constrained submodular maximization. Existing approaches for distributed\nsubmodular maximization have a critical drawback: The capacity - number of\ninstances that can fit in memory - must grow with the data set size. In\npractice, while one can provision many machines, the capacity of each machine\nis limited by physical constraints. We propose a truly scalable approach for\ndistributed submodular maximization under fixed capacity. The proposed\nframework applies to a broad class of algorithms and constraints and provides\ntheoretical guarantees on the approximation factor for any available capacity.\nWe empirically evaluate the proposed algorithm on a variety of data sets and\ndemonstrate that it achieves performance competitive with the centralized\ngreedy solution.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 13:18:30 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Lucic", "Mario", ""], ["Bachem", "Olivier", ""], ["Zadimoghaddam", "Morteza", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.09646", "submitter": "Quentin Berthet", "authors": "Tengyao Wang, Quentin Berthet, Yaniv Plan", "title": "Average-case Hardness of RIP Certification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted isometry property (RIP) for design matrices gives guarantees\nfor optimal recovery in sparse linear models. It is of high interest in\ncompressed sensing and statistical learning. This property is particularly\nimportant for computationally efficient recovery methods. As a consequence,\neven though it is in general NP-hard to check that RIP holds, there have been\nsubstantial efforts to find tractable proxies for it. These would allow the\nconstruction of RIP matrices and the polynomial-time verification of RIP given\nan arbitrary matrix. We consider the framework of average-case certifiers, that\nnever wrongly declare that a matrix is RIP, while being often correct for\nrandom instances. While there are such functions which are tractable in a\nsuboptimal parameter regime, we show that this is a computationally hard task\nin any better regime. Our results are based on a new, weaker assumption on the\nproblem of detecting dense subgraphs.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 14:38:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Wang", "Tengyao", ""], ["Berthet", "Quentin", ""], ["Plan", "Yaniv", ""]]}, {"id": "1605.09658", "submitter": "Edouard Duchesnay", "authors": "Fouad Hadj-Selem, Tommy Lofstedt, Elvis Dohmatob, Vincent Frouin,\n  Mathieu Dubois, Vincent Guillemot and Edouard Duchesnay", "title": "Continuation of Nesterov's Smoothing for Regression with Structured\n  Sparsity in High-Dimensional Neuroimaging", "comments": "11 pages, 6 figures, accepted in IEEE TMI, IEEE Transactions on\n  Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models can be used on high-dimensional brain images for diagnosis\nof a clinical condition. Spatial regularization through structured sparsity\noffers new perspectives in this context and reduces the risk of overfitting the\nmodel while providing interpretable neuroimaging signatures by forcing the\nsolution to adhere to domain-specific constraints. Total Variation (TV)\nenforces spatial smoothness of the solution while segmenting predictive regions\nfrom the background. We consider the problem of minimizing the sum of a smooth\nconvex loss, a non-smooth convex penalty (whose proximal operator is known) and\na wide range of possible complex, non-smooth convex structured penalties such\nas TV or overlapping group Lasso. Existing solvers are either limited in the\nfunctions they can minimize or in their practical capacity to scale to\nhigh-dimensional imaging data. Nesterov's smoothing technique can be used to\nminimize a large number of non-smooth convex structured penalties but\nreasonable precision requires a small smoothing parameter, which slows down the\nconvergence speed. To benefit from the versatility of Nesterov's smoothing\ntechnique, we propose a first order continuation algorithm, CONESTA, which\nautomatically generates a sequence of decreasing smoothing parameters. The\ngenerated sequence maintains the optimal convergence speed towards any globally\ndesired precision. Our main contributions are: To propose an expression of the\nduality gap to probe the current distance to the global optimum in order to\nadapt the smoothing parameter and the convergence speed. We provide a\nconvergence rate, which is an improvement over classical proximal gradient\nsmoothing methods. We demonstrate on both simulated and high-dimensional\nstructural neuroimaging data that CONESTA significantly outperforms many\nstate-of-the-art solvers in regard to convergence speed and precision.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:09:13 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 09:15:58 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 13:15:38 GMT"}, {"version": "v4", "created": "Wed, 23 Nov 2016 17:07:19 GMT"}, {"version": "v5", "created": "Mon, 12 Mar 2018 16:54:38 GMT"}, {"version": "v6", "created": "Sun, 22 Apr 2018 11:29:40 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Hadj-Selem", "Fouad", ""], ["Lofstedt", "Tommy", ""], ["Dohmatob", "Elvis", ""], ["Frouin", "Vincent", ""], ["Dubois", "Mathieu", ""], ["Guillemot", "Vincent", ""], ["Duchesnay", "Edouard", ""]]}, {"id": "1605.09674", "submitter": "Rein Houthooft", "authors": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck,\n  Pieter Abbeel", "title": "VIME: Variational Information Maximizing Exploration", "comments": "Published in Advances in Neural Information Processing Systems 29\n  (NIPS), pages 1109-1117", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and effective exploration remains a key challenge in reinforcement\nlearning (RL). While there are methods with optimality guarantees in the\nsetting of discrete state and action spaces, these methods cannot be applied in\nhigh-dimensional deep RL scenarios. As such, most contemporary RL relies on\nsimple heuristics such as epsilon-greedy exploration or adding Gaussian noise\nto the controls. This paper introduces Variational Information Maximizing\nExploration (VIME), an exploration strategy based on maximization of\ninformation gain about the agent's belief of environment dynamics. We propose a\npractical implementation, using variational inference in Bayesian neural\nnetworks which efficiently handles continuous state and action spaces. VIME\nmodifies the MDP reward function, and can be applied with several different\nunderlying RL algorithms. We demonstrate that VIME achieves significantly\nbetter performance compared to heuristic exploration methods across a variety\nof continuous control tasks and algorithms, including tasks with very sparse\nrewards.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:34:36 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 18:25:42 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 12:58:44 GMT"}, {"version": "v4", "created": "Fri, 27 Jan 2017 09:26:28 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Houthooft", "Rein", ""], ["Chen", "Xi", ""], ["Duan", "Yan", ""], ["Schulman", "John", ""], ["De Turck", "Filip", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1605.09721", "submitter": "Dimitris Papailiopoulos", "authors": "Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce\n  Zhang, Michael I. Jordan, Kannan Ramchandran, Chris Re, Benjamin Recht", "title": "CYCLADES: Conflict-free Asynchronous Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 17:15:01 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Pan", "Xinghao", ""], ["Lam", "Maximilian", ""], ["Tu", "Stephen", ""], ["Papailiopoulos", "Dimitris", ""], ["Zhang", "Ce", ""], ["Jordan", "Michael I.", ""], ["Ramchandran", "Kannan", ""], ["Re", "Chris", ""], ["Recht", "Benjamin", ""]]}, {"id": "1605.09735", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Keyan Ghazi-Zahedi, Nihat Ay", "title": "Information Theoretically Aided Reinforcement Learning for Embodied\n  Agents", "comments": "10 pages, 4 figures, 8 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning for embodied agents is a challenging problem. The\naccumulated reward to be optimized is often a very rugged function, and\ngradient methods are impaired by many local optimizers. We demonstrate, in an\nexperimental setting, that incorporating an intrinsic reward can smoothen the\noptimization landscape while preserving the global optimizers of interest. We\nshow that policy gradient optimization for locomotion in a complex morphology\nis significantly improved when supplementing the extrinsic reward by an\nintrinsic reward defined in terms of the mutual information of time consecutive\nsensor readings.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 17:30:54 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Montufar", "Guido", ""], ["Ghazi-Zahedi", "Keyan", ""], ["Ay", "Nihat", ""]]}, {"id": "1605.09774", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, Christopher R\\'e", "title": "Asynchrony begets Momentum, with an Application to Deep Learning", "comments": "Full version of a paper published in Annual Allerton Conference on\n  Communication, Control, and Computing (Allerton) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous methods are widely used in deep learning, but have limited\ntheoretical justification when applied to non-convex problems. We show that\nrunning stochastic gradient descent (SGD) in an asynchronous manner can be\nviewed as adding a momentum-like term to the SGD iteration. Our result does not\nassume convexity of the objective function, so it is applicable to deep\nlearning systems. We observe that a standard queuing model of asynchrony\nresults in a form of momentum that is commonly used by deep learning\npractitioners. This forges a link between queuing theory and asynchrony in deep\nlearning systems, which could be useful for systems builders. For convolutional\nneural networks, we experimentally validate that the degree of asynchrony\ndirectly correlates with the momentum, confirming our main result. An important\nimplication is that tuning the momentum parameter is important when considering\ndifferent levels of asynchrony. We assert that properly tuned momentum reduces\nthe number of steps required for convergence. Finally, our theory suggests new\nways of counteracting the adverse effects of asynchrony: a simple mechanism\nlike using negative algorithmic momentum can improve performance under high\nasynchrony. Since asynchronous methods have better hardware efficiency, this\nresult may shed light on when asynchronous execution is more efficient for deep\nlearning systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:16:56 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 12:00:28 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Zhang", "Ce", ""], ["Hadjis", "Stefan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1605.09782", "submitter": "Jeff Donahue", "authors": "Jeff Donahue, Philipp Kr\\\"ahenb\\\"uhl, Trevor Darrell", "title": "Adversarial Feature Learning", "comments": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to averaging predictions over 10 crops at test\n  time, as done in Noroozi & Favaro; Table 3 VOC classification results\n  slightly improved due to minor bugfix. (See v6 changelog for previous\n  versions.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:37:29 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 19:52:42 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 03:25:03 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 18:40:47 GMT"}, {"version": "v5", "created": "Fri, 6 Jan 2017 02:49:57 GMT"}, {"version": "v6", "created": "Mon, 9 Jan 2017 05:38:18 GMT"}, {"version": "v7", "created": "Mon, 3 Apr 2017 20:34:36 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Donahue", "Jeff", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""]]}]