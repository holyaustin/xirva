[{"id": "1802.00002", "submitter": "Abhishek Dubey", "authors": "Fangzhou sun and Abhishek Dubey and Jules White", "title": "DxNAT - Deep Neural Networks for Explaining Non-Recurring Traffic\n  Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-recurring traffic congestion is caused by temporary disruptions, such as\naccidents, sports games, adverse weather, etc. We use data related to real-time\ntraffic speed, jam factors (a traffic congestion indicator), and events\ncollected over a year from Nashville, TN to train a multi-layered deep neural\nnetwork. The traffic dataset contains over 900 million data records. The\nnetwork is thereafter used to classify the real-time data and identify\nanomalous operations. Compared with traditional approaches of using statistical\nor machine learning techniques, our model reaches an accuracy of 98.73 percent\nwhen identifying traffic congestion caused by football games. Our approach\nfirst encodes the traffic across a region as a scaled image. After that the\nimage data from different timestamps is fused with event- and time-related\ndata. Then a crossover operator is used as a data augmentation method to\ngenerate training datasets with more balanced classes. Finally, we use the\nreceiver operating characteristic (ROC) analysis to tune the sensitivity of the\nclassifier. We present the analysis of the training time and the inference time\nseparately.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 23:18:11 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["sun", "Fangzhou", ""], ["Dubey", "Abhishek", ""], ["White", "Jules", ""]]}, {"id": "1802.00008", "submitter": "Eric Metodiev", "authors": "Eric M. Metodiev, Jesse Thaler", "title": "On the Topic of Jets: Disentangling Quarks and Gluons at Colliders", "comments": "8 pages, 4 figures, 1 table. v2: Improved discussion to match PRL\n  version", "journal-ref": "Phys. Rev. Lett. 120, 241602 (2018)", "doi": "10.1103/PhysRevLett.120.241602", "report-no": "MIT-CTP 4979", "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce jet topics: a framework to identify underlying classes of jets\nfrom collider data. Because of a close mathematical relationship between\ndistributions of observables in jets and emergent themes in sets of documents,\nwe can apply recent techniques in \"topic modeling\" to extract jet topics from\ndata with minimal or no input from simulation or theory. As a proof of concept\nwith parton shower samples, we apply jet topics to determine separate quark and\ngluon jet distributions for constituent multiplicity. We also determine\nseparate quark and gluon rapidity spectra from a mixed Z-plus-jet sample. While\njet topics are defined directly from hadron-level multi-differential cross\nsections, one can also predict jet topics from first-principles theoretical\ncalculations, with potential implications for how to define quark and gluon\njets beyond leading-logarithmic accuracy. These investigations suggest that jet\ntopics will be useful for extracting underlying jet distributions and fractions\nin a wide range of contexts at the Large Hadron Collider.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:00:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 19:18:09 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Metodiev", "Eric M.", ""], ["Thaler", "Jesse", ""]]}, {"id": "1802.00030", "submitter": "Marcio Nicolau", "authors": "M\\'arcio Nicolau, M\\'arcia Barrocas Moreira Pimentel, Casiane Salete\n  Tibola, Jos\\'e Mauricio Cunha Fernandes, Willingthon Pavan", "title": "Fusarium Damaged Kernels Detection Using Transfer Learning on Deep\n  Neural Network Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work shows the application of transfer learning for a pre-trained\ndeep neural network (DNN), using a small image dataset ($\\approx$ 12,000) on a\nsingle workstation with enabled NVIDIA GPU card that takes up to 1 hour to\ncomplete the training task and archive an overall average accuracy of $94.7\\%$.\nThe DNN presents a $20\\%$ score of misclassification for an external test\ndataset. The accuracy of the proposed methodology is equivalent to ones using\nHSI methodology $(81\\%-91\\%)$ used for the same task, but with the advantage of\nbeing independent on special equipment to classify wheat kernel for FHB\nsymptoms.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:31:25 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Nicolau", "M\u00e1rcio", ""], ["Pimentel", "M\u00e1rcia Barrocas Moreira", ""], ["Tibola", "Casiane Salete", ""], ["Fernandes", "Jos\u00e9 Mauricio Cunha", ""], ["Pavan", "Willingthon", ""]]}, {"id": "1802.00043", "submitter": "Fredrik Hallgren", "authors": "Fredrik Hallgren and Paul Northrop", "title": "Incremental kernel PCA and the Nystr\\\"om method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental versions of batch algorithms are often desired, for increased\ntime efficiency in the streaming data setting, or increased memory efficiency\nin general. In this paper we present a novel algorithm for incremental kernel\nPCA, based on rank one updates to the eigendecomposition of the kernel matrix,\nwhich is more computationally efficient than comparable existing algorithms. We\nextend our algorithm to incremental calculation of the Nystr\\\"om approximation\nto the kernel matrix, the first such algorithm proposed. Incremental\ncalculation of the Nystr\\\"om approximation leads to further gains in memory\nefficiency, and allows for empirical evaluation of when a subset of sufficient\nsize has been obtained.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:57:26 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Hallgren", "Fredrik", ""], ["Northrop", "Paul", ""]]}, {"id": "1802.00045", "submitter": "Xiuming Liu", "authors": "Xiuming Liu, Dave Zachariah, Edith C. H. Ngai", "title": "Composite Gaussian Processes: Scalable Computation and Performance\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) models provide a powerful tool for prediction but are\ncomputationally prohibitive using large data sets. In such scenarios, one has\nto resort to approximate methods. We derive an approximation based on a\ncomposite likelihood approach using a general belief updating framework, which\nleads to a recursive computation of the predictor as well as of learning the\nhyper-parameters. We then provide an analysis of the derived composite GP model\nin predictive and information-theoretic terms. Finally, we evaluate the\napproximation with both synthetic data and a real-world application.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 20:01:37 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Xiuming", ""], ["Zachariah", "Dave", ""], ["Ngai", "Edith C. H.", ""]]}, {"id": "1802.00047", "submitter": "Rui Zhang", "authors": "Alexander Shapiro, Yao Xie, Rui Zhang", "title": "Matrix completion with deterministic pattern - a geometric perspective", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2885494", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the matrix completion problem with a deterministic pattern of\nobserved entries. In this setting, we aim to answer the question: under what\ncondition there will be (at least locally) unique solution to the matrix\ncompletion problem, i.e., the underlying true matrix is identifiable. We answer\nthe question from a certain point of view and outline a geometric perspective.\nWe give an algebraically verifiable sufficient condition, which we call the\nwell-posedness condition, for the local uniqueness of MRMC solutions. We argue\nthat this condition is necessary for local stability of MRMC solutions, and we\nshow that the condition is generic using the characteristic rank. We also argue\nthat the low-rank approximation approaches are more stable than MRMC and\nfurther propose a sequential statistical testing procedure to determine the\n\"true\" rank from observed entries. Finally, we provide numerical examples aimed\nat verifying validity of the presented theory.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 20:03:07 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 02:29:26 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 03:25:36 GMT"}, {"version": "v4", "created": "Wed, 29 Aug 2018 19:03:53 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Shapiro", "Alexander", ""], ["Xie", "Yao", ""], ["Zhang", "Rui", ""]]}, {"id": "1802.00086", "submitter": "Amartya Sanyal", "authors": "Amartya Sanyal, Pawan Kumar, Purushottam Kar, Sanjay Chawla, Fabrizio\n  Sebastiani", "title": "Optimizing Non-decomposable Measures with Deep Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5736-y", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of algorithms capable of directly training deep neural\nnetworks with respect to large families of task-specific performance measures\nsuch as the F-measure and the Kullback-Leibler divergence that are structured\nand non-decomposable. This presents a departure from standard deep learning\ntechniques that typically use squared or cross-entropy loss functions (that are\ndecomposable) to train neural networks. We demonstrate that directly training\nwith task-specific loss functions yields much faster and more stable\nconvergence across problems and datasets. Our proposed algorithms and\nimplementations have several novel features including (i) convergence to first\norder stationary points despite optimizing complex objective functions; (ii)\nuse of fewer training samples to achieve a desired level of convergence, (iii)\na substantial reduction in training time, and (iv) a seamless integration of\nour implementation into existing symbolic gradient frameworks. We implement our\ntechniques on a variety of deep architectures including multi-layer perceptrons\nand recurrent neural networks and show that on a variety of benchmark and real\ndata sets, our algorithms outperform traditional approaches to training deep\nnetworks, as well as some recent approaches to task-specific training of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 22:14:09 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Sanyal", "Amartya", ""], ["Kumar", "Pawan", ""], ["Kar", "Purushottam", ""], ["Chawla", "Sanjay", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1802.00123", "submitter": "Feng Li", "authors": "Feng Li, Yan Liu, Khidir Shaib Mohamed, Wei Wu", "title": "A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of\n  Multinomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural\nnetworks can provide more powerful mapping capability than the traditional\nfeedforward neural networks (Sigma-Sigma neural networks). In the existing\nliterature, in order to reduce the number of the Pi nodes in the Pi layer, a\nspecial multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with\nrespect to each particular variable sigma_i when the other variables are taken\nas constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with\nn>1 are not included. This choice may be somehow intuitive, but is not\nnecessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural\nnetwork (MSPSNN) with an adaptive approach to find a better multinomial for a\ngiven problem. To elaborate, we start from a complete multinomial with a given\norder. Then we employ a regularization technique in the learning process for\nthe given problem to reduce the number of monomials used in the multinomial,\nand end up with a new SPSNN involving the same number of monomials (= the\nnumber of nodes in the Pi-layer) as in P_s. Numerical experiments on some\nbenchmark problems show that our MSPSNN behaves better than the traditional\nSPSNN with P_s.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 01:55:11 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Li", "Feng", ""], ["Liu", "Yan", ""], ["Mohamed", "Khidir Shaib", ""], ["Wu", "Wei", ""]]}, {"id": "1802.00130", "submitter": "Yu-Hsiang Lin", "authors": "Chien-Chih Wang, Kent Loong Tan, Chun-Ting Chen, Yu-Hsiang Lin, S.\n  Sathiya Keerthi, Dhruv Mahajan, S. Sundararajan, Chih-Jen Lin", "title": "Distributed Newton Methods for Deep Neural Networks", "comments": "Supplementary materials and experimental code are available at\n  https://www.csie.ntu.edu.tw/~cjlin/papers/dnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning involves a difficult non-convex optimization problem with a\nlarge number of weights between any two adjacent layers of a deep structure. To\nhandle large data sets or complicated networks, distributed training is needed,\nbut the calculation of function, gradient, and Hessian is expensive. In\nparticular, the communication and the synchronization cost may become a\nbottleneck. In this paper, we focus on situations where the model is\ndistributedly stored, and propose a novel distributed Newton method for\ntraining deep neural networks. By variable and feature-wise data partitions,\nand some careful designs, we are able to explicitly use the Jacobian matrix for\nmatrix-vector products in the Newton method. Some techniques are incorporated\nto reduce the running time as well as the memory consumption. First, to reduce\nthe communication cost, we propose a diagonalization method such that an\napproximate Newton direction can be obtained without communication between\nmachines. Second, we consider subsampled Gauss-Newton matrices for reducing the\nrunning time as well as the communication cost. Third, to reduce the\nsynchronization cost, we terminate the process of finding an approximate Newton\ndirection even though some nodes have not finished their tasks. Details of some\nimplementation issues in distributed environments are thoroughly investigated.\nExperiments demonstrate that the proposed method is effective for the\ndistributed training of deep neural networks. In compared with stochastic\ngradient methods, it is more robust and may give better test accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 02:44:56 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Wang", "Chien-Chih", ""], ["Tan", "Kent Loong", ""], ["Chen", "Chun-Ting", ""], ["Lin", "Yu-Hsiang", ""], ["Keerthi", "S. Sathiya", ""], ["Mahajan", "Dhruv", ""], ["Sundararajan", "S.", ""], ["Lin", "Chih-Jen", ""]]}, {"id": "1802.00150", "submitter": "Chen Xu", "authors": "Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\n  Wang, Hongbin Zha", "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have achieved excellent performance in many\napplications. However, on portable devices with limited resources, the models\nare often too large to deploy. For applications on the server with large scale\nconcurrent requests, the latency during inference can also be very critical for\ncostly computing resources. In this work, we address these problems by\nquantizing the network, both weights and activations, into multiple binary\ncodes {-1,+1}. We formulate the quantization as an optimization problem. Under\nthe key observation that once the quantization coefficients are fixed the\nbinary codes can be derived efficiently by binary search tree, alternating\nminimization is then applied. We test the quantization for two well-known RNNs,\ni.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the\nlanguage models. Compared with the full-precision counter part, by 2-bit\nquantization we can achieve ~16x memory saving and ~6x real inference\nacceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit\nquantization, we can achieve almost no loss in the accuracy or even surpass the\noriginal model, with ~10.5x memory saving and ~3x real inference acceleration.\nBoth results beat the exiting quantization works with large margins. We extend\nour alternating quantization to image classification tasks. In both RNNs and\nfeedforward neural networks, the method also achieves excellent performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 04:06:39 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Xu", "Chen", ""], ["Yao", "Jianqiang", ""], ["Lin", "Zhouchen", ""], ["Ou", "Wenwu", ""], ["Cao", "Yuanbin", ""], ["Wang", "Zhirong", ""], ["Zha", "Hongbin", ""]]}, {"id": "1802.00168", "submitter": "Bao Wang", "authors": "Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi and Stanley J.\n  Osher", "title": "Deep Neural Nets with Interpolating Function as Output Activation", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replace the output layer of deep neural nets, typically the softmax\nfunction, by a novel interpolating function. And we propose end-to-end training\nand testing algorithms for this new architecture. Compared to classical neural\nnets with softmax function as output activation, the surrogate with\ninterpolating function as output activation combines advantages of both deep\nand manifold learning. The new framework demonstrates the following major\nadvantages: First, it is better applicable to the case with insufficient\ntraining data. Second, it significantly improves the generalization accuracy on\na wide variety of networks. The algorithm is implemented in PyTorch, and code\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 06:25:39 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 17:38:08 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 00:56:50 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Wang", "Bao", ""], ["Luo", "Xiyang", ""], ["Li", "Zhen", ""], ["Zhu", "Wei", ""], ["Shi", "Zuoqiang", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1802.00209", "submitter": "Ahmed Osman", "authors": "Ahmed Osman and Wojciech Samek", "title": "Dual Recurrent Attention Units for Visual Question Answering", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) requires AI models to comprehend data in two\ndomains, vision and text. Current state-of-the-art models use learned attention\nmechanisms to extract relevant information from the input domains to answer a\ncertain question. Thus, robust attention mechanisms are essential for powerful\nVQA models. In this paper, we propose a recurrent attention mechanism and show\nits benefits compared to the traditional convolutional approach. We perform two\nablation studies to evaluate recurrent attention. First, we introduce a\nbaseline VQA model with visual attention and test the performance difference\nbetween convolutional and recurrent attention on the VQA 2.0 dataset. Secondly,\nwe design an architecture for VQA which utilizes dual (textual and visual)\nRecurrent Attention Units (RAUs). Using this model, we show the effect of all\npossible combinations of recurrent and convolutional dual attention. Our single\nmodel outperforms the first place winner on the VQA 2016 challenge and to the\nbest of our knowledge, it is the second best performing single model on the VQA\n1.0 dataset. Furthermore, our model noticeably improves upon the winner of the\nVQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in\nstate-of-the-art models with our RAUs and show increased performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 09:35:33 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 16:27:26 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 13:41:21 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Osman", "Ahmed", ""], ["Samek", "Wojciech", ""]]}, {"id": "1802.00243", "submitter": "Ray-Bing Chen", "authors": "Hsiang-Ling Hsu, Yuan-Chin Ivan Chang, Ray-Bing Chen", "title": "Greedy Active Learning Algorithm for Logistic Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a logistic model-based active learning procedure for binary\nclassification problems, in which we adopt a batch subject selection strategy\nwith a modified sequential experimental design method. Moreover, accompanying\nthe proposed subject selection scheme, we simultaneously conduct a greedy\nvariable selection procedure such that we can update the classification model\nwith all labeled training subjects. The proposed algorithm repeatedly performs\nboth subject and variable selection steps until a prefixed stopping criterion\nis reached. Our numerical results show that the proposed procedure has\ncompetitive performance, with smaller training size and a more compact model,\ncomparing with that of the classifier trained with all variables and a full\ndata set. We also apply the proposed procedure to a well-known wave data set\n(Breiman et al., 1984) to confirm the performance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 11:12:14 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Hsu", "Hsiang-Ling", ""], ["Chang", "Yuan-Chin Ivan", ""], ["Chen", "Ray-Bing", ""]]}, {"id": "1802.00255", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Yusaku Imai", "title": "A Nonparametric Delayed Feedback Model for Conversion Rate Prediction", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting conversion rates (CVRs) in display advertising (e.g., predicting\nthe proportion of users who purchase an item (i.e., a conversion) after its\ncorresponding ad is clicked) is important when measuring the effects of ads\nshown to users and to understanding the interests of the users. There is\ngenerally a time delay (i.e., so-called {\\it delayed feedback}) between the ad\nclick and conversion. Owing to the delayed feedback, samples that are converted\nafter an observation period may be treated as negative. To overcome this\ndrawback, CVR prediction assuming that the time delay follows an exponential\ndistribution has been proposed. In practice, however, there is no guarantee\nthat the delay is generated from the exponential distribution, and the best\ndistribution with which to represent the delay depends on the data. In this\npaper, we propose a nonparametric delayed feedback model for CVR prediction\nthat represents the distribution of the time delay without assuming a\nparametric distribution, such as an exponential or Weibull distribution.\nBecause the distribution of the time delay is modeled depending on the content\nof an ad and the features of a user, various shapes of the distribution can be\nrepresented potentially. In experiments, we show that the proposed model can\ncapture the distribution for the time delay on a synthetic dataset, even when\nthe distribution is complicated. Moreover, on a real dataset, we show that the\nproposed model outperforms the existing method that assumes an exponential\ndistribution for the time delay in terms of conversion rate prediction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:01:23 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Imai", "Yusaku", ""]]}, {"id": "1802.00280", "submitter": "Gael Sent\\'is", "authors": "Gael Sent\\'is, Esteban Mart\\'inez-Vargas, and Ramon Mu\\~noz-Tapia", "title": "Online optimal exact identification of a quantum change point", "comments": null, "journal-ref": "Phys. Rev. A 98, 052305 (2018)", "doi": "10.1103/PhysRevA.98.052305", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online detection strategies for identifying a change point in a\nstream of quantum particles allegedly prepared in identical states. We show\nthat the identification of the change point can be done without error via\nsequential local measurements while attaining the optimal performance bound set\nby quantum mechanics. In this way, we establish the task of exactly identifying\na quantum change point as an instance where local protocols are as powerful as\nglobal ones. The optimal online detection strategy requires only one bit of\nmemory between subsequent measurements, and it is amenable to experimental\nrealization with current technology.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 13:39:53 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 11:35:53 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Sent\u00eds", "Gael", ""], ["Mart\u00ednez-Vargas", "Esteban", ""], ["Mu\u00f1oz-Tapia", "Ramon", ""]]}, {"id": "1802.00324", "submitter": "Nhien-An Le-Khac", "authors": "Nga Nguyen Thi, Van Loi Cao, Nhien-An Le-Khac", "title": "One-class Collective Anomaly Detection based on Long Short-Term Memory\n  Recurrent Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.09752", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection for computer network systems has been becoming one of the\nmost critical tasks for network administrators today. It has an important role\nfor organizations, governments and our society due to the valuable resources\nhosted on computer networks. Traditional misuse detection strategies are unable\nto detect new and unknown intrusion types. In contrast, anomaly detection in\nnetwork security aims to distinguish between illegal or malicious events and\nnormal behavior of network systems. Anomaly detection can be considered as a\nclassification problem where it builds models of normal network behavior, of\nwhich it uses to detect new patterns that significantly deviate from the model.\nMost of the current approaches on anomaly detection is based on the learning of\nnormal behavior and anomalous actions. They do not include memory that is they\ndo not take into account previous events classify new ones. In this paper, we\npropose a one class collective anomaly detection model based on neural network\nlearning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN)\nis trained only on normal data, and it is capable of predicting several time\nsteps ahead of an input. In our approach, a LSTM RNN is trained on normal time\nseries data before performing a prediction for each time step. Instead of\nconsidering each time-step separately, the observation of prediction errors\nfrom a certain number of time-steps is now proposed as a new idea for detecting\ncollective anomalies. The prediction errors of a certain number of the latest\ntime-steps above a threshold will indicate a collective anomaly. The model is\nevaluated on a time series version of the KDD 1999 dataset. The experiments\ndemonstrate that the proposed model is capable to detect collective anomaly\nefficiently\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 10:38:07 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Thi", "Nga Nguyen", ""], ["Cao", "Van Loi", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "1802.00382", "submitter": "Amitabha Karmakar", "authors": "Amitabha Karmakar", "title": "Classifying medical notes into standard disease codes using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the automatic classification of patient discharge notes into\nstandard disease labels. We find that Convolutional Neural Networks with\nAttention outperform previous algorithms used in this task, and suggest further\nareas for improvement.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 16:46:00 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Karmakar", "Amitabha", ""]]}, {"id": "1802.00430", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Mung Chiang, Christoph Studer", "title": "Linearized Binary Regression", "comments": "To be presented at CISS (http://ee-ciss.princeton.edu/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probit regression was first proposed by Bliss in 1934 to study mortality\nrates of insects. Since then, an extensive body of work has analyzed and used\nprobit or related binary regression methods (such as logistic regression) in\nnumerous applications and fields. This paper provides a fresh angle to such\nwell-established binary regression methods. Concretely, we demonstrate that\nlinearizing the probit model in combination with linear estimators performs on\npar with state-of-the-art nonlinear regression methods, such as posterior mean\nor maximum aposteriori estimation, for a broad range of real-world regression\nproblems. We derive exact, closed-form, and nonasymptotic expressions for the\nmean-squared error of our linearized estimators, which clearly separates them\nfrom nonlinear regression methods that are typically difficult to analyze. We\nshowcase the efficacy of our methods and results for a number of synthetic and\nreal-world datasets, which demonstrates that linearized binary regression finds\npotential use in a variety of inference, estimation, signal processing, and\nmachine learning applications that deal with binary-valued observations or\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 18:49:14 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Lan", "Andrew S.", ""], ["Chiang", "Mung", ""], ["Studer", "Christoph", ""]]}, {"id": "1802.00459", "submitter": "Peilin Zhong", "authors": "Wei Hu, Zhao Song, Lin F. Yang, Peilin Zhong", "title": "Nearly Optimal Dynamic $k$-Means Clustering for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the $k$-means clustering problem in the dynamic streaming\nsetting, where points from a discrete Euclidean space $\\{1, 2, \\ldots,\n\\Delta\\}^d$ can be dynamically inserted to or deleted from the dataset. For\nthis problem, we provide a one-pass coreset construction algorithm using space\n$\\tilde{O}(k\\cdot \\mathrm{poly}(d, \\log\\Delta))$, where $k$ is the target\nnumber of centers. To our knowledge, this is the first dynamic geometric data\nstream algorithm for $k$-means using space polynomial in dimension and nearly\noptimal (linear) in $k$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 19:07:51 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 18:47:20 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Hu", "Wei", ""], ["Song", "Zhao", ""], ["Yang", "Lin F.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1802.00474", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep (Deep) Mukhopadhyay and Douglas Fletcher", "title": "Bayesian Modeling via Goodness-of-fit", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two key issues of modern Bayesian statistics are: (i) establishing\nprincipled approach for distilling statistical prior that is consistent with\nthe given data from an initial believable scientific prior; and (ii)\ndevelopment of a Bayes-frequentist consolidated data analysis workflow that is\nmore effective than either of the two separately. In this paper, we propose the\nidea of \"Bayes via goodness of fit\" as a framework for exploring these\nfundamental questions, in a way that is general enough to embrace almost all of\nthe familiar probability models. Several illustrative examples show the benefit\nof this new point of view as a practical data analysis tool. Relationship with\nother Bayesian cultures is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 20:15:31 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 03:08:44 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 22:28:39 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Subhadeep", "", "", "Deep"], ["Mukhopadhyay", "", ""], ["Fletcher", "Douglas", ""]]}, {"id": "1802.00530", "submitter": "Phillip Alexander Jang", "authors": "Phillip A. Jang, Andrew E. Loeb, Matthew B. Davidow, Andrew Gordon\n  Wilson", "title": "Scalable L\\'evy Process Priors for Spectral Kernel Learning", "comments": "Appears in Advances in Neural Information Processing Systems 30\n  (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are rich distributions over functions, with generalization\nproperties determined by a kernel function. When used for long-range\nextrapolation, predictions are particularly sensitive to the choice of kernel\nparameters. It is therefore critical to account for kernel uncertainty in our\npredictive distributions. We propose a distribution over kernels formed by\nmodelling a spectral mixture density with a L\\'evy process. The resulting\ndistribution has support for all stationary covariances--including the popular\nRBF, periodic, and Mat\\'ern kernels--combined with inductive biases which\nenable automatic and data efficient learning, long-range extrapolation, and\nstate of the art predictive performance. The proposed model also presents an\napproach to spectral regularization, as the L\\'evy process introduces a\nsparsity-inducing prior over mixture components, allowing automatic selection\nover model order and pruning of extraneous components. We exploit the algebraic\nstructure of the proposed process for $\\mathcal{O}(n)$ training and\n$\\mathcal{O}(1)$ predictions. We perform extrapolations having reasonable\nuncertainty estimates on several benchmarks, show that the proposed model can\nrecover flexible ground truth covariances and that it is robust to errors in\ninitialization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 01:40:46 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Jang", "Phillip A.", ""], ["Loeb", "Andrew E.", ""], ["Davidow", "Matthew B.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1802.00541", "submitter": "Brian Ruttenberg", "authors": "Michael Harradon, Jeff Druce, Brian Ruttenberg", "title": "Causal Learning and Explanation of Deep Neural Networks via Autoencoded\n  Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are complex and opaque. As they enter application in a\nvariety of important and safety critical domains, users seek methods to explain\ntheir output predictions. We develop an approach to explaining deep neural\nnetworks by constructing causal models on salient concepts contained in a CNN.\nWe develop methods to extract salient concepts throughout a target network by\nusing autoencoders trained to extract human-understandable representations of\nnetwork activations. We then build a bayesian causal model using these\nextracted concepts as variables in order to explain image classification.\nFinally, we use this causal model to identify and visualize features with\nsignificant causal influence on final classification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 02:24:24 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Harradon", "Michael", ""], ["Druce", "Jeff", ""], ["Ruttenberg", "Brian", ""]]}, {"id": "1802.00543", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik, Monica Agrawal, Jure Leskovec", "title": "Modeling polypharmacy side effects with graph convolutional networks", "comments": "Presented at ISMB 2018", "journal-ref": "Bioinformatics, 34:13, 457-466, 2018", "doi": "10.1093/bioinformatics/bty294", "report-no": null, "categories": "cs.LG q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of drug combinations, termed polypharmacy, is common to treat\npatients with complex diseases and co-existing conditions. However, a major\nconsequence of polypharmacy is a much higher risk of adverse side effects for\nthe patient. Polypharmacy side effects emerge because of drug-drug\ninteractions, in which activity of one drug may change if taken with another\ndrug. The knowledge of drug interactions is limited because these complex\nrelationships are rare, and are usually not observed in relatively small\nclinical testing. Discovering polypharmacy side effects thus remains an\nimportant challenge with significant implications for patient mortality. Here,\nwe present Decagon, an approach for modeling polypharmacy side effects. The\napproach constructs a multimodal graph of protein-protein interactions,\ndrug-protein target interactions, and the polypharmacy side effects, which are\nrepresented as drug-drug interactions, where each side effect is an edge of a\ndifferent type. Decagon is developed specifically to handle such multimodal\ngraphs with a large number of edge types. Our approach develops a new graph\nconvolutional neural network for multirelational link prediction in multimodal\nnetworks. Decagon predicts the exact side effect, if any, through which a given\ndrug combination manifests clinically. Decagon accurately predicts polypharmacy\nside effects, outperforming baselines by up to 69%. We find that it\nautomatically learns representations of side effects indicative of\nco-occurrence of polypharmacy in patients. Furthermore, Decagon models\nparticularly well side effects with a strong molecular basis, while on\npredominantly non-molecular side effects, it achieves good performance because\nof effective sharing of model parameters across edge types. Decagon creates\nopportunities to use large pharmacogenomic and patient data to flag and\nprioritize side effects for follow-up analysis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 02:43:29 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 21:39:44 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zitnik", "Marinka", ""], ["Agrawal", "Monica", ""], ["Leskovec", "Jure", ""]]}, {"id": "1802.00560", "submitter": "Xuan Liu", "authors": "Xuan Liu, Xiaoguang Wang, Stan Matwin", "title": "Interpretable Deep Convolutional Neural Networks via Meta-learning", "comments": "9 pages, 9 figures, 2018 International Joint Conference on Neural\n  Networks, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretability is a requirement in many applications in which crucial\ndecisions are made by users relying on a model's outputs. The recent movement\nfor \"algorithmic fairness\" also stipulates explainability, and therefore\ninterpretability of learning models. And yet the most successful contemporary\nMachine Learning approaches, the Deep Neural Networks, produce models that are\nhighly non-interpretable. We attempt to address this challenge by proposing a\ntechnique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)\nvia meta-learning. In this work, we interpret a specific hidden layer of the\ndeep CNN model on the MNIST image dataset. We use a clustering algorithm in a\ntwo-level structure to find the meta-level training data and Random Forest as\nbase learning algorithms to generate the meta-level test data. The\ninterpretation results are displayed visually via diagrams, which clearly\nindicates how a specific test instance is classified. Our method achieves\nglobal interpretation for all the test instances without sacrificing the\naccuracy obtained by the original deep CNN model. This means our model is\nfaithful to the deep CNN model, which leads to reliable interpretations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 05:09:10 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 03:20:07 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Liu", "Xuan", ""], ["Wang", "Xiaoguang", ""], ["Matwin", "Stan", ""]]}, {"id": "1802.00568", "submitter": "Andrea Montanari", "authors": "Behrooz Ghorbani, Hamid Javadi, Andrea Montanari", "title": "An Instability in Variational Inference for Topic Models", "comments": "69 pages; 18 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are Bayesian models that are frequently used to capture the\nlatent structure of certain corpora of documents or images. Each data element\nin such a corpus (for instance each item in a collection of scientific\narticles) is regarded as a convex combination of a small number of vectors\ncorresponding to `topics' or `components'. The weights are assumed to have a\nDirichlet prior distribution. The standard approach towards approximating the\nposterior is to use variational inference algorithms, and in particular a mean\nfield approximation.\n  We show that this approach suffers from an instability that can produce\nmisleading conclusions. Namely, for certain regimes of the model parameters,\nvariational inference outputs a non-trivial decomposition into topics. However\n--for the same parameter values-- the data contain no actual information about\nthe true decomposition, and hence the output of the algorithm is uncorrelated\nwith the true topic decomposition. Among other consequences, the estimated\nposterior mean is significantly wrong, and estimated Bayesian credible regions\ndo not achieve the nominal coverage. We discuss how this instability is\nremedied by more accurate mean field approximations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 05:52:48 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Ghorbani", "Behrooz", ""], ["Javadi", "Hamid", ""], ["Montanari", "Andrea", ""]]}, {"id": "1802.00662", "submitter": "Thai Hung Le", "authors": "Hung Le, Truyen Tran and Svetha Venkatesh", "title": "Dual Memory Neural Computer for Asynchronous Two-view Sequential\n  Learning", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core tasks in multi-view learning is to capture relations among\nviews. For sequential data, the relations not only span across views, but also\nextend throughout the view length to form long-term intra-view and inter-view\ninteractions. In this paper, we present a new memory augmented neural network\nmodel that aims to model these complex interactions between two asynchronous\nsequential views. Our model uses two encoders for reading from and writing to\ntwo external memories for encoding input views. The intra-view interactions and\nthe long-term dependencies are captured by the use of memories during this\nencoding process. There are two modes of memory accessing in our system:\nlate-fusion and early-fusion, corresponding to late and early inter-view\ninteractions. In the late-fusion mode, the two memories are separated,\ncontaining only view-specific contents. In the early-fusion mode, the two\nmemories share the same addressing space, allowing cross-memory accessing. In\nboth cases, the knowledge from the memories will be combined by a decoder to\nmake predictions over the output space. The resulting dual memory neural\ncomputer is demonstrated on a comprehensive set of experiments, including a\nsynthetic task of summing two sequences and the tasks of drug prescription and\ndisease progression in healthcare. The results demonstrate competitive\nperformance over both traditional algorithms and deep learning methods designed\nfor multi-view problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 12:32:29 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 03:19:38 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Le", "Hung", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1802.00680", "submitter": "William Wilkinson", "authors": "William J. Wilkinson, Joshua D. Reiss, Dan Stowell", "title": "A Generative Model for Natural Sounds Based on Latent Force Modelling", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in analysis of subband amplitude envelopes of natural sounds\nhave resulted in convincing synthesis, showing subband amplitudes to be a\ncrucial component of perception. Probabilistic latent variable analysis is\nparticularly revealing, but existing approaches don't incorporate prior\nknowledge about the physical behaviour of amplitude envelopes, such as\nexponential decay and feedback. We use latent force modelling, a probabilistic\nlearning paradigm that incorporates physical knowledge into Gaussian process\nregression, to model correlation across spectral subband envelopes. We augment\nthe standard latent force model approach by explicitly modelling correlations\nover multiple time steps. Incorporating this prior knowledge strengthens the\ninterpretation of the latent functions as the source that generated the signal.\nWe examine this interpretation via an experiment which shows that sounds\ngenerated by sampling from our probabilistic model are perceived to be more\nrealistic than those generated by similar models based on nonnegative matrix\nfactorisation, even in cases where our model is outperformed from a\nreconstruction error perspective.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 13:34:46 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 16:03:38 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Wilkinson", "William J.", ""], ["Reiss", "Joshua D.", ""], ["Stowell", "Dan", ""]]}, {"id": "1802.00748", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio", "title": "Short-term Memory of Deep RNN", "comments": "This is a pre-print (pre-review) version of the paper accepted for\n  presentation at the 26th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN), Bruges (Belgium),\n  25-27 April 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of deep learning towards temporal data processing is gaining an\nincreasing research interest. In this paper we investigate the properties of\nstate dynamics developed in successive levels of deep recurrent neural networks\n(RNNs) in terms of short-term memory abilities. Our results reveal interesting\ninsights that shed light on the nature of layering as a factor of RNN design.\nNoticeably, higher layers in a hierarchically organized RNN architecture\nresults to be inherently biased towards longer memory spans even prior to\ntraining of the recurrent connections. Moreover, in the context of Reservoir\nComputing framework, our analysis also points out the benefit of a layered\nrecurrent organization as an efficient approach to improve the memory skills of\nreservoir models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 16:14:59 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Gallicchio", "Claudio", ""]]}, {"id": "1802.00822", "submitter": "Ruizhe Cai", "authors": "Ruizhe Cai, Ao Ren, Ning Liu, Caiwen Ding, Luhao Wang, Xuehai Qian,\n  Massoud Pedram, Yanzhi Wang", "title": "VIBNN: Hardware Acceleration of Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3173162.3173212", "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Neural Networks (BNNs) have been proposed to address the problem of\nmodel uncertainty in training and inference. By introducing weights associated\nwith conditioned probability distributions, BNNs are capable of resolving the\noverfitting issue commonly seen in conventional neural networks and allow for\nsmall-data training, through the variational inference process. Frequent usage\nof Gaussian random variables in this process requires a properly optimized\nGaussian Random Number Generator (GRNG). The high hardware cost of conventional\nGRNG makes the hardware implementation of BNNs challenging.\n  In this paper, we propose VIBNN, an FPGA-based hardware accelerator design\nfor variational inference on BNNs. We explore the design space for massive\namount of Gaussian variable sampling tasks in BNNs. Specifically, we introduce\ntwo high performance Gaussian (pseudo) random number generators: the RAM-based\nLinear Feedback Gaussian Random Number Generator (RLF-GRNG), which is inspired\nby the properties of binomial distribution and linear feedback logics; and the\nBayesian Neural Network-oriented Wallace Gaussian Random Number Generator. To\nachieve high scalability and efficient memory access, we propose a deep\npipelined accelerator architecture with fast execution and good hardware\nutilization. Experimental results demonstrate that the proposed VIBNN\nimplementations on an FPGA can achieve throughput of 321,543.4 Images/s and\nenergy efficiency upto 52,694.8 Images/J while maintaining similar accuracy as\nits software counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 19:26:59 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Cai", "Ruizhe", ""], ["Ren", "Ao", ""], ["Liu", "Ning", ""], ["Ding", "Caiwen", ""], ["Wang", "Luhao", ""], ["Qian", "Xuehai", ""], ["Pedram", "Massoud", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1802.00850", "submitter": "Rohit  Tripathy", "authors": "Rohit Tripathy and Ilias Bilionis", "title": "Deep UQ: Learning deep neural network surrogate models for high\n  dimensional uncertainty quantification", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.08.036", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art computer codes for simulating real physical systems are\noften characterized by a vast number of input parameters. Performing\nuncertainty quantification (UQ) tasks with Monte Carlo (MC) methods is almost\nalways infeasible because of the need to perform hundreds of thousands or even\nmillions of forward model evaluations in order to obtain convergent statistics.\nOne, thus, tries to construct a cheap-to-evaluate surrogate model to replace\nthe forward model solver. For systems with large numbers of input parameters,\none has to deal with the curse of dimensionality - the exponential increase in\nthe volume of the input space, as the number of parameters increases linearly.\nIn this work, we demonstrate the use of deep neural networks (DNN) to construct\nsurrogate models for numerical simulators. We parameterize the structure of the\nDNN in a manner that lends the DNN surrogate the interpretation of recovering a\nlow dimensional nonlinear manifold. The model response is a parameterized\nnonlinear function of the low dimensional projections of the input. We think of\nthis low dimensional manifold as a nonlinear generalization of the notion of\nthe active subspace. Our approach is demonstrated with a problem on uncertainty\npropagation in a stochastic elliptic partial differential equation (SPDE) with\nuncertain diffusion coefficient. We deviate from traditional formulations of\nthe SPDE problem by not imposing a specific covariance structure on the random\ndiffusion coefficient. Instead, we attempt to solve a more challenging problem\nof learning a map between an arbitrary snapshot of the diffusion field and the\nresponse.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 21:20:17 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Tripathy", "Rohit", ""], ["Bilionis", "Ilias", ""]]}, {"id": "1802.00868", "submitter": "Yize Chen", "authors": "Yize Chen, Pan Li, Baosen Zhang", "title": "Bayesian Renewables Scenario Generation via Deep Generative Networks", "comments": "Paper accepted to Annual Conference on Information Sciences and\n  Systems (CISS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to generate renewable scenarios using Bayesian\nprobabilities by implementing the Bayesian generative adversarial\nnetwork~(Bayesian GAN), which is a variant of generative adversarial networks\nbased on two interconnected deep neural networks. By using a Bayesian\nformulation, generators can be constructed and trained to produce scenarios\nthat capture different salient modes in the data, allowing for better diversity\nand more accurate representation of the underlying physical process. Compared\nto conventional statistical models that are often hard to scale or sample from,\nthis method is model-free and can generate samples extremely efficiently. For\nvalidation, we use wind and solar times-series data from NREL integration data\nsets to train the Bayesian GAN. We demonstrate that proposed method is able to\ngenerate clusters of wind scenarios with different variance and mean value, and\nis able to distinguish and generate wind and solar scenarios simultaneously\neven if the historical data are intentionally mixed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 22:24:17 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chen", "Yize", ""], ["Li", "Pan", ""], ["Zhang", "Baosen", ""]]}, {"id": "1802.00891", "submitter": "Rui Xia", "authors": "Huihui He, Rui Xia", "title": "Joint Binary Neural Network for Multi-label Learning with Applications\n  to Emotion Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the deep learning techniques have achieved success in multi-label\nclassification due to its automatic representation learning ability and the\nend-to-end learning framework. Existing deep neural networks in multi-label\nclassification can be divided into two kinds: binary relevance neural network\n(BRNN) and threshold dependent neural network (TDNN). However, the former needs\nto train a set of isolate binary networks which ignore dependencies between\nlabels and have heavy computational load, while the latter needs an additional\nthreshold function mechanism to transform the multi-class probabilities to\nmulti-label outputs. In this paper, we propose a joint binary neural network\n(JBNN), to address these shortcomings. In JBNN, the representation of the text\nis fed to a set of logistic functions instead of a softmax function, and the\nmultiple binary classifications are carried out synchronously in one neural\nnetwork framework. Moreover, the relations between labels are captured via\ntraining on a joint binary cross entropy (JBCE) loss. To better meet\nmulti-label emotion classification, we further proposed to incorporate the\nprior label relations into the JBCE loss. The experimental results on the\nbenchmark dataset show that our model performs significantly better than the\nstate-of-the-art multi-label emotion classification methods, in both\nclassification performance and computational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 01:42:32 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["He", "Huihui", ""], ["Xia", "Rui", ""]]}, {"id": "1802.00912", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Jae Y. Shin, Suryakanth R. Gurudu, Michael B. Gotway,\n  Jianming Liang", "title": "Active, Continual Fine Tuning of Convolutional Neural Networks for\n  Reducing Annotation Efforts", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2021.101997", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The splendid success of convolutional neural networks (CNNs) in computer\nvision is largely attributable to the availability of massive annotated\ndatasets, such as ImageNet and Places. However, in medical imaging, it is\nchallenging to create such large annotated datasets, as annotating medical\nimages is not only tedious, laborious, and time consuming, but it also demands\ncostly, specialty-oriented skills, which are not easily accessible. To\ndramatically reduce annotation cost, this paper presents a novel method to\nnaturally integrate active learning and transfer learning (fine-tuning) into a\nsingle framework, which starts directly with a pre-trained CNN to seek \"worthy\"\nsamples for annotation and gradually enhances the (fine-tuned) CNN via\ncontinual fine-tuning. We have evaluated our method using three distinct\nmedical imaging applications, demonstrating that it can reduce annotation\nefforts by at least half compared with random selection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 05:01:17 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:13:28 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 18:03:48 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 00:19:51 GMT"}, {"version": "v5", "created": "Sat, 10 Apr 2021 22:38:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Zongwei", ""], ["Shin", "Jae Y.", ""], ["Gurudu", "Suryakanth R.", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1802.00924", "submitter": "Paul Pu Liang", "authors": "Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltru\\v{s}aitis, Amir\n  Zadeh, Louis-Philippe Morency", "title": "Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement\n  Learning", "comments": "ICMI 2017 Oral Presentation, Honorable Mention Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of video sharing websites such as YouTube and\nFacebook, multimodal sentiment analysis has received increasing attention from\nthe scientific community. Contrary to previous works in multimodal sentiment\nanalysis which focus on holistic information in speech segments such as bag of\nwords representations and average facial expression intensity, we develop a\nnovel deep architecture for multimodal sentiment analysis that performs\nmodality fusion at the word level. In this paper, we propose the Gated\nMultimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is\ncomposed of 2 modules. The Gated Multimodal Embedding alleviates the\ndifficulties of fusion when there are noisy modalities. The LSTM with Temporal\nAttention performs word level fusion at a finer fusion resolution between input\nmodalities and attends to the most important time steps. As a result, the\nGME-LSTM(A) is able to better model the multimodal structure of speech through\ntime and perform better sentiment comprehension. We demonstrate the\neffectiveness of this approach on the publicly-available Multimodal Corpus of\nSentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving\nstate-of-the-art sentiment classification and regression results. Qualitative\nanalysis on our model emphasizes the importance of the Temporal Attention Layer\nin sentiment prediction because the additional acoustic and visual modalities\nare noisy. We also demonstrate the effectiveness of the Gated Multimodal\nEmbedding in selectively filtering these noisy modalities out. Our results and\nanalysis open new areas in the study of sentiment analysis in human\ncommunication and provide new models for multimodal fusion.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 06:30:09 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chen", "Minghai", ""], ["Wang", "Sen", ""], ["Liang", "Paul Pu", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1802.00926", "submitter": "I-Hsiang Wang", "authors": "I Chien, Chung-Yi Lin, and I-Hsiang Wang", "title": "On the Minimax Misclassification Ratio of Hypergraph Community Detection", "comments": "Submitted to IEEE Transactions on Information Theory. Parts of this\n  paper was presented at ISIT 2017 and to appear at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in hypergraphs is explored. Under a generative hypergraph\nmodel called \"d-wise hypergraph stochastic block model\" (d-hSBM) which\nnaturally extends the Stochastic Block Model from graphs to d-uniform\nhypergraphs, the asymptotic minimax mismatch ratio is characterized. For\nproving the achievability, we propose a two-step polynomial time algorithm that\nachieves the fundamental limit. The first step of the algorithm is a hypergraph\nspectral clustering method which achieves partial recovery to a certain\nprecision level. The second step is a local refinement method which leverages\nthe underlying probabilistic model along with parameter estimation from the\noutcome of the first step. To characterize the asymptotic performance of the\nproposed algorithm, we first derive a sufficient condition for attaining weak\nconsistency in the hypergraph spectral clustering step. Then, under the\nguarantee of weak consistency in the first step, we upper bound the worst-case\nrisk attained in the local refinement step by an exponentially decaying\nfunction of the size of the hypergraph and characterize the decaying rate. For\nproving the converse, the lower bound of the minimax mismatch ratio is set by\nfinding a smaller parameter space which contains the most dominant error\nevents, inspired by the analysis in the achievability part. It turns out that\nthe minimax mismatch ratio decays exponentially fast to zero as the number of\nnodes tends to infinity, and the rate function is a weighted combination of\nseveral divergence terms, each of which is the Renyi divergence of order 1/2\nbetween two Bernoulli's. The Bernoulli's involved in the characterization of\nthe rate function are those governing the random instantiation of hyperedges in\nd-hSBM. Experimental results on synthetic data validate our theoretical finding\nthat the refinement step is critical in achieving the optimal statistical\nlimit.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 06:35:23 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chien", "I", ""], ["Lin", "Chung-Yi", ""], ["Wang", "I-Hsiang", ""]]}, {"id": "1802.00934", "submitter": "Mohammad Asif Khan", "authors": "Agustinus Kristiadi, Mohammad Asif Khan, Denis Lukovnikov, Jens\n  Lehmann, Asja Fischer", "title": "Incorporating Literals into Knowledge Graph Embeddings", "comments": "9 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs, on top of entities and their relationships, contain other\nimportant elements: literals. Literals encode interesting properties (e.g. the\nheight) of entities that are not captured by links between entities alone. Most\nof the existing work on embedding (or latent feature) based knowledge graph\nanalysis focuses mainly on the relations between entities. In this work, we\nstudy the effect of incorporating literal information into existing link\nprediction methods. Our approach, which we name LiteralE, is an extension that\ncan be plugged into existing latent feature methods. LiteralE merges entity\nembeddings with their literal information using a learnable, parametrized\nfunction, such as a simple linear or nonlinear transformation, or a multilayer\nneural network. We extend several popular embedding models based on LiteralE\nand evaluate their performance on the task of link prediction. Despite its\nsimplicity, LiteralE proves to be an effective way to incorporate literal\ninformation into existing embedding based methods, improving their performance\non different standard datasets, which we augmented with their literals and\nprovide as testbed for further research.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 08:16:31 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 10:11:23 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 14:39:54 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kristiadi", "Agustinus", ""], ["Khan", "Mohammad Asif", ""], ["Lukovnikov", "Denis", ""], ["Lehmann", "Jens", ""], ["Fischer", "Asja", ""]]}, {"id": "1802.00981", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, Irina Rish", "title": "Contextual Bandit with Adaptive Feature Extraction", "comments": "IEEE ICDMW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online decision making setting known as contextual bandit\nproblem, and propose an approach for improving contextual bandit performance by\nusing an adaptive feature extraction (representation learning) based on online\nclustering. Our approach starts with an off-line pre-training on unlabeled\nhistory of contexts (which can be exploited by our approach, but not by the\nstandard contextual bandit), followed by an online selection and adaptation of\nencoders. Specifically, given an input sample (context), the proposed approach\nselects the most appropriate encoding function to extract a feature vector\nwhich becomes an input for a contextual bandit, and updates both the bandit and\nthe encoding function based on the context and on the feedback (reward). Our\nexperiments on a variety of datasets, and both in stationary and non-stationary\nenvironments of several kinds demonstrate clear advantages of the proposed\nadaptive representation learning over the standard contextual bandit based on\n\"raw\" input contexts.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 14:44:51 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 19:30:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 01:28:57 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2020 15:32:13 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""], ["Rish", "Irina", ""]]}, {"id": "1802.01034", "submitter": "Himani Arora", "authors": "Himani Arora, Rajath Kumar, Jason Krone, Chong Li", "title": "Multi-task Learning for Continuous Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reliable and effective multi-task learning is a prerequisite for the\ndevelopment of robotic agents that can quickly learn to accomplish related,\neveryday tasks. However, in the reinforcement learning domain, multi-task\nlearning has not exhibited the same level of success as in other domains, such\nas computer vision. In addition, most reinforcement learning research on\nmulti-task learning has been focused on discrete action spaces, which are not\nused for robotic control in the real-world. In this work, we apply multi-task\nlearning methods to continuous action spaces and benchmark their performance on\na series of simulated continuous control tasks. Most notably, we show that\nmulti-task learning outperforms our baselines and alternative knowledge sharing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 21:20:03 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Arora", "Himani", ""], ["Kumar", "Rajath", ""], ["Krone", "Jason", ""], ["Li", "Chong", ""]]}, {"id": "1802.01053", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Nitin Viswanathan", "title": "Using Poisson Binomial GLMs to Reveal Voter Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modeling technique for solving the problem of ecological\ninference, in which individual-level associations are inferred from labeled\ndata available only at the aggregate level. We model aggregate count data as\narising from the Poisson binomial, the distribution of the sum of independent\nbut not identically distributed Bernoulli random variables. We relate\nindividual-level probabilities to individual covariates using both a logistic\nregression and a neural network. A normal approximation is derived via the\nLyapunov Central Limit Theorem, allowing us to efficiently fit these models on\nlarge datasets. We apply this technique to the problem of revealing voter\npreferences in the 2016 presidential election, fitting a model to a sample of\nover four million voters from the highly contested swing state of Pennsylvania.\nWe validate the model at the precinct level via a holdout set, and at the\nindividual level using weak labels, finding that the model is predictive and it\nlearns intuitively reasonable associations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 00:55:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rosenman", "Evan", ""], ["Viswanathan", "Nitin", ""]]}, {"id": "1802.01059", "submitter": "Naveen Sai Madiraju", "authors": "Naveen Sai Madiraju, Seid M. Sadat, Dimitry Fisher, Homa Karimabadi", "title": "Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain\n  Features", "comments": "11 pages, 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of time series data, also known as temporal clustering,\nis a challenging problem in machine learning. Here we propose a novel\nalgorithm, Deep Temporal Clustering (DTC), to naturally integrate\ndimensionality reduction and temporal clustering into a single end-to-end\nlearning framework, fully unsupervised. The algorithm utilizes an autoencoder\nfor temporal dimensionality reduction and a novel temporal clustering layer for\ncluster assignment. Then it jointly optimizes the clustering objective and the\ndimensionality reduction objec tive. Based on requirement and application, the\ntemporal clustering layer can be customized with any temporal similarity\nmetric. Several similarity metrics and state-of-the-art algorithms are\nconsidered and compared. To gain insight into temporal features that the\nnetwork has learned for its clustering, we apply a visualization method that\ngenerates a region of interest heatmap for the time series. The viability of\nthe algorithm is demonstrated using time series data from diverse domains,\nranging from earthquakes to spacecraft sensor data. In each case, we show that\nthe proposed algorithm outperforms traditional methods. The superior\nperformance is attributed to the fully integrated temporal dimensionality\nreduction and clustering criterion.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 02:18:25 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Madiraju", "Naveen Sai", ""], ["Sadat", "Seid M.", ""], ["Fisher", "Dimitry", ""], ["Karimabadi", "Homa", ""]]}, {"id": "1802.01071", "submitter": "Mohamed Ishmael Belghazi", "authors": "Mohamed Ishmael Belghazi, Sai Rajeswar, Olivier Mastropietro, Negar\n  Rostamzadeh, Jovana Mitrovic and Aaron Courville", "title": "Hierarchical Adversarially Learned Inference", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hierarchical generative model with a simple Markovian\nstructure and a corresponding inference model. Both the generative and\ninference model are trained using the adversarial learning paradigm. We\ndemonstrate that the hierarchical structure supports the learning of\nprogressively more abstract representations as well as providing semantically\nmeaningful reconstructions with different levels of fidelity. Furthermore, we\nshow that minimizing the Jensen-Shanon divergence between the generative and\ninference network is enough to minimize the reconstruction error. The resulting\nsemantically meaningful hierarchical latent structure discovery is exemplified\non the CelebA dataset. There, we show that the features learned by our model in\nan unsupervised way outperform the best handcrafted features. Furthermore, the\nextracted features remain competitive when compared to several recent deep\nsupervised approaches on an attribute prediction task on CelebA. Finally, we\nleverage the model's inference network to achieve state-of-the-art performance\non a semi-supervised variant of the MNIST digit classification task.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 04:49:18 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Belghazi", "Mohamed Ishmael", ""], ["Rajeswar", "Sai", ""], ["Mastropietro", "Olivier", ""], ["Rostamzadeh", "Negar", ""], ["Mitrovic", "Jovana", ""], ["Courville", "Aaron", ""]]}, {"id": "1802.01152", "submitter": "Andrew Blumberg", "authors": "Andrew J. Blumberg, Prithwish Bhaumik, Stephen G. Walker", "title": "Testing to distinguish measures on metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distinguishing between two distributions on a metric\nspace; i.e., given metric measure spaces $({\\mathbb X}, d, \\mu_1)$ and\n$({\\mathbb X}, d, \\mu_2)$, we are interested in the problem of determining from\nfinite data whether or not $\\mu_1$ is $\\mu_2$. The key is to use pairwise\ndistances between observations and, employing a reconstruction theorem of\nGromov, we can perform such a test using a two sample Kolmogorov--Smirnov test.\nA real analysis using phylogenetic trees and flu data is presented.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 16:25:53 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Blumberg", "Andrew J.", ""], ["Bhaumik", "Prithwish", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1802.01212", "submitter": "Jose Manuel Zorrilla Matilla", "authors": "Arushi Gupta, Jos\\'e Manuel Zorrilla Matilla, Daniel Hsu, Zolt\\'an\n  Haiman", "title": "Non-Gaussian information from weak lensing data via deep learning", "comments": "15 pages, 13 figures, accepted to PRD", "journal-ref": "Phys. Rev. D 97, 103515 (2018)", "doi": "10.1103/PhysRevD.97.103515", "report-no": null, "categories": "astro-ph.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak lensing maps contain information beyond two-point statistics on small\nscales. Much recent work has tried to extract this information through a range\nof different observables or via nonlinear transformations of the lensing field.\nHere we train and apply a 2D convolutional neural network to simulated\nnoiseless lensing maps covering 96 different cosmological models over a range\nof {$\\Omega_m,\\sigma_8$}. Using the area of the confidence contour in the\n{$\\Omega_m,\\sigma_8$} plane as a figure-of-merit, derived from simulated\nconvergence maps smoothed on a scale of 1.0 arcmin, we show that the neural\nnetwork yields $\\approx 5 \\times$ tighter constraints than the power spectrum,\nand $\\approx 4 \\times$ tighter than the lensing peaks. Such gains illustrate\nthe extent to which weak lensing data encode cosmological information not\naccessible to the power spectrum or even other, non-Gaussian statistics such as\nlensing peaks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 22:40:17 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 02:58:07 GMT"}, {"version": "v3", "created": "Tue, 1 May 2018 10:43:32 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Gupta", "Arushi", ""], ["Matilla", "Jos\u00e9 Manuel Zorrilla", ""], ["Hsu", "Daniel", ""], ["Haiman", "Zolt\u00e1n", ""]]}, {"id": "1802.01223", "submitter": "Samet Oymak", "authors": "Samet Oymak", "title": "Learning Compact Neural Networks with Regularization", "comments": "ICML 2018, 46 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper regularization is critical for speeding up training, improving\ngeneralization performance, and learning compact models that are cost\nefficient. We propose and analyze regularized gradient descent algorithms for\nlearning shallow neural networks. Our framework is general and covers\nweight-sharing (convolutional networks), sparsity (network pruning), and\nlow-rank constraints among others. We first introduce covering dimension to\nquantify the complexity of the constraint set and provide insights on the\ngeneralization properties. Then, we show that proposed algorithms become\nwell-behaved and local linear convergence occurs once the amount of data\nexceeds the covering dimension. Overall, our results demonstrate that\nnear-optimal sample complexity is sufficient for efficient learning and\nillustrate how regularization can be beneficial to learn over-parameterized\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 00:22:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 23:28:22 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Oymak", "Samet", ""]]}, {"id": "1802.01239", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang", "title": "Counting and Sampling from Markov Equivalent DAGs Using Clique Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed acyclic graph (DAG) is the most common graphical model for\nrepresenting causal relationships among a set of variables. When restricted to\nusing only observational data, the structure of the ground truth DAG is\nidentifiable only up to Markov equivalence, based on conditional independence\nrelations among the variables. Therefore, the number of DAGs equivalent to the\nground truth DAG is an indicator of the causal complexity of the underlying\nstructure--roughly speaking, it shows how many interventions or how much\nadditional information is further needed to recover the underlying DAG. In this\npaper, we propose a new technique for counting the number of DAGs in a Markov\nequivalence class. Our approach is based on the clique tree representation of\nchordal graphs. We show that in the case of bounded degree graphs, the proposed\nalgorithm is polynomial time. We further demonstrate that this technique can be\nutilized for uniform sampling from a Markov equivalence class, which provides a\nstochastic way to enumerate DAGs in the equivalence class and may be needed for\nfinding the best DAG or for causal inference given the equivalence class as\ninput. We also extend our counting and sampling method to the case where prior\nknowledge about the underlying DAG is available, and present applications of\nthis extension in causal experiment design and estimating the causal effect of\njoint interventions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:32:05 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 01:49:04 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1802.01240", "submitter": "Rakesh Katuwal", "authors": "Rakesh Katuwal and P.N. Suganthan", "title": "Enhancing Multi-Class Classification of Random Forest using Random\n  Vector Functional Neural Network and Oblique Decision Surfaces", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both neural networks and decision trees are popular machine learning methods\nand are widely used to solve problems from diverse domains. These two\nclassifiers are commonly used base classifiers in an ensemble framework. In\nthis paper, we first present a new variant of oblique decision tree based on a\nlinear classifier, then construct an ensemble classifier based on the fusion of\na fast neural network, random vector functional link network and oblique\ndecision trees. Random Vector Functional Link Network has an elegant closed\nform solution with extremely short training time. The neural network partitions\neach training bag (obtained using bagging) at the root level into C subsets\nwhere C is the number of classes in the dataset and subsequently, C oblique\ndecision trees are trained on such partitions. The proposed method provides a\nrich insight into the data by grouping the confusing or hard to classify\nsamples for each class and thus, provides an opportunity to employ fine-grained\nclassification rule over the data. The performance of the ensemble classifier\nis evaluated on several multi-class datasets where it demonstrates a superior\nperformance compared to other state-of- the-art classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:42:39 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Katuwal", "Rakesh", ""], ["Suganthan", "P. N.", ""]]}, {"id": "1802.01267", "submitter": "Yohei Kikuta", "authors": "Kazuma Arino, Yohei Kikuta", "title": "ClassSim: Similarity between Classes Defined by Misclassification Ratios\n  of Trained Classifiers", "comments": "15 pages, 2 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved exceptional performances in many\ntasks, particularly, in supervised classification tasks. However, achievements\nwith supervised classification tasks are based on large datasets with\nwell-separated classes. Typically, real-world applications involve wild\ndatasets that include similar classes; thus, evaluating similarities between\nclasses and understanding relations among classes are important. To address\nthis issue, a similarity metric, ClassSim, based on the misclassification\nratios of trained DNNs is proposed herein. We conducted image recognition\nexperiments to demonstrate that the proposed method provides better\nsimilarities compared with existing methods and is useful for classification\nproblems. Source code including all experimental results is available at\nhttps://github.com/karino2/ClassSim/.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 05:00:35 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Arino", "Kazuma", ""], ["Kikuta", "Yohei", ""]]}, {"id": "1802.01284", "submitter": "Maya Kabkab", "authors": "Maya Kabkab, Pouya Samangouei, Rama Chellappa", "title": "Task-Aware Compressed Sensing with Generative Adversarial Networks", "comments": "Accepted for publication at the Thirty-Second AAAI Conference on\n  Artificial Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural network approaches have been widely adopted for\nmachine learning tasks, with applications in computer vision. More recently,\nunsupervised generative models based on neural networks have been successfully\napplied to model data distributions via low-dimensional latent spaces. In this\npaper, we use Generative Adversarial Networks (GANs) to impose structure in\ncompressed sensing problems, replacing the usual sparsity constraint. We\npropose to train the GANs in a task-aware fashion, specifically for\nreconstruction tasks. We also show that it is possible to train our model\nwithout using any (or much) non-compressed data. Finally, we show that the\nlatent space of the GAN carries discriminative information and can further be\nregularized to generate input features for general inference tasks. We\ndemonstrate the effectiveness of our method on a variety of reconstruction and\nclassification problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 06:56:55 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Kabkab", "Maya", ""], ["Samangouei", "Pouya", ""], ["Chellappa", "Rama", ""]]}, {"id": "1802.01301", "submitter": "Kajsa M{\\o}llersen", "authors": "Kajsa M{\\o}llersen, Maciel Zortea, Thomas R. Schopf, Herbert\n  Kirchesch, Fred Godtliebsen", "title": "Comparison of computer systems and ranking criteria for automatic\n  melanoma detection in dermoscopic images", "comments": null, "journal-ref": "PLoS ONE 12(12): e0190112, 2017", "doi": "10.1371/journal.pone.0190112", "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Melanoma is the deadliest form of skin cancer. Computer systems can assist in\nmelanoma detection, but are not widespread in clinical practice. In 2016, an\nopen challenge in classification of dermoscopic images of skin lesions was\nannounced. A training set of 900 images with corresponding class labels and\nsemi-automatic/manual segmentation masks was released for the challenge. An\nindependent test set of 379 images was used to rank the participants. This\narticle demonstrates the impact of ranking criteria, segmentation method and\nclassifier, and highlights the clinical perspective. We compare five different\nmeasures for diagnostic accuracy by analysing the resulting ranking of the\ncomputer systems in the challenge. Choice of performance measure had great\nimpact on the ranking. Systems that were ranked among the top three for one\nmeasure, dropped to the bottom half when changing performance measure. Nevus\nDoctor, a computer system previously developed by the authors, was used to\ninvestigate the impact of segmentation and classifier. The unexpected small\nimpact of automatic versus semi-automatic/manual segmentation suggests that\nimprovements of the automatic segmentation method w.r.t. resemblance to\nsemi-automatic/manual segmentation will not improve diagnostic accuracy\nsubstantially. A small set of similar classification algorithms are used to\ninvestigate the impact of classifier on the diagnostic accuracy. The\nvariability in diagnostic accuracy for different classifier algorithms was\nlarger than the variability for segmentation methods, and suggests a focus for\nfuture investigations. From a clinical perspective, the misclassification of a\nmelanoma as benign has far greater cost than the misclassification of a benign\nlesion. For computer systems to have clinical impact, their performance should\nbe ranked by a high-sensitivity measure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 08:37:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["M\u00f8llersen", "Kajsa", ""], ["Zortea", "Maciel", ""], ["Schopf", "Thomas R.", ""], ["Kirchesch", "Herbert", ""], ["Godtliebsen", "Fred", ""]]}, {"id": "1802.01334", "submitter": "Manuel Morante", "authors": "Manuel Morante and Yannis Kopsinis and Sergios Theodoridis and\n  Athanassios Protopapas", "title": "Information Assisted Dictionary Learning for fMRI data analysis", "comments": "46 pages, 19 figures, Complete Study of the IADL algorithm on both\n  synthetic and real fMRI data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the task-related fMRI problem is treated in its matrix\nfactorization formulation, focused on the Dictionary Learning (DL) approach.\nThe new method allows the incorporation of a priori knowledge associated both\nwith the experimental design as well as with available brain Atlases. Moreover,\nthe proposed method can efficiently cope with uncertainties related to the HRF\nmodeling. In addition, the proposed method bypasses one of the major drawbacks\nthat are associated with DL methods; that is, the selection of the\nsparsity-related regularization parameters. In our formulation, an alternative\nsparsity promoting constraint is employed, that bears a direct relation to the\nnumber of voxels in the spatial maps. Hence, the related parameters can be\ntuned using information that is available from brain atlases. The proposed\nmethod is evaluated against several other popular techniques, including GLM.\nThe obtained performance gains are reported via a novel realistic synthetic\nfMRI dataset as well as real data that are related to a challenging\nexperimental design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 10:27:47 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 16:21:21 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 16:21:16 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Morante", "Manuel", ""], ["Kopsinis", "Yannis", ""], ["Theodoridis", "Sergios", ""], ["Protopapas", "Athanassios", ""]]}, {"id": "1802.01396", "submitter": "Siyuan Ma", "authors": "Mikhail Belkin, Siyuan Ma, Soumik Mandal", "title": "To understand deep learning we need to understand kernel learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization performance of classifiers in deep learning has recently\nbecome a subject of intense study. Deep models, typically over-parametrized,\ntend to fit the training data exactly. Despite this \"overfitting\", they perform\nwell on test data, a phenomenon not yet fully understood.\n  The first point of our paper is that strong performance of overfitted\nclassifiers is not a unique feature of deep learning. Using six real-world and\ntwo synthetic datasets, we establish experimentally that kernel machines\ntrained to have zero classification or near zero regression error perform very\nwell on test data, even when the labels are corrupted with a high level of\nnoise. We proceed to give a lower bound on the norm of zero loss solutions for\nsmooth kernels, showing that they increase nearly exponentially with data size.\nWe point out that this is difficult to reconcile with the existing\ngeneralization bounds. Moreover, none of the bounds produce non-trivial results\nfor interpolating solutions.\n  Second, we show experimentally that (non-smooth) Laplacian kernels easily fit\nrandom labels, a finding that parallels results for ReLU neural networks. In\ncontrast, fitting noisy data requires many more epochs for smooth Gaussian\nkernels. Similar performance of overfitted Laplacian and Gaussian classifiers\non test, suggests that generalization is tied to the properties of the kernel\nfunction rather than the optimization process.\n  Certain key phenomena of deep learning are manifested similarly in kernel\nmethods in the modern \"overfitted\" regime. The combination of the experimental\nand theoretical results presented in this paper indicates a need for new\ntheoretical ideas for understanding properties of classical kernel methods. We\nargue that progress on understanding deep learning will be difficult until more\ntractable \"shallow\" kernel methods are better understood.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:14:28 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:19:04 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 21:25:54 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Belkin", "Mikhail", ""], ["Ma", "Siyuan", ""], ["Mandal", "Soumik", ""]]}, {"id": "1802.01415", "submitter": "Mohammed Hadi", "authors": "Mohammed S. Hadi, Ahmed Q. Lawey, Taisir E. H. El-Gorashi, and Jaafar\n  M. H. Elmirghani", "title": "Big Data Analytics for Wireless and Wired Network Design: A Survey", "comments": "23 pages, 4 figures, 2 tables, Journal paper accepted for publication\n  at Elsevier Computer Networks Journal", "journal-ref": null, "doi": "10.1016/j.comnet.2018.01.016", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the world is witnessing a mounting avalanche of data due to the\nincreasing number of mobile network subscribers, Internet websites, and online\nservices. This trend is continuing to develop in a quick and diverse manner in\nthe form of big data. Big data analytics can process large amounts of raw data\nand extract useful, smaller-sized information, which can be used by different\nparties to make reliable decisions. In this paper, we conduct a survey on the\nrole that big data analytics can play in the design of data communication\nnetworks. Integrating the latest advances that employ big data analytics with\nthe networks control/traffic layers might be the best way to build robust data\ncommunication networks with refined performance and intelligent features.\nFirst, the survey starts with the introduction of the big data basic concepts,\nframework, and characteristics. Second, we illustrate the main network design\ncycle employing big data analytics. This cycle represents the umbrella concept\nthat unifies the surveyed topics. Third, there is a detailed review of the\ncurrent academic and industrial efforts toward network design using big data\nanalytics. Forth, we identify the challenges confronting the utilization of big\ndata analytics in network design. Finally, we highlight several future research\ndirections. To the best of our knowledge, this is the first survey that\naddresses the use of big data analytics techniques for the design of a broad\nrange of networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 16:35:48 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Hadi", "Mohammed S.", ""], ["Lawey", "Ahmed Q.", ""], ["El-Gorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "1802.01421", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel, Yann Ollivier, L\\'eon Bottou, Bernhard\n  Sch\\\"olkopf, David Lopez-Paz", "title": "First-order Adversarial Vulnerability of Neural Networks and Input\n  Dimension", "comments": "Paper previously called: \"Adversarial Vulnerability of Neural\n  Networks Increases with Input Dimension\". 9 pages main text and references,\n  11 pages appendix, 14 figures", "journal-ref": "Proceedings of ICML 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:36:44 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 13:26:43 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 19:01:10 GMT"}, {"version": "v4", "created": "Sun, 16 Jun 2019 20:55:06 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Ollivier", "Yann", ""], ["Bottou", "L\u00e9on", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1802.01435", "submitter": "Joshua Chacksfield", "authors": "Alexey Chaplygin and Joshua Chacksfield", "title": "A Method for Restoring the Training Set Distribution in an Image\n  Classifier", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are a well-known staple of modern image\nclassification. However, it can be difficult to assess the quality and\nrobustness of such models. Deep models are known to perform well on a given\ntraining and estimation set, but can easily be fooled by data that is\nspecifically generated for the purpose. It has been shown that one can produce\nan artificial example that does not represent the desired class, but activates\nthe network in the desired way. This paper describes a new way of\nreconstructing a sample from the training set distribution of an image\nclassifier without deep knowledge about the underlying distribution. This\nenables access to the elements of images that most influence the decision of a\nconvolutional network and to extract meaningful information about the training\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:49:06 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chaplygin", "Alexey", ""], ["Chacksfield", "Joshua", ""]]}, {"id": "1802.01448", "submitter": "Deepak Vijaykeerthy", "authors": "Deepak Vijaykeerthy, Anshuman Suri, Sameep Mehta, Ponnurangam\n  Kumaraguru", "title": "Hardening Deep Neural Networks via Adversarial Model Cascades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to malicious inputs crafted by an\nadversary to produce erroneous outputs. Works on securing neural networks\nagainst adversarial examples achieve high empirical robustness on simple\ndatasets such as MNIST. However, these techniques are inadequate when\nempirically tested on complex data sets such as CIFAR-10 and SVHN. Further,\nexisting techniques are designed to target specific attacks and fail to\ngeneralize across attacks. We propose the Adversarial Model Cascades (AMC) as a\nway to tackle the above inadequacies. Our approach trains a cascade of models\nsequentially where each model is optimized to be robust towards a mixture of\nmultiple attacks. Ultimately, it yields a single model which is secure against\na wide range of attacks; namely FGSM, Elastic, Virtual Adversarial\nPerturbations and Madry. On an average, AMC increases the model's empirical\nrobustness against various attacks simultaneously, by a significant margin (of\n6.225% for MNIST, 5.075% for SVHN and 2.65% for CIFAR10). At the same time, the\nmodel's performance on non-adversarial inputs is comparable to the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 09:02:38 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 16:38:56 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 06:28:25 GMT"}, {"version": "v4", "created": "Sun, 4 Nov 2018 11:16:23 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Vijaykeerthy", "Deepak", ""], ["Suri", "Anshuman", ""], ["Mehta", "Sameep", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "1802.01458", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB, UCSD), Shibin Parameswaran (UCSD),\n  Truong Q. Nguyen (UCSD)", "title": "Image denoising with generalized Gaussian mixture model patch priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch priors have become an important component of image restoration. A\npowerful approach in this category of restoration algorithms is the popular\nExpected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture\nmodel (GMM) prior learned on clean image patches as a way to regularize\ndegraded patches. In this paper, we show that a generalized Gaussian mixture\nmodel (GGMM) captures the underlying distribution of patches better than a GMM.\nEven though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity\nof its components presents major challenges to be applied to a computationally\nintensive process of image restoration. Specifically, each patch has to undergo\na patch classification step and a shrinkage step. These two steps can be\nefficiently solved with a GMM prior but are computationally impractical when\nusing a GGMM prior. In this paper, we provide approximations and computational\nrecipes for fast evaluation of these two steps, so that EPLL can embed a GGMM\nprior on an image with more than tens of thousands of patches. Our main\ncontribution is to analyze the accuracy of our approximations based on thorough\ntheoretical analysis. Our evaluations indicate that the GGMM prior is\nconsistently a better fit formodeling image patch distribution and performs\nbetter on average in image denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 15:18:21 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:00:13 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB, UCSD"], ["Parameswaran", "Shibin", "", "UCSD"], ["Nguyen", "Truong Q.", "", "UCSD"]]}, {"id": "1802.01504", "submitter": "Wei Hu", "authors": "Simon S. Du, Wei Hu", "title": "Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave\n  Saddle Point Problems without Strong Convexity", "comments": "Published in AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the convex-concave saddle point problem $\\min_{x}\\max_{y}\nf(x)+y^\\top A x-g(y)$ where $f$ is smooth and convex and $g$ is smooth and\nstrongly convex. We prove that if the coupling matrix $A$ has full column rank,\nthe vanilla primal-dual gradient method can achieve linear convergence even if\n$f$ is not strongly convex. Our result generalizes previous work which either\nrequires $f$ and $g$ to be quadratic functions or requires proximal mappings\nfor both $f$ and $g$. We adopt a novel analysis technique that in each\niteration uses a \"ghost\" update as a reference, and show that the iterates in\nthe primal-dual gradient method converge to this \"ghost\" sequence. Using the\nsame technique we further give an analysis for the primal-dual stochastic\nvariance reduced gradient (SVRG) method for convex-concave saddle point\nproblems with a finite-sum structure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:27:06 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 06:28:39 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Du", "Simon S.", ""], ["Hu", "Wei", ""]]}, {"id": "1802.01528", "submitter": "Terence Parr", "authors": "Terence Parr, Jeremy Howard", "title": "The Matrix Calculus You Need For Deep Learning", "comments": "PDF version of mobile/web friendly version\n  http://explained.ai/matrix-calculus/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an attempt to explain all the matrix calculus you need in order\nto understand the training of deep neural networks. We assume no math knowledge\nbeyond what you learned in calculus 1, and provide links to help you refresh\nthe necessary math where needed. Note that you do not need to understand this\nmaterial before you start learning to train and use deep learning in practice;\nrather, this material is for those who are already familiar with the basics of\nneural networks, and wish to deepen their understanding of the underlying math.\nDon't worry if you get stuck at some point along the way---just go back and\nreread the previous section, and try writing down and working through some\nexamples. And if you're still stuck, we're happy to answer your questions in\nthe Theory category at forums.fast.ai. Note: There is a reference section at\nthe end of the paper summarizing all the key matrix calculus rules and\nterminology discussed here. See related articles at http://explained.ai\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 17:37:59 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 17:35:28 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 17:36:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Parr", "Terence", ""], ["Howard", "Jeremy", ""]]}, {"id": "1802.01549", "submitter": "Zhezhi He", "authors": "Adnan Siraj Rakin, Zhezhi He, Boqing Gong, Deliang Fan", "title": "Blind Pre-Processing: A Robust Defense Method Against Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms and networks are vulnerable to perturbed inputs\nwhich is known as the adversarial attack. Many defense methodologies have been\ninvestigated to defend against such adversarial attack. In this work, we\npropose a novel methodology to defend the existing powerful attack model. We\nfor the first time introduce a new attacking scheme for the attacker and set a\npractical constraint for white box attack. Under this proposed attacking\nscheme, we present the best defense ever reported against some of the recent\nstrong attacks. It consists of a set of nonlinear function to process the input\ndata which will make it more robust over the adversarial attack. However, we\nmake this processing layer completely hidden from the attacker. Blind\npre-processing improves the white box attack accuracy of MNIST from 94.3\\% to\n98.7\\%. Even with increasing defense when others defenses completely fail,\nblind pre-processing remains one of the strongest ever reported. Another\nstrength of our defense is that it eliminates the need for adversarial training\nas it can significantly increase the MNIST accuracy without adversarial\ntraining as well. Additionally, blind pre-processing can also increase the\ninference accuracy in the face of a powerful attack on CIFAR-10 and SVHN data\nset as well without much sacrificing clean data accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:21:31 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 17:46:02 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Gong", "Boqing", ""], ["Fan", "Deliang", ""]]}, {"id": "1802.01610", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller", "title": "Fast and accurate approximation of the full conditional for gamma shape\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gamma distribution arises frequently in Bayesian models, but there is not\nan easy-to-use conjugate prior for the shape parameter of a gamma. This\ninconvenience is usually dealt with by using either Metropolis-Hastings moves,\nrejection sampling methods, or numerical integration. However, in models with a\nlarge number of shape parameters, these existing methods are slower or more\ncomplicated than one would like, making them burdensome in practice. It turns\nout that the full conditional distribution of the gamma shape parameter is well\napproximated by a gamma distribution, even for small sample sizes, when the\nprior on the shape parameter is also a gamma distribution. This article\nintroduces a quick and easy algorithm for finding a gamma distribution that\napproximates the full conditional distribution of the shape parameter. We\nempirically demonstrate the speed and accuracy of the approximation across a\nwide range of conditions. If exactness is required, the approximation can be\nused as a proposal distribution for Metropolis-Hastings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:22:34 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 22:46:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Miller", "Jeffrey W.", ""]]}, {"id": "1802.01697", "submitter": "Yao-Yuan Yang", "authors": "Yao-Yuan Yang, Yi-An Lin, Hong-Min Chu and Hsuan-Tien Lin", "title": "Deep Learning with a Rethinking Structure for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification (MLC) is an important class of machine learning\nproblems that come with a wide spectrum of applications, each demanding a\npossibly different evaluation criterion. When solving the MLC problems, we\ngenerally expect the learning algorithm to take the hidden correlation of the\nlabels into account to improve the prediction performance. Extracting the\nhidden correlation is generally a challenging task. In this work, we propose a\nnovel deep learning framework to better extract the hidden correlation with the\nhelp of the memory structure within recurrent neural networks. The memory\nstores the temporary guesses on the labels and effectively allows the framework\nto rethink about the goodness and correlation of the guesses before making the\nfinal prediction. Furthermore, the rethinking process makes it easy to adapt to\ndifferent evaluation criteria to match real-world application needs. In\nparticular, the framework can be trained in an end-to-end style with respect to\nany given MLC evaluation criteria. The end-to-end design can be seamlessly\ncombined with other deep learning techniques to conquer challenging MLC\nproblems like image tagging. Experimental results across many real-world data\nsets justify that the rethinking framework indeed improves MLC performance\nacross different evaluation criteria and leads to superior performance over\nstate-of-the-art MLC algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 21:27:59 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 07:27:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yang", "Yao-Yuan", ""], ["Lin", "Yi-An", ""], ["Chu", "Hong-Min", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1802.01709", "submitter": "Zeyu You", "authors": "Zeyu You, Raviv Raich, Xiaoli Z. Fern, and Jinsub Kim", "title": "Weakly-supervised Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2807422", "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic modeling and inference framework for\ndiscriminative analysis dictionary learning under a weak supervision setting.\nDictionary learning approaches have been widely used for tasks such as\nlow-level signal denoising and restoration as well as high-level classification\ntasks, which can be applied to audio and image analysis. Synthesis dictionary\nlearning aims at jointly learning a dictionary and corresponding sparse\ncoefficients to provide accurate data representation. This approach is useful\nfor denoising and signal restoration, but may lead to sub-optimal\nclassification performance. By contrast, analysis dictionary learning provides\na transform that maps data to a sparse discriminative representation suitable\nfor classification. We consider the problem of analysis dictionary learning for\ntime-series data under a weak supervision setting in which signals are assigned\nwith a global label instead of an instantaneous label signal. We propose a\ndiscriminative probabilistic model that incorporates both label information and\nsparsity constraints on the underlying latent instantaneous label signal using\ncardinality control. We present the expectation maximization (EM) procedure for\nmaximum likelihood estimation (MLE) of the proposed model. To facilitate a\ncomputationally efficient E-step, we propose both a chain and a novel tree\ngraph reformulation of the graphical model. The performance of the proposed\nmodel is demonstrated on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 21:55:52 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["You", "Zeyu", ""], ["Raich", "Raviv", ""], ["Fern", "Xiaoli Z.", ""], ["Kim", "Jinsub", ""]]}, {"id": "1802.01737", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Tamara Broderick", "title": "Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent", "comments": "Appearing in the 2018 International Conference on Machine Learning\n  (ICML). 13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coherent uncertainty quantification is a key strength of Bayesian methods.\nBut modern algorithms for approximate Bayesian posterior inference often\nsacrifice accurate posterior uncertainty estimation in the pursuit of\nscalability. This work shows that previous Bayesian coreset construction\nalgorithms---which build a small, weighted subset of the data that approximates\nthe full dataset---are no exception. We demonstrate that these algorithms scale\nthe coreset log-likelihood suboptimally, resulting in underestimated posterior\nuncertainty. To address this shortcoming, we develop greedy iterative geodesic\nascent (GIGA), a novel algorithm for Bayesian coreset construction that scales\nthe coreset log-likelihood optimally. GIGA provides geometric decay in\nposterior approximation error as a function of coreset size, and maintains the\nfast running time of its predecessors. The paper concludes with validation of\nGIGA on both synthetic and real datasets, demonstrating that it reduces\nposterior approximation error by orders of magnitude compared with previous\ncoreset constructions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 23:52:12 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 23:46:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1802.01751", "submitter": "Wai Ming Tai", "authors": "Jeff M. Phillips, Wai Ming Tai", "title": "Near-Optimal Coresets of Kernel Density Estimates", "comments": "This paper is combined with arXiv:1710.04325", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct near-optimal coresets for kernel density estimates for points in\n$\\mathbb{R}^d$ when the kernel is positive definite. Specifically we show a\npolynomial time construction for a coreset of size $O(\\sqrt{d}/\\varepsilon\\cdot\n\\sqrt{\\log 1/\\varepsilon} )$, and we show a near-matching lower bound of size\n$\\Omega(\\min\\{\\sqrt{d}/\\varepsilon, 1/\\varepsilon^2\\})$. When $d\\geq\n1/\\varepsilon^2$, it is known that the size of coreset can be\n$O(1/\\varepsilon^2)$. The upper bound is a polynomial-in-$(1/\\varepsilon)$\nimprovement when $d \\in [3,1/\\varepsilon^2)$ and the lower bound is the first\nknown lower bound to depend on $d$ for this problem. Moreover, the upper bound\nrestriction that the kernel is positive definite is significant in that it\napplies to a wide-variety of kernels, specifically those most important for\nmachine learning. This includes kernels for information distances and the sinc\nkernel which can be negative.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 01:06:47 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 17:04:17 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 13:34:29 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 00:45:09 GMT"}, {"version": "v5", "created": "Thu, 11 Apr 2019 23:06:34 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Tai", "Wai Ming", ""]]}, {"id": "1802.01756", "submitter": "Xiuzhen Huang Dr.", "authors": "Jason Causey, Junyu Zhang, Shiqian Ma, Bo Jiang, Jake Qualls, David G.\n  Politte, Fred Prior, Shuzhong Zhang and Xiuzhen Huang", "title": "Highly accurate model for prediction of lung nodule malignancy with CT\n  scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) examinations are commonly used to predict lung\nnodule malignancy in patients, which are shown to improve noninvasive early\ndiagnosis of lung cancer. It remains challenging for computational approaches\nto achieve performance comparable to experienced radiologists. Here we present\nNoduleX, a systematic approach to predict lung nodule malignancy from CT data,\nbased on deep learning convolutional neural networks (CNN). For training and\nvalidation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.\nAll nodules were identified and classified by four experienced thoracic\nradiologists who participated in the LIDC project. NoduleX achieves high\naccuracy for nodule malignancy classification, with an AUC of ~0.99. This is\ncommensurate with the analysis of the dataset by experienced radiologists. Our\napproach, NoduleX, provides an effective framework for highly accurate nodule\nmalignancy prediction with the model trained on a large patient population. Our\nresults are replicable with software available at\nhttp://bioinformatics.astate.edu/NoduleX.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 01:42:21 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Causey", "Jason", ""], ["Zhang", "Junyu", ""], ["Ma", "Shiqian", ""], ["Jiang", "Bo", ""], ["Qualls", "Jake", ""], ["Politte", "David G.", ""], ["Prior", "Fred", ""], ["Zhang", "Shuzhong", ""], ["Huang", "Xiuzhen", ""]]}, {"id": "1802.01765", "submitter": "Xu Chen", "authors": "Xu Chen, Jiang Wang, Hao Ge", "title": "Training Generative Adversarial Networks via Primal-Dual Subgradient\n  Methods: A Lagrangian Perspective on GAN", "comments": "Accepted by Sixth International Conference on Learning\n  Representations (ICLR 2018). Xu Chen and Jiang Wang contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We relate the minimax game of generative adversarial networks (GANs) to\nfinding the saddle points of the Lagrangian function for a convex optimization\nproblem, where the discriminator outputs and the distribution of generator\noutputs play the roles of primal variables and dual variables, respectively.\nThis formulation shows the connection between the standard GAN training process\nand the primal-dual subgradient methods for convex optimization. The inherent\nconnection does not only provide a theoretical convergence proof for training\nGANs in the function space, but also inspires a novel objective function for\ntraining. The modified objective function forces the distribution of generator\noutputs to be updated along the direction according to the primal-dual\nsubgradient methods. A toy example shows that the proposed method is able to\nresolve mode collapse, which in this case cannot be avoided by the standard GAN\nor Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and\nreal-world image datasets demonstrate the performance of the proposed method on\ngenerating diverse samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 02:31:43 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Chen", "Xu", ""], ["Wang", "Jiang", ""], ["Ge", "Hao", ""]]}, {"id": "1802.01786", "submitter": "Amir Karami", "authors": "Amir Karami, London S. Bennett, Xiaoyun He", "title": "Mining Public Opinion about Economic Issues: Twitter and the U.S.\n  Presidential Election", "comments": null, "journal-ref": null, "doi": "10.4018/IJSDS.2018010102", "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion polls have been the bridge between public opinion and politicians in\nelections. However, developing surveys to disclose people's feedback with\nrespect to economic issues is limited, expensive, and time-consuming. In recent\nyears, social media such as Twitter has enabled people to share their opinions\nregarding elections. Social media has provided a platform for collecting a\nlarge amount of social media data. This paper proposes a computational public\nopinion mining approach to explore the discussion of economic issues in social\nmedia during an election. Current related studies use text mining methods\nindependently for election analysis and election prediction; this research\ncombines two text mining methods: sentiment analysis and topic modeling. The\nproposed approach has effectively been deployed on millions of tweets to\nanalyze economic concerns of people during the 2012 US presidential election.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:55:37 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Karami", "Amir", ""], ["Bennett", "London S.", ""], ["He", "Xiaoyun", ""]]}, {"id": "1802.02137", "submitter": "Kevan Yuen", "authors": "Kevan Yuen and Mohan M. Trivedi", "title": "An Occluded Stacked Hourglass Approach to Facial Landmark Localization\n  and Occlusion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key step to driver safety is to observe the driver's activities with the\nface being a key step in this process to extracting information such as head\npose, blink rate, yawns, talking to passenger which can then help derive higher\nlevel information such as distraction, drowsiness, intent, and where they are\nlooking. In the context of driving safety, it is important for the system\nperform robust estimation under harsh lighting and occlusion but also be able\nto detect when the occlusion occurs so that information predicted from occluded\nparts of the face can be taken into account properly. This paper introduces the\nOccluded Stacked Hourglass, based on the work of original Stacked Hourglass\nnetwork for body pose joint estimation, which is retrained to process a\ndetected face window and output 68 occlusion heat maps, each corresponding to a\nfacial landmark. Landmark location, occlusion levels and a refined face\ndetection score, to reject false positives, are extracted from these heat maps.\nUsing the facial landmark locations, features such as head pose and eye/mouth\nopenness can be extracted to derive driver attention and activity. The system\nis evaluated for face detection, head pose, and occlusion estimation on various\ndatasets in the wild, both quantitatively and qualitatively, and shows\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:17:52 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Yuen", "Kevan", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.02147", "submitter": "Hanyuan Zhang", "authors": "Hanyuan Zhang, Hao Wu, Weiwei Sun, Baihua Zheng", "title": "DeepTravel: a Neural Network Based Travel Time Estimation Model with\n  Auxiliary Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the travel time of a path is of great importance to smart urban\nmobility. Existing approaches are either based on estimating the time cost of\neach road segment which are not able to capture many cross-segment complex\nfactors, or designed heuristically in a non-learning-based way which fail to\nutilize the existing abundant temporal labels of the data, i.e., the time stamp\nof each trajectory point. In this paper, we leverage on new development of deep\nneural networks and propose a novel auxiliary supervision model, namely\nDeepTravel, that can automatically and effectively extract different features,\nas well as make full use of the temporal labels of the trajectory data. We have\nconducted comprehensive experiments on real datasets to demonstrate the\nout-performance of DeepTravel over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 16:08:06 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Hanyuan", ""], ["Wu", "Hao", ""], ["Sun", "Weiwei", ""], ["Zheng", "Baihua", ""]]}, {"id": "1802.02154", "submitter": "Zohar Ringel", "authors": "Zohar Ringel, Rodrigo de Bem", "title": "Critical Percolation as a Framework to Analyze the Training of Deep\n  Networks", "comments": "Accepted to ICLR 2018 as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we approach two relevant deep learning topics: i) tackling of\ngraph structured input data and ii) a better understanding and analysis of deep\nnetworks and related learning algorithms. With this in mind we focus on the\ntopological classification of reachability in a particular subset of planar\ngraphs (Mazes). Doing so, we are able to model the topology of data while\nstaying in Euclidean space, thus allowing its processing with standard CNN\narchitectures. We suggest a suitable architecture for this problem and show\nthat it can express a perfect solution to the classification task. The shape of\nthe cost function around this solution is also derived and, remarkably, does\nnot depend on the size of the maze in the large maze limit. Responsible for\nthis behavior are rare events in the dataset which strongly regulate the shape\nof the cost function near this global minimum. We further identify an obstacle\nto learning in the form of poorly performing local minima in which the network\nchooses to ignore some of the inputs. We further support our claims with\ntraining experiments and numerical analysis of the cost function on networks\nwith up to $128$ layers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 19:00:01 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ringel", "Zohar", ""], ["de Bem", "Rodrigo", ""]]}, {"id": "1802.02163", "submitter": "Brandon Stewart", "authors": "Naoki Egami, Christian J. Fong, Justin Grimmer, Margaret E. Roberts,\n  Brandon M. Stewart", "title": "How to Make Causal Inferences Using Texts", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New text as data techniques offer a great promise: the ability to inductively\ndiscover measures that are useful for testing social science theories of\ninterest from large collections of text. We introduce a conceptual framework\nfor making causal inferences with discovered measures as a treatment or\noutcome. Our framework enables researchers to discover high-dimensional textual\ninterventions and estimate the ways that observed treatments affect text-based\noutcomes. We argue that nearly all text-based causal inferences depend upon a\nlatent representation of the text and we provide a framework to learn the\nlatent representation. But estimating this latent representation, we show,\ncreates new risks: we may introduce an identification problem or overfit. To\naddress these risks we describe a split-sample framework and apply it to\nestimate causal effects from an experiment on immigration attitudes and a study\non bureaucratic response. Our work provides a rigorous foundation for\ntext-based causal inferences.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 19:00:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Egami", "Naoki", ""], ["Fong", "Christian J.", ""], ["Grimmer", "Justin", ""], ["Roberts", "Margaret E.", ""], ["Stewart", "Brandon M.", ""]]}, {"id": "1802.02212", "submitter": "Eric Tramel", "authors": "Pierre Courtiol, Eric W. Tramel, Marc Sanselme, Gilles Wainrib", "title": "Classification and Disease Localization in Histopathology Using Only\n  Global Labels: A Weakly-Supervised Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of histopathology slides is a critical step for many diagnoses, and\nin particular in oncology where it defines the gold standard. In the case of\ndigital histopathological analysis, highly trained pathologists must review\nvast whole-slide-images of extreme digital resolution ($100,000^2$ pixels)\nacross multiple zoom levels in order to locate abnormal regions of cells, or in\nsome cases single cells, out of millions. The application of deep learning to\nthis problem is hampered not only by small sample sizes, as typical datasets\ncontain only a few hundred samples, but also by the generation of ground-truth\nlocalized annotations for training interpretable classification and\nsegmentation models. We propose a method for disease localization in the\ncontext of weakly supervised learning, where only image-level labels are\navailable during training. Even without pixel-level annotations, we are able to\ndemonstrate performance comparable with models trained with strong annotations\non the Camelyon-16 lymph node metastases detection challenge. We accomplish\nthis through the use of pre-trained deep convolutional networks, feature\nembedding, as well as learning via top instances and negative evidence, a\nmultiple instance learning technique from the field of semantic segmentation\nand object detection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 15:21:14 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:25:47 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Courtiol", "Pierre", ""], ["Tramel", "Eric W.", ""], ["Sanselme", "Marc", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1802.02219", "submitter": "Matthias Feurer", "authors": "Matthias Feurer, Benjamin Letham, Frank Hutter, Eytan Bakshy", "title": "Practical Transfer Learning for Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has become a standard technique for hyperparameter\noptimization of machine learning algorithms. We consider the setting where\nprevious optimization runs are available, and we wish to transfer their\noutcomes to a new optimization run and thereby accelerate the search. We\ndevelop a new hyperparameter-free ensemble model for Bayesian optimization,\nbased on a linear combination of Gaussian Processes and Agnostic Bayesian\nLearning of Ensembles. We show that this is a generalization of two existing\ntransfer learning extensions to Bayesian optimization and establish a\nworst-case bound compared to vanilla Bayesian optimization. Using a large\ncollection of hyperparameter optimization benchmark problems, we demonstrate\nthat our contributions substantially reduce optimization time compared to\nstandard Gaussian process-based Bayesian optimization and improve over the\ncurrent state-of-the-art for warm-starting Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 21:02:59 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 11:41:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Feurer", "Matthias", ""], ["Letham", "Benjamin", ""], ["Hutter", "Frank", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1802.02226", "submitter": "Nhat Nguyen", "authors": "Nhat M. Nguyen, Nilanjan Ray", "title": "Generative Adversarial Networks using Adaptive Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing GANs architectures that generate images use transposed\nconvolution or resize-convolution as their upsampling algorithm from lower to\nhigher resolution feature maps in the generator. We argue that this kind of\nfixed operation is problematic for GANs to model objects that have very\ndifferent visual appearances. We propose a novel adaptive convolution method\nthat learns the upsampling algorithm based on the local context at each\nlocation to address this problem. We modify a baseline GANs architecture by\nreplacing normal convolutions with adaptive convolutions in the generator.\nExperiments on CIFAR-10 dataset show that our modified models improve the\nbaseline model by a large margin. Furthermore, our models achieve\nstate-of-the-art performance on CIFAR-10 and STL-10 datasets in the\nunsupervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 23:49:19 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Nguyen", "Nhat M.", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1802.02242", "submitter": "Diogo R. Ferreira", "authors": "Diogo R. Ferreira, Pedro J. Carvalho, Hor\\'acio Fernandes (JET\n  Contributors)", "title": "Full-pulse Tomographic Reconstruction with Deep Neural Networks", "comments": null, "journal-ref": "Fusion Science and Technology (2018)", "doi": "10.1080/15361055.2017.1390386", "report-no": null, "categories": "physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasma tomography consists in reconstructing the 2D radiation profile in a\npoloidal cross-section of a fusion device, based on line-integrated\nmeasurements along several lines of sight. The reconstruction process is\ncomputationally intensive and, in practice, only a few reconstructions are\nusually computed per pulse. In this work, we trained a deep neural network\nbased on a large collection of sample tomograms that have been produced at JET\nover several years. Once trained, the network is able to reproduce those\nresults with high accuracy. More importantly, it can compute all the\ntomographic reconstructions for a given pulse in just a few seconds. This makes\nit possible to visualize several phenomena -- such as plasma heating,\ndisruptions and impurity transport -- over the course of a discharge.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 16:44:49 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Ferreira", "Diogo R.", "", "JET\n  Contributors"], ["Carvalho", "Pedro J.", "", "JET\n  Contributors"], ["Fernandes", "Hor\u00e1cio", "", "JET\n  Contributors"]]}, {"id": "1802.02290", "submitter": "Siyu Chen", "authors": "Siyu Chen and Danping Liao and Yuntao Qian", "title": "Spectral Image Visualization Using Generative Adversarial Networks", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 02:23:47 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chen", "Siyu", ""], ["Liao", "Danping", ""], ["Qian", "Yuntao", ""]]}, {"id": "1802.02343", "submitter": "Simon Kamronn", "authors": "Simon Kamronn, Andreas Trier Poulsen, Lars Kai Hansen", "title": "Multi-View Bayesian Correlated Component Analysis", "comments": null, "journal-ref": "Neural Computation, 27, (10):220730, 2015", "doi": "10.1162/NECO_a_00774", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated component analysis as proposed by Dmochowski et al. (2012) is a\ntool for investigating brain process similarity in the responses to multiple\nviews of a given stimulus. Correlated components are identified under the\nassumption that the involved spatial networks are identical. Here we propose a\nhierarchical probabilistic model that can infer the level of universality in\nsuch multi-view data, from completely unrelated representations, corresponding\nto canonical correlation analysis, to identical representations as in\ncorrelated component analysis. This new model, which we denote Bayesian\ncorrelated component analysis, evaluates favourably against three relevant\nalgorithms in simulated data. A well-established benchmark EEG dataset is used\nto further validate the new model and infer the variability of spatial\nrepresentations across multiple subjects.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:21:14 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kamronn", "Simon", ""], ["Poulsen", "Andreas Trier", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1802.02436", "submitter": "Joshua Chacksfield", "authors": "Alexey Chaplygin and Joshua Chacksfield", "title": "Stochastic Deconvolutional Neural Network Ensemble Training on\n  Generative Pseudo-Adversarial Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of Generative Adversarial Networks is a difficult task mainly\ndue to the nature of the networks. One such issue is when the generator and\ndiscriminator start oscillating, rather than converging to a fixed point.\nAnother case can be when one agent becomes more adept than the other which\nresults in the decrease of the other agent's ability to learn, reducing the\nlearning capacity of the system as a whole. Additionally, there exists the\nproblem of Mode Collapse which involves the generators output collapsing to a\nsingle sample or a small set of similar samples. To train GANs a careful\nselection of the architecture that is used along with a variety of other\nmethods to improve training. Even when applying these methods there is low\nstability of training in relation to the parameters that are chosen. Stochastic\nensembling is suggested as a method for improving the stability while training\nGANs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 14:36:15 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chaplygin", "Alexey", ""], ["Chacksfield", "Joshua", ""]]}, {"id": "1802.02498", "submitter": "Chicheng Zhang", "authors": "Chicheng Zhang, Eran A. Mukamel, Kamalika Chaudhuri", "title": "Spectral Learning of Binomial HMMs for DNA Methylation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning parameters of Binomial Hidden Markov Models, which may\nbe used to model DNA methylation data. The standard algorithm for the problem\nis EM, which is computationally expensive for sequences of the scale of the\nmammalian genome. Recently developed spectral algorithms can learn parameters\nof latent variable models via tensor decomposition, and are highly efficient\nfor large data. However, these methods have only been applied to categorial\nHMMs, and the main challenge is how to extend them to Binomial HMMs while still\nretaining computational efficiency. We address this challenge by introducing a\nnew feature-map based approach that exploits specific properties of Binomial\nHMMs. We provide theoretical performance guarantees for our algorithm and\nevaluate it on real DNA methylation data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 16:00:05 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Chicheng", ""], ["Mukamel", "Eran A.", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1802.02500", "submitter": "Alexander New", "authors": "Alexander New, Curt Breneman, and Kristin P. Bennett", "title": "Cadre Modeling: Simultaneously Discovering Subpopulations and Predictive\n  Models", "comments": "8 pages, 6 figures", "journal-ref": "In 2018 International Joint Conference on Neural Networks (IJCNN),\n  Rio de Janeiro, Brazil, 2018", "doi": "10.1109/IJCNN.2018.8489618", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem in regression analysis of identifying subpopulations\nthat exhibit different patterns of response, where each subpopulation requires\na different underlying model. Unlike statistical cohorts, these subpopulations\nare not known a priori; thus, we refer to them as cadres. When the cadres and\ntheir associated models are interpretable, modeling leads to insights about the\nsubpopulations and their associations with the regression target. We introduce\na discriminative model that simultaneously learns cadre assignment and\ntarget-prediction rules. Sparsity-inducing priors are placed on the model\nparameters, under which independent feature selection is performed for both the\ncadre assignment and target-prediction processes. We learn models using\nadaptive step size stochastic gradient descent, and we assess cadre quality\nwith bootstrapped sample analysis. We present simulated results showing that,\nwhen the true clustering rule does not depend on the entire set of features,\nour method significantly outperforms methods that learn subpopulation-discovery\nand target-prediction rules separately. In a materials-by-design case study,\nour model provides state-of-the-art prediction of polymer glass transition\ntemperature. Importantly, the method identifies cadres of polymers that respond\ndifferently to structural perturbations, thus providing design insight for\ntargeting or avoiding specific transition temperature ranges. It identifies\nchemically meaningful cadres, each with interpretable models. Further\nexperimental results show that cadre methods have generalization that is\ncompetitive with linear and nonlinear regression models and can identify robust\nsubpopulations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 16:05:22 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 19:28:58 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["New", "Alexander", ""], ["Breneman", "Curt", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1802.02511", "submitter": "Avesh Singh", "authors": "Brandon Ballinger, Johnson Hsieh, Avesh Singh, Nimit Sohoni, Jack\n  Wang, Geoffrey H. Tison, Gregory M. Marcus, Jose M. Sanchez, Carol Maguire,\n  Jeffrey E. Olgin, Mark J. Pletcher", "title": "DeepHeart: Semi-Supervised Sequence Learning for Cardiovascular Risk\n  Prediction", "comments": "Presented at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train and validate a semi-supervised, multi-task LSTM on 57,675\nperson-weeks of data from off-the-shelf wearable heart rate sensors, showing\nhigh accuracy at detecting multiple medical conditions, including diabetes\n(0.8451), high cholesterol (0.7441), high blood pressure (0.8086), and sleep\napnea (0.8298). We compare two semi-supervised train- ing methods,\nsemi-supervised sequence learning and heuristic pretraining, and show they\noutperform hand-engineered biomarkers from the medical literature. We believe\nour work suggests a new approach to patient risk stratification based on\ncardiovascular risk scores derived from popular wearables such as Fitbit, Apple\nWatch, or Android Wear.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 16:31:50 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Ballinger", "Brandon", ""], ["Hsieh", "Johnson", ""], ["Singh", "Avesh", ""], ["Sohoni", "Nimit", ""], ["Wang", "Jack", ""], ["Tison", "Geoffrey H.", ""], ["Marcus", "Gregory M.", ""], ["Sanchez", "Jose M.", ""], ["Maguire", "Carol", ""], ["Olgin", "Jeffrey E.", ""], ["Pletcher", "Mark J.", ""]]}, {"id": "1802.02538", "submitter": "Yuling Yao", "authors": "Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman", "title": "Yes, but Did It Work?: Evaluating Variational Inference", "comments": "Appearing at International Conference on Machine Learning 2018", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:5581-5590, 2018.\n  http://proceedings.mlr.press/v80/yao18a.html", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it's always possible to compute a variational approximation to a\nposterior distribution, it can be difficult to discover problems with this\napproximation. We propose two diagnostic algorithms to alleviate this problem.\nThe Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of\nfit measurement for joint distributions, while simultaneously improving the\nerror in the estimate. The variational simulation-based calibration (VSBC)\nassesses the average performance of point estimates.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:25:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 21:21:25 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Yao", "Yuling", ""], ["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "1802.02547", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Adam Klivans and Raghu Meka", "title": "Learning One Convolutional Layer with Overlapping Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first provably efficient algorithm for learning a one hidden\nlayer convolutional network with respect to a general class of (potentially\noverlapping) patches. Additionally, our algorithm requires only mild conditions\non the underlying distribution. We prove that our framework captures commonly\nused schemes from computer vision, including one-dimensional and\ntwo-dimensional \"patch and stride\" convolutions.\n  Our algorithm-- $Convotron$ -- is inspired by recent work applying isotonic\nregression to learning neural networks. Convotron uses a simple, iterative\nupdate rule that is stochastic in nature and tolerant to noise (requires only\nthat the conditional mean function is a one layer convolutional network, as\nopposed to the realizable setting). In contrast to gradient descent, Convotron\nrequires no special initialization or learning-rate tuning to converge to the\nglobal optimum.\n  We also point out that learning one hidden convolutional layer with respect\nto a Gaussian distribution and just $one$ disjoint patch $P$ (the other patches\nmay be arbitrary) is $easy$ in the following sense: Convotron can efficiently\nrecover the hidden weight vector by updating $only$ in the direction of $P$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:41:25 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1802.02548", "submitter": "Sam Ganzfried", "authors": "Sheila Alemany, Jonathan Beltran, Adrian Perez, Sam Ganzfried", "title": "Predicting Hurricane Trajectories using a Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricanes are cyclones circulating about a defined center whose closed wind\nspeeds exceed 75 mph originating over tropical and subtropical waters. At\nlandfall, hurricanes can result in severe disasters. The accuracy of predicting\ntheir trajectory paths is critical to reduce economic loss and save human\nlives. Given the complexity and nonlinearity of weather data, a recurrent\nneural network (RNN) could be beneficial in modeling hurricane behavior. We\npropose the application of a fully connected RNN to predict the trajectory of\nhurricanes. We employed the RNN over a fine grid to reduce typical truncation\nerrors. We utilized their latitude, longitude, wind speed, and pressure\npublicly provided by the National Hurricane Center (NHC) to predict the\ntrajectory of a hurricane at 6-hour intervals. Results show that this proposed\ntechnique is competitive to methods currently employed by the NHC and can\npredict up to approximately 120 hours of hurricane path.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 08:17:58 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 08:59:55 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 07:30:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Alemany", "Sheila", ""], ["Beltran", "Jonathan", ""], ["Perez", "Adrian", ""], ["Ganzfried", "Sam", ""]]}, {"id": "1802.02550", "submitter": "Yoon Kim", "authors": "Yoon Kim, Sam Wiseman, Andrew C. Miller, David Sontag, Alexander M.\n  Rush", "title": "Semi-Amortized Variational Autoencoders", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amortized variational inference (AVI) replaces instance-specific local\ninference with a global inference network. While AVI has enabled efficient\ntraining of deep generative models such as variational autoencoders (VAE),\nrecent empirical work suggests that inference networks can produce suboptimal\nvariational parameters. We propose a hybrid approach, to use AVI to initialize\nthe variational parameters and run stochastic variational inference (SVI) to\nrefine them. Crucially, the local SVI procedure is itself differentiable, so\nthe inference network and generative model can be trained end-to-end with\ngradient-based optimization. This semi-amortized approach enables the use of\nrich generative models without experiencing the posterior-collapse phenomenon\ncommon in training VAEs for problems like text generation. Experiments show\nthis approach outperforms strong autoregressive and variational baselines on\nstandard text and image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:06:42 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 17:02:04 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 05:18:57 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 17:05:42 GMT"}, {"version": "v5", "created": "Thu, 24 May 2018 20:06:49 GMT"}, {"version": "v6", "created": "Fri, 8 Jun 2018 21:17:42 GMT"}, {"version": "v7", "created": "Mon, 23 Jul 2018 19:31:28 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Kim", "Yoon", ""], ["Wiseman", "Sam", ""], ["Miller", "Andrew C.", ""], ["Sontag", "David", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1802.02557", "submitter": "Yang Feng", "authors": "Xin Tong, Lucy Xia, Jiacheng Wang and Yang Feng", "title": "Neyman-Pearson classification: parametrics and sample size requirement", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers\nthat achieve a minimal type II error while enforcing the prioritized type I\nerror controlled under some user-specified level $\\alpha$. This paradigm serves\nnaturally in applications such as severe disease diagnosis and spam detection,\nwhere people have clear priorities among the two error types. Recently, Tong,\nFeng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all\nscoring-type classification methods (e.g., logistic regression, support vector\nmachines, random forest) to respect the given type I error upper bound $\\alpha$\nwith high probability, without specific distributional assumptions on the\nfeatures and the responses. Universal the umbrella algorithm is, it demands an\nexplicit minimum sample size requirement on class $0$, which is often the more\nscarce class, such as in rare disease diagnosis applications. In this work, we\nemploy the parametric linear discriminant analysis (LDA) model and propose a\nnew parametric thresholding algorithm, which does not need the minimum sample\nsize requirements on class $0$ observations and thus is suitable for small\nsample applications such as rare disease diagnosis. Leveraging both the\nexisting nonparametric and the newly proposed parametric thresholding rules, we\npropose four LDA-based NP classifiers, for both low- and high-dimensional\nsettings. On the theoretical front, we prove NP oracle inequalities for one\nproposed classifier, where the rate for excess type II error benefits from the\nexplicit parametric model assumption. Furthermore, as NP classifiers involve a\nsample splitting step of class $0$ observations, we construct a new adaptive\nsample splitting scheme that can be applied universally to NP classifiers, and\nthis adaptive strategy reduces the type II error of these classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:32:47 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 20:23:36 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 14:43:55 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 09:50:12 GMT"}, {"version": "v5", "created": "Tue, 28 Jan 2020 19:37:52 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Tong", "Xin", ""], ["Xia", "Lucy", ""], ["Wang", "Jiacheng", ""], ["Feng", "Yang", ""]]}, {"id": "1802.02558", "submitter": "Richard Zhao", "authors": "Lucy Xia, Richard Zhao, Yanhui Wu and Xin Tong", "title": "Intentional Control of Type I Error over Unconscious Data Distortion: a\n  Neyman-Pearson Approach to Text Classification", "comments": null, "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2020.1740711", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenges in classifying textual data obtained from\nopen online platforms, which are vulnerable to distortion. Most existing\nclassification methods minimize the overall classification error and may yield\nan undesirably large type I error (relevant textual messages are classified as\nirrelevant), particularly when available data exhibit an asymmetry between\nrelevant and irrelevant information. Data distortion exacerbates this situation\nand often leads to fallacious prediction. To deal with inestimable data\ndistortion, we propose the use of the Neyman-Pearson (NP) classification\nparadigm, which minimizes type II error under a user-specified type I error\nconstraint. Theoretically, we show that the NP oracle is unaffected by data\ndistortion when the class conditional distributions remain the same.\nEmpirically, we study a case of classifying posts about worker strikes obtained\nfrom a leading Chinese microblogging platform, which are frequently prone to\nextensive, unpredictable and inestimable censorship. We demonstrate that, even\nthough the training and test data are susceptible to different distortion and\ntherefore potentially follow different distributions, our proposed NP methods\ncontrol the type I error on test data at the targeted level. The methods and\nimplementation pipeline proposed in our case study are applicable to many other\nproblems involving data distortion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:33:20 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 19:21:05 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 03:49:46 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Xia", "Lucy", ""], ["Zhao", "Richard", ""], ["Wu", "Yanhui", ""], ["Tong", "Xin", ""]]}, {"id": "1802.02565", "submitter": "Johannes Wagner", "authors": "Johannes Wagner, Tobias Baur, Yue Zhang, Michel F. Valstar, Bj\\\"orn\n  Schuller, Elisabeth Andr\\'e", "title": "Applying Cooperative Machine Learning to Speed Up the Annotation of\n  Social Signals in Large Multi-modal Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific disciplines, such as Behavioural Psychology, Anthropology and\nrecently Social Signal Processing are concerned with the systematic exploration\nof human behaviour. A typical work-flow includes the manual annotation (also\ncalled coding) of social signals in multi-modal corpora of considerable size.\nFor the involved annotators this defines an exhausting and time-consuming task.\nIn the article at hand we present a novel method and also provide the tools to\nspeed up the coding procedure. To this end, we suggest and evaluate the use of\nCooperative Machine Learning (CML) techniques to reduce manual labelling\nefforts by combining the power of computational capabilities and human\nintelligence. The proposed CML strategy starts with a small number of labelled\ninstances and concentrates on predicting local parts first. Afterwards, a\nsession-independent classification model is created to finish the remaining\nparts of the database. Confidence values are computed to guide the manual\ninspection and correction of the predictions. To bring the proposed approach\ninto application we introduce NOVA - an open-source tool for collaborative and\nmachine-aided annotations. In particular, it gives labellers immediate access\nto CML strategies and directly provides visual feedback on the results. Our\nexperiments show that the proposed method has the potential to significantly\nreduce human labelling efforts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:47:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Wagner", "Johannes", ""], ["Baur", "Tobias", ""], ["Zhang", "Yue", ""], ["Valstar", "Michel F.", ""], ["Schuller", "Bj\u00f6rn", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "1802.02617", "submitter": "Fady Medhat", "authors": "Fady Medhat, David Chesmore, John Robinson", "title": "Recognition of Acoustic Events Using Masked Conditional Neural Networks", "comments": "Restricted Boltzmann Machine, RBM, Conditional Restricted Boltzmann\n  Machine, CRBM, Conditional Neural Networks, CLNN, Masked Conditional Neural\n  Networks, MCLNN, Deep Neural Network, Environmental Sound Recognition, ESR", "journal-ref": "IEEE International Conference on Machine Learning and Applications\n  (ICMLA) Year: 2017 Pages: 199 - 206", "doi": "10.1109/ICMLA.2017.0-158", "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic feature extraction using neural networks has accomplished\nremarkable success for images, but for sound recognition, these models are\nusually modified to fit the nature of the multi-dimensional temporal\nrepresentation of the audio signal in spectrograms. This may not efficiently\nharness the time-frequency representation of the signal. The ConditionaL Neural\nNetwork (CLNN) takes into consideration the interrelation between the temporal\nframes, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN\nby forcing a systematic sparseness over the network's weights using a binary\nmask. The masking allows the network to learn about frequency bands rather than\nbins, mimicking a filterbank used in signal transformations such as MFCC.\nAdditionally, the Mask is designed to consider various combinations of\nfeatures, which automates the feature hand-crafting process. We applied the\nMCLNN for the Environmental Sound Recognition problem using the Urbansound8k,\nYorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive\nperformance compared to state-of-the-art Convolutional Neural Networks and\nhand-crafted attempts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:58:50 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 10:01:26 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Medhat", "Fady", ""], ["Chesmore", "David", ""], ["Robinson", "John", ""]]}, {"id": "1802.02643", "submitter": "Pavel Gurevich", "authors": "Pavel Gurevich, Hannes Stuke", "title": "Gradient conjugate priors and multi-layer neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with learning probability distributions of observed data by\nartificial neural networks. We suggest a so-called gradient conjugate prior\n(GCP) update appropriate for neural networks, which is a modification of the\nclassical Bayesian update for conjugate priors. We establish a connection\nbetween the gradient conjugate prior update and the maximization of the\nlog-likelihood of the predictive distribution. Unlike for the Bayesian neural\nnetworks, we use deterministic weights of neural networks, but rather assume\nthat the ground truth distribution is normal with unknown mean and variance and\nlearn by the neural networks the parameters of a prior (normal-gamma\ndistribution) for these unknown mean and variance. The update of the parameters\nis done, using the gradient that, at each step, directs towards minimizing the\nKullback--Leibler divergence from the prior to the posterior distribution (both\nbeing normal-gamma). We obtain a corresponding dynamical system for the prior's\nparameters and analyze its properties. In particular, we study the limiting\nbehavior of all the prior's parameters and show how it differs from the case of\nthe classical full Bayesian update. The results are validated on synthetic and\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 21:29:52 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 14:27:07 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 09:20:22 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Gurevich", "Pavel", ""], ["Stuke", "Hannes", ""]]}, {"id": "1802.02664", "submitter": "Valentin Khrulkov", "authors": "Valentin Khrulkov and Ivan Oseledets", "title": "Geometry Score: A Method For Comparing Generative Adversarial Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges in the research of generative adversarial\nnetworks (GANs) is assessing the quality of generated samples and detecting\nvarious levels of mode collapse. In this work, we construct a novel measure of\nperformance of a GAN by comparing geometrical properties of the underlying data\nmanifold and the generated one, which provides both qualitative and\nquantitative means for evaluation. Our algorithm can be applied to datasets of\nan arbitrary nature and is not limited to visual data. We test the obtained\nmetric on various real-life models and datasets and demonstrate that our method\nprovides new insights into properties of GANs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 22:44:37 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 21:00:36 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 14:44:41 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Khrulkov", "Valentin", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1802.02798", "submitter": "Sean Rowan", "authors": "Sean Rowan", "title": "Transductive Adversarial Networks (TAN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive Adversarial Networks (TAN) is a novel domain-adaptation machine\nlearning framework that is designed for learning a conditional probability\ndistribution on unlabelled input data in a target domain, while also only\nhaving access to: (1) easily obtained labelled data from a related source\ndomain, which may have a different conditional probability distribution than\nthe target domain, and (2) a marginalised prior distribution on the labels for\nthe target domain. TAN leverages a fully adversarial training procedure and a\nunique generator/encoder architecture which approximates the transductive\ncombination of the available source- and target-domain data. A benefit of TAN\nis that it allows the distance between the source- and target-domain\nlabel-vector marginal probability distributions to be greater than 0 (i.e.\ndifferent tasks across the source and target domains) whereas other\ndomain-adaptation algorithms require this distance to equal 0 (i.e. a single\ntask across the source and target domains). TAN can, however, still handle the\nlatter case and is a more generalised approach to this case. Another benefit of\nTAN is that due to being a fully adversarial algorithm, it has the potential to\naccurately approximate highly complex distributions. Theoretical analysis\ndemonstrates the viability of the TAN framework.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 10:44:53 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Rowan", "Sean", ""]]}, {"id": "1802.02840", "submitter": "Shuohui Li", "authors": "Shuo-Hui Li and Lei Wang", "title": "Neural Network Renormalization Group", "comments": "Main text: 4.5 pages, 4 figures. Supplement: 3 pages. Github link:\n  https://github.com/li012589/NeuralRG", "journal-ref": "Phys. Rev. Lett. 121, 260601 (2018)", "doi": "10.1103/PhysRevLett.121.260601", "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variational renormalization group (RG) approach using a deep\ngenerative model based on normalizing flows. The model performs hierarchical\nchange-of-variables transformations from the physical space to a latent space\nwith reduced mutual information. Conversely, the neural net directly maps\nindependent Gaussian noises to physical configurations following the inverse RG\nflow. The model has an exact and tractable likelihood, which allows unbiased\ntraining and direct access to the renormalized energy function of the latent\nvariables. To train the model, we employ probability density distillation for\nthe bare energy function of the physical problem, in which the training loss\nprovides a variational upper bound of the physical free energy. We demonstrate\npractical usage of the approach by identifying mutually independent collective\nvariables of the Ising model and performing accelerated hybrid Monte Carlo\nsampling in the latent space. Lastly, we comment on the connection of the\npresent approach to the wavelet formulation of RG and the modern pursuit of\ninformation preserving RG.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 13:16:01 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 09:17:25 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 13:48:25 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 04:14:43 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Shuo-Hui", ""], ["Wang", "Lei", ""]]}, {"id": "1802.02852", "submitter": "Emmi Jokinen", "authors": "Emmi Jokinen, Markus Heinonen, Harri L\\\"ahdesm\\\"aki", "title": "mGPfusion: Predicting protein stability changes with Gaussian process\n  kernel learning and data fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proteins are commonly used by biochemical industry for numerous processes.\nRefining these proteins' properties via mutations causes stability effects as\nwell. Accurate computational method to predict how mutations affect protein\nstability are necessary to facilitate efficient protein design. However,\naccuracy of predictive models is ultimately constrained by the limited\navailability of experimental data. We have developed mGPfusion, a novel\nGaussian process (GP) method for predicting protein's stability changes upon\nsingle and multiple mutations. This method complements the limited experimental\ndata with large amounts of molecular simulation data. We introduce a Bayesian\ndata fusion model that re-calibrates the experimental and in silico data\nsources and then learns a predictive GP model from the combined data. Our\nprotein-specific model requires experimental data only regarding the protein of\ninterest and performs well even with few experimental measurements. The\nmGPfusion models proteins by contact maps and infers the stability effects\ncaused by mutations with a mixture of graph kernels. Our results show that\nmGPfusion outperforms state-of-the-art methods in predicting protein stability\non a dataset of 15 different proteins and that incorporating molecular\nsimulation data improves the model learning and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 13:41:37 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 08:20:12 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Jokinen", "Emmi", ""], ["Heinonen", "Markus", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]]}, {"id": "1802.02856", "submitter": "Didier Fraix-Burnet", "authors": "Tanuka Chattopadhyay, Didier Fraix-Burnet (IPAG), Saptarshi Mondal", "title": "Unsupervised Classification of Galaxies. I. ICA feature selection", "comments": "The first version of this paper was entitled \"Multivariate Study of\n  the Star Formation Rate in Galaxies: Bimodality Revisited\" and submitted in\n  June 2017. This accepted version is slightly shortened and more focused on\n  the statistical analysis", "journal-ref": "Publications of the Astronomical Society of the Pacific,\n  Astronomical Society of the Pacific, In press", "doi": "10.1088/1538-3873/aaf7c6", "report-no": null, "categories": "astro-ph.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective classification of galaxies can mislead us in the quest of the\norigin regarding formation and evolution of galaxies since this is necessarily\nlimited to a few features. The human mind is not able to apprehend the complex\ncorrelations in a manyfold parameter space, and multivariate analyses are the\nbest tools to understand the differences among various kinds of objects. In\nthis series of papers, an objective classification of 362,923 galaxies from the\nValue Added Galaxy Catalogue (VAGC) is carried out with the help of two methods\nof multivariate analysis. First, Independent Component Analysis (ICA) is used\nto determine a set of derived independent components that are linear\ncombinations of 47 observed features (viz. ionized lines, Lick indices,\nphotometric and morphological properties, star formation rates etc.) of the\ngalaxies. Subsequently, a K-means cluster analysis is applied on the nine\nindependent components to obtain ten distinct and homogeneous groups. In this\nfirst paper, we describe the methods and the main results. It appears that the\nnine Independent Components represent a complete physical description of\ngalaxies (velocity dispersion, ionisation, metallicity, surface brightness and\nstructure). We find that our ten groups can be essentially placed into\ntraditional and empirical classes (from colour-magnitude and emission-line\ndiagnostic diagrams, early- vs late-types) despite the classical corresponding\nfeatures (colour, line ratios and morphology) being not significantly\ncorrelated with the nine Independent Components. More detailed physical\ninterpretation of the groups will be performed in subsequent papers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 13:57:05 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:36:13 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chattopadhyay", "Tanuka", "", "IPAG"], ["Fraix-Burnet", "Didier", "", "IPAG"], ["Mondal", "Saptarshi", ""]]}, {"id": "1802.02885", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong, Nikos Deligiannis, S{\\o}ren Forchhammer, and Andr\\'e\n  Kaup", "title": "Online Decomposition of Compressive Streaming Data Using $n$-$\\ell_1$\n  Cluster-Weighted Minimization", "comments": "accepted to Data Compression Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decomposition method for compressive streaming data in the\ncontext of online compressive Robust Principle Component Analysis (RPCA). The\nproposed decomposition solves an $n$-$\\ell_1$ cluster-weighted minimization to\ndecompose a sequence of frames (or vectors), into sparse and low-rank\ncomponents, from compressive measurements. Our method processes a data vector\nof the stream per time instance from a small number of measurements in contrast\nto conventional batch RPCA, which needs to access full data. The $n$-$\\ell_1$\ncluster-weighted minimization leverages the sparse components along with their\ncorrelations with multiple previously-recovered sparse vectors. Moreover, the\nproposed minimization can exploit the structures of sparse components via\nclustering and re-weighting iteratively. The method outperforms the existing\nmethods for both numerical data and actual video data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 14:42:13 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Van Luong", "Huynh", ""], ["Deligiannis", "Nikos", ""], ["Forchhammer", "S\u00f8ren", ""], ["Kaup", "Andr\u00e9", ""]]}, {"id": "1802.02896", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan Rossi, John Boaz Lee, Theodore L. Willke, Rong\n  Zhou, Xiangnan Kong, Hoda Eldardiry", "title": "Learning Role-based Graph Embeddings", "comments": "StarAI workshop @ IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walks are at the heart of many existing network embedding methods.\nHowever, such algorithms have many limitations that arise from the use of\nrandom walks, e.g., the features resulting from these methods are unable to\ntransfer to new nodes and graphs as they are tied to vertex identity. In this\nwork, we introduce the Role2Vec framework which uses the flexible notion of\nattributed random walks, and serves as a basis for generalizing existing\nmethods such as DeepWalk, node2vec, and many others that leverage random walks.\nOur proposed framework enables these methods to be more widely applicable for\nboth transductive and inductive learning as well as for use on graphs with\nattributes (if available). This is achieved by learning functions that\ngeneralize to new nodes and graphs. We show that our proposed framework is\neffective with an average AUC improvement of 16.55% while requiring on average\n853x less space than existing methods on a variety of graphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 00:29:44 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 23:36:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan", ""], ["Lee", "John Boaz", ""], ["Willke", "Theodore L.", ""], ["Zhou", "Rong", ""], ["Kong", "Xiangnan", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "1802.02907", "submitter": "Rui Zhang", "authors": "Rui Zhang and Quanyan Zhu", "title": "A Game-Theoretic Approach to Design Secure and Resilient Distributed\n  Support Vector Machines", "comments": "arXiv admin note: text overlap with arXiv:1710.04677", "journal-ref": null, "doi": "10.1109/TNNLS.2018.2802721", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Support Vector Machines (DSVM) have been developed to solve\nlarge-scale classification problems in networked systems with a large number of\nsensors and control units. However, the systems become more vulnerable as\ndetection and defense are increasingly difficult and expensive. This work aims\nto develop secure and resilient DSVM algorithms under adversarial environments\nin which an attacker can manipulate the training data to achieve his objective.\nWe establish a game-theoretic framework to capture the conflicting interests\nbetween an adversary and a set of distributed data processing units. The Nash\nequilibrium of the game allows predicting the outcome of learning algorithms in\nadversarial environments, and enhancing the resilience of the machine learning\nthrough dynamic distributed learning algorithms. We prove that the convergence\nof the distributed algorithm is guaranteed without assumptions on the training\ndata or network topologies. Numerical experiments are conducted to corroborate\nthe results. We show that network topology plays an important role in the\nsecurity of DSVM. Networks with fewer nodes and higher average degrees are more\nsecure. Moreover, a balanced network is found to be less vulnerable to attacks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:06:48 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhang", "Rui", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1802.02920", "submitter": "Anru Zhang", "authors": "Anru Zhang and Mengdi Wang", "title": "Spectral State Compression of Markov Processes", "comments": "to appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model reduction of Markov processes is a basic problem in modeling\nstate-transition systems. Motivated by the state aggregation approach rooted in\ncontrol theory, we study the statistical state compression of a discrete-state\nMarkov chain from empirical trajectories. Through the lens of spectral\ndecomposition, we study the rank and features of Markov processes, as well as\nproperties like representability, aggregability, and lumpability. We develop\nspectral methods for estimating the transition matrix of a low-rank Markov\nmodel, estimating the leading subspace spanned by Markov features, and\nrecovering latent structures like state aggregation and lumpable partition of\nthe state space. We prove statistical upper bounds for the estimation errors\nand nearly matching minimax lower bounds. Numerical studies are performed on\nsynthetic data and a dataset of New York City taxi trips.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:28:46 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 03:34:14 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 03:08:25 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhang", "Anru", ""], ["Wang", "Mengdi", ""]]}, {"id": "1802.02961", "submitter": "Daniel Recoskie", "authors": "Daniel Recoskie and Richard Mann", "title": "Learning Sparse Wavelet Representations", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method for learning wavelet filters directly from\ndata. We accomplish this by framing the discrete wavelet transform as a\nmodified convolutional neural network. We introduce an autoencoder wavelet\ntransform network that is trained using gradient descent. We show that the\nmodel is capable of learning structured wavelet filters from synthetic and real\ndata. The learned wavelets are shown to be similar to traditional wavelets that\nare derived using Fourier methods. Our method is simple to implement and easily\nincorporated into neural network architectures. A major advantage to our model\nis that we can learn from raw audio data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 16:49:00 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Recoskie", "Daniel", ""], ["Mann", "Richard", ""]]}, {"id": "1802.03001", "submitter": "Shin Matsushima", "authors": "Shin Matsushima", "title": "Statistical Learnability of Generalized Additive Models based on Total\n  Variation Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized additive model (GAM, Hastie and Tibshirani (1987)) is a\nnonparametric model by the sum of univariate functions with respect to each\nexplanatory variable, i.e., $f({\\mathbf x}) = \\sum f_j(x_j)$, where\n$x_j\\in\\mathbb{R}$ is $j$-th component of a sample ${\\mathbf x}\\in\n\\mathbb{R}^p$. In this paper, we introduce the total variation (TV) of a\nfunction as a measure of the complexity of functions in $L^1_{\\rm\nc}(\\mathbb{R})$-space. Our analysis shows that a GAM based on TV-regularization\nexhibits a Rademacher complexity of $O(\\sqrt{\\frac{\\log p}{m}})$, which is\ntight in terms of both $m$ and $p$ in the agnostic case of the classification\nproblem. In result, we obtain generalization error bounds for finite samples\naccording to work by Bartlett and Mandelson (2002).\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 18:36:28 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 18:18:23 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Matsushima", "Shin", ""]]}, {"id": "1802.03039", "submitter": "Akisato Kimura", "authors": "Akisato Kimura, Zoubin Ghahramani, Koh Takeuchi, Tomoharu Iwata,\n  Naonori Ueda", "title": "Few-shot learning of neural networks from scratch by pseudo example\n  optimization", "comments": "14 pages, 2 figures, will be presented at BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple but effective method for training neural\nnetworks with a limited amount of training data. Our approach inherits the idea\nof knowledge distillation that transfers knowledge from a deep or wide\nreference model to a shallow or narrow target model. The proposed method\nemploys this idea to mimic predictions of reference estimators that are more\nrobust against overfitting than the network we want to train. Different from\nalmost all the previous work for knowledge distillation that requires a large\namount of labeled training data, the proposed method requires only a small\namount of training data. Instead, we introduce pseudo training examples that\nare optimized as a part of model parameters. Experimental results for several\nbenchmark datasets demonstrate that the proposed method outperformed all the\nother baselines, such as naive training of the target model and standard\nknowledge distillation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 20:28:01 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:00:17 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 15:13:58 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Kimura", "Akisato", ""], ["Ghahramani", "Zoubin", ""], ["Takeuchi", "Koh", ""], ["Iwata", "Tomoharu", ""], ["Ueda", "Naonori", ""]]}, {"id": "1802.03041", "submitter": "Luis Mu\\~noz-Gonz\\'alez", "authors": "Andrea Paudice, Luis Mu\\~noz-Gonz\\'alez, Andras Gyorgy, Emil C. Lupu", "title": "Detection of Adversarial Training Examples in Poisoning Attacks through\n  Anomaly Detection", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become an important component for many systems and\napplications including computer vision, spam filtering, malware and network\nintrusion detection, among others. Despite the capabilities of machine learning\nalgorithms to extract valuable information from data and produce accurate\npredictions, it has been shown that these algorithms are vulnerable to attacks.\nData poisoning is one of the most relevant security threats against machine\nlearning systems, where attackers can subvert the learning process by injecting\nmalicious samples in the training data. Recent work in adversarial machine\nlearning has shown that the so-called optimal attack strategies can\nsuccessfully poison linear classifiers, degrading the performance of the system\ndramatically after compromising a small fraction of the training dataset. In\nthis paper we propose a defence mechanism to mitigate the effect of these\noptimal poisoning attacks based on outlier detection. We show empirically that\nthe adversarial examples generated by these attack strategies are quite\ndifferent from genuine points, as no detectability constrains are considered to\ncraft the attack. Hence, they can be detected with an appropriate pre-filtering\nof the training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 20:34:19 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Paudice", "Andrea", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Gyorgy", "Andras", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1802.03050", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Matyas Sustik, Quoc Tran and Brian Seaman", "title": "Thompson Sampling for Dynamic Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply active learning algorithms for dynamic pricing in a\nprominent e-commerce website. Dynamic pricing involves changing the price of\nitems on a regular basis, and uses the feedback from the pricing decisions to\nupdate prices of the items. Most popular approaches to dynamic pricing use a\npassive learning approach, where the algorithm uses historical data to learn\nvarious parameters of the pricing problem, and uses the updated parameters to\ngenerate a new set of prices. We show that one can use active learning\nalgorithms such as Thompson sampling to more efficiently learn the underlying\nparameters in a pricing problem. We apply our algorithms to a real e-commerce\nsystem and show that the algorithms indeed improve revenue compared to pricing\nalgorithms that use passive learning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 21:17:28 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Ganti", "Ravi", ""], ["Sustik", "Matyas", ""], ["Tran", "Quoc", ""], ["Seaman", "Brian", ""]]}, {"id": "1802.03063", "submitter": "Ozsel Kilinc", "authors": "Ozsel Kilinc, Ismail Uysal", "title": "Learning Latent Representations in Neural Networks for Clustering\n  through Pseudo Supervision and Graph-based Activity Regularization", "comments": "To appear in proceedings of the International Conference on Learning\n  Representations 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised clustering approach exploiting\nthe hidden information that is indirectly introduced through a pseudo\nclassification objective. Specifically, we randomly assign a pseudo\nparent-class label to each observation which is then modified by applying the\ndomain specific transformation associated with the assigned label. Generated\npseudo observation-label pairs are subsequently used to train a neural network\nwith Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes\nfor each pseudo parent-class. Due to the unsupervised objective based on\nGraph-based Activity Regularization (GAR) terms, softmax duplicates of each\nparent-class are specialized as the hidden information captured through the\nhelp of domain specific transformations is propagated during training.\nUltimately we obtain a k-means friendly latent representation. Furthermore, we\ndemonstrate how the chosen transformation type impacts performance and helps\npropagate the latent information that is useful in revealing unknown clusters.\nOur results show state-of-the-art performance for unsupervised clustering tasks\non MNIST, SVHN and USPS datasets, with the highest accuracies reported to date\nin the literature.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 22:13:59 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Kilinc", "Ozsel", ""], ["Uysal", "Ismail", ""]]}, {"id": "1802.03065", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Tuanfeng Zhang, Peter Tilke, Lin Liang, William Bailey", "title": "Generating Realistic Geology Conditioned on Physical Measurements with\n  Generative Adversarial Networks", "comments": "Added ICML workshop info, more specific training details and more\n  details on how images are chosen", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.comp-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in geostatistics is to build models of the subsurface of\nthe Earth given physical measurements at sparse spatial locations. Typically,\nthis is done using spatial interpolation methods or by reproducing patterns\nfrom a reference image. However, these algorithms fail to produce realistic\npatterns and do not exhibit the wide range of uncertainty inherent in the\nprediction of geology. In this paper, we show how semantic inpainting with\nGenerative Adversarial Networks can be used to generate varied realizations of\ngeology which honor physical measurements while matching the expected\ngeological patterns. In contrast to other algorithms, our method scales well\nwith the number of data points and mimics a distribution of patterns as opposed\nto a single pattern or image. The generated conditional samples are state of\nthe art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 22:41:22 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 00:21:05 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 17:56:39 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Dupont", "Emilien", ""], ["Zhang", "Tuanfeng", ""], ["Tilke", "Peter", ""], ["Liang", "Lin", ""], ["Bailey", "William", ""]]}, {"id": "1802.03101", "submitter": "Martin Loncaric", "authors": "Martin Loncaric and Bowei Liu and Ryan Weber", "title": "Convolutional Hashing for Automated Scene Matching", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 02:11:18 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Loncaric", "Martin", ""], ["Liu", "Bowei", ""], ["Weber", "Ryan", ""]]}, {"id": "1802.03127", "submitter": "Takayuki Kawashima", "authors": "Takayuki Kawashima and Hironori Fujisawa", "title": "Robust and Sparse Regression in GLM by Stochastic Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear model (GLM) plays a key role in regression analyses.\nIn high-dimensional data, the sparse GLM has been used but it is not robust\nagainst outliers. Recently, the robust methods have been proposed for the\nspecific example of the sparse GLM. Among them, we focus on the robust and\nsparse linear regression based on the $\\gamma$-divergence. The estimator of the\n$\\gamma$-divergence has strong robustness under heavy contamination. In this\npaper, we extend the robust and sparse linear regression based on the\n$\\gamma$-divergence to the robust and sparse GLM based on the\n$\\gamma$-divergence with a stochastic optimization approach in order to obtain\nthe estimate. We adopt the randomized stochastic projected gradient descent as\na stochastic optimization approach and extend the established convergence\nproperty to the classical first-order necessary condition. By virtue of the\nstochastic optimization approach, we can efficiently estimate parameters for\nvery large problems. Particularly, we show the linear regression, logistic\nregression and Poisson regression with $L_1$ regularization in detail as\nspecific examples of robust and sparse GLM. In numerical experiments and real\ndata analysis, the proposed method outperformed comparative methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 04:51:50 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Kawashima", "Takayuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1802.03133", "submitter": "Liang Lin", "authors": "Guangrun Wang and Jiefeng Peng and Ping Luo and Xinjiang Wang and\n  Liang Lin", "title": "Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches", "comments": "We presented how to improve and accelerate the training of DNNs,\n  particularly under the context of micro-batches. (Submitted to IJCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 05:19:16 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:01:50 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Wang", "Guangrun", ""], ["Peng", "Jiefeng", ""], ["Luo", "Ping", ""], ["Wang", "Xinjiang", ""], ["Lin", "Liang", ""]]}, {"id": "1802.03145", "submitter": "Qinxue Meng", "authors": "Qinxue Meng, Daniel Catchpoole, David Skillicorn, Paul J. Kennedy", "title": "Relational Autoencoder for Feature Extraction", "comments": "IJCNN-2017", "journal-ref": "2017 International Joint Conference on Neural Networks (IJCNN),\n  Anchorage, AK, 2017, pp. 364-371", "doi": "10.1109/IJCNN.2017.7965877", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction becomes increasingly important as data grows high\ndimensional. Autoencoder as a neural network based feature extraction method\nachieves great success in generating abstract features of high dimensional\ndata. However, it fails to consider the relationships of data samples which may\naffect experimental results of using original and new features. In this paper,\nwe propose a Relation Autoencoder model considering both data features and\ntheir relationships. We also extend it to work with other major autoencoder\nmodels including Sparse Autoencoder, Denoising Autoencoder and Variational\nAutoencoder. The proposed relational autoencoder models are evaluated on a set\nof benchmark datasets and the experimental results show that considering data\nrelationships can generate more robust features which achieve lower\nconstruction loss and then lower error rate in further classification compared\nto the other variants of autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 06:42:38 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Meng", "Qinxue", ""], ["Catchpoole", "Daniel", ""], ["Skillicorn", "David", ""], ["Kennedy", "Paul J.", ""]]}, {"id": "1802.03151", "submitter": "Seyed Ali Osia", "authors": "Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis Katevas,\n  Hamed Haddadi, Hamid R. Rabiee", "title": "Deep Private-Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model\nwhich is trained and evaluated based on information theoretic constraints.\nUsing the selective exchange of information between a user's device and a\nservice provider, DPFE enables the user to prevent certain sensitive\ninformation from being shared with a service provider, while allowing them to\nextract approved information using their model. We introduce and utilize the\nlog-rank privacy, a novel measure to assess the effectiveness of DPFE in\nremoving sensitive information and compare different models based on their\naccuracy-privacy tradeoff. We then implement and evaluate the performance of\nDPFE on smartphones to understand its complexity, resource demands, and\nefficiency tradeoffs. Our results on benchmark image datasets demonstrate that\nunder moderate resource utilization, DPFE can achieve high accuracy for primary\ntasks while preserving the privacy of sensitive features.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:12:29 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:32:47 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Osia", "Seyed Ali", ""], ["Taheri", "Ali", ""], ["Shamsabadi", "Ali Shahin", ""], ["Katevas", "Kleomenis", ""], ["Haddadi", "Hamed", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1802.03170", "submitter": "Xiang Li", "authors": "Shuo Chen, Chen Gong, Jian Yang, Xiang Li, Yang Wei, Jun Li", "title": "Adversarial Metric Learning", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, intensive efforts have been put to design various loss\nfunctions and metric forms for metric learning problem. These improvements have\nshown promising results when the test data is similar to the training data.\nHowever, the trained models often fail to produce reliable distances on the\nambiguous test pairs due to the distribution bias between training set and test\nset. To address this problem, the Adversarial Metric Learning (AML) is proposed\nin this paper, which automatically generates adversarial pairs to remedy the\ndistribution bias and facilitate robust metric learning. Specifically, AML\nconsists of two adversarial stages, i.e. confusion and distinguishment. In\nconfusion stage, the ambiguous but critical adversarial data pairs are\nadaptively generated to mislead the learned metric. In distinguishment stage, a\nmetric is exhaustively learned to try its best to distinguish both the\nadversarial pairs and the original training pairs. Thanks to the challenges\nposed by the confusion stage in such competing process, the AML model is able\nto grasp plentiful difficult knowledge that has not been contained by the\noriginal training pairs, so the discriminability of AML can be significantly\nimproved. The entire model is formulated into optimization framework, of which\nthe global convergence is theoretically proved. The experimental results on toy\ndata and practical datasets clearly demonstrate the superiority of AML to the\nrepresentative state-of-the-art metric learning methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 08:43:57 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Chen", "Shuo", ""], ["Gong", "Chen", ""], ["Yang", "Jian", ""], ["Li", "Xiang", ""], ["Wei", "Yang", ""], ["Li", "Jun", ""]]}, {"id": "1802.03171", "submitter": "Long Yang", "authors": "Long Yang, Minhao Shi, Qian Zheng, Wenjia Meng, Gang Pan", "title": "A Unified Approach for Multi-step Temporal-Difference Learning with\n  Eligibility Traces in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a new multi-step temporal learning algorithm, called $Q(\\sigma)$,\nunifies $n$-step Tree-Backup (when $\\sigma=0$) and $n$-step Sarsa (when\n$\\sigma=1$) by introducing a sampling parameter $\\sigma$. However, similar to\nother multi-step temporal-difference learning algorithms, $Q(\\sigma)$ needs\nmuch memory consumption and computation time. Eligibility trace is an important\nmechanism to transform the off-line updates into efficient on-line ones which\nconsume less memory and computation time. In this paper, we further develop the\noriginal $Q(\\sigma)$, combine it with eligibility traces and propose a new\nalgorithm, called $Q(\\sigma ,\\lambda)$, in which $\\lambda$ is trace-decay\nparameter. This idea unifies Sarsa$(\\lambda)$ (when $\\sigma =1$) and\n$Q^{\\pi}(\\lambda)$ (when $\\sigma =0$). Furthermore, we give an upper error\nbound of $Q(\\sigma ,\\lambda)$ policy evaluation algorithm. We prove that\n$Q(\\sigma,\\lambda)$ control algorithm can converge to the optimal value\nfunction exponentially. We also empirically compare it with conventional\ntemporal-difference learning methods. Results show that, with an intermediate\nvalue of $\\sigma$, $Q(\\sigma ,\\lambda)$ creates a mixture of the existing\nalgorithms that can learn the optimal value significantly faster than the\nextreme end ($\\sigma=0$, or $1$).\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 08:46:21 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Long", ""], ["Shi", "Minhao", ""], ["Zheng", "Qian", ""], ["Meng", "Wenjia", ""], ["Pan", "Gang", ""]]}, {"id": "1802.03184", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Christian Walder", "title": "Self-Bounded Prediction Suffix Tree via Approximate String Matching", "comments": "Proceedings of the 35th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction suffix trees (PST) provide an effective tool for sequence\nmodelling and prediction. Current prediction techniques for PSTs rely on exact\nmatching between the suffix of the current sequence and the previously observed\nsequence. We present a provably correct algorithm for learning a PST with\napproximate suffix matching by relaxing the exact matching condition. We then\npresent a self-bounded enhancement of our algorithm where the depth of suffix\ntree grows automatically in response to the model performance on a training\nsequence. Through experiments on synthetic datasets as well as three real-world\ndatasets, we show that the approximate matching PST results in better\npredictive performance than the other variants of PST.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 09:52:35 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 03:58:01 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Kim", "Dongwoo", ""], ["Walder", "Christian", ""]]}, {"id": "1802.03203", "submitter": "J\\'er\\'emy Emile Cohen", "authors": "Jeremy Emile Cohen, Rodrigo Cabral Farias, Bertrand Rivet", "title": "Curve Registered Coupled Low Rank Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the canonical polyadic (CP) tensor model where one\nof the latent factors is allowed to vary through data slices in a constrained\nway. The components of the latent factors, which we want to retrieve from data,\ncan vary from one slice to another up to a diffeomorphism. We suppose that the\ndiffeomorphisms are also unknown, thus merging curve registration and tensor\ndecomposition in one model, which we call registered CP. We present an\nalgorithm to retrieve both the latent factors and the diffeomorphism, which is\nassumed to be in a parametrized form. At the end of the paper, we show\nsimulation results comparing registered CP with other models from the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 11:08:00 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Cohen", "Jeremy Emile", ""], ["Farias", "Rodrigo Cabral", ""], ["Rivet", "Bertrand", ""]]}, {"id": "1802.03212", "submitter": "Louis Falissard", "authors": "Louis Falissard, Guy Fagherazzi, Newton Howard and Bruno Falissard", "title": "Deep clustering of longitudinal data", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a family of computational models that have led to a\ndramatical improvement of the state of the art in several domains such as\nimage, voice or text analysis. These methods provide a framework to model\ncomplex, non-linear interactions in large datasets, and are naturally suited to\nthe analysis of hierarchical data such as, for instance, longitudinal data with\nthe use of recurrent neural networks. In the other hand, cohort studies have\nbecome a tool of importance in the research field of epidemiology. In such\nstudies, variables are measured repeatedly over time, to allow the practitioner\nto study their temporal evolution as trajectories, and, as such, as\nlongitudinal data. This paper investigates the application of the advanced\nmodelling techniques provided by the deep learning framework in the analysis of\nthe longitudinal data provided by cohort studies. Methods: A method for\nvisualizing and clustering longitudinal dataset is proposed, and compared to\nother widely used approaches to the problem on both real and simulated\ndatasets. Results: The proposed method is shown to be coherent with the\npreexisting procedures on simple tasks, and to outperform them on more complex\ntasks such as the partitioning of longitudinal datasets into non-spherical\nclusters. Conclusion: Deep artificial neural networks can be used to visualize\nlongitudinal data in a low dimensional manifold that is much simpler to\ninterpret than traditional longitudinal plots are. Consequently, practitioners\nshould start considering the use of deep artificial neural networks for the\nanalysis of their longitudinal data in studies to come.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 11:44:21 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Falissard", "Louis", ""], ["Fagherazzi", "Guy", ""], ["Howard", "Newton", ""], ["Falissard", "Bruno", ""]]}, {"id": "1802.03236", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Timothy A. Mann, Pierre-Luc Bacon, Doina Precup\n  and Shie Mannor", "title": "Learning Robust Options", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust reinforcement learning aims to produce policies that have strong\nguarantees even in the face of environments/transition models whose parameters\nhave strong uncertainty. Existing work uses value-based methods and the usual\nprimitive action setting. In this paper, we propose robust methods for learning\ntemporally abstract actions, in the framework of options. We present a Robust\nOptions Policy Iteration (ROPI) algorithm with convergence guarantees, which\nlearns options that are robust to model uncertainty. We utilize ROPI to learn\nrobust options with the Robust Options Deep Q Network (RO-DQN) that solves\nmultiple tasks and mitigates model misspecification due to model uncertainty.\nWe present experimental results which suggest that policy iteration with linear\nfeatures may have an inherent form of robustness when using coarse feature\nrepresentations. In addition, we present experimental results which demonstrate\nthat robustness helps policy iteration implemented on top of deep neural\nnetworks to generalize over a much broader range of dynamics than non-robust\npolicy iteration.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 12:52:06 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Mann", "Timothy A.", ""], ["Bacon", "Pierre-Luc", ""], ["Precup", "Doina", ""], ["Mannor", "Shie", ""]]}, {"id": "1802.03239", "submitter": "Avi Rosenfeld", "authors": "Avi Rosenfeld and Ron Illuz and Dovid Gottesman and Mark Last", "title": "Using Discretization for Extending the Set of Predictive Features", "comments": "14 pages", "journal-ref": "EURASIP Journal on Advances in Signal Processing 2018:7", "doi": "10.1186/s13634-018-0528-x", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, attribute discretization is typically performed by replacing the\noriginal set of continuous features with a transposed set of discrete ones.\nThis paper provides support for a new idea that discretized features should\noften be used in addition to existing features and as such, datasets should be\nextended, and not replaced, by discretization. We also claim that\ndiscretization algorithms should be developed with the explicit purpose of\nenriching a non-discretized dataset with discretized values. We present such an\nalgorithm, D-MIAT, a supervised algorithm that discretizes data based on\nMinority Interesting Attribute Thresholds. D-MIAT only generates new features\nwhen strong indications exist for one of the target values needing to be\nlearned and thus is intended to be used in addition to the original data. We\npresent extensive empirical results demonstrating the success of using D-MIAT\non $ 28 $ benchmark datasets. We also demonstrate that $ 10 $ other\ndiscretization algorithms can also be used to generate features that yield\nimproved performance when used in combination with the original non-discretized\ndata. Our results show that the best predictive performance is attained using a\ncombination of the original dataset with added features from a \"standard\"\nsupervised discretization algorithm and D-MIAT.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:00:44 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Rosenfeld", "Avi", ""], ["Illuz", "Ron", ""], ["Gottesman", "Dovid", ""], ["Last", "Mark", ""]]}, {"id": "1802.03248", "submitter": "Zicheng Liao", "authors": "Chaowei Fang, Zicheng Liao, Yizhou Yu", "title": "Piecewise Flat Embedding for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat\nEmbedding (PFE) -- for image segmentation. Based on the theory of sparse signal\nrecovery, piecewise flat embedding with diverse channels attempts to recover a\npiecewise constant image representation with sparse region boundaries and\nsparse cluster value scattering. The resultant piecewise flat embedding\nexhibits interesting properties such as suppressing slowly varying signals, and\noffers an image representation with higher region identifiability which is\ndesirable for image segmentation or high-level semantic analysis tasks. We\nformulate our embedding as a variant of the Laplacian Eigenmap embedding with\nan $L_{1,p} (0<p\\leq1)$ regularization term to promote sparse solutions. First,\nwe devise a two-stage numerical algorithm based on Bregman iterations to\ncompute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize\nthis algorithm through iterative reweighting to solve the general\n$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE\ninto two existing image segmentation frameworks, segmentation based on\nclustering and hierarchical segmentation based on contour detection.\nExperiments on four major benchmark datasets, BSDS500, MSRC, Stanford\nBackground Dataset, and PASCAL Context, show that segmentation algorithms\nincorporating our embedding achieve significantly improved results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:19:14 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 06:58:28 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 11:24:59 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 04:42:53 GMT"}, {"version": "v5", "created": "Sun, 20 May 2018 07:22:43 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Fang", "Chaowei", ""], ["Liao", "Zicheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1802.03268", "submitter": "Hieu Pham", "authors": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean", "title": "Efficient Neural Architecture Search via Parameter Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:14:37 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 03:34:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Pham", "Hieu", ""], ["Guan", "Melody Y.", ""], ["Zoph", "Barret", ""], ["Le", "Quoc V.", ""], ["Dean", "Jeff", ""]]}, {"id": "1802.03284", "submitter": "Feihu Huang", "authors": "Feihu Huang and Songcan Chen", "title": "Mini-Batch Stochastic ADMMs for Nonconvex Nonsmooth Optimization", "comments": "We have fixed some errors in the proofs. arXiv admin note: text\n  overlap with arXiv:1610.02758", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large rising of complex data, the nonconvex models such as nonconvex\nloss function and nonconvex regularizer are widely used in machine learning and\npattern recognition. In this paper, we propose a class of mini-batch stochastic\nADMMs (alternating direction method of multipliers) for solving large-scale\nnonconvex nonsmooth problems. We prove that, given an appropriate mini-batch\nsize, the mini-batch stochastic ADMM without variance reduction (VR) technique\nis convergent and reaches a convergence rate of $O(1/T)$ to obtain a stationary\npoint of the nonconvex optimization, where $T$ denotes the number of\niterations. Moreover, we extend the mini-batch stochastic gradient method to\nboth the nonconvex SVRG-ADMM and SAGA-ADMM proposed in our initial manuscript\n\\cite{huang2016stochastic}, and prove these mini-batch stochastic ADMMs also\nreaches the convergence rate of $O(1/T)$ without condition on the mini-batch\nsize. In particular, we provide a specific parameter selection for step size\n$\\eta$ of stochastic gradients and penalty parameter $\\rho$ of augmented\nLagrangian function. Finally, extensive experimental results on both simulated\nand real-world data demonstrate the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 02:48:22 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 04:54:31 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 02:45:42 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Huang", "Feihu", ""], ["Chen", "Songcan", ""]]}, {"id": "1802.03300", "submitter": "Johan Segers", "authors": "Simon Guillotte, Fran\\c{c}ois Perron, Johan Segers", "title": "Bayesian inference for bivariate ranks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system based on ranks is proposed, where an expert's ranking of\na set of objects and a user's ranking of a subset of those objects are combined\nto make a prediction of the user's ranking of all objects. The rankings are\nassumed to be induced by latent continuous variables corresponding to the\ngrades assigned by the expert and the user to the objects. The dependence\nbetween the expert and user grades is modelled by a copula in some parametric\nfamily. Given a prior distribution on the copula parameter, the user's complete\nranking is predicted by the mode of the posterior predictive distribution of\nthe user's complete ranking conditional on the expert's complete and the user's\nincomplete rankings. Various Markov chain Monte-Carlo algorithms are proposed\nto approximate the predictive distribution or only its mode. The predictive\ndistribution can be obtained exactly for the Farlie-Gumbel-Morgenstern copula\nfamily, providing a benchmark for the approximation accuracy of the algorithms.\nThe method is applied to the MovieLens 100k dataset with a Gaussian copula\nmodelling dependence between the expert's and user's grades.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:10:14 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Guillotte", "Simon", ""], ["Perron", "Fran\u00e7ois", ""], ["Segers", "Johan", ""]]}, {"id": "1802.03319", "submitter": "Samaneh Ebrahimi", "authors": "Samaneh Ebrahimi, Hossein Vahabi, Matthew Prockup, Oriol Nieto", "title": "Predicting Audio Advertisement Quality", "comments": "WSDM '18 Proceedings of the Eleventh ACM International Conference on\n  Web Search and Data Mining, 9 pages", "journal-ref": "2018. In Proceedings of the Eleventh ACM International Conference\n  on Web Search and Data Mining (WSDM '18)", "doi": "10.1145/3159652.3159701", "report-no": null, "categories": "stat.ML cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online audio advertising is a particular form of advertising used abundantly\nin online music streaming services. In these platforms, which tend to host tens\nof thousands of unique audio advertisements (ads), providing high quality ads\nensures a better user experience and results in longer user engagement.\nTherefore, the automatic assessment of these ads is an important step toward\naudio ads ranking and better audio ads creation. In this paper we propose one\nway to measure the quality of the audio ads using a proxy metric called Long\nClick Rate (LCR), which is defined by the amount of time a user engages with\nthe follow-up display ad (that is shown while the audio ad is playing) divided\nby the impressions. We later focus on predicting the audio ad quality using\nonly acoustic features such as harmony, rhythm, and timbre of the audio,\nextracted from the raw waveform. We discuss how the characteristics of the\nsound can be connected to concepts such as the clarity of the audio ad message,\nits trustworthiness, etc. Finally, we propose a new deep learning model for\naudio ad quality prediction, which outperforms the other discussed models\ntrained on hand-crafted features. To the best of our knowledge, this is the\nfirst large-scale audio ad quality prediction study.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:59:09 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Ebrahimi", "Samaneh", ""], ["Vahabi", "Hossein", ""], ["Prockup", "Matthew", ""], ["Nieto", "Oriol", ""]]}, {"id": "1802.03334", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Thomas B. Sch\\\"on", "title": "Learning Localized Spatio-Temporal Models From Streaming Data", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting spatio-temporal processes with temporal\npatterns that vary across spatial regions, when data is obtained as a stream.\nThat is, when the training dataset is augmented sequentially. Specifically, we\ndevelop a localized spatio-temporal covariance model of the process that can\ncapture spatially varying temporal periodicities in the data. We then apply a\ncovariance-fitting methodology to learn the model parameters which yields a\npredictor that can be updated sequentially with each new data point. The\nproposed method is evaluated using both synthetic and real climate data which\ndemonstrate its ability to accurately predict data missing in spatial regions\nover time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:36:23 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 15:00:01 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1802.03335", "submitter": "Dennis Prangle", "authors": "Thomas Ryder, Andrew Golightly, A. Stephen McGough, Dennis Prangle", "title": "Black-box Variational Inference for Stochastic Differential Equations", "comments": "V3 - revised based on ICML reviewer comments V2 - added\n  acknowledgements and link to code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference for stochastic differential equations is challenging due\nto the presence of a latent diffusion process. Working with an Euler-Maruyama\ndiscretisation for the diffusion, we use variational inference to jointly learn\nthe parameters and the diffusion paths. We use a standard mean-field\nvariational approximation of the parameter posterior, and introduce a recurrent\nneural network to approximate the posterior for the diffusion paths conditional\non the parameters. This neural network learns how to provide Gaussian state\ntransitions which bridge between observations in a very similar way to the\nconditioned diffusion process. The resulting black-box inference method can be\napplied to any SDE system with light tuning requirements. We illustrate the\nmethod on a Lotka-Volterra system and an epidemic model, producing accurate\nparameter estimates in a few hours.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:36:28 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 14:21:28 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 10:31:52 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ryder", "Thomas", ""], ["Golightly", "Andrew", ""], ["McGough", "A. Stephen", ""], ["Prangle", "Dennis", ""]]}, {"id": "1802.03337", "submitter": "Di Wang", "authors": "Di Wang and Jinhui Xu", "title": "Large Scale Constrained Linear Regression Revisited: Faster Algorithms\n  via Preconditioning", "comments": "Appear in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the large-scale constrained linear regression\nproblem and propose faster methods based on some recent developments in\nsketching and optimization. Our algorithms combine (accelerated) mini-batch SGD\nwith a new method called two-step preconditioning to achieve an approximate\nsolution with a time complexity lower than that of the state-of-the-art\ntechniques for the low precision case. Our idea can also be extended to the\nhigh precision case, which gives an alternative implementation to the Iterative\nHessian Sketch (IHS) method with significantly improved time complexity.\nExperiments on benchmark and synthetic datasets suggest that our methods indeed\noutperform existing ones considerably in both the low and high precision cases.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:39:26 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Wang", "Di", ""], ["Xu", "Jinhui", ""]]}, {"id": "1802.03358", "submitter": "Yu-Jhe Li", "authors": "Yun-Chun Chen, Yu-Jhe Li, Aragorn Tseng, and Tsungnan Lin", "title": "Deep Learning for Malicious Flow Detection", "comments": "7 pages, 5 figures, 2017 IEEE 28th Annual International Symposium on\n  Personal, Indoor, and Mobile Radio Communications (PIMRC) (IEEE PIMRC 2017\n  Track 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber security has grown up to be a hot issue in recent years. How to\nidentify potential malware becomes a challenging task. To tackle this\nchallenge, we adopt deep learning approaches and perform flow detection on real\ndata. However, real data often encounters an issue of imbalanced data\ndistribution which will lead to a gradient dilution issue. When training a\nneural network, this problem will not only result in a bias toward the majority\nclass but show the inability to learn from the minority classes. In this paper,\nwe propose an end-to-end trainable Tree-Shaped Deep Neural Network (TSDNN)\nwhich classifies the data in a layer-wise manner. To better learn from the\nminority classes, we propose a Quantity Dependent Backpropagation (QDBP)\nalgorithm which incorporates the knowledge of the disparity between classes. We\nevaluate our method on an imbalanced data set. Experimental result demonstrates\nthat our approach outperforms the state-of-the-art methods and justifies that\nthe proposed method is able to overcome the difficulty of imbalanced learning.\nWe also conduct a partial flow experiment which shows the feasibility of\nreal-time detection and a zero-shot learning experiment which justifies the\ngeneralization capability of deep learning in cyber security.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 17:16:02 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Li", "Yu-Jhe", ""], ["Tseng", "Aragorn", ""], ["Lin", "Tsungnan", ""]]}, {"id": "1802.03360", "submitter": "Vadim Smolyakov", "authors": "Vadim Smolyakov", "title": "Information Planning for Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information planning enables faster learning with fewer training examples. It\nis particularly applicable when training examples are costly to obtain. This\nwork examines the advantages of information planning for text data by focusing\non three supervised models: Naive Bayes, supervised LDA and deep neural\nnetworks. We show that planning based on entropy and mutual information\noutperforms random selection baseline and therefore accelerates learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 17:25:43 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 19:22:30 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 14:45:32 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Smolyakov", "Vadim", ""]]}, {"id": "1802.03375", "submitter": "Bartosz Piotrowski", "authors": "Bartosz Piotrowski, Josef Urban", "title": "ATPboost: Learning Premise Selection in Binary Setting with ATP Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ATPboost is a system for solving sets of large-theory problems by\ninterleaving ATP runs with state-of-the-art machine learning of premise\nselection from the proofs. Unlike many previous approaches that use multi-label\nsetting, the learning is implemented as binary classification that estimates\nthe pairwise-relevance of (theorem, premise) pairs. ATPboost uses for this the\nXGBoost gradient boosting algorithm, which is fast and has state-of-the-art\nperformance on many tasks. Learning in the binary setting however requires\nnegative examples, which is nontrivial due to many alternative proofs. We\ndiscuss and implement several solutions in the context of the ATP/ML feedback\nloop, and show that ATPboost with such methods significantly outperforms the\nk-nearest neighbors multilabel classifier.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 18:29:26 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Piotrowski", "Bartosz", ""], ["Urban", "Josef", ""]]}, {"id": "1802.03396", "submitter": "Bryan Gregory", "authors": "Bryan Gregory", "title": "Predicting Customer Churn: Extreme Gradient Boosting with Temporal Data", "comments": "WSDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting customer churn using large scale time-series data is a\ncommon problem facing many business domains. The creation of model features\nacross various time windows for training and testing can be particularly\nchallenging due to temporal issues common to time-series data. In this paper,\nwe will explore the application of extreme gradient boosting (XGBoost) on a\ncustomer dataset with a wide-variety of temporal features in order to create a\nhighly-accurate customer churn model. In particular, we describe an effective\nmethod for handling temporally sensitive feature engineering. The proposed\nmodel was submitted in the WSDM Cup 2018 Churn Challenge and achieved\nfirst-place out of 575 teams.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 17:24:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Gregory", "Bryan", ""]]}, {"id": "1802.03418", "submitter": "C\\'edric Beaulac", "authors": "C\\'edric Beaulac and Jeffrey S. Rosenthal", "title": "Predicting University Students' Academic Success and Major using Random\n  Forests", "comments": null, "journal-ref": "Research in Higher Education 2019", "doi": "10.1007/s11162-019-09546-y", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, a large data set containing every course taken by every\nundergraduate student in a major university in Canada over 10 years is\nanalysed. Modern machine learning algorithms can use large data sets to build\nuseful tools for the data provider, in this case, the university. In this\narticle, two classifiers are constructed using random forests. To begin, the\nfirst two semesters of courses completed by a student are used to predict if\nthey will obtain an undergraduate degree. Secondly, for the students that\ncompleted a program, their major is predicted using once again the first few\ncourses they have registered to. A classification tree is an intuitive and\npowerful classifier and building a random forest of trees improves this\nclassifier. Random forests also allow for reliable variable importance\nmeasurements. These measures explain what variables are useful to the\nclassifiers and can be used to better understand what is statistically related\nto the students' situation. The results are two accurate classifiers and a\nvariable importance analysis that provides useful information to university\nadministrations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 19:19:17 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 17:10:21 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 16:13:33 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Beaulac", "C\u00e9dric", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1802.03426", "submitter": "Leland McInnes", "authors": "Leland McInnes, John Healy and James Melville", "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension\n  Reduction", "comments": "Reference implementation available at http://github.com/lmcinnes/umap", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UMAP (Uniform Manifold Approximation and Projection) is a novel manifold\nlearning technique for dimension reduction. UMAP is constructed from a\ntheoretical framework based in Riemannian geometry and algebraic topology. The\nresult is a practical scalable algorithm that applies to real world data. The\nUMAP algorithm is competitive with t-SNE for visualization quality, and\narguably preserves more of the global structure with superior run time\nperformance. Furthermore, UMAP has no computational restrictions on embedding\ndimension, making it viable as a general purpose dimension reduction technique\nfor machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 19:39:33 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:54:07 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 01:56:41 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["McInnes", "Leland", ""], ["Healy", "John", ""], ["Melville", "James", ""]]}, {"id": "1802.03451", "submitter": "Ryan Adams", "authors": "Ryan P. Adams, Jeffrey Pennington, Matthew J. Johnson, Jamie Smith,\n  Yaniv Ovadia, Brian Patton, James Saunderson", "title": "Estimating the Spectral Density of Large Implicit Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems are characterized by the eigenvalues of a large\nmatrix. For example, the difficulty of many optimization problems, such as\nthose arising from the fitting of large models in statistics and machine\nlearning, can be investigated via the spectrum of the Hessian of the empirical\nloss function. Network data can be understood via the eigenstructure of a graph\nLaplacian matrix using spectral graph theory. Quantum simulations and other\nmany-body problems are often characterized via the eigenvalues of the solution\nspace, as are various dynamic systems. However, naive eigenvalue estimation is\ncomputationally expensive even when the matrix can be represented; in many of\nthese situations the matrix is so large as to only be available implicitly via\nproducts with vectors. Even worse, one may only have noisy estimates of such\nmatrix vector products. In this work, we combine several different techniques\nfor randomized estimation and show that it is possible to construct unbiased\nestimators to answer a broad class of questions about the spectra of such\nimplicit matrices, even in the presence of noise. We validate these methods on\nlarge-scale problems in which graph theory and random matrix theory provide\nground truth.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 21:26:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Adams", "Ryan P.", ""], ["Pennington", "Jeffrey", ""], ["Johnson", "Matthew J.", ""], ["Smith", "Jamie", ""], ["Ovadia", "Yaniv", ""], ["Patton", "Brian", ""], ["Saunderson", "James", ""]]}, {"id": "1802.03471", "submitter": "Mathias Lecuyer", "authors": "Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu,\n  Suman Jana", "title": "Certified Robustness to Adversarial Examples with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples that fool machine learning models, particularly deep\nneural networks, have been a topic of intense research interest, with attacks\nand defenses being developed in a tight back-and-forth. Most past defenses are\nbest effort and have been shown to be vulnerable to sophisticated attacks.\nRecently a set of certified defenses have been introduced, which provide\nguarantees of robustness to norm-bounded attacks, but they either do not scale\nto large datasets or are limited in the types of models they can support. This\npaper presents the first certified defense that both scales to large networks\nand datasets (such as Google's Inception network for ImageNet) and applies\nbroadly to arbitrary model types. Our defense, called PixelDP, is based on a\nnovel connection between robustness against adversarial examples and\ndifferential privacy, a cryptographically-inspired formalism, that provides a\nrigorous, generic, and flexible foundation for defense.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 22:24:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 15:54:44 GMT"}, {"version": "v3", "created": "Sun, 7 Oct 2018 18:37:06 GMT"}, {"version": "v4", "created": "Wed, 29 May 2019 15:17:39 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Lecuyer", "Mathias", ""], ["Atlidakis", "Vaggelis", ""], ["Geambasu", "Roxana", ""], ["Hsu", "Daniel", ""], ["Jana", "Suman", ""]]}, {"id": "1802.03475", "submitter": "Min Ye", "authors": "Min Ye and Emmanuel Abbe", "title": "Communication-Computation Efficient Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops coding techniques to reduce the running time of\ndistributed learning tasks. It characterizes the fundamental tradeoff to\ncompute gradients (and more generally vector summations) in terms of three\nparameters: computation load, straggler tolerance and communication cost. It\nfurther gives an explicit coding scheme that achieves the optimal tradeoff\nbased on recursive polynomial constructions, coding both across data subsets\nand vector components. As a result, the proposed scheme allows to minimize the\nrunning time for gradient computations. Implementations are made on Amazon EC2\nclusters using Python with mpi4py package. Results show that the proposed\nscheme maintains the same generalization error while reducing the running time\nby $32\\%$ compared to uncoded schemes and $23\\%$ compared to prior coded\nschemes focusing only on stragglers (Tandon et al., ICML 2017).\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:13:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ye", "Min", ""], ["Abbe", "Emmanuel", ""]]}, {"id": "1802.03487", "submitter": "Chulhee Yun", "authors": "Chulhee Yun, Suvrit Sra, Ali Jadbabaie", "title": "Small nonlinearities in activation functions create bad local minima in\n  neural networks", "comments": "33 pages, appeared at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the loss surface of neural networks. We prove that even for\none-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks\nhave spurious local minima in most cases. Our results thus indicate that in\ngeneral \"no spurious local minima\" is a property limited to deep linear\nnetworks, and insights obtained from linear networks may not be robust.\nSpecifically, for ReLU(-like) networks we constructively prove that for almost\nall practical datasets there exist infinitely many local minima. We also\npresent a counterexample for more general activations (sigmoid, tanh, arctan,\nReLU, etc.), for which there exists a bad local minimum. Our results make the\nleast restrictive assumptions relative to existing results on spurious local\noptima in neural networks. We complete our discussion by presenting a\ncomprehensive characterization of global optimality for deep linear networks,\nwhich unifies other results on this topic.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 00:49:17 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 20:58:56 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2018 04:27:13 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 15:25:47 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Yun", "Chulhee", ""], ["Sra", "Suvrit", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1802.03488", "submitter": "Katja Seeliger", "authors": "Marjolein Troost, Katja Seeliger and Marcel van Gerven", "title": "Generalization of an Upper Bound on the Number of Nodes Needed to\n  Achieve Linear Separability", "comments": "Presented at the 29th Benelux Conference on Artificial Intelligence\n  (BNAIC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important issue in neural network research is how to choose the number of\nnodes and layers such as to solve a classification problem. We provide new\nintuitions based on earlier results by An et al. (2015) by deriving an upper\nbound on the number of nodes in networks with two hidden layers such that\nlinear separability can be achieved. Concretely, we show that if the data can\nbe described in terms of N finite sets and the used activation function f is\nnon-constant, increasing and has a left asymptote, we can derive how many nodes\nare needed to linearly separate these sets. This will be an upper bound that\ndepends on the structure of the data. This structure can be analyzed using an\nalgorithm. For the leaky rectified linear activation function, we prove\nseparately that under some conditions on the slope, the same number of layers\nand nodes as for the aforementioned activation functions is sufficient. We\nempirically validate our claims.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 00:49:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Troost", "Marjolein", ""], ["Seeliger", "Katja", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1802.03497", "submitter": "Scott Gigante", "authors": "Scott Gigante, David van Dijk, Kevin Moon, Alexander Strzalkowski, Guy\n  Wolf, Smita Krishnaswamy", "title": "Modeling Global Dynamics from Local Snapshots with Deep Generative\n  Neural Networks", "comments": "Published in SampTA 2019", "journal-ref": "Sampling Theory & Applications (2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Complex high dimensional stochastic dynamic systems arise in many\napplications in the natural sciences and especially biology. However, while\nthese systems are difficult to describe analytically, \"snapshot\" measurements\nthat sample the output of the system are often available. In order to model the\ndynamics of such systems given snapshot data, or local transitions, we present\na deep neural network framework we call Dynamics Modeling Network or DyMoN.\nDyMoN is a neural network framework trained as a deep generative Markov model\nwhose next state is a probability distribution based on the current state.\nDyMoN is trained using samples of current and next-state pairs, and thus does\nnot require longitudinal measurements. We show the advantage of DyMoN over\nshallow models such as Kalman filters and hidden Markov models, and other deep\nmodels such as recurrent neural networks in its ability to embody the dynamics\n(which can be studied via perturbation of the neural network) and generate\nlongitudinal hypothetical trajectories. We perform three case studies in which\nwe apply DyMoN to different types of biological systems and extract features of\nthe dynamics in each case by examining the learned model.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:51:33 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 01:33:56 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 21:10:53 GMT"}, {"version": "v4", "created": "Fri, 28 Sep 2018 23:32:56 GMT"}, {"version": "v5", "created": "Mon, 10 Jun 2019 23:50:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gigante", "Scott", ""], ["van Dijk", "David", ""], ["Moon", "Kevin", ""], ["Strzalkowski", "Alexander", ""], ["Wolf", "Guy", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1802.03501", "submitter": "Yinlam Chow", "authors": "Ofir Nachum, Yinlam Chow, and Mohammad Ghavamzadeh", "title": "Path Consistency Learning in Tsallis Entropy Regularized MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sparse entropy-regularized reinforcement learning (ERL) problem\nin which the entropy term is a special form of the Tsallis entropy. The optimal\npolicy of this formulation is sparse, i.e.,~at each state, it has non-zero\nprobability for only a small number of actions. This addresses the main\ndrawback of the standard Shannon entropy-regularized RL (soft ERL) formulation,\nin which the optimal policy is softmax, and thus, may assign a non-negligible\nprobability mass to non-optimal actions. This problem is aggravated as the\nnumber of actions is increased. In this paper, we follow the work of Nachum et\nal. (2017) in the soft ERL setting, and propose a class of novel path\nconsistency learning (PCL) algorithms, called {\\em sparse PCL}, for the sparse\nERL problem that can work with both on-policy and off-policy data. We first\nderive a {\\em sparse consistency} equation that specifies a relationship\nbetween the optimal value function and policy of the sparse ERL along any\nsystem trajectory. Crucially, a weak form of the converse is also true, and we\nquantify the sub-optimality of a policy which satisfies sparse consistency, and\nshow that as we increase the number of actions, this sub-optimality is better\nthan that of the soft ERL optimal policy. We then use this result to derive the\nsparse PCL algorithms. We empirically compare sparse PCL with its soft\ncounterpart, and show its advantage, especially in problems with a large number\nof actions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:57:49 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Nachum", "Ofir", ""], ["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1802.03522", "submitter": "Kyongche Kang", "authors": "Kyongche Kang and Jack Michalak", "title": "Enhanced version of AdaBoostM1 with J48 Tree learning method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning focuses on the construction and study of systems that can\nlearn from data. This is connected with the classification problem, which\nusually is what Machine Learning algorithms are designed to solve. When a\nmachine learning method is used by people with no special expertise in machine\nlearning, it is important that the method be robust in classification, in the\nsense that reasonable performance is obtained with minimal tuning of the\nproblem at hand. Algorithms are evaluated based on how robust they can classify\nthe given data. In this paper, we propose a quantifiable measure of robustness,\nand describe a particular learning method that is robust according to this\nmeasure in the context of classification problem. We proposed Adaptive Boosting\n(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold\n(P) and number of iterations (I) for boosting algorithm. To benchmark the\nperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump as\nbase learner without tuning parameters. By tuning parameters and using J48 as\nbase learner, we are able to reduce the overall average error rate ratio\n(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2\nfor evaluation sets of data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 05:35:16 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kang", "Kyongche", ""], ["Michalak", "Jack", ""]]}, {"id": "1802.03532", "submitter": "Wenyi Wang Mr.", "authors": "Wenyi Wang and William J. Welch", "title": "Bayesian Optimization Using Monotonicity Information and Its Application\n  in Machine Learning Hyperparameter", "comments": "Citation style errors fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for a family of optimization problems where the\nobjective can be decomposed as a sum of functions with monotonicity properties.\nThe motivating problem is optimization of hyperparameters of machine learning\nalgorithms, where we argue that the objective, validation error, can be\ndecomposed as monotonic functions of the hyperparameters. Our proposed\nalgorithm adapts Bayesian optimization methods to incorporate the monotonicity\nconstraints. We illustrate the advantages of exploiting monotonicity using\nillustrative examples and demonstrate the improvements in optimization\nefficiency for some machine learning hyperparameter tuning applications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 07:14:53 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 22:38:34 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Wang", "Wenyi", ""], ["Welch", "William J.", ""]]}, {"id": "1802.03567", "submitter": "Gilles Ducharme", "authors": "Gilles R. Ducharme", "title": "Crit\\`eres de qualit\\'e d'un classifieur g\\'en\\'eraliste", "comments": "24 pages, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of choosing a good classifier. For each\nproblem there exist an optimal classifier, but none are optimal, regarding the\nerror rate, in all cases. Because there exists a large number of classifiers, a\nuser would rather prefer an all-purpose classifier that is easy to adjust, in\nthe hope that it will do almost as good as the optimal. In this paper we\nestablish a list of criteria that a good generalist classifier should satisfy .\nWe first discuss data analytic, these criteria are presented. Six among the\nmost popular classifiers are selected and scored according to these criteria.\nTables allow to easily appreciate the relative values of each. In the end,\nrandom forests turn out to be the best classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 11:25:27 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 09:30:05 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ducharme", "Gilles R.", ""]]}, {"id": "1802.03569", "submitter": "Tam Le", "authors": "Tam Le, Makoto Yamada", "title": "Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence\n  Diagrams", "comments": "to appear at the 32nd Conference on Neural Information Processing\n  Systems (NIPS), Canada, 2018. (Camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic topology methods have recently played an important role for\nstatistical analysis with complicated geometric structured data such as shapes,\nlinked twist maps, and material data. Among them, \\textit{persistent homology}\nis a well-known tool to extract robust topological features, and outputs as\n\\textit{persistence diagrams} (PDs). However, PDs are point multi-sets which\ncan not be used in machine learning algorithms for vector data. To deal with\nit, an emerged approach is to use kernel methods, and an appropriate geometry\nfor PDs is an important factor to measure the similarity of PDs. A popular\ngeometry for PDs is the \\textit{Wasserstein metric}. However, Wasserstein\ndistance is not \\textit{negative definite}. Thus, it is limited to build\npositive definite kernels upon the Wasserstein distance \\textit{without\napproximation}. In this work, we rely upon the alternative \\textit{Fisher\ninformation geometry} to propose a positive definite kernel for PDs\n\\textit{without approximation}, namely the Persistence Fisher (PF) kernel.\nThen, we analyze eigensystem of the integral operator induced by the proposed\nkernel for kernel machines. Based on that, we derive generalization error\nbounds via covering numbers and Rademacher averages for kernel machines with\nthe PF kernel. Additionally, we show some nice properties such as stability and\ninfinite divisibility for the proposed kernel. Furthermore, we also propose a\nlinear time complexity over the number of points in PDs for an approximation of\nour proposed kernel with a bounded error. Throughout experiments with many\ndifferent tasks on various benchmark datasets, we illustrate that the PF kernel\ncompares favorably with other baseline kernels for PDs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 12:02:24 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 23:18:08 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 06:18:03 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 08:14:05 GMT"}, {"version": "v5", "created": "Sat, 27 Oct 2018 01:41:04 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Le", "Tam", ""], ["Yamada", "Makoto", ""]]}, {"id": "1802.03583", "submitter": "Ali Shahin Shamsabadi", "authors": "Ali Shahin Shamsabadi, Hamed Haddadi and Andrea Cavallaro", "title": "Distributed One-class Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cloud-based filter trained to block third parties from uploading\nprivacy-sensitive images of others to online social media. The proposed filter\nuses Distributed One-Class Learning, which decomposes the cloud-based filter\ninto multiple one-class classifiers. Each one-class classifier captures the\nproperties of a class of privacy-sensitive images with an autoencoder. The\nmulti-class filter is then reconstructed by combining the parameters of the\none-class autoencoders. The training takes place on edge devices (e.g.\nsmartphones) and therefore users do not need to upload their private and/or\nsensitive images to the cloud. A major advantage of the proposed filter over\nexisting distributed learning approaches is that users cannot access, even\nindirectly, the parameters of other users. Moreover, the filter can cope with\nthe imbalanced and complex distribution of the image content and the\nindependent probability of addition of new users. We evaluate the performance\nof the proposed distributed filter using the exemplar task of blocking a user\nfrom sharing privacy-sensitive images of other users. In particular, we\nvalidate the behavior of the proposed multi-class filter with\nnon-privacy-sensitive images, the accuracy when the number of classes\nincreases, and the robustness to attacks when an adversary user has access to\nprivacy-sensitive images of other users.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 13:09:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shamsabadi", "Ali Shahin", ""], ["Haddadi", "Hamed", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1802.03604", "submitter": "Gong-Duo Zhang", "authors": "Gong-Duo Zhang, Shen-Yi Zhao, Hao Gao, Wu-Jun Li", "title": "Feature-Distributed SVRG for High-Dimensional Linear Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear classification has been widely used in many high-dimensional\napplications like text classification. To perform linear classification for\nlarge-scale tasks, we often need to design distributed learning methods on a\ncluster of multiple machines. In this paper, we propose a new distributed\nlearning method, called feature-distributed stochastic variance reduced\ngradient (FD-SVRG) for high-dimensional linear classification. Unlike most\nexisting distributed learning methods which are instance-distributed, FD-SVRG\nis feature-distributed. FD-SVRG has lower communication cost than other\ninstance-distributed methods when the data dimensionality is larger than the\nnumber of data instances. Experimental results on real data demonstrate that\nFD-SVRG can outperform other state-of-the-art distributed methods for\nhigh-dimensional linear classification in terms of both communication cost and\nwall-clock time, when the dimensionality is larger than the number of instances\nin training data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:53:57 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Zhang", "Gong-Duo", ""], ["Zhao", "Shen-Yi", ""], ["Gao", "Hao", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1802.03605", "submitter": "Matthew Guzdial", "authors": "Matthew Guzdial and Mark O. Riedl", "title": "Combinets: Creativity via Recombination of Neural Networks", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the defining characteristics of human creativity is the ability to\nmake conceptual leaps, creating something surprising from typical knowledge. In\ncomparison, deep neural networks often struggle to handle cases outside of\ntheir training data, which is especially problematic for problems with limited\ntraining data. Approaches exist to transfer knowledge from problems with\nsufficient data to those with insufficient data, but they tend to require\nadditional training or a domain-specific method of transfer. We present a new\napproach, conceptual expansion, that serves as a general representation for\nreusing existing trained models to derive new models without backpropagation.\nWe evaluate our approach on few-shot variations of two tasks: image\nclassification and image generation, and outperform standard transfer learning\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:54:10 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 03:38:08 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:16:21 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 21:44:51 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Guzdial", "Matthew", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1802.03628", "submitter": "Thanh Lam Hoang", "authors": "Han Qiu, Hoang Thanh Lam, Francesco Fusco and Mathieu Sinn", "title": "Learning Correlation Space for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approximation algorithm for efficient correlation search in\ntime series data. In our method, we use Fourier transform and neural network to\nembed time series into a low-dimensional Euclidean space. The given space is\nlearned such that time series correlation can be effectively approximated from\nEuclidean distance between corresponding embedded vectors. Therefore, search\nfor correlated time series can be done using an index in the embedding space\nfor efficient nearest neighbor search. Our theoretical analysis illustrates\nthat our method's accuracy can be guaranteed under certain regularity\nconditions. We further conduct experiments on real-world datasets and the\nresults show that our method indeed outperforms the baseline solution. In\nparticular, for approximation of correlation, our method reduces the\napproximation loss by a half in most test cases compared to the baseline\nsolution. For top-$k$ highest correlation search, our method improves the\nprecision from 5\\% to 20\\% while the query time is similar to the baseline\napproach query time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 17:59:25 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 07:34:06 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 12:15:58 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Qiu", "Han", ""], ["Lam", "Hoang Thanh", ""], ["Fusco", "Francesco", ""], ["Sinn", "Mathieu", ""]]}, {"id": "1802.03644", "submitter": "Ruilin Li", "authors": "Ruilin Li, Xiaojing Ye, Haomin Zhou, Hongyuan Zha", "title": "Learning to Match via Inverse Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified data-driven framework based on inverse optimal transport\nthat can learn adaptive, nonlinear interaction cost function from noisy and\nincomplete empirical matching matrix and predict new matching in various\nmatching contexts. We emphasize that the discrete optimal transport plays the\nrole of a variational principle which gives rise to an optimization-based\nframework for modeling the observed empirical matching data. Our formulation\nleads to a non-convex optimization problem which can be solved efficiently by\nan alternating optimization method. A key novel aspect of our formulation is\nthe incorporation of marginal relaxation via regularized Wasserstein distance,\nsignificantly improving the robustness of the method in the face of noisy or\nmissing empirical matching data. Our model falls into the category of\nprescriptive models, which not only predict potential future matching, but is\nalso able to explain what leads to empirical matching and quantifies the impact\nof changes in matching factors. The proposed approach has wide applicability\nincluding predicting matching in online dating, labor market, college\napplication and crowdsourcing. We back up our claims with numerical experiments\non both synthetic data and real world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 19:33:38 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 03:12:50 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 00:22:27 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Li", "Ruilin", ""], ["Ye", "Xiaojing", ""], ["Zhou", "Haomin", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1802.03651", "submitter": "Sotirios Chatzis", "authors": "Harris Partaourides, Sotirios Chatzis", "title": "Deep learning with t-exponential Bayesian kitchen sinks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning has been recently considered as an effective means of\naccounting for uncertainty in trained deep network parameters. This is of\ncrucial importance when dealing with small or sparse training datasets. On the\nother hand, shallow models that compute weighted sums of their inputs, after\npassing them through a bank of arbitrary randomized nonlinearities, have been\nrecently shown to enjoy good test error bounds that depend on the number of\nnonlinearities. Inspired from these advances, in this paper we examine novel\ndeep network architectures, where each layer comprises a bank of arbitrary\nnonlinearities, linearly combined using multiple alternative sets of weights.\nWe effect model training by means of approximate inference based on a\nt-divergence measure; this generalizes the Kullback-Leibler divergence in the\ncontext of the t-exponential family of distributions. We adopt the\nt-exponential family since it can more flexibly accommodate real-world data,\nthat entail outliers and distributions with fat tails, compared to conventional\nGaussian model assumptions. We extensively evaluate our approach using several\nchallenging benchmarks, and provide comparative results to related\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 21:27:02 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Partaourides", "Harris", ""], ["Chatzis", "Sotirios", ""]]}, {"id": "1802.03654", "submitter": "Jonathan Efroni", "authors": "Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor", "title": "Beyond the One Step Greedy Approach in Reinforcement Learning", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The famous Policy Iteration algorithm alternates between policy improvement\nand policy evaluation. Implementations of this algorithm with several variants\nof the latter evaluation stage, e.g, $n$-step and trace-based returns, have\nbeen analyzed in previous works. However, the case of multiple-step lookahead\npolicy improvement, despite the recent increase in empirical evidence of its\nstrength, has to our knowledge not been carefully analyzed yet. In this work,\nwe introduce the first such analysis. Namely, we formulate variants of\nmultiple-step policy improvement, derive new algorithms using these definitions\nand prove their convergence. Moreover, we show that recent prominent\nReinforcement Learning algorithms are, in fact, instances of our framework. We\nthus shed light on their empirical success and give a recipe for deriving new\nalgorithms for future study.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 22:22:03 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 17:19:20 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 18:02:11 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Efroni", "Yonathan", ""], ["Dalal", "Gal", ""], ["Scherrer", "Bruno", ""], ["Mannor", "Shie", ""]]}, {"id": "1802.03675", "submitter": "Sandeep Konam", "authors": "Sandeep Konam, Ian Quah, Stephanie Rosenthal, Manuela Veloso", "title": "Understanding Convolutional Networks with APPLE : Automatic Patch\n  Pattern Labeling for Explanation", "comments": "AAAI/ACM Conference on AI, Ethics, and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning, recent efforts have been focused on\nanalyzing how learned networks make their classifications. We are interested in\nanalyzing the network output based on the network structure and information\nflow through the network layers. We contribute an algorithm for 1) analyzing a\ndeep network to find neurons that are 'important' in terms of the network\nclassification outcome, and 2)automatically labeling the patches of the input\nimage that activate these important neurons. We propose several measures of\nimportance for neurons and demonstrate that our technique can be used to gain\ninsight into, and explain how a network decomposes an image to make its final\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 01:33:33 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Konam", "Sandeep", ""], ["Quah", "Ian", ""], ["Rosenthal", "Stephanie", ""], ["Veloso", "Manuela", ""]]}, {"id": "1802.03676", "submitter": "Mathieu Blondel", "authors": "Arthur Mensch, Mathieu Blondel", "title": "Differentiable Dynamic Programming for Structured Prediction and\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming (DP) solves a variety of structured combinatorial\nproblems by iteratively breaking them down into smaller subproblems. In spite\nof their versatility, DP algorithms are usually non-differentiable, which\nhampers their use as a layer in neural networks trained by backpropagation. To\naddress this issue, we propose to smooth the max operator in the dynamic\nprogramming recursion, using a strongly convex regularizer. This allows to\nrelax both the optimal value and solution of the original combinatorial\nproblem, and turns a broad class of DP algorithms into differentiable\noperators. Theoretically, we provide a new probabilistic perspective on\nbackpropagating through these DP operators, and relate them to inference in\ngraphical models. We derive two particular instantiations of our framework, a\nsmoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm\nfor time-series alignment. We showcase these instantiations on two structured\nprediction tasks and on structured and sparse attention for neural machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 01:36:06 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:01:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mensch", "Arthur", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1802.03688", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Tongliang Liu, Dacheng Tao", "title": "On the Rates of Convergence from Surrogate Risk Minimizers to the Bayes\n  Optimal Classifier", "comments": "Under Minor Revision in TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the rates of convergence from empirical surrogate risk minimizers to\nthe Bayes optimal classifier. Specifically, we introduce the notion of\n\\emph{consistency intensity} to characterize a surrogate loss function and\nexploit this notion to obtain the rate of convergence from an empirical\nsurrogate risk minimizer to the Bayes optimal classifier, enabling fair\ncomparisons of the excess risks of different surrogate risk minimizers. The\nmain result of the paper has practical implications including (1) showing that\nhinge loss is superior to logistic and exponential loss in the sense that its\nempirical minimizer converges faster to the Bayes optimal classifier and (2)\nguiding to modify surrogate loss functions to accelerate the convergence to the\nBayes optimal classifier.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 03:43:28 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 16:41:38 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhang", "Jingwei", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1802.03689", "submitter": "Thai Hung Le", "authors": "Hung Le, Truyen Tran and Svetha Venkatesh", "title": "Dual Control Memory Augmented Neural Networks for Treatment\n  Recommendations", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-assisted treatment recommendations hold a promise to reduce physician\ntime and decision errors. We formulate the task as a sequence-to-sequence\nprediction model that takes the entire time-ordered medical history as input,\nand predicts a sequence of future clinical procedures and medications. It is\nbuilt on the premise that an effective treatment plan may have long-term\ndependencies from previous medical history. We approach the problem by using a\nmemory-augmented neural network, in particular, by leveraging the recent\ndifferentiable neural computer that consists of a neural controller and an\nexternal memory module. But differing from the original model, we use dual\ncontrollers, one for encoding the history followed by another for decoding the\ntreatment sequences. In the encoding phase, the memory is updated as new input\nis read; at the end of this phase, the memory holds not only the medical\nhistory but also the information about the current illness. During the decoding\nphase, the memory is write-protected. The decoding controller generates a\ntreatment sequence, one treatment option at a time. The resulting dual\ncontroller write-protected memory-augmented neural network is demonstrated on\nthe MIMIC-III dataset on two tasks: procedure prediction and medication\nprescription. The results show improved performance over both traditional\nbag-of-words and sequence-to-sequence methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 03:50:41 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Le", "Hung", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1802.03690", "submitter": "Shubhendu Trivedi", "authors": "Risi Kondor, Shubhendu Trivedi", "title": "On the Generalization of Equivariance and Convolution in Neural Networks\n  to the Action of Compact Groups", "comments": "Final version that appeared in the proceedings of the 35th\n  International Conference on Machine Learning (ICML 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been extremely successful in the image\nrecognition domain because they ensure equivariance to translations. There have\nbeen many recent attempts to generalize this framework to other domains,\nincluding graphs and data lying on manifolds. In this paper we give a rigorous,\ntheoretical treatment of convolution and equivariance in neural networks with\nrespect to not just translations, but the action of any compact group. Our main\nresult is to prove that (given some natural constraints) convolutional\nstructure is not just a sufficient, but also a necessary condition for\nequivariance to the action of a compact group. Our exposition makes use of\nconcepts from representation theory and noncommutative harmonic analysis and\nderives new generalized convolution formulae.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 04:32:33 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 06:22:57 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 23:20:43 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kondor", "Risi", ""], ["Trivedi", "Shubhendu", ""]]}, {"id": "1802.03692", "submitter": "Yang Cao", "authors": "Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie", "title": "Nearly Optimal Adaptive Procedure with Change Detection for\n  Piecewise-Stationary Bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit (MAB) is a class of online learning problems where a\nlearning agent aims to maximize its expected cumulative reward while repeatedly\nselecting to pull arms with unknown reward distributions. We consider a\nscenario where the reward distributions may change in a piecewise-stationary\nfashion at unknown time steps. We show that by incorporating a simple\nchange-detection component with classic UCB algorithms to detect and adapt to\nchanges, our so-called M-UCB algorithm can achieve nearly optimal regret bound\non the order of $O(\\sqrt{MKT\\log T})$, where $T$ is the number of time steps,\n$K$ is the number of arms, and $M$ is the number of stationary segments.\nComparison with the best available lower bound shows that our M-UCB is nearly\noptimal in $T$ up to a logarithmic factor. We also compare M-UCB with the\nstate-of-the-art algorithms in numerical experiments using a public Yahoo!\ndataset to demonstrate its superior performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 04:57:28 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 18:29:03 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 20:00:00 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 06:13:01 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Cao", "Yang", ""], ["Wen", "Zheng", ""], ["Kveton", "Branislav", ""], ["Xie", "Yao", ""]]}, {"id": "1802.03699", "submitter": "Jintao Ke", "authors": "Jintao Ke, Shuaichao Zhang, Hai Yang, Xiqun Chen", "title": "PCA-Based Missing Information Imputation for Real-Time Crash Likelihood\n  Prediction Under Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time crash likelihood prediction has been an important research\ntopic. Various classifiers, such as support vector machine (SVM) and tree-based\nboosting algorithms, have been proposed in traffic safety studies. However, few\nresearch focuses on the missing data imputation in real-time crash likelihood\nprediction, although missing values are commonly observed due to breakdown of\nsensors or external interference. Besides, classifying imbalanced data is also\na difficult problem in real-time crash likelihood prediction, since it is hard\nto distinguish crash-prone cases from non-crash cases which compose the\nmajority of the observed samples. In this paper, principal component analysis\n(PCA) based approaches, including LS-PCA, PPCA, and VBPCA, are employed for\nimputing missing values, while two kinds of solutions are developed to solve\nthe problem in imbalanced data. The results show that PPCA and VBPCA not only\noutperform LS-PCA and other imputation methods (including mean imputation and\nk-means clustering imputation), in terms of the root mean square error (RMSE),\nbut also help the classifiers achieve better predictive performance. The two\nsolutions, i.e., cost-sensitive learning and synthetic minority oversampling\ntechnique (SMOTE), help improve the sensitivity by adjusting the classifiers to\npay more attention to the minority class.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 06:21:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ke", "Jintao", ""], ["Zhang", "Shuaichao", ""], ["Yang", "Hai", ""], ["Chen", "Xiqun", ""]]}, {"id": "1802.03707", "submitter": "Andreas Holzinger", "authors": "Bernd Malle, Nicola Giuliani, Peter Kieseberg, Andreas Holzinger", "title": "The Need for Speed of AI Applications: Performance Comparison of Native\n  vs. Browser-based Algorithm Implementations", "comments": "21 Pages, Technical Report of the Holzinger Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI applications pose increasing demands on performance, so it is not\nsurprising that the era of client-side distributed software is becoming\nimportant. On top of many AI applications already using mobile hardware, and\neven browsers for computationally demanding AI applications, we are already\nwitnessing the emergence of client-side (federated) machine learning\nalgorithms, driven by the interests of large corporations and startups alike.\nApart from mathematical and algorithmic concerns, this trend especially demands\nnew levels of computational efficiency from client environments. Consequently,\nthis paper deals with the question of state-of-the-art performance by\npresenting a comparison study between native code and different browser-based\nimplementations: JavaScript, ASM.js as well as WebAssembly on a representative\nmix of algorithms. Our results show that current efforts in runtime\noptimization push the boundaries well towards (and even beyond) native binary\nperformance. We analyze the results obtained and speculate on the reasons\nbehind some surprises, rounding the paper off by outlining future possibilities\nas well as some of our own research efforts.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 08:09:17 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Malle", "Bernd", ""], ["Giuliani", "Nicola", ""], ["Kieseberg", "Peter", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1802.03713", "submitter": "Qi Meng", "authors": "Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, Tie-Yan\n  Liu", "title": "$\\mathcal{G}$-SGD: Optimizing ReLU Neural Networks in its Positively\n  Scale-Invariant Space", "comments": null, "journal-ref": "ICLR2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that neural networks with rectified linear units (ReLU)\nactivation functions are positively scale-invariant. Conventional algorithms\nlike stochastic gradient descent optimize the neural networks in the vector\nspace of weights, which is, however, not positively scale-invariant. This\nmismatch may lead to problems during the optimization process. Then, a natural\nquestion is: \\emph{can we construct a new vector space that is positively\nscale-invariant and sufficient to represent ReLU neural networks so as to\nbetter facilitate the optimization process }? In this paper, we provide our\npositive answer to this question. First, we conduct a formal study on the\npositive scaling operators which forms a transformation group, denoted as\n$\\mathcal{G}$. We show that the value of a path (i.e. the product of the\nweights along the path) in the neural network is invariant to positive scaling\nand prove that the value vector of all the paths is sufficient to represent the\nneural networks under mild conditions. Second, we show that one can identify\nsome basis paths out of all the paths and prove that the linear span of their\nvalue vectors (denoted as $\\mathcal{G}$-space) is an invariant space with lower\ndimension under the positive scaling group. Finally, we design stochastic\ngradient descent algorithm in $\\mathcal{G}$-space (abbreviated as\n$\\mathcal{G}$-SGD) to optimize the value vector of the basis paths of neural\nnetworks with little extra cost by leveraging back-propagation. Our experiments\nshow that $\\mathcal{G}$-SGD significantly outperforms the conventional SGD\nalgorithm in optimizing ReLU networks on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 08:57:08 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 08:27:07 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 12:43:48 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 07:36:56 GMT"}, {"version": "v5", "created": "Mon, 25 Jun 2018 08:43:51 GMT"}, {"version": "v6", "created": "Tue, 9 Oct 2018 12:40:55 GMT"}, {"version": "v7", "created": "Wed, 7 Nov 2018 03:26:11 GMT"}, {"version": "v8", "created": "Tue, 23 Mar 2021 13:46:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Meng", "Qi", ""], ["Zheng", "Shuxin", ""], ["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1802.03725", "submitter": "Shubhm Gupta", "authors": "Shubham Gupta, Gaurav Sharma and Ambedkar Dukkipati", "title": "A Generative Model for Dynamic Networks with Applications", "comments": "Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks observed in real world like social networks, collaboration networks\netc., exhibit temporal dynamics, i.e. nodes and edges appear and/or disappear\nover time. In this paper, we propose a generative, latent space based,\nstatistical model for such networks (called dynamic networks). We consider the\ncase where the number of nodes is fixed, but the presence of edges can vary\nover time. Our model allows the number of communities in the network to be\ndifferent at different time steps. We use a neural network based methodology to\nperform approximate inference in the proposed model and its simplified version.\nExperiments done on synthetic and real world networks for the task of community\ndetection and link prediction demonstrate the utility and effectiveness of our\nmodel as compared to other similar existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 11:20:49 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 17:20:05 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gupta", "Shubham", ""], ["Sharma", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1802.03752", "submitter": "Sourav Mishra", "authors": "Sourav Mishra, Toshihiko Yamasaki, Hideaki Imaizumi", "title": "Supervised classification of Dermatological diseases by Deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a deep-learning based efficient classifier for common\ndermatological conditions, aimed at people without easy access to skin\nspecialists. We report approximately 80% accuracy, in a situation where primary\ncare doctors have attained 57% success rate, according to recent literature.\nThe rationale of its design is centered on deploying and updating it on\nhandheld devices in near future. Dermatological diseases are common in every\npopulation and have a wide spectrum in severity. With a shortage of\ndermatological expertise being observed in several countries, machine learning\nsolutions can augment medical services and advise regarding existence of common\ndiseases. The paper implements supervised classification of nine distinct\nconditions which have high occurrence in East Asian countries. Our current\nattempt establishes that deep learning based techniques are viable avenues for\npreliminary information to aid patients.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 15:34:20 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 16:17:12 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 17:23:02 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mishra", "Sourav", ""], ["Yamasaki", "Toshihiko", ""], ["Imaizumi", "Hideaki", ""]]}, {"id": "1802.03753", "submitter": "Pawe{\\l} Budzianowski", "authors": "Gell\\'ert Weisz, Pawe{\\l} Budzianowski, Pei-Hao Su, Milica Ga\\v{s}i\\'c", "title": "Sample Efficient Deep Reinforcement Learning for Dialogue Systems with\n  Large Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. A part of this effort\nis the policy optimisation task, which attempts to find a policy describing how\nto respond to humans, in the form of a function taking the current state of the\ndialogue and returning the response of the system. In this paper, we\ninvestigate deep reinforcement learning approaches to solve this problem.\nParticular attention is given to actor-critic methods, off-policy reinforcement\nlearning with experience replay, and various methods aimed at reducing the bias\nand variance of estimators. When combined, these methods result in the\npreviously proposed ACER algorithm that gave competitive results in gaming\nenvironments. These environments however are fully observable and have a\nrelatively small action set so in this paper we examine the application of ACER\nto dialogue policy optimisation. We show that this method beats the current\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\nnot only leads to a more sample efficient algorithm that can train faster, but\nalso allows us to apply the algorithm in more difficult environments than\nbefore. We thus experiment with learning in a very large action space, which\nhas two orders of magnitude more actions than previously considered. We find\nthat ACER trains significantly faster than the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 15:37:37 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Weisz", "Gell\u00e9rt", ""], ["Budzianowski", "Pawe\u0142", ""], ["Su", "Pei-Hao", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "1802.03759", "submitter": "Lucas Parra", "authors": "Lucas C Parra", "title": "Multi-set Canonical Correlation Analysis simply explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a multitude of methods to perform multi-set correlated component\nanalysis (MCCA), including some that require iterative solutions. The methods\ndiffer on the criterion they optimize and the constraints placed on the\nsolutions. This note focuses perhaps on the simplest version, which can be\nsolved in a single step as the eigenvectors of matrix ${\\bf D}^{-1} {\\bf R}$.\nHere ${\\bf R}$ is the covariance matrix of the concatenated data, and ${\\bf D}$\nis its block-diagonal. This note shows that this solution maximizes inter-set\ncorrelation (ISC) without further constraints. It also relates the solution to\na two step procedure, which first whitens each dataset using PCA, and then\nperforms an additional PCA on the concatenated and whitened data. Both these\nsolutions are known, although a clear derivation and simple implementation are\nhard to find. This short note aims to remedy this.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:05:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Parra", "Lucas C", ""]]}, {"id": "1802.03761", "submitter": "Paul Rubenstein", "authors": "Paul K. Rubenstein, Bernhard Schoelkopf, Ilya Tolstikhin", "title": "On the Latent Space of Wasserstein Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of latent space dimensionality in Wasserstein auto-encoders\n(WAEs). Through experimentation on synthetic and real datasets, we argue that\nrandom encoders should be preferred over deterministic encoders. We highlight\nthe potential of WAEs for representation learning with promising results on a\nbenchmark disentanglement task.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:10:41 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Schoelkopf", "Bernhard", ""], ["Tolstikhin", "Ilya", ""]]}, {"id": "1802.03765", "submitter": "Matt Olfat", "authors": "Matt Olfat, Anil Aswani", "title": "Convex Formulations for Fair Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though there is a growing body of literature on fairness for supervised\nlearning, the problem of incorporating fairness into unsupervised learning has\nbeen less well-studied. This paper studies fairness in the context of principal\ncomponent analysis (PCA). We first present a definition of fairness for\ndimensionality reduction, and our definition can be interpreted as saying that\na reduction is fair if information about a protected class (e.g., race or\ngender) cannot be inferred from the dimensionality-reduced data points. Next,\nwe develop convex optimization formulations that can improve the fairness (with\nrespect to our definition) of PCA and kernel PCA. These formulations are\nsemidefinite programs (SDP's), and we demonstrate the effectiveness of our\nformulations using several datasets. We conclude by showing how our approach\ncan be used to perform a fair (with respect to age) clustering of health data\nthat may be used to set health insurance rates.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:47:45 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 05:35:20 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 00:45:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Olfat", "Matt", ""], ["Aswani", "Anil", ""]]}, {"id": "1802.03774", "submitter": "Shiyu Duan", "authors": "Shiyu Duan, Shujian Yu, Yunmei Chen, Jose Principe", "title": "On Kernel Method-Based Connectionist Models and Supervised Deep Learning\n  Without Backpropagation", "comments": "8 pages main text, 16 pages of references and appendix, 2 figures", "journal-ref": "Neural Computation, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel family of connectionist models based on kernel machines\nand consider the problem of learning layer-by-layer a compositional hypothesis\nclass, i.e., a feedforward, multilayer architecture, in a supervised setting.\nIn terms of the models, we present a principled method to \"kernelize\" (partly\nor completely) any neural network (NN). With this method, we obtain a\ncounterpart of any given NN that is powered by kernel machines instead of\nneurons. In terms of learning, when learning a feedforward deep architecture in\na supervised setting, one needs to train all the components simultaneously\nusing backpropagation (BP) since there are no explicit targets for the hidden\nlayers (Rumelhart86). We consider without loss of generality the two-layer case\nand present a general framework that explicitly characterizes a target for the\nhidden layer that is optimal for minimizing the objective function of the\nnetwork. This characterization then makes possible a purely greedy training\nscheme that learns one layer at a time, starting from the input layer. We\nprovide realizations of the abstract framework under certain architectures and\nobjective functions. Based on these realizations, we present a layer-wise\ntraining algorithm for an l-layer feedforward network for classification, where\nl>=2 can be arbitrary. This algorithm can be given an intuitive geometric\ninterpretation that makes the learning dynamics transparent. Empirical results\nare provided to complement our theory. We show that the kernelized networks,\ntrained layer-wise, compare favorably with classical kernel machines as well as\nother connectionist models trained by BP. We also visualize the inner workings\nof the greedy kernelized models to validate our claim on the transparency of\nthe layer-wise algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 17:18:28 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 03:30:06 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 05:09:06 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 03:48:21 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Duan", "Shiyu", ""], ["Yu", "Shujian", ""], ["Chen", "Yunmei", ""], ["Principe", "Jose", ""]]}, {"id": "1802.03788", "submitter": "Klas Leino", "authors": "Klas Leino, Shayak Sen, Anupam Datta, Matt Fredrikson, Linyi Li", "title": "Influence-Directed Explanations for Deep Convolutional Networks", "comments": "To appear in International Test Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of explaining a rich class of behavioral properties of\ndeep neural networks. Distinctively, our influence-directed explanations\napproach this problem by peering inside the network to identify neurons with\nhigh influence on a quantity and distribution of interest, using an\naxiomatically-justified influence measure, and then providing an interpretation\nfor the concepts these neurons represent. We evaluate our approach by\ndemonstrating a number of its unique capabilities on convolutional neural\nnetworks trained on ImageNet. Our evaluation demonstrates that\ninfluence-directed explanations (1) identify influential concepts that\ngeneralize across instances, (2) can be used to extract the \"essence\" of what\nthe network learned about a class, and (3) isolate individual features the\nnetwork uses to make decisions and distinguish related classes.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 18:28:56 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 06:02:57 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Leino", "Klas", ""], ["Sen", "Shayak", ""], ["Datta", "Anupam", ""], ["Fredrikson", "Matt", ""], ["Li", "Linyi", ""]]}, {"id": "1802.03800", "submitter": "Mehmet Tan", "authors": "Mehmet Tan, Ozan F{\\i}rat \\\"Ozg\\\"ul, Batuhan Bardak, I\\c{s}{\\i}ksu\n  Ek\\c{s}io\\u{g}lu, Suna Sabuncuo\\u{g}lu", "title": "Drug response prediction by ensemble learning and drug-induced gene\n  expression signatures", "comments": "Will appear in Genomics Journal", "journal-ref": null, "doi": "10.1016/j.ygeno.2018.07.002", "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemotherapeutic response of cancer cells to a given compound is one of the\nmost fundamental information one requires to design anti-cancer drugs. Recent\nadvances in producing large drug screens against cancer cell lines provided an\nopportunity to apply machine learning methods for this purpose. In addition to\ncytotoxicity databases, considerable amount of drug-induced gene expression\ndata has also become publicly available. Following this, several methods that\nexploit omics data were proposed to predict drug activity on cancer cells.\nHowever, due to the complexity of cancer drug mechanisms, none of the existing\nmethods are perfect. One possible direction, therefore, is to combine the\nstrengths of both the methods and the databases for improved performance. We\ndemonstrate that integrating a large number of predictions by the proposed\nmethod improves the performance for this task. The predictors in the ensemble\ndiffer in several aspects such as the method itself, the number of tasks method\nconsiders (multi-task vs. single-task) and the subset of data considered\n(sub-sampling). We show that all these different aspects contribute to the\nsuccess of the final ensemble. In addition, we attempt to use the drug screen\ndata together with two novel signatures produced from the drug-induced gene\nexpression profiles of cancer cell lines. Finally, we evaluate the method\npredictions by in vitro experiments in addition to the tests on data sets.The\npredictions of the methods, the signatures and the software are available from\n\\url{http://mtan.etu.edu.tr/drug-response-prediction/}.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:34:10 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:25:44 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 08:36:58 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tan", "Mehmet", ""], ["\u00d6zg\u00fcl", "Ozan F\u0131rat", ""], ["Bardak", "Batuhan", ""], ["Ek\u015fio\u011flu", "I\u015f\u0131ksu", ""], ["Sabuncuo\u011flu", "Suna", ""]]}, {"id": "1802.03801", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richt\\'arik,\n  Katya Scheinberg, Martin Tak\\'a\\v{c}", "title": "SGD and Hogwild! Convergence Without the Bounded Gradients Assumption", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:3747-3755, 2018", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is the optimization algorithm of choice in\nmany machine learning applications such as regularized empirical risk\nminimization and training deep neural networks. The classical convergence\nanalysis of SGD is carried out under the assumption that the norm of the\nstochastic gradient is uniformly bounded. While this might hold for some loss\nfunctions, it is always violated for cases where the objective function is\nstrongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD\nis performed under the assumption that stochastic gradients are bounded with\nrespect to the true gradient norm. Here we show that for stochastic problems\narising in machine learning such bound always holds; and we also propose an\nalternative convergence analysis of SGD with diminishing learning rate regime,\nwhich results in more relaxed conditions than those in (Bottou et al.,2016). We\nthen move on the asynchronous parallel setting, and prove convergence of\nHogwild! algorithm in the same regime, obtaining the first convergence results\nfor this method in the case of diminished learning rate.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:35:46 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 04:23:40 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Nguyen", "Phuong Ha", ""], ["van Dijk", "Marten", ""], ["Richt\u00e1rik", "Peter", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1802.03824", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "L. Felsberger, P.S. Koutsourelakis", "title": "Physics-constrained, data-driven discovery of coarse-grained dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of high-dimensionality and disparity of time scales\nencountered in many problems in computational physics has motivated the\ndevelopment of coarse-grained (CG) models. In this paper, we advocate the\nparadigm of data-driven discovery for extract- ing governing equations by\nemploying fine-scale simulation data. In particular, we cast the\ncoarse-graining process under a probabilistic state-space model where the\ntransition law dic- tates the evolution of the CG state variables and the\nemission law the coarse-to-fine map. The directed probabilistic graphical model\nimplied, suggests that given values for the fine- grained (FG) variables,\nprobabilistic inference tools must be employed to identify the cor- responding\nvalues for the CG states and to that end, we employ Stochastic Variational In-\nference. We advocate a sparse Bayesian learning perspective which avoids\noverfitting and reveals the most salient features in the CG evolution law. The\nformulation adopted enables the quantification of a crucial, and often\nneglected, component in the CG process, i.e. the pre- dictive uncertainty due\nto information loss. Furthermore, it is capable of reconstructing the evolution\nof the full, fine-scale system. We demonstrate the efficacy of the proposed\nframe- work in high-dimensional systems of random walkers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 21:33:47 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Felsberger", "L.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1802.03830", "submitter": "Mladen Kolar", "authors": "Weiran Wang, Jialei Wang, Mladen Kolar, Nathan Srebro", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 22:23:34 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wang", "Weiran", ""], ["Wang", "Jialei", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1802.03832", "submitter": "Marina Munkhoeva", "authors": "Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets", "title": "Quadrature-based features for kernel approximation", "comments": "Accepted to NIPS 2018; 9 pages, 3 figures, Appendix: 4 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of improving kernel approximation via randomized\nfeature maps. These maps arise as Monte Carlo approximation to integral\nrepresentations of kernel functions and scale up kernel methods for larger\ndatasets. Based on an efficient numerical integration technique, we propose a\nunifying approach that reinterprets the previous random features methods and\nextends to better estimates of the kernel approximation. We derive the\nconvergence behaviour and conduct an extensive empirical study that supports\nour hypothesis.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 22:35:28 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 09:00:13 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 10:34:02 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 21:29:40 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Munkhoeva", "Marina", ""], ["Kapushev", "Yermek", ""], ["Burnaev", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1802.03839", "submitter": "Casey Kneale", "authors": "Casey Kneale, Steven D. Brown", "title": "Band Target Entropy Minimization and Target Partial Least Squares for\n  Spectral Recovery and Calibration", "comments": null, "journal-ref": null, "doi": "10.1016/j.aca.2018.07.054", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resolution and calibration of pure spectra of minority components in\nmeasurements of chemical mixtures without prior knowledge of the mixture is a\nchallenging problem. In this work, a combination of band target entropy\nminimization (BTEM) and target partial least squares (T-PLS) was used to obtain\nestimates for single pure component spectra and to calibrate those estimates in\na true, one-at-a-time fashion. This approach allows for minor components to be\ntargeted and their relative amounts estimated in the presence of other varying\ncomponents in spectral data. The use of T-PLS estimation is an improvement to\nthe BTEM method because it overcomes the need to identify all of the pure\ncomponents prior to estimation. Estimated amounts from this combination were\nfound to be similar to those obtained from a standard method, multivariate\ncurve resolution-alternating least squares (MCR-ALS), on a simple, three\ncomponent mixture dataset. Studies from two experimental datasets demonstrate\nwhere the combination of BTEM and T-PLS could model the pure component spectra\nand obtain concentration profiles of minor components but MCR-ALS could not.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 23:21:38 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 16:53:41 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kneale", "Casey", ""], ["Brown", "Steven D.", ""]]}, {"id": "1802.03840", "submitter": "Casey Kneale", "authors": "Casey Kneale, Steven D. Brown", "title": "Uncharted Forest a Technique for Exploratory Data Analysis", "comments": null, "journal-ref": "Talanta. 189, (2018), 71-78", "doi": "10.1016/j.talanta.2018.06.061", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory data analysis is crucial for developing and understanding\nclassification models from high-dimensional datasets. We explore the utility of\na new unsupervised tree ensemble called uncharted forest for visualizing class\nassociations, sample-sample associations, class heterogeneity, and\nuninformative classes for provenance studies. The uncharted forest algorithm\ncan be used to partition data using random selections of variables and metrics\nbased on statistical spread. After each tree is grown, a tally of the samples\nthat arrive at every terminal node is maintained. Those tallies are stored in\nsingle sample association matrix and a likelihood measure for each sample being\npartitioned with one another can be made. That matrix may be readily viewed as\na heat map, and the probabilities can be quantified via new metrics that\naccount for class or cluster membership. We display the advantages and\nlimitations of using this technique by applying it to two classification\ndatasets and three provenance study datasets. Two of the metrics presented in\nthis paper are also compared with widely used metrics from two algorithms that\nhave variance-based clustering mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 23:22:36 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 22:21:49 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 12:27:25 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kneale", "Casey", ""], ["Brown", "Steven D.", ""]]}, {"id": "1802.03848", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Vahid Tarokh", "title": "Region Detection in Markov Random Fields: Gaussian Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model selection in Gaussian Markov fields in the\nsample deficient scenario. The benchmark information-theoretic results in the\ncase of d-regular graphs require the number of samples to be at least\nproportional to the logarithm of the number of vertices to allow consistent\ngraph recovery. When the number of samples is less than this amount, reliable\ndetection of all edges is impossible. In many applications, it is more\nimportant to learn the distribution of the edge (coupling) parameters over the\nnetwork than the specific locations of the edges. Assuming that the entire\ngraph can be partitioned into a number of spatial regions with similar edge\nparameters and reasonably regular boundaries, we develop new\ninformation-theoretic sample complexity bounds and show that a bounded number\nof samples can be sufficient to consistently recover these regions. Finally, we\nintroduce and analyze an efficient region growing algorithm capable of\nrecovering the regions with high accuracy. We show that it is consistent and\ndemonstrate its performance benefits in synthetic simulations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 00:21:32 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 04:29:07 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 06:15:34 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 18:29:35 GMT"}, {"version": "v5", "created": "Thu, 8 Mar 2018 06:19:14 GMT"}, {"version": "v6", "created": "Thu, 22 Mar 2018 06:05:21 GMT"}, {"version": "v7", "created": "Wed, 28 Mar 2018 05:32:06 GMT"}, {"version": "v8", "created": "Thu, 29 Mar 2018 03:43:28 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1802.03866", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of minimizing sum-of-nonconvex functions (i.e., convex functions\nthat are average of non-convex ones) is becoming increasingly important in\nmachine learning, and is the core machinery for PCA, SVD, regularized Newton's\nmethod, accelerated non-convex optimization, and more.\n  We show how to provably obtain an accelerated stochastic algorithm for\nminimizing sum-of-nonconvex functions, by $\\textit{adding one additional line}$\nto the well-known SVRG method. This line corresponds to momentum, and shows how\nto directly apply momentum to the finite-sum stochastic minimization of\nsum-of-nonconvex functions. As a side result, our method enjoys linear parallel\nspeed-up using mini-batch.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 02:49:23 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1802.03875", "submitter": "Craig Atkinson", "authors": "Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins", "title": "Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, neural networks are not currently capable of learning tasks in a\nsequential fashion. When a novel, unrelated task is learnt by a neural network,\nit substantially forgets how to solve previously learnt tasks. One of the\noriginal solutions to this problem is pseudo-rehearsal, which involves learning\nthe new task while rehearsing generated items representative of the previous\ntask/s. This is very effective for simple tasks. However, pseudo-rehearsal has\nnot yet been successfully applied to very complex tasks because in these tasks\nit is difficult to generate representative items. We accomplish\npseudo-rehearsal by using a Generative Adversarial Network to generate items so\nthat our deep network can learn to sequentially classify the CIFAR-10, SVHN and\nMNIST datasets. After training on all tasks, our network loses only 1.67%\nabsolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our\nmodel's performance is a substantial improvement compared to the current state\nof the art solution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 03:51:41 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 01:42:04 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Atkinson", "Craig", ""], ["McCane", "Brendan", ""], ["Szymanski", "Lech", ""], ["Robins", "Anthony", ""]]}, {"id": "1802.03877", "submitter": "Ryosuke Kamesawa", "authors": "Ryosuke Kamesawa, Issei Sato, and Masashi Sugiyama", "title": "Gaussian Process Classification with Privileged Information by\n  Soft-to-Hard Labeling Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning using privileged information is an attractive problem setting that\nhelps many learning scenarios in the real world. A state-of-the-art method of\nGaussian process classification (GPC) with privileged information is GPC+,\nwhich incorporates privileged information into a noise term of the likelihood.\nA drawback of GPC+ is that it requires numerical quadrature to calculate the\nposterior distribution of the latent function, which is extremely\ntime-consuming. To overcome this limitation, we propose a novel classification\nmethod with privileged information based on Gaussian processes, called\n\"soft-label-transferred Gaussian process (SLT-GP).\" Our basic idea is that we\nconstruct another learning task of predicting soft labels (continuous values)\nobtained from privileged information and we perform transfer learning from this\ntask to the target task of predicting hard labels. We derive a PAC-Bayesian\nbound of our proposed method, which justifies optimizing hyperparameters by the\nempirical Bayes method. We also experimentally show the usefulness of our\nproposed method compared with GPC and GPC+.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 03:58:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kamesawa", "Ryosuke", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.03882", "submitter": "Nathan Lay", "authors": "Nathan Lay, Adam P. Harrison, Sharon Schreiber, Gitesh Dawer, Adrian\n  Barbu", "title": "Random Hinge Forest for Differentiable Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose random hinge forests, a simple, efficient, and novel variant of\ndecision forests. Importantly, random hinge forests can be readily incorporated\nas a general component within arbitrary computation graphs that are optimized\nend-to-end with stochastic gradient descent or variants thereof. We derive\nrandom hinge forest and ferns, focusing on their sparse and efficient nature,\ntheir min-max margin property, strategies to initialize them for arbitrary\nnetwork architectures, and the class of optimizers most suitable for optimizing\nrandom hinge forest. The performance and versatility of random hinge forests\nare demonstrated by experiments incorporating a variety of of small and large\nUCI machine learning data sets and also ones involving the MNIST, Letter, and\nUSPS image datasets. We compare random hinge forests with random forests and\nthe more recent backpropagating deep neural decision forests.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:08:53 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 06:25:27 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Lay", "Nathan", ""], ["Harrison", "Adam P.", ""], ["Schreiber", "Sharon", ""], ["Dawer", "Gitesh", ""], ["Barbu", "Adrian", ""]]}, {"id": "1802.03888", "submitter": "Scott Lundberg", "authors": "Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee", "title": "Consistent Individualized Feature Attribution for Tree Ensembles", "comments": "Follow-up to 2017 ICML Workshop arXiv:1706.06060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting predictions from tree ensemble methods such as gradient boosting\nmachines and random forests is important, yet feature attribution for trees is\noften heuristic and not individualized for each prediction. Here we show that\npopular feature attribution methods are inconsistent, meaning they can lower a\nfeature's assigned importance when the true impact of that feature actually\nincreases. This is a fundamental problem that casts doubt on any comparison\nbetween features. To address it we turn to recent applications of game theory\nand develop fast exact tree solutions for SHAP (SHapley Additive exPlanation)\nvalues, which are the unique consistent and locally accurate attribution\nvalues. We then extend SHAP values to interaction effects and define SHAP\ninteraction values. We propose a rich visualization of individualized feature\nattributions that improves over classic attribution summaries and partial\ndependence plots, and a unique \"supervised\" clustering (clustering based on\nfeature attributions). We demonstrate better agreement with human intuition\nthrough a user study, exponential improvements in run time, improved clustering\nperformance, and better identification of influential features. An\nimplementation of our algorithm has also been merged into XGBoost and LightGBM,\nsee http://github.com/slundberg/shap for details.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:23:03 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 15:40:29 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 00:06:09 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Lundberg", "Scott M.", ""], ["Erion", "Gabriel G.", ""], ["Lee", "Su-In", ""]]}, {"id": "1802.03900", "submitter": "Qiaomin Xie", "authors": "Devavrat Shah and Qiaomin Xie", "title": "Q-learning with Nearest Neighbors", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model-free reinforcement learning for infinite-horizon discounted\nMarkov Decision Processes (MDPs) with a continuous state space and unknown\ntransition kernel, when only a single sample path under an arbitrary policy of\nthe system is available. We consider the Nearest Neighbor Q-Learning (NNQL)\nalgorithm to learn the optimal Q function using nearest neighbor regression\nmethod. As the main contribution, we provide tight finite sample analysis of\nthe convergence rate. In particular, for MDPs with a $d$-dimensional state\nspace and the discounted factor $\\gamma \\in (0,1)$, given an arbitrary sample\npath with \"covering time\" $ L $, we establish that the algorithm is guaranteed\nto output an $\\varepsilon$-accurate estimate of the optimal Q-function using\n$\\tilde{O}\\big(L/(\\varepsilon^3(1-\\gamma)^7)\\big)$ samples. For instance, for a\nwell-behaved MDP, the covering time of the sample path under the purely random\npolicy scales as $ \\tilde{O}\\big(1/\\varepsilon^d\\big),$ so the sample\ncomplexity scales as $\\tilde{O}\\big(1/\\varepsilon^{d+3}\\big).$ Indeed, we\nestablish a lower bound that argues that the dependence of $\n\\tilde{\\Omega}\\big(1/\\varepsilon^{d+2}\\big)$ is necessary.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 05:54:43 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 02:38:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Shah", "Devavrat", ""], ["Xie", "Qiaomin", ""]]}, {"id": "1802.03903", "submitter": "Haowen Xu", "authors": "Haowen Xu (1), Wenxiao Chen (1), Nengwen Zhao (1), Zeyan Li (1),\n  Jiahao Bu (1), Zhihan Li (1), Ying Liu (1), Youjian Zhao (1), Dan Pei (1),\n  Yang Feng (2), Jie Chen (2), Zhaogang Wang (2), Honglin Qiao (2) ((1)\n  Tsinghua University, (2) Alibaba Group)", "title": "Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal\n  KPIs in Web Applications", "comments": "12 pages (including references), 17 figures, submitted to WWW 2018:\n  The 2018 Web Conference, April 23--27, 2018, Lyon, France. The contents\n  discarded from the conference version due to the 9-page limitation are also\n  included in this version", "journal-ref": null, "doi": "10.1145/3178876.3185996", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure undisrupted business, large Internet companies need to closely\nmonitor various KPIs (e.g., Page Views, number of online users, and number of\norders) of its Web applications, to accurately detect anomalies and trigger\ntimely troubleshooting/mitigation. However, anomaly detection for these\nseasonal KPIs with various patterns and data quality has been a great\nchallenge, especially without labels. In this paper, we proposed Donut, an\nunsupervised anomaly detection algorithm based on VAE. Thanks to a few of our\nkey techniques, Donut greatly outperforms a state-of-arts supervised ensemble\napproach and a baseline VAE approach, and its best F-scores range from 0.75 to\n0.9 for the studied KPIs from a top global Internet company. We come up with a\nnovel KDE interpretation of reconstruction for Donut, making it the first\nVAE-based anomaly detection algorithm with solid theoretical explanation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 06:23:04 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Xu", "Haowen", ""], ["Chen", "Wenxiao", ""], ["Zhao", "Nengwen", ""], ["Li", "Zeyan", ""], ["Bu", "Jiahao", ""], ["Li", "Zhihan", ""], ["Liu", "Ying", ""], ["Zhao", "Youjian", ""], ["Pei", "Dan", ""], ["Feng", "Yang", ""], ["Chen", "Jie", ""], ["Wang", "Zhaogang", ""], ["Qiao", "Honglin", ""]]}, {"id": "1802.03913", "submitter": "Reza Zafarani", "authors": "Reza Zafarani, Sara Eftekharnejad, Urvi Patel", "title": "Assessing the Utility of Weather Data for Photovoltaic Power Prediction", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photovoltaic systems have been widely deployed in recent times to meet the\nincreased electricity demand as an environmental-friendly energy source. The\nmajor challenge for integrating photovoltaic systems in power systems is the\nunpredictability of the solar power generated. In this paper, we analyze the\nimpact of having access to weather information for solar power generation\nprediction and find weather information that can help best predict photovoltaic\npower.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 06:59:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Zafarani", "Reza", ""], ["Eftekharnejad", "Sara", ""], ["Patel", "Urvi", ""]]}, {"id": "1802.03916", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Yu-Xiang Wang, Alex Smola", "title": "Detecting and Correcting for Label Shift with Black Box Predictors", "comments": "Published at the International Conference on Machine Learning (ICML)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with distribution shift between training and test set, we wish to\ndetect and quantify the shift, and to correct our classifiers without test set\nlabels. Motivated by medical diagnosis, where diseases (targets) cause symptoms\n(observations), we focus on label shift, where the label marginal $p(y)$\nchanges but the conditional $p(x| y)$ does not. We propose Black Box Shift\nEstimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits\narbitrary black box predictors to reduce dimensionality prior to shift\ncorrection. While better predictors give tighter estimates, BBSE works even\nwhen predictors are biased, inaccurate, or uncalibrated, so long as their\nconfusion matrices are invertible. We prove BBSE's consistency, bound its\nerror, and introduce a statistical test that uses BBSE to detect shift. We also\nleverage BBSE to correct classifiers. Experiments demonstrate accurate\nestimates and improved prediction, even on high-dimensional datasets of natural\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 07:16:03 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 17:23:51 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 12:51:37 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Wang", "Yu-Xiang", ""], ["Smola", "Alex", ""]]}, {"id": "1802.03923", "submitter": "Tomoki Yoshida", "authors": "Tomoki Yoshida, Ichiro Takeuchi, Masayuki Karasuyama", "title": "Safe Triplet Screening for Distance Metric Learning", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study safe screening for metric learning. Distance metric learning can\noptimize a metric over a set of triplets, each one of which is defined by a\npair of same class instances and an instance in a different class. However, the\nnumber of possible triplets is quite huge even for a small dataset. Our safe\ntriplet screening identifies triplets which can be safely removed from the\noptimization problem without losing the optimality. Compared with existing safe\nscreening studies, triplet screening is particularly significant because of (1)\nthe huge number of possible triplets, and (2) the semi-definite constraint in\nthe optimization. We derive several variants of screening rules, and analyze\ntheir relationships. Numerical experiments on benchmark datasets demonstrate\nthe effectiveness of safe triplet screening.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:04:35 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 06:32:38 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Yoshida", "Tomoki", ""], ["Takeuchi", "Ichiro", ""], ["Karasuyama", "Masayuki", ""]]}, {"id": "1802.03938", "submitter": "Tatsuhiro Aoshima", "authors": "Tatsuhiro Aoshima, Kei Kobayashi, Mihoko Minami", "title": "Revisiting the Vector Space Model: Sparse Weighted Nearest-Neighbor\n  Method for Extreme Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has played an important role in information retrieval (IR)\nin recent times. In search engines, for example, query keywords are accepted\nand documents are returned in order of relevance to the given query; this can\nbe cast as a multi-label ranking problem in machine learning. Generally, the\nnumber of candidate documents is extremely large (from several thousand to\nseveral million); thus, the classifier must handle many labels. This problem is\nreferred to as extreme multi-label classification (XMLC). In this paper, we\npropose a novel approach to XMLC termed the Sparse Weighted Nearest-Neighbor\nMethod. This technique can be derived as a fast implementation of\nstate-of-the-art (SOTA) one-versus-rest linear classifiers for very sparse\ndatasets. In addition, we show that the classifier can be written as a sparse\ngeneralization of a representer theorem with a linear kernel. Furthermore, our\nmethod can be viewed as the vector space model used in IR. Finally, we show\nthat the Sparse Weighted Nearest-Neighbor Method can process data points in\nreal time on XMLC datasets with equivalent performance to SOTA models, with a\nsingle thread and smaller storage footprint. In particular, our method exhibits\nsuperior performance to the SOTA models on a dataset with 3 million labels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:51:57 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Aoshima", "Tatsuhiro", ""], ["Kobayashi", "Kei", ""], ["Minami", "Mihoko", ""]]}, {"id": "1802.03981", "submitter": "Karan Singh", "authors": "Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang", "title": "Spectral Filtering for General Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial-time algorithm for learning latent-state linear\ndynamical systems without system identification, and without assumptions on the\nspectral radius of the system's transition matrix. The algorithm extends the\nrecently introduced technique of spectral filtering, previously applied only to\nsystems with a symmetric transition matrix, using a novel convex relaxation to\nallow for the efficient identification of phases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:20:30 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Hazan", "Elad", ""], ["Lee", "Holden", ""], ["Singh", "Karan", ""], ["Zhang", "Cyril", ""], ["Zhang", "Yi", ""]]}, {"id": "1802.03987", "submitter": "Federico Tomasi", "authors": "Federico Tomasi, Veronica Tozzo, Saverio Salzo, Alessandro Verri", "title": "Latent Variable Time-varying Network Inference", "comments": "9 pages, 5 figures, 1 table", "journal-ref": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD 2018). ACM, New York, NY, USA,\n  2338-2346", "doi": "10.1145/3219819.3220121", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of finance, biology and sociology, complex systems\ninvolve entities interacting with each other. These processes have the\npeculiarity of evolving over time and of comprising latent factors, which\ninfluence the system without being explicitly measured. In this work we present\nlatent variable time-varying graphical lasso (LTGL), a method for multivariate\ntime-series graphical modelling that considers the influence of hidden or\nunmeasurable factors. The estimation of the contribution of the latent factors\nis embedded in the model which produces both sparse and low-rank components for\neach time point. In particular, the first component represents the connectivity\nstructure of observable variables of the system, while the second represents\nthe influence of hidden factors, assumed to be few with respect to the observed\nvariables. Our model includes temporal consistency on both components,\nproviding an accurate evolutionary pattern of the system. We derive a tractable\noptimisation algorithm based on alternating direction method of multipliers,\nand develop a scalable and efficient implementation which exploits proximity\noperators in closed form. LTGL is extensively validated on synthetic data,\nachieving optimal performance in terms of accuracy, structure learning and\nscalability with respect to ground truth and state-of-the-art methods for\ngraphical inference. We conclude with the application of LTGL to real case\nstudies, from biology and finance, to illustrate how our method can be\nsuccessfully employed to gain insights on multivariate time-series data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:39:14 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 09:48:48 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Tomasi", "Federico", ""], ["Tozzo", "Veronica", ""], ["Salzo", "Saverio", ""], ["Verri", "Alessandro", ""]]}, {"id": "1802.04020", "submitter": "Ronan Fruit", "authors": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Ronald Ortner", "title": "Efficient Bias-Span-Constrained Exploration-Exploitation in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SCAL, an algorithm designed to perform efficient\nexploration-exploitation in any unknown weakly-communicating Markov decision\nprocess (MDP) for which an upper bound $c$ on the span of the optimal bias\nfunction is known. For an MDP with $S$ states, $A$ actions and $\\Gamma \\leq S$\npossible next states, we prove a regret bound of $\\widetilde{O}(c\\sqrt{\\Gamma\nSAT})$, which significantly improves over existing algorithms (e.g., UCRL and\nPSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the\noptimal bias span is finite and often much smaller than $D$ (e.g., $D=\\infty$\nin non-communicating MDPs). A similar result was originally derived by Bartlett\nand Tewari (2009) for REGAL.C, for which no tractable algorithm is available.\nIn this paper, we relax the optimization problem at the core of REGAL.C, we\ncarefully analyze its properties, and we provide the first computationally\nefficient algorithm to solve it. Finally, we report numerical simulations\nsupporting our theoretical findings and showing how SCAL significantly\noutperforms UCRL in MDPs with large diameter and small span.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 12:58:45 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 12:53:35 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Fruit", "Ronan", ""], ["Pirotta", "Matteo", ""], ["Lazaric", "Alessandro", ""], ["Ortner", "Ronald", ""]]}, {"id": "1802.04023", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun\n  Kathuria and Nisheeth K. Vishnoi", "title": "Fair and Diverse DPP-based Data Summarization", "comments": "A short version of this paper appeared in the workshop FAT/ML 2016 -\n  arXiv:1610.07183", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling methods that choose a subset of the data proportional to its\ndiversity in the feature space are popular for data summarization. However,\nrecent studies have noted the occurrence of bias (under- or over-representation\nof a certain gender or race) in such data summarization methods. In this paper\nwe initiate a study of the problem of outputting a diverse and fair summary of\na given dataset. We work with a well-studied determinantal measure of diversity\nand corresponding distributions (DPPs) and present a framework that allows us\nto incorporate a general class of fairness constraints into such distributions.\nComing up with efficient algorithms to sample from these constrained\ndeterminantal distributions, however, suffers from a complexity barrier and we\npresent a fast sampler that is provably good when the input vectors satisfy a\nnatural property. Our experimental results on a real-world and an image dataset\nshow that the diversity of the samples produced by adding fairness constraints\nis not too far from the unconstrained case, and we also provide a theoretical\nexplanation of it.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:12:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Celis", "L. Elisa", ""], ["Keswani", "Vijay", ""], ["Straszak", "Damian", ""], ["Deshpande", "Amit", ""], ["Kathuria", "Tarun", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.04034", "submitter": "Yusuke Tsuzuku", "authors": "Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama", "title": "Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural Networks", "comments": "To appear in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:37:09 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 06:14:31 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 19:02:36 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tsuzuku", "Yusuke", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.04064", "submitter": "Alberto Bietti", "authors": "Alberto Bietti, Alekh Agarwal, John Langford", "title": "A Contextual Bandit Bake-off", "comments": "JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms are essential for solving many real-world\ninteractive machine learning problems. Despite multiple recent successes on\nstatistically and computationally efficient methods, the practical behavior of\nthese algorithms is still poorly understood. We leverage the availability of\nlarge numbers of supervised learning datasets to empirically evaluate\ncontextual bandit algorithms, focusing on practical methods that learn by\nrelying on optimization oracles from supervised learning. We find that a recent\nmethod (Foster et al., 2018) using optimism under uncertainty works the best\noverall. A surprisingly close second is a simple greedy baseline that only\nexplores implicitly through the diversity of contexts, followed by a variant of\nOnline Cover (Agarwal et al., 2014) which tends to be more conservative but\nrobust to problem specification by design. Along the way, we also evaluate\nvarious components of contextual bandit algorithm design such as loss\nestimators. Overall, this is a thorough study and review of contextual bandit\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:30:56 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 15:18:07 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 23:38:57 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 09:13:07 GMT"}, {"version": "v5", "created": "Fri, 4 Jun 2021 21:46:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bietti", "Alberto", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""]]}, {"id": "1802.04065", "submitter": "Tian Guo", "authors": "Tian Guo, Albert Bifet, Nino Antulov-Fantulin", "title": "Bitcoin Volatility Forecasting with a Glimpse into Buy and Sell Orders", "comments": "Full version of the paper published at IEEE International Conference\n  on Data Mining (ICDM), 2018", "journal-ref": "2018 IEEE International Conference on Data Mining (ICDM). IEEE,\n  2018: 989-994", "doi": "10.1109/ICDM.2018.00123", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the ability to make the short-term prediction of the\nexchange price fluctuations towards the United States dollar for the Bitcoin\nmarket. We use the data of realized volatility collected from one of the\nlargest Bitcoin digital trading offices in 2016 and 2017 as well as order\ninformation. Experiments are performed to evaluate a variety of statistical and\nmachine learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:31:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 14:24:40 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 20:56:15 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Guo", "Tian", ""], ["Bifet", "Albert", ""], ["Antulov-Fantulin", "Nino", ""]]}, {"id": "1802.04085", "submitter": "Di Wang", "authors": "Di Wang and Marco Gaboardi and Jinhui Xu", "title": "Empirical Risk Minimization in Non-interactive Local Differential\n  Privacy: Efficiency and High Dimensional Case", "comments": "Add a new section on high dimensional case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Empirical Risk Minimization problem in the\nnon-interactive local model of differential privacy. In the case of constant or\nlow dimensionality ($p\\ll n$), we first show that if the ERM loss function is\n$(\\infty, T)$-smooth, then we can avoid a dependence of the sample complexity,\nto achieve error $\\alpha$, on the exponential of the dimensionality $p$ with\nbase $1/\\alpha$ (i.e., $\\alpha^{-p}$), which answers a question in [smith 2017\ninteraction]. Our approach is based on polynomial approximation. Then, we\npropose player-efficient algorithms with $1$-bit communication complexity and\n$O(1)$ computation cost for each player. The error bound is asymptotically the\nsame as the original one. Also with additional assumptions we show a server\nefficient algorithm. Next we consider the high dimensional case ($n\\ll p$), we\nshow that if the loss function is Generalized Linear function and convex, then\nwe could get an error bound which is dependent on the Gaussian width of the\nunderlying constrained set instead of $p$, which is lower than that in [smith\n2017 interaction].\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:52:24 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 03:05:28 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 20:45:46 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Wang", "Di", ""], ["Gaboardi", "Marco", ""], ["Xu", "Jinhui", ""]]}, {"id": "1802.04087", "submitter": "Min Xu", "authors": "Chang Liu, Xiangrui Zeng, Ruogu Lin, Xiaodan Liang, Zachary Freyberg,\n  Eric Xing, Min Xu", "title": "Deep learning based supervised semantic segmentation of Electron\n  Cryo-Subtomograms", "comments": "9 pages", "journal-ref": "IEEE International Conference on Image Processing (ICIP) 2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for\nthe 3D visualization of cellular structure and organization at submolecular\nresolution. It enables analyzing the native structures of macromolecular\ncomplexes and their spatial organization inside single cells. However, due to\nthe high degree of structural complexity and practical imaging limitations,\nsystematic macromolecular structural recovery inside CECT images remains\nchallenging. Particularly, the recovery of a macromolecule is likely to be\nbiased by its neighbor structures due to the high molecular crowding. To reduce\nthe bias, here we introduce a novel 3D convolutional neural network inspired by\nFully Convolutional Network and Encoder-Decoder Architecture for the supervised\nsegmentation of macromolecules of interest in subtomograms. The tests of our\nmodels on realistically simulated CECT data demonstrate that our new approach\nhas significantly improved segmentation performance compared to our baseline\napproach. Also, we demonstrate that the proposed model has generalization\nability to segment new structures that do not exist in training data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:54:49 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Liu", "Chang", ""], ["Zeng", "Xiangrui", ""], ["Lin", "Ruogu", ""], ["Liang", "Xiaodan", ""], ["Freyberg", "Zachary", ""], ["Xing", "Eric", ""], ["Xu", "Min", ""]]}, {"id": "1802.04145", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, Guillermo Sapiro", "title": "DCFNet: Deep Neural Network with Decomposed Convolutional Filters", "comments": "Published at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters in a Convolutional Neural Network (CNN) contain model parameters\nlearned from enormous amounts of data. In this paper, we suggest to decompose\nconvolutional filters in CNN as a truncated expansion with pre-fixed bases,\nnamely the Decomposed Convolutional Filters network (DCFNet), where the\nexpansion coefficients remain learned from data. Such a structure not only\nreduces the number of trainable parameters and computation, but also imposes\nfilter regularity by bases truncation. Through extensive experiments, we\nconsistently observe that DCFNet maintains accuracy for image classification\ntasks with a significant reduction of model parameters, particularly with\nFourier-Bessel (FB) bases, and even with random bases. Theoretically, we\nanalyze the representation stability of DCFNet with respect to input\nvariations, and prove representation stability under generic assumptions on the\nexpansion coefficients. The analysis is consistent with the empirical\nobservations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 15:58:54 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 20:55:55 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 23:58:20 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Qiu", "Qiang", ""], ["Cheng", "Xiuyuan", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1802.04170", "submitter": "Simon Olofsson", "authors": "Simon Olofsson, Marc Peter Deisenroth, Ruth Misener", "title": "Design of Experiments for Model Discrimination Hybridising Analytical\n  and Data-Driven Approaches", "comments": null, "journal-ref": "Proc.Mach.Learn.Res. 80 (2018) pp. 3908-3917", "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare companies must submit pharmaceutical drugs or medical devices to\nregulatory bodies before marketing new technology. Regulatory bodies frequently\nrequire transparent and interpretable computational modelling to justify a new\nhealthcare technology, but researchers may have several competing models for a\nbiological system and too little data to discriminate between the models. In\ndesign of experiments for model discrimination, the goal is to design maximally\ninformative physical experiments in order to discriminate between rival\npredictive models. Prior work has focused either on analytical approaches,\nwhich cannot manage all functions, or on data-driven approaches, which may have\ncomputational difficulties or lack interpretable marginal predictive\ndistributions. We develop a methodology introducing Gaussian process surrogates\nin lieu of the original mechanistic models. We thereby extend existing design\nand model discrimination methods developed for analytical models to cases of\nnon-analytical models in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 16:34:06 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 07:39:19 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Olofsson", "Simon", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "1802.04178", "submitter": "Robert Bridges", "authors": "Robert A. Bridges, Chris Felder, Chelsey Hoff", "title": "Dimension Reduction Using Active Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists and engineers rely on accurate mathematical models to quantify the\nobjects of their studies, which are often high-dimensional. Unfortunately,\nhigh-dimensional models are inherently difficult, i.e. when observations are\nsparse or expensive to determine. One way to address this problem is to\napproximate the original model with fewer input dimensions. Our project goal\nwas to recover a function f that takes n inputs and returns one output, where n\nis potentially large. For any given n-tuple, we assume that we can observe a\nsample of the gradient and output of the function but it is computationally\nexpensive to do so. This project was inspired by an approach known as Active\nSubspaces, which works by linearly projecting to a linear subspace where the\nfunction changes most on average. Our research gives mathematical developments\ninforming a novel algorithm for this problem. Our approach, Active Manifolds,\nincreases accuracy by seeking nonlinear analogues that approximate the\nfunction. The benefits of our approach are eliminated unprincipled parameter,\nchoices, guaranteed accessible visualization, and improved estimation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 16:07:34 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Bridges", "Robert A.", ""], ["Felder", "Chris", ""], ["Hoff", "Chelsey", ""]]}, {"id": "1802.04181", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Natalia D\\'iaz-Rodr\\'iguez, Jean-Fran\\c{c}ois\n  Goudou, David Filliat", "title": "State Representation Learning for Control: An Overview", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.07.006", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning algorithms are designed to learn abstract features\nthat characterize data. State representation learning (SRL) focuses on a\nparticular kind of representation learning where learned features are in low\ndimension, evolve through time, and are influenced by actions of an agent. The\nrepresentation is learned to capture the variation in the environment generated\nby the agent's actions; this kind of representation is particularly suitable\nfor robotics and control scenarios. In particular, the low dimension\ncharacteristic of the representation helps to overcome the curse of\ndimensionality, provides easier interpretation and utilization by humans and\ncan help improve performance and speed in policy learning algorithms such as\nreinforcement learning.\n  This survey aims at covering the state-of-the-art on state representation\nlearning in the most recent years. It reviews different SRL methods that\ninvolve interaction with the environment, their implementations and their\napplications in robotics control tasks (simulated or real). In particular, it\nhighlights how generic learning objectives are differently exploited in the\nreviewed algorithms. Finally, it discusses evaluation methods to assess the\nrepresentation learned and summarizes current and future lines of research.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 16:53:48 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:52:23 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Goudou", "Jean-Fran\u00e7ois", ""], ["Filliat", "David", ""]]}, {"id": "1802.04198", "submitter": "Leonardo Baldassini", "authors": "Leonardo Baldassini and Jose Antonio Rodr\\'iguez Serrano", "title": "client2vec: Towards Systematic Baselines for Banking Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow of data scientists normally involves potentially inefficient\nprocesses such as data mining, feature engineering and model selection. Recent\nresearch has focused on automating this workflow, partly or in its entirety, to\nimprove productivity. We choose the former approach and in this paper share our\nexperience in designing the client2vec: an internal library to rapidly build\nbaselines for banking applications. Client2vec uses marginalized stacked\ndenoising autoencoders on current account transactions data to create vector\nembeddings which represent the behaviors of our clients. These representations\ncan then be used in, and optimized against, a variety of tasks such as client\nsegmentation, profiling and targeting. Here we detail how we selected the\nalgorithmic machinery of client2vec and the data it works on and present\nexperimental results on several business cases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 17:31:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Baldassini", "Leonardo", ""], ["Serrano", "Jose Antonio Rodr\u00edguez", ""]]}, {"id": "1802.04204", "submitter": "Akshay Mehra", "authors": "Akshay Mehra, Jihun Hamm, Mikhail Belkin", "title": "Fast Interactive Image Retrieval using large-scale unlabeled data", "comments": "15 Pages, Submitted to KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interactive image retrieval system learns which images in the database\nbelong to a user's query concept, by analyzing the example images and feedback\nprovided by the user. The challenge is to retrieve the relevant images with\nminimal user interaction. In this work, we propose to solve this problem by\nposing it as a binary classification task of classifying all images in the\ndatabase as being relevant or irrelevant to the user's query concept. Our\nmethod combines active learning with graph-based semi-supervised learning\n(GSSL) to tackle this problem. Active learning reduces the number of user\ninteractions by querying the labels of the most informative points and GSSL\nallows to use abundant unlabeled data along with the limited labeled data\nprovided by the user. To efficiently find the most informative point, we use an\nuncertainty sampling based method that queries the label of the point nearest\nto the decision boundary of the classifier. We estimate this decision boundary\nusing our heuristic of adaptive threshold. To utilize huge volumes of unlabeled\ndata we use an efficient approximation based method that reduces the complexity\nof GSSL from $O(n^3)$ to $O(n)$, making GSSL scalable. We make the classifier\nrobust to the diversity and noisy labels associated with images in large\ndatabases by incorporating information from multiple modalities such as visual\ninformation extracted from deep learning based models and semantic information\nextracted from the WordNet. High F1 scores within few relevance feedback rounds\nin our experiments with concepts defined on AnimalWithAttributes and Imagenet\n(1.2 million images) datasets indicate the effectiveness and scalability of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 17:45:28 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Mehra", "Akshay", ""], ["Hamm", "Jihun", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1802.04220", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz, Michalis K. Titsias, Adji B. Dieng, David M.\n  Blei", "title": "Augment and Reduce: Stochastic Inference for Large Categorical\n  Distributions", "comments": "11 pages, 2 figures", "journal-ref": "Francisco J. R. Ruiz, Michalis K. Titsias, Adji B. Dieng, and\n  David M. Blei. Augment and Reduce: Stochastic Inference for Large Categorical\n  Distributions. International Conference on Machine Learning. Stockholm\n  (Sweden), July 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical distributions are ubiquitous in machine learning, e.g., in\nclassification, language models, and recommendation systems. However, when the\nnumber of possible outcomes is very large, using categorical distributions\nbecomes computationally expensive, as the complexity scales linearly with the\nnumber of outcomes. To address this problem, we propose augment and reduce\n(A&R), a method to alleviate the computational complexity. A&R uses two ideas:\nlatent variable augmentation and stochastic variational inference. It maximizes\na lower bound on the marginal likelihood of the data. Unlike existing methods\nwhich are specific to softmax, A&R is more general and is amenable to other\ncategorical models, such as multinomial probit. On several large-scale\nclassification problems, we show that A&R provides a tighter bound on the\nmarginal likelihood and has better predictive performance than existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:04:06 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 07:14:20 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 12:53:53 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Titsias", "Michalis K.", ""], ["Dieng", "Adji B.", ""], ["Blei", "David M.", ""]]}, {"id": "1802.04223", "submitter": "Vlad Niculae", "authors": "Vlad Niculae, Andr\\'e F. T. Martins, Mathieu Blondel, Claire Cardie", "title": "SparseMAP: Differentiable Sparse Structured Inference", "comments": "Published in ICML 2018. 14 pages, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction requires searching over a combinatorial number of\nstructures. To tackle it, we introduce SparseMAP: a new method for sparse\nstructured inference, and its natural loss function. SparseMAP automatically\nselects only a few global structures: it is situated between MAP inference,\nwhich picks a single structure, and marginal inference, which assigns\nprobability mass to all structures, including implausible ones. Importantly,\nSparseMAP can be computed using only calls to a MAP oracle, making it\napplicable to problems with intractable marginal inference, e.g., linear\nassignment. Sparsity makes gradient backpropagation efficient regardless of the\nstructure, enabling us to augment deep neural networks with generic and sparse\nstructured hidden layers. Experiments in dependency parsing and natural\nlanguage inference reveal competitive accuracy, improved interpretability, and\nthe ability to capture natural language ambiguities, which is attractive for\npipeline systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:07:34 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 16:09:16 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Niculae", "Vlad", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Blondel", "Mathieu", ""], ["Cardie", "Claire", ""]]}, {"id": "1802.04240", "submitter": "Mohammadreza Nazari", "authors": "Mohammadreza Nazari, Afshin Oroojlooy, Lawrence V. Snyder, Martin\n  Tak\\'a\\v{c}", "title": "Reinforcement Learning for Solving the Vehicle Routing Problem", "comments": "more results and illustrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end framework for solving the Vehicle Routing Problem\n(VRP) using reinforcement learning. In this approach, we train a single model\nthat finds near-optimal solutions for problem instances sampled from a given\ndistribution, only by observing the reward signals and following feasibility\nrules. Our model represents a parameterized stochastic policy, and by applying\na policy gradient algorithm to optimize its parameters, the trained model\nproduces the solution as a sequence of consecutive actions in real time,\nwithout the need to re-train for every new problem instance. On capacitated\nVRP, our approach outperforms classical heuristics and Google's OR-Tools on\nmedium-sized instances in solution quality with comparable computation time\n(after training). We demonstrate how our approach can handle problems with\nsplit delivery and explore the effect of such deliveries on the solution\nquality. Our proposed framework can be applied to other variants of the VRP\nsuch as the stochastic VRP, and has the potential to be applied more generally\nto combinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:41:57 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 18:25:38 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Nazari", "Mohammadreza", ""], ["Oroojlooy", "Afshin", ""], ["Snyder", "Lawrence V.", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1802.04253", "submitter": "Chengliang Yang", "authors": "Chengliang Yang, Anand Rangarajan, Sanjay Ranka", "title": "Global Model Interpretation via Recursive Partitioning", "comments": "Accepted by The 4th IEEE International Conference on Data Science and\n  Systems (DSS-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple but effective method to interpret black-box\nmachine learning models globally. That is, we use a compact binary tree, the\ninterpretation tree, to explicitly represent the most important decision rules\nthat are implicitly contained in the black-box machine learning models. This\ntree is learned from the contribution matrix which consists of the\ncontributions of input variables to predicted scores for each single\nprediction. To generate the interpretation tree, a unified process recursively\npartitions the input variable space by maximizing the difference in the average\ncontribution of the split variable between the divided spaces. We demonstrate\nthe effectiveness of our method in diagnosing machine learning models on\nmultiple tasks. Also, it is useful for new knowledge discovery as such insights\nare not easily identifiable when only looking at single predictions. In\ngeneral, our work makes it easier and more efficient for human beings to\nunderstand machine learning models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 00:24:32 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 04:05:40 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Yang", "Chengliang", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1802.04302", "submitter": "Ishita Dasgupta", "authors": "Ishita Dasgupta, Demi Guo, Andreas Stuhlm\\\"uller, Samuel J. Gershman\n  and Noah D. Goodman", "title": "Evaluating Compositionality in Sentence Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge for human-like AI is compositional semantics. Recent\nresearch has attempted to address this by using deep neural networks to learn\nvector space embeddings of sentences, which then serve as input to other tasks.\nWe present a new dataset for one such task, `natural language inference' (NLI),\nthat cannot be solved using only word-level knowledge and requires some\ncompositionality. We find that the performance of state of the art sentence\nembeddings (InferSent; Conneau et al., 2017) on our new dataset is poor. We\nanalyze the decision rules learned by InferSent and find that they are\nconsistent with simple heuristics that are ecologically valid in its training\ndataset. Further, we find that augmenting training with our dataset improves\ntest performance on our dataset without loss of performance on the original\ntraining dataset. This highlights the importance of structured datasets in\nbetter understanding and improving AI systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:02:52 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 19:01:47 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Dasgupta", "Ishita", ""], ["Guo", "Demi", ""], ["Stuhlm\u00fcller", "Andreas", ""], ["Gershman", "Samuel J.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1802.04307", "submitter": "Yujia Xie", "authors": "Yujia Xie, Xiangfeng Wang, Ruijia Wang, Hongyuan Zha", "title": "A Fast Proximal Point Method for Computing Exact Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein distance plays increasingly important roles in machine learning,\nstochastic programming and image processing. Major efforts have been under way\nto address its high computational complexity, some leading to approximate or\nregularized variations such as Sinkhorn distance. However, as we will\ndemonstrate, regularized variations with large regularization parameter will\ndegradate the performance in several important machine learning applications,\nand small regularization parameter will fail due to numerical stability issues\nwith existing algorithms. We address this challenge by developing an Inexact\nProximal point method for exact Optimal Transport problem (IPOT) with the\nproximal operator approximately evaluated at each iteration using projections\nto the probability simplex. The algorithm (a) converges to exact Wasserstein\ndistance with theoretical guarantee and robust regularization parameter\nselection, (b) alleviates numerical stability issue, (c) has similar\ncomputational complexity to Sinkhorn, and (d) avoids the shrinking problem when\napply to generative models. Furthermore, a new algorithm is proposed based on\nIPOT to obtain sharper Wasserstein barycenter.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:06:59 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 05:14:58 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 19:08:54 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Xie", "Yujia", ""], ["Wang", "Xiangfeng", ""], ["Wang", "Ruijia", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1802.04310", "submitter": "Adrian Wills", "authors": "Adrian Wills and Thomas Sch\\\"on", "title": "Stochastic quasi-Newton with adaptive step lengths for large-scale\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a numerically robust and fast method capable of exploiting the\nlocal geometry when solving large-scale stochastic optimisation problems. Our\nkey innovation is an auxiliary variable construction coupled with an inverse\nHessian approximation computed using a receding history of iterates and\ngradients. It is the Markov chain nature of the classic stochastic gradient\nalgorithm that enables this development. The construction offers a mechanism\nfor stochastic line search adapting the step length. We numerically evaluate\nand compare against current state-of-the-art with encouraging performance on\nreal-world benchmark problems where the number of observations and unknowns is\nin the order of millions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:13:28 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wills", "Adrian", ""], ["Sch\u00f6n", "Thomas", ""]]}, {"id": "1802.04325", "submitter": "Dane Corneil", "authors": "Dane Corneil, Wulfram Gerstner and Johanni Brea", "title": "Efficient Model-Based Deep Reinforcement Learning with Variational State\n  Tabulation", "comments": "Accepted at ICML 2018; camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern reinforcement learning algorithms reach super-human performance on\nmany board and video games, but they are sample inefficient, i.e. they\ntypically require significantly more playing experience than humans to reach an\nequal performance level. To improve sample efficiency, an agent may build a\nmodel of the environment and use planning methods to update its policy. In this\narticle we introduce Variational State Tabulation (VaST), which maps an\nenvironment with a high-dimensional state space (e.g. the space of visual\ninputs) to an abstract tabular model. Prioritized sweeping with small backups,\na highly efficient planning method, can then be used to update state-action\nvalues. We show how VaST can rapidly learn to maximize reward in tasks like 3D\nnavigation and efficiently adapt to sudden changes in rewards or transition\nprobabilities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:38:44 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 15:27:18 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Corneil", "Dane", ""], ["Gerstner", "Wulfram", ""], ["Brea", "Johanni", ""]]}, {"id": "1802.04346", "submitter": "Tong Wang", "authors": "Tong Wang", "title": "Gaining Free or Low-Cost Transparency with Interpretable Partial\n  Substitute", "comments": null, "journal-ref": "International Conference on Machine Learning, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the situation where a black-box model with good\npredictive performance is chosen over its interpretable competitors, and we\nshow interpretability is still achievable in this case. Our solution is to find\nan interpretable substitute on a subset of data where the black-box model is\noverkill or nearly overkill while leaving the rest to the black-box. This\ntransparency is obtained at minimal cost or no cost of the predictive\nperformance. Under this framework, we develop a Hybrid Rule Sets (HyRS) model\nthat uses decision rules to capture the subspace of data where the rules are as\naccurate or almost as accurate as the black-box provided. To train a HyRS, we\ndevise an efficient search algorithm that iteratively finds the optimal model\nand exploits theoretically grounded strategies to reduce computation. Our\nframework is agnostic to the black-box during training. Experiments on\nstructured and text data show that HyRS obtains an effective trade-off between\ntransparency and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:24:47 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 16:31:32 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Wang", "Tong", ""]]}, {"id": "1802.04350", "submitter": "Longyun Guo", "authors": "Longyun Guo and Jean Honorio and John Morgan", "title": "Cost-Aware Learning for Improved Identifiability with Multiple\n  Experiments", "comments": "17 pages, 4 figures", "journal-ref": "IEEE International Symposium on Information Theory (ISIT) 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sample complexity of learning from multiple experiments where\nthe experimenter has a total budget for obtaining samples. In this problem, the\nlearner should choose a hypothesis that performs well with respect to multiple\nexperiments, and their related data distributions. Each collected sample is\nassociated with a cost which depends on the particular experiments. In our\nsetup, a learner performs $m$ experiments, while incurring a total cost $C$. We\nfirst show that learning from multiple experiments allows to improve\nidentifiability. Additionally, by using a Rademacher complexity approach, we\nshow that the gap between the training and generalization error is\n$O(C^{-1/2})$. We also provide some examples for linear prediction, two-layer\nneural networks and kernel methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:31:58 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 02:08:53 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 22:32:09 GMT"}, {"version": "v4", "created": "Fri, 18 Jan 2019 18:41:30 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 20:34:57 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Guo", "Longyun", ""], ["Honorio", "Jean", ""], ["Morgan", "John", ""]]}, {"id": "1802.04364", "submitter": "Wengong Jin", "authors": "Wengong Jin, Regina Barzilay, Tommi Jaakkola", "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:19:39 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 00:17:35 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 17:20:47 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 14:44:43 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Jin", "Wengong", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1802.04365", "submitter": "Mehadi Hassen", "authors": "Mehadi Hassen and Philip K. Chan", "title": "Learning a Neural-network-based Representation for Open Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set recognition problems exist in many domains. For example in security,\nnew malware classes emerge regularly; therefore malware classification systems\nneed to identify instances from unknown classes in addition to discriminating\nbetween known classes. In this paper we present a neural network based\nrepresentation for addressing the open set recognition problem. In this\nrepresentation instances from the same class are close to each other while\ninstances from different classes are further apart, resulting in statistically\nsignificant improvement when compared to other approaches on three datasets\nfrom two different domains.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:20:30 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Hassen", "Mehadi", ""], ["Chan", "Philip K.", ""]]}, {"id": "1802.04374", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Giambattista Parascandolo and Arash Mehrjou\n  and Bernhard Sch\\\"olkopf", "title": "Tempered Adversarial Networks", "comments": "accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been shown to produce realistic\nsamples from high-dimensional distributions, but training them is considered\nhard. A possible explanation for training instabilities is the inherent\nimbalance between the networks: While the discriminator is trained directly on\nboth real and fake samples, the generator only has control over the fake\nsamples it produces since the real data distribution is fixed by the choice of\na given dataset. We propose a simple modification that gives the generator\ncontrol over the real samples which leads to a tempered learning process for\nboth generator and discriminator. The real data distribution passes through a\nlens before being revealed to the discriminator, balancing the generator and\ndiscriminator by gradually revealing more detailed features necessary to\nproduce high-quality results. The proposed module automatically adjusts the\nlearning process to the current strength of the networks, yet is generic and\neasy to add to any GAN variant. In a number of experiments, we show that this\ncan improve quality, stability and/or convergence speed across a range of\ndifferent GAN architectures (DCGAN, LSGAN, WGAN-GP).\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:49:07 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 18:21:57 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 14:45:23 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 12:13:54 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Parascandolo", "Giambattista", ""], ["Mehrjou", "Arash", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1802.04376", "submitter": "Nathan Hilliard", "authors": "Nathan Hilliard and Lawrence Phillips, Scott Howland, Art\\\"em Yankov,\n  Courtney D. Corley, Nathan O. Hodas", "title": "Few-Shot Learning with Metric-Agnostic Conditional Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning high quality class representations from few examples is a key\nproblem in metric-learning approaches to few-shot learning. To accomplish this,\nwe introduce a novel architecture where class representations are conditioned\nfor each few-shot trial based on a target image. We also deviate from\ntraditional metric-learning approaches by training a network to perform\ncomparisons between classes rather than relying on a static metric comparison.\nThis allows the network to decide what aspects of each class are important for\nthe comparison at hand. We find that this flexible architecture works well in\npractice, achieving state-of-the-art performance on the Caltech-UCSD birds\nfine-grained classification task.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:56:12 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Hilliard", "Nathan", ""], ["Phillips", "Lawrence", ""], ["Howland", "Scott", ""], ["Yankov", "Art\u00ebm", ""], ["Corley", "Courtney D.", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "1802.04397", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Chen Dan, Eric P. Xing, Pradeep Ravikumar", "title": "Identifiability of Nonparametric Mixture Models and Bayes Optimal\n  Clustering", "comments": "35 pages, to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems in data clustering, we establish general conditions\nunder which families of nonparametric mixture models are identifiable, by\nintroducing a novel framework involving clustering overfitted \\emph{parametric}\n(i.e. misspecified) mixture models. These identifiability conditions generalize\nexisting conditions in the literature, and are flexible enough to include for\nexample mixtures of Gaussian mixtures. In contrast to the recent literature on\nestimating nonparametric mixtures, we allow for general nonparametric mixture\ncomponents, and instead impose regularity assumptions on the underlying mixing\nmeasure. As our primary application, we apply these results to partition-based\nclustering, generalizing the notion of a Bayes optimal partition from classical\nparametric model-based clustering to nonparametric settings. Furthermore, this\nframework is constructive so that it yields a practical algorithm for learning\nidentified mixtures, which is illustrated through several examples on real\ndata. The key conceptual device in the analysis is the convex, metric geometry\nof probability measures on metric spaces and its connection to the Wasserstein\nconvergence of mixing measures. The result is a flexible framework for\nnonparametric clustering with formal consistency guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 23:53:52 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 16:20:25 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 20:31:51 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 03:52:29 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Aragam", "Bryon", ""], ["Dan", "Chen", ""], ["Xing", "Eric P.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1802.04403", "submitter": "Haque Ishfaq", "authors": "Haque Ishfaq, Assaf Hoogi and Daniel Rubin", "title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "comments": "After submission, we realized that our work is very similar to work\n  done in \"Bayesian representation learning with oracle constraints\" by\n  Karaletsos et al (arXiv:1506.05011). This paper somehow didn't come into our\n  notice earlier and now that we know the idea we presented in our paper was\n  already explored there, we decided to withdraw our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has been demonstrated to be highly effective in learning\nsemantic representation and encoding information that can be used to measure\ndata similarity, by relying on the embedding learned from metric learning. At\nthe same time, variational autoencoder (VAE) has widely been used to\napproximate inference and proved to have a good performance for directed\nprobabilistic models. However, for traditional VAE, the data label or feature\ninformation are intractable. Similarly, traditional representation learning\napproaches fail to represent many salient aspects of the data. In this project,\nwe propose a novel integrated framework to learn latent embedding in VAE by\nincorporating deep metric learning. The features are learned by optimizing a\ntriplet loss on the mean vectors of VAE in conjunction with standard evidence\nlower bound (ELBO) of VAE. This approach, which we call Triplet based\nVariational Autoencoder (TVAE), allows us to capture more fine-grained\ninformation in the latent embedding. Our model is tested on MNIST data set and\nachieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &\nWelling, 2013) achieves triplet accuracy of 75.08%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:05:19 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 15:31:47 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Ishfaq", "Haque", ""], ["Hoogi", "Assaf", ""], ["Rubin", "Daniel", ""]]}, {"id": "1802.04407", "submitter": "Guodong Long Dr", "authors": "Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, Chengqi\n  Zhang", "title": "Adversarially Regularized Graph Autoencoder for Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding is an effective method to represent graph data in a low\ndimensional space for graph analytics. Most existing embedding algorithms\ntypically focus on preserving the topological structure or minimizing the\nreconstruction errors of graph data, but they have mostly ignored the data\ndistribution of the latent codes from the graphs, which often results in\ninferior embedding in real-world graph data. In this paper, we propose a novel\nadversarial graph embedding framework for graph data. The framework encodes the\ntopological structure and node content in a graph to a compact representation,\non which a decoder is trained to reconstruct the graph structure. Furthermore,\nthe latent representation is enforced to match a prior distribution via an\nadversarial training scheme. To learn a robust embedding, two variants of\nadversarial approaches, adversarially regularized graph autoencoder (ARGA) and\nadversarially regularized variational graph autoencoder (ARVGA), are developed.\nExperimental studies on real-world graphs validate our design and demonstrate\nthat our algorithms outperform baselines by a wide margin in link prediction,\ngraph clustering, and graph visualization tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:29:11 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 00:26:13 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Pan", "Shirui", ""], ["Hu", "Ruiqi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Yao", "Lina", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1802.04412", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli and Animashree Anandkumar", "title": "Efficient Exploration through Bayesian Deep Q-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning (RL) in high dimensional episodic Markov\ndecision processes (MDP). We consider value-based RL when the optimal Q-value\nis a linear function of d-dimensional state-action feature representation. For\ninstance, in deep-Q networks (DQN), the Q-value is a linear function of the\nfeature representation layer (output layer). We propose two algorithms, one\nbased on optimism, LINUCB, and another based on posterior sampling, LINPSRL. We\nguarantee frequentist and Bayesian regret upper bounds of O(d sqrt{T}) for\nthese two algorithms, where T is the number of episodes. We extend these\nmethods to deep RL and propose Bayesian deep Q-networks (BDQN), which uses an\nefficient Thompson sampling algorithm for high dimensional RL. We deploy the\ndouble DQN (DDQN) approach, and instead of learning the last layer of Q-network\nusing linear regression, we use Bayesian linear regression, resulting in an\napproximated posterior over Q-function. This allows us to directly incorporate\nthe uncertainty over the Q-function and deploy Thompson sampling on the learned\nposterior distribution resulting in efficient exploration/exploitation\ntrade-off. We empirically study the behavior of BDQN on a wide range of Atari\ngames. Since BDQN carries out more efficient exploration and exploitation, it\nis able to reach higher return substantially faster compared to DDQN.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:48:17 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 03:26:22 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 18:44:40 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 20:54:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1802.04420", "submitter": "Yifan Wu", "authors": "Yifan Wu, Barnabas Poczos, Aarti Singh", "title": "Towards Understanding the Generalization Bias of Two Layer Convolutional\n  Linear Classifiers with Gradient Descent", "comments": "The 22nd International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in understanding the generalization of deep learning is to\nexplain why (stochastic) gradient descent can exploit the network architecture\nto find solutions that have good generalization performance when using high\ncapacity models. We find simple but realistic examples showing that this\nphenomenon exists even when learning linear classifiers --- between two linear\nnetworks with the same capacity, the one with a convolutional layer can\ngeneralize better than the other when the data distribution has some underlying\nspatial structure. We argue that this difference results from a combination of\nthe convolution architecture, data distribution and gradient descent, all of\nwhich are necessary to be included in a meaningful analysis. We provide a\ngeneral analysis of the generalization performance as a function of data\ndistribution and convolutional filter size, given gradient descent as the\noptimization algorithm, then interpret the results using concrete examples.\nExperimental results show that our analysis is able to explain what happens in\nour introduced examples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 01:20:21 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 00:37:16 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Wu", "Yifan", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""]]}, {"id": "1802.04422", "submitter": "Sorelle Friedler", "authors": "Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian,\n  Sonam Choudhary, Evan P. Hamilton, Derek Roth", "title": "A comparative study of fairness-enhancing interventions in machine\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computers are increasingly used to make decisions that have significant\nimpact in people's lives. Often, these predictions can affect different\npopulation subgroups disproportionately. As a result, the issue of fairness has\nreceived much recent interest, and a number of fairness-enhanced classifiers\nand predictors have appeared in the literature. This paper seeks to study the\nfollowing questions: how do these different techniques fundamentally compare to\none another, and what accounts for the differences? Specifically, we seek to\nbring attention to many under-appreciated aspects of such fairness-enhancing\ninterventions. Concretely, we present the results of an open benchmark we have\ndeveloped that lets us compare a number of different algorithms under a variety\nof fairness measures, and a large number of existing datasets. We find that\nalthough different algorithms tend to prefer specific formulations of fairness\npreservations, many of these measures strongly correlate with one another. In\naddition, we find that fairness-preserving algorithms tend to be sensitive to\nfluctuations in dataset composition (simulated in our benchmark by varying\ntraining-test splits), indicating that fairness interventions might be more\nbrittle than previously thought.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 01:31:51 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Friedler", "Sorelle A.", ""], ["Scheidegger", "Carlos", ""], ["Venkatasubramanian", "Suresh", ""], ["Choudhary", "Sonam", ""], ["Hamilton", "Evan P.", ""], ["Roth", "Derek", ""]]}, {"id": "1802.04431", "submitter": "Kyle Hundman", "authors": "Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian\n  Colwell, Tom Soderstrom", "title": "Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic\n  Thresholding", "comments": "KDD 2018 camera-ready version", "journal-ref": null, "doi": "10.1145/3219819.3219845", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As spacecraft send back increasing amounts of telemetry data, improved\nanomaly detection systems are needed to lessen the monitoring burden placed on\noperations engineers and reduce operational risk. Current spacecraft monitoring\nsystems only target a subset of anomaly types and often require costly expert\nknowledge to develop and maintain due to challenges involving scale and\ncomplexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs)\nnetworks, a type of Recurrent Neural Network (RNN), in overcoming these issues\nusing expert-labeled telemetry anomaly data from the Soil Moisture Active\nPassive (SMAP) satellite and the Mars Science Laboratory (MSL) rover,\nCuriosity. We also propose a complementary unsupervised and nonparametric\nanomaly thresholding approach developed during a pilot implementation of an\nanomaly detection system for SMAP, and offer false positive mitigation\nstrategies along with other key improvements and lessons learned during\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:09:32 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 01:52:08 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 20:39:29 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Hundman", "Kyle", ""], ["Constantinou", "Valentino", ""], ["Laporte", "Christopher", ""], ["Colwell", "Ian", ""], ["Soderstrom", "Tom", ""]]}, {"id": "1802.04443", "submitter": "William Guss", "authors": "William H. Guss, Ruslan Salakhutdinov", "title": "On Characterizing the Capacity of Neural Networks using Algebraic\n  Topology", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.NE math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learnability of different neural architectures can be characterized\ndirectly by computable measures of data complexity. In this paper, we reframe\nthe problem of architecture selection as understanding how data determines the\nmost expressive and generalizable architectures suited to that data, beyond\ninductive bias. After suggesting algebraic topology as a measure for data\ncomplexity, we show that the power of a network to express the topological\ncomplexity of a dataset in its decision region is a strictly limiting factor in\nits ability to generalize. We then provide the first empirical characterization\nof the topological capacity of neural networks. Our empirical analysis shows\nthat at every level of dataset complexity, neural networks exhibit topological\nphase transitions. This observation allowed us to connect existing theory to\nempirically driven conjectures on the choice of architectures for\nfully-connected neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:32:10 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Guss", "William H.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1802.04457", "submitter": "Angus Galloway", "authors": "Angus Galloway and Graham W. Taylor and Medhat Moussa", "title": "Predicting Adversarial Examples with High Confidence", "comments": "Under review by the International Conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested that adversarial examples cause deep learning models to\nmake incorrect predictions with high confidence. In this work, we take the\nopposite stance: an overly confident model is more likely to be vulnerable to\nadversarial examples. This work is one of the most proactive approaches taken\nto date, as we link robustness with non-calibrated model confidence on noisy\nimages, providing a data-augmentation-free path forward. The adversarial\nexamples phenomenon is most easily explained by the trend of increasing\nnon-regularized model capacity, while the diversity and number of samples in\ncommon datasets has remained flat. Test accuracy has incorrectly been\nassociated with true generalization performance, ignoring that training and\ntest splits are often extremely similar in terms of the overall representation\nspace. The transferability property of adversarial examples was previously used\nas evidence against overfitting arguments, a perceived random effect, but\noverfitting is not always random.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 03:45:28 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Galloway", "Angus", ""], ["Taylor", "Graham W.", ""], ["Moussa", "Medhat", ""]]}, {"id": "1802.04474", "submitter": "Masaaki Imaizumi", "authors": "Masaaki Imaizumi, Kenji Fukumizu", "title": "Deep Neural Networks Learn Non-Smooth Functions Effectively", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically discuss why deep neural networks (DNNs) performs better than\nother models in some cases by investigating statistical properties of DNNs for\nnon-smooth functions. While DNNs have empirically shown higher performance than\nother standard methods, understanding its mechanism is still a challenging\nproblem. From an aspect of the statistical theory, it is known many standard\nmethods attain the optimal rate of generalization errors for smooth functions\nin large sample asymptotics, and thus it has not been straightforward to find\ntheoretical advantages of DNNs. This paper fills this gap by considering\nlearning of a certain class of non-smooth functions, which was not covered by\nthe previous theory. We derive the generalization error of estimators by DNNs\nwith a ReLU activation, and show that convergence rates of the generalization\nby DNNs are almost optimal to estimate the non-smooth functions, while some of\nthe popular models do not attain the optimal rate. In addition, our theoretical\nresult provides guidelines for selecting an appropriate number of layers and\nedges of DNNs. We provide numerical experiments to support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 06:24:27 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 05:24:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Imaizumi", "Masaaki", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1802.04475", "submitter": "Muni Sreenivas Pydi", "authors": "Muni Sreenivas Pydi, Varun Jog, Po-Ling Loh", "title": "Graph-Based Ascent Algorithms for Function Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the maximum of a function defined on the\nnodes of a connected graph. The goal is to identify a node where the function\nobtains its maximum. We focus on local iterative algorithms, which traverse the\nnodes of the graph along a path, and the next iterate is chosen from the\nneighbors of the current iterate with probability distribution determined by\nthe function values at the current iterate and its neighbors. We study two\nalgorithms corresponding to a Metropolis-Hastings random walk with different\ntransition kernels: (i) The first algorithm is an exponentially weighted random\nwalk governed by a parameter $\\gamma$. (ii) The second algorithm is defined\nwith respect to the graph Laplacian and a smoothness parameter $k$. We derive\nconvergence rates for the two algorithms in terms of total variation distance\nand hitting times. We also provide simulations showing the relative convergence\nrates of our algorithms in comparison to an unbiased random walk, as a function\nof the smoothness of the graph function. Our algorithms may be categorized as a\nnew class of \"descent-based\" methods for function maximization on the nodes of\na graph.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 06:31:15 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Pydi", "Muni Sreenivas", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1802.04477", "submitter": "Zhize Li", "authors": "Zhize Li, Jian Li", "title": "A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex\n  Optimization", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth\nfinite-sum problems. In particular, the objective function is given by the\nsummation of a differentiable (possibly nonconvex) component, together with a\npossibly non-differentiable but convex component. We propose a proximal\nstochastic gradient algorithm based on variance reduction, called ProxSVRG+.\nOur main contribution lies in the analysis of ProxSVRG+. It recovers several\nexisting convergence results and improves/generalizes them (in terms of the\nnumber of stochastic gradient oracle calls and proximal oracle calls). In\nparticular, ProxSVRG+ generalizes the best results given by the SCSG algorithm,\nrecently proposed by [Lei et al., 2017] for the smooth nonconvex case.\nProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.\nMoreover, ProxSVRG+ outperforms the deterministic proximal gradient descent\n(ProxGD) for a wide range of minibatch sizes, which partially solves an open\nproblem proposed in [Reddi et al., 2016b]. Also, ProxSVRG+ uses much less\nproximal oracle calls than ProxSVRG [Reddi et al., 2016b]. Moreover, for\nnonconvex functions satisfied Polyak-\\L{}ojasiewicz condition, we prove that\nProxSVRG+ achieves a global linear convergence rate without restart unlike\nProxSVRG. Thus, it can \\emph{automatically} switch to the faster linear\nconvergence in some regions as long as the objective function satisfies the PL\ncondition locally in these regions. ProxSVRG+ also improves ProxGD and\nProxSVRG/SAGA, and generalizes the results of SCSG in this case. Finally, we\nconduct several experiments and the experimental results are consistent with\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 06:34:22 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:56:56 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 11:31:17 GMT"}, {"version": "v4", "created": "Sat, 1 Dec 2018 20:10:29 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Zhize", ""], ["Li", "Jian", ""]]}, {"id": "1802.04502", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda", "title": "Legendre Decomposition for Tensors", "comments": "12 pages, 6 figures, accepted to the 32nd Annual Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": "10.1088/1742-5468/ab3196", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel nonnegative tensor decomposition method, called Legendre\ndecomposition, which factorizes an input tensor into a multiplicative\ncombination of parameters. Thanks to the well-developed theory of information\ngeometry, the reconstructed tensor is unique and always minimizes the KL\ndivergence from an input tensor. We empirically show that Legendre\ndecomposition can more accurately reconstruct tensors than other nonnegative\ntensor decomposition methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 08:27:49 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 14:54:37 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Nakahara", "Hiroyuki", ""], ["Tsuda", "Koji", ""]]}, {"id": "1802.04537", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison,\n  Maximilian Igl, Frank Wood, Yee Whye Teh", "title": "Tighter Variational Bounds are Not Necessarily Better", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical and empirical evidence that using tighter evidence\nlower bounds (ELBOs) can be detrimental to the process of learning an inference\nnetwork by reducing the signal-to-noise ratio of the gradient estimator. Our\nresults call into question common implicit assumptions that tighter ELBOs are\nbetter variational objectives for simultaneous model learning and inference\namortization schemes. Based on our insights, we introduce three new algorithms:\nthe partially importance weighted auto-encoder (PIWAE), the multiply importance\nweighted auto-encoder (MIWAE), and the combination importance weighted\nauto-encoder (CIWAE), each of which includes the standard importance weighted\nauto-encoder (IWAE) as a special case. We show that each can deliver\nimprovements over IWAE, even when performance is measured by the IWAE target\nitself. Furthermore, our results suggest that PIWAE may be able to deliver\nsimultaneous improvements in the training of both the inference and generative\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:17:32 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 16:51:15 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 19:25:55 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Rainforth", "Tom", ""], ["Kosiorek", "Adam R.", ""], ["Le", "Tuan Anh", ""], ["Maddison", "Chris J.", ""], ["Igl", "Maximilian", ""], ["Wood", "Frank", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1802.04551", "submitter": "Hideaki Imamura", "authors": "Hideaki Imamura and Issei Sato and Masashi Sugiyama", "title": "Analysis of Minimax Error Rate for Crowdsourcing and Its Application to\n  Worker Clustering Model", "comments": "Accepted to ICML2018 (International Conference on Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While crowdsourcing has become an important means to label data, there is\ngreat interest in estimating the ground truth from unreliable labels produced\nby crowdworkers. The Dawid and Skene (DS) model is one of the most well-known\nmodels in the study of crowdsourcing. Despite its practical popularity,\ntheoretical error analysis for the DS model has been conducted only under\nrestrictive assumptions on class priors, confusion matrices, or the number of\nlabels each worker provides. In this paper, we derive a minimax error rate\nunder more practical setting for a broader class of crowdsourcing models\nincluding the DS model as a special case. We further propose the worker\nclustering model, which is more practical than the DS model under real\ncrowdsourcing settings. The wide applicability of our theoretical analysis\nallows us to immediately investigate the behavior of this proposed model, which\ncan not be analyzed by existing studies. Experimental results showed that there\nis a strong similarity between the lower bound of the minimax error rate\nderived by our theoretical analysis and the empirical error of the estimated\nvalue.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:47:14 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 05:35:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Imamura", "Hideaki", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.04564", "submitter": "Zhang-Wei Hong", "authors": "Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Chun-Yi\n  Lee", "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration remains a challenging research problem in reinforcement\nlearning, especially when an environment contains large state spaces, deceptive\nlocal optima, or sparse rewards. To tackle this problem, we present a\ndiversity-driven approach for exploration, which can be easily combined with\nboth off- and on-policy reinforcement learning algorithms. We show that by\nsimply adding a distance measure to the loss function, the proposed methodology\nsignificantly enhances an agent's exploratory behaviors, and thus preventing\nthe policy from being trapped in local optima. We further propose an adaptive\nscaling method for stabilizing the learning process. Our experimental results\nin Atari 2600 show that our method outperforms baseline approaches in several\ntasks in terms of mean scores and exploration efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 11:18:41 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 04:47:56 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hong", "Zhang-Wei", ""], ["Shann", "Tzu-Yun", ""], ["Su", "Shih-Yang", ""], ["Chang", "Yi-Hsiang", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1802.04591", "submitter": "Calvin Seward", "authors": "Calvin Seward, Thomas Unterthiner, Urs Bergmann, Nikolay Jetchev, Sepp\n  Hochreiter", "title": "First Order Generative Adversarial Networks", "comments": "Accepted to 35th International Conference on Machine Learning (ICML).\n  Code to reproduce experiments is available\n  https://github.com/zalandoresearch/first_order_gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs excel at learning high dimensional distributions, but they can update\ngenerator parameters in directions that do not correspond to the steepest\ndescent direction of the objective. Prominent examples of problematic update\ndirections include those used in both Goodfellow's original GAN and the\nWGAN-GP. To formally describe an optimal update direction, we introduce a\ntheoretical framework which allows the derivation of requirements on both the\ndivergence and corresponding method for determining an update direction, with\nthese requirements guaranteeing unbiased mini-batch updates in the direction of\nsteepest descent. We propose a novel divergence which approximates the\nWasserstein distance while regularizing the critic's first order information.\nTogether with an accompanying update direction, this divergence fulfills the\nrequirements for unbiased steepest descent updates. We verify our method, the\nFirst Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a\nnew state of the art on the One Billion Word language generation task. Code to\nreproduce experiments is available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 12:42:58 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:16:05 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Seward", "Calvin", ""], ["Unterthiner", "Thomas", ""], ["Bergmann", "Urs", ""], ["Jetchev", "Nikolay", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1802.04617", "submitter": "Chao Qu", "authors": "Chao Qu, Yan Li, Huan Xu", "title": "Fast Global Convergence via Landscape of Empirical Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While optimizing convex objective (loss) functions has been a powerhouse for\nmachine learning for at least two decades, non-convex loss functions have\nattracted fast growing interests recently, due to many desirable properties\nsuch as superior robustness and classification accuracy, compared with their\nconvex counterparts. The main obstacle for non-convex estimators is that it is\nin general intractable to find the optimal solution. In this paper, we study\nthe computational issues for some non-convex M-estimators. In particular, we\nshow that the stochastic variance reduction methods converge to the global\noptimal with linear rate, by exploiting the statistical property of the\npopulation loss. En route, we improve the convergence analysis for the batch\ngradient method in \\cite{mei2016landscape}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 13:37:11 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Qu", "Chao", ""], ["Li", "Yan", ""], ["Xu", "Huan", ""]]}, {"id": "1802.04626", "submitter": "S\\\"oren Klemm", "authors": "Soeren Klemm and Aaron Scherzinger and Dominik Drees and Xiaoyi Jiang", "title": "Barista - a Graphical Tool for Designing and Training Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the importance of deep learning has significantly increased\nin pattern recognition, computer vision, and artificial intelligence research,\nas well as in industry. However, despite the existence of multiple deep\nlearning frameworks, there is a lack of comprehensible and easy-to-use\nhigh-level tools for the design, training, and testing of deep neural networks\n(DNNs). In this paper, we introduce Barista, an open-source graphical\nhigh-level interface for the Caffe deep learning framework. While Caffe is one\nof the most popular frameworks for training DNNs, editing prototext files in\norder to specify the net architecture and hyper parameters can become a\ncumbersome and error-prone task. Instead, Barista offers a fully graphical user\ninterface with a graph-based net topology editor and provides an end-to-end\ntraining facility for DNNs, which allows researchers to focus on solving their\nproblems without having to write code, edit text files, or manually parse\nlogged data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:02:48 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Klemm", "Soeren", ""], ["Scherzinger", "Aaron", ""], ["Drees", "Dominik", ""], ["Jiang", "Xiaoyi", ""]]}, {"id": "1802.04630", "submitter": "Akifumi Okuno", "authors": "Akifumi Okuno, Tetsuya Hada, Hidetoshi Shimodaira", "title": "A probabilistic framework for multi-view feature learning with\n  many-to-many associations via neural networks", "comments": "16 pages (with Supplementary Material), 5 figures, ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is\nproposed for multi-view feature learning with many-to-many associations so that\nit generalizes various existing multi-view methods. PMvGE is a probabilistic\nmodel for predicting new associations via graph embedding of the nodes of data\nvectors with links of their associations. Multi-view data vectors with\nmany-to-many associations are transformed by neural networks to feature vectors\nin a shared space, and the probability of new association between two data\nvectors is modeled by the inner product of their feature vectors. While\nexisting multi-view feature learning techniques can treat only either of\nmany-to-many association or non-linear transformation, PMvGE can treat both\nsimultaneously. By combining Mercer's theorem and the universal approximation\ntheorem, we prove that PMvGE learns a wide class of similarity measures across\nviews. Our likelihood-based estimator enables efficient computation of\nnon-linear transformations of data vectors in large-scale datasets by minibatch\nSGD, and numerical experiments illustrate that PMvGE outperforms existing\nmulti-view methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:09:18 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 00:35:15 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Okuno", "Akifumi", ""], ["Hada", "Tetsuya", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1802.04664", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "Recovering Loss to Followup Information Using Denoising Autoencoders", "comments": "Copyright IEEE 2017, IEEE International Conference on Big Data (Big\n  Data)", "journal-ref": null, "doi": "10.1109/BigData.2017.8258139", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss to followup is a significant issue in healthcare and has serious\nconsequences for a study's validity and cost. Methods available at present for\nrecovering loss to followup information are restricted by their expressive\ncapabilities and struggle to model highly non-linear relations and complex\ninteractions. In this paper we propose a model based on overcomplete denoising\nautoencoders to recover loss to followup information. Designed to work with\nhigh volume data, results on various simulated and real life datasets show our\nmodel is appropriate under varying dataset and loss to followup conditions and\noutperforms the state-of-the-art methods by a wide margin ($\\ge 20\\%$ in some\nscenarios) while preserving the dataset utility for final analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 03:34:03 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1802.04676", "submitter": "JunYong Jeong", "authors": "Jun-Yong Jeong and Chi-Hyuck Jun", "title": "Variable Selection and Task Grouping for Multi-Task Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": "10.1145/3219819.3219992", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-task learning, which simultaneously learns related\nprediction tasks, to improve generalization performance. We factorize a\ncoefficient matrix as the product of two matrices based on a low-rank\nassumption. These matrices have sparsities to simultaneously perform variable\nselection and learn and overlapping group structure among the tasks. The\nresulting bi-convex objective function is minimized by alternating optimization\nwhere sub-problems are solved using alternating direction method of multipliers\nand accelerated proximal gradient descent. Moreover, we provide the performance\nbound of the proposed method. The effectiveness of the proposed method is\nvalidated for both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:15:32 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Jeong", "Jun-Yong", ""], ["Jun", "Chi-Hyuck", ""]]}, {"id": "1802.04684", "submitter": "Mehmet Eren Ahsen", "authors": "Mehmet Eren Ahsen, Robert Vogel, Gustavo Stolovitzky", "title": "Unsupervised Evaluation and Weighted Aggregation of Ranked Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms that aggregate predictions from an ensemble of diverse\nbase classifiers consistently outperform individual methods. Many of these\nstrategies have been developed in a supervised setting, where the accuracy of\neach base classifier can be empirically measured and this information is\nincorporated in the training process. However, the reliance on labeled data\nprecludes the application of ensemble methods to many real world problems where\nlabeled data has not been curated. To this end we developed a new theoretical\nframework for binary classification, the Strategy for Unsupervised Multiple\nMethod Aggregation (SUMMA), to estimate the performances of base classifiers\nand an optimal strategy for ensemble learning from unlabeled data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:28:20 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Ahsen", "Mehmet Eren", ""], ["Vogel", "Robert", ""], ["Stolovitzky", "Gustavo", ""]]}, {"id": "1802.04687", "submitter": "Thomas Kipf", "authors": "Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel", "title": "Neural Relational Inference for Interacting Systems", "comments": "ICML (2018). Code available under https://github.com/ethanfetaya/NRI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting systems are prevalent in nature, from dynamical systems in\nphysics to complex societal dynamics. The interplay of components can give rise\nto complex behavior, which can often be explained using a simple model of the\nsystem's constituent parts. In this work, we introduce the neural relational\ninference (NRI) model: an unsupervised model that learns to infer interactions\nwhile simultaneously learning the dynamics purely from observational data. Our\nmodel takes the form of a variational auto-encoder, in which the latent code\nrepresents the underlying interaction graph and the reconstruction is based on\ngraph neural networks. In experiments on simulated physical systems, we show\nthat our NRI model can accurately recover ground-truth interactions in an\nunsupervised manner. We further demonstrate that we can find an interpretable\nstructure and predict complex dynamics in real motion capture and sports\ntracking data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:35:11 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 14:02:33 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Kipf", "Thomas", ""], ["Fetaya", "Ethan", ""], ["Wang", "Kuan-Chieh", ""], ["Welling", "Max", ""], ["Zemel", "Richard", ""]]}, {"id": "1802.04697", "submitter": "Arthur Guez", "authors": "Arthur Guez, Th\\'eophane Weber, Ioannis Antonoglou, Karen Simonyan,\n  Oriol Vinyals, Daan Wierstra, R\\'emi Munos, David Silver", "title": "Learning to Search with MCTSnets", "comments": "ICML 2018 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning problems are among the most important and well-studied problems in\nartificial intelligence. They are most typically solved by tree search\nalgorithms that simulate ahead into the future, evaluate future states, and\nback-up those evaluations to the root of a search tree. Among these algorithms,\nMonte-Carlo tree search (MCTS) is one of the most general, powerful and widely\nused. A typical implementation of MCTS uses cleverly designed rules, optimized\nto the particular characteristics of the domain. These rules control where the\nsimulation traverses, what to evaluate in the states that are reached, and how\nto back-up those evaluations. In this paper we instead learn where, what and\nhow to search. Our architecture, which we call an MCTSnet, incorporates\nsimulation-based search inside a neural network, by expanding, evaluating and\nbacking-up a vector embedding. The parameters of the network are trained\nend-to-end using gradient-based optimisation. When applied to small searches in\nthe well known planning problem Sokoban, the learned search algorithm\nsignificantly outperformed MCTS baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:10:10 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 14:16:12 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Guez", "Arthur", ""], ["Weber", "Th\u00e9ophane", ""], ["Antonoglou", "Ioannis", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""], ["Wierstra", "Daan", ""], ["Munos", "R\u00e9mi", ""], ["Silver", "David", ""]]}, {"id": "1802.04712", "submitter": "Jakub Tomczak Ph.D.", "authors": "Maximilian Ilse and Jakub M. Tomczak and Max Welling", "title": "Attention-based Deep Multiple Instance Learning", "comments": "ICML 2018 paper, code source:\n  https://github.com/AMLab-Amsterdam/AttentionDeepMIL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a variation of supervised learning where\na single class label is assigned to a bag of instances. In this paper, we state\nthe MIL problem as learning the Bernoulli distribution of the bag label where\nthe bag label probability is fully parameterized by neural networks.\nFurthermore, we propose a neural network-based permutation-invariant\naggregation operator that corresponds to the attention mechanism. Notably, an\napplication of the proposed attention-based operator provides insight into the\ncontribution of each instance to the bag label. We show empirically that our\napproach achieves comparable performance to the best MIL methods on benchmark\nMIL datasets and it outperforms other methods on a MNIST-based MIL dataset and\ntwo real-life histopathology datasets without sacrificing interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:27:19 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 14:53:24 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 13:31:48 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 13:33:03 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ilse", "Maximilian", ""], ["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1802.04715", "submitter": "Zal\\'an Borsos", "authors": "Zal\\'an Borsos, Andreas Krause, Kfir Y. Levy", "title": "Online Variance Reduction for Stochastic Optimization", "comments": "COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern stochastic optimization methods often rely on uniform sampling which\nis agnostic to the underlying characteristics of the data. This might degrade\nthe convergence by yielding estimates that suffer from a high variance. A\npossible remedy is to employ non-uniform importance sampling techniques, which\ntake the structure of the dataset into account. In this work, we investigate a\nrecently proposed setting which poses variance reduction as an online\noptimization problem with bandit feedback. We devise a novel and efficient\nalgorithm for this setting that finds a sequence of importance sampling\ndistributions competitive with the best fixed distribution in hindsight, the\nfirst result of this kind. While we present our method for sampling datapoints,\nit naturally extends to selecting coordinates or even blocks of thereof.\nEmpirical validations underline the benefits of our method in several settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:28:45 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 09:54:15 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 13:54:58 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Borsos", "Zal\u00e1n", ""], ["Krause", "Andreas", ""], ["Levy", "Kfir Y.", ""]]}, {"id": "1802.04725", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Xu Chen and Lawrence Carin", "title": "Superposition-Assisted Stochastic Optimization for Hawkes Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the learning of multi-agent Hawkes processes, a model containing\nmultiple Hawkes processes with shared endogenous impact functions and different\nexogenous intensities. In the framework of stochastic maximum likelihood\nestimation, we explore the associated risk bound. Further, we consider the\nsuperposition of Hawkes processes within the model, and demonstrate that under\ncertain conditions such an operation is beneficial for tightening the risk\nbound. Accordingly, we propose a stochastic optimization algorithm assisted\nwith a diversity-driven superposition strategy, achieving better learning\nresults with improved convergence properties. The effectiveness of the proposed\nmethod is verified on synthetic data, and its potential to solve the cold-start\nproblem of sequential recommendation systems is demonstrated on real-world\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:44:40 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 16:29:50 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Xu", "Hongteng", ""], ["Chen", "Xu", ""], ["Carin", "Lawrence", ""]]}, {"id": "1802.04734", "submitter": "Yvonne Anne Pignolet", "authors": "Qin Wang, Sandro Schoenborn, Yvonne-Anne Pignolet, Theo Widmer,\n  Carsten Franke", "title": "Substation Signal Matching with a Bagged Token Classifier", "comments": "To be presented at the 31st International Conference on Industrial,\n  Engineering & Other Applications of Applied Intelligent Systems (IEA/AIE)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, engineers at substation service providers match customer data with\nthe corresponding internally used signal names manually. This paper proposes a\nmachine learning method to automate this process based on substation signal\nmapping data from a repository of executed projects. To this end, a bagged\ntoken classifier is proposed, letting words (tokens) in the customer signal\nname vote for provider signal names. In our evaluation, the proposed method\nexhibits better performance in terms of both accuracy and efficiency over\nstandard classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:57:31 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wang", "Qin", ""], ["Schoenborn", "Sandro", ""], ["Pignolet", "Yvonne-Anne", ""], ["Widmer", "Theo", ""], ["Franke", "Carsten", ""]]}, {"id": "1802.04742", "submitter": "Thomas Vandal", "authors": "Thomas Vandal, Evan Kodra, Jennifer Dy, Sangram Ganguly, Ramakrishna\n  Nemani, Auroop R. Ganguly", "title": "Quantifying Uncertainty in Discrete-Continuous and Skewed Data with\n  Bayesian Deep Learning", "comments": "10 Pages", "journal-ref": "The 24th ACM SIGKDD International Conference on Knowledge\n  Discovery & Data Mining, August 19--23, 2018, London, United Kingdom", "doi": "10.1145/3219819.3219996", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) methods have been transforming computer vision with\ninnovative adaptations to other domains including climate change. For DL to\npervade Science and Engineering (S&E) applications where risk management is a\ncore component, well-characterized uncertainty estimates must accompany\npredictions. However, S&E observations and model-simulations often follow\nheavily skewed distributions and are not well modeled with DL approaches, since\nthey usually optimize a Gaussian, or Euclidean, likelihood loss. Recent\ndevelopments in Bayesian Deep Learning (BDL), which attempts to capture\nuncertainties from noisy observations, aleatoric, and from unknown model\nparameters, epistemic, provide us a foundation. Here we present a\ndiscrete-continuous BDL model with Gaussian and lognormal likelihoods for\nuncertainty quantification (UQ). We demonstrate the approach by developing UQ\nestimates on `DeepSD', a super-resolution based DL model for Statistical\nDownscaling (SD) in climate applied to precipitation, which follows an\nextremely skewed distribution. We find that the discrete-continuous models\noutperform a basic Gaussian distribution in terms of predictive accuracy and\nuncertainty calibration. Furthermore, we find that the lognormal distribution,\nwhich can handle skewed distributions, produces quality uncertainty estimates\nat the extremes. Such results may be important across S&E, as well as other\ndomains such as finance and economics, where extremes are often of significant\ninterest. Furthermore, to our knowledge, this is the first UQ model in SD where\nboth aleatoric and epistemic uncertainties are characterized.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:07:13 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 16:35:23 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Vandal", "Thomas", ""], ["Kodra", "Evan", ""], ["Dy", "Jennifer", ""], ["Ganguly", "Sangram", ""], ["Nemani", "Ramakrishna", ""], ["Ganguly", "Auroop R.", ""]]}, {"id": "1802.04765", "submitter": "Glen Berseth", "authors": "Glen Berseth, Cheng Xie, Paul Cernek, Michiel Van de Panne", "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled\n  Motion Control", "comments": "15 pages, Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has demonstrated increasing capabilities for\ncontinuous control problems, including agents that can move with skill and\nagility through their environment. An open problem in this setting is that of\ndeveloping good strategies for integrating or merging policies for multiple\nskills, where each individual skill is a specialist in a specific skill and its\nassociated state distribution. We extend policy distillation methods to the\ncontinuous action setting and leverage this technique to combine expert\npolicies, as evaluated in the domain of simulated bipedal locomotion across\ndifferent classes of terrain. We also introduce an input injection method for\naugmenting an existing policy network to exploit new input features. Lastly,\nour method uses transfer learning to assist in the efficient acquisition of new\nskills. The combination of these methods allows a policy to be incrementally\naugmented with new skills. We compare our progressive learning and integration\nvia distillation (PLAID) method against three alternative baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:57:21 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Berseth", "Glen", ""], ["Xie", "Cheng", ""], ["Cernek", "Paul", ""], ["Van de Panne", "Michiel", ""]]}, {"id": "1802.04782", "submitter": "Henry Chai", "authors": "Henry Chai and Roman Garnett", "title": "Improving Quadrature for Constrained Integrands", "comments": "13 pages, 4 figures, 4 tables. Accepted by AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved Bayesian framework for performing inference of affine\ntransformations of constrained functions. We focus on quadrature with\nnonnegative functions, a common task in Bayesian inference. We consider\nconstraints on the range of the function of interest, such as nonnegativity or\nboundedness. Although our framework is general, we derive explicit\napproximation schemes for these constraints, and argue for the use of a log\ntransformation for functions with high dynamic range such as likelihood\nsurfaces. We propose a novel method for optimizing hyperparameters in this\nframework: we optimize the marginal likelihood in the original space, as\nopposed to in the transformed space. The result is a model that better explains\nthe actual data. Experiments on synthetic and real-world data demonstrate our\nframework achieves superior estimates using less wall-clock time than existing\nBayesian quadrature procedures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:30:52 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 19:12:42 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 20:29:27 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 16:25:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Chai", "Henry", ""], ["Garnett", "Roman", ""]]}, {"id": "1802.04784", "submitter": "Zoltan Szabo", "authors": "Matthieu Lerasle, Zoltan Szabo, Timothee Mathieu, Guillaume Lecue", "title": "MONK -- Outlier-Robust Mean Embedding Estimation by Median-of-Means", "comments": "ICML-2019: camera-ready paper. Code:\n  https://bitbucket.org/TimotheeMathieu/monk-mmd", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.FA math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean embeddings provide an extremely flexible and powerful tool in machine\nlearning and statistics to represent probability distributions and define a\nsemi-metric (MMD, maximum mean discrepancy; also called N-distance or energy\ndistance), with numerous successful applications. The representation is\nconstructed as the expectation of the feature map defined by a kernel. As a\nmean, its classical empirical estimator, however, can be arbitrary severely\naffected even by a single outlier in case of unbounded features. To the best of\nour knowledge, unfortunately even the consistency of the existing few\ntechniques trying to alleviate this serious sensitivity bottleneck is unknown.\nIn this paper, we show how the recently emerged principle of median-of-means\ncan be used to design estimators for kernel mean embedding and MMD with\nexcessive resistance properties to outliers, and optimal sub-Gaussian deviation\nbounds under mild assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:35:25 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 12:41:04 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 16:40:19 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 20:12:22 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Lerasle", "Matthieu", ""], ["Szabo", "Zoltan", ""], ["Mathieu", "Timothee", ""], ["Lecue", "Guillaume", ""]]}, {"id": "1802.04791", "submitter": "Quanquan Gu", "authors": "Difan Zou and Pan Xu and Quanquan Gu", "title": "Stochastic Variance-Reduced Hamilton Monte Carlo Methods", "comments": "23 pages, 3 figures, 4 tables. In ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling\nfrom a smooth and strongly log-concave distribution. At the core of our\nproposed method is a variance reduction technique inspired by the recent\nadvance in stochastic optimization. We show that, to achieve $\\epsilon$\naccuracy in 2-Wasserstein distance, our algorithm achieves $\\tilde\nO(n+\\kappa^{2}d^{1/2}/\\epsilon+\\kappa^{4/3}d^{1/3}n^{2/3}/\\epsilon^{2/3})$\ngradient complexity (i.e., number of component gradient evaluations), which\noutperforms the state-of-the-art HMC and stochastic gradient HMC methods in a\nwide regime. We also extend our algorithm for sampling from smooth and general\nlog-concave distributions, and prove the corresponding gradient complexity as\nwell. Experiments on both synthetic and real data demonstrate the superior\nperformance of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:54:54 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 00:40:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zou", "Difan", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1802.04826", "submitter": "Pierre-Alexandre Mattei", "authors": "Pierre-Alexandre Mattei and Jes Frellsen", "title": "Leveraging the Exact Likelihood of Deep Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep latent variable models (DLVMs) combine the approximation abilities of\ndeep neural networks and the statistical foundations of generative models.\nVariational methods are commonly used for inference; however, the exact\nlikelihood of these models has been largely overlooked. The purpose of this\nwork is to study the general properties of this quantity and to show how they\ncan be leveraged in practice. We focus on important inferential problems that\nrely on the likelihood: estimation and missing data imputation. First, we\ninvestigate maximum likelihood estimation for DLVMs: in particular, we show\nthat most unconstrained models used for continuous data have an unbounded\nlikelihood function. This problematic behaviour is demonstrated to be a source\nof mode collapse. We also show how to ensure the existence of maximum\nlikelihood estimates, and draw useful connections with nonparametric mixture\nmodels. Finally, we describe an algorithm for missing data imputation using the\nexact conditional likelihood of a deep latent variable model. On several data\nsets, our algorithm consistently and significantly outperforms the usual\nimputation scheme used for DLVMs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:27:38 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:44:46 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 14:57:29 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 13:44:24 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Mattei", "Pierre-Alexandre", ""], ["Frellsen", "Jes", ""]]}, {"id": "1802.04838", "submitter": "Ben Mark", "authors": "Benjamin Mark, Garvesh Raskutti, Rebecca Willett", "title": "Network Estimation from Point Process Data", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observing a collection of discrete events within a network that\nreflect how network nodes influence one another. Such data are common in spike\ntrains recorded from biological neural networks, interactions within a social\nnetwork, and a variety of other settings. Data of this form may be modeled as\nself-exciting point processes, in which the likelihood of future events depends\non the past events. This paper addresses the problem of estimating\nself-excitation parameters and inferring the underlying functional network\nstructure from self-exciting point process data. Past work in this area was\nlimited by strong assumptions which are addressed by the novel approach here.\nSpecifically, in this paper we (1) incorporate saturation in a point process\nmodel which both ensures stability and models non-linear thresholding effects;\n(2) impose general low-dimensional structural assumptions that include\nsparsity, group sparsity and low-rankness that allows bounds to be developed in\nthe high-dimensional setting; and (3) incorporate long-range memory effects\nthrough moving average and higher-order auto-regressive components. Using our\ngeneral framework, we provide a number of novel theoretical guarantees for\nhigh-dimensional self-exciting point processes that reflect the role played by\nthe underlying network structure and long-term memory. We also provide\nsimulations and real data examples to support our methodology and main results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:06:07 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Mark", "Benjamin", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""]]}, {"id": "1802.04846", "submitter": "Hannes Nickisch", "authors": "Hannes Nickisch, Arno Solin, Alexander Grigorievskiy", "title": "State Space Gaussian Processes with Non-Gaussian Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comprehensive overview and tooling for GP modeling with\nnon-Gaussian likelihoods using state space methods. The state space formulation\nallows for solving one-dimensional GP models in $\\mathcal{O}(n)$ time and\nmemory complexity. While existing literature has focused on the connection\nbetween GP regression and state space methods, the computational primitives\nallowing for inference using general likelihoods in combination with the\nLaplace approximation (LA), variational Bayes (VB), and assumed density\nfiltering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has\nbeen largely overlooked. We present means of combining the efficient\n$\\mathcal{O}(n)$ state space methodology with existing inference methods. We\nextend existing methods, and provide unifying code implementing all approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:34:44 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 15:54:02 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 20:47:20 GMT"}, {"version": "v4", "created": "Tue, 3 Jul 2018 07:55:42 GMT"}, {"version": "v5", "created": "Thu, 5 Jul 2018 06:59:41 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Nickisch", "Hannes", ""], ["Solin", "Arno", ""], ["Grigorievskiy", "Alexander", ""]]}, {"id": "1802.04849", "submitter": "Michael Gallaugher Ph.D.", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Clustering and Semi-Supervised Classification for Clickstream Data via\n  Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have been used for unsupervised learning for some time,\nand their use within the semi-supervised paradigm is becoming more commonplace.\nClickstream data is one of the various emerging data types that demands\nparticular attention because there is a notable paucity of statistical learning\napproaches currently available. A mixture of first-order continuous time Markov\nmodels is introduced for unsupervised and semi-supervised learning of\nclickstream data. This approach assumes continuous time, which distinguishes it\nfrom existing mixture model-based approaches; practically, this allows account\nto be taken of the amount of time each user spends on each webpage. The\napproach is evaluated, and compared to the discrete time approach, using\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:43:04 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 08:28:57 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1802.04852", "submitter": "Mateusz Juda", "authors": "Bartosz Zielinski, Michal Lipinski, Mateusz Juda, Matthias\n  Zeppelzauer, Pawel Dlotko", "title": "Persistence Codebooks for Topological Data Analysis", "comments": "minor update, remove heading", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology (PH) is a rigorous mathematical theory that provides a\nrobust descriptor of data in the form of persistence diagrams (PDs) which are\n2D multisets of points. Their variable size makes them, however, difficult to\ncombine with typical machine learning workflows. In this paper we introduce\npersistence codebooks, a novel expressive and discriminative fixed-size\nvectorized representation of PDs. To this end, we adapt bag-of-words (BoW),\nvectors of locally aggregated descriptors (VLAD) and Fischer vectors (FV) for\nthe quantization of PDs. Persistence codebooks represent PDs in a convenient\nway for machine learning and statistical analysis and have a number of\nfavorable practical and theoretical properties including 1-Wasserstein\nstability. We evaluate the presented representations on several heterogeneous\ndatasets and show their (high) discriminative power. Our approach achieves\nstate-of-the-art performance and beyond in much less time than alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:49:01 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 11:38:01 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 15:16:14 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 09:37:59 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Zielinski", "Bartosz", ""], ["Lipinski", "Michal", ""], ["Juda", "Mateusz", ""], ["Zeppelzauer", "Matthias", ""], ["Dlotko", "Pawel", ""]]}, {"id": "1802.04865", "submitter": "Terrance DeVries", "authors": "Terrance DeVries and Graham W. Taylor", "title": "Learning Confidence for Out-of-Distribution Detection in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are very powerful predictive models, but they are\noften incapable of recognizing when their predictions may be wrong. Closely\nrelated to this is the task of out-of-distribution detection, where a network\nmust determine whether or not an input is outside of the set on which it is\nexpected to safely perform. To jointly address these issues, we propose a\nmethod of learning confidence estimates for neural networks that is simple to\nimplement and produces intuitively interpretable outputs. We demonstrate that\non the task of out-of-distribution detection, our technique surpasses recently\nproposed techniques which construct confidence based on the network's output\ndistribution, without requiring any additional labels or access to\nout-of-distribution examples. Additionally, we address the problem of\ncalibrating out-of-distribution detectors, where we demonstrate that\nmisclassified in-distribution examples can be used as a proxy for\nout-of-distribution examples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 21:31:36 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["DeVries", "Terrance", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1802.04868", "submitter": "Seyed Mehran Kazemi", "authors": "Seyed Mehran Kazemi, David Poole", "title": "SimplE Embedding for Link Prediction in Knowledge Graphs", "comments": "Accepted for publication at conference on neural information\n  processing systems (NIPS 2018). 12 pages, 2 figure, 2 tables, 5 propositions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge graphs contain knowledge about the world and provide a structured\nrepresentation of this knowledge. Current knowledge graphs contain only a small\nsubset of what is true in the world. Link prediction approaches aim at\npredicting new links for a knowledge graph given the existing links among the\nentities. Tensor factorization approaches have proved promising for such link\nprediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is\namong the first tensor factorization approaches. CP generally performs poorly\nfor link prediction as it learns two independent embedding vectors for each\nentity, whereas they are really tied. We present a simple enhancement of CP\n(which we call SimplE) to allow the two embeddings of each entity to be learned\ndependently. The complexity of SimplE grows linearly with the size of\nembeddings. The embeddings learned through SimplE are interpretable, and\ncertain types of background knowledge can be incorporated into these embeddings\nthrough weight tying. We prove SimplE is fully expressive and derive a bound on\nthe size of its embeddings for full expressivity. We show empirically that,\ndespite its simplicity, SimplE outperforms several state-of-the-art tensor\nfactorization techniques. SimplE's code is available on GitHub at\nhttps://github.com/Mehran-k/SimplE.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 21:41:32 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 03:38:08 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1802.04874", "submitter": "Ian Fischer", "authors": "Alexander A. Alemi, Ian Fischer", "title": "GILBO: One Metric to Measure Them All", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, tractable lower bound on the mutual information\ncontained in the joint generative density of any latent variable generative\nmodel: the GILBO (Generative Information Lower BOund). It offers a\ndata-independent measure of the complexity of the learned latent variable\ndescription, giving the log of the effective description length. It is\nwell-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs\neach trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and\ndiscuss the results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 21:58:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 00:26:50 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 18:51:19 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Alemi", "Alexander A.", ""], ["Fischer", "Ian", ""]]}, {"id": "1802.04876", "submitter": "Weijie J. Su", "authors": "Weijie J. Su and Yuancheng Zhu", "title": "Uncertainty Quantification for Online Learning and Stochastic\n  Approximation via Hierarchical Incremental Gradient Descent", "comments": "Changed the title and polished writing. For more details, please\n  visit the HiGrad webpage http://stat.wharton.upenn.edu/~suw/higrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is an immensely popular approach for online\nlearning in settings where data arrives in a stream or data sizes are very\nlarge. However, despite an ever- increasing volume of work on SGD, much less is\nknown about the statistical inferential properties of SGD-based predictions.\nTaking a fully inferential viewpoint, this paper introduces a novel procedure\ntermed HiGrad to conduct statistical inference for online learning, without\nincurring additional computational cost compared with SGD. The HiGrad procedure\nbegins by performing SGD updates for a while and then splits the single thread\ninto several threads, and this procedure hierarchically operates in this\nfashion along each thread. With predictions provided by multiple threads in\nplace, a t-based confidence interval is constructed by decorrelating\npredictions using covariance structures given by a Donsker-style extension of\nthe Ruppert--Polyak averaging scheme, which is a technical contribution of\nindependent interest. Under certain regularity conditions, the HiGrad\nconfidence interval is shown to attain asymptotically exact coverage\nprobability. Finally, the performance of HiGrad is evaluated through extensive\nsimulation studies and a real data example. An R package higrad has been\ndeveloped to implement the method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:15:10 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 15:16:31 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Su", "Weijie J.", ""], ["Zhu", "Yuancheng", ""]]}, {"id": "1802.04889", "submitter": "Yunhui Long", "authors": "Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang,\n  Haixu Tang, Carl A. Gunter, and Kai Chen", "title": "Understanding Membership Inferences on Well-Generalized Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membership Inference Attack (MIA) determines the presence of a record in a\nmachine learning model's training data by querying the model. Prior work has\nshown that the attack is feasible when the model is overfitted to its training\ndata or when the adversary controls the training algorithm. However, when the\nmodel is not overfitted and the adversary does not control the training\nalgorithm, the threat is not well understood. In this paper, we report a study\nthat discovers overfitting to be a sufficient but not a necessary condition for\nan MIA to succeed. More specifically, we demonstrate that even a\nwell-generalized model contains vulnerable instances subject to a new\ngeneralized MIA (GMIA). In GMIA, we use novel techniques for selecting\nvulnerable instances and detecting their subtle influences ignored by\noverfitting metrics. Specifically, we successfully identify individual records\nwith high precision in real-world datasets by querying black-box machine\nlearning models. Further we show that a vulnerable record can even be\nindirectly attacked by querying other related records and existing\ngeneralization techniques are found to be less effective in protecting the\nvulnerable instances. Our findings sharpen the understanding of the fundamental\ncause of the problem: the unique influences the training instance may have on\nthe model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 23:05:05 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Long", "Yunhui", ""], ["Bindschaedler", "Vincent", ""], ["Wang", "Lei", ""], ["Bu", "Diyue", ""], ["Wang", "Xiaofeng", ""], ["Tang", "Haixu", ""], ["Gunter", "Carl A.", ""], ["Chen", "Kai", ""]]}, {"id": "1802.04893", "submitter": "Andrei Atanov", "authors": "Andrei Atanov, Arsenii Ashukha, Dmitry Molchanov, Kirill Neklyudov,\n  Dmitry Vetrov", "title": "Uncertainty Estimation via Stochastic Batch Normalization", "comments": "Under review as a workshop paper at ICLR 2018", "journal-ref": "Workshop track - ICLR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate Batch Normalization technique and propose its\nprobabilistic interpretation. We propose a probabilistic model and show that\nBatch Normalization maximazes the lower bound of its marginalized\nlog-likelihood. Then, according to the new probabilistic model, we design an\nalgorithm which acts consistently during train and test. However, inference\nbecomes computationally inefficient. To reduce memory and computational cost,\nwe propose Stochastic Batch Normalization -- an efficient approximation of\nproper inference procedure. This method provides us with a scalable uncertainty\nestimation technique. We demonstrate the performance of Stochastic Batch\nNormalization on popular architectures (including deep convolutional\narchitectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 23:22:16 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 20:21:48 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Atanov", "Andrei", ""], ["Ashukha", "Arsenii", ""], ["Molchanov", "Dmitry", ""], ["Neklyudov", "Kirill", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1802.04907", "submitter": "Nezihe Merve G\\\"urel", "authors": "Nezihe Merve G\\\"urel, Kaan Kara, Alen Stojanov, Tyler Smith, Thomas\n  Lemmin, Dan Alistarh, Markus P\\\"uschel, Ce Zhang", "title": "Compressive Sensing Using Iterative Hard Thresholding with Low Precision\n  Data Representation: Theory and Applications", "comments": "19 pages, 5 figures, 1 table, in IEEE Transactions on Signal\n  Processing Vol. 68, No. 7, pp. 4268-4282, 2020", "journal-ref": null, "doi": "10.1109/TSP.2020.3010355", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific instruments produce vast amounts of data, which can\noverwhelm the processing ability of computer systems. Lossy compression of data\nis an intriguing solution, but comes with its own drawbacks, such as potential\nsignal loss, and the need for careful optimization of the compression ratio. In\nthis work, we focus on a setting where this problem is especially acute:\ncompressive sensing frameworks for interferometry and medical imaging. We ask\nthe following question: can the precision of the data representation be lowered\nfor all inputs, with recovery guarantees and practical performance? Our first\ncontribution is a theoretical analysis of the normalized Iterative Hard\nThresholding (IHT) algorithm when all input data, meaning both the measurement\nmatrix and the observation vector are quantized aggressively. We present a\nvariant of low precision normalized {IHT} that, under mild conditions, can\nstill provide recovery guarantees. The second contribution is the application\nof our quantization framework to radio astronomy and magnetic resonance\nimaging. We show that lowering the precision of the data can significantly\naccelerate image recovery. We evaluate our approach on telescope data and\nsamples of brain images using CPU and FPGA implementations achieving up to a 9x\nspeed-up with negligible loss of recovery quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 00:41:30 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 16:48:34 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 09:54:44 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 09:49:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["G\u00fcrel", "Nezihe Merve", ""], ["Kara", "Kaan", ""], ["Stojanov", "Alen", ""], ["Smith", "Tyler", ""], ["Lemmin", "Thomas", ""], ["Alistarh", "Dan", ""], ["P\u00fcschel", "Markus", ""], ["Zhang", "Ce", ""]]}, {"id": "1802.04908", "submitter": "Brian Trippe", "authors": "Brian L Trippe, Richard E Turner", "title": "Conditional Density Estimation with Bayesian Normalising Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling complex conditional distributions is critical in a variety of\nsettings. Despite a long tradition of research into conditional density\nestimation, current methods employ either simple parametric forms or are\ndifficult to learn in practice. This paper employs normalising flows as a\nflexible likelihood model and presents an efficient method for fitting them to\ncomplex densities. These estimators must trade-off between modeling\ndistributional complexity, functional complexity and heteroscedasticity without\noverfitting. We recognize these trade-offs as modeling decisions and develop a\nBayesian framework for placing priors over these conditional density estimators\nusing variational Bayesian neural networks. We evaluate this method on several\nsmall benchmark regression datasets, on some of which it obtains state of the\nart performance. Finally, we apply the method to two spatial density modeling\ntasks with over 1 million datapoints using the New York City yellow taxi\ndataset and the Chicago crime dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 00:44:47 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Trippe", "Brian L", ""], ["Turner", "Richard E", ""]]}, {"id": "1802.04911", "submitter": "Richard Zhang", "authors": "Richard Y. Zhang, Salar Fattahi, Somayeh Sojoudi", "title": "Large-Scale Sparse Inverse Covariance Estimation via Thresholding and\n  Max-Det Matrix Completion", "comments": "35-th International Conference on Machine Learning (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse inverse covariance estimation problem is commonly solved using an\n$\\ell_{1}$-regularized Gaussian maximum likelihood estimator known as\n\"graphical lasso\", but its computational cost becomes prohibitive for large\ndata sets. A recent line of results showed--under mild assumptions--that the\ngraphical lasso estimator can be retrieved by soft-thresholding the sample\ncovariance matrix and solving a maximum determinant matrix completion (MDMC)\nproblem. This paper proves an extension of this result, and describes a\nNewton-CG algorithm to efficiently solve the MDMC problem. Assuming that the\nthresholded sample covariance matrix is sparse with a sparse Cholesky\nfactorization, we prove that the algorithm converges to an $\\epsilon$-accurate\nsolution in $O(n\\log(1/\\epsilon))$ time and $O(n)$ memory. The algorithm is\nhighly efficient in practice: we solve the associated MDMC problems with as\nmany as 200,000 variables to 7-9 digits of accuracy in less than an hour on a\nstandard laptop computer running MATLAB.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:00:10 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 03:24:32 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 01:13:24 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhang", "Richard Y.", ""], ["Fattahi", "Salar", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1802.04918", "submitter": "Michael Lash", "authors": "Michael T. Lash and Qihang Lin and W. Nick Street", "title": "Prophit: Causal inverse classification for multiple continuously valued\n  treatment policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse classification uses an induced classifier as a queryable oracle to\nguide test instances towards a preferred posterior class label. The result\nproduced from the process is a set of instance-specific feature perturbations,\nor recommendations, that optimally improve the probability of the class label.\nIn this work, we adopt a causal approach to inverse classification, eliciting\ntreatment policies (i.e., feature perturbations) for models induced with causal\nproperties. In so doing, we solve a long-standing problem of eliciting\nmultiple, continuously valued treatment policies, using an updated framework\nand corresponding set of assumptions, which we term the inverse classification\npotential outcomes framework (ICPOF), along with a new measure, referred to as\nthe individual future estimated effects ($i$FEE). We also develop the\napproximate propensity score (APS), based on Gaussian processes, to weight\ntreatments, much like the inverse propensity score weighting used in past\nworks. We demonstrate the viability of our methods on student performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:33:01 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Lash", "Michael T.", ""], ["Lin", "Qihang", ""], ["Street", "W. Nick", ""]]}, {"id": "1802.04920", "submitter": "Arash Vahdat", "authors": "Arash Vahdat, William G. Macready, Zhengbing Bian, Amir Khoshaman,\n  Evgeny Andriyash", "title": "DVAE++: Discrete Variational Autoencoders with Overlapping\n  Transformations", "comments": "Published as a conference paper at International Conference on\n  Machine Learning (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of discrete latent variable models remains challenging because\npassing gradient information through discrete units is difficult. We propose a\nnew class of smoothing transformations based on a mixture of two overlapping\ndistributions, and show that the proposed transformation can be used for\ntraining binary latent models with either directed or undirected priors. We\nderive a new variational bound to efficiently train with Boltzmann machine\npriors. Using this bound, we develop DVAE++, a generative model with a global\ndiscrete prior and a hierarchy of convolutional continuous variables.\nExperiments on several benchmarks show that overlapping transformations\noutperform other recent continuous relaxations of discrete latent variables\nincluding Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and\ndiscrete variational autoencoders (Rolfe 2016).\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:39:05 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 23:29:08 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Vahdat", "Arash", ""], ["Macready", "William G.", ""], ["Bian", "Zhengbing", ""], ["Khoshaman", "Amir", ""], ["Andriyash", "Evgeny", ""]]}, {"id": "1802.04942", "submitter": "Ricky T. Q. Chen", "authors": "Ricky T. Q. Chen, Xuechen Li, Roger Grosse, David Duvenaud", "title": "Isolating Sources of Disentanglement in Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose the evidence lower bound to show the existence of a term\nmeasuring the total correlation between latent variables. We use this to\nmotivate our $\\beta$-TCVAE (Total Correlation Variational Autoencoder), a\nrefinement of the state-of-the-art $\\beta$-VAE objective for learning\ndisentangled representations, requiring no additional hyperparameters during\ntraining. We further propose a principled classifier-free measure of\ndisentanglement called the mutual information gap (MIG). We perform extensive\nquantitative and qualitative experiments, in both restricted and non-restricted\nsettings, and show a strong relation between total correlation and\ndisentanglement, when the latent variables model is trained using our\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 03:48:06 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 21:01:39 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 22:26:42 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2019 21:50:57 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2019 17:20:14 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Chen", "Ricky T. Q.", ""], ["Li", "Xuechen", ""], ["Grosse", "Roger", ""], ["Duvenaud", "David", ""]]}, {"id": "1802.04944", "submitter": "Chao Shang", "authors": "Chao Shang, Qinqing Liu, Ko-Shin Chen, Jiangwen Sun, Jin Lu, Jinfeng\n  Yi, Jinbo Bi", "title": "Edge Attention-based Multi-Relational Graph Convolutional Networks", "comments": "Haven't meet my expectations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) is generalization of convolutional neural\nnetwork (CNN) to work with arbitrarily structured graphs. A binary adjacency\nmatrix is commonly used in training a GCN. Recently, the attention mechanism\nallows the network to learn a dynamic and adaptive aggregation of the\nneighborhood. We propose a new GCN model on the graphs where edges are\ncharacterized in multiple views or precisely in terms of multiple\nrelationships. For instance, in chemical graph theory, compound structures are\noften represented by the hydrogen-depleted molecular graph where nodes\ncorrespond to atoms and edges correspond to chemical bonds. Multiple attributes\ncan be important to characterize chemical bonds, such as atom pair (the types\nof atoms that a bond connects), aromaticity, and whether a bond is in a ring.\nThe different attributes lead to different graph representations for the same\nmolecule. There is growing interests in both chemistry and machine learning\nfields to directly learn molecular properties of compounds from the molecular\ngraph, instead of from fingerprints predefined by chemists. The proposed GCN\nmodel, which we call edge attention-based multi-relational GCN (EAGCN), jointly\nlearns attention weights and node features in graph convolution. For each bond\nattribute, a real-valued attention matrix is used to replace the binary\nadjacency matrix. By designing a dictionary for the edge attention, and forming\nthe attention matrix of each molecule by looking up the dictionary, the EAGCN\nexploits correspondence between bonds in different molecules. The prediction of\ncompound properties is based on the aggregated node features, which is\nindependent of the varying molecule (graph) size. We demonstrate the efficacy\nof the EAGCN on multiple chemical datasets: Tox21, HIV, Freesolv, and\nLipophilicity, and interpret the resultant attention weights.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 03:52:58 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 14:28:06 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Shang", "Chao", ""], ["Liu", "Qinqing", ""], ["Chen", "Ko-Shin", ""], ["Sun", "Jiangwen", ""], ["Lu", "Jin", ""], ["Yi", "Jinfeng", ""], ["Bi", "Jinbo", ""]]}, {"id": "1802.04956", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep Ravikumar, Michael\n  Witbrock", "title": "D2KE: From Distance to Kernel and Embedding", "comments": "15 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many machine learning problem settings, particularly with structured\ninputs such as sequences or sets of objects, a distance measure between inputs\ncan be specified more naturally than a feature representation. However, most\nstandard machine models are designed for inputs with a vector feature\nrepresentation. In this work, we consider the estimation of a function\n$f:\\mathcal{X} \\rightarrow \\R$ based solely on a dissimilarity measure\n$d:\\mathcal{X}\\times\\mathcal{X} \\rightarrow \\R$ between inputs. In particular,\nwe propose a general framework to derive a family of \\emph{positive definite\nkernels} from a given dissimilarity measure, which subsumes the widely-used\n\\emph{representative-set method} as a special case, and relates to the\nwell-known \\emph{distance substitution kernel} in a limiting case. We show that\nfunctions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are\nLipschitz-continuous w.r.t. the given distance metric. We provide a tractable\nalgorithm to estimate a function from this RKHS, and show that it enjoys better\ngeneralizability than Nearest-Neighbor estimates. Our approach draws from the\nliterature of Random Features, but instead of deriving feature maps from an\nexisting kernel, we construct novel kernels from a random feature map, that we\nspecify given the distance measure. We conduct classification experiments with\nsuch disparate domains as strings, time series, and sets of vectors, where our\nproposed framework compares favorably to existing distance-based learning\nmethods such as $k$-nearest-neighbors, distance-substitution kernels,\npseudo-Euclidean embedding, and the representative-set method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 04:58:13 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 18:43:50 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 04:58:45 GMT"}, {"version": "v4", "created": "Fri, 25 May 2018 06:02:45 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Wu", "Lingfei", ""], ["Yen", "Ian En-Hsu", ""], ["Xu", "Fangli", ""], ["Ravikumar", "Pradeep", ""], ["Witbrock", "Michael", ""]]}, {"id": "1802.04960", "submitter": "Donniell Fishkind", "authors": "Jordan Yoder, Li Chen, Henry Pao, Eric Bridgeford, Keith Levin,\n  Donniell Fishkind, Carey Priebe, Vince Lyzinski", "title": "Vertex nomination: The canonical sampling and the extended spectral\n  nomination schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that one particular block in a stochastic block model is of interest,\nbut block labels are only observed for a few of the vertices in the network.\nUtilizing a graph realized from the model and the observed block labels, the\nvertex nomination task is to order the vertices with unobserved block labels\ninto a ranked nomination list with the goal of having an abundance of\ninteresting vertices near the top of the list. There are vertex nomination\nschemes in the literature, including the optimally precise canonical nomination\nscheme~$\\mathcal{L}^C$ and the consistent spectral partitioning nomination\nscheme~$\\mathcal{L}^P$. While the canonical nomination scheme $\\mathcal{L}^C$\nis provably optimally precise, it is computationally intractable, being\nimpractical to implement even on modestly sized graphs. With this in mind, an\napproximation of the canonical scheme---denoted the {\\it canonical sampling\nnomination scheme} $\\mathcal{L}^{CS}$---is introduced; $\\mathcal{L}^{CS}$\nrelies on a scalable, Markov chain Monte Carlo-based approximation of\n$\\mathcal{L}^{C}$, and converges to $\\mathcal{L}^{C}$ as the amount of sampling\ngoes to infinity. The spectral partitioning nomination scheme is also extended\nto the {\\it extended spectral partitioning nomination scheme},\n$\\mathcal{L}^{EP}$, which introduces a novel semisupervised clustering\nframework to improve upon the precision of $\\mathcal{L}^P$. Real-data and\nsimulation experiments are employed to illustrate the precision of these vertex\nnomination schemes, as well as their empirical computational complexity.\nKeywords: vertex nomination, Markov chain Monte Carlo, spectral partitioning,\nMclust MSC[2010]: 60J22, 65C40, 62H30, 62H25\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 05:20:03 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 01:21:19 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yoder", "Jordan", ""], ["Chen", "Li", ""], ["Pao", "Henry", ""], ["Bridgeford", "Eric", ""], ["Levin", "Keith", ""], ["Fishkind", "Donniell", ""], ["Priebe", "Carey", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1802.05027", "submitter": "Patrick Schwab", "authors": "Patrick Schwab, Emanuela Keller, Carl Muroi, David J. Mack, Christian\n  Str\\\"assle, Walter Karlen", "title": "Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical\n  Care", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:4518-4527, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients in the intensive care unit (ICU) require constant and close\nsupervision. To assist clinical staff in this task, hospitals use monitoring\nsystems that trigger audiovisual alarms if their algorithms indicate that a\npatient's condition may be worsening. However, current monitoring systems are\nextremely sensitive to movement artefacts and technical errors. As a result,\nthey typically trigger hundreds to thousands of false alarms per patient per\nday - drowning the important alarms in noise and adding to the exhaustion of\nclinical staff. In this setting, data is abundantly available, but obtaining\ntrustworthy annotations by experts is laborious and expensive. We frame the\nproblem of false alarm reduction from multivariate time series as a\nmachine-learning task and address it with a novel multitask network\narchitecture that utilises distant supervision through multiple related\nauxiliary tasks in order to reduce the number of expensive labels required for\ntraining. We show that our approach leads to significant improvements over\nseveral state-of-the-art baselines on real-world ICU data and provide new\ninsights on the importance of task selection and architectural choices in\ndistantly supervised multitask learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 10:35:08 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 23:31:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Schwab", "Patrick", ""], ["Keller", "Emanuela", ""], ["Muroi", "Carl", ""], ["Mack", "David J.", ""], ["Str\u00e4ssle", "Christian", ""], ["Karlen", "Walter", ""]]}, {"id": "1802.05035", "submitter": "J\\'er\\'emy Emile Cohen", "authors": "Jeremy E.Cohen, Rasmus Bro", "title": "Nonnegative PARAFAC2: a flexible coupling approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling variability in tensor decomposition methods is one of the challenges\nof source separation. One possible solution to account for variations from one\ndata set to another, jointly analysed, is to resort to the PARAFAC2 model.\nHowever, so far imposing constraints on the mode with variability has not been\npossible. In the following manuscript, a relaxation of the PARAFAC2 model is\nintroduced, that allows for imposing nonnegativity constraints on the varying\nmode. An algorithm to compute the proposed flexible PARAFAC2 model is derived,\nand its performance is studied on both synthetic and chemometrics data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 11:03:17 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Cohen", "Jeremy E.", ""], ["Bro", "Rasmus", ""]]}, {"id": "1802.05036", "submitter": "Xiao He", "authors": "Xiao He, Luis Moreira-Matias", "title": "Robust Continuous Co-Clustering", "comments": "Under reviewing process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering consists of grouping together samples giving their similar\nproperties. The problem of modeling simultaneously groups of samples and\nfeatures is known as Co-Clustering. This paper introduces ROCCO - a Robust\nContinuous Co-Clustering algorithm. ROCCO is a scalable, hyperparameter-free,\neasy and ready to use algorithm to address Co-Clustering problems in practice\nover massive cross-domain datasets. It operates by learning a graph-based\ntwo-sided representation of the input matrix. The underlying proposed\noptimization problem is non-convex, which assures a flexible pool of solutions.\nMoreover, we prove that it can be solved with a near linear time complexity on\nthe input size. An exhaustive large-scale experimental testbed conducted with\nboth synthetic and real-world datasets demonstrates ROCCO's properties in\npractice: (i) State-of-the-art performance in cross-domain real-world problems\nincluding Biomedicine and Text Mining; (ii) very low sensitivity to\nhyperparameter settings; (iii) robustness to noise and (iv) a linear empirical\nscalability in practice. These results highlight ROCCO as a powerful\ngeneral-purpose co-clustering algorithm for cross-domain practitioners,\nregardless of their technical background.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 11:07:16 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["He", "Xiao", ""], ["Moreira-Matias", "Luis", ""]]}, {"id": "1802.05046", "submitter": "Yishai Shimoni", "authors": "Yishai Shimoni, Chen Yanover, Ehud Karavani and Yaara Goldschmnidt", "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference\n  Analysis", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Causal inference analysis is the estimation of the effects of actions on\noutcomes. In the context of healthcare data this means estimating the outcome\nof counter-factual treatments (i.e. including treatments that were not\nobserved) on a patient's outcome. Compared to classic machine learning methods,\nevaluation and validation of causal inference analysis is more challenging\nbecause ground truth data of counter-factual outcome can never be obtained in\nany real-world scenario. Here, we present a comprehensive framework for\nbenchmarking algorithms that estimate causal effect. The framework includes\nunlabeled data for prediction, labeled data for validation, and code for\nautomatic evaluation of algorithm predictions using both established and novel\nmetrics. The data is based on real-world covariates, and the treatment\nassignments and outcomes are based on simulations, which provides the basis for\nvalidation. In this framework we address two questions: one of scaling, and the\nother of data-censoring. The framework is available as open source code at\nhttps://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 11:41:56 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 16:12:47 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Shimoni", "Yishai", ""], ["Yanover", "Chen", ""], ["Karavani", "Ehud", ""], ["Goldschmnidt", "Yaara", ""]]}, {"id": "1802.05074", "submitter": "Michal Rolinek", "authors": "Michal Rolinek, and Georg Martius", "title": "L4: Practical loss-based stepsize adaptation for deep learning", "comments": "NeurIPS, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stepsize adaptation scheme for stochastic gradient descent. It\noperates directly with the loss function and rescales the gradient in order to\nmake fixed predicted progress on the loss. We demonstrate its capabilities by\nconclusively improving the performance of Adam and Momentum optimizers. The\nenhanced optimizers with default hyperparameters consistently outperform their\nconstant stepsize counterparts, even the best ones, without a measurable\nincrease in computational cost. The performance is validated on multiple\narchitectures including dense nets, CNNs, ResNets, and the recurrent\nDifferential Neural Computer on classical datasets MNIST, fashion MNIST,\nCIFAR10 and others.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 12:47:37 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 19:31:13 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 18:51:03 GMT"}, {"version": "v4", "created": "Tue, 5 Jun 2018 10:09:37 GMT"}, {"version": "v5", "created": "Fri, 30 Nov 2018 14:42:32 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Rolinek", "Michal", ""], ["Martius", "Georg", ""]]}, {"id": "1802.05101", "submitter": "James Bagrow", "authors": "James P. Bagrow", "title": "Democratizing AI: Non-expert design of prediction tasks", "comments": "17 pages, 6 figures", "journal-ref": "PeerJ Computer Science, 6: e296, 2020", "doi": "10.7717/peerj-cs.296", "report-no": null, "categories": "cs.HC cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-experts have long made important contributions to machine learning (ML)\nby contributing training data, and recent work has shown that non-experts can\nalso help with feature engineering by suggesting novel predictive features.\nHowever, non-experts have only contributed features to prediction tasks already\nposed by experienced ML practitioners. Here we study how non-experts can design\nprediction tasks themselves, what types of tasks non-experts will design, and\nwhether predictive models can be automatically trained on data sourced for\ntheir tasks. We use a crowdsourcing platform where non-experts design\npredictive tasks that are then categorized and ranked by the crowd.\nCrowdsourced data are collected for top-ranked tasks and predictive models are\nthen trained and evaluated automatically using those data. We show that\nindividuals without ML experience can collectively construct useful datasets\nand that predictive models can be learned on these datasets, but challenges\nremain. The prediction tasks designed by non-experts covered a broad range of\ndomains, from politics and current events to health behavior, demographics, and\nmore. Proper instructions are crucial for non-experts, so we also conducted a\nrandomized trial to understand how different instructions may influence the\ntypes of prediction tasks being proposed. In general, understanding better how\nnon-experts can contribute to ML can further leverage advances in Automatic ML\nand has important implications as ML continues to drive workplace automation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:16:13 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 20:04:50 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bagrow", "James P.", ""]]}, {"id": "1802.05141", "submitter": "Kelvin Loh", "authors": "Kelvin Loh, Pejman Shoeibi Omrani, Ruud van der Linden", "title": "Deep Learning and Data Assimilation for Real-Time Production Prediction\n  in Natural Gas Wells", "comments": "Reduced length preprint submitted to IJCAI 2018 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI physics.flu-dyn physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of the gas production from mature gas wells, due to their\ncomplex end-of-life behavior, is challenging and crucial for operational\ndecision making. In this paper, we apply a modified deep LSTM model for\nprediction of the gas flow rates in mature gas wells, including the\nuncertainties in input parameters. Additionally, due to changes in the system\nin time and in order to increase the accuracy and robustness of the prediction,\nthe Ensemble Kalman Filter (EnKF) is used to update the flow rate predictions\nbased on new observations. The developed approach was tested on the data from\ntwo mature gas production wells in which their production is highly dynamic and\nsuffering from salt deposition. The results show that the flow predictions\nusing the EnKF updated model leads to better Jeffreys' J-divergences than the\npredictions without the EnKF model updating scheme.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 15:03:09 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 03:16:08 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Loh", "Kelvin", ""], ["Omrani", "Pejman Shoeibi", ""], ["van der Linden", "Ruud", ""]]}, {"id": "1802.05155", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Zhehui Chen, Enlu Zhou, Tuo Zhao", "title": "A Diffusion Approximation Theory of Momentum SGD in Nonconvex\n  Optimization", "comments": "arXiv admin note: text overlap with arXiv:1806.01660", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Momentum Stochastic Gradient Descent (MSGD) algorithm has been widely applied\nto many nonconvex optimization problems in machine learning, e.g., training\ndeep neural networks, variational Bayesian inference, and etc. Despite its\nempirical success, there is still a lack of theoretical understanding of\nconvergence properties of MSGD. To fill this gap, we propose to analyze the\nalgorithmic behavior of MSGD by diffusion approximations for nonconvex\noptimization problems with strict saddle points and isolated local optima. Our\nstudy shows that the momentum helps escape from saddle points, but hurts the\nconvergence within the neighborhood of optima (if without the step size\nannealing or momentum annealing). Our theoretical discovery partially\ncorroborates the empirical success of MSGD in training deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 15:26:59 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:17:15 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 21:29:06 GMT"}, {"version": "v4", "created": "Wed, 9 Oct 2019 03:13:03 GMT"}, {"version": "v5", "created": "Sat, 6 Mar 2021 20:32:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Tianyi", ""], ["Chen", "Zhehui", ""], ["Zhou", "Enlu", ""], ["Zhao", "Tuo", ""]]}, {"id": "1802.05187", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Shai Fine, Daniel Soudry", "title": "On the Blindspots of Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional network has been the state-of-the-art approach for a wide\nvariety of tasks over the last few years. Its successes have, in many cases,\nturned it into the default model in quite a few domains. In this work, we will\ndemonstrate that convolutional networks have limitations that may, in some\ncases, hinder it from learning properties of the data, which are easily\nrecognizable by traditional, less demanding, models. To this end, we present a\nseries of competitive analysis studies on image recognition and text analysis\ntasks, for which convolutional networks are known to provide state-of-the-art\nresults. In our studies, we inject a truth-revealing signal, indiscernible for\nthe network, thus hitting time and again the network's blind spots. The signal\ndoes not impair the network's existing performances, but it does provide an\nopportunity for a significant performance boost by models that can capture it.\nThe various forms of the carefully designed signals shed a light on the\nstrengths and weaknesses of convolutional network, which may provide insights\nfor both theoreticians that study the power of deep architectures, and for\npractitioners that consider applying convolutional networks to the task at\nhand.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:24:54 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 07:14:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Hoffer", "Elad", ""], ["Fine", "Shai", ""], ["Soudry", "Daniel", ""]]}, {"id": "1802.05193", "submitter": "Tao Liu", "authors": "Qi Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, Wujie Wen", "title": "Security Analysis and Enhancement of Model Compressed Deep Learning\n  Systems under Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN is presenting human-level performance for many complex intelligent tasks\nin real-world applications. However, it also introduces ever-increasing\nsecurity concerns. For example, the emerging adversarial attacks indicate that\neven very small and often imperceptible adversarial input perturbations can\neasily mislead the cognitive function of deep learning systems (DLS). Existing\nDNN adversarial studies are narrowly performed on the ideal software-level DNN\nmodels with a focus on single uncertainty factor, i.e. input perturbations,\nhowever, the impact of DNN model reshaping on adversarial attacks, which is\nintroduced by various hardware-favorable techniques such as hash-based weight\ncompression during modern DNN hardware implementation, has never been\ndiscussed. In this work, we for the first time investigate the multi-factor\nadversarial attack problem in practical model optimized deep learning systems\nby jointly considering the DNN model-reshaping (e.g. HashNet based deep\ncompression) and the input perturbations. We first augment adversarial example\ngenerating method dedicated to the compressed DNN models by incorporating the\nsoftware-based approaches and mathematical modeled DNN reshaping. We then\nconduct a comprehensive robustness and vulnerability analysis of deep\ncompressed DNN models under derived adversarial attacks. A defense technique\nnamed \"gradient inhibition\" is further developed to ease the generating of\nadversarial examples thus to effectively mitigate adversarial attacks towards\nboth software and hardware-oriented DNNs. Simulation results show that\n\"gradient inhibition\" can decrease the average success rate of adversarial\nattacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10)\nbenchmark with marginal accuracy degradation across various DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:31:35 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 18:02:11 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Liu", "Qi", ""], ["Liu", "Tao", ""], ["Liu", "Zihao", ""], ["Wang", "Yanzhi", ""], ["Jin", "Yier", ""], ["Wen", "Wujie", ""]]}, {"id": "1802.05196", "submitter": "John Seymour", "authors": "John Seymour and Philip Tully", "title": "Generative Models for Spear Phishing Posts on Social Media", "comments": "Presented at NIPS Workshop on Machine Deception (2017), 4 page limit\n  plus references, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, machine learning in computer security has prioritized defense:\nthink intrusion detection systems, malware classification, and botnet traffic\nidentification. Offense can benefit from data just as well. Social networks,\nwith their access to extensive personal data, bot-friendly APIs, colloquial\nsyntax, and prevalence of shortened links, are the perfect venues for spreading\nmachine-generated malicious content. We aim to discover what capabilities an\nadversary might utilize in such a domain. We present a long short-term memory\n(LSTM) neural network that learns to socially engineer specific users into\nclicking on deceptive URLs. The model is trained with word vector\nrepresentations of social media posts, and in order to make a click-through\nmore likely, it is dynamically seeded with topics extracted from the target's\ntimeline. We augment the model with clustering to triage high value targets\nbased on their level of social engagement, and measure success of the LSTM's\nphishing expedition using click-rates of IP-tracked links. We achieve state of\nthe art success rates, tripling those of historic email attack campaigns, and\noutperform humans manually performing the same task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:40:02 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Seymour", "John", ""], ["Tully", "Philip", ""]]}, {"id": "1802.05214", "submitter": "Ayan Chakrabarti", "authors": "Francesco Pittaluga, Sanjeev J. Koppal, Ayan Chakrabarti", "title": "Learning Privacy Preserving Encodings through Adversarial Training", "comments": "To appear in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to learn privacy-preserving encodings of images that\ninhibit inference of chosen private attributes, while allowing recovery of\nother desirable information. Rather than simply inhibiting a given fixed\npre-trained estimator, our goal is that an estimator be unable to learn to\naccurately predict the private attributes even with knowledge of the encoding\nfunction. We use a natural adversarial optimization-based formulation for\nthis---training the encoding function against a classifier for the private\nattribute, with both modeled as deep neural networks. The key contribution of\nour work is a stable and convergent optimization approach that is successful at\nlearning an encoder with our desired properties---maintaining utility while\ninhibiting inference of private attributes, not just within the adversarial\noptimization, but also by classifiers that are trained after the encoder is\nfixed. We adopt a rigorous experimental protocol for verification wherein\nclassifiers are trained exhaustively till saturation on the fixed encoders. We\nevaluate our approach on tasks of real-world complexity---learning\nhigh-dimensional encodings that inhibit detection of different scene\ncategories---and find that it yields encoders that are resilient at maintaining\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 17:04:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 21:33:11 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 19:24:57 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Pittaluga", "Francesco", ""], ["Koppal", "Sanjeev J.", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1802.05234", "submitter": "Jirong Yi", "authors": "Jirong Yi and Weiyu Xu", "title": "Necessary and Sufficient Null Space Condition for Nuclear Norm\n  Minimization in Low-Rank Matrix Recovery", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix recovery has found many applications in science and\nengineering such as machine learning, signal processing, collaborative\nfiltering, system identification, and Euclidean embedding. But the low-rank\nmatrix recovery problem is an NP hard problem and thus challenging. A commonly\nused heuristic approach is the nuclear norm minimization. In [12,14,15], the\nauthors established the necessary and sufficient null space conditions for\nnuclear norm minimization to recover every possible low-rank matrix with rank\nat most r (the strong null space condition). In addition, in [12], Oymak et al.\nestablished a null space condition for successful recovery of a given low-rank\nmatrix (the weak null space condition) using nuclear norm minimization, and\nderived the phase transition for the nuclear norm minimization. In this paper,\nwe show that the weak null space condition in [12] is only a sufficient\ncondition for successful matrix recovery using nuclear norm minimization, and\nis not a necessary condition as claimed in [12]. In this paper, we further give\na weak null space condition for low-rank matrix recovery, which is both\nnecessary and sufficient for the success of nuclear norm minimization. At the\ncore of our derivation are an inequality for characterizing the nuclear norms\nof block matrices, and the conditions for equality to hold in that inequality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 17:44:22 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Yi", "Jirong", ""], ["Xu", "Weiyu", ""]]}, {"id": "1802.05249", "submitter": "Matthew Staib", "authors": "Matthew Staib, Bryan Wilder, Stefanie Jegelka", "title": "Distributionally Robust Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions have applications throughout machine learning, but in\nmany settings, we do not have direct access to the underlying function $f$. We\nfocus on stochastic functions that are given as an expectation of functions\nover a distribution $P$. In practice, we often have only a limited set of\nsamples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by\nmaximizing the sum of $f_i$. However, this ignores generalization to the true\n(unknown) distribution. In this paper, we achieve better performance on the\nactual underlying function $f$ by directly optimizing a combination of bias and\nvariance. Algorithmically, we accomplish this by showing how to carry out\ndistributionally robust optimization (DRO) for submodular functions, providing\nefficient algorithms backed by theoretical guarantees which leverage several\nnovel contributions to the general theory of DRO. We also show compelling\nempirical evidence that DRO improves generalization to the unknown stochastic\nsubmodular function.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 18:16:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 02:06:25 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Staib", "Matthew", ""], ["Wilder", "Bryan", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1802.05251", "submitter": "Di Wang", "authors": "Di Wang and Minwei Ye and Jinhui Xu", "title": "Differentially Private Empirical Risk Minimization Revisited: Faster and\n  More General", "comments": "Thirty-first Annual Conference on Neural Information Processing\n  Systems (NIPS-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the differentially private Empirical Risk Minimization\n(ERM) problem in different settings. For smooth (strongly) convex loss function\nwith or without (non)-smooth regularization, we give algorithms that achieve\neither optimal or near optimal utility bounds with less gradient complexity\ncompared with previous work. For ERM with smooth convex loss function in\nhigh-dimensional ($p\\gg n$) setting, we give an algorithm which achieves the\nupper bound with less gradient complexity than previous ones. At last, we\ngeneralize the expected excess empirical risk from convex loss functions to\nnon-convex ones satisfying the Polyak-Lojasiewicz condition and give a tighter\nupper bound on the utility than the one in \\cite{ijcai2017-548}.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 18:20:48 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Wang", "Di", ""], ["Ye", "Minwei", ""], ["Xu", "Jinhui", ""]]}, {"id": "1802.05283", "submitter": "Abir De", "authors": "Bidisha Samanta, Abir De, Gourhari Jana, Pratim Kumar Chattaraj, Niloy\n  Ganguly, Manuel Gomez-Rodriguez", "title": "NeVAE: A Deep Generative Model for Molecular Graphs", "comments": "Accepted in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been praised for their ability to learn smooth\nlatent representation of images, text, and audio, which can then be used to\ngenerate new, plausible data. However, current generative models are unable to\nwork with molecular graphs due to their unique characteristics-their underlying\nstructure is not Euclidean or grid-like, they remain isomorphic under\npermutation of the nodes labels, and they come with a different number of nodes\nand edges. In this paper, we first propose a novel variational autoencoder for\nmolecular graphs, whose encoder and decoder are specially designed to account\nfor the above properties by means of several technical innovations. Moreover,\nin contrast with the state of the art, our decoder is able to provide the\nspatial coordinates of the atoms of the molecules it generates. Then, we\ndevelop a gradient-based algorithm to optimize the decoder of our model so that\nit learns to generate molecules that maximize the value of certain property of\ninterest and, given a molecule of interest, it is able to optimize the spatial\nconfiguration of its atoms for greater stability. Experiments reveal that our\nvariational autoencoder can discover plausible, diverse and novel molecules\nmore effectively than several state of the art models. Moreover, for several\nproperties of interest, our optimized decoder is able to identify molecules\nwith property values 121% higher than those identified by several state of the\nart methods based on Bayesian optimization and reinforcement learning\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 19:00:34 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 18:01:28 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 19:11:08 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 15:16:18 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Samanta", "Bidisha", ""], ["De", "Abir", ""], ["Jana", "Gourhari", ""], ["Chattaraj", "Pratim Kumar", ""], ["Ganguly", "Niloy", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1802.05312", "submitter": "Karl Ridgeway", "authors": "Karl Ridgeway, Michael C. Mozer", "title": "Learning Deep Disentangled Embeddings with the F-Statistic Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-embedding methods aim to discover representations of a domain that make\nexplicit the domain's class structure and thereby support few-shot learning.\nDisentangling methods aim to make explicit compositional or factorial\nstructure. We combine these two active but independent lines of research and\npropose a new paradigm suitable for both goals. We propose and evaluate a novel\nloss function based on the $F$ statistic, which describes the separation of two\nor more distributions. By ensuring that distinct classes are well separated on\na subset of embedding dimensions, we obtain embeddings that are useful for\nfew-shot learning. By not requiring separation on all dimensions, we encourage\nthe discovery of disentangled representations. Our embedding method matches or\nbeats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot\nlearning tasks. Our method also obtains performance superior to a variety of\nalternatives on disentangling, as evaluated by two key properties of a\ndisentangled representation: modularity and explicitness. The goal of our work\nis to obtain more interpretable, manipulable, and generalizable deep\nrepresentations of concepts and categories.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:28:38 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 00:25:08 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ridgeway", "Karl", ""], ["Mozer", "Michael C.", ""]]}, {"id": "1802.05313", "submitter": "Huazhe Xu", "authors": "Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, Trevor Darrell", "title": "Reinforcement Learning from Imperfect Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust real-world learning should benefit from both demonstrations and\ninteractions with the environment. Current approaches to learning from\ndemonstration and reward perform supervised learning on expert demonstration\ndata and use reinforcement learning to further improve performance based on the\nreward received from the environment. These tasks have divergent losses which\nare difficult to jointly optimize and such methods can be very sensitive to\nnoisy demonstrations. We propose a unified reinforcement learning algorithm,\nNormalized Actor-Critic (NAC), that effectively normalizes the Q-function,\nreducing the Q-values of actions unseen in the demonstration data. NAC learns\nan initial policy network from demonstrations and refines the policy in the\nenvironment, surpassing the demonstrator's performance. Crucially, both\nlearning from demonstration and interactive refinement use the same objective,\nunlike prior approaches that combine distinct supervised and reinforcement\nlosses. This makes NAC robust to suboptimal demonstration data since the method\nis not forced to mimic all of the examples in the dataset. We show that our\nunified reinforcement learning algorithm can learn robustly and outperform\nexisting baselines when evaluated on several realistic driving games.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:37:38 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 04:39:22 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gao", "Yang", ""], ["Xu", "Huazhe", ""], ["Lin", "Ji", ""], ["Yu", "Fisher", ""], ["Levine", "Sergey", ""], ["Darrell", "Trevor", ""]]}, {"id": "1802.05315", "submitter": "Dong Yin", "authors": "Andr\\'es Mu\\~noz Medina, Sergei Vassilvitskii, Dong Yin", "title": "Online Learning for Non-Stationary A/B Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rollout of new versions of a feature in modern applications is a manual\nmulti-stage process, as the feature is released to ever larger groups of users,\nwhile its performance is carefully monitored. This kind of A/B testing is\nubiquitous, but suboptimal, as the monitoring requires heavy human\nintervention, is not guaranteed to capture consistent, but short-term\nfluctuations in performance, and is inefficient, as better versions take a long\ntime to reach the full population.\n  In this work we formulate this question as that of expert learning, and give\na new algorithm Follow-The-Best-Interval, FTBI, that works in dynamic,\nnon-stationary environments. Our approach is practical, simple, and efficient,\nand has rigorous guarantees on its performance. Finally, we perform a thorough\nevaluation on synthetic and real world datasets and show that our approach\noutperforms current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:42:51 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 06:42:39 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Medina", "Andr\u00e9s Mu\u00f1oz", ""], ["Vassilvitskii", "Sergei", ""], ["Yin", "Dong", ""]]}, {"id": "1802.05319", "submitter": "Suvodeep Majumder", "authors": "Suvodeep Majumder, Nikhila Balaji, Katie Brey, Wei Fu, Tim Menzies", "title": "500+ Times Faster Than Deep Learning (A Case Study Exploring Faster\n  Methods for Text Mining StackOverflow)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are useful for high-dimensional data and are becoming\nwidely used in many areas of software engineering. Deep learners utilizes\nextensive computational power and can take a long time to train-- making it\ndifficult to widely validate and repeat and improve their results. Further,\nthey are not the best solution in all domains. For example, recent results show\nthat for finding related Stack Overflow posts, a tuned SVM performs similarly\nto a deep learner, but is significantly faster to train. This paper extends\nthat recent result by clustering the dataset, then tuning very learners within\neach cluster. This approach is over 500 times faster than deep learning (and\nover 900 times faster if we use all the cores on a standard laptop computer).\nSignificantly, this faster approach generates classifiers nearly as good\n(within 2\\% F1 Score) as the much slower deep learning method. Hence we\nrecommend this faster methods since it is much easier to reproduce and utilizes\nfar fewer CPU resources. More generally, we recommend that before researchers\nrelease research results, that they compare their supposedly sophisticated\nmethods against simpler alternatives (e.g applying simpler learners to build\nlocal models).\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:57:48 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Majumder", "Suvodeep", ""], ["Balaji", "Nikhila", ""], ["Brey", "Katie", ""], ["Fu", "Wei", ""], ["Menzies", "Tim", ""]]}, {"id": "1802.05335", "submitter": "Mike Wu", "authors": "Mike Wu and Noah Goodman", "title": "Multimodal Generative Models for Scalable Weakly-Supervised Learning", "comments": "To appear at NIPS 2018; 9 pages with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple modalities often co-occur when describing natural phenomena.\nLearning a joint representation of these modalities should yield deeper and\nmore useful representations. Previous generative approaches to multi-modal\ninput either do not learn a joint distribution or require additional\ncomputation to handle missing data. Here, we introduce a multimodal variational\nautoencoder (MVAE) that uses a product-of-experts inference network and a\nsub-sampled training paradigm to solve the multi-modal inference problem.\nNotably, our model shares parameters to efficiently learn under any combination\nof missing modalities. We apply the MVAE on four datasets and match\nstate-of-the-art performance using many fewer parameters. In addition, we show\nthat the MVAE is directly applicable to weakly-supervised learning, and is\nrobust to incomplete supervision. We then consider two case studies, one of\nlearning image transformations---edge detection, colorization,\nsegmentation---as a set of modalities, followed by one of machine translation\nbetween two languages. We find appealing results across this range of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 22:08:05 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 18:14:29 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 16:39:10 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wu", "Mike", ""], ["Goodman", "Noah", ""]]}, {"id": "1802.05351", "submitter": "Binghui Wang", "authors": "Binghui Wang, Neil Zhenqiang Gong", "title": "Stealing Hyperparameters in Machine Learning", "comments": "In the 39th IEEE Symposium on Security and Privacy (IEEE S & P), May\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters are critical in machine learning, as different\nhyperparameters often result in models with significantly different\nperformance. Hyperparameters may be deemed confidential because of their\ncommercial value and the confidentiality of the proprietary algorithms that the\nlearner uses to learn them. In this work, we propose attacks on stealing the\nhyperparameters that are learned by a learner. We call our attacks\nhyperparameter stealing attacks. Our attacks are applicable to a variety of\npopular machine learning algorithms such as ridge regression, logistic\nregression, support vector machine, and neural network. We evaluate the\neffectiveness of our attacks both theoretically and empirically. For instance,\nwe evaluate our attacks on Amazon Machine Learning. Our results demonstrate\nthat our attacks can accurately steal hyperparameters. We also study\ncountermeasures. Our results highlight the need for new defenses against our\nhyperparameter stealing attacks for certain machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 22:58:31 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:21:27 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 01:48:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Binghui", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "1802.05355", "submitter": "Leonardo Rey Vega", "authors": "Mat\\'ias Vera, Pablo Piantanida, Leonardo Rey Vega", "title": "The Role of Information Complexity and Randomization in Representation\n  Learning", "comments": "35 pages, 3 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grand challenge in representation learning is to learn the different\nexplanatory factors of variation behind the high dimen- sional data. Encoder\nmodels are often determined to optimize performance on training data when the\nreal objective is to generalize well to unseen data. Although there is enough\nnumerical evidence suggesting that noise injection (during training) at the\nrepresentation level might improve the generalization ability of encoders, an\ninformation-theoretic understanding of this principle remains elusive. This\npaper presents a sample-dependent bound on the generalization gap of the\ncross-entropy loss that scales with the information complexity (IC) of the\nrepresentations, meaning the mutual information between inputs and their\nrepresentations. The IC is empirically investigated for standard multi-layer\nneural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the\ngap and the IC appear to be in direct correlation, suggesting that SGD selects\nencoders to implicitly minimize the IC. We specialize the IC to study the role\nof Dropout on the generalization capacity of deep encoders which is shown to be\ndirectly related to the encoder capacity, being a measure of the\ndistinguishability among samples from their representations. Our results\nsupport some recent regularization methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 23:31:11 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Vera", "Mat\u00edas", ""], ["Piantanida", "Pablo", ""], ["Vega", "Leonardo Rey", ""]]}, {"id": "1802.05370", "submitter": "Alistair Shilton", "authors": "Alistair Shilton, Sunil Gupta, Santu Rana, Pratibha Vellanki, Cheng\n  Li, Laurence Park, Svetha Venkatesh, Alessandra Sutti, David Rubin, Thomas\n  Dorin, Alireza Vahid, Murray Height", "title": "Covariance Function Pre-Training with m-Kernels for Accelerated Bayesian\n  Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel approach to direct covariance function learning\nfor Bayesian optimisation, with particular emphasis on experimental design\nproblems where an existing corpus of condensed knowledge is present. The method\npresented borrows techniques from reproducing kernel Banach space theory\n(specifically m-kernels) and leverages them to convert (or re-weight) existing\ncovariance functions into new, problem-specific covariance functions. The key\nadvantage of this approach is that rather than relying on the user to manually\nselect (with some hyperparameter tuning and experimentation) an appropriate\ncovariance function it constructs the covariance function to specifically match\nthe problem at hand. The technique is demonstrated on two real-world problems -\nspecifically alloy design and short-polymer fibre manufacturing - as well as a\nselected test function.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 00:38:28 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 00:37:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Shilton", "Alistair", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Vellanki", "Pratibha", ""], ["Li", "Cheng", ""], ["Park", "Laurence", ""], ["Venkatesh", "Svetha", ""], ["Sutti", "Alessandra", ""], ["Rubin", "David", ""], ["Dorin", "Thomas", ""], ["Vahid", "Alireza", ""], ["Height", "Murray", ""]]}, {"id": "1802.05374", "submitter": "Hao-Jun Shi", "authors": "Raghu Bollapragada, Dheevatsa Mudigere, Jorge Nocedal, Hao-Jun Michael\n  Shi, Ping Tak Peter Tang", "title": "A Progressive Batching L-BFGS Method for Machine Learning", "comments": "ICML 2018. 25 pages, 17 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard L-BFGS method relies on gradient approximations that are not\ndominated by noise, so that search directions are descent directions, the line\nsearch is reliable, and quasi-Newton updating yields useful quadratic models of\nthe objective function. All of this appears to call for a full batch approach,\nbut since small batch sizes give rise to faster algorithms with better\ngeneralization properties, L-BFGS is currently not considered an algorithm of\nchoice for large-scale machine learning applications. One need not, however,\nchoose between the two extremes represented by the full batch or highly\nstochastic regimes, and may instead follow a progressive batching approach in\nwhich the sample size increases during the course of the optimization. In this\npaper, we present a new version of the L-BFGS algorithm that combines three\nbasic components - progressive batching, a stochastic line search, and stable\nquasi-Newton updating - and that performs well on training logistic regression\nand deep neural networks. We provide supporting convergence theory for the\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 01:02:36 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 04:38:48 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Bollapragada", "Raghu", ""], ["Mudigere", "Dheevatsa", ""], ["Nocedal", "Jorge", ""], ["Shi", "Hao-Jun Michael", ""], ["Tang", "Ping Tak Peter", ""]]}, {"id": "1802.05380", "submitter": "Sheng-Jun Huang", "authors": "Sheng-Jun Huang, Miao Xu, Ming-Kun Xie, Masashi Sugiyama, Gang Niu and\n  Songcan Chen", "title": "Active Feature Acquisition with Supervised Matrix Completion", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature missing is a serious problem in many applications, which may lead to\nlow quality of training data and further significantly degrade the learning\nperformance. While feature acquisition usually involves special devices or\ncomplex process, it is expensive to acquire all feature values for the whole\ndataset. On the other hand, features may be correlated with each other, and\nsome values may be recovered from the others. It is thus important to decide\nwhich features are most informative for recovering the other features as well\nas improving the learning performance. In this paper, we try to train an\neffective classification model with least acquisition cost by jointly\nperforming active feature querying and supervised matrix completion. When\ncompleting the feature matrix, a novel target function is proposed to\nsimultaneously minimize the reconstruction error on observed entries and the\nsupervised loss on training data. When querying the feature value, the most\nuncertain entry is actively selected based on the variance of previous\niterations. In addition, a bi-objective optimization method is presented for\ncost-aware active selection when features bear different acquisition costs. The\neffectiveness of the proposed approach is well validated by both theoretical\nanalysis and experimental study.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 01:46:59 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 02:02:01 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Huang", "Sheng-Jun", ""], ["Xu", "Miao", ""], ["Xie", "Ming-Kun", ""], ["Sugiyama", "Masashi", ""], ["Niu", "Gang", ""], ["Chen", "Songcan", ""]]}, {"id": "1802.05386", "submitter": "Fenglei Fan", "authors": "Fenglei Fan, Ziyu Su, Yueyang Teng, Ge Wang", "title": "Shamap: Shape-based Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For manifold learning, it is assumed that high-dimensional sample/data points\nare embedded on a low-dimensional manifold. Usually, distances among samples\nare computed to capture an underlying data structure. Here we propose a metric\naccording to angular changes along a geodesic line, thereby reflecting the\nunderlying shape-oriented information or a topological similarity between high-\nand low-dimensional representations of a data cloud. Our results demonstrate\nthe feasibility and merits of the proposed dimensionality reduction scheme.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 02:09:23 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 20:51:56 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Fan", "Fenglei", ""], ["Su", "Ziyu", ""], ["Teng", "Yueyang", ""], ["Wang", "Ge", ""]]}, {"id": "1802.05392", "submitter": "Jun Lu", "authors": "Jun Lu, Meng Li, David Dunson", "title": "Reducing over-clustering via the powered Chinese restaurant process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture (DPM) models tend to produce many small clusters\nregardless of whether they are needed to accurately characterize the data -\nthis is particularly true for large data sets. However, interpretability,\nparsimony, data storage and communication costs all are hampered by having\noverly many clusters. We propose a powered Chinese restaurant process to limit\nthis kind of problem and penalize over clustering. The method is illustrated\nusing some simulation examples and data with large and small sample size\nincluding MNIST and the Old Faithful Geyser data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 02:53:30 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Lu", "Jun", ""], ["Li", "Meng", ""], ["Dunson", "David", ""]]}, {"id": "1802.05394", "submitter": "Sheng-Jun Huang", "authors": "Sheng-Jun Huang and Jia-Wei Zhao and Zhao-Yang Liu", "title": "Cost-Effective Training of Deep CNNs with Active Model Adaptation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved great success in various\napplications. However, training an effective DNN model for a specific task is\nrather challenging because it requires a prior knowledge or experience to\ndesign the network architecture, repeated trial-and-error process to tune the\nparameters, and a large set of labeled data to train the model. In this paper,\nwe propose to overcome these challenges by actively adapting a pre-trained\nmodel to a new task with less labeled examples. Specifically, the pre-trained\nmodel is iteratively fine tuned based on the most useful examples. The examples\nare actively selected based on a novel criterion, which jointly estimates the\npotential contribution of an instance on optimizing the feature representation\nas well as improving the classification model for the target task. On one hand,\nthe pre-trained model brings plentiful information from its original task,\navoiding redesign of the network architecture or training from scratch; and on\nthe other hand, the labeling cost can be significantly reduced by active label\nquerying. Experiments on multiple datasets and different pre-trained models\ndemonstrate that the proposed approach can achieve cost-effective training of\nDNNs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 03:06:06 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 08:52:57 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Huang", "Sheng-Jun", ""], ["Zhao", "Jia-Wei", ""], ["Liu", "Zhao-Yang", ""]]}, {"id": "1802.05400", "submitter": "Cheng Li", "authors": "Cheng Li, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh,\n  Alistair Shilton", "title": "High Dimensional Bayesian Optimization Using Dropout", "comments": "7 pages; Proceedings of the Twenty-Sixth International Joint\n  Conference on Artificial Intelligence 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling Bayesian optimization to high dimensions is challenging task as the\nglobal optimization of high-dimensional acquisition function can be expensive\nand often infeasible. Existing methods depend either on limited active\nvariables or the additive form of the objective function. We propose a new\nmethod for high-dimensional Bayesian optimization, that uses a dropout strategy\nto optimize only a subset of variables at each iteration. We derive theoretical\nbounds for the regret and show how it can inform the derivation of our\nalgorithm. We demonstrate the efficacy of our algorithms for optimization on\ntwo benchmark functions and two real-world applications- training cascade\nclassifiers and optimizing alloy composition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 04:30:59 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Li", "Cheng", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Nguyen", "Vu", ""], ["Venkatesh", "Svetha", ""], ["Shilton", "Alistair", ""]]}, {"id": "1802.05408", "submitter": "Yao-Hung Tsai", "authors": "Denny Wu, Yixiu Zhao, Yao-Hung Hubert Tsai, Makoto Yamada, Ruslan\n  Salakhutdinov", "title": "\"Dependency Bottleneck\" in Auto-encoding Architectures: an Empirical\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works investigated the generalization properties in deep neural\nnetworks (DNNs) by studying the Information Bottleneck in DNNs. However, the\nmea- surement of the mutual information (MI) is often inaccurate due to the\ndensity estimation. To address this issue, we propose to measure the dependency\ninstead of MI between layers in DNNs. Specifically, we propose to use\nHilbert-Schmidt Independence Criterion (HSIC) as the dependency measure, which\ncan measure the dependence of two random variables without estimating\nprobability densities. Moreover, HSIC is a special case of the Squared-loss\nMutual Information (SMI). In the experiment, we empirically evaluate the\ngeneralization property using HSIC in both the reconstruction and prediction\nauto-encoding (AE) architectures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 05:22:19 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Wu", "Denny", ""], ["Zhao", "Yixiu", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Yamada", "Makoto", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1802.05411", "submitter": "Yao-Hung Tsai", "authors": "Yao-Hung Hubert Tsai, Makoto Yamada, Denny Wu, Ruslan Salakhutdinov,\n  Ichiro Takeuchi, Kenji Fukumizu", "title": "Selecting the Best in GANs Family: a Post Selection Inference Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Which Generative Adversarial Networks (GANs) generates the most plausible\nimages?\" has been a frequently asked question among researchers. To address\nthis problem, we first propose an \\emph{incomplete} U-statistics estimate of\nmaximum mean discrepancy $\\mathrm{MMD}_{inc}$ to measure the distribution\ndiscrepancy between generated and real images. $\\mathrm{MMD}_{inc}$ enjoys the\nadvantages of asymptotic normality, computation efficiency, and model\nagnosticity. We then propose a GANs analysis framework to select and test the\n\"best\" member in GANs family using the Post Selection Inference (PSI) with\n$\\mathrm{MMD}_{inc}$. In the experiments, we adopt the proposed framework on 7\nGANs variants and compare their $\\mathrm{MMD}_{inc}$ scores.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 05:27:54 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 21:03:59 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Yamada", "Makoto", ""], ["Wu", "Denny", ""], ["Salakhutdinov", "Ruslan", ""], ["Takeuchi", "Ichiro", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1802.05429", "submitter": "Antoine Rolet", "authors": "Antoine Rolet, Vivien Seguy, Mathieu Blondel, Hiroshi Sawada", "title": "Blind Source Separation with Optimal Transport Non-negative Matrix\n  Factorization", "comments": "22 pages, 7 figures, 2 additional files", "journal-ref": null, "doi": "10.1186/s13634-018-0576-2", "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport as a loss for machine learning optimization problems has\nrecently gained a lot of attention. Building upon recent advances in\ncomputational optimal transport, we develop an optimal transport non-negative\nmatrix factorization (NMF) algorithm for supervised speech blind source\nseparation (BSS). Optimal transport allows us to design and leverage a cost\nbetween short-time Fourier transform (STFT) spectrogram frequencies, which\ntakes into account how humans perceive sound. We give empirical evidence that\nusing our proposed optimal transport NMF leads to perceptually better results\nthan Euclidean NMF, for both isolated voice reconstruction and BSS tasks.\nFinally, we demonstrate how to use optimal transport for cross domain sound\nprocessing tasks, where frequencies represented in the input spectrograms may\nbe different from one spectrogram to another.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 08:01:48 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Rolet", "Antoine", ""], ["Seguy", "Vivien", ""], ["Blondel", "Mathieu", ""], ["Sawada", "Hiroshi", ""]]}, {"id": "1802.05431", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Nicolas Flammarion, Yi-An Ma, Peter L. Bartlett\n  and Michael I. Jordan", "title": "On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo", "comments": "37 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide convergence guarantees in Wasserstein distance for a variety of\nvariance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion\nand control-variate underdamped Langevin diffusion. We analyze these methods\nunder a uniform set of assumptions on the log-posterior distribution, assuming\nit to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a\nnew proof technique combining ideas from finite-sum optimization and the\nanalysis of sampling methods. Our sharp theoretical bounds allow us to identify\nregimes of interest where each method performs better than the others. Our\ntheory is verified with experiments on real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 08:09:00 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Flammarion", "Nicolas", ""], ["Ma", "Yi-An", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1802.05447", "submitter": "Puyudi Yang", "authors": "Puyudi Yang, Cho-Jui Hsieh, Jane-Ling Wang", "title": "History PCA: A New Algorithm for Streaming PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new algorithm for streaming principal component\nanalysis. With limited memory, small devices cannot store all the samples in\nthe high-dimensional regime. Streaming principal component analysis aims to\nfind the $k$-dimensional subspace which can explain the most variation of the\n$d$-dimensional data points that come into memory sequentially. In order to\ndeal with large $d$ and large $N$ (number of samples), most streaming PCA\nalgorithms update the current model using only the incoming sample and then\ndump the information right away to save memory. However the information\ncontained in previously streamed data could be useful. Motivated by this idea,\nwe develop a new streaming PCA algorithm called History PCA that achieves this\ngoal. By using $O(Bd)$ memory with $B\\approx 10$ being the block size, our\nalgorithm converges much faster than existing streaming PCA algorithms. By\nchanging the number of inner iterations, the memory usage can be further\nreduced to $O(d)$ while maintaining a comparable convergence speed. We provide\ntheoretical guarantees for the convergence of our algorithm along with the rate\nof convergence. We also demonstrate on synthetic and real world data sets that\nour algorithm compares favorably with other state-of-the-art streaming PCA\nmethods in terms of the convergence speed and performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:33:00 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Yang", "Puyudi", ""], ["Hsieh", "Cho-Jui", ""], ["Wang", "Jane-Ling", ""]]}, {"id": "1802.05451", "submitter": "Roei Herzig", "authors": "Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir\n  Globerson", "title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured\n  Prediction", "comments": "Paper is accepted for NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine understanding of complex images is a key goal of artificial\nintelligence. One challenge underlying this task is that visual scenes contain\nmultiple inter-related objects, and that global context plays an important role\nin interpreting the scene. A natural modeling framework for capturing such\neffects is structured prediction, which optimizes over complex labels, while\nmodeling within-label interactions. However, it is unclear what principles\nshould guide the design of a structured prediction model that utilizes the\npower of deep learning components. Here we propose a design principle for such\narchitectures that follows from a natural requirement of permutation\ninvariance. We prove a necessary and sufficient characterization for\narchitectures that follow this invariance, and discuss its implication on model\ndesign. Finally, we show that the resulting model achieves new state of the art\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\nrecent approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:50:15 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 18:07:49 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 13:26:04 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 21:48:19 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Herzig", "Roei", ""], ["Raboh", "Moshiko", ""], ["Chechik", "Gal", ""], ["Berant", "Jonathan", ""], ["Globerson", "Amir", ""]]}, {"id": "1802.05472", "submitter": "Yan Zhu", "authors": "Yan Zhu, Abdullah Mueen, Eamonn Keogh", "title": "Admissible Time Series Motif Discovery with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of time series motifs has emerged as one of the most useful\nprimitives in time series data mining. Researchers have shown its utility for\nexploratory data mining, summarization, visualization, segmentation,\nclassification, clustering, and rule discovery. Although there has been more\nthan a decade of extensive research, there is still no technique to allow the\ndiscovery of time series motifs in the presence of missing data, despite the\nwell-documented ubiquity of missing data in scientific, industrial, and medical\ndatasets. In this work, we introduce a technique for motif discovery in the\npresence of missing data. We formally prove that our method is admissible,\nproducing no false negatives. We also show that our method can piggy-back off\nthe fastest known motif discovery method with a small constant factor\ntime/space overhead. We will demonstrate our approach on diverse datasets with\nvarying amounts of missing data\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:45:46 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Zhu", "Yan", ""], ["Mueen", "Abdullah", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1802.05550", "submitter": "Przemys{\\l}aw Spurek", "authors": "P. Spurek, P. Rola, J. Tabor, A. Czechowski", "title": "ICA based on Split Generalized Gaussian", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.09160", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) - one of the basic tools in data\nanalysis - aims to find a coordinate system in which the components of the data\nare independent. Most popular ICA methods use kurtosis as a metric of\nnon-Gaussianity to maximize, such as FastICA and JADE. However, their\nassumption of fourth-order moment (kurtosis) may not always be satisfied in\npractice. One of the possible solution is to use third-order moment (skewness)\ninstead of kurtosis, which was applied in $ICA_{SG}$ and EcoICA.\n  In this paper we present a competitive approach to ICA based on the Split\nGeneralized Gaussian distribution (SGGD), which is well adapted to heavy-tailed\nas well as asymmetric data. Consequently, we obtain a method which works better\nthan the classical approaches, in both cases: heavy tails and non-symmetric\ndata. \\end{abstract}\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 12:32:56 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Spurek", "P.", ""], ["Rola", "P.", ""], ["Tabor", "J.", ""], ["Czechowski", "A.", ""]]}, {"id": "1802.05584", "submitter": "Il Yong Chun", "authors": "Il Yong Chun and Jeffrey A. Fessler", "title": "Convolutional Analysis Operator Learning: Acceleration and Convergence", "comments": "22 pages, 11 figures, fixed incorrect math theorem numbers in fig. 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:51:38 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 19:11:31 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 01:29:57 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 07:19:14 GMT"}, {"version": "v5", "created": "Fri, 9 Aug 2019 21:28:38 GMT"}, {"version": "v6", "created": "Thu, 22 Aug 2019 12:45:28 GMT"}, {"version": "v7", "created": "Wed, 11 Sep 2019 10:03:54 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Chun", "Il Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1802.05622", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Conditioning of three-dimensional generative adversarial networks for\n  pore and reservoir-scale models", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical modeling of petrophysical properties is a key step in modern\nintegrated oil and gas reservoir studies. Recently, generative adversarial\nnetworks (GAN) have been shown to be a successful method for generating\nunconditional simulations of pore- and reservoir-scale models. This\ncontribution leverages the differentiable nature of neural networks to extend\nGANs to the conditional simulation of three-dimensional pore- and\nreservoir-scale models. Based on the previous work of Yeh et al. (2016), we use\na content loss to constrain to the conditioning data and a perceptual loss\nobtained from the evaluation of the GAN discriminator network. The technique is\ntested on the generation of three-dimensional micro-CT images of a Ketton\nlimestone constrained by two-dimensional cross-sections, and on the simulation\nof the Maules Creek alluvial aquifer constrained by one-dimensional sections.\nOur results show that GANs represent a powerful method for sampling conditioned\npore and reservoir samples for stochastic reservoir evaluation workflows.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 15:34:23 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1802.05637", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Masanori Koyama", "title": "cGANs with Projection Discriminator", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, projection based way to incorporate the conditional\ninformation into the discriminator of GANs that respects the role of the\nconditional information in the underlining probabilistic model. This approach\nis in contrast with most frameworks of conditional GANs used in application\ntoday, which use the conditional information by concatenating the (embedded)\nconditional vector to the feature vectors. With this modification, we were able\nto significantly improve the quality of the class conditional image generation\non ILSVRC2012 (ImageNet) 1000-class image dataset from the current\nstate-of-the-art result, and we achieved this with a single pair of a\ndiscriminator and a generator. We were also able to extend the application to\nsuper-resolution and succeeded in producing highly discriminative\nsuper-resolution images. This new structure also enabled high quality category\ntransformation based on parametric functional transformation of conditional\nbatch normalization layers in the generator.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:19:21 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 00:02:46 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Miyato", "Takeru", ""], ["Koyama", "Masanori", ""]]}, {"id": "1802.05664", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "DeepMatch: Balancing Deep Covariate Representations for Causal Inference\n  Using Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal covariate balance for causal inferences from observational\ndata when rich covariates and complex relationships necessitate flexible\nmodeling with neural networks. Standard approaches such as propensity weighting\nand matching/balancing fail in such settings due to miscalibrated propensity\nnets and inappropriate covariate representations, respectively. We propose a\nnew method based on adversarial training of a weighting and a discriminator\nnetwork that effectively addresses this methodological gap. This is\ndemonstrated through new theoretical characterizations of the method as well as\nempirical results using both fully connected architectures to learn complex\nrelationships and convolutional architectures to handle image confounders,\nshowing how this new method can enable strong causal analyses in these\nchallenging settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:58:21 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1802.05666", "submitter": "Jonathan Uesato", "authors": "Jonathan Uesato, Brendan O'Donoghue, Aaron van den Oord, Pushmeet\n  Kohli", "title": "Adversarial Risk and the Dangers of Evaluating Against Weak Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates recently proposed approaches for defending against\nadversarial examples and evaluating adversarial robustness. We motivate\n'adversarial risk' as an objective for achieving models robust to worst-case\ninputs. We then frame commonly used attacks and evaluation metrics as defining\na tractable surrogate objective to the true adversarial risk. This suggests\nthat models may optimize this surrogate rather than the true adversarial risk.\nWe formalize this notion as 'obscurity to an adversary,' and develop tools and\nheuristics for identifying obscured models and designing transparent models. We\ndemonstrate that this is a significant problem in practice by repurposing\ngradient-free optimization techniques into adversarial attacks, which we use to\ndecrease the accuracy of several recently proposed defenses to near zero. Our\nhope is that our formulations and results will help researchers to develop more\npowerful defenses.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 17:13:18 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 14:20:27 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Uesato", "Jonathan", ""], ["O'Donoghue", "Brendan", ""], ["Oord", "Aaron van den", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1802.05680", "submitter": "Maurizio Filippone", "authors": "Marco Lorenzi and Maurizio Filippone", "title": "Constraining the Dynamics of Deep Probabilistic Models", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel generative formulation of deep probabilistic models\nimplementing \"soft\" constraints on their function dynamics. In particular, we\ndevelop a flexible methodological framework where the modeled functions and\nderivatives of a given order are subject to inequality or equality constraints.\nWe then characterize the posterior distribution over model and constraint\nparameters through stochastic variational inference. As a result, the proposed\napproach allows for accurate and scalable uncertainty quantification on the\npredictions and on all parameters. We demonstrate the application of equality\nconstraints in the challenging problem of parameter inference in ordinary\ndifferential equation models, while we showcase the application of inequality\nconstraints on the problem of monotonic regression of count data. The proposed\napproach is extensively tested in several experimental settings, leading to\nhighly competitive results in challenging modeling applications, while offering\nhigh expressiveness, flexibility and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 17:54:08 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 08:34:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lorenzi", "Marco", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1802.05688", "submitter": "David Craft", "authors": "Timo M. Deist, Andrew Patti, Zhaoqi Wang, David Krane, Taylor\n  Sorenson, David Craft", "title": "Simulation assisted machine learning", "comments": "This manuscript has been accepted for publication in Bioinformatics\n  published by Oxford University Press:\n  https://doi.org/10.1093/bioinformatics/btz199 (open access). Timo M. Deist\n  and Andrew Patti contributed equally to this work", "journal-ref": null, "doi": "10.1093/bioinformatics/btz199", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: In a predictive modeling setting, if sufficient details of the\nsystem behavior are known, one can build and use a simulation for making\npredictions. When sufficient system details are not known, one typically turns\nto machine learning, which builds a black-box model of the system using a large\ndataset of input sample features and outputs. We consider a setting which is\nbetween these two extremes: some details of the system mechanics are known but\nnot enough for creating simulations that can be used to make high quality\npredictions. In this context we propose using approximate simulations to build\na kernel for use in kernelized machine learning methods, such as support vector\nmachines. The results of multiple simulations (under various uncertainty\nscenarios) are used to compute similarity measures between every pair of\nsamples: sample pairs are given a high similarity score if they behave\nsimilarly under a wide range of simulation parameters. These similarity values,\nrather than the original high dimensional feature data, are used to build the\nkernel.\n  Results: We demonstrate and explore the simulation based kernel (SimKern)\nconcept using four synthetic complex systems--three biologically inspired\nmodels and one network flow optimization model. We show that, when the number\nof training samples is small compared to the number of features, the SimKern\napproach dominates over no-prior-knowledge methods. This approach should be\napplicable in all disciplines where predictive models are sought and\ninformative yet approximate simulations are available.\n  Availability: The Python SimKern software, the demonstration models (in\nMATLAB, R), and the datasets are available at\nhttps://github.com/davidcraft/SimKern.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:04:34 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 16:55:40 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 14:51:51 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 18:38:09 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Deist", "Timo M.", ""], ["Patti", "Andrew", ""], ["Wang", "Zhaoqi", ""], ["Krane", "David", ""], ["Sorenson", "Taylor", ""], ["Craft", "David", ""]]}, {"id": "1802.05693", "submitter": "Virag Shah", "authors": "Virag Shah, Jose Blanchet, Ramesh Johari", "title": "Bandit Learning with Positive Externalities", "comments": "31 pages, 1 table, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many platforms, user arrivals exhibit a self-reinforcing behavior: future\nuser arrivals are likely to have preferences similar to users who were\nsatisfied in the past. In other words, arrivals exhibit positive externalities.\nWe study multiarmed bandit (MAB) problems with positive externalities. We show\nthat the self-reinforcing preferences may lead standard benchmark algorithms\nsuch as UCB to exhibit linear regret. We develop a new algorithm, Balanced\nExploration (BE), which explores arms carefully to avoid suboptimal convergence\nof arrivals before sufficient evidence is gathered. We also introduce an\nadaptive variant of BE which successively eliminates suboptimal arms. We\nanalyze their asymptotic regret, and establish optimality by showing that no\nalgorithm can perform better.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:22:06 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 23:57:43 GMT"}, {"version": "v3", "created": "Sat, 2 Jun 2018 21:35:16 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 23:34:54 GMT"}, {"version": "v5", "created": "Wed, 6 Mar 2019 20:56:15 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Shah", "Virag", ""], ["Blanchet", "Jose", ""], ["Johari", "Ramesh", ""]]}, {"id": "1802.05694", "submitter": "Xilun Chen", "authors": "Xilun Chen, Claire Cardie", "title": "Multinomial Adversarial Networks for Multi-Domain Text Classification", "comments": "NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many text classification tasks are known to be highly domain-dependent.\nUnfortunately, the availability of training data can vary drastically across\ndomains. Worse still, for some domains there may not be any annotated data at\nall. In this work, we propose a multinomial adversarial network (MAN) to tackle\nthe text classification problem in this real-world multidomain setting (MDTC).\nWe provide theoretical justifications for the MAN framework, proving that\ndifferent instances of MANs are essentially minimizers of various f-divergence\nmetrics (Ali and Silvey, 1966) among multiple probability distributions. MANs\nare thus a theoretically sound generalization of traditional adversarial\nnetworks that discriminate over two distributions. More specifically, for the\nMDTC task, MAN learns features that are invariant across multiple domains by\nresorting to its ability to reduce the divergence among the feature\ndistributions of each domain. We present experimental results showing that MANs\nsignificantly outperform the prior art on the MDTC task. We also show that MANs\nachieve state-of-the-art performance for domains with no labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:22:58 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Chen", "Xilun", ""], ["Cardie", "Claire", ""]]}, {"id": "1802.05695", "submitter": "James Mullenbach", "authors": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob\n  Eisenstein", "title": "Explainable Prediction of Medical Codes from Clinical Text", "comments": "NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes are text documents that are created by clinicians for each\npatient encounter. They are typically accompanied by medical codes, which\ndescribe the diagnosis and treatment. Annotating these codes is labor intensive\nand error prone; furthermore, the connection between the codes and the text is\nnot annotated, obscuring the reasons and details behind specific diagnoses and\ntreatments. We present an attentional convolutional network that predicts\nmedical codes from clinical text. Our method aggregates information across the\ndocument using a convolutional neural network, and uses an attention mechanism\nto select the most relevant segments for each of the thousands of possible\ncodes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of\n0.54, which are both better than the prior state of the art. Furthermore,\nthrough an interpretability evaluation by a physician, we show that the\nattention mechanism identifies meaningful explanations for each code assignment\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:25:32 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 21:45:35 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Mullenbach", "James", ""], ["Wiegreffe", "Sarah", ""], ["Duke", "Jon", ""], ["Sun", "Jimeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1802.05733", "submitter": "Sergei Vassilvitskii", "authors": "Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Sergei Vassilvitskii", "title": "Fair Clustering Through Fairlets", "comments": null, "journal-ref": "NIPS 2017: 5036-5044", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of fair clustering under the {\\em disparate impact}\ndoctrine, where each protected class must have approximately equal\nrepresentation in every cluster. We formulate the fair clustering problem under\nboth the $k$-center and the $k$-median objectives, and show that even with two\nprotected classes the problem is challenging, as the optimum solution can\nviolate common conventions---for instance a point may no longer be assigned to\nits nearest cluster center! En route we introduce the concept of fairlets,\nwhich are minimal sets that satisfy fair representation while approximately\npreserving the clustering objective. We show that any fair clustering problem\ncan be decomposed into first finding good fairlets, and then using existing\nmachinery for traditional clustering algorithms. While finding good fairlets\ncan be NP-hard, we proceed to obtain efficient approximation algorithms based\non minimum cost flow. We empirically quantify the value of fair clustering on\nreal-world datasets with sensitive attributes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 19:52:49 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Chierichetti", "Flavio", ""], ["Kumar", "Ravi", ""], ["Lattanzi", "Silvio", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "1802.05756", "submitter": "C\\'edric B\\'eny", "authors": "C\\'edric B\\'eny", "title": "Inferring relevant features: from QFT to PCA", "comments": null, "journal-ref": "IJQI 16, 1840012 (2018)", "doi": "10.1142/S0219749918400129", "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many-body physics, renormalization techniques are used to extract aspects\nof a statistical or quantum state that are relevant at large scale, or for low\nenergy experiments. Recent works have proposed that these features can be\nformally identified as those perturbations of the states whose\ndistinguishability most resist coarse-graining. Here, we examine whether this\nsame strategy can be used to identify important features of an unlabeled\ndataset. This approach indeed results in a technique very similar to kernel PCA\n(principal component analysis), but with a kernel function that is\nautomatically adapted to the data, or \"learned\". We test this approach on\nhandwritten digits, and find that the most relevant features are significantly\nbetter for classification than those obtained from a simple gaussian kernel.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 06:24:04 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["B\u00e9ny", "C\u00e9dric", ""]]}, {"id": "1802.05757", "submitter": "Sebastian Claici", "authors": "Sebastian Claici and Edward Chien and Justin Solomon", "title": "Stochastic Wasserstein Barycenters", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic algorithm to compute the barycenter of a set of\nprobability distributions under the Wasserstein metric from optimal transport.\nUnlike previous approaches, our method extends to continuous input\ndistributions and allows the support of the barycenter to be adjusted in each\niteration. We tackle the problem without regularization, allowing us to recover\na sharp output whose support is contained within the support of the true\nbarycenter. We give examples where our algorithm recovers a more meaningful\nbarycenter than previous work. Our method is versatile and can be extended to\napplications such as generating super samples from a given distribution and\nrecovering blue noise approximations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 20:47:32 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 15:10:36 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 03:02:55 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Claici", "Sebastian", ""], ["Chien", "Edward", ""], ["Solomon", "Justin", ""]]}, {"id": "1802.05779", "submitter": "Walter Vinci", "authors": "Amir Khoshaman, Walter Vinci, Brandon Denis, Evgeny Andriyash, Hossein\n  Sadeghi, Mohammad H. Amin", "title": "Quantum Variational Autoencoder", "comments": "v2: published version. 13 pages, 3 figures, 2 tables", "journal-ref": "Quantum Sci. Technol. 4 (2019) 014001", "doi": "10.1088/2058-9565/aada1f", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) are powerful generative models with the\nsalient ability to perform inference. Here, we introduce a quantum variational\nautoencoder (QVAE): a VAE whose latent generative process is implemented as a\nquantum Boltzmann machine (QBM). We show that our model can be trained\nend-to-end by maximizing a well-defined loss-function: a 'quantum' lower-bound\nto a variational approximation of the log-likelihood. We use quantum Monte\nCarlo (QMC) simulations to train and evaluate the performance of QVAEs. To\nachieve the best performance, we first create a VAE platform with discrete\nlatent space generated by a restricted Boltzmann machine (RBM). Our model\nachieves state-of-the-art performance on the MNIST dataset when compared\nagainst similar approaches that only involve discrete variables in the\ngenerative process. We consider QVAEs with a smaller number of latent units to\nbe able to perform QMC simulations, which are computationally expensive. We\nshow that QVAEs can be trained effectively in regimes where quantum effects are\nrelevant despite training via the quantum bound. Our findings open the way to\nthe use of quantum computers to train QVAEs to achieve competitive performance\nfor generative models. Placing a QBM in the latent space of a VAE leverages the\nfull potential of current and next-generation quantum computers as sampling\ndevices.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 22:13:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 18:20:12 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Khoshaman", "Amir", ""], ["Vinci", "Walter", ""], ["Denis", "Brandon", ""], ["Andriyash", "Evgeny", ""], ["Sadeghi", "Hossein", ""], ["Amin", "Mohammad H.", ""]]}, {"id": "1802.05786", "submitter": "Papis Wongchaisuwat", "authors": "Papis Wongchaisuwat and Diego Klabjan", "title": "Truth Validation with Evidence", "comments": "40 pages (including Appendix), 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern era, abundant information is easily accessible from various\nsources, however only a few of these sources are reliable as they mostly\ncontain unverified contents. We develop a system to validate the truthfulness\nof a given statement together with underlying evidence. The proposed system\nprovides supporting evidence when the statement is tagged as false. Our work\nrelies on an inference method on a knowledge graph (KG) to identify the\ntruthfulness of statements. In order to extract the evidence of falseness, the\nproposed algorithm takes into account combined knowledge from KG and\nontologies. The system shows very good results as it provides valid and concise\nevidence. The quality of KG plays a role in the performance of the inference\nmethod which explicitly affects the performance of our evidence-extracting\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:01:04 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Wongchaisuwat", "Papis", ""], ["Klabjan", "Diego", ""]]}, {"id": "1802.05792", "submitter": "Fady Medhat", "authors": "Fady Medhat, David Chesmore, John Robinson", "title": "Masked Conditional Neural Networks for Automatic Sound Events\n  Recognition", "comments": "Restricted Boltzmann Machine, RBM, Conditional RBM, CRBM, Deep Belief\n  Net, DBN, Conditional Neural Network, CLNN, Masked Conditional Neural\n  Network, MCLNN, Environmental Sound Recognition, ESR", "journal-ref": "IEEE International Conference on Data Science and Advanced\n  Analytics (DSAA) Year: 2017, Pages: 389 - 394", "doi": "10.1109/DSAA.2017.43", "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network architectures designed for application domains other than\nsound, especially image recognition, may not optimally harness the\ntime-frequency representation when adapted to the sound recognition problem. In\nthis work, we explore the ConditionaL Neural Network (CLNN) and the Masked\nConditionaL Neural Network (MCLNN) for multi-dimensional temporal signal\nrecognition. The CLNN considers the inter-frame relationship, and the MCLNN\nenforces a systematic sparseness over the network's links to enable learning in\nfrequency bands rather than bins allowing the network to be frequency shift\ninvariant mimicking a filterbank. The mask also allows considering several\ncombinations of features concurrently, which is usually handcrafted through\nexhaustive manual search. We applied the MCLNN to the environmental sound\nrecognition problem using the ESC-10 and ESC-50 datasets. MCLNN achieved\ncompetitive performance, using 12% of the parameters and without augmentation,\ncompared to state-of-the-art Convolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:24:39 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 09:53:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Medhat", "Fady", ""], ["Chesmore", "David", ""], ["Robinson", "John", ""]]}, {"id": "1802.05799", "submitter": "Alexander Sergeev", "authors": "Alexander Sergeev and Mike Del Balso", "title": "Horovod: fast and easy distributed deep learning in TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training modern deep learning models requires large amounts of computation,\noften provided by GPUs. Scaling computation from one GPU to many can enable\nmuch faster training and research progress but entails two complications.\nFirst, the training library must support inter-GPU communication. Depending on\nthe particular methods employed, this communication may entail anywhere from\nnegligible to significant overhead. Second, the user must modify his or her\ntraining code to take advantage of inter-GPU communication. Depending on the\ntraining library's API, the modification required may be either significant or\nminimal.\n  Existing methods for enabling multi-GPU training under the TensorFlow library\nentail non-negligible communication overhead and require users to heavily\nmodify their model-building code, leading many researchers to avoid the whole\nmess and stick with slower single-GPU training. In this paper we introduce\nHorovod, an open source library that improves on both obstructions to scaling:\nit employs efficient inter-GPU communication via ring reduction and requires\nonly a few lines of modification to user code, enabling faster, easier\ndistributed training in TensorFlow. Horovod is available under the Apache 2.0\nlicense at https://github.com/uber/horovod\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:36:51 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 21:41:57 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 04:30:30 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sergeev", "Alexander", ""], ["Del Balso", "Mike", ""]]}, {"id": "1802.05800", "submitter": "Deboleena Roy", "authors": "Deboleena Roy, Priyadarshini Panda, Kaushik Roy", "title": "Tree-CNN: A Hierarchical Deep Convolutional Neural Network for\n  Incremental Learning", "comments": "8 pages, 6 figures, 7 tables Accepted in Neural Networks, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown\nremarkable performance in most computer vision tasks. These tasks traditionally\nuse a fixed dataset, and the model, once trained, is deployed as is. Adding new\ninformation to such a model presents a challenge due to complex training\nissues, such as \"catastrophic forgetting\", and sensitivity to hyper-parameter\ntuning. However, in this modern world, data is constantly evolving, and our\ndeep learning models are required to adapt to these changes. In this paper, we\npropose an adaptive hierarchical network structure composed of DCNNs that can\ngrow and learn as new data becomes available. The network grows in a tree-like\nfashion to accommodate new classes of data, while preserving the ability to\ndistinguish the previously trained classes. The network organizes the\nincrementally available data into feature-driven super-classes and improves\nupon existing hierarchical CNN models by adding the capability of self-growth.\nThe proposed hierarchical model, when compared against fine-tuning a deep\nnetwork, achieves significant reduction of training effort, while maintaining\ncompetitive accuracy on CIFAR-10 and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:36:56 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 18:45:12 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 15:58:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Roy", "Deboleena", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1802.05811", "submitter": "Ashok Cutkosky", "authors": "Ashok Cutkosky and Robert Busa-Fekete", "title": "Distributed Stochastic Optimization via Adaptive SGD", "comments": "NIPS 2018, 21 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic convex optimization algorithms are the most popular way to train\nmachine learning models on large-scale data. Scaling up the training process of\nthese models is crucial, but the most popular algorithm, Stochastic Gradient\nDescent (SGD), is a serial method that is surprisingly hard to parallelize. In\nthis paper, we propose an efficient distributed stochastic optimization method\nby combining adaptivity with variance reduction techniques. Our analysis yields\na linear speedup in the number of machines, constant memory footprint, and only\na logarithmic number of communication rounds. Critically, our approach is a\nblack-box reduction that parallelizes any serial online learning algorithm,\nstreamlining prior analysis and allowing us to leverage the significant\nprogress that has been made in designing adaptive algorithms. In particular, we\nachieve optimal convergence rates without any prior knowledge of smoothness\nparameters, yielding a more robust algorithm that reduces the need for\nhyperparameter tuning. We implement our algorithm in the Spark distributed\nframework and exhibit dramatic performance gains on large-scale logistic\nregression problems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 00:47:00 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 21:11:01 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 01:25:57 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cutkosky", "Ashok", ""], ["Busa-Fekete", "Robert", ""]]}, {"id": "1802.05814", "submitter": "Dawen Liang", "authors": "Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara", "title": "Variational Autoencoders for Collaborative Filtering", "comments": "10 pages, 3 figures. WWW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend variational autoencoders (VAEs) to collaborative filtering for\nimplicit feedback. This non-linear probabilistic model enables us to go beyond\nthe limited modeling capacity of linear factor models which still largely\ndominate collaborative filtering research.We introduce a generative model with\nmultinomial likelihood and use Bayesian inference for parameter estimation.\nDespite widespread use in language modeling and economics, the multinomial\nlikelihood receives less attention in the recommender systems literature. We\nintroduce a different regularization parameter for the learning objective,\nwhich proves to be crucial for achieving competitive performance. Remarkably,\nthere is an efficient way to tune the parameter using annealing. The resulting\nmodel and learning algorithm has information-theoretic connections to maximum\nentropy discrimination and the information bottleneck principle. Empirically,\nwe show that the proposed approach significantly outperforms several\nstate-of-the-art baselines, including two recently-proposed neural network\napproaches, on several real-world datasets. We also provide extended\nexperiments comparing the multinomial likelihood with other commonly used\nlikelihood functions in the latent factor collaborative filtering literature\nand show favorable results. Finally, we identify the pros and cons of employing\na principled Bayesian inference approach and characterize settings where it\nprovides the most significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 01:41:20 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Liang", "Dawen", ""], ["Krishnan", "Rahul G.", ""], ["Hoffman", "Matthew D.", ""], ["Jebara", "Tony", ""]]}, {"id": "1802.05821", "submitter": "Yuejie Chi", "authors": "Kaiyi Ji, Jian Tan, Jinfeng Xu, Yuejie Chi", "title": "Learning Latent Features with Pairwise Penalties in Low-Rank Matrix\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion has achieved great success in many real-world data\napplications. A matrix factorization model that learns latent features is\nusually employed and, to improve prediction performance, the similarities\nbetween latent variables can be exploited by pairwise learning using the graph\nregularized matrix factorization (GRMF) method. However, existing GRMF\napproaches often use the squared loss to measure the pairwise differences,\nwhich may be overly influenced by dissimilar pairs and lead to inferior\nprediction. To fully empower pairwise learning for matrix completion, we\npropose a general optimization framework that allows a rich class of\n(non-)convex pairwise penalty functions. A new and efficient algorithm is\ndeveloped to solve the proposed optimization problem, with a theoretical\nconvergence guarantee under mild assumptions. In an important situation where\nthe latent variables form a small number of subgroups, its statistical\nguarantee is also fully considered. In particular, we theoretically\ncharacterize the performance of the complexity-regularized maximum likelihood\nestimator, as a special case of our framework, which is shown to have smaller\nerrors when compared to the standard matrix completion framework without\npairwise penalties. We conduct extensive experiments on both synthetic and real\ndatasets to demonstrate the superior performance of this general framework.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 02:17:58 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 21:47:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ji", "Kaiyi", ""], ["Tan", "Jian", ""], ["Xu", "Jinfeng", ""], ["Chi", "Yuejie", ""]]}, {"id": "1802.05822", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, Aram Galstyan", "title": "Auto-Encoding Total Correlation Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in unsupervised learning enable reconstruction and generation of\nsamples from complex distributions, but this success is marred by the\ninscrutability of the representations learned. We propose an\ninformation-theoretic approach to characterizing disentanglement and dependence\nin representation learning using multivariate mutual information, also called\ntotal correlation. The principle of total Cor-relation Ex-planation (CorEx) has\nmotivated successful unsupervised learning applications across a variety of\ndomains, but under some restrictive assumptions. Here we relax those\nrestrictions by introducing a flexible variational lower bound to CorEx.\nSurprisingly, we find that this lower bound is equivalent to the one in\nvariational autoencoders (VAE) under certain conditions. This\ninformation-theoretic view of VAE deepens our understanding of hierarchical VAE\nand motivates a new algorithm, AnchorVAE, that makes latent codes more\ninterpretable through information maximization and enables generation of richer\nand more realistic samples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 02:33:25 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gao", "Shuyang", ""], ["Brekelmans", "Rob", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1802.05841", "submitter": "Cheng Li", "authors": "Cheng Li, David Rubin de Celis Leal, Santu Rana, Sunil Gupta,\n  Alessandra Sutti, Stewart Greenhill, Teo Slezak, Murray Height, Svetha\n  Venkatesh", "title": "Rapid Bayesian optimisation for synthesis of short polymer fiber\n  materials", "comments": "Scientific Report 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of processes for the synthesis of new materials involves many\ndecisions about process design, operation, and material properties.\nExperimentation is crucial but as complexity increases, exploration of\nvariables can become impractical using traditional combinatorial approaches. We\ndescribe an iterative method which uses machine learning to optimise process\ndevelopment, incorporating multiple qualitative and quantitative objectives. We\ndemonstrate the method with a novel fluid processing platform for synthesis of\nshort polymer fibers, and show how the synthesis process can be efficiently\ndirected to achieve material and process objectives.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 05:31:35 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Li", "Cheng", ""], ["Leal", "David Rubin de Celis", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Sutti", "Alessandra", ""], ["Greenhill", "Stewart", ""], ["Slezak", "Teo", ""], ["Height", "Murray", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1802.05842", "submitter": "Ian Covert", "authors": "Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, Emily Fox", "title": "Neural Granger Causality", "comments": "IEEE TPAMI accepted version", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3065601", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most classical approaches to Granger causality detection assume linear\ndynamics, many interactions in real-world applications, like neuroscience and\ngenomics, are inherently nonlinear. In these cases, using linear models may\nlead to inconsistent estimation of Granger causal interactions. We propose a\nclass of nonlinear methods by applying structured multilayer perceptrons (MLPs)\nor recurrent neural networks (RNNs) combined with sparsity-inducing penalties\non the weights. By encouraging specific sets of weights to be zero--in\nparticular, through the use of convex group-lasso penalties--we can extract the\nGranger causal structure. To further contrast with traditional approaches, our\nframework naturally enables us to efficiently capture long-range dependencies\nbetween series either via our RNNs or through an automatic lag selection in the\nMLP. We show that our neural Granger causality methods outperform\nstate-of-the-art nonlinear Granger causality methods on the DREAM3 challenge\ndata. This data consists of nonlinear gene expression and regulation time\ncourses with only a limited number of time points. The successes we show in\nthis challenging dataset provide a powerful example of how deep learning can be\nuseful in cases that go beyond prediction on large datasets. We likewise\nillustrate our methods in detecting nonlinear interactions in a human motion\ncapture dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 06:08:28 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 20:17:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tank", "Alex", ""], ["Covert", "Ian", ""], ["Foti", "Nicholas", ""], ["Shojaie", "Ali", ""], ["Fox", "Emily", ""]]}, {"id": "1802.05844", "submitter": "Kui Yu", "authors": "Kui Yu, Lin Liu, and Jiuyong Li", "title": "A Unified View of Causal and Non-causal Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop a unified view of causal and non-causal\nfeature selection methods. The unified view will fill in the gap in the\nresearch of the relation between the two types of methods. Based on the\nBayesian network framework and information theory, we first show that causal\nand non-causal feature selection methods share the same objective. That is to\nfind the Markov blanket of a class attribute, the theoretically optimal feature\nset for classification. We then examine the assumptions made by causal and\nnon-causal feature selection methods when searching for the optimal feature\nset, and unify the assumptions by mapping them to the restrictions on the\nstructure of the Bayesian network model of the studied problem. We further\nanalyze in detail how the structural assumptions lead to the different levels\nof approximations employed by the methods in their search, which then result in\nthe approximations in the feature sets found by the methods with respect to the\noptimal feature set. With the unified view, we are able to interpret the output\nof non-causal methods from a causal perspective and derive the error bounds of\nboth types of methods. Finally, we present practical understanding of the\nrelation between causal and non-causal methods using extensive experiments with\nsynthetic data and various types of real-word data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 06:18:06 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 23:49:40 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 06:38:53 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2018 03:45:56 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Yu", "Kui", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""]]}, {"id": "1802.05846", "submitter": "Guy Tennenholtz", "authors": "Guy Tennenholtz, Tom Zahavy, Shie Mannor", "title": "Train on Validation: Squeezing the Data Lemon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection on validation data is an essential step in machine learning.\nWhile the mixing of data between training and validation is considered taboo,\npractitioners often violate it to increase performance. Here, we offer a\nsimple, practical method for using the validation set for training, which\nallows for a continuous, controlled trade-off between performance and\noverfitting of model selection. We define the notion of\non-average-validation-stable algorithms as one in which using small portions of\nvalidation data for training does not overfit the model selection process. We\nthen prove that stable algorithms are also validation stable. Finally, we\ndemonstrate our method on the MNIST and CIFAR-10 datasets using stable\nalgorithms as well as state-of-the-art neural networks. Our results show\nsignificant increase in test performance with a minor trade-off in bias\nadmitted to the model selection process.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 06:50:19 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Tennenholtz", "Guy", ""], ["Zahavy", "Tom", ""], ["Mannor", "Shie", ""]]}, {"id": "1802.05872", "submitter": "Robert Palovics", "authors": "Andr\\'as A. Bencz\\'ur, Levente Kocsis and R\\'obert P\\'alovics", "title": "Online Machine Learning in Big Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of online machine learning in big data streams covers algorithms\nthat are (1) distributed and (2) work from data streams with only a limited\npossibility to store past data. The first requirement mostly concerns software\narchitectures and efficient algorithms. The second one also imposes nontrivial\ntheoretical restrictions on the modeling methods: In the data stream model,\nolder data is no longer available to revise earlier suboptimal modeling\ndecisions as the fresh data arrives.\n  In this article, we provide an overview of distributed software architectures\nand libraries as well as machine learning models for online learning. We\nhighlight the most important ideas for classification, regression,\nrecommendation, and unsupervised modeling from streaming data, and we show how\nthey are implemented in various distributed data stream processing systems.\n  This article is a reference material and not a survey. We do not attempt to\nbe comprehensive in describing all existing methods and solutions; rather, we\ngive pointers to the most important resources in the field. All related\nsub-fields, online algorithms, online learning, and distributed data processing\nare hugely dominant in current research and development with conceptually new\nresearch results and software components emerging at the time of writing. In\nthis article, we refer to several survey results, both for distributed data\nprocessing and for online machine learning. Compared to past surveys, our\narticle is different because we discuss recommender systems in extended detail.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 09:15:27 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bencz\u00far", "Andr\u00e1s A.", ""], ["Kocsis", "Levente", ""], ["P\u00e1lovics", "R\u00f3bert", ""]]}, {"id": "1802.05889", "submitter": "Chao Li", "authors": "Chao Li and Shohei Shimizu", "title": "Combining Linear Non-Gaussian Acyclic Model with Logistic Regression\n  Model for Estimating Causal Structure from Mixed Continuous and Discrete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal models from observational data is a crucial task in data\nanalysis. For continuous-valued data, Shimizu et al. have proposed a linear\nacyclic non-Gaussian model to understand the data generating process, and have\nshown that their model is identifiable when the number of data is sufficiently\nlarge. However, situations in which continuous and discrete variables coexist\nin the same problem are common in practice. Most existing causal discovery\nmethods either ignore the discrete data and apply a continuous-valued algorithm\nor discretize all the continuous data and then apply a discrete Bayesian\nnetwork approach. These methods possibly loss important information when we\nignore discrete data or introduce the approximation error due to\ndiscretization. In this paper, we define a novel hybrid causal model which\nconsists of both continuous and discrete variables. The model assumes: (1) the\nvalue of a continuous variable is a linear function of its parent variables\nplus a non-Gaussian noise, and (2) each discrete variable is a logistic\nvariable whose distribution parameters depend on the values of its parent\nvariables. In addition, we derive the BIC scoring function for model selection.\nThe new discovery algorithm can learn causal structures from mixed continuous\nand discrete data without discretization. We empirically demonstrate the power\nof our method through thorough simulations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 10:45:59 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Li", "Chao", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1802.05910", "submitter": "Steven Van Vaerenbergh", "authors": "Steven Van Vaerenbergh, Ignacio Santamaria, Victor Elvira, Matteo\n  Salvatori", "title": "Pattern Localization in Time Series through Signal-To-Model Alignment in\n  Latent Space", "comments": "IEEE ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of locating a predefined sequence of\npatterns in a time series. In particular, the studied scenario assumes a\ntheoretical model is available that contains the expected locations of the\npatterns. This problem is found in several contexts, and it is commonly solved\nby first synthesizing a time series from the model, and then aligning it to the\ntrue time series through dynamic time warping. We propose a technique that\nincreases the similarity of both time series before aligning them, by mapping\nthem into a latent correlation space. The mapping is learned from the data\nthrough a machine-learning setup. Experiments on data from non-destructive\ntesting demonstrate that the proposed approach shows significant improvements\nover the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:31:17 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 12:04:20 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Van Vaerenbergh", "Steven", ""], ["Santamaria", "Ignacio", ""], ["Elvira", "Victor", ""], ["Salvatori", "Matteo", ""]]}, {"id": "1802.05957", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida", "title": "Spectral Normalization for Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in the study of generative adversarial networks is the\ninstability of its training. In this paper, we propose a novel weight\nnormalization technique called spectral normalization to stabilize the training\nof the discriminator. Our new normalization technique is computationally light\nand easy to incorporate into existing implementations. We tested the efficacy\nof spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we\nexperimentally confirmed that spectrally normalized GANs (SN-GANs) is capable\nof generating images of better or equal quality relative to the previous\ntraining stabilization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 14:41:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Miyato", "Takeru", ""], ["Kataoka", "Toshiki", ""], ["Koyama", "Masanori", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1802.05968", "submitter": "James Stone Dr", "authors": "James V Stone", "title": "Information Theory: A Tutorial Introduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Shannon's mathematical theory of communication defines fundamental limits on\nhow much information can be transmitted between the different components of any\nman-made or biological system. This paper is an informal but rigorous\nintroduction to the main ideas implicit in Shannon's theory. An annotated\nreading list is provided for further reading.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:02:40 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 23:28:45 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 06:30:57 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Stone", "James V", ""]]}, {"id": "1802.05980", "submitter": "Marine Le Morvan", "authors": "Marine Le Morvan (CBIO), Jean-Philippe Vert (CBIO, DMA)", "title": "WHInter: A Working set algorithm for High-dimensional sparse second\n  order Interaction models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse linear models with two-way interactions is desirable in many\napplication domains such as genomics. l1-regularised linear models are popular\nto estimate sparse models, yet standard implementations fail to address\nspecifically the quadratic explosion of candidate two-way interactions in high\ndimensions, and typically do not scale to genetic data with hundreds of\nthousands of features. Here we present WHInter, a working set algorithm to\nsolve large l1-regularised problems with two-way interactions for binary design\nmatrices. The novelty of WHInter stems from a new bound to efficiently identify\nworking sets while avoiding to scan all features, and on fast computations\ninspired from solutions to the maximum inner product search problem. We apply\nWHInter to simulated and real genetic data and show that it is more scalable\nand two orders of magnitude faster than the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:36:07 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Morvan", "Marine Le", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO, DMA"]]}, {"id": "1802.05981", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis,\n  Antonis Nikitakis, Athanasios Voulodimos", "title": "Tensor-based Nonlinear Classifier for High-Order Data Analysis", "comments": "To appear in IEEE ICASSP 2018. arXiv admin note: text overlap with\n  arXiv:1709.08164", "journal-ref": null, "doi": "10.1109/ICASSP.2018.8461418", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a tensor-based nonlinear model for high-order data\nclassification. The advantages of the proposed scheme are that (i) it\nsignificantly reduces the number of weight parameters, and hence of required\ntraining samples, and (ii) it retains the spatial structure of the input\nsamples. The proposed model, called \\textit{Rank}-1 FNN, is based on a\nmodification of a feedforward neural network (FNN), such that its weights\nsatisfy the {\\it rank}-1 canonical decomposition. We also introduce a new\nlearning algorithm to train the model, and we evaluate the \\textit{Rank}-1 FNN\non third-order hyperspectral data. Experimental results and comparisons\nindicate that the proposed model outperforms state of the art classification\nmethods, including deep learning based ones, especially in cases with small\nnumbers of available training samples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:49:38 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""], ["Nikitakis", "Antonis", ""], ["Voulodimos", "Athanasios", ""]]}, {"id": "1802.05983", "submitter": "Hyunjik Kim", "authors": "Hyunjik Kim, Andriy Mnih", "title": "Disentangling by Factorising", "comments": "Shorter version appeared in Learning Disentangled Representations:\n  From Perception to Control workshop at NIPS, 2017:\n  https://sites.google.com/corp/view/disentanglenips2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and address the problem of unsupervised learning of disentangled\nrepresentations on data generated from independent factors of variation. We\npropose FactorVAE, a method that disentangles by encouraging the distribution\nof representations to be factorial and hence independent across the dimensions.\nWe show that it improves upon $\\beta$-VAE by providing a better trade-off\nbetween disentanglement and reconstruction quality. Moreover, we highlight the\nproblems of a commonly used disentanglement metric and introduce a new metric\nthat does not suffer from them.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:43:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 16:25:57 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 10:43:41 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kim", "Hyunjik", ""], ["Mnih", "Andriy", ""]]}, {"id": "1802.05992", "submitter": "Jakub \\'Swi\\k{a}tkowski", "authors": "Maciej Ja\\'skowski (1), Jakub \\'Swi\\k{a}tkowski (1), Micha{\\l}\n  Zaj\\k{a}c (1), Maciej Klimek (1), Jarek Potiuk (1), Piotr Rybicki (1), Piotr\n  Polatowski (1), Przemys{\\l}aw Walczyk (1), Kacper Nowicki (1), Marek Cygan (1\n  and 2) ((1) NoMagic.AI, (2) Institute of Informatics, University of Warsaw)", "title": "Improved GQ-CNN: Deep Learning Model for Planning Robust Grasps", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the field of robot grasping have shown great\nimprovements in the grasp success rates when dealing with unknown objects. In\nthis work we improve on one of the most promising approaches, the Grasp Quality\nConvolutional Neural Network (GQ-CNN) trained on the DexNet 2.0 dataset. We\npropose a new architecture for the GQ-CNN and describe practical improvements\nthat increase the model validation accuracy from 92.2% to 95.8% and from 85.9%\nto 88.0% on respectively image-wise and object-wise training and validation\nsplits.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:54:31 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Ja\u015bkowski", "Maciej", "", "NoMagic.AI"], ["\u015awi\u0105tkowski", "Jakub", "", "NoMagic.AI"], ["Zaj\u0105c", "Micha\u0142", "", "NoMagic.AI"], ["Klimek", "Maciej", "", "NoMagic.AI"], ["Potiuk", "Jarek", "", "NoMagic.AI"], ["Rybicki", "Piotr", "", "NoMagic.AI"], ["Polatowski", "Piotr", "", "NoMagic.AI"], ["Walczyk", "Przemys\u0142aw", "", "NoMagic.AI"], ["Nowicki", "Kacper", "", "NoMagic.AI"], ["Cygan", "Marek", "", "1\n  and 2"]]}, {"id": "1802.06009", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks", "title": "Dropout Model Evaluation in MOOCs", "comments": null, "journal-ref": "Eighth AAAI Symposium on Educational Advances in Artificial\n  Intelligence (EAAI-18), 2018", "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of learning analytics needs to adopt a more rigorous approach for\npredictive model evaluation that matches the complex practice of\nmodel-building. In this work, we present a procedure to statistically test\nhypotheses about model performance which goes beyond the state-of-the-practice\nin the community to analyze both algorithms and feature extraction methods from\nraw data. We apply this method to a series of algorithms and feature sets\nderived from a large sample of Massive Open Online Courses (MOOCs). While a\ncomplete comparison of all potential modeling approaches is beyond the scope of\nthis paper, we show that this approach reveals a large gap in dropout\nprediction performance between forum-, assignment-, and clickstream-based\nfeature extraction methods, where the latter is significantly better than the\nformer two, which are in turn indistinguishable from one another. This work has\nmethodological implications for evaluating predictive or AI-based models of\nstudent success, and practical implications for the design and targeting of\nat-risk student models and interventions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:13:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1802.06014", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Wei Wu, Yichen Zhu, Eric P. Xing", "title": "Orthogonality-Promoting Distance Metric Learning: Convex Relaxation and\n  Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML), which learns a distance metric from labeled\n\"similar\" and \"dissimilar\" data pairs, is widely utilized. Recently, several\nworks investigate orthogonality-promoting regularization (OPR), which\nencourages the projection vectors in DML to be close to being orthogonal, to\nachieve three effects: (1) high balancedness -- achieving comparable\nperformance on both frequent and infrequent classes; (2) high compactness --\nusing a small number of projection vectors to achieve a \"good\" metric; (3) good\ngeneralizability -- alleviating overfitting to training data. While showing\npromising results, these approaches suffer three problems. First, they involve\nsolving non-convex optimization problems where achieving the global optimal is\nNP-hard. Second, it lacks a theoretical understanding why OPR can lead to\nbalancedness. Third, the current generalization error analysis of OPR is not\ndirectly on the regularizer. In this paper, we address these three issues by\n(1) seeking convex relaxations of the original nonconvex problems so that the\nglobal optimal is guaranteed to be achievable; (2) providing a formal analysis\non OPR's capability of promoting balancedness; (3) providing a theoretical\nanalysis that directly reveals the relationship between OPR and generalization\nperformance. Experiments on various datasets demonstrate that our convex\nmethods are more effective in promoting balancedness, compactness, and\ngeneralization, and are computationally more efficient, compared with the\nnonconvex methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:28:22 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Xie", "Pengtao", ""], ["Wu", "Wei", ""], ["Zhu", "Yichen", ""], ["Xing", "Eric P.", ""]]}, {"id": "1802.06037", "submitter": "Angela Zhou", "authors": "Nathan Kallus and Angela Zhou", "title": "Policy Evaluation and Optimization with Continuous Treatments", "comments": "appearing at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of policy evaluation and learning from batched\ncontextual bandit data when treatments are continuous, going beyond previous\nwork on discrete treatments. Previous work for discrete treatment/action spaces\nfocuses on inverse probability weighting (IPW) and doubly robust (DR) methods\nthat use a rejection sampling approach for evaluation and the equivalent\nweighted classification problem for learning. In the continuous setting, this\nreduction fails as we would almost surely reject all observations. To tackle\nthe case of continuous treatments, we extend the IPW and DR approaches to the\ncontinuous setting using a kernel function that leverages treatment proximity\nto attenuate discrete rejection. Our policy estimator is consistent and we\ncharacterize the optimal bandwidth. The resulting continuous policy optimizer\n(CPO) approach using our estimator achieves convergent regret and approaches\nthe best-in-class policy for learnable policy classes. We demonstrate that the\nestimator performs well and, in particular, outperforms a discretization-based\nbenchmark. We further study the performance of our policy optimizer in a case\nstudy on personalized dosing based on a dataset of Warfarin patients, their\ncovariates, and final therapeutic doses. Our learned policy outperforms\nbenchmarks and nears the oracle-best linear policy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:23:02 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Kallus", "Nathan", ""], ["Zhou", "Angela", ""]]}, {"id": "1802.06052", "submitter": "Lin Chen", "authors": "Lin Chen, Hamed Hassani, Amin Karbasi", "title": "Online Continuous Submodular Maximization", "comments": "Accepted by AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an online optimization process, where the\nobjective functions are not convex (nor concave) but instead belong to a broad\nclass of continuous submodular functions. We first propose a variant of the\nFrank-Wolfe algorithm that has access to the full gradient of the objective\nfunctions. We show that it achieves a regret bound of $O(\\sqrt{T})$ (where $T$\nis the horizon of the online optimization problem) against a\n$(1-1/e)$-approximation to the best feasible solution in hindsight. However, in\nmany scenarios, only an unbiased estimate of the gradients are available. For\nsuch settings, we then propose an online stochastic gradient ascent algorithm\nthat also achieves a regret bound of $O(\\sqrt{T})$ regret, albeit against a\nweaker $1/2$-approximation to the best feasible solution in hindsight. We also\ngeneralize our results to $\\gamma$-weakly submodular functions and prove the\nsame sublinear regret bounds. Finally, we demonstrate the efficiency of our\nalgorithms on a few problem instances, including non-convex/non-concave\nquadratic programs, multilinear extensions of submodular set functions, and\nD-optimal design.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:56:48 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Chen", "Lin", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.06093", "submitter": "Phil Long", "authors": "Peter L. Bartlett, David P. Helmbold and Philip M. Long", "title": "Gradient descent with identity initialization efficiently learns\n  positive definite linear transformations by deep residual networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping\n$\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a\nfunction $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by\n$h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that\nlearn through gradient descent on the population quadratic loss in the case\nthat the distribution over the inputs is isotropic.\n  We provide polynomial bounds on the number of iterations for gradient descent\nto approximate the least squares matrix $\\Phi$, in the case where the initial\nhypothesis $\\Theta_1 = ... = \\Theta_L = I$ has excess loss bounded by a small\nenough constant. On the other hand, we show that gradient descent fails to\nconverge for $\\Phi$ whose distance from the identity is a larger constant, and\nwe show that some forms of regularization toward the identity in each layer do\nnot help.\n  If $\\Phi$ is symmetric positive definite, we show that an algorithm that\ninitializes $\\Theta_i = I$ learns an $\\epsilon$-approximation of $f$ using a\nnumber of updates polynomial in $L$, the condition number of $\\Phi$, and\n$\\log(d/\\epsilon)$. In contrast, we show that if the least squares matrix\n$\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class\nof algorithms that perform gradient descent with identity initialization, and\noptionally regularize toward the identity in each layer, fail to converge.\n  We analyze an algorithm for the case that $\\Phi$ satisfies $u^{\\top} \\Phi u >\n0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers:\none that maintains the invariant $u^{\\top} \\Theta_L \\Theta_{L-1} ... \\Theta_1 u\n> 0$ for all $u$, and another that \"balances\" $\\Theta_1, ..., \\Theta_L$ so that\nthey have the same singular values.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:24:29 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 15:48:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 16:10:40 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 16:46:26 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1802.06095", "submitter": "Saurabh Verma", "authors": "Saurabh Agrawal, Saurabh Verma, Gowtham Atluri, Anuj Karpatne, Stefan\n  Liess, Angus Macdonald III, Snigdhansu Chatterjee, Vipin Kumar", "title": "Mining Sub-Interval Relationships In Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data is being increasingly collected and stud- ied in several\nareas such as neuroscience, climate science, transportation, and social media.\nDiscovery of complex patterns of relationships between individual time-series,\nusing data-driven approaches can improve our understanding of real-world\nsystems. While traditional approaches typically study relationships between two\nentire time series, many interesting relationships in real-world applications\nexist in small sub-intervals of time while remaining absent or feeble during\nother sub-intervals. In this paper, we define the notion of a sub-interval\nrelationship (SIR) to capture inter- actions between two time series that are\nprominent only in certain sub-intervals of time. We propose a novel and\nefficient approach to find most interesting SIR in a pair of time series. We\nevaluate our proposed approach on two real-world datasets from climate science\nand neuroscience domain and demonstrated the scalability and computational\nefficiency of our proposed approach. We further evaluated our discovered SIRs\nbased on a randomization based procedure. Our results indicated the existence\nof several such relationships that are statistically significant, some of which\nwere also found to have physical interpretation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:30:19 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Agrawal", "Saurabh", ""], ["Verma", "Saurabh", ""], ["Atluri", "Gowtham", ""], ["Karpatne", "Anuj", ""], ["Liess", "Stefan", ""], ["Macdonald", "Angus", "III"], ["Chatterjee", "Snigdhansu", ""], ["Kumar", "Vipin", ""]]}, {"id": "1802.06104", "submitter": "Chuyang Ke", "authors": "Chuyang Ke, Jean Honorio", "title": "Information-theoretic Limits for Community Detection in Network Models", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS), 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the information-theoretic limits for the recovery of node labels\nin several network models. This includes the Stochastic Block Model, the\nExponential Random Graph Model, the Latent Space Model, the Directed\nPreferential Attachment Model, and the Directed Small-world Model. For the\nStochastic Block Model, the non-recoverability condition depends on the\nprobabilities of having edges inside a community, and between different\ncommunities. For the Latent Space Model, the non-recoverability condition\ndepends on the dimension of the latent space, and how far and spread are the\ncommunities in the latent space. For the Directed Preferential Attachment Model\nand the Directed Small-world Model, the non-recoverability condition depends on\nthe ratio between homophily and neighborhood size. We also consider dynamic\nversions of the Stochastic Block Model and the Latent Space Model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 20:09:16 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 00:29:35 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ke", "Chuyang", ""], ["Honorio", "Jean", ""]]}, {"id": "1802.06108", "submitter": "Ismael Tito Freire Gonz\\'alez", "authors": "Ismael T. Freire, Clement Moulin-Frier, Marti Sanchez-Fibla, Xerxes D.\n  Arsiwalla, Paul Verschure", "title": "Modeling the Formation of Social Conventions from Embodied Real-Time\n  Interactions", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the role of real-time control and learning in the formation of social\nconventions? To answer this question, we propose a computational model that\nmatches human behavioral data in a social decision-making game that was\nanalyzed both in discrete-time and continuous-time setups. Furthermore, unlike\nprevious approaches, our model takes into account the role of sensorimotor\ncontrol loops in embodied decision-making scenarios. For this purpose, we\nintroduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded\nin the Distributed Adaptive Control (DAC) theory of mind and brain, where\nlow-level sensorimotor control is modulated through perceptual and behavioral\nlearning in a layered structure. CRL follows these principles by implementing a\nfeedback control loop handling the agent's reactive behaviors (pre-wired\nreflexes), along with an adaptive layer that uses reinforcement learning to\nmaximize long-term reward. We test our model in a multi-agent game-theoretic\ntask in which coordination must be achieved to find an optimal solution. We\nshow that CRL is able to reach human-level performance on standard\ngame-theoretic metrics such as efficiency in acquiring rewards and fairness in\nreward distribution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 20:22:41 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 16:59:56 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 20:08:44 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Freire", "Ismael T.", ""], ["Moulin-Frier", "Clement", ""], ["Sanchez-Fibla", "Marti", ""], ["Arsiwalla", "Xerxes D.", ""], ["Verschure", "Paul", ""]]}, {"id": "1802.06132", "submitter": "James Stokes", "authors": "Tengyuan Liang, James Stokes", "title": "Interaction Matters: A Note on Non-asymptotic Local Convergence of\n  Generative Adversarial Networks", "comments": "To appear in the proceedings of the 22nd International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2019", "journal-ref": "The 22nd International Conference on Artificial Intelligence and\n  Statistics 89 (2019) 907-915", "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the pursuit of a systematic computational and algorithmic\nunderstanding of Generative Adversarial Networks (GANs), we present a simple\nyet unified non-asymptotic local convergence theory for smooth two-player\ngames, which subsumes several discrete-time gradient-based saddle point\ndynamics. The analysis reveals the surprising nature of the off-diagonal\ninteraction term as both a blessing and a curse. On the one hand, this\ninteraction term explains the origin of the slow-down effect in the convergence\nof Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other\nhand, for the unstable equilibria, exponential convergence can be proved thanks\nto the interaction term, for four modified dynamics proposed to stabilize GAN\ntraining: Optimistic Mirror Descent (OMD), Consensus Optimization (CO),\nImplicit Updates (IU) and Predictive Method (PM). The analysis uncovers the\nintimate connections among these stabilizing techniques, and provides detailed\ncharacterization on the choice of learning rate. As a by-product, we present a\nnew analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017]\nwith improved rates.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 21:30:19 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 20:26:16 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Stokes", "James", ""]]}, {"id": "1802.06153", "submitter": "Jeffrey Chan", "authors": "Jeffrey Chan, Valerio Perrone, Jeffrey P. Spence, Paul A. Jenkins,\n  Sara Mathieson, Yun S. Song", "title": "A Likelihood-Free Inference Framework for Population Genetic Data using\n  Exchangeable Neural Networks", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explosion of high-throughput DNA sequencing in the past decade has led to\na surge of interest in population-scale inference with whole-genome data.\nRecent work in population genetics has centered on designing inference methods\nfor relatively simple model classes, and few scalable general-purpose inference\ntechniques exist for more realistic, complex models. To achieve this, two\ninferential challenges need to be addressed: (1) population data are\nexchangeable, calling for methods that efficiently exploit the symmetries of\nthe data, and (2) computing likelihoods is intractable as it requires\nintegrating over a set of correlated, extremely high-dimensional latent\nvariables. These challenges are traditionally tackled by likelihood-free\nmethods that use scientific simulators to generate datasets and reduce them to\nhand-designed, permutation-invariant summary statistics, often leading to\ninaccurate inference. In this work, we develop an exchangeable neural network\nthat performs summary statistic-free, likelihood-free inference. Our framework\ncan be applied in a black-box fashion across a variety of simulation-based\ntasks, both within and outside biology. We demonstrate the power of our\napproach on the recombination hotspot testing problem, outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 22:36:16 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 02:25:35 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Chan", "Jeffrey", ""], ["Perrone", "Valerio", ""], ["Spence", "Jeffrey P.", ""], ["Jenkins", "Paul A.", ""], ["Mathieson", "Sara", ""], ["Song", "Yun S.", ""]]}, {"id": "1802.06167", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, Premkumar Natarajan", "title": "CapsuleGAN: Generative Adversarial Capsule Network", "comments": "To appear in Proceedings of ECCV Workshop on Brain Driven Computer\n  Vision (BDCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Generative Adversarial Capsule Network (CapsuleGAN), a framework\nthat uses capsule networks (CapsNets) instead of the standard convolutional\nneural networks (CNNs) as discriminators within the generative adversarial\nnetwork (GAN) setting, while modeling image data. We provide guidelines for\ndesigning CapsNet discriminators and the updated GAN objective function, which\nincorporates the CapsNet margin loss, for training CapsuleGAN models. We show\nthat CapsuleGAN outperforms convolutional-GAN at modeling image data\ndistribution on MNIST and CIFAR-10 datasets, evaluated on the generative\nadversarial metric and at semi-supervised image classification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 01:04:53 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 20:20:51 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 03:05:23 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 21:28:54 GMT"}, {"version": "v5", "created": "Mon, 6 Aug 2018 00:22:53 GMT"}, {"version": "v6", "created": "Tue, 25 Sep 2018 23:42:04 GMT"}, {"version": "v7", "created": "Tue, 2 Oct 2018 05:49:41 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Wu", "Yue", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1802.06179", "submitter": "Rafael Dos Santos De Oliveira", "authors": "Rafael Oliveira, Fernando H.M. Rocha, Lionel Ott, Vitor Guizilini,\n  Fabio Ramos and Valdir Grassi Jr", "title": "Learning to Race through Coordinate Descent Bayesian Optimisation", "comments": "Accepted as conference paper for the 2018 IEEE International\n  Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460735", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the automation of many kinds of processes, the observable outcome can\noften be described as the combined effect of an entire sequence of actions, or\ncontrols, applied throughout its execution. In these cases, strategies to\noptimise control policies for individual stages of the process might not be\napplicable, and instead the whole policy might have to be optimised at once. On\nthe other hand, the cost to evaluate the policy's performance might also be\nhigh, being desirable that a solution can be found with as few interactions as\npossible with the real system. We consider the problem of optimising control\npolicies to allow a robot to complete a given race track within a minimum\namount of time. We assume that the robot has no prior information about the\ntrack or its own dynamical model, just an initial valid driving example.\nLocalisation is only applied to monitor the robot and to provide an indication\nof its position along the track's centre axis. We propose a method for finding\na policy that minimises the time per lap while keeping the vehicle on the track\nusing a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert\nspace. We apply an algorithm to search more efficiently over high-dimensional\npolicy-parameter spaces with BO, by iterating over each dimension individually,\nin a sequential coordinate descent-like scheme. Experiments demonstrate the\nperformance of the algorithm against other methods in a simulated car racing\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 03:19:43 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Oliveira", "Rafael", ""], ["Rocha", "Fernando H. M.", ""], ["Ott", "Lionel", ""], ["Guizilini", "Vitor", ""], ["Ramos", "Fabio", ""], ["Grassi", "Valdir", "Jr"]]}, {"id": "1802.06182", "submitter": "Jong Wook Kim", "authors": "Jong Wook Kim, Justin Salamon, Peter Li, Juan Pablo Bello", "title": "CREPE: A Convolutional Representation for Pitch Estimation", "comments": "ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of estimating the fundamental frequency of a monophonic sound\nrecording, also known as pitch tracking, is fundamental to audio processing\nwith multiple applications in speech processing and music information\nretrieval. To date, the best performing techniques, such as the pYIN algorithm,\nare based on a combination of DSP pipelines and heuristics. While such\ntechniques perform very well on average, there remain many cases in which they\nfail to correctly estimate the pitch. In this paper, we propose a data-driven\npitch tracking algorithm, CREPE, which is based on a deep convolutional neural\nnetwork that operates directly on the time-domain waveform. We show that the\nproposed model produces state-of-the-art results, performing equally or better\nthan pYIN. Furthermore, we evaluate the model's generalizability in terms of\nnoise robustness. A pre-trained version of CREPE is made freely available as an\nopen-source Python module for easy application.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 03:50:11 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kim", "Jong Wook", ""], ["Salamon", "Justin", ""], ["Li", "Peter", ""], ["Bello", "Juan Pablo", ""]]}, {"id": "1802.06222", "submitter": "Houssam Zenati", "authors": "Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, Vijay\n  Ramaseshan Chandrasekhar", "title": "Efficient GAN-Based Anomaly Detection", "comments": "Updated version of this work is published at ICDM 2018, see\n  arXiv:1812.02288 . Submitted to the ICLR Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are able to model the complex\nhighdimensional distributions of real-world data, which suggests they could be\neffective for anomaly detection. However, few works have explored the use of\nGANs for the anomaly detection task. We leverage recently developed GAN models\nfor anomaly detection, and achieve state-of-the-art performance on image and\nnetwork intrusion datasets, while being several hundred-fold faster at test\ntime than the only published GAN-based method.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 11:26:53 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 21:54:16 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zenati", "Houssam", ""], ["Foo", "Chuan Sheng", ""], ["Lecouat", "Bruno", ""], ["Manek", "Gaurav", ""], ["Chandrasekhar", "Vijay Ramaseshan", ""]]}, {"id": "1802.06226", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Denny Wu, Yao-Hung Hubert Tsai, Ichiro Takeuchi, Ruslan\n  Salakhutdinov, Kenji Fukumizu", "title": "Post Selection Inference with Incomplete Maximum Mean Discrepancy\n  Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring divergence between two distributions is essential in machine\nlearning and statistics and has various applications including binary\nclassification, change point detection, and two-sample test. Furthermore, in\nthe era of big data, designing divergence measure that is interpretable and can\nhandle high-dimensional and complex data becomes extremely important. In the\npaper, we propose a post selection inference (PSI) framework for divergence\nmeasure, which can select a set of statistically significant features that\ndiscriminate two distributions. Specifically, we employ an additive variant of\nmaximum mean discrepancy (MMD) for features and introduce a general hypothesis\ntest for PSI. A novel MMD estimator using the incomplete U-statistics, which\nhas an asymptotically Normal distribution (under mild assumptions) and gives\nhigh detection power in PSI, is also proposed and analyzed theoretically.\nThrough synthetic and real-world feature selection experiments, we show that\nthe proposed framework can successfully detect statistically significant\nfeatures. Last, we propose a sample selection framework for analyzing different\nmembers in the Generative Adversarial Networks (GANs) family.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 11:48:02 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Yamada", "Makoto", ""], ["Wu", "Denny", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Takeuchi", "Ichiro", ""], ["Salakhutdinov", "Ruslan", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1802.06286", "submitter": "Yuanxin Li", "authors": "Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi", "title": "Nonconvex Matrix Factorization from Rank-One Measurements", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering low-rank matrices from random rank-one\nmeasurements, which spans numerous applications including covariance sketching,\nphase retrieval, quantum state tomography, and learning shallow polynomial\nneural networks, among others. Our approach is to directly estimate the\nlow-rank factor by minimizing a nonconvex quadratic loss function via vanilla\ngradient descent, following a tailored spectral initialization. When the true\nrank is small, this algorithm is guaranteed to converge to the ground truth (up\nto global ambiguity) with near-optimal sample complexity and computational\ncomplexity. To the best of our knowledge, this is the first guarantee that\nachieves near-optimality in both metrics. In particular, the key enabler of\nnear-optimal computational guarantees is an implicit regularization phenomenon:\nwithout explicit regularization, both spectral initialization and the gradient\ndescent iterates automatically stay within a region incoherent with the\nmeasurement vectors. This feature allows one to employ much more aggressive\nstep sizes compared with the ones suggested in prior literature, without the\nneed of sample splitting.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:30:47 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 05:37:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Yuanxin", ""], ["Ma", "Cong", ""], ["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""]]}, {"id": "1802.06287", "submitter": "Allon G. Percus", "authors": "Justin Sunu, Blake Hunter, Allon G. Percus", "title": "Unsupervised vehicle recognition using incremental reseeding of acoustic\n  signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle recognition and classification have broad applications, ranging from\ntraffic flow management to military target identification. We demonstrate an\nunsupervised method for automated identification of moving vehicles from\nroadside audio sensors. Using a short-time Fourier transform to decompose audio\nsignals, we treat the frequency signature in each time window as an individual\ndata point. We then use a spectral embedding for dimensionality reduction.\nBased on the leading eigenvectors, we relate the performance of an incremental\nreseeding algorithm to that of spectral clustering. We find that incremental\nreseeding accurately identifies individual vehicles using their acoustic\nsignatures.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:36:54 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sunu", "Justin", ""], ["Hunter", "Blake", ""], ["Percus", "Allon G.", ""]]}, {"id": "1802.06292", "submitter": "Fan Zhou", "authors": "Fan Zhou", "title": "Nonparametric Estimation of Low Rank Matrix Valued Function", "comments": null, "journal-ref": null, "doi": "10.1214/19-EJS1582", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A:[0,1]\\rightarrow\\mathbb{H}_m$ (the space of Hermitian matrices) be a\nmatrix valued function which is low rank with entries in H\\\"{o}lder class\n$\\Sigma(\\beta,L)$. The goal of this paper is to study statistical estimation of\n$A$ based on the regression model $\\mathbb{E}(Y_j|\\tau_j,X_j) = \\langle\nA(\\tau_j), X_j \\rangle,$ where $\\tau_j$ are i.i.d. uniformly distributed in\n$[0,1]$, $X_j$ are i.i.d. matrix completion sampling matrices, $Y_j$ are\nindependent bounded responses. We propose an innovative nuclear norm penalized\nlocal polynomial estimator and establish an upper bound on its point-wise risk\nmeasured by Frobenius norm. Then we extend this estimator globally and prove an\nupper bound on its integrated risk measured by $L_2$-norm. We also propose\nanother new estimator based on bias-reducing kernels to study the case when $A$\nis not necessarily low rank and establish an upper bound on its risk measured\nby $L_{\\infty}$-norm. We show that the obtained rates are all optimal up to\nsome logarithmic factor in minimax sense. Finally, we propose an adaptive\nestimation procedure based on Lepskii's method and model selection with data\nsplitting which is computationally efficient and can be easily implemented and\nparallelized.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:56:33 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 02:21:13 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 21:58:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Fan", ""]]}, {"id": "1802.06293", "submitter": "Ashok Cutkosky", "authors": "Ashok Cutkosky and Francesco Orabona", "title": "Black-Box Reductions for Parameter-free Online Learning in Banach Spaces", "comments": "Appears in Conference on Learning Theory 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several new black-box reductions that significantly improve the\ndesign of adaptive and parameter-free online learning algorithms by simplifying\nanalysis, improving regret guarantees, and sometimes even improving runtime. We\nreduce parameter-free online learning to online exp-concave optimization, we\nreduce optimization in a Banach space to one-dimensional optimization, and we\nreduce optimization over a constrained domain to unconstrained optimization.\nAll of our reductions run as fast as online gradient descent. We use our new\ntechniques to improve upon the previously best regret bounds for parameter-free\nlearning, and do so for arbitrary norms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:59:02 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 02:17:16 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Cutkosky", "Ashok", ""], ["Orabona", "Francesco", ""]]}, {"id": "1802.06300", "submitter": "Kaspar Wuthrich", "authors": "Victor Chernozhukov and Kaspar Wuthrich and Yinchu Zhu", "title": "Exact and Robust Conformal Inference Methods for Predictive Machine\n  Learning With Dependent Data", "comments": null, "journal-ref": "Proceedings of COLT 2018 (PMLR 75:732-749)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend conformal inference to general settings that allow for time series\ndata. Our proposal is developed as a randomization method and accounts for\npotential serial dependence by including block structures in the permutation\nscheme. As a result, the proposed method retains the exact, model-free validity\nwhen the data are i.i.d. or more generally exchangeable, similar to usual\nconformal inference methods. When exchangeability fails, as is the case for\ncommon time series data, the proposed approach is approximately valid under\nweak assumptions on the conformity score.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 21:43:28 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 20:17:34 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 09:16:27 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Wuthrich", "Kaspar", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1802.06307", "submitter": "Keith Levin", "authors": "Keith Levin, Farbod Roosta-Khorasani, Michael W. Mahoney and Carey E.\n  Priebe", "title": "Out-of-sample extension of graph adjacency spectral embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular dimensionality reduction procedures have out-of-sample\nextensions, which allow a practitioner to apply a learned embedding to\nobservations not seen in the initial training sample. In this work, we consider\nthe problem of obtaining an out-of-sample extension for the adjacency spectral\nembedding, a procedure for embedding the vertices of a graph into Euclidean\nspace. We present two different approaches to this problem, one based on a\nleast-squares objective and the other based on a maximum-likelihood\nformulation. We show that if the graph of interest is drawn according to a\ncertain latent position model called a random dot product graph, then both of\nthese out-of-sample extensions estimate the true latent position of the\nout-of-sample vertex with the same error rate. Further, we prove a central\nlimit theorem for the least-squares-based extension, showing that the estimate\nis asymptotically normal about the truth in the large-graph limit.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:21:20 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Levin", "Keith", ""], ["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1802.06308", "submitter": "Guang Cheng", "authors": "Meimei Liu and Zuofeng Shang and Guang Cheng", "title": "Nonparametric Testing under Random Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in nonparametric inference is its high computational\ncomplexity when data volume is large. In this paper, we develop computationally\nefficient nonparametric testing by employing a random projection strategy. In\nthe specific kernel ridge regression setup, a simple distance-based test\nstatistic is proposed. Notably, we derive the minimum number of random\nprojections that is sufficient for achieving testing optimality in terms of the\nminimax rate. An adaptive testing procedure is further established without\nprior knowledge of regularity. One technical contribution is to establish upper\nbounds for a range of tail sums of empirical kernel eigenvalues. Simulations\nand real data analysis are conducted to support our theory.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:22:34 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liu", "Meimei", ""], ["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1802.06309", "submitter": "Elliot Creager", "authors": "David Madras, Elliot Creager, Toniann Pitassi, Richard Zemel", "title": "Learning Adversarially Fair and Transferable Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advocate for representation learning as the key to\nmitigating unfair prediction outcomes downstream. Motivated by a scenario where\nlearned representations are used by third parties with unknown objectives, we\npropose and explore adversarial representation learning as a natural method of\nensuring those parties act fairly. We connect group fairness (demographic\nparity, equalized odds, and equal opportunity) to different adversarial\nobjectives. Through worst-case theoretical guarantees and experimental\nvalidation, we show that the choice of this objective is crucial to fair\nprediction. Furthermore, we present the first in-depth experimental\ndemonstration of fair transfer learning and demonstrate empirically that our\nlearned representations admit fair predictions on new tasks while maintaining\nutility, an essential goal of fair representation learning.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:38:29 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 17:37:44 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 17:01:59 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Madras", "David", ""], ["Creager", "Elliot", ""], ["Pitassi", "Toniann", ""], ["Zemel", "Richard", ""]]}, {"id": "1802.06355", "submitter": "Insu Han", "authors": "Insu Han, Haim Avron, Jinwoo Shin", "title": "Stochastic Chebyshev Gradient Descent for Spectral Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of machine learning techniques requires the solution of\noptimization problems involving spectral functions of parametric matrices, e.g.\nlog-determinant and nuclear norm. Unfortunately, computing the gradient of a\nspectral function is generally of cubic complexity, as such gradient descent\nmethods are rather expensive for optimizing objectives involving the spectral\nfunction. Thus, one naturally turns to stochastic gradient methods in hope that\nthey will provide a way to reduce or altogether avoid the computation of full\ngradients. However, here a new challenge appears: there is no straightforward\nway to compute unbiased stochastic gradients for spectral functions. In this\npaper, we develop unbiased stochastic gradients for spectral-sums, an important\nsubclass of spectral functions. Our unbiased stochastic gradients are based on\ncombining randomized trace estimators with stochastic truncation of the\nChebyshev expansions. A careful design of the truncation distribution allows us\nto offer distributions that are variance-optimal, which is crucial for fast and\nstable convergence of stochastic gradient methods. We further leverage our\nproposed stochastic gradients to devise stochastic methods for objective\nfunctions involving spectral-sums, and rigorously analyze their convergence\nrate. The utility of our methods is demonstrated in numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 09:15:56 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 16:04:16 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 14:46:55 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Han", "Insu", ""], ["Avron", "Haim", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1802.06357", "submitter": "Yunwen Lei", "authors": "Yunwen Lei and Ding-Xuan Zhou", "title": "Convergence of Online Mirror Descent", "comments": "Published in Applied and Computational Harmonic Analysis, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider online mirror descent (OMD) algorithms, a class of\nscalable online learning algorithms exploiting data geometric structures\nthrough mirror maps. Necessary and sufficient conditions are presented in terms\nof the step size sequence $\\{\\eta_t\\}_{t}$ for the convergence of an OMD\nalgorithm with respect to the expected Bregman distance induced by the mirror\nmap. The condition is $\\lim_{t\\to\\infty}\\eta_t=0,\n\\sum_{t=1}^{\\infty}\\eta_t=\\infty$ in the case of positive variances. It is\nreduced to $\\sum_{t=1}^{\\infty}\\eta_t=\\infty$ in the case of zero variances for\nwhich the linear convergence may be achieved by taking a constant step size\nsequence. A sufficient condition on the almost sure convergence is also given.\nWe establish tight error bounds under mild conditions on the mirror map, the\nloss function, and the regularizer. Our results are achieved by some novel\nanalysis on the one-step progress of the OMD algorithm using smoothness and\nstrong convexity of the mirror map and the loss function.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 09:36:09 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 05:44:02 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Lei", "Yunwen", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1802.06360", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (University of Sydney and Capital Markets\n  Cooperative Research Centre (CMCRC)), Aditya Krishna Menon (Data61/CSIRO and\n  the Australian National University), Sanjay Chawla (Qatar Computing Research\n  Institute (QCRI), HBKU)", "title": "Anomaly Detection using One-Class Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a one-class neural network (OC-NN) model to detect anomalies in\ncomplex data sets. OC-NN combines the ability of deep networks to extract a\nprogressively rich representation of data with the one-class objective of\ncreating a tight envelope around normal data. The OC-NN approach breaks new\nground for the following crucial reason: data representation in the hidden\nlayer is driven by the OC-NN objective and is thus customized for anomaly\ndetection. This is a departure from other approaches which use a hybrid\napproach of learning deep features using an autoencoder and then feeding the\nfeatures into a separate anomaly detection method like one-class SVM (OC-SVM).\nThe hybrid OC-SVM approach is sub-optimal because it is unable to influence\nrepresentational learning in the hidden layers. A comprehensive set of\nexperiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN\nperforms on par with state-of-the-art methods and outperformed conventional\nshallow methods in some scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 10:44:53 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 00:05:05 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Chalapathy", "Raghavendra", "", "University of Sydney and Capital Markets\n  Cooperative Research Centre"], ["Menon", "Aditya Krishna", "", "Data61/CSIRO and\n  the Australian National University"], ["Chawla", "Sanjay", "", "Qatar Computing Research\n  Institute"]]}, {"id": "1802.06368", "submitter": "Kento Nozawa", "authors": "Kento Nozawa, Masanari Kimura, Atsunori Kanemura", "title": "Node Centralities and Classification Performance for Characterizing Node\n  Embedding Algorithms", "comments": "Under review at ICLR 2018 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding graph nodes into a vector space can allow the use of machine\nlearning to e.g. predict node classes, but the study of node embedding\nalgorithms is immature compared to the natural language processing field\nbecause of a diverse nature of graphs. We examine the performance of node\nembedding algorithms with respect to graph centrality measures that\ncharacterize diverse graphs, through systematic experiments with four node\nembedding algorithms, four or five graph centralities, and six datasets.\nExperimental results give insights into the properties of node embedding\nalgorithms, which can be a basis for further research on this topic.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 13:03:08 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Nozawa", "Kento", ""], ["Kimura", "Masanari", ""], ["Kanemura", "Atsunori", ""]]}, {"id": "1802.06382", "submitter": "Yasuo Tabei", "authors": "Yasuo Tabei, Yoshihiro Yamanishi, Rasmus Pagh", "title": "Space-efficient Feature Maps for String Alignment Kernels", "comments": "Full version for ICDM'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String kernels are attractive data analysis tools for analyzing string data.\nAmong them, alignment kernels are known for their high prediction accuracies in\nstring classifications when tested in combination with SVM in various\napplications. However, alignment kernels have a crucial drawback in that they\nscale poorly due to their quadratic computation complexity in the number of\ninput strings, which limits large-scale applications in practice. We address\nthis need by presenting the first approximation for string alignment kernels,\nwhich we call space-efficient feature maps for edit distance with moves\n(SFMEDM), by leveraging a metric embedding named edit sensitive parsing (ESP)\nand feature maps (FMs) of random Fourier features (RFFs) for large-scale string\nanalyses. The original FMs for RFFs consume a huge amount of memory\nproportional to the dimension d of input vectors and the dimension D of output\nvectors, which prohibits its large-scale applications. We present novel\nspace-efficient feature maps (SFMs) of RFFs for a space reduction from O(dD) of\nthe original FMs to O(d) of SFMs with a theoretical guarantee with respect to\nconcentration bounds. We experimentally test SFMEDM on its ability to learn SVM\nfor large-scale string classifications with various massive string data, and we\ndemonstrate the superior performance of SFMEDM with respect to prediction\naccuracy, scalability and computation efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 14:16:28 GMT"}, {"version": "v10", "created": "Thu, 14 Nov 2019 02:42:51 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 06:29:59 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 14:37:53 GMT"}, {"version": "v4", "created": "Sun, 25 Mar 2018 05:46:27 GMT"}, {"version": "v5", "created": "Wed, 28 Mar 2018 01:33:34 GMT"}, {"version": "v6", "created": "Wed, 6 Feb 2019 08:45:03 GMT"}, {"version": "v7", "created": "Mon, 26 Aug 2019 08:36:23 GMT"}, {"version": "v8", "created": "Mon, 2 Sep 2019 07:04:07 GMT"}, {"version": "v9", "created": "Tue, 10 Sep 2019 08:15:08 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Tabei", "Yasuo", ""], ["Yamanishi", "Yoshihiro", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1802.06383", "submitter": "Florian Wenzel", "authors": "Florian Wenzel, Theo Galy-Fajou, Christan Donner, Marius Kloft,\n  Manfred Opper", "title": "Efficient Gaussian Process Classification Using Polya-Gamma Data\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable stochastic variational approach to GP classification\nbuilding on Polya-Gamma data augmentation and inducing points. Unlike former\napproaches, we obtain closed-form updates based on natural gradients that lead\nto efficient optimization. We evaluate the algorithm on real-world datasets\ncontaining up to 11 million data points and demonstrate that it is up to two\norders of magnitude faster than the state-of-the-art while being competitive in\nterms of prediction performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 14:20:54 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 10:43:32 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wenzel", "Florian", ""], ["Galy-Fajou", "Theo", ""], ["Donner", "Christan", ""], ["Kloft", "Marius", ""], ["Opper", "Manfred", ""]]}, {"id": "1802.06384", "submitter": "Luca Venturi", "authors": "Luca Venturi, Afonso S. Bandeira, Joan Bruna", "title": "Spurious Valleys in Two-layer Neural Network Optimization Landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks provide a rich class of high-dimensional, non-convex\noptimization problems. Despite their non-convexity, gradient-descent methods\noften successfully optimize these models. This has motivated a recent spur in\nresearch attempting to characterize properties of their loss surface that may\nexplain such success.\n  In this paper, we address this phenomenon by studying a key topological\nproperty of the loss: the presence or absence of spurious valleys, defined as\nconnected components of sub-level sets that do not include a global minimum.\nFocusing on a class of two-layer neural networks defined by smooth (but\ngenerally non-linear) activation functions, we identify a notion of intrinsic\ndimension and show that it provides necessary and sufficient conditions for the\nabsence of spurious valleys. More concretely, finite intrinsic dimension\nguarantees that for sufficiently overparametrised models no spurious valleys\nexist, independently of the data distribution. Conversely, infinite intrinsic\ndimension implies that spurious valleys do exist for certain data\ndistributions, independently of model overparametrisation. Besides these\npositive and negative results, we show that, although spurious valleys may\nexist in general, they are confined to low risk levels and avoided with high\nprobability on overparametrised models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 14:23:44 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:54:56 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 13:39:18 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2020 02:17:45 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Venturi", "Luca", ""], ["Bandeira", "Afonso S.", ""], ["Bruna", "Joan", ""]]}, {"id": "1802.06394", "submitter": "Fabian Gieseke", "authors": "Fabian Gieseke and Christian Igel", "title": "Training Big Random Forests with Little Resources", "comments": "9 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without access to large compute clusters, building random forests on large\ndatasets is still a challenging problem. This is, in particular, the case if\nfully-grown trees are desired. We propose a simple yet effective framework that\nallows to efficiently construct ensembles of huge trees for hundreds of\nmillions or even billions of training instances using a cheap desktop computer\nwith commodity hardware. The basic idea is to consider a multi-level\nconstruction scheme, which builds top trees for small random subsets of the\navailable data and which subsequently distributes all training instances to the\ntop trees' leaves for further processing. While being conceptually simple, the\noverall efficiency crucially depends on the particular implementation of the\ndifferent phases. The practical merits of our approach are demonstrated using\ndense datasets with hundreds of millions of training instances.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 15:56:28 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Gieseke", "Fabian", ""], ["Igel", "Christian", ""]]}, {"id": "1802.06398", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov and Ivan Oseledets", "title": "HybridSVD: When Collaborative Information is Not Enough", "comments": "accepted as a long paper at ACM RecSys 2019; 9 pages, 2 figures, 2\n  tables", "journal-ref": null, "doi": "10.1145/3298689.3347055", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new hybrid algorithm that allows incorporating both user and\nitem side information within the standard collaborative filtering technique.\nOne of its key features is that it naturally extends a simple PureSVD approach\nand inherits its unique advantages, such as highly efficient Lanczos-based\noptimization procedure, simplified hyper-parameter tuning and a quick\nfolding-in computation for generating recommendations instantly even in highly\ndynamic online environments. The algorithm utilizes a generalized formulation\nof the singular value decomposition, which adds flexibility to the solution and\nallows imposing the desired structure on its latent space. Conveniently, the\nresulting model also admits an efficient and straightforward solution for the\ncold start scenario. We evaluate our approach on a diverse set of datasets and\nshow its superiority over similar classes of hybrid models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 16:39:01 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 13:45:53 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 18:30:37 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 10:03:18 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1802.06402", "submitter": "Caiwen Ding", "authors": "Yanzhi Wang, Caiwen Ding, Zhe Li, Geng Yuan, Siyu Liao, Xiaolong Ma,\n  Bo Yuan, Xuehai Qian, Jian Tang, Qinru Qiu, Xue Lin", "title": "Towards Ultra-High Performance and Energy Efficiency of Deep Learning\n  Systems: An Algorithm-Hardware Co-Optimization Framework", "comments": "6 figures, AAAI Conference on Artificial Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerations of deep learning systems have been extensively\ninvestigated in industry and academia. The aim of this paper is to achieve\nultra-high energy efficiency and performance for hardware implementations of\ndeep neural networks (DNNs). An algorithm-hardware co-optimization framework is\ndeveloped, which is applicable to different DNN types, sizes, and application\nscenarios. The algorithm part adopts the general block-circulant matrices to\nachieve a fine-grained tradeoff between accuracy and compression ratio. It\napplies to both fully-connected and convolutional layers and contains a\nmathematically rigorous proof of the effectiveness of the method. The proposed\nalgorithm reduces computational complexity per layer from O($n^2$) to O($n\\log\nn$) and storage complexity from O($n^2$) to O($n$), both for training and\ninference. The hardware part consists of highly efficient Field Programmable\nGate Array (FPGA)-based implementations using effective reconfiguration, batch\nprocessing, deep pipelining, resource re-using, and hierarchical control.\nExperimental results demonstrate that the proposed framework achieves at least\n152X speedup and 71X energy efficiency gain compared with IBM TrueNorth\nprocessor under the same test accuracy. It achieves at least 31X energy\nefficiency gain compared with the reference FPGA-based work.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 16:51:04 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Wang", "Yanzhi", ""], ["Ding", "Caiwen", ""], ["Li", "Zhe", ""], ["Yuan", "Geng", ""], ["Liao", "Siyu", ""], ["Ma", "Xiaolong", ""], ["Yuan", "Bo", ""], ["Qian", "Xuehai", ""], ["Tang", "Jian", ""], ["Qiu", "Qinru", ""], ["Lin", "Xue", ""]]}, {"id": "1802.06403", "submitter": "Jinsung Yoon", "authors": "Jinsung Yoon, James Jordon, Mihaela van der Schaar", "title": "RadialGAN: Leveraging multiple datasets to improve target-specific\n  predictive models using Generative Adversarial Networks", "comments": "12 pages, 8 figures, 2018 International Conference of Machine\n  Learning (2018 ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training complex machine learning models for prediction often requires a\nlarge amount of data that is not always readily available. Leveraging these\nexternal datasets from related but different sources is therefore an important\ntask if good predictive models are to be built for deployment in settings where\ndata can be rare. In this paper we propose a novel approach to the problem in\nwhich we use multiple GAN architectures to learn to translate from one dataset\nto another, thereby allowing us to effectively enlarge the target dataset, and\ntherefore learn better predictive models than if we simply used the target\ndataset. We show the utility of such an approach, demonstrating that our method\nimproves the prediction performance on the target domain over using just the\ntarget dataset and also show that our framework outperforms several other\nbenchmarks on a collection of real-world medical datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 16:55:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 23:14:31 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Yoon", "Jinsung", ""], ["Jordon", "James", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1802.06412", "submitter": "Florian Kreyssig", "authors": "Florian Kreyssig, Chao Zhang, Philip Woodland", "title": "Improved TDNNs using Deep Kernels and Frequency Dependent Grid-RNNs", "comments": "5 pages, 3 figures, 2 tables, to appear in 2018 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time delay neural networks (TDNNs) are an effective acoustic model for large\nvocabulary speech recognition. The strength of the model can be attributed to\nits ability to effectively model long temporal contexts. However, current TDNN\nmodels are relatively shallow, which limits the modelling capability. This\npaper proposes a method of increasing the network depth by deepening the kernel\nused in the TDNN temporal convolutions. The best performing kernel consists of\nthree fully connected layers with a residual (ResNet) connection from the\noutput of the first to the output of the third. The addition of\nspectro-temporal processing as the input to the TDNN in the form of a\nconvolutional neural network (CNN) and a newly designed Grid-RNN was\ninvestigated. The Grid-RNN strongly outperforms a CNN if different sets of\nparameters for different frequency bands are used and can be further enhanced\nby using a bi-directional Grid-RNN. Experiments using the multi-genre broadcast\n(MGB3) English data (275h) show that deep kernel TDNNs reduces the word error\nrate (WER) by 6% relative and when combined with the frequency dependent\nGrid-RNN gives a relative WER reduction of 9%.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 17:54:19 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 14:07:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kreyssig", "Florian", ""], ["Zhang", "Chao", ""], ["Woodland", "Philip", ""]]}, {"id": "1802.06416", "submitter": "Zhenqiang Su", "authors": "Yongxi Tan, Jin Yang, Xin Chen, Qitao Song, Yunjun Chen, Zhangxiang\n  Ye, Zhenqiang Su", "title": "Sim-to-Real Optimization of Complex Real World Mobile Network with\n  Imperfect Information via Deep Reinforcement Learning from Self-play", "comments": "Accepted by NIPS 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile network that millions of people use every day is one of the most\ncomplex systems in the world. Optimization of mobile network to meet exploding\ncustomer demand and reduce capital/operation expenditures poses great\nchallenges. Despite recent progress, application of deep reinforcement learning\n(DRL) to complex real world problem still remains unsolved, given data\nscarcity, partial observability, risk and complex rules/dynamics in real world,\nas well as the huge reality gap between simulation and real world. To bridge\nthe reality gap, we introduce a Sim-to-Real framework to directly transfer\nlearning from simulation to real world via graph convolutional neural network\n(CNN) - by abstracting partially observable mobile network into graph, then\ndistilling domain-variant irregular graph into domain-invariant tensor in\nlocally Euclidean space as input to CNN -, domain randomization and multi-task\nlearning. We use a novel self-play mechanism to encourage competition among DRL\nagents for best record on multiple tasks via simulated annealing, just like\nathletes compete for world record in decathlon. We also propose a decentralized\nmulti-agent, competitive and cooperative DRL method to coordinate the actions\nof multi-cells to maximize global reward and minimize negative impact to\nneighbor cells. Using 6 field trials on commercial mobile networks, we\ndemonstrate for the first time that a DRL agent can successfully transfer\nlearning from simulation to complex real world problem with imperfect\ninformation, complex rules/dynamics, huge state/action space, and multi-agent\ninteractions, without any training in the real world.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 18:03:39 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 23:45:16 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 12:09:16 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Tan", "Yongxi", ""], ["Yang", "Jin", ""], ["Chen", "Xin", ""], ["Song", "Qitao", ""], ["Chen", "Yunjun", ""], ["Ye", "Zhangxiang", ""], ["Su", "Zhenqiang", ""]]}, {"id": "1802.06428", "submitter": "Fengyi Tang", "authors": "Fengyi Tang, Kaixiang Lin, Ikechukwu Uchendu, Hiroko H. Dodge, Jiayu\n  Zhou", "title": "Improving Mild Cognitive Impairment Prediction via Reinforcement\n  Learning and Dialogue Simulation", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild cognitive impairment (MCI) is a prodromal phase in the progression from\nnormal aging to dementia, especially Alzheimers disease. Even though there is\nmild cognitive decline in MCI patients, they have normal overall cognition and\nthus is challenging to distinguish from normal aging. Using transcribed data\nobtained from recorded conversational interactions between participants and\ntrained interviewers, and applying supervised learning models to these data, a\nrecent clinical trial has shown a promising result in differentiating MCI from\nnormal aging. However, the substantial amount of interactions with medical\nstaff can still incur significant medical care expenses in practice. In this\npaper, we propose a novel reinforcement learning (RL) framework to train an\nefficient dialogue agent on existing transcripts from clinical trials.\nSpecifically, the agent is trained to sketch disease-specific lexical\nprobability distribution, and thus to converse in a way that maximizes the\ndiagnosis accuracy and minimizes the number of conversation turns. We evaluate\nthe performance of the proposed reinforcement learning framework on the MCI\ndiagnosis from a real clinical trial. The results show that while using only a\nfew turns of conversation, our framework can significantly outperform\nstate-of-the-art supervised learning approaches.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 19:27:24 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Tang", "Fengyi", ""], ["Lin", "Kaixiang", ""], ["Uchendu", "Ikechukwu", ""], ["Dodge", "Hiroko H.", ""], ["Zhou", "Jiayu", ""]]}, {"id": "1802.06432", "submitter": "Fady Medhat", "authors": "Fady Medhat, David Chesmore, John Robinson", "title": "Music Genre Classification using Masked Conditional Neural Networks", "comments": "Conditional Neural Networks (CLNN), Masked Conditional Neural\n  Networks (MCLNN), Conditional Restricted Boltzmann Machine (CRBM), Deep\n  Belief Nets (DBN), Music Information Retrieval (MIR)", "journal-ref": "International Conference on Neural Information Processing (ICONIP)\n  Year: 2017, Pages: 470-481", "doi": "10.1007/978-3-319-70096-0_49", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ConditionaL Neural Networks (CLNN) and the Masked ConditionaL Neural\nNetworks (MCLNN) exploit the nature of multi-dimensional temporal signals. The\nCLNN captures the conditional temporal influence between the frames in a window\nand the mask in the MCLNN enforces a systematic sparseness that follows a\nfilterbank-like pattern over the network links. The mask induces the network to\nlearn about time-frequency representations in bands, allowing the network to\nsustain frequency shifts. Additionally, the mask in the MCLNN automates the\nexploration of a range of feature combinations, usually done through an\nexhaustive manual search. We have evaluated the MCLNN performance using the\nBallroom and Homburg datasets of music genres. MCLNN has achieved accuracies\nthat are competitive to state-of-the-art handcrafted attempts in addition to\nmodels based on Convolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 19:55:09 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 18:14:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Medhat", "Fady", ""], ["Chesmore", "David", ""], ["Robinson", "John", ""]]}, {"id": "1802.06439", "submitter": "Maxim Raginsky", "authors": "Belinda Tzen and Tengyuan Liang and Maxim Raginsky", "title": "Local Optimality and Generalization Guarantees for the Langevin\n  Algorithm via Empirical Metastability", "comments": "19 pages", "journal-ref": "Proceedings of the 31st Conference on Learning Theory 75 (2018)\n  857-875", "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detailed path-wise behavior of the discrete-time Langevin\nalgorithm for non-convex Empirical Risk Minimization (ERM) through the lens of\nmetastability, adopting some techniques from Berglund and Gentz (2003.\n  For a particular local optimum of the empirical risk, with an arbitrary\ninitialization, we show that, with high probability, at least one of the\nfollowing two events will occur: (1) the Langevin trajectory ends up somewhere\noutside the $\\varepsilon$-neighborhood of this particular optimum within a\nshort recurrence time; (2) it enters this $\\varepsilon$-neighborhood by the\nrecurrence time and stays there until a potentially exponentially long escape\ntime. We call this phenomenon empirical metastability.\n  This two-timescale characterization aligns nicely with the existing\nliterature in the following two senses. First, the effective recurrence time\n(i.e., number of iterations multiplied by stepsize) is dimension-independent,\nand resembles the convergence time of continuous-time deterministic Gradient\nDescent (GD). However unlike GD, the Langevin algorithm does not require strong\nconditions on local initialization, and has the possibility of eventually\nvisiting all optima. Second, the scaling of the escape time is consistent with\nthe Eyring-Kramers law, which states that the Langevin scheme will eventually\nvisit all local minima, but it will take an exponentially long time to transit\namong them. We apply this path-wise concentration result in the context of\nstatistical learning to examine local notions of generalization and optimality.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 20:34:06 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 16:00:36 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Tzen", "Belinda", ""], ["Liang", "Tengyuan", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1802.06441", "submitter": "Christopher Chamberland", "authors": "Christopher Chamberland, Pooya Ronagh", "title": "Deep neural decoders for near term fault-tolerant experiments", "comments": "26 pages, 8 tables, 28 figures. Comments welcome! v2 includes fixes\n  to minor typos and expanded discussions", "journal-ref": "Quantum Science and Technology, vol. 3, no. 4, p. 044002, (2018)", "doi": "10.1088/2058-9565/aad1f7", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding efficient decoders for quantum error correcting codes adapted to\nrealistic experimental noise in fault-tolerant devices represents a significant\nchallenge. In this paper we introduce several decoding algorithms complemented\nby deep neural decoders and apply them to analyze several fault-tolerant error\ncorrection protocols such as the surface code as well as Steane and Knill error\ncorrection. Our methods require no knowledge of the underlying noise model\nafflicting the quantum device making them appealing for real-world experiments.\nOur analysis is based on a full circuit-level noise model. It considers both\ndistance-three and five codes, and is performed near the codes pseudo-threshold\nregime. Training deep neural decoders in low noise rate regimes appears to be a\nchallenging machine learning endeavour. We provide a detailed description of\nour neural network architectures and training methodology. We then discuss both\nthe advantages and limitations of deep neural decoders. Lastly, we provide a\nrigorous analysis of the decoding runtime of trained deep neural decoders and\ncompare our methods with anticipated gate times in future quantum devices.\nGiven the broad applications of our decoding schemes, we believe that the\nmethods presented in this paper could have practical applications for near term\nfault-tolerant experiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 20:48:06 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 23:49:49 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Chamberland", "Christopher", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1802.06455", "submitter": "Hossein Azizpour", "authors": "Mattias Teye, Hossein Azizpour, Kevin Smith", "title": "Bayesian Uncertainty Estimation for Batch Normalized Deep Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that training a deep network using batch normalization is equivalent\nto approximate inference in Bayesian models. We further demonstrate that this\nfinding allows us to make meaningful estimates of the model uncertainty using\nconventional architectures, without modifications to the network or the\ntraining procedure. Our approach is thoroughly validated by measuring the\nquality of uncertainty in a series of empirical experiments on different tasks.\nIt outperforms baselines with strong statistical significance, and displays\ncompetitive performance with recent Bayesian approaches.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:21:36 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 14:56:19 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Teye", "Mattias", ""], ["Azizpour", "Hossein", ""], ["Smith", "Kevin", ""]]}, {"id": "1802.06458", "submitter": "Jayaraman J. Thiagarajan", "authors": "Deepta Rajan and Jayaraman J. Thiagarajan", "title": "A Generative Modeling Approach to Limited Channel ECG Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing temporal sequences is central to a variety of applications in\nhealth care, and in particular multi-channel Electrocardiogram (ECG) is a\nhighly prevalent diagnostic modality that relies on robust sequence modeling.\nWhile Recurrent Neural Networks (RNNs) have led to significant advances in\nautomated diagnosis with time-series data, they perform poorly when models are\ntrained using a limited set of channels. A crucial limitation of existing\nsolutions is that they rely solely on discriminative models, which tend to\ngeneralize poorly in such scenarios. In order to combat this limitation, we\ndevelop a generative modeling approach to limited channel ECG classification.\nThis approach first uses a Seq2Seq model to implicitly generate the missing\nchannel information, and then uses the latent representation to perform the\nactual supervisory task. This decoupling enables the use of unsupervised data\nand also provides highly robust metric spaces for subsequent discriminative\nlearning. Our experiments with the Physionet dataset clearly evidence the\neffectiveness of our approach over standard RNNs in disease prediction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:29:31 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 01:53:25 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 01:32:04 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Rajan", "Deepta", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1802.06463", "submitter": "Haoyu Fu", "authors": "Haoyu Fu, Yuejie Chi, Yingbin Liang", "title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross\n  Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study model recovery for data classification, where the training labels\nare generated from a one-hidden-layer neural network with sigmoid activations,\nalso known as a single-layer feedforward network, and the goal is to recover\nthe weights of the neural network. We consider two network models, the\nfully-connected network (FCN) and the non-overlapping convolutional neural\nnetwork (CNN). We prove that with Gaussian inputs, the empirical risk based on\ncross entropy exhibits strong convexity and smoothness {\\em uniformly} in a\nlocal neighborhood of the ground truth, as soon as the sample complexity is\nsufficiently large. This implies that if initialized in this neighborhood,\ngradient descent converges linearly to a critical point that is provably close\nto the ground truth. Furthermore, we show such an initialization can be\nobtained via the tensor method. This establishes the global convergence\nguarantee for empirical risk minimization using cross entropy via gradient\ndescent for learning one-hidden-layer neural networks, at the near-optimal\nsample and computational complexity with respect to the network input dimension\nwithout unrealistic assumptions such as requiring a fresh set of samples at\neach iteration.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:49:56 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 04:40:20 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 04:09:34 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fu", "Haoyu", ""], ["Chi", "Yuejie", ""], ["Liang", "Yingbin", ""]]}, {"id": "1802.06476", "submitter": "Bin Liu", "authors": "Bin Liu, Ying Li, Soumya Ghosh, Zhaonan Sun, Kenney Ng and Jianying Hu", "title": "Simultaneous Modeling of Multiple Complications for Risk Profiling in\n  Diabetes Care", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2019", "doi": "10.1109/TKDE.2019.2904060", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in\nmultiple complications. Risk prediction and profiling of T2DM complications is\ncritical for healthcare professionals to design personalized treatment plans\nfor patients in diabetes care for improved outcomes. In this paper, we study\nthe risk of developing complications after the initial T2DM diagnosis from\nlongitudinal patient records. We propose a novel multi-task learning approach\nto simultaneously model multiple complications where each task corresponds to\nthe risk modeling of one complication. Specifically, the proposed method\nstrategically captures the relationships (1) between the risks of multiple T2DM\ncomplications, (2) between the different risk factors, and (3) between the risk\nfactor selection patterns. The method uses coefficient shrinkage to identify an\ninformative subset of risk factors from high-dimensional data, and uses a\nhierarchical Bayesian framework to allow domain knowledge to be incorporated as\npriors. The proposed method is favorable for healthcare applications because in\nadditional to improved prediction performance, relationships among the\ndifferent risks and risk factors are also identified. Extensive experimental\nresults on a large electronic medical claims database show that the proposed\nmethod outperforms state-of-the-art models by a significant margin.\nFurthermore, we show that the risk associations learned and the risk factors\nidentified lead to meaningful clinical insights.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:01:33 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Liu", "Bin", ""], ["Li", "Ying", ""], ["Ghosh", "Soumya", ""], ["Sun", "Zhaonan", ""], ["Ng", "Kenney", ""], ["Hu", "Jianying", ""]]}, {"id": "1802.06480", "submitter": "Qingkai Liang", "authors": "Qingkai Liang, Fanyu Que, Eytan Modiano", "title": "Accelerated Primal-Dual Policy Optimization for Safe Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Markov Decision Process (CMDP) is a natural framework for\nreinforcement learning tasks with safety constraints, where agents learn a\npolicy that maximizes the long-term reward while satisfying the constraints on\nthe long-term cost. A canonical approach for solving CMDPs is the primal-dual\nmethod which updates parameters in primal and dual spaces in turn. Existing\nmethods for CMDPs only use on-policy data for dual updates, which results in\nsample inefficiency and slow convergence. In this paper, we propose a policy\nsearch method for CMDPs called Accelerated Primal-Dual Optimization (APDO),\nwhich incorporates an off-policy trained dual variable in the dual update\nprocedure while updating the policy in primal space with on-policy likelihood\nratio gradient. Experimental results on a simulated robot locomotion task show\nthat APDO achieves better sample efficiency and faster convergence than\nstate-of-the-art approaches for CMDPs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:25:26 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liang", "Qingkai", ""], ["Que", "Fanyu", ""], ["Modiano", "Eytan", ""]]}, {"id": "1802.06485", "submitter": "Arun Sai Suggala", "authors": "Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, Pradeep\n  Ravikumar", "title": "Robust Estimation via Robust Gradient Estimation", "comments": "48 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new computationally-efficient class of estimators for risk\nminimization. We show that these estimators are robust for general statistical\nmodels: in the classical Huber epsilon-contamination model and in heavy-tailed\nsettings. Our workhorse is a novel robust variant of gradient descent, and we\nprovide conditions under which our gradient descent variant provides accurate\nestimators in a general convex risk minimization problem. We provide specific\nconsequences of our theory for linear regression, logistic regression and for\nestimation of the canonical parameters in an exponential family. These results\nprovide some of the first computationally tractable and provably robust\nestimators for these canonical statistical models. Finally, we study the\nempirical performance of our proposed methods on synthetic and real datasets,\nand find that our methods convincingly outperform a variety of baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:49:31 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 16:07:30 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Prasad", "Adarsh", ""], ["Suggala", "Arun Sai", ""], ["Balakrishnan", "Sivaraman", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1802.06501", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao and Liang Zhang and Zhuoye Ding and Long Xia and Jiliang\n  Tang and Dawei Yin", "title": "Recommendations with Negative Feedback via Pairwise Deep Reinforcement\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.00209", "journal-ref": null, "doi": "10.1145/3219819.3219886", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedback. Users' feedback can be positive and negative and both\ntypes of feedback have great potentials to boost recommendations. However, the\nnumber of negative feedback is much larger than that of positive one; thus\nincorporating them simultaneously is challenging since positive feedback could\nbe buried by negative one. In this paper, we develop a novel approach to\nincorporate them into the proposed deep recommender system (DEERS) framework.\nThe experimental results based on real-world e-commerce data demonstrate the\neffectiveness of the proposed framework. Further experiments have been\nconducted to understand the importance of both positive and negative feedback\nin recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:30:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 11:49:04 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 02:33:08 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Zhang", "Liang", ""], ["Ding", "Zhuoye", ""], ["Xia", "Long", ""], ["Tang", "Jiliang", ""], ["Yin", "Dawei", ""]]}, {"id": "1802.06516", "submitter": "Mengying Sun", "authors": "Mengying Sun, Inci M. Baytas, Liang Zhan, Zhangyang Wang, Jiayu Zhou", "title": "Subspace Network: Deep Multi-Task Censored Regression for Modeling\n  Neurodegenerative Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade a wide spectrum of machine learning models have been\ndeveloped to model the neurodegenerative diseases, associating biomarkers,\nespecially non-intrusive neuroimaging markers, with key clinical scores\nmeasuring the cognitive status of patients. Multi-task learning (MTL) has been\ncommonly utilized by these studies to address high dimensionality and small\ncohort size challenges. However, most existing MTL approaches are based on\nlinear models and suffer from two major limitations: 1) they cannot explicitly\nconsider upper/lower bounds in these clinical scores; 2) they lack the\ncapability to capture complicated non-linear interactions among the variables.\nIn this paper, we propose Subspace Network, an efficient deep modeling approach\nfor non-linear multi-task censored regression. Each layer of the subspace\nnetwork performs a multi-task censored regression to improve upon the\npredictions from the last layer via sketching a low-dimensional subspace to\nperform knowledge transfer among learning tasks. Under mild assumptions, for\neach layer the parametric subspace can be recovered using only one pass of\ntraining data. Empirical results demonstrate that the proposed subspace network\nquickly picks up the correct parameter subspaces, and outperforms\nstate-of-the-arts in predicting neurodegenerative clinical scores using\ninformation in brain imaging.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 04:50:14 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 04:34:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Sun", "Mengying", ""], ["Baytas", "Inci M.", ""], ["Zhan", "Liang", ""], ["Wang", "Zhangyang", ""], ["Zhou", "Jiayu", ""]]}, {"id": "1802.06526", "submitter": "Daniel Rugeles", "authors": "Daniel Rugeles, Zhen Hai, Gao Cong and Manoranjan Dash", "title": "Heron Inference for Bayesian Graphical Models", "comments": "9 pages, 6 Tables and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian graphical models have been shown to be a powerful tool for\ndiscovering uncertainty and causal structure from real-world data in many\napplication fields. Current inference methods primarily follow different kinds\nof trade-offs between computational complexity and predictive accuracy. At one\nend of the spectrum, variational inference approaches perform well in\ncomputational efficiency, while at the other end, Gibbs sampling approaches are\nknown to be relatively accurate for prediction in practice. In this paper, we\nextend an existing Gibbs sampling method, and propose a new deterministic Heron\ninference (Heron) for a family of Bayesian graphical models. In addition to the\nsupport for nontrivial distributability, one more benefit of Heron is that it\nis able to not only allow us to easily assess the convergence status but also\nlargely improve the running efficiency. We evaluate Heron against the standard\ncollapsed Gibbs sampler and state-of-the-art state augmentation method in\ninference for well-known graphical models. Experimental results using publicly\navailable real-life data have demonstrated that Heron significantly outperforms\nthe baseline methods for inferring Bayesian graphical models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 05:52:47 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Rugeles", "Daniel", ""], ["Hai", "Zhen", ""], ["Cong", "Gao", ""], ["Dash", "Manoranjan", ""]]}, {"id": "1802.06552", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, John Bradshaw, Yash Sharma", "title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rising interest in studying the robustness of deep neural network\nclassifiers against adversaries, with both advanced attack and defence\ntechniques being actively developed. However, most recent work focuses on\ndiscriminative classifiers, which only model the conditional distribution of\nthe labels given the inputs. In this paper, we propose and investigate the deep\nBayes classifier, which improves classical naive Bayes with conditional deep\ngenerative models. We further develop detection methods for adversarial\nexamples, which reject inputs with low likelihood under the generative model.\nExperimental results suggest that deep Bayes classifiers are more robust than\ndeep discriminative classifiers, and that the proposed detection methods are\neffective against many recently proposed attacks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:58:00 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 17:07:51 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 10:54:08 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Li", "Yingzhen", ""], ["Bradshaw", "John", ""], ["Sharma", "Yash", ""]]}, {"id": "1802.06640", "submitter": "Boris Sharchilev", "authors": "Boris Sharchilev, Yury Ustinovsky, Pavel Serdyukov, Maarten de Rijke", "title": "Finding Influential Training Samples for Gradient Boosted Decision Trees", "comments": "Added the \"Acknowledgements\" section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding influential training samples for a\nparticular case of tree ensemble-based models, e.g., Random Forest (RF) or\nGradient Boosted Decision Trees (GBDT). A natural way of formalizing this\nproblem is studying how the model's predictions change upon leave-one-out\nretraining, leaving out each individual training sample. Recent work has shown\nthat, for parametric models, this analysis can be conducted in a\ncomputationally efficient way. We propose several ways of extending this\nframework to non-parametric GBDT ensembles under the assumption that tree\nstructures remain fixed. Furthermore, we introduce a general scheme of\nobtaining further approximations to our method that balance the trade-off\nbetween performance and computational complexity. We evaluate our approaches on\nvarious experimental setups and use-case scenarios and demonstrate both the\nquality of our approach to finding influential training samples in comparison\nto the baselines and its computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 14:19:40 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 19:12:03 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Sharchilev", "Boris", ""], ["Ustinovsky", "Yury", ""], ["Serdyukov", "Pavel", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1802.06677", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng, Jiangchao Yao, Ya Zhang, Ivor W. Tsang", "title": "Degeneration in VAE: in the Light of Fisher Information Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While enormous progress has been made to Variational Autoencoder (VAE) in\nrecent years, similar to other deep networks, VAE with deep networks suffers\nfrom the problem of degeneration, which seriously weakens the correlation\nbetween the input and the corresponding latent codes, deviating from the goal\nof the representation learning. To investigate how degeneration affects VAE\nfrom a theoretical perspective, we illustrate the information transmission in\nVAE and analyze the intermediate layers of the encoders/decoders. Specifically,\nwe propose a Fisher Information measure for the layer-wise analysis. With such\nmeasure, we demonstrate that information loss is ineluctable in feed-forward\nnetworks and causes the degeneration in VAE. We show that skip connections in\nVAE enable the preservation of information without changing the model\narchitecture. We call this class of VAE equipped with skip connections as SCVAE\nand perform a range of experiments to show its advantages in information\npreservation and degeneration mitigation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:45:36 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 08:06:16 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 00:53:57 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Zheng", "Huangjie", ""], ["Yao", "Jiangchao", ""], ["Zhang", "Ya", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "1802.06678", "submitter": "Roi Naveiro", "authors": "Roi Naveiro, Sim\\'on Rodr\\'iguez, David R\\'ios Insua", "title": "Large Scale Automated Forecasting for Monitoring Network Safety and\n  Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time large scale streaming data pose major challenges to forecasting, in\nparticular defying the presence of human experts to perform the corresponding\nanalysis. We present here a class of models and methods used to develop an\nautomated, scalable and versatile system for large scale forecasting oriented\ntowards safety and security monitoring. Our system provides short and long term\nforecasts and uses them to detect safety and security issues in relation with\nmultiple internet connected devices well in advance they might take place.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:50:02 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:16:43 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Naveiro", "Roi", ""], ["Rodr\u00edguez", "Sim\u00f3n", ""], ["Insua", "David R\u00edos", ""]]}, {"id": "1802.06739", "submitter": "Liyang Xie", "authors": "Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, Jiayu Zhou", "title": "Differentially Private Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Network (GAN) and its variants have recently attracted\nintensive research interests due to their elegant theoretical foundation and\nexcellent empirical performance as generative models. These tools provide a\npromising direction in the studies where data availability is limited. One\ncommon issue in GANs is that the density of the learned generative distribution\ncould concentrate on the training data points, meaning that they can easily\nremember training samples due to the high model complexity of deep networks.\nThis becomes a major concern when GANs are applied to private or sensitive data\nsuch as patient medical records, and the concentration of distribution may\ndivulge critical patient information. To address this issue, in this paper we\npropose a differentially private GAN (DPGAN) model, in which we achieve\ndifferential privacy in GANs by adding carefully designed noise to gradients\nduring the learning procedure. We provide rigorous proof for the privacy\nguarantee, as well as comprehensive empirical evidence to support our analysis,\nwhere we demonstrate that our method can generate high quality data points at a\nreasonable privacy level.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 18:05:33 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Xie", "Liyang", ""], ["Lin", "Kaixiang", ""], ["Wang", "Shu", ""], ["Wang", "Fei", ""], ["Zhou", "Jiayu", ""]]}, {"id": "1802.06765", "submitter": "Samuel Ainsworth", "authors": "Samuel Ainsworth, Nicholas Foti, Adrian KC Lee, Emily Fox", "title": "Interpretable VAEs for nonlinear group factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have recently yielded encouraging results in producing\nsubjectively realistic samples of complex data. Far less attention has been\npaid to making these generative models interpretable. In many scenarios,\nranging from scientific applications to finance, the observed variables have a\nnatural grouping. It is often of interest to understand systems of interaction\namongst these groups, and latent factor models (LFMs) are an attractive\napproach. However, traditional LFMs are limited by assuming a linear\ncorrelation structure. We present an output interpretable VAE (oi-VAE) for\ngrouped data that models complex, nonlinear latent-to-observed relationships.\nWe combine a structured VAE comprised of group-specific generators with a\nsparsity-inducing prior. We demonstrate that oi-VAE yields meaningful notions\nof interpretability in the analysis of motion capture and MEG data. We further\nshow that in these situations, the regularization inherent to oi-VAE can\nactually lead to improved generalization and learned generative processes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 01:47:13 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Ainsworth", "Samuel", ""], ["Foti", "Nicholas", ""], ["Lee", "Adrian KC", ""], ["Fox", "Emily", ""]]}, {"id": "1802.06807", "submitter": "Utkarsh Upadhyay", "authors": "Utkarsh Upadhyay, Abir De, Aasish Pappu, Manuel Gomez-Rodriguez", "title": "On the Complexity of Opinions and Online Discussions", "comments": "Proceedings of 12th ACM International Conference on Web Search and\n  Data Mining", "journal-ref": null, "doi": "10.1145/3289600.3290965", "report-no": null, "categories": "cs.SI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an increasingly polarized world, demagogues who reduce complexity down to\nsimple arguments based on emotion are gaining in popularity. Are opinions and\nonline discussions falling into demagoguery? In this work, we aim to provide\ncomputational tools to investigate this question and, by doing so, explore the\nnature and complexity of online discussions and their space of opinions,\nuncovering where each participant lies.\n  More specifically, we present a modeling framework to construct latent\nrepresentations of opinions in online discussions which are consistent with\nhuman judgements, as measured by online voting. If two opinions are close in\nthe resulting latent space of opinions, it is because humans think they are\nsimilar. Our modeling framework is theoretically grounded and establishes a\nsurprising connection between opinions and voting models and the sign-rank of a\nmatrix. Moreover, it also provides a set of practical algorithms to both\nestimate the dimension of the latent space of opinions and infer where opinions\nexpressed by the participants of an online discussion lie in this space.\nExperiments on a large dataset from Yahoo! News, Yahoo! Finance, Yahoo! Sports,\nand the Newsroom app suggest that unidimensional opinion models may often be\nunable to accurately represent online discussions, provide insights into human\njudgements and opinions, and show that our framework is able to circumvent\nlanguage nuances such as sarcasm or humor by relying on human judgements\ninstead of textual analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:02:49 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 16:42:48 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Upadhyay", "Utkarsh", ""], ["De", "Abir", ""], ["Pappu", "Aasish", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1802.06820", "submitter": "Austin Benson", "authors": "Austin R. Benson", "title": "Tools for higher-order network analysis", "comments": "Ph.D. Thesis, Stanford University, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.stat-mech cs.NA physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a fundamental model of complex systems throughout the sciences,\nand network datasets are typically analyzed through lower-order connectivity\npatterns described at the level of individual nodes and edges. However,\nhigher-order connectivity patterns captured by small subgraphs, also called\nnetwork motifs, describe the fundamental structures that control and mediate\nthe behavior of many complex systems. We develop three tools for network\nanalysis that use higher-order connectivity patterns to gain new insights into\nnetwork datasets: (1) a framework to cluster nodes into modules based on joint\nparticipation in network motifs; (2) a generalization of the clustering\ncoefficient measurement to investigate higher-order closure patterns; and (3) a\ndefinition of network motifs for temporal networks and fast algorithms for\ncounting them. Using these tools, we analyze data from biology, ecology,\neconomics, neuroscience, online social networks, scientific collaborations,\ntelecommunications, transportation, and the World Wide Web.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:26:48 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Benson", "Austin R.", ""]]}, {"id": "1802.06823", "submitter": "Jaroslaw Zola", "authors": "Frank Schoeneman, Varun Chandola, Nils Napp, Olga Wodo, Jaroslaw Zola", "title": "Entropy-Isomap: Manifold Learning for High-dimensional Dynamic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific and engineering processes deliver massive high-dimensional data\nsets that are generated as non-linear transformations of an initial state and\nfew process parameters. Mapping such data to a low-dimensional manifold\nfacilitates better understanding of the underlying processes, and enables their\noptimization. In this paper, we first show that off-the-shelf non-linear\nspectral dimensionality reduction methods, e.g., Isomap, fail for such data,\nprimarily due to the presence of strong temporal correlations. Then, we propose\na novel method, Entropy-Isomap, to address the issue. The proposed method is\nsuccessfully applied to large data describing a fabrication process of organic\nmaterials. The resulting low-dimensional representation correctly captures\nprocess control variables, allows for low-dimensional visualization of the\nmaterial morphology evolution, and provides key insights to improve the\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:39:59 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 09:48:28 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Schoeneman", "Frank", ""], ["Chandola", "Varun", ""], ["Napp", "Nils", ""], ["Wodo", "Olga", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1802.06847", "submitter": "Mihaela Rosca", "authors": "Mihaela Rosca, Balaji Lakshminarayanan, Shakir Mohamed", "title": "Distribution Matching in Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasingly widespread deployment of generative models, there is a\nmounting need for a deeper understanding of their behaviors and limitations. In\nthis paper, we expose the limitations of Variational Autoencoders (VAEs), which\nconsistently fail to learn marginal distributions in both latent and visible\nspaces. We show this to be a consequence of learning by matching conditional\ndistributions, and the limitations of explicit model and posterior\ndistributions. It is popular to consider Generative Adversarial Networks (GANs)\nas a means of overcoming these limitations, leading to hybrids of VAEs and\nGANs. We perform a large-scale evaluation of several VAE-GAN hybrids and\nanalyze the implications of class probability estimation for learning\ndistributions. While promising, we conclude that at present, VAE-GAN hybrids\nhave limited applicability: they are harder to scale, evaluate, and use for\ninference compared to VAEs; and they do not improve over the generation quality\nof GANs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 20:59:33 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 20:40:54 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 22:08:35 GMT"}, {"version": "v4", "created": "Mon, 10 Jun 2019 20:36:10 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Rosca", "Mihaela", ""], ["Lakshminarayanan", "Balaji", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1802.06875", "submitter": "Benjamin Cowen", "authors": "Benjamin Cowen, Apoorva Nandini Saridena, Anna Choromanska", "title": "LSALSA: Accelerated Source Separation via Learned Sparse Coding", "comments": "ECML-PKDD 2019 via journal track; Special Issue Mach Learn (2019)", "journal-ref": null, "doi": "10.1007/s10994-019-05812-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for the generalized sparse coding (SC)\ninference problem. The proposed framework applies to both the single dictionary\nsetting, where each data point is represented as a sparse combination of the\ncolumns of one dictionary matrix, as well as the multiple dictionary setting as\ngiven in morphological component analysis (MCA), where the goal is to separate\na signal into additive parts such that each part has distinct sparse\nrepresentation within a corresponding dictionary. Both the SC task and its\ngeneralization via MCA have been cast as $\\ell_1$-regularized least-squares\noptimization problems. To accelerate traditional acquisition of sparse codes,\nwe propose a deep learning architecture that constitutes a trainable\ntime-unfolded version of the Split Augmented Lagrangian Shrinkage Algorithm\n(SALSA), a special case of the Alternating Direction Method of Multipliers\n(ADMM). We empirically validate both variants of the algorithm, that we refer\nto as LSALSA (learned-SALSA), on image vision tasks and demonstrate that at\ninference our networks achieve vast improvements in terms of the running time,\nthe quality of estimated sparse codes, and visual clarity on both classic SC\nand MCA problems. Finally, we present a theoretical framework for analyzing\nLSALSA network: we show that the proposed approach exactly implements a\ntruncated ADMM applied to a new, learned cost function with curvature modified\nby one of the learned parameterized matrices. We extend a very recent\nStochastic Alternating Optimization analysis framework to show that a gradient\ndescent step along this learned loss landscape is equivalent to a modified\ngradient descent step along the original loss landscape. In this framework, the\nacceleration achieved by LSALSA could potentially be explained by the network's\nability to learn a correction to the gradient direction of steeper descent.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:10:00 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 16:04:14 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Cowen", "Benjamin", ""], ["Saridena", "Apoorva Nandini", ""], ["Choromanska", "Anna", ""]]}, {"id": "1802.06894", "submitter": "Kejun Huang", "authors": "Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos", "title": "Learning Hidden Markov Models from Pairwise Co-occurrences with\n  Application to Topic Modeling", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for identifying the transition and emission\nprobabilities of a hidden Markov model (HMM) from the emitted data.\nExpectation-maximization becomes computationally prohibitive for long\nobservation records, which are often required for identification. The new\nalgorithm is particularly suitable for cases where the available sample size is\nlarge enough to accurately estimate second-order output probabilities, but not\nhigher-order ones. We show that if one is only able to obtain a reliable\nestimate of the pairwise co-occurrence probabilities of the emissions, it is\nstill possible to uniquely identify the HMM if the emission probability is\n\\emph{sufficiently scattered}. We apply our method to hidden topic Markov\nmodeling, and demonstrate that we can learn topics with higher quality if\ndocuments are modeled as observations of HMMs sharing the same emission (topic)\nprobability, compared to the simple but widely used bag-of-words model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 22:33:56 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:15:46 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Huang", "Kejun", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1802.06901", "submitter": "Jason Lee", "authors": "Jason Lee, Elman Mansimov, Kyunghyun Cho", "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative\n  Refinement", "comments": "Accepted to EMNLP'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a conditional non-autoregressive neural sequence model based on\niterative refinement. The proposed model is designed based on the principles of\nlatent variable models and denoising autoencoders, and is generally applicable\nto any sequence generation task. We extensively evaluate the proposed model on\nmachine translation (En-De and En-Ro) and image caption generation, and observe\nthat it significantly speeds up decoding while maintaining the generation\nquality comparable to the autoregressive counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 22:57:54 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 21:20:55 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 18:00:08 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Lee", "Jason", ""], ["Mansimov", "Elman", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1802.06903", "submitter": "Yi Zhou", "authors": "Yi Zhou and Yingbin Liang and Huishuai Zhang", "title": "Generalization Error Bounds with Probabilistic Guarantee for SGD in\n  Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning has led to a rising interest in the\ngeneralization property of the stochastic gradient descent (SGD) method, and\nstability is one popular approach to study it. Existing works based on\nstability have studied nonconvex loss functions, but only considered the\ngeneralization error of the SGD in expectation. In this paper, we establish\nvarious generalization error bounds with probabilistic guarantee for the SGD.\nSpecifically, for both general nonconvex loss functions and gradient dominant\nloss functions, we characterize the on-average stability of the iterates\ngenerated by SGD in terms of the on-average variance of the stochastic\ngradients. Such characterization leads to improved bounds for the\ngeneralization error for SGD. We then study the regularized risk minimization\nproblem with strongly convex regularizers, and obtain improved generalization\nerror bounds for proximal SGD. With strongly convex regularizers, we further\nestablish the generalization error bounds for nonconvex loss functions under\nproximal SGD with high-probability guarantee, i.e., exponential concentration\nin probability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 23:04:20 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 19:13:12 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 03:30:18 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Zhang", "Huishuai", ""]]}, {"id": "1802.06916", "submitter": "Austin Benson", "authors": "Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, Jon\n  Kleinberg", "title": "Simplicial Closure and higher-order link prediction", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences Nov 2018, 115 (48)\n  E11221-E11230", "doi": "10.1073/pnas.1800683115", "report-no": null, "categories": "cs.SI cond-mat.stat-mech math.AT physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks provide a powerful formalism for modeling complex systems by using a\nmodel of pairwise interactions. But much of the structure within these systems\ninvolves interactions that take place among more than two nodes at once; for\nexample, communication within a group rather than person-to person,\ncollaboration among a team rather than a pair of coauthors, or biological\ninteraction between a set of molecules rather than just two. Such higher-order\ninteractions are ubiquitous, but their empirical study has received limited\nattention, and little is known about possible organizational principles of such\nstructures. Here we study the temporal evolution of 19 datasets with explicit\naccounting for higher-order interactions. We show that there is a rich variety\nof structure in our datasets but datasets from the same system types have\nconsistent patterns of higher-order structure. Furthermore, we find that tie\nstrength and edge density are competing positive indicators of higher-order\norganization, and these trends are consistent across interactions involving\ndiffering numbers of nodes. To systematically further the study of theories for\nsuch higher-order structures, we propose higher-order link prediction as a\nbenchmark problem to assess models and algorithms that predict higher-order\nstructure. We find a fundamental differences from traditional pairwise link\nprediction, with a greater role for local rather than long-range information in\npredicting the appearance of new interactions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 00:05:25 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 19:29:28 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Benson", "Austin R.", ""], ["Abebe", "Rediet", ""], ["Schaub", "Michael T.", ""], ["Jadbabaie", "Ali", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1802.06924", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha and Shihan Su and Yuxin Chen and Pietro Perona and\n  Yisong Yue", "title": "Teaching Categories to Human Learners with Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computer-assisted teaching with explanations.\nConventional approaches for machine teaching typically only provide feedback at\nthe instance level e.g., the category or label of the instance. However, it is\nintuitive that clear explanations from a knowledgeable teacher can\nsignificantly improve a student's ability to learn a new concept. To address\nthese existing limitations, we propose a teaching framework that provides\ninterpretable explanations as feedback and models how the learner incorporates\nthis additional information. In the case of images, we show that we can\nautomatically generate explanations that highlight the parts of the image that\nare responsible for the class label. Experiments on human learners illustrate\nthat, on average, participants achieve better test set performance on\nchallenging categorization tasks when taught with our interpretable approach\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 00:49:38 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Su", "Shihan", ""], ["Chen", "Yuxin", ""], ["Perona", "Pietro", ""], ["Yue", "Yisong", ""]]}, {"id": "1802.06939", "submitter": "Ayaka Sakata", "authors": "Ayaka Sakata", "title": "Estimator of Prediction Error Based on Approximate Message Passing for\n  Penalized Linear Regression", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/aac910", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimator of prediction error using an approximate message\npassing (AMP) algorithm that can be applied to a broad range of sparse\npenalties. Following Stein's lemma, the estimator of the generalized degrees of\nfreedom, which is a key quantity for the construction of the estimator of the\nprediction error, is calculated at the AMP fixed point. The resulting form of\nthe AMP-based estimator does not depend on the penalty function, and its value\ncan be further improved by considering the correlation between predictors. The\nproposed estimator is asymptotically unbiased when the components of the\npredictors and response variables are independently generated according to a\nGaussian distribution. We examine the behaviour of the estimator for real data\nunder nonconvex sparse penalties, where Akaike's information criterion does not\ncorrespond to an unbiased estimator of the prediction error. The model selected\nby the proposed estimator is close to that which minimizes the true prediction\nerror.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 02:47:21 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 04:37:50 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Sakata", "Ayaka", ""]]}, {"id": "1802.06942", "submitter": "Ehsan Kazemi", "authors": "Ehsan Kazemi and Lin Chen and Sanjoy Dasgupta and Amin Karbasi", "title": "Comparison Based Learning from Weak Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in learning algorithms that involve interaction\nbetween human and machine. Comparison-based queries are among the most natural\nways to get feedback from humans. A challenge in designing comparison-based\ninteractive learning algorithms is coping with noisy answers. The most common\nfix is to submit a query several times, but this is not applicable in many\nsituations due to its prohibitive cost and due to the unrealistic assumption of\nindependent noise in different repetitions of the same query.\n  In this paper, we introduce a new weak oracle model, where a non-malicious\nuser responds to a pairwise comparison query only when she is quite sure about\nthe answer. This model is able to mimic the behavior of a human in noise-prone\nregions. We also consider the application of this weak oracle model to the\nproblem of content search (a variant of the nearest neighbor search problem)\nthrough comparisons. More specifically, we aim at devising efficient algorithms\nto locate a target object in a database equipped with a dissimilarity metric\nvia invocation of the weak comparison oracle. We propose two algorithms termed\nWORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provably\nlocate the target object in a number of comparisons close to the entropy of the\ntarget distribution. While WORCS-I provides better theoretical guarantees,\nWORCS-II is applicable to more technically challenging scenarios where the\nalgorithm has limited access to the ranking dissimilarity between objects. A\nseries of experiments validate the performance of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 02:57:25 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Chen", "Lin", ""], ["Dasgupta", "Sanjoy", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.06944", "submitter": "Sara Baghsorkhi", "authors": "Matthew Sotoudeh, Sara S. Baghsorkhi", "title": "DeepThin: A Self-Compressing Library for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the industry deploys increasingly large and complex neural networks to\nmobile devices, more pressure is put on the memory and compute resources of\nthose devices. Deep compression, or compression of deep neural network weight\nmatrices, is a technique to stretch resources for such scenarios. Existing\ncompression methods cannot effectively compress models smaller than 1-2% of\ntheir original size. We develop a new compression technique, DeepThin, building\non existing research in the area of low rank factorization. We identify and\nbreak artificial constraints imposed by low rank approximations by combining\nrank factorization with a reshaping process that adds nonlinearity to the\napproximation function. We deploy DeepThin as a plug-gable library integrated\nwith TensorFlow that enables users to seamlessly compress models at different\ngranularities. We evaluate DeepThin on two state-of-the-art acoustic models,\nTFKaldi and DeepSpeech, comparing it to previous compression work (Pruning,\nHashNet, and Rank Factorization), empirical limit study approaches, and\nhand-tuned models. For TFKaldi, our DeepThin networks show better word error\nrates (WER) than competing methods at practically all tested compression rates,\nachieving an average of 60% relative improvement over rank factorization, 57%\nover pruning, 23% over hand-tuned same-size networks, and 6% over the\ncomputationally expensive HashedNets. For DeepSpeech, DeepThin-compressed\nnetworks achieve better test loss than all other compression methods, reaching\na 28% better result than rank factorization, 27% better than pruning, 20%\nbetter than hand-tuned same-size networks, and 12% better than HashedNets.\nDeepThin also provide inference performance benefits ranging from 2X to 14X\nspeedups, depending on the compression ratio and platform cache sizes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 03:03:40 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Baghsorkhi", "Sara S.", ""]]}, {"id": "1802.06967", "submitter": "Ming Yu", "authors": "Ming Yu, Varun Gupta, Mladen Kolar", "title": "Recovery of simultaneous low rank and two-way sparse coefficient\n  matrices, a nonconvex approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovery of matrices that are simultaneously low rank\nand row and/or column sparse. Such matrices appear in recent applications in\ncognitive neuroscience, imaging, computer vision, macroeconomics, and genetics.\nWe propose a GDT (Gradient Descent with hard Thresholding) algorithm to\nefficiently recover matrices with such structure, by minimizing a bi-convex\nfunction over a nonconvex set of constraints. We show linear convergence of the\niterates obtained by GDT to a region within statistical error of an optimal\nsolution. As an application of our method, we consider multi-task learning\nproblems and show that the statistical error rate obtained by GDT is near\noptimal compared to minimax rate. Experiments demonstrate competitive\nperformance and much faster running speed compared to existing methods, on both\nsimulations and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:52:26 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:28:24 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Yu", "Ming", ""], ["Gupta", "Varun", ""], ["Kolar", "Mladen", ""]]}, {"id": "1802.07007", "submitter": "Zhiyong Cui", "authors": "Zhiyong Cui, Kristian Henrickson, Ruimin Ke, Ziyuan Pu, Yinhai Wang", "title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning\n  Framework for Network-Scale Traffic Learning and Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic forecasting is a particularly challenging application of\nspatiotemporal forecasting, due to the time-varying traffic patterns and the\ncomplicated spatial dependencies on road networks. To address this challenge,\nwe learn the traffic network as a graph and propose a novel deep learning\nframework, Traffic Graph Convolutional Long Short-Term Memory Neural Network\n(TGC-LSTM), to learn the interactions between roadways in the traffic network\nand forecast the network-wide traffic state. We define the traffic graph\nconvolution based on the physical network topology. The relationship between\nthe proposed traffic graph convolution and the spectral graph convolution is\nalso discussed. An L1-norm on graph convolution weights and an L2-norm on graph\nconvolution features are added to the model's loss function to enhance the\ninterpretability of the proposed model. Experimental results show that the\nproposed model outperforms baseline methods on two real-world traffic state\ndatasets. The visualization of the graph convolution weights indicates that the\nproposed framework can recognize the most influential road segments in\nreal-world traffic networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 08:40:21 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:06:39 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 01:09:07 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Cui", "Zhiyong", ""], ["Henrickson", "Kristian", ""], ["Ke", "Ruimin", ""], ["Pu", "Ziyuan", ""], ["Wang", "Yinhai", ""]]}, {"id": "1802.07008", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Segmentation hi\\'erarchique faiblement supervis\\'ee", "comments": "in French", "journal-ref": "26e colloque GRETSI, Sep 2017, Juan-les-Pins, France", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. An application of this method\nto the weakly-supervised segmentation problem is presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 08:42:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1802.07024", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Amr Alexandari, Anshul Kundaje", "title": "A Flexible and Adaptive Framework for Abstention Under Class Imbalance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications of machine learning, it is often desirable to\nidentify and abstain on examples where the model's predictions are likely to be\nincorrect. Much of the prior work on this topic focused on out-of-distribution\ndetection or performance metrics such as top-k accuracy. Comparatively little\nattention was given to metrics such as area-under-the-curve or Cohen's Kappa,\nwhich are extremely relevant for imbalanced datasets. Abstention strategies\naimed at top-k accuracy can produce poor results on these metrics when applied\nto imbalanced datasets, even when all examples are in-distribution. We propose\na framework to address this gap. Our framework leverages the insight that\ncalibrated probability estimates can be used as a proxy for the true class\nlabels, thereby allowing us to estimate the change in an arbitrary metric if an\nexample were abstained on. Using this framework, we derive computationally\nefficient metric-specific abstention algorithms for optimizing the sensitivity\nat a target specificity level, the area under the ROC, and the weighted Cohen's\nKappa. Because our method relies only on calibrated probability estimates, we\nfurther show that by leveraging recent work on domain adaptation under label\nshift, we can generalize to test-set distributions that may have a different\nclass imbalance compared to the training set distribution. On various\nexperiments involving medical imaging, natural language processing, computer\nvision and genomics, we demonstrate the effectiveness of our approach. Source\ncode available at https://github.com/blindauth/abstention. Colab notebooks\nreproducing results available at\nhttps://github.com/blindauth/abstention_experiments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:24:49 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 00:21:34 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 05:11:25 GMT"}, {"version": "v4", "created": "Wed, 30 Oct 2019 06:02:57 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Alexandari", "Amr", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1802.07028", "submitter": "Paul Rolland", "authors": "Paul Rolland, Jonathan Scarlett, Ilija Bogunovic and Volkan Cevher", "title": "High-Dimensional Bayesian Optimization via Additive Models with\n  Overlapping Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a popular technique for sequential black-box\nfunction optimization, with applications including parameter tuning, robotics,\nenvironmental monitoring, and more. One of the most important challenges in BO\nis the development of algorithms that scale to high dimensions, which remains a\nkey open problem despite recent progress. In this paper, we consider the\napproach of Kandasamy et al. (2015), in which the high-dimensional function\ndecomposes as a sum of lower-dimensional functions on subsets of the underlying\nvariables. In particular, we significantly generalize this approach by lifting\nthe assumption that the subsets are disjoint, and consider additive models with\narbitrary overlap among the subsets. By representing the dependencies via a\ngraph, we deduce an efficient message passing algorithm for optimizing the\nacquisition function. In addition, we provide an algorithm for learning the\ngraph from samples based on Gibbs sampling. We empirically demonstrate the\neffectiveness of our methods on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:42:03 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 13:05:53 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Rolland", "Paul", ""], ["Scarlett", "Jonathan", ""], ["Bogunovic", "Ilija", ""], ["Cevher", "Volkan", ""]]}, {"id": "1802.07051", "submitter": "Hanti Lin", "authors": "Hanti Lin and Jiji Zhang", "title": "On Learning Causal Structures from Non-Experimental Data without Any\n  Faithfulness Assumption", "comments": "To be published in Proceedings of Machine Learning Research, volume\n  117", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of learning, from non-experimental data, the causal\n(Markov equivalence) structure of the true, unknown causal Bayesian network\n(CBN) on a given, fixed set of (categorical) variables. This learning problem\nis known to be so hard that there is no learning algorithm that converges to\nthe truth for all possible CBNs (on the given set of variables). So the\nconvergence property has to be sacrificed for some CBNs---but for which? In\nresponse, the standard practice has been to design and employ learning\nalgorithms that secure the convergence property for at least all the CBNs that\nsatisfy the famous faithfulness condition, which implies sacrificing the\nconvergence property for some CBNs that violate the faithfulness condition\n(Spirtes et al. 2000). This standard design practice can be justified by\nassuming---that is, accepting on faith---that the true, unknown CBN satisfies\nthe faithfulness condition. But the real question is this: Is it possible to\nexplain, without assuming the faithfulness condition or any of its weaker\nvariants, why it is mandatory rather than optional to follow the standard\ndesign practice? This paper aims to answer the above question in the\naffirmative. We first define an array of modes of convergence to the truth as\ndesiderata that might or might not be achieved by a causal learning algorithm.\nThose modes of convergence concern (i) how pervasive the domain of convergence\nis on the space of all possible CBNs and (ii) how uniformly the convergence\nhappens. Then we prove a result to the following effect: for any learning\nalgorithm that tackles the causal learning problem in question, if it achieves\nthe best achievable mode of convergence (considered in this paper), then it\nmust follow the standard design practice of converging to the truth for at\nleast all CBNs that satisfy the faithfulness condition---it is a requirement,\nnot an option.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 10:46:12 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 05:49:25 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Lin", "Hanti", ""], ["Zhang", "Jiji", ""]]}, {"id": "1802.07073", "submitter": "Ilija Bogunovic", "authors": "Ilija Bogunovic, Junyao Zhao, Volkan Cevher", "title": "Robust Maximization of Non-Submodular Objectives", "comments": "Revision of Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone set function subject to a\ncardinality constraint $k$ in the setting where some number of elements $\\tau$\nis deleted from the returned set. The focus of this work is on the worst-case\nadversarial setting. While there exist constant-factor guarantees when the\nfunction is submodular, there are no guarantees for non-submodular objectives.\nIn this work, we present a new algorithm Oblivious-Greedy and prove the first\nconstant-factor approximation guarantees for a wider class of non-submodular\nobjectives. The obtained theoretical bounds are the first constant-factor\nbounds that also hold in the linear regime, i.e. when the number of deletions\n$\\tau$ is linear in $k$. Our bounds depend on established parameters such as\nthe submodularity ratio and some novel ones such as the inverse curvature. We\nbound these parameters for two important objectives including support selection\nand variance reduction. Finally, we numerically demonstrate the robust\nperformance of Oblivious-Greedy for these two objectives on various datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:02:01 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 10:58:37 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 11:24:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Zhao", "Junyao", ""], ["Cevher", "Volkan", ""]]}, {"id": "1802.07079", "submitter": "Garoe Dorta", "authors": "Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill D.F. Campbell, Ivor\n  Simpson", "title": "Structured Uncertainty Prediction Networks", "comments": "CVPR 2018 (final version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first work to propose a network to predict a structured\nuncertainty distribution for a synthesized image. Previous approaches have been\nmostly limited to predicting diagonal covariance matrices. Our novel model\nlearns to predict a full Gaussian covariance matrix for each reconstruction,\nwhich permits efficient sampling and likelihood evaluation.\n  We demonstrate that our model can accurately reconstruct ground truth\ncorrelated residual distributions for synthetic datasets and generate plausible\nhigh frequency samples for real face images. We also illustrate the use of\nthese predicted covariances for structure preserving image denoising.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:19:59 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 17:43:24 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Dorta", "Garoe", ""], ["Vicente", "Sara", ""], ["Agapito", "Lourdes", ""], ["Campbell", "Neill D. F.", ""], ["Simpson", "Ivor", ""]]}, {"id": "1802.07088", "submitter": "Edouard Oyallon", "authors": "J\\\"orn-Henrik Jacobsen (IvI), Arnold Smeulders (IvI), Edouard Oyallon\n  (CVN, GALEN, SEQUEL, DI-ENS)", "title": "i-RevNet: Deep Invertible Networks", "comments": null, "journal-ref": "ICLR 2018 - International Conference on Learning Representations,\n  Apr 2018, Vancouver, Canada. 2018, https://iclr.cc/", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:38:49 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", "", "IvI"], ["Smeulders", "Arnold", "", "IvI"], ["Oyallon", "Edouard", "", "CVN, GALEN, SEQUEL, DI-ENS"]]}, {"id": "1802.07107", "submitter": "Yakov Babichenko", "authors": "Yakov Babichenko and Dan Garber", "title": "Learning of Optimal Forecast Aggregation in Partial Evidence\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the forecast aggregation problem in repeated settings, where the\nforecasts are done on a binary event. At each period multiple experts provide\nforecasts about an event. The goal of the aggregator is to aggregate those\nforecasts into a subjective accurate forecast. We assume that experts are\nBayesian; namely they share a common prior, each expert is exposed to some\nevidence, and each expert applies Bayes rule to deduce his forecast. The\naggregator is ignorant with respect to the information structure (i.e.,\ndistribution over evidence) according to which experts make their prediction.\nThe aggregator observes the experts' forecasts only. At the end of each period\nthe actual state is realized. We focus on the question whether the aggregator\ncan learn to aggregate optimally the forecasts of the experts, where the\noptimal aggregation is the Bayesian aggregation that takes into account all the\ninformation (evidence) in the system.\n  We consider the class of partial evidence information structures, where each\nexpert is exposed to a different subset of conditionally independent signals.\nOur main results are positive; We show that optimal aggregation can be learned\nin polynomial time in a quite wide range of instances of the partial evidence\nenvironments. We provide a tight characterization of the instances where\nlearning is possible and impossible.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 13:35:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Babichenko", "Yakov", ""], ["Garber", "Dan", ""]]}, {"id": "1802.07124", "submitter": "Mahdieh Abbasi", "authors": "Mahdieh Abbasi, Christian Gagn\\'e", "title": "Out-distribution training confers robustness to deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The easiness at which adversarial instances can be generated in deep neural\nnetworks raises some fundamental questions on their functioning and concerns on\ntheir use in critical systems. In this paper, we draw a connection between\nover-generalization and adversaries: a possible cause of adversaries lies in\nmodels designed to make decisions all over the input space, leading to\ninappropriate high-confidence decisions in parts of the input space not\nrepresented in the training set. We empirically show an augmented neural\nnetwork, which is not trained on any types of adversaries, can increase the\nrobustness by detecting black-box one-step adversaries, i.e. assimilated to\nout-distribution samples, and making generation of white-box one-step\nadversaries harder.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 14:27:40 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 14:19:41 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 17:14:35 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1802.07126", "submitter": "Venkata Sriram Siddhardh (Sid) Nadendla", "authors": "Venkata Sriram Siddhardh Nadendla and Cedric Langbort", "title": "On Estimating Multi-Attribute Choice Preferences using Private Signals\n  and Matrix Factorization", "comments": "6 pages, 2 figures, to be presented at CISS conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revealed preference theory studies the possibility of modeling an agent's\nrevealed preferences and the construction of a consistent utility function.\nHowever, modeling agent's choices over preference orderings is not always\npractical and demands strong assumptions on human rationality and\ndata-acquisition abilities. Therefore, we propose a simple generative choice\nmodel where agents are assumed to generate the choice probabilities based on\nlatent factor matrices that capture their choice evaluation across multiple\nattributes. Since the multi-attribute evaluation is typically hidden within the\nagent's psyche, we consider a signaling mechanism where agents are provided\nwith choice information through private signals, so that the agent's choices\nprovide more insight about his/her latent evaluation across multiple\nattributes. We estimate the choice model via a novel multi-stage matrix\nfactorization algorithm that minimizes the average deviation of the factor\nestimates from choice data. Simulation results are presented to validate the\nestimation performance of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:46:37 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Nadendla", "Venkata Sriram Siddhardh", ""], ["Langbort", "Cedric", ""]]}, {"id": "1802.07127", "submitter": "Tom Decroos", "authors": "Tom Decroos, Lotte Bransen, Jan Van Haaren, Jesse Davis", "title": "Actions Speak Louder Than Goals: Valuing Player Actions in Soccer", "comments": "Significant update of the paper. The same core idea, but with a\n  clearer methodology, applied on a different data set, and more extensive\n  experiments. 9 pages + 2 pages appendix. To be published at SIGKDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330758", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the impact of the individual actions performed by soccer players\nduring games is a crucial aspect of the player recruitment process.\nUnfortunately, most traditional metrics fall short in addressing this task as\nthey either focus on rare actions like shots and goals alone or fail to account\nfor the context in which the actions occurred. This paper introduces (1) a new\nlanguage for describing individual player actions on the pitch and (2) a\nframework for valuing any type of player action based on its impact on the game\noutcome while accounting for the context in which the action happened. By\naggregating soccer players' action values, their total offensive and defensive\ncontributions to their team can be quantified. We show how our approach\nconsiders relevant contextual information that traditional player evaluation\nmetrics ignore and present a number of use cases related to scouting and\nplaying style characterization in the 2016/2017 and 2017/2018 seasons in\nEurope's top competitions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 21:28:36 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 12:26:32 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Decroos", "Tom", ""], ["Bransen", "Lotte", ""], ["Van Haaren", "Jan", ""], ["Davis", "Jesse", ""]]}, {"id": "1802.07129", "submitter": "Il Yong Chun", "authors": "Il Yong Chun and Jeffrey A. Fessler", "title": "Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for\n  Iterative Image Recovery", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"extreme\" computational imaging that collects extremely undersampled or\nnoisy measurements, obtaining an accurate image within a reasonable computing\ntime is challenging. Incorporating image mapping convolutional neural networks\n(CNN) into iterative image recovery has great potential to resolve this issue.\nThis paper 1) incorporates image mapping CNN using identical convolutional\nkernels in both encoders and decoders into a block coordinate descent (BCD)\nsignal recovery method and 2) applies alternating direction method of\nmultipliers to train the aforementioned image mapping CNN. We refer to the\nproposed recurrent network as BCD-Net using identical encoding-decoding CNN\nstructures. Numerical experiments show that, for a) denoising low\nsignal-to-noise-ratio images and b) extremely undersampled magnetic resonance\nimaging, the proposed BCD-Net achieves significantly more accurate image\nrecovery, compared to BCD-Net using distinct encoding-decoding structures\nand/or the conventional image recovery model using both wavelets and total\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 14:37:30 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 22:16:11 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chun", "Il Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1802.07167", "submitter": "Tim Pearce", "authors": "Tim Pearce, Mohamed Zaki, Alexandra Brintrup, Andy Neely", "title": "High-Quality Prediction Intervals for Deep Learning: A\n  Distribution-Free, Ensembled Approach", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, 2018", "doi": null, "report-no": "PMLR 80:4075-4084, 2018", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the generation of prediction intervals (PIs) by neural\nnetworks for quantifying uncertainty in regression tasks. It is axiomatic that\nhigh-quality PIs should be as narrow as possible, whilst capturing a specified\nportion of data. We derive a loss function directly from this axiom that\nrequires no distributional assumption. We show how its form derives from a\nlikelihood principle, that it can be used with gradient descent, and that model\nuncertainty is accounted for in ensembled form. Benchmark experiments show the\nmethod outperforms current state-of-the-art uncertainty quantification methods,\nreducing average PI width by over 10%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 16:02:21 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 15:58:44 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 17:12:59 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Pearce", "Tim", ""], ["Zaki", "Mohamed", ""], ["Brintrup", "Alexandra", ""], ["Neely", "Andy", ""]]}, {"id": "1802.07176", "submitter": "Sumeet Katariya", "authors": "Sumeet Katariya, Lalit Jain, Nandana Sengupta, James Evans, Robert\n  Nowak", "title": "Adaptive Sampling for Coarse Ranking", "comments": "Accepted at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of active coarse ranking, where the goal is to sort\nitems according to their means into clusters of pre-specified sizes, by\nadaptively sampling from their reward distributions. This setting is useful in\nmany social science applications involving human raters and the approximate\nrank of every item is desired. Approximate or coarse ranking can significantly\nreduce the number of ratings required in comparison to the number needed to\nfind an exact ranking. We propose a computationally efficient PAC algorithm\nLUCBRank for coarse ranking, and derive an upper bound on its sample\ncomplexity. We also derive a nearly matching distribution-dependent lower\nbound. Experiments on synthetic as well as real-world data show that LUCBRank\nperforms better than state-of-the-art baseline methods, even when these methods\nhave the advantage of knowing the underlying parametric model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 16:16:38 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Katariya", "Sumeet", ""], ["Jain", "Lalit", ""], ["Sengupta", "Nandana", ""], ["Evans", "James", ""], ["Nowak", "Robert", ""]]}, {"id": "1802.07182", "submitter": "Wessel Bruinsma", "authors": "James Requeima and Will Tebbutt and Wessel Bruinsma and Richard E.\n  Turner", "title": "The Gaussian Process Autoregressive Regression Model (GPAR)", "comments": "14 pages, 8 figures, 5 tables, includes appendices; to appear in\n  AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output regression models must exploit dependencies between outputs to\nmaximise predictive performance. The application of Gaussian processes (GPs) to\nthis setting typically yields models that are computationally demanding and\nhave limited representational power. We present the Gaussian Process\nAutoregressive Regression (GPAR) model, a scalable multi-output GP model that\nis able to capture nonlinear, possibly input-varying, dependencies between\noutputs in a simple and tractable way: the product rule is used to decompose\nthe joint distribution over the outputs into a set of conditionals, each of\nwhich is modelled by a standard GP. GPAR's efficacy is demonstrated on a\nvariety of synthetic and real-world problems, outperforming existing GP models\nand achieving state-of-the-art performance on established benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 16:21:59 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 11:26:29 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 15:53:01 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2019 23:12:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Requeima", "James", ""], ["Tebbutt", "Will", ""], ["Bruinsma", "Wessel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1802.07191", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas\n  Poczos, Eric Xing", "title": "Neural Architecture Search with Bayesian Optimisation and Optimal\n  Transport", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS) 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimisation (BO) refers to a class of methods for global\noptimisation of a function $f$ which is only accessible via point evaluations.\nIt is typically used in settings where $f$ is expensive to evaluate. A common\nuse case for BO in machine learning is model selection, where it is not\npossible to analytically model the generalisation performance of a statistical\nmodel, and we resort to noisy and expensive training and validation procedures\nto choose the best model. Conventional BO methods have focused on Euclidean and\ncategorical domains, which, in the context of model selection, only permits\ntuning scalar hyper-parameters of machine learning algorithms. However, with\nthe surge of interest in deep learning, there is an increasing demand to tune\nneural network \\emph{architectures}. In this work, we develop NASBOT, a\nGaussian process based BO framework for neural architecture search. To\naccomplish this, we develop a distance metric in the space of neural network\narchitectures which can be computed efficiently via an optimal transport\nprogram. This distance might be of independent interest to the deep learning\ncommunity as it may find applications outside of BO. We demonstrate that NASBOT\noutperforms other alternatives for architecture search in several cross\nvalidation based model selection tasks on multi-layer perceptrons and\nconvolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 17:45:44 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 15:17:17 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 18:23:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Neiswanger", "Willie", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""], ["Xing", "Eric", ""]]}, {"id": "1802.07207", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian\n  Optimization with Structured Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical prognostic models derived from largescale healthcare data can inform\ncritical diagnostic and therapeutic decisions. To enable off-theshelf usage of\nmachine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a\nsystem for automating the design of predictive modeling pipelines tailored for\nclinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline\nconfigurations efficiently using a novel batched Bayesian optimization (BO)\nalgorithm that learns a low-dimensional decomposition of the pipelines\nhigh-dimensional hyperparameter space in concurrence with the BO procedure.\nThis is achieved by modeling the pipelines performances as a black-box function\nwith a Gaussian process prior, and modeling the similarities between the\npipelines baseline algorithms via a sparse additive kernel with a Dirichlet\nprior. Meta-learning is used to warmstart BO with external data from similar\npatient cohorts by calibrating the priors using an algorithm that mimics the\nempirical Bayes method. The system automatically explains its predictions by\npresenting the clinicians with logical association rules that link patients\nfeatures to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS\nusing 10 major patient cohorts representing various aspects of cardiovascular\npatient care.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 17:18:56 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1802.07229", "submitter": "Gautam Kamath", "authors": "Steve Hanneke, Adam Kalai, Gautam Kamath, Christos Tzamos", "title": "Actively Avoiding Nonsense in Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model may generate utter nonsense when it is fit to maximize the\nlikelihood of observed data. This happens due to \"model error,\" i.e., when the\ntrue data generating distribution does not fit within the class of generative\nmodels being learned. To address this, we propose a model of active\ndistribution learning using a binary invalidity oracle that identifies some\nexamples as clearly invalid, together with random positive examples sampled\nfrom the true distribution. The goal is to maximize the likelihood of the\npositive examples subject to the constraint of (almost) never generating\nexamples labeled invalid by the oracle. Guarantees are agnostic compared to a\nclass of probability distributions. We show that, while proper learning often\nrequires exponentially many queries to the invalidity oracle, improper\ndistribution learning can be done using polynomially many queries.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:08:53 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Hanneke", "Steve", ""], ["Kalai", "Adam", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}, {"id": "1802.07244", "submitter": "Ali Zarezade", "authors": "Ali Zarezade, Abir De, Utkarsh Upadhyay, Hamid R. Rabiee, Manuel\n  Gomez-Rodriguez", "title": "Steering Social Activity: A Stochastic Optimal Control Point Of View", "comments": "To appear in JMLR 2018. arXiv admin note: substantial text overlap\n  with arXiv:1610.05773, arXiv:1703.02059", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement in online social networking depends critically on the level\nof social activity in the corresponding platform--the number of online actions,\nsuch as posts, shares or replies, taken by their users. Can we design\ndata-driven algorithms to increase social activity? At a user level, such\nalgorithms may increase activity by helping users decide when to take an action\nto be more likely to be noticed by their peers. At a network level, they may\nincrease activity by incentivizing a few influential users to take more\nactions, which in turn will trigger additional actions by other users. In this\npaper, we model social activity using the framework of marked temporal point\nprocesses, derive an alternate representation of these processes using\nstochastic differential equations (SDEs) with jumps and, exploiting this\nalternate representation, develop two efficient online algorithms with provable\nguarantees to steer social activity both at a user and at a network level. In\ndoing so, we establish a previously unexplored connection between optimal\ncontrol of jump SDEs and doubly stochastic marked temporal point processes,\nwhich is of independent interest. Finally, we experiment both with synthetic\nand real data gathered from Twitter and show that our algorithms consistently\nsteer social activity more effectively than the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:03:26 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Zarezade", "Ali", ""], ["De", "Abir", ""], ["Upadhyay", "Utkarsh", ""], ["Rabiee", "Hamid R.", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1802.07295", "submitter": "Christopher Frederickson", "authors": "Christopher Frederickson, Michael Moore, Glenn Dawson, Robi Polikar", "title": "Attack Strength vs. Detectability Dilemma in Adversarial Machine\n  Learning", "comments": "8 pages, 9 figures, submitted to IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the prevalence and everyday use of machine learning algorithms, along with\nour reliance on these algorithms grow dramatically, so do the efforts to attack\nand undermine these algorithms with malicious intent, resulting in a growing\ninterest in adversarial machine learning. A number of approaches have been\ndeveloped that can render a machine learning algorithm ineffective through\npoisoning or other types of attacks. Most attack algorithms typically use\nsophisticated optimization approaches, whose objective function is designed to\ncause maximum damage with respect to accuracy and performance of the algorithm\nwith respect to some task. In this effort, we show that while such an objective\nfunction is indeed brutally effective in causing maximum damage on an embedded\nfeature selection task, it often results in an attack mechanism that can be\neasily detected with an embarrassingly simple novelty or outlier detection\nalgorithm. We then propose an equally simple yet elegant solution by adding a\nregularization term to the attacker's objective function that penalizes\noutlying attack points.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:20:31 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Frederickson", "Christopher", ""], ["Moore", "Michael", ""], ["Dawson", "Glenn", ""], ["Polikar", "Robi", ""]]}, {"id": "1802.07301", "submitter": "Marco Mondelli", "authors": "Marco Mondelli and Andrea Montanari", "title": "On the Connection Between Learning Two-Layers Neural Networks and Tensor\n  Decomposition", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish connections between the problem of learning a two-layer neural\nnetwork and tensor decomposition. We consider a model with feature vectors\n$\\boldsymbol x \\in \\mathbb R^d$, $r$ hidden units with weights $\\{\\boldsymbol\nw_i\\}_{1\\le i \\le r}$ and output $y\\in \\mathbb R$, i.e., $y=\\sum_{i=1}^r\n\\sigma( \\boldsymbol w_i^{\\mathsf T}\\boldsymbol x)$, with activation functions\ngiven by low-degree polynomials. In particular, if $\\sigma(x) =\na_0+a_1x+a_3x^3$, we prove that no polynomial-time learning algorithm can\noutperform the trivial predictor that assigns to each example the response\nvariable $\\mathbb E(y)$, when $d^{3/2}\\ll r\\ll d^2$. Our conclusion holds for a\n`natural data distribution', namely standard Gaussian feature vectors\n$\\boldsymbol x$, and output distributed according to a two-layer neural network\nwith random isotropic weights, and under a certain complexity-theoretic\nassumption on tensor decomposition. Roughly speaking, we assume that no\npolynomial-time algorithm can substantially outperform current methods for\ntensor decomposition based on the sum-of-squares hierarchy.\n  We also prove generalizations of this statement for higher degree polynomial\nactivations, and non-random weight vectors. Remarkably, several existing\nalgorithms for learning two-layer networks with rigorous guarantees are based\non tensor decomposition. Our results support the idea that this is indeed the\ncore computational difficulty in learning such networks, under the stated\ngenerative model for the data. As a side result, we show that under this model\nlearning the network requires accurate learning of its weights, a property that\ndoes not hold in a more general setting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:40:32 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 21:05:02 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 10:34:16 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Mondelli", "Marco", ""], ["Montanari", "Andrea", ""]]}, {"id": "1802.07307", "submitter": "Valentin Stanev G.", "authors": "Valentin Stanev, Velimir V. Vesselinov, A. Gilad Kusne, Graham\n  Antoszewski, Ichiro Takeuchi, Boian S. Alexandrov", "title": "Unsupervised Phase Mapping of X-ray Diffraction Data by Nonnegative\n  Matrix Factorization Integrated with Custom Clustering", "comments": "26 pages, 9 figures", "journal-ref": "npj Computational Materialsvolume 4, Article number: 43 (2018)", "doi": "10.1038/s41524-018-0099-2", "report-no": null, "categories": "cond-mat.mtrl-sci stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large X-ray diffraction (XRD) datasets is a key step in\nhigh-throughput mapping of the compositional phase diagrams of combinatorial\nmaterials libraries. Optimizing and automating this task can help accelerate\nthe process of discovery of materials with novel and desirable properties.\nHere, we report a new method for pattern analysis and phase extraction of XRD\ndatasets. The method expands the Nonnegative Matrix Factorization method, which\nhas been used previously to analyze such datasets, by combining it with custom\nclustering and cross-correlation algorithms. This new method is capable of\nrobust determination of the number of basis patterns present in the data which,\nin turn, enables straightforward identification of any possible peak-shifted\npatterns. Peak-shifting arises due to continuous change in the lattice\nconstants as a function of composition, and is ubiquitous in XRD datasets from\ncomposition spread libraries. Successful identification of the peak-shifted\npatterns allows proper quantification and classification of the basis XRD\npatterns, which is necessary in order to decipher the contribution of each\nunique single-phase structure to the multi-phase regions. The process can be\nutilized to determine accurately the compositional phase diagram of a system\nunder study. The presented method is applied to one synthetic and one\nexperimental dataset, and demonstrates robust accuracy and identification\nabilities.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 20:13:37 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Stanev", "Valentin", ""], ["Vesselinov", "Velimir V.", ""], ["Kusne", "A. Gilad", ""], ["Antoszewski", "Graham", ""], ["Takeuchi", "Ichiro", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "1802.07309", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui and Michael I. Jordan", "title": "Detection limits in the high-dimensional spiked rectangular model", "comments": "28 pages. Appears in the proc. of the 31st annual Conference on\n  Learning Theory (COLT) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting the presence of a single unknown spike in a\nrectangular data matrix, in a high-dimensional regime where the spike has fixed\nstrength and the aspect ratio of the matrix converges to a finite limit. This\nsetup includes Johnstone's spiked covariance model. We analyze the likelihood\nratio of the spiked model against an \"all noise\" null model of reference, and\nshow it has asymptotically Gaussian fluctuations in a region below---but in\ngeneral not up to---the so-called BBP threshold from random matrix theory. Our\nresult parallels earlier findings of Onatski et al.\\ (2013) and\nJohnstone-Onatski (2015) for spherical spikes. We present a probabilistic\napproach capable of treating generic product priors. In particular, sparsity in\nthe spike is allowed. Our approach is based on Talagrand's interpretation of\nthe cavity method from spin-glass theory. The question of the maximal parameter\nregion where asymptotic normality is expected to hold is left open. This region\nis shaped by the prior in a non-trivial way. We conjecture that this is the\nentire paramagnetic phase of an associated spin-glass model, and is defined by\nthe vanishing of the replica-symmetric solution of Lesieur et al.\\ (2015).\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 20:19:06 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 22:11:32 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 18:29:20 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1802.07329", "submitter": "Max Kochurov", "authors": "Max Kochurov, Timur Garipov, Dmitry Podoprikhin, Dmitry Molchanov,\n  Arsenii Ashukha, Dmitry Vetrov", "title": "Bayesian Incremental Learning for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial machine learning pipelines, data often arrive in parts.\nParticularly in the case of deep neural networks, it may be too expensive to\ntrain the model from scratch each time, so one would rather use a previously\nlearned model and the new data to improve performance. However, deep neural\nnetworks are prone to getting stuck in a suboptimal solution when trained on\nonly new data as compared to the full dataset. Our work focuses on a continuous\nlearning setup where the task is always the same and new parts of data arrive\nsequentially. We apply a Bayesian approach to update the posterior\napproximation with each new piece of data and find this method to outperform\nthe traditional approach in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:11:17 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 14:56:37 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 22:28:02 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kochurov", "Max", ""], ["Garipov", "Timur", ""], ["Podoprikhin", "Dmitry", ""], ["Molchanov", "Dmitry", ""], ["Ashukha", "Arsenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1802.07330", "submitter": "Michail Tsagris", "authors": "Michail Tsagris and Connie Stewart", "title": "A folded model for compositional data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A folded type model is developed for analyzing compositional data. The\nproposed model involves an extension of the $\\alpha$-transformation for\ncompositional data and provides a new and flexible class of distributions for\nmodeling data defined on the simplex sample space. Despite its rather seemingly\ncomplex structure, employment of the EM algorithm guarantees efficient\nparameter estimation. The model is validated through simulation studies and\nexamples which illustrate that the proposed model performs better in terms of\ncapturing the data structure, when compared to the popular logistic normal\ndistribution, and can be advantageous over a similar model without folding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:16:09 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 10:41:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Tsagris", "Michail", ""], ["Stewart", "Connie", ""]]}, {"id": "1802.07369", "submitter": "Qiuyi Wu", "authors": "Qiuyi Wu, Ernest Fokoue, Dhireesha Kudithipudi", "title": "On the Statistical Challenges of Echo State Networks and Some Potential\n  Remedies", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state networks are powerful recurrent neural networks. However, they are\noften unstable and shaky, making the process of finding an good ESN for a\nspecific dataset quite hard. Obtaining a superb accuracy by using the Echo\nState Network is a challenging task. We create, develop and implement a family\nof predictably optimal robust and stable ensemble of Echo State Networks via\nregularizing the training and perturbing the input. Furthermore, several\ndistributions of weights have been tried based on the shape to see if the shape\nof the distribution has the impact for reducing the error. We found ESN can\ntrack in short term for most dataset, but it collapses in the long run.\nShort-term tracking with large size reservoir enables ESN to perform strikingly\nwith superior prediction. Based on this scenario, we go a further step to\naggregate many of ESNs into an ensemble to lower the variance and stabilize the\nsystem by stochastic replications and bootstrapping of input data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 22:56:17 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Wu", "Qiuyi", ""], ["Fokoue", "Ernest", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1802.07372", "submitter": "Zhe Wang", "authors": "Zhe Wang, Yi Zhou, Yingbin Liang, Guanghui Lan", "title": "Stochastic Variance-Reduced Cubic Regularization for Nonconvex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cubic regularization (CR) is an optimization method with emerging popularity\ndue to its capability to escape saddle points and converge to second-order\nstationary solutions for nonconvex optimization. However, CR encounters a high\nsample complexity issue for finite-sum problems with a large data size.\n%Various inexact variants of CR have been proposed to improve the sample\ncomplexity. In this paper, we propose a stochastic variance-reduced\ncubic-regularization (SVRC) method under random sampling, and study its\nconvergence guarantee as well as sample complexity. We show that the iteration\ncomplexity of SVRC for achieving a second-order stationary solution within\n$\\epsilon$ accuracy is $O(\\epsilon^{-3/2})$, which matches the state-of-art\nresult on CR types of methods. Moreover, our proposed variance reduction scheme\nsignificantly reduces the per-iteration sample complexity. The resulting total\nHessian sample complexity of our SVRC is ${\\Oc}(N^{2/3} \\epsilon^{-3/2})$,\nwhich outperforms the state-of-art result by a factor of $O(N^{2/15})$. We also\nstudy our SVRC under random sampling without replacement scheme, which yields a\nlower per-iteration sample complexity, and hence justifies its practical\napplicability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 23:15:17 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 00:25:29 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wang", "Zhe", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Lan", "Guanghui", ""]]}, {"id": "1802.07384", "submitter": "Xin Zhang", "authors": "Xin Zhang, Armando Solar-Lezama, and Rishabh Singh", "title": "Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic\n  Corrections", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm to generate minimal, stable, and symbolic\ncorrections to an input that will cause a neural network with ReLU activations\nto change its output. We argue that such a correction is a useful way to\nprovide feedback to a user when the network's output is different from a\ndesired output. Our algorithm generates such a correction by solving a series\nof linear constraint satisfaction problems. The technique is evaluated on three\nneural network models: one predicting whether an applicant will pay a mortgage,\none predicting whether a first-order theorem can be proved efficiently by a\nsolver using certain heuristics, and the final one judging whether a drawing is\nan accurate rendition of a canonical drawing of a cat.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 00:47:32 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:33:26 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zhang", "Xin", ""], ["Solar-Lezama", "Armando", ""], ["Singh", "Rishabh", ""]]}, {"id": "1802.07389", "submitter": "Hyeontaek Lim", "authors": "Hyeontaek Lim and David G. Andersen and Michael Kaminsky", "title": "3LC: Lightweight and Effective Traffic Compression for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance and efficiency of distributed machine learning (ML) depends\nsignificantly on how long it takes for nodes to exchange state changes.\nOverly-aggressive attempts to reduce communication often sacrifice final model\naccuracy and necessitate additional ML techniques to compensate for this loss,\nlimiting their generality. Some attempts to reduce communication incur high\ncomputation overhead, which makes their performance benefits visible only over\nslow networks.\n  We present 3LC, a lossy compression scheme for state change traffic that\nstrikes balance between multiple goals: traffic reduction, accuracy,\ncomputation overhead, and generality. It combines three new\ntechniques---3-value quantization with sparsity multiplication, quartic\nencoding, and zero-run encoding---to leverage strengths of quantization and\nsparsification techniques and avoid their drawbacks. It achieves a data\ncompression ratio of up to 39--107X, almost the same test accuracy of trained\nmodels, and high compression speed. Distributed ML frameworks can employ 3LC\nwithout modifications to existing ML algorithms. Our experiments show that 3LC\nreduces wall-clock training time of ResNet-110--based image classifiers for\nCIFAR-10 on a 10-GPU cluster by up to 16--23X compared to TensorFlow's baseline\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 01:08:58 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Lim", "Hyeontaek", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""]]}, {"id": "1802.07400", "submitter": "Cynthia Rudin", "authors": "Cynthia Rudin, Yining Wang", "title": "Direct Learning to Rank and Rerank", "comments": null, "journal-ref": "AISTATS 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning-to-rank techniques have proven to be extremely useful for\nprioritization problems, where we rank items in order of their estimated\nprobabilities, and dedicate our limited resources to the top-ranked items. This\nwork exposes a serious problem with the state of learning-to-rank algorithms,\nwhich is that they are based on convex proxies that lead to poor\napproximations. We then discuss the possibility of \"exact\" reranking algorithms\nbased on mathematical programming. We prove that a relaxed version of the\n\"exact\" problem has the same optimal solution, and provide an empirical\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 02:04:40 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Rudin", "Cynthia", ""], ["Wang", "Yining", ""]]}, {"id": "1802.07401", "submitter": "Arjun Karuvally", "authors": "Arjun Karuvally", "title": "A Study into the similarity in generator and discriminator in GAN\n  architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular generative model that has high-quality results is the Generative\nAdversarial Networks(GAN). This type of architecture consists of two separate\nnetworks that play against each other. The generator creates an output from the\ninput noise that is given to it. The discriminator has the task of determining\nif the input to it is real or fake. This takes place constantly eventually\nleads to the generator modeling the target distribution. This paper includes a\nstudy into the actual weights learned by the network and a study into the\nsimilarity of the discriminator and generator networks. The paper also tries to\nleverage the similarity between these networks and shows that indeed both the\nnetworks may have a similar structure with experimental evidence with a novel\nshared architecture.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 02:24:28 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Karuvally", "Arjun", ""]]}, {"id": "1802.07426", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Yoshua Bengio, Vikas Verma, Leslie Pack Kaelbling", "title": "Generalization in Machine Learning via Analytical Learning Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": "Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2018-019", "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel measure-theoretic theory for machine learning\nthat does not require statistical assumptions. Based on this theory, a new\nregularization method in deep learning is derived and shown to outperform\nprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\ntheory provides a theoretical basis for a family of practically successful\nregularization methods in deep learning. We discuss several consequences of our\nresults on one-shot learning, representation learning, deep learning, and\ncurriculum learning. Unlike statistical learning theory, the proposed learning\ntheory analyzes each problem instance individually via measure theory, rather\nthan a set of problem instances via statistics. As a result, it provides\ndifferent types of results and insights when compared to statistical learning\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 05:03:52 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:39:36 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 22:23:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Bengio", "Yoshua", ""], ["Verma", "Vikas", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1802.07434", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari, Joydeep Ghosh, Mingyuan Zhou", "title": "Nonparametric Bayesian Sparse Graph Linear Dynamical Systems", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is\nproposed to model sequentially observed multivariate data. SGLDS uses the\nBernoulli-Poisson link together with a gamma process to generate an infinite\ndimensional sparse random graph to model state transitions. Depending on the\nsparsity pattern of the corresponding row and column of the graph affinity\nmatrix, a latent state of SGLDS can be categorized as either a non-dynamic\nstate or a dynamic one. A normal-gamma construction is used to shrink the\nenergy captured by the non-dynamic states, while the dynamic states can be\nfurther categorized into live, absorbing, or noise-injection states, which\ncapture different types of dynamical components of the underlying time series.\nThe state-of-the-art performance of SGLDS is demonstrated with experiments on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:15:21 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Kalantari", "Rahi", ""], ["Ghosh", "Joydeep", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1802.07442", "submitter": "Nick Haber", "authors": "Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins", "title": "Learning to Play with Intrinsically-Motivated Self-Aware Agents", "comments": "In NIPS 2018. 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to mathematically formalize these abilities using a\nneural network that implements curiosity-driven intrinsic motivation. Using a\nsimple but ecologically naturalistic simulated environment in which an agent\ncan move and interact with objects it sees, we propose a \"world-model\" network\nthat learns to predict the dynamic consequences of the agent's actions.\nSimultaneously, we train a separate explicit \"self-model\" that allows the agent\nto track the error map of its own world-model, and then uses the self-model to\nadversarially challenge the developing world-model. We demonstrate that this\npolicy causes the agent to explore novel and informative interactions with its\nenvironment, leading to the generation of a spectrum of complex behaviors,\nincluding ego-motion prediction, object attention, and object gathering.\nMoreover, the world-model that the agent learns supports improved performance\non object dynamics prediction, detection, localization and recognition tasks.\nTaken together, our results are initial steps toward creating flexible\nautonomous agents that self-supervise in complex novel physical environments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:01:43 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 20:08:46 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Haber", "Nick", ""], ["Mrowca", "Damian", ""], ["Fei-Fei", "Li", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1802.07444", "submitter": "Chen Luo", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and\npopular variants of MCMC for problems when an MCMC state consists of an unknown\nnumber of components. It is well known that state-of-the-art methods for\nsplit-merge MCMC do not scale well. Strategies for rapid mixing requires smart\nand informative proposals to reduce the rejection rate. However, all known\nsmart proposals involve expensive operations to suggest informative\ntransitions. As a result, the cost of each iteration is prohibitive for massive\nscale datasets. It is further known that uninformative but computationally\nefficient proposals, such as random split-merge, leads to extremely slow\nconvergence. This tradeoff between mixing time and per update cost seems hard\nto get around.\n  In this paper, we show a sweet spot. We leverage some unique properties of\nweighted MinHash, which is a popular LSH, to design a novel class of\nsplit-merge proposals which are significantly more informative than random\nsampling but at the same time efficient to compute. Overall, we obtain a\nsuperior tradeoff between convergence and per update cost. As a direct\nconsequence, our proposals are around 6X faster than the state-of-the-art\nsampling methods on two large real datasets KDDCUP and PubMed with several\nmillions of entities and thousands of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:03:32 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 20:36:49 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 05:06:40 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1802.07461", "submitter": "Nick Haber", "authors": "Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins", "title": "Emergence of Structured Behaviors from Curiosity-Based Intrinsic\n  Motivation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to replicate some of these abilities with a neural\nnetwork that implements curiosity-driven intrinsic motivation. Using a simple\nbut ecologically naturalistic simulated environment in which the agent can move\nand interact with objects it sees, the agent learns a world model predicting\nthe dynamic consequences of its actions. Simultaneously, the agent learns to\ntake actions that adversarially challenge the developing world model, pushing\nthe agent to explore novel and informative interactions with its environment.\nWe demonstrate that this policy leads to the self-supervised emergence of a\nspectrum of complex behaviors, including ego motion prediction, object\nattention, and object gathering. Moreover, the world model that the agent\nlearns supports improved performance on object dynamics prediction and\nlocalization tasks. Our results are a proof-of-principle that computational\nmodels of intrinsic motivation might account for key features of developmental\nvisuomotor learning in infants.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:13:12 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Haber", "Nick", ""], ["Mrowca", "Damian", ""], ["Fei-Fei", "Li", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1802.07481", "submitter": "Mathurin Massias", "authors": "Mathurin Massias and Alexandre Gramfort and Joseph Salmon", "title": "Celer: a Fast Solver for the Lasso with Dual Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex sparsity-inducing regularizations are ubiquitous in high-dimensional\nmachine learning, but solving the resulting optimization problems can be slow.\nTo accelerate solvers, state-of-the-art approaches consist in reducing the size\nof the optimization problem at hand. In the context of regression, this can be\nachieved either by discarding irrelevant features (screening techniques) or by\nprioritizing features likely to be included in the support of the solution\n(working set techniques). Duality comes into play at several steps in these\ntechniques. Here, we propose an extrapolation technique starting from a\nsequence of iterates in the dual that leads to the construction of improved\ndual points. This enables a tighter control of optimality as used in stopping\ncriterion, as well as better screening performance of Gap Safe rules. Finally,\nwe propose a working set strategy based on an aggressive use of Gap Safe\nscreening rules. Thanks to our new dual point construction, we show significant\ncomputational speedups on multiple real-world problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 09:40:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 15:13:35 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 16:15:16 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Massias", "Mathurin", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1802.07510", "submitter": "Andreas Loukas", "authors": "Andreas Loukas, Pierre Vandergheynst", "title": "Spectrally approximating large graphs with smaller graphs", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does coarsening affect the spectrum of a general graph? We provide\nconditions such that the principal eigenvalues and eigenspaces of a coarsened\nand original graph Laplacian matrices are close. The achieved approximation is\nshown to depend on standard graph-theoretic properties, such as the degree and\neigenvalue distributions, as well as on the ratio between the coarsened and\nactual graph sizes. Our results carry implications for learning methods that\nutilize coarsening. For the particular case of spectral clustering, they imply\nthat coarse eigenvectors can be used to derive good quality assignments even\nwithout refinement---this phenomenon was previously observed, but lacked formal\njustification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:58:25 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Loukas", "Andreas", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1802.07513", "submitter": "Roi Naveiro", "authors": "Roi Naveiro, Alberto Redondo, David R\\'ios Insua, Fabrizio Ruggeri", "title": "Adversarial classification: An adversarial risk analysis approach", "comments": "Published in the International Journal for Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning, 113, 133-148\n  (2019)", "doi": "10.1016/j.ijar.2019.07.003", "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification problems in security settings are usually contemplated as\nconfrontations in which one or more adversaries try to fool a classifier to\nobtain a benefit. Most approaches to such adversarial classification problems\nhave focused on game theoretical ideas with strong underlying common knowledge\nassumptions, which are actually not realistic in security domains. We provide\nan alternative framework to such problem based on adversarial risk analysis,\nwhich we illustrate with several examples. Computational and implementation\nissues are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 11:07:55 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 15:26:38 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 15:10:28 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Naveiro", "Roi", ""], ["Redondo", "Alberto", ""], ["Insua", "David R\u00edos", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "1802.07528", "submitter": "Zilong Tan", "authors": "Zilong Tan and Sayan Mukherjee", "title": "Learning Integral Representations of Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a representation of Gaussian processes (GPs) based on powers of\nthe integral operator defined by a kernel function, we call these stochastic\nprocesses integral Gaussian processes (IGPs). Sample paths from IGPs are\nfunctions contained within the reproducing kernel Hilbert space (RKHS) defined\nby the kernel function, in contrast sample paths from the standard GP are not\nfunctions within the RKHS. We develop computationally efficient non-parametric\nregression models based on IGPs. The main innovation in our regression\nalgorithm is the construction of a low dimensional subspace that captures the\ninformation most relevant to explaining variation in the response. We use ideas\nfrom supervised dimension reduction to compute this subspace. The result of\nusing the construction we propose involves significant improvements in the\ncomputational complexity of estimating kernel hyper-parameters as well as\nreducing the prediction variance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 11:54:00 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:30:11 GMT"}, {"version": "v3", "created": "Sat, 6 Oct 2018 14:44:33 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 19:38:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Tan", "Zilong", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1802.07535", "submitter": "Iryna Korshunova", "authors": "Iryna Korshunova, Jonas Degrave, Ferenc Husz\\'ar, Yarin Gal, Arthur\n  Gretton, Joni Dambre", "title": "BRUNO: A Deep Recurrent Model for Exchangeable Data", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel model architecture which leverages deep learning tools to\nperform exact Bayesian inference on sets of high dimensional, complex\nobservations. Our model is provably exchangeable, meaning that the joint\ndistribution over observations is invariant under permutation: this property\nlies at the heart of Bayesian inference. The model does not require variational\napproximations to train, and new samples can be generated conditional on\nprevious samples, with cost linear in the size of the conditioning set. The\nadvantages of our architecture are demonstrated on learning tasks that require\ngeneralisation from short observed sequences while modelling sequence\nvariability, such as conditional image generation, few-shot learning, and\nanomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 12:07:48 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 12:11:44 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 11:52:47 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Korshunova", "Iryna", ""], ["Degrave", "Jonas", ""], ["Husz\u00e1r", "Ferenc", ""], ["Gal", "Yarin", ""], ["Gretton", "Arthur", ""], ["Dambre", "Joni", ""]]}, {"id": "1802.07543", "submitter": "Tim van Erven", "authors": "Dirk van der Hoeven and Tim van Erven and Wojciech Kot{\\l}owski", "title": "The Many Faces of Exponential Weights in Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard introduction to online learning might place Online Gradient\nDescent at its center and then proceed to develop generalizations and\nextensions like Online Mirror Descent and second-order methods. Here we explore\nthe alternative approach of putting Exponential Weights (EW) first. We show\nthat many standard methods and their regret bounds then follow as a special\ncase by plugging in suitable surrogate losses and playing the EW posterior\nmean. For instance, we easily recover Online Gradient Descent by using EW with\na Gaussian prior on linearized losses, and, more generally, all instances of\nOnline Mirror Descent based on regular Bregman divergences also correspond to\nEW with a prior that depends on the mirror map. Furthermore, appropriate\nquadratic surrogate losses naturally give rise to Online Gradient Descent for\nstrongly convex losses and to Online Newton Step. We further interpret several\nrecent adaptive methods (iProd, Squint, and a variation of Coin Betting for\nexperts) as a series of closely related reductions to exp-concave surrogate\nlosses that are then handled by Exponential Weights. Finally, a benefit of our\nEW interpretation is that it opens up the possibility of sampling from the EW\nposterior distribution instead of playing the mean. As already observed by\nBubeck and Eldan, this recovers the best-known rate in Online Bandit Linear\nOptimization.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 12:38:36 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:08:28 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["van der Hoeven", "Dirk", ""], ["van Erven", "Tim", ""], ["Kot\u0142owski", "Wojciech", ""]]}, {"id": "1802.07564", "submitter": "Yasuhiro Fujita", "authors": "Yasuhiro Fujita and Shin-ichi Maeda", "title": "Clipped Action Policy Gradient", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many continuous control tasks have bounded action spaces. When policy\ngradient methods are applied to such tasks, out-of-bound actions need to be\nclipped before execution, while policies are usually optimized as if the\nactions are not clipped. We propose a policy gradient estimator that exploits\nthe knowledge of actions being clipped to reduce the variance in estimation. We\nprove that our estimator, named clipped action policy gradient (CAPG), is\nunbiased and achieves lower variance than the conventional estimator that\nignores action bounds. Experimental results demonstrate that CAPG generally\noutperforms the conventional estimator, indicating that it is a better policy\ngradient estimator for continuous control tasks. The source code is available\nat https://github.com/pfnet-research/capg.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:39:28 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 10:19:00 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Fujita", "Yasuhiro", ""], ["Maeda", "Shin-ichi", ""]]}, {"id": "1802.07569", "submitter": "German I. Parisi", "authors": "German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan,\n  Stefan Wermter", "title": "Continual Lifelong Learning with Neural Networks: A Review", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2019.01.012.", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humans and animals have the ability to continually acquire, fine-tune, and\ntransfer knowledge and skills throughout their lifespan. This ability, referred\nto as lifelong learning, is mediated by a rich set of neurocognitive mechanisms\nthat together contribute to the development and specialization of our\nsensorimotor skills as well as to long-term memory consolidation and retrieval.\nConsequently, lifelong learning capabilities are crucial for autonomous agents\ninteracting in the real world and processing continuous streams of information.\nHowever, lifelong learning remains a long-standing challenge for machine\nlearning and neural network models since the continual acquisition of\nincrementally available information from non-stationary data distributions\ngenerally leads to catastrophic forgetting or interference. This limitation\nrepresents a major drawback for state-of-the-art deep neural network models\nthat typically learn representations from stationary batches of training data,\nthus without accounting for situations in which information becomes\nincrementally available over time. In this review, we critically summarize the\nmain challenges linked to lifelong learning for artificial learning systems and\ncompare existing neural network approaches that alleviate, to different\nextents, catastrophic forgetting. We discuss well-established and emerging\nresearch motivated by lifelong learning factors in biological systems such as\nstructural plasticity, memory replay, curriculum and transfer learning,\nintrinsic motivation, and multisensory integration.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:53:35 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 06:59:28 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 18:40:25 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 01:28:39 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Parisi", "German I.", ""], ["Kemker", "Ronald", ""], ["Part", "Jose L.", ""], ["Kanan", "Christopher", ""], ["Wermter", "Stefan", ""]]}, {"id": "1802.07572", "submitter": "David  McAllester", "authors": "David McAllester", "title": "Information Theoretic Co-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an information theoretic co-training objective for\nunsupervised learning. We consider the problem of predicting the future. Rather\nthan predict future sensations (image pixels or sound waves) we predict\n\"hypotheses\" to be confirmed by future sensations. More formally, we assume a\npopulation distribution on pairs $(x,y)$ where we can think of $x$ as a past\nsensation and $y$ as a future sensation. We train both a predictor model\n$P_\\Phi(z|x)$ and a confirmation model $P_\\Psi(z|y)$ where we view $z$ as\nhypotheses (when predicted) or facts (when confirmed). For a population\ndistribution on pairs $(x,y)$ we focus on the problem of measuring the mutual\ninformation between $x$ and $y$. By the data processing inequality this mutual\ninformation is at least as large as the mutual information between $x$ and $z$\nunder the distribution on triples $(x,z,y)$ defined by the confirmation model\n$P_\\Psi(z|y)$. The information theoretic training objective for $P_\\Phi(z|x)$\nand $P_\\Psi(z|y)$ can be viewed as a form of co-training where we want the\nprediction from $x$ to match the confirmation from $y$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:01:20 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 13:06:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["McAllester", "David", ""]]}, {"id": "1802.07575", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Marc Goodfellow", "title": "Emulating dynamic non-linear simulators using Gaussian processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 139, 178 - 196 (2019)", "doi": "10.1016/j.csda.2019.05.006", "report-no": null, "categories": "stat.ML math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic emulation of non-linear deterministic computer codes where the\noutput is a time series, possibly multivariate, is examined. Such computer\nmodels simulate the evolution of some real-world phenomenon over time, for\nexample models of the climate or the functioning of the human brain. The models\nwe are interested in are highly non-linear and exhibit tipping points,\nbifurcations and chaotic behaviour. However, each simulation run could be too\ntime-consuming to perform analyses that require many runs, including\nquantifying the variation in model output with respect to changes in the\ninputs. Therefore, Gaussian process emulators are used to approximate the\noutput of the code. To do this, the flow map of the system under study is\nemulated over a short time period. Then, it is used in an iterative way to\npredict the whole time series. A number of ways are proposed to take into\naccount the uncertainty of inputs to the emulators, after fixed initial\nconditions, and the correlation between them through the time series. The\nmethodology is illustrated with two examples: the highly non-linear dynamical\nsystems described by the Lorenz and Van der Pol equations. In both cases, the\npredictive performance is relatively high and the measure of uncertainty\nprovided by the method reflects the extent of predictability in each system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:07:36 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 13:57:27 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 16:54:16 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 19:32:23 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Goodfellow", "Marc", ""]]}, {"id": "1802.07581", "submitter": "Shengyu Zhu", "authors": "Shengyu Zhu, Biao Chen, Pengfei Yang, Zhitang Chen", "title": "Universal Hypothesis Testing with Kernels: Asymptotically Optimal Tests\n  for Goodness of Fit", "comments": "camera-ready version for AISTATS 2019 (with supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the asymptotic performance of nonparametric goodness of fit\ntesting. The exponential decay rate of the type-II error probability is used as\nthe asymptotic performance metric, and a test is optimal if it achieves the\nmaximum rate subject to a constant level constraint on the type-I error\nprobability. We show that two classes of Maximum Mean Discrepancy (MMD) based\ntests attain this optimality on $\\mathbb R^d$, while the quadratic-time Kernel\nStein Discrepancy (KSD) based tests achieve the maximum exponential decay rate\nunder a relaxed level constraint. Under the same performance metric, we proceed\nto show that the quadratic-time MMD based two-sample tests are also optimal for\ngeneral two-sample problems, provided that kernels are bounded continuous and\ncharacteristic. Key to our approach are Sanov's theorem from large deviation\ntheory and the weak metrizable properties of the MMD and KSD.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:24:30 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 12:04:39 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 03:38:17 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhu", "Shengyu", ""], ["Chen", "Biao", ""], ["Yang", "Pengfei", ""], ["Chen", "Zhitang", ""]]}, {"id": "1802.07606", "submitter": "Luisa Zintgraf", "authors": "Luisa M Zintgraf, Diederik M Roijers, Sjoerd Linders, Catholijn M\n  Jonker, Ann Now\\'e", "title": "Ordered Preference Elicitation Strategies for Supporting Multi-Objective\n  Decision Making", "comments": "AAMAS 2018, Source code at\n  https://github.com/lmzintgraf/gp_pref_elicit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-objective decision planning and learning, much attention is paid to\nproducing optimal solution sets that contain an optimal policy for every\npossible user preference profile. We argue that the step that follows, i.e,\ndetermining which policy to execute by maximising the user's intrinsic utility\nfunction over this (possibly infinite) set, is under-studied. This paper aims\nto fill this gap. We build on previous work on Gaussian processes and pairwise\ncomparisons for preference modelling, extend it to the multi-objective decision\nsupport scenario, and propose new ordered preference elicitation strategies\nbased on ranking and clustering. Our main contribution is an in-depth\nevaluation of these strategies using computer and human-based experiments. We\nshow that our proposed elicitation strategies outperform the currently used\npairwise methods, and found that users prefer ranking most. Our experiments\nfurther show that utilising monotonicity information in GPs by using a linear\nprior mean at the start and virtual comparisons to the nadir and ideal points,\nincreases performance. We demonstrate our decision support framework in a\nreal-world study on traffic regulation, conducted with the city of Amsterdam.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 15:05:15 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Zintgraf", "Luisa M", ""], ["Roijers", "Diederik M", ""], ["Linders", "Sjoerd", ""], ["Jonker", "Catholijn M", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1802.07648", "submitter": "Nicolas Gillis", "authors": "Maryam Abdolali, Nicolas Gillis, Mohammad Rahmati", "title": "Scalable and Robust Sparse Subspace Clustering Using Randomized\n  Clustering and Multilayer Graphs", "comments": "25 pages, v2: typos corrected", "journal-ref": "Signal Processing 163, pp. 166-180, 2019", "doi": "10.1016/j.sigpro.2019.05.017", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) is one of the current state-of-the-art\nmethods for partitioning data points into the union of subspaces, with strong\ntheoretical guarantees. However, it is not practical for large data sets as it\nrequires solving a LASSO problem for each data point, where the number of\nvariables in each LASSO problem is the number of data points. To improve the\nscalability of SSC, we propose to select a few sets of anchor points using a\nrandomized hierarchical clustering method, and, for each set of anchor points,\nsolve the LASSO problems for each data point allowing only anchor points to\nhave a non-zero weight (this reduces drastically the number of variables). This\ngenerates a multilayer graph where each layer corresponds to a different set of\nanchor points. Using the Grassmann manifold of orthogonal matrices, the shared\nconnectivity among the layers is summarized within a single subspace. Finally,\nwe use $k$-means clustering within that subspace to cluster the data points,\nsimilarly as done by spectral clustering in SSC. We show on both synthetic and\nreal-world data sets that the proposed method not only allows SSC to scale to\nlarge-scale data sets, but that it is also much more robust as it performs\nsignificantly better on noisy data and on data with close susbspaces and\noutliers, while it is not prone to oversegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:21:42 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 06:59:12 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Abdolali", "Maryam", ""], ["Gillis", "Nicolas", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "1802.07687", "submitter": "Emily Denton", "authors": "Emily Denton and Rob Fergus", "title": "Stochastic Video Generation with a Learned Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating video frames that accurately predict future world states is\nchallenging. Existing approaches either fail to capture the full distribution\nof outcomes, or yield blurry generations, or both. In this paper we introduce\nan unsupervised video generation model that learns a prior model of uncertainty\nin a given environment. Video frames are generated by drawing samples from this\nprior and combining them with a deterministic estimate of the future frame. The\napproach is simple and easily trained end-to-end on a variety of datasets.\nSample generations are both varied and sharp, even many frames into the future,\nand compare favorably to those from existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:36:27 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 17:39:23 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Denton", "Emily", ""], ["Fergus", "Rob", ""]]}, {"id": "1802.07714", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto", "title": "Detecting Learning vs Memorization in Deep Neural Networks using Shared\n  Structure Validation Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The roles played by learning and memorization represent an important topic in\ndeep learning research. Recent work on this subject has shown that the\noptimization behavior of DNNs trained on shuffled labels is qualitatively\ndifferent from DNNs trained with real labels. Here, we propose a novel\npermutation approach that can differentiate memorization from learning in deep\nneural networks (DNNs) trained as usual (i.e., using the real labels to guide\nthe learning, rather than shuffled labels). The evaluation of weather the DNN\nhas learned and/or memorized, happens in a separate step where we compare the\npredictive performance of a shallow classifier trained with the features\nlearned by the DNN, against multiple instances of the same classifier, trained\non the same input, but using shuffled labels as outputs. By evaluating these\nshallow classifiers in validation sets that share structure with the training\nset, we are able to tell apart learning from memorization. Application of our\npermutation approach to multi-layer perceptrons and convolutional neural\nnetworks trained on image data corroborated many findings from other groups.\nMost importantly, our illustrations also uncovered interesting dynamic patterns\nabout how DNNs memorize over increasing numbers of training epochs, and support\nthe surprising result that DNNs are still able to learn, rather than only\nmemorize, when trained with pure Gaussian noise as input.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 18:45:23 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Neto", "Elias Chaibub", ""]]}, {"id": "1802.07756", "submitter": "Ritabrata Maiti", "authors": "Ritabrata Maiti", "title": "Determining the best classifier for predicting the value of a boolean\n  field on a blood donor database using genetic algorithms", "comments": "GitHub Repository here:\n  https://github.com/ritabratamaiti/Blooddonorprediction", "journal-ref": null, "doi": "10.5281/zenodo.1336304", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: Thanks to digitization, we often have access to large databases,\nconsisting of various fields of information, ranging from numbers to texts and\neven boolean values. Such databases lend themselves especially well to machine\nlearning, classification and big data analysis tasks. We are able to train\nclassifiers, using already existing data and use them for predicting the values\nof a certain field, given that we have information regarding the other fields.\nMost specifically, in this study, we look at the Electronic Health Records\n(EHRs) that are compiled by hospitals. These EHRs are convenient means of\naccessing data of individual patients, but there processing as a whole still\nremains a task. However, EHRs that are composed of coherent, well-tabulated\nstructures lend themselves quite well to the application to machine language,\nvia the usage of classifiers. In this study, we look at a Blood Transfusion\nService Center Data Set (Data taken from the Blood Transfusion Service Center\nin Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python.\nFrom Support Vector Machines(SVM), we use Support Vector Classification(SVC),\nfrom the linear model we import Perceptron. We also used the\nK.neighborsclassifier and the decision tree classifiers. Furthermore, we use\nthe TPOT library to find an optimized pipeline using genetic algorithms. Using\nthe above classifiers, we score each one of them using k fold cross-validation.\n  Contact: ritabratamaiti@hiretrex.com GitHub Repository:\nhttps://github.com/ritabratamaiti/Blooddonorprediction\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:09:17 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 13:11:03 GMT"}, {"version": "v3", "created": "Sat, 21 Apr 2018 20:17:24 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 20:38:14 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Maiti", "Ritabrata", ""]]}, {"id": "1802.07773", "submitter": "Yihong Wu", "authors": "Jason M. Klusowski and Yihong Wu", "title": "Counting Motifs with Graph Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied researchers often construct a network from a random sample of nodes\nin order to infer properties of the parent network. Two of the most widely used\nsampling schemes are subgraph sampling, where we sample each vertex\nindependently with probability $p$ and observe the subgraph induced by the\nsampled vertices, and neighborhood sampling, where we additionally observe the\nedges between the sampled vertices and their neighbors.\n  In this paper, we study the problem of estimating the number of motifs as\ninduced subgraphs under both models from a statistical perspective. We show\nthat: for any connected $h$ on $k$ vertices, to estimate $s=\\mathsf{s}(h,G)$,\nthe number of copies of $h$ in the parent graph $G$ of maximum degree $d$, with\na multiplicative error of $\\epsilon$, (a) For subgraph sampling, the optimal\nsampling ratio $p$ is $\\Theta_{k}(\\max\\{ (s\\epsilon^2)^{-\\frac{1}{k}}, \\;\n\\frac{d^{k-1}}{s\\epsilon^{2}} \\})$, achieved by Horvitz-Thompson type of\nestimators. (b) For neighborhood sampling, we propose a family of estimators,\nencompassing and outperforming the Horvitz-Thompson estimator and achieving the\nsampling ratio $O_{k}(\\min\\{ (\\frac{d}{s\\epsilon^2})^{\\frac{1}{k-1}}, \\;\n\\sqrt{\\frac{d^{k-2}}{s\\epsilon^2}}\\})$. This is shown to be optimal for all\nmotifs with at most $4$ vertices and cliques of all sizes.\n  The matching minimax lower bounds are established using certain algebraic\nproperties of subgraph counts. These results quantify how much more informative\nneighborhood sampling is than subgraph sampling, as empirically verified by\nexperiments on both synthetic and real-world data. We also address the issue of\nadaptation to the unknown maximum degree, and study specific problems for\nparent graphs with additional structures, e.g., trees or planar graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:51:45 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Wu", "Yihong", ""]]}, {"id": "1802.07796", "submitter": "D. Khu\\^e L\\^e-Huu", "authors": "D. Khu\\^e L\\^e-Huu and Nikos Paragios", "title": "Continuous Relaxation of MAP Inference: A Nonconvex Perspective", "comments": "Accepted for publication at the 2018 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a nonconvex continuous relaxation of MAP inference in\ndiscrete Markov random fields (MRFs). We show that for arbitrary MRFs, this\nrelaxation is tight, and a discrete stationary point of it can be easily\nreached by a simple block coordinate descent algorithm. In addition, we study\nthe resolution of this relaxation using popular gradient methods, and further\npropose a more effective solution using a multilinear decomposition framework\nbased on the alternating direction method of multipliers (ADMM). Experiments on\nmany real-world problems demonstrate that the proposed ADMM significantly\noutperforms other nonconvex relaxation based methods, and compares favorably\nwith state of the art MRF optimization algorithms in different settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:42:58 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 22:12:55 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["L\u00ea-Huu", "D. Khu\u00ea", ""], ["Paragios", "Nikos", ""]]}, {"id": "1802.07814", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan", "title": "Learning to Explain: An Information-Theoretic Perspective on Model\n  Interpretation", "comments": "Accepted to ICML 2018 as a long oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce instancewise feature selection as a methodology for model\ninterpretation. Our method is based on learning a function to extract a subset\nof features that are most informative for each given example. This feature\nselector is trained to maximize the mutual information between selected\nfeatures and the response variable, where the conditional distribution of the\nresponse variable given the input is the model to be explained. We develop an\nefficient variational approximation to the mutual information, and show the\neffectiveness of our method on a variety of synthetic and real data sets using\nboth quantitative metrics and human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 21:16:21 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 03:38:00 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Chen", "Jianbo", ""], ["Song", "Le", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1802.07833", "submitter": "Tianbing Xu", "authors": "Tianbing Xu", "title": "Variational Inference for Policy Gradient", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the seminal work on Stein Variational Inference and Stein\nVariational Policy Gradient, we derived a method to generate samples from the\nposterior variational parameter distribution by \\textit{explicitly} minimizing\nthe KL divergence to match the target distribution in an amortize fashion.\nConsequently, we applied this varational inference technique into vanilla\npolicy gradient, TRPO and PPO with Bayesian Neural Network parameterizations\nfor reinforcement learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:18:20 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 23:57:28 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xu", "Tianbing", ""]]}, {"id": "1802.07834", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer, Vladislav\n  Tempez", "title": "Learning to Gather without Communication", "comments": "Preliminary version, presented at the 5th Biological Distributed\n  Algorithms Workshop. Washington D.C, July 28th, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard belief on emerging collective behavior is that it emerges from\nsimple individual rules. Most of the mathematical research on such collective\nbehavior starts from imperative individual rules, like always go to the center.\nBut how could an (optimal) individual rule emerge during a short period within\nthe group lifetime, especially if communication is not available. We argue that\nsuch rules can actually emerge in a group in a short span of time via\ncollective (multi-agent) reinforcement learning, i.e learning via rewards and\npunishments. We consider the gathering problem: several agents (social animals,\nswarming robots...) must gather around a same position, which is not determined\nin advance. They must do so without communication on their planned decision,\njust by looking at the position of other agents. We present the first\nexperimental evidence that a gathering behavior can be learned without\ncommunication in a partially observable environment. The learned behavior has\nthe same properties as a self-stabilizing distributed algorithm, as processes\ncan gather from any initial state (and thus tolerate any transient failure).\nBesides, we show that it is possible to tolerate the brutal loss of up to 90\\%\nof agents without significant impact on the behavior.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:26:21 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Maurer", "Alexandre", ""], ["Tempez", "Vladislav", ""]]}, {"id": "1802.07877", "submitter": "Maryam Sabzevari", "authors": "Maryam Sabzevari, Gonzalo Mart\\'inez-Mu\\~noz, Alberto Su\\'arez", "title": "Pooling homogeneous ensembles to build heterogeneous ones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ensemble methods, the outputs of a collection of diverse classifiers are\ncombined in the expectation that the global prediction be more accurate than\nthe individual ones. Heterogeneous ensembles consist of predictors of different\ntypes, which are likely to have different biases. If these biases are\ncomplementary, the combination of their decisions is beneficial. In this work,\na family of heterogeneous ensembles is built by pooling classifiers from M\nhomogeneous ensembles of different types of size T. Depending on the fraction\nof base classifiers of each type, a particular heterogeneous combination in\nthis family is represented by a point in a regular simplex in M dimensions. The\nM vertices of this simplex represent the different homogeneous ensembles. A\ndisplacement away from one of these vertices effects a smooth transformation of\nthe corresponding homogeneous ensemble into a heterogeneous one. The optimal\ncomposition of such heterogeneous ensemble can be determined using\ncross-validation or, if bootstrap samples are used to build the individual\nclassifiers, out-of-bag data. An empirical analysis of such combinations of\nbootstraped ensembles composed of neural networks, SVMs, and random trees (i.e.\nfrom a standard random forest) illustrates the gains that can be achieved by\nthis heterogeneous ensemble creation method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:17:42 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 21:15:33 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sabzevari", "Maryam", ""], ["Mart\u00ednez-Mu\u00f1oz", "Gonzalo", ""], ["Su\u00e1rez", "Alberto", ""]]}, {"id": "1802.07889", "submitter": "Jiantao Jiao", "authors": "Yanjun Han and Jiantao Jiao and Chuan-Zheng Lee and Tsachy Weissman\n  and Yihong Wu and Tiancheng Yu", "title": "Entropy Rate Estimation for Markov Chains with Large State Space", "comments": "Published as a conference paper on NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the entropy based on data is one of the prototypical problems in\ndistribution property testing and estimation. For estimating the Shannon\nentropy of a distribution on $S$ elements with independent samples,\n[Paninski2004] showed that the sample complexity is sublinear in $S$, and\n[Valiant--Valiant2011] showed that consistent estimation of Shannon entropy is\npossible if and only if the sample size $n$ far exceeds $\\frac{S}{\\log S}$. In\nthis paper we consider the problem of estimating the entropy rate of a\nstationary reversible Markov chain with $S$ states from a sample path of $n$\nobservations. We show that:\n  (1) As long as the Markov chain mixes not too slowly, i.e., the relaxation\ntime is at most $O(\\frac{S}{\\ln^3 S})$, consistent estimation is achievable\nwhen $n \\gg \\frac{S^2}{\\log S}$.\n  (2) As long as the Markov chain has some slight dependency, i.e., the\nrelaxation time is at least $1+\\Omega(\\frac{\\ln^2 S}{\\sqrt{S}})$, consistent\nestimation is impossible when $n \\lesssim \\frac{S^2}{\\log S}$.\n  Under both assumptions, the optimal estimation accuracy is shown to be\n$\\Theta(\\frac{S^2}{n \\log S})$. In comparison, the empirical entropy rate\nrequires at least $\\Omega(S^2)$ samples to be consistent, even when the Markov\nchain is memoryless. In addition to synthetic experiments, we also apply the\nestimators that achieve the optimal sample complexity to estimate the entropy\nrate of the English language in the Penn Treebank and the Google One Billion\nWords corpora, which provides a natural benchmark for language modeling and\nrelates it directly to the widely used perplexity measure.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 03:10:17 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 04:51:30 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 06:45:27 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Han", "Yanjun", ""], ["Jiao", "Jiantao", ""], ["Lee", "Chuan-Zheng", ""], ["Weissman", "Tsachy", ""], ["Wu", "Yihong", ""], ["Yu", "Tiancheng", ""]]}, {"id": "1802.07917", "submitter": "Cong Shen", "authors": "Zhiyang Wang, Ruida Zhou, Cong Shen", "title": "Regional Multi-Armed Bandits", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the classic multi-armed bandit problem where the\nexpected reward of each arm is a function of an unknown parameter. The arms are\ndivided into different groups, each of which has a common parameter. Therefore,\nwhen the player selects an arm at each time slot, information of other arms in\nthe same group is also revealed. This regional bandit model naturally bridges\nthe non-informative bandit setting where the player can only learn the chosen\narm, and the global bandit model where sampling one arms reveals information of\nall arms. We propose an efficient algorithm, UCB-g, that solves the regional\nbandit problem by combining the Upper Confidence Bound (UCB) and greedy\nprinciples. Both parameter-dependent and parameter-free regret upper bounds are\nderived. We also establish a matching lower bound, which proves the\norder-optimality of UCB-g. Moreover, we propose SW-UCB-g, which is an extension\nof UCB-g for a non-stationary environment where the parameters slowly vary over\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:03:23 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wang", "Zhiyang", ""], ["Zhou", "Ruida", ""], ["Shen", "Cong", ""]]}, {"id": "1802.07927", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, S\\'ebastien Rouault", "title": "The Hidden Vulnerability of Distributed Learning in Byzantium", "comments": "Accepted to ICML 2018 as a long talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While machine learning is going through an era of celebrated success,\nconcerns have been raised about the vulnerability of its backbone: stochastic\ngradient descent (SGD). Recent approaches have been proposed to ensure the\nrobustness of distributed SGD against adversarial (Byzantine) workers sending\npoisoned gradients during the training phase. Some of these approaches have\nbeen proven Byzantine-resilient: they ensure the convergence of SGD despite the\npresence of a minority of adversarial workers.\n  We show in this paper that convergence is not enough. In high dimension $d\n\\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make\nSGD converge to ineffective models. More precisely, we bring to light that\nexisting Byzantine-resilient schemes leave a margin of poisoning of\n$\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$.\nBased on this leeway, we build a simple attack, and experimentally show its\nstrong to utmost effectivity on CIFAR-10 and MNIST.\n  We introduce Bulyan, and prove it significantly reduces the attackers leeway\nto a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan\ndoes not suffer the fragility of existing aggregation rules and, at a\nreasonable cost in terms of required batch size, achieves convergence as if\nonly non-Byzantine gradients had been used to update the model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:42:00 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:10:23 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "S\u00e9bastien", ""]]}, {"id": "1802.07928", "submitter": "El Mahdi El Mhamdi", "authors": "Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Rhicheek\n  Patra, Mahsa Taziki", "title": "Asynchronous Byzantine Machine Learning (the case of SGD)", "comments": "accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Asynchronous distributed machine learning solutions have proven very\neffective so far, but always assuming perfectly functioning workers. In\npractice, some of the workers can however exhibit Byzantine behavior, caused by\nhardware failures, software bugs, corrupt data, or even malicious attacks. We\nintroduce \\emph{Kardam}, the first distributed asynchronous stochastic gradient\ndescent (SGD) algorithm that copes with Byzantine workers. Kardam consists of\ntwo complementary components: a filtering and a dampening component. The first\nis scalar-based and ensures resilience against $\\frac{1}{3}$ Byzantine workers.\nEssentially, this filter leverages the Lipschitzness of cost functions and acts\nas a self-stabilizer against Byzantine workers that would attempt to corrupt\nthe progress of SGD. The dampening component bounds the convergence rate by\nadjusting to stale information through a generic gradient weighting scheme. We\nprove that Kardam guarantees almost sure convergence in the presence of\nasynchrony and Byzantine behavior, and we derive its convergence rate. We\nevaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead\nwith respect to non Byzantine-resilient solutions. We empirically show that\nKardam does not introduce additional noise to the learning procedure but does\ninduce a slowdown (the cost of Byzantine resilience) that we both theoretically\nand empirically show to be less than $f/n$, where $f$ is the number of\nByzantine failures tolerated and $n$ the total number of workers.\nInterestingly, we also empirically observe that the dampening component is\ninteresting in its own right for it enables to build an SGD algorithm that\noutperforms alternative staleness-aware asynchronous competitors in\nenvironments with honest workers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:47:35 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 17:48:06 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Damaskinos", "Georgios", ""], ["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Patra", "Rhicheek", ""], ["Taziki", "Mahsa", ""]]}, {"id": "1802.07935", "submitter": "Arunselvan Ramaswamy Dr.", "authors": "Arunselvan Ramaswamy, Shalabh Bhatnagar, Daniel E. Quevedo", "title": "Asynchronous stochastic approximations with asymptotically biased errors\n  and deep multi-agent learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous stochastic approximations (SAs) are an important class of\nmodel-free algorithms, tools and techniques that are popular in multi-agent and\ndistributed control scenarios. To counter Bellman's curse of dimensionality,\nsuch algorithms are coupled with function approximations. Although the\nlearning/ control problem becomes more tractable, function approximations\naffect stability and convergence. In this paper, we present verifiable\nsufficient conditions for stability and convergence of asynchronous SAs with\nbiased approximation errors. The theory developed herein is used to analyze\nPolicy Gradient methods and noisy Value Iteration schemes. Specifically, we\nanalyze the asynchronous approximate counterparts of the policy gradient (A2PG)\nand value iteration (A2VI) schemes. It is shown that the stability of these\nalgorithms is unaffected by biased approximation errors, provided they are\nasymptotically bounded. With respect to convergence (of A2VI and A2PG), a\nrelationship between the limiting set and the approximation errors is\nestablished. Finally, experimental results are presented that support the\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:13:53 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 08:24:49 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""], ["Quevedo", "Daniel E.", ""]]}, {"id": "1802.07954", "submitter": "Fabrice Rossi", "authors": "A. Endert, W. Ribarsky, C. Turkay, W Wong, I. Nabney, I D\\'iaz Blanco,\n  Fabrice Rossi (SAMM)", "title": "The State of the Art in Integrating Machine Learning into Visual\n  Analytics", "comments": null, "journal-ref": "Computer Graphics Forum, Wiley, 2017, 36 (8), pp.458 - 486", "doi": "10.1111/cgf.13092", "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics systems combine machine learning or other analytic\ntechniques with interactive data visualization to promote sensemaking and\nanalytical reasoning. It is through such techniques that people can make sense\nof large, complex data. While progress has been made, the tactful combination\nof machine learning and data visualization is still under-explored. This\nstate-of-the-art report presents a summary of the progress that has been made\nby highlighting and synthesizing select research advances. Further, it presents\nopportunities and challenges to enhance the synergy between machine learning\nand visual analytics for impactful future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 09:48:56 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Endert", "A.", "", "SAMM"], ["Ribarsky", "W.", "", "SAMM"], ["Turkay", "C.", "", "SAMM"], ["Wong", "W", "", "SAMM"], ["Nabney", "I.", "", "SAMM"], ["Blanco", "I D\u00edaz", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1802.07971", "submitter": "Jean-Yves Franceschi", "authors": "Jean-Yves Franceschi (LIP), Alhussein Fawzi, Omar Fawzi (LIP)", "title": "Robustness of classifiers to uniform $\\ell\\_p$ and Gaussian noise", "comments": null, "journal-ref": "21st International Conference on Artificial Intelligence and\n  Statistics (AISTATS) 2018, Apr 2018, Playa Blanca, Spain. 2018,\n  http://www.aistats.org/", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of classifiers to various kinds of random noise\nmodels. In particular, we consider noise drawn uniformly from the $\\ell\\_p$\nball for $p \\in [1, \\infty]$ and Gaussian noise with an arbitrary covariance\nmatrix. We characterize this robustness to random noise in terms of the\ndistance to the decision boundary of the classifier. This analysis applies to\nlinear classifiers as well as classifiers with locally approximately flat\ndecision boundaries, a condition which is satisfied by state-of-the-art deep\nneural networks. The predicted robustness is verified experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 10:31:21 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Franceschi", "Jean-Yves", "", "LIP"], ["Fawzi", "Alhussein", "", "LIP"], ["Fawzi", "Omar", "", "LIP"]]}, {"id": "1802.08009", "submitter": "Gergely Neu", "authors": "Gergely Neu and Lorenzo Rosasco", "title": "Iterate averaging as regularization for stochastic gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a variant of the classic Polyak-Ruppert averaging\nscheme, broadly used in stochastic gradient methods. Rather than a uniform\naverage of the iterates, we consider a weighted average, with weights decaying\nin a geometric fashion. In the context of linear least squares regression, we\nshow that this averaging scheme has a the same regularizing effect, and indeed\nis asymptotically equivalent, to ridge regression. In particular, we derive\nfinite-sample bounds for the proposed approach that match the best known\nresults for regularized stochastic gradient methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:27:40 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Neu", "Gergely", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1802.08012", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano", "title": "Learning Topic Models by Neighborhood Aggregation", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are frequently used in machine learning owing to their high\ninterpretability and modular structure. However, extending a topic model to\ninclude a supervisory signal, to incorporate pre-trained word embedding vectors\nand to include a nonlinear output function is not an easy task because one has\nto resort to a highly intricate approximate inference procedure. The present\npaper shows that topic modeling with pre-trained word embedding vectors can be\nviewed as implementing a neighborhood aggregation algorithm where messages are\npassed through a network defined over words. From the network view of topic\nmodels, nodes correspond to words in a document and edges correspond to either\na relationship describing co-occurring words in a document or a relationship\ndescribing the same word in the corpus. The network view allows us to extend\nthe model to include supervisory signals, incorporate pre-trained word\nembedding vectors and include a nonlinear output function in a simple manner.\nIn experiments, we show that our approach outperforms the state-of-the-art\nsupervised Latent Dirichlet Allocation implementation in terms of held-out\ndocument classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:39:59 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 05:56:31 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 06:33:12 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 21:52:57 GMT"}, {"version": "v5", "created": "Sun, 2 Jun 2019 12:33:30 GMT"}, {"version": "v6", "created": "Mon, 16 Sep 2019 10:23:33 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Hisano", "Ryohei", ""]]}, {"id": "1802.08013", "submitter": "Daniel Tanneberg", "authors": "Daniel Tanneberg, Jan Peters, Elmar Rueckert", "title": "Intrinsic Motivation and Mental Replay enable Efficient Online\n  Adaptation in Stochastic Recurrent Networks", "comments": "accepted in Neural Networks", "journal-ref": "Volume 109, January 2019, Pages 67-80", "doi": "10.1016/j.neunet.2018.10.005", "report-no": null, "categories": "cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots need to interact with unknown, unstructured and changing\nenvironments, constantly facing novel challenges. Therefore, continuous online\nadaptation for lifelong-learning and the need of sample-efficient mechanisms to\nadapt to changes in the environment, the constraints, the tasks, or the robot\nitself are crucial. In this work, we propose a novel framework for\nprobabilistic online motion planning with online adaptation based on a\nbio-inspired stochastic recurrent neural network. By using learning signals\nwhich mimic the intrinsic motivation signalcognitive dissonance in addition\nwith a mental replay strategy to intensify experiences, the stochastic\nrecurrent network can learn from few physical interactions and adapts to novel\nenvironments in seconds. We evaluate our online planning and adaptation\nframework on an anthropomorphic KUKA LWR arm. The rapid online adaptation is\nshown by learning unknown workspace constraints sample-efficiently from few\nphysical interactions while following given way points.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:41:06 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 08:36:19 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Tanneberg", "Daniel", ""], ["Peters", "Jan", ""], ["Rueckert", "Elmar", ""]]}, {"id": "1802.08021", "submitter": "Cedric Renggli", "authors": "Cedric Renggli and Saleh Ashkboos and Mehdi Aghagolzadeh and Dan\n  Alistarh and Torsten Hoefler", "title": "SparCML: High-Performance Sparse Communication for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning techniques to the quickly growing data in science\nand industry requires highly-scalable algorithms. Large datasets are most\ncommonly processed \"data parallel\" distributed across many nodes. Each node's\ncontribution to the overall gradient is summed using a global allreduce. This\nallreduce is the single communication and thus scalability bottleneck for most\nmachine learning workloads. We observe that frequently, many gradient values\nare (close to) zero, leading to sparse of sparsifyable communications. To\nexploit this insight, we analyze, design, and implement a set of\ncommunication-efficient protocols for sparse input data, in conjunction with\nefficient machine learning algorithms which can leverage these primitives. Our\ncommunication protocols generalize standard collective operations, by allowing\nprocesses to contribute arbitrary sparse input data vectors. Our generic\ncommunication library, SparCML, extends MPI to support additional features,\nsuch as non-blocking (asynchronous) operations and low-precision data\nrepresentations. As such, SparCML and its techniques will form the basis of\nfuture highly-scalable machine learning frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:02:53 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 04:05:22 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 07:46:14 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Renggli", "Cedric", ""], ["Ashkboos", "Saleh", ""], ["Aghagolzadeh", "Mehdi", ""], ["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1802.08054", "submitter": "Diego Granziol", "authors": "Diego Granziol, Edward Wagstaff, Bin Xin Ru, Michael Osborne, Stephen\n  Roberts", "title": "VBALD - Variational Bayesian Approximation of Log Determinants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the log determinant of a positive definite matrix is ubiquitous in\nmachine learning. Applications thereof range from Gaussian processes,\nminimum-volume ellipsoids, metric learning, kernel learning, Bayesian neural\nnetworks, Determinental Point Processes, Markov random fields to partition\nfunctions of discrete graphical models. In order to avoid the canonical, yet\nprohibitive, Cholesky $\\mathcal{O}(n^{3})$ computational cost, we propose a\nnovel approach, with complexity $\\mathcal{O}(n^{2})$, based on a constrained\nvariational Bayes algorithm. We compare our method to Taylor, Chebyshev and\nLanczos approaches and show state of the art performance on both synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:11:18 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Granziol", "Diego", ""], ["Wagstaff", "Edward", ""], ["Ru", "Bin Xin", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""]]}, {"id": "1802.08061", "submitter": "Zhijian Wang", "authors": "Nan Zhou, Li Zhang, Shijian Li, Zhijian Wang", "title": "Algorithmic Collusion in Cournot Duopoly Market: Evidence from\n  Experimental Economics", "comments": "22 pages, 7 figures; algorithmic collusion; Cournot duopoly model;\n  experimental economics; game theory; collusion algorithm design; iterated\n  prisoner's dilemma; antitrust; mechanism design", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.GT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic collusion is an emerging concept in current artificial\nintelligence age. Whether algorithmic collusion is a creditable threat remains\nas an argument. In this paper, we propose an algorithm which can extort its\nhuman rival to collude in a Cournot duopoly competing market. In experiments,\nwe show that, the algorithm can successfully extorted its human rival and gets\nhigher profit in long run, meanwhile the human rival will fully collude with\nthe algorithm. As a result, the social welfare declines rapidly and stably.\nBoth in theory and in experiment, our work confirms that, algorithmic collusion\ncan be a creditable threat. In application, we hope, the frameworks, the\nalgorithm design as well as the experiment environment illustrated in this\nwork, can be an incubator or a test bed for researchers and policymakers to\nhandle the emerging algorithmic collusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 18:34:27 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Zhou", "Nan", ""], ["Zhang", "Li", ""], ["Li", "Shijian", ""], ["Wang", "Zhijian", ""]]}, {"id": "1802.08089", "submitter": "Andre Wibisono", "authors": "Andre Wibisono", "title": "Sampling as optimization in the space of measures: The Langevin dynamics\n  as a composite optimization problem", "comments": "To appear at the Conference on Learning Theory (COLT), July 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sampling as optimization in the space of measures. We focus on\ngradient flow-based optimization with the Langevin dynamics as a case study. We\ninvestigate the source of the bias of the unadjusted Langevin algorithm (ULA)\nin discrete time, and consider how to remove or reduce the bias. We point out\nthe difficulty is that the heat flow is exactly solvable, but neither its\nforward nor backward method is implementable in general, except for Gaussian\ndata. We propose the symmetrized Langevin algorithm (SLA), which should have a\nsmaller bias than ULA, at the price of implementing a proximal gradient step in\nspace. We show SLA is in fact consistent for Gaussian target measure, whereas\nULA is not. We also illustrate various algorithms explicitly for Gaussian\ntarget measure, including gradient descent, proximal gradient, and\nForward-Backward, and show they are all consistent.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 15:06:51 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 18:39:51 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Wibisono", "Andre", ""]]}, {"id": "1802.08139", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and Thomas P. S. Gillam", "title": "Path-Specific Counterfactual Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning fair decision systems in complex\nscenarios in which a sensitive attribute might affect the decision along both\nfair and unfair pathways. We introduce a causal approach to disregard effects\nalong unfair pathways that simplifies and generalizes previous literature. Our\nmethod corrects observations adversely affected by the sensitive attribute, and\nuses these to form a decision. This avoids disregarding fair information, and\ndoes not require an often intractable computation of the path-specific effect.\nWe leverage recent developments in deep learning and approximate inference to\nachieve a solution that is widely applicable to complex, non-linear scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:23:51 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Chiappa", "Silvia", ""], ["Gillam", "Thomas P. S.", ""]]}, {"id": "1802.08163", "submitter": "Mark Rowland", "authors": "Mark Rowland, Marc G. Bellemare, Will Dabney, R\\'emi Munos, Yee Whye\n  Teh", "title": "An Analysis of Categorical Distributional Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional approaches to value-based reinforcement learning model the\nentire distribution of returns, rather than just their expected values, and\nhave recently been shown to yield state-of-the-art empirical performance. This\nwas demonstrated by the recently proposed C51 algorithm, based on categorical\ndistributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However,\nthe theoretical properties of CDRL algorithms are not yet well understood. In\nthis paper, we introduce a framework to analyse CDRL algorithms, establish the\nimportance of the projected distributional Bellman operator in distributional\nRL, draw fundamental connections between CDRL and the Cram\\'er distance, and\ngive a proof of convergence for sample-based categorical distributional\nreinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:50:08 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Rowland", "Mark", ""], ["Bellemare", "Marc G.", ""], ["Dabney", "Will", ""], ["Munos", "R\u00e9mi", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1802.08167", "submitter": "Wessel Bruinsma", "authors": "Wessel Bruinsma and Richard E. Turner", "title": "Learning Causally-Generated Stationary Time Series", "comments": "13 pages, 7 figures, 2 tables, includes appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Causal Gaussian Process Convolution Model (CGPCM), a doubly\nnonparametric model for causal, spectrally complex dynamical phenomena. The\nCGPCM is a generative model in which white noise is passed through a causal,\nnonparametric-window moving-average filter, a construction that we show to be\nequivalent to a Gaussian process with a nonparametric kernel that is biased\ntowards causally-generated signals. We develop enhanced variational inference\nand learning schemes for the CGPCM and its previous acausal variant, the GPCM\n(Tobar et al., 2015b), that significantly improve statistical accuracy. These\nmodelling and inferential contributions are demonstrated on a range of\nsynthetic and real-world signals.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:53:09 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Bruinsma", "Wessel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1802.08183", "submitter": "Lin Chen", "authors": "Lin Chen, Christopher Harshaw, Hamed Hassani, Amin Karbasi", "title": "Projection-Free Online Optimization with Stochastic Gradient: From\n  Convexity to Submodularity", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization has been a successful framework for solving large-scale\nproblems under computational constraints and partial information. Current\nmethods for online convex optimization require either a projection or exact\ngradient computation at each step, both of which can be prohibitively expensive\nfor large-scale applications. At the same time, there is a growing trend of\nnon-convex optimization in machine learning community and a need for online\nmethods. Continuous DR-submodular functions, which exhibit a natural\ndiminishing returns condition, have recently been proposed as a broad class of\nnon-convex functions which may be efficiently optimized. Although online\nmethods have been introduced, they suffer from similar problems. In this work,\nwe propose Meta-Frank-Wolfe, the first online projection-free algorithm that\nuses stochastic gradient estimates. The algorithm relies on a careful sampling\nof gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial\nregret bounds for convex and continuous submodular optimization. We also\npropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single\nstochastic gradient estimate in each round and achieves an $O(T^{2/3})$\nstochastic regret bound for convex and continuous submodular optimization. We\napply our methods to develop a novel \"lifting\" framework for the online\ndiscrete submodular maximization and also see that they outperform current\nstate-of-the-art techniques on various experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:13:36 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 21:53:04 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 00:36:28 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 01:10:34 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Chen", "Lin", ""], ["Harshaw", "Christopher", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.08195", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot,\n  Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein", "title": "Adversarial Examples that Fool both Computer Vision and Time-Limited\n  Humans", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. However, it is still an open question whether\nhumans are prone to similar mistakes. Here, we address this question by\nleveraging recent techniques that transfer adversarial examples from computer\nvision models with known parameters and architecture to other models with\nunknown parameters and architecture, and by matching the initial processing of\nthe human visual system. We find that adversarial examples that strongly\ntransfer across computer vision models influence the classifications made by\ntime-limited human observers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:40:51 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 18:46:56 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 03:02:41 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Shankar", "Shreya", ""], ["Cheung", "Brian", ""], ["Papernot", "Nicolas", ""], ["Kurakin", "Alex", ""], ["Goodfellow", "Ian", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1802.08235", "submitter": "Daniel Vieira Mr.", "authors": "Daniel Vieira, Fabio Rangel, Fabricio Firmino and Joao Paixao", "title": "Vector Field Based Neural Networks", "comments": "6 pages, 5 figures. To appear in the Proceedings of the 26th European\n  Symposium on Artificial Neural Networks, Computational Intelligence and\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel Neural Network architecture is proposed using the mathematically and\nphysically rich idea of vector fields as hidden layers to perform nonlinear\ntransformations in the data. The data points are interpreted as particles\nmoving along a flow defined by the vector field which intuitively represents\nthe desired movement to enable classification. The architecture moves the data\npoints from their original configuration to anew one following the streamlines\nof the vector field with the objective of achieving a final configuration where\nclasses are separable. An optimization problem is solved through gradient\ndescent to learn this vector field.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:46:15 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Vieira", "Daniel", ""], ["Rangel", "Fabio", ""], ["Firmino", "Fabricio", ""], ["Paixao", "Joao", ""]]}, {"id": "1802.08241", "submitter": "Amir Gholami", "authors": "Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney", "title": "Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries", "comments": "Presented in NeurIPS'18 conference", "journal-ref": "NeurIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1].\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:55:00 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 19:41:12 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 22:25:00 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 19:58:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yao", "Zhewei", ""], ["Gholami", "Amir", ""], ["Lei", "Qi", ""], ["Keutzer", "Kurt", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1802.08242", "submitter": "Konstantin Usevich", "authors": "Jonathan Gillard and Konstantin Usevich", "title": "Structured low-rank matrix completion for forecasting in time series\n  analysis", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the low-rank matrix completion problem with\nspecific application to forecasting in time series analysis. Briefly, the\nlow-rank matrix completion problem is the problem of imputing missing values of\na matrix under a rank constraint. We consider a matrix completion problem for\nHankel matrices and a convex relaxation based on the nuclear norm. Based on new\ntheoretical results and a number of numerical and real examples, we investigate\nthe cases when the proposed approach can work. Our results highlight the\nimportance of choosing a proper weighting scheme for the known observations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:56:27 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Gillard", "Jonathan", ""], ["Usevich", "Konstantin", ""]]}, {"id": "1802.08246", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Jason Lee, Daniel Soudry, Nathan Srebro", "title": "Characterizing Implicit Bias in Terms of Optimization Geometry", "comments": "(1) A bug in the proof of implicit bias for matrix factorization was\n  fixed. v2 gives a characterization of the asymptotic bias of the factor\n  matrices, while v1 made a stronger claim on the limit direction of the\n  unfactored matrix. (2) v2 also includes new results on implicit bias of\n  mirror descent with realizable affine constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the implicit bias of generic optimization methods, such as mirror\ndescent, natural gradient descent, and steepest descent with respect to\ndifferent potentials and norms, when optimizing underdetermined linear\nregression or separable linear classification problems. We explore the question\nof whether the specific global minimum (among the many possible global minima)\nreached by an algorithm can be characterized in terms of the potential or norm\nof the optimization geometry, and independently of hyperparameter choices such\nas step-size and momentum.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:58:31 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 03:45:12 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 21:05:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Lee", "Jason", ""], ["Soudry", "Daniel", ""], ["Srebro", "Nathan", ""]]}, {"id": "1802.08249", "submitter": "Maziar Sanjabi", "authors": "Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, Jason D. Lee", "title": "On the Convergence and Robustness of Training GANs with Regularized\n  Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are one of the most practical methods\nfor learning data distributions. A popular GAN formulation is based on the use\nof Wasserstein distance as a metric between probability distributions.\nUnfortunately, minimizing the Wasserstein distance between the data\ndistribution and the generative model distribution is a computationally\nchallenging problem as its objective is non-convex, non-smooth, and even hard\nto compute. In this work, we show that obtaining gradient information of the\nsmoothed Wasserstein GAN formulation, which is based on regularized Optimal\nTransport (OT), is computationally effortless and hence one can apply first\norder optimization methods to minimize this objective. Consequently, we\nestablish theoretical convergence guarantee to stationarity for a proposed\nclass of GAN optimization algorithms. Unlike the original non-smooth\nformulation, our algorithm only requires solving the discriminator to\napproximate optimality. We apply our method to learning MNIST digits as well as\nCIFAR-10images. Our experiments show that our method is computationally\nefficient and generates images comparable to the state of the art algorithms\ngiven the same architecture and computational power.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 04:11:58 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:11:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sanjabi", "Maziar", ""], ["Ba", "Jimmy", ""], ["Razaviyayn", "Meisam", ""], ["Lee", "Jason D.", ""]]}, {"id": "1802.08250", "submitter": "Lu\\'is Alexandre", "authors": "Abel S. Zacarias and Lu\\'is A. Alexandre", "title": "SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural\n  Networks by Selective Network Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning aims to develop machine learning systems that can learn new\ntasks while preserving the performance on previous learned tasks. In this paper\nwe present a method to overcome catastrophic forgetting on convolutional neural\nnetworks, that learns new tasks and preserves the performance on old tasks\nwithout accessing the data of the original model, by selective network\naugmentation. The experiment results showed that SeNA-CNN, in some scenarios,\noutperforms the state-of-art Learning without Forgetting algorithm. Results\nalso showed that in some situations it is better to use SeNA-CNN instead of\ntraining a neural network using isolated learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 10:23:36 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 08:59:04 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Zacarias", "Abel S.", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "1802.08314", "submitter": "Chao Zhang", "authors": "Chao Zhang, Philip Woodland", "title": "High Order Recurrent Neural Networks for Acoustic Modelling", "comments": "5 pages, 2 figures, 2 tables, to appear in 2018 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vanishing long-term gradients are a major issue in training standard\nrecurrent neural networks (RNNs), which can be alleviated by long short-term\nmemory (LSTM) models with memory cells. However, the extra parameters\nassociated with the memory cells mean an LSTM layer has four times as many\nparameters as an RNN with the same hidden vector size. This paper addresses the\nvanishing gradient problem using a high order RNN (HORNN) which has additional\nconnections from multiple previous time steps. Speech recognition experiments\nusing British English multi-genre broadcast (MGB3) data showed that the\nproposed HORNN architectures for rectified linear unit and sigmoid activation\nfunctions reduced word error rates (WER) by 4.2% and 6.3% over the\ncorresponding RNNs, and gave similar WERs to a (projected) LSTM while using\nonly 20%--50% of the recurrent layer parameters and computation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:01:05 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Zhang", "Chao", ""], ["Woodland", "Philip", ""]]}, {"id": "1802.08318", "submitter": "Uthaipon Tantipongpipat", "authors": "Aleksandar Nikolov and Mohit Singh and Uthaipon Tao Tantipongpipat", "title": "Proportional Volume Sampling and Approximation Algorithms for A-Optimal\n  Design", "comments": "Add that proportional volume sampling also solves D-optimal and\n  generalized ratio problem. Add some reference from last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal design problems where the goal is to choose a set of\nlinear measurements to obtain the most accurate estimate of an unknown vector\nin $d$ dimensions. We study the $A$-optimal design variant where the objective\nis to minimize the average variance of the error in the maximum likelihood\nestimate of the vector being measured. The problem also finds applications in\nsensor placement in wireless networks, sparse least squares regression, feature\nselection for $k$-means clustering, and matrix approximation. In this paper, we\nintroduce proportional volume sampling to obtain improved approximation\nalgorithms for $A$-optimal design. Our main result is to obtain improved\napproximation algorithms for the $A$-optimal design problem by introducing the\nproportional volume sampling algorithm. Our results nearly optimal bounds in\nthe asymptotic regime when the number of measurements done, $k$, is\nsignificantly more than the dimension $d$. We also give first approximation\nalgorithms when $k$ is small including when $k=d$. The proportional\nvolume-sampling algorithm also gives approximation algorithms for other optimal\ndesign objectives such as $D$-optimal design and generalized ratio objective\nmatching or improving previous best known results. Interestingly, we show that\na similar guarantee cannot be obtained for the $E$-optimal design problem. We\nalso show that the $A$-optimal design problem is NP-hard to approximate within\na fixed constant when $k=d$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:02:42 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 21:17:45 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 14:45:10 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 19:05:11 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 15:10:52 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nikolov", "Aleksandar", ""], ["Singh", "Mohit", ""], ["Tantipongpipat", "Uthaipon Tao", ""]]}, {"id": "1802.08323", "submitter": "Kyongmin Yeo", "authors": "Kyongmin Yeo and Igor Melnyk", "title": "Deep learning algorithm for data-driven simulation of noisy dynamical\n  system", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.10.024", "report-no": null, "categories": "physics.comp-ph cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning model, DE-LSTM, for the simulation of a stochastic\nprocess with an underlying nonlinear dynamics. The deep learning model aims to\napproximate the probability density function of a stochastic process via\nnumerical discretization and the underlying nonlinear dynamics is modeled by\nthe Long Short-Term Memory (LSTM) network. It is shown that, when the numerical\ndiscretization is used, the function estimation problem can be solved by a\nmulti-label classification problem. A penalized maximum log likelihood method\nis proposed to impose a smoothness condition in the prediction of the\nprobability distribution. We show that the time evolution of the probability\ndistribution can be computed by a high-dimensional integration of the\ntransition probability of the LSTM internal states. A Monte Carlo algorithm to\napproximate the high-dimensional integration is outlined. The behavior of\nDE-LSTM is thoroughly investigated by using the Ornstein-Uhlenbeck process and\nnoisy observations of nonlinear dynamical systems; Mackey-Glass time series and\nforced Van der Pol oscillator. It is shown that DE-LSTM makes a good prediction\nof the probability distribution without assuming any distributional properties\nof the stochastic process. For a multiple-step forecast of the Mackey-Glass\ntime series, the prediction uncertainty, denoted by the 95\\% confidence\ninterval, first grows, then dynamically adjusts following the evolution of the\nsystem, while in the simulation of the forced Van der Pol oscillator, the\nprediction uncertainty does not grow in time even for a 3,000-step forecast.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:08:14 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 15:09:06 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yeo", "Kyongmin", ""], ["Melnyk", "Igor", ""]]}, {"id": "1802.08334", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Horia Mania, Stephen Tu, Michael I. Jordan, Benjamin\n  Recht", "title": "Learning Without Mixing: Towards A Sharp Analysis of Linear System\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the ordinary least-squares (OLS) estimator attains nearly\nminimax optimal performance for the identification of linear dynamical systems\nfrom a single observed trajectory. Our upper bound relies on a generalization\nof Mendelson's small-ball method to dependent data, eschewing the use of\nstandard mixing-time arguments. Our lower bounds reveal that these upper bounds\nmatch up to logarithmic factors. In particular, we capture the correct\nsignal-to-noise behavior of the problem, showing that more unstable linear\nsystems are easier to estimate. This behavior is qualitatively different from\narguments which rely on mixing-time calculations that suggest that unstable\nsystems are more difficult to estimate. We generalize our technique to provide\nbounds for a more general class of linear response time-series.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:48:11 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 21:13:21 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 03:05:45 GMT"}, {"version": "v4", "created": "Thu, 24 May 2018 05:57:45 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Simchowitz", "Max", ""], ["Mania", "Horia", ""], ["Tu", "Stephen", ""], ["Jordan", "Michael I.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1802.08352", "submitter": "Phi Vu Tran", "authors": "Phi Vu Tran", "title": "Learning to Make Predictions on Graphs with Autoencoders", "comments": "Published as a conference paper at IEEE DSAA 2018", "journal-ref": null, "doi": "10.1109/DSAA.2018.00034", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine two fundamental tasks associated with graph representation\nlearning: link prediction and semi-supervised node classification. We present a\nnovel autoencoder architecture capable of learning a joint representation of\nboth local graph structure and available node features for the multi-task\nlearning of link prediction and node classification. Our autoencoder\narchitecture is efficiently trained end-to-end in a single learning stage to\nsimultaneously perform link prediction and node classification, whereas\nprevious related methods require multiple training steps that are difficult to\noptimize. We provide a comprehensive empirical evaluation of our models on nine\nbenchmark graph-structured datasets and demonstrate significant improvement\nover related methods for graph representation learning. Reference code and data\nare available at https://github.com/vuptran/graph-representation-learning\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 00:02:59 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 12:02:52 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Tran", "Phi Vu", ""]]}, {"id": "1802.08363", "submitter": "Ranjan Maitra", "authors": "Andrew Lithio and Ranjan Maitra", "title": "An efficient $k$-means-type algorithm for clustering datasets with\n  incomplete records", "comments": "21 pages, 12 figures, 3 tables, in press, Statistical Analysis and\n  Data Mining -- The ASA Data Science Journal, 2018", "journal-ref": null, "doi": "10.1002/sam.11392", "report-no": null, "categories": "stat.ML astro-ph.HE cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is arguably the most popular nonparametric clustering\nmethod but cannot generally be applied to datasets with incomplete records. The\nusual practice then is to either impute missing values under an assumed\nmissing-completely-at-random mechanism or to ignore the incomplete records, and\napply the algorithm on the resulting dataset. We develop an efficient version\nof the $k$-means algorithm that allows for clustering in the presence of\nincomplete records. Our extension is called $k_m$-means and reduces to the\n$k$-means algorithm when all records are complete. We also provide\ninitialization strategies for our algorithm and methods to estimate the number\nof groups in the dataset. Illustrations and simulations demonstrate the\nefficacy of our approach in a variety of settings and patterns of missing data.\nOur methods are also applied to the analysis of activation images obtained from\na functional Magnetic Resonance Imaging experiment.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:24:14 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 13:15:48 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lithio", "Andrew", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1802.08372", "submitter": "Weijun Xie", "authors": "Mohit Singh and Weijun Xie", "title": "Approximation Algorithms for D-optimal Design", "comments": "34 pages, accepted by Mathematics of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental design is a classical statistics problem and its aim is to\nestimate an unknown $m$-dimensional vector $\\beta$ from linear measurements\nwhere a Gaussian noise is introduced in each measurement. For the combinatorial\nexperimental design problem, the goal is to pick $k$ out of the given $n$\nexperiments so as to make the most accurate estimate of the unknown parameters,\ndenoted as $\\hat{\\beta}$. In this paper, we will study one of the most robust\nmeasures of error estimation - $D$-optimality criterion, which corresponds to\nminimizing the volume of the confidence ellipsoid for the estimation error\n$\\beta-\\hat{\\beta}$. The problem gives rise to two natural variants depending\non whether repetitions of experiments are allowed or not. We first propose an\napproximation algorithm with a $\\frac1e$-approximation for the $D$-optimal\ndesign problem with and without repetitions, giving the first constant factor\napproximation for the problem. We then analyze another sampling approximation\nalgorithm and prove that it is $(1-\\epsilon)$-approximation if $k\\geq\n\\frac{4m}{\\epsilon}+\\frac{12}{\\epsilon^2}\\log(\\frac{1}{\\epsilon})$ for any\n$\\epsilon \\in (0,1)$. Finally, for $D$-optimal design with repetitions, we\nstudy a different algorithm proposed by literature and show that it can improve\nthis asymptotic approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:48:16 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:27:44 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Singh", "Mohit", ""], ["Xie", "Weijun", ""]]}, {"id": "1802.08375", "submitter": "Zhenisbek Assylbekov", "authors": "Zhenisbek Assylbekov and Rustem Takhanov", "title": "Reusing Weights in Subword-aware Neural Language Models", "comments": "accepted to NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose several ways of reusing subword embeddings and other weights in\nsubword-aware neural language models. The proposed techniques do not benefit a\ncompetitive character-aware model, but some of them improve the performance of\nsyllable- and morpheme-aware models while showing significant reductions in\nmodel sizes. We discover a simple hands-on principle: in a multi-layer input\nembedding model, layers should be tied consecutively bottom-up if reused at\noutput. Our best morpheme-aware model with properly reused weights beats the\ncompetitive word-level model by a large margin across multiple languages and\nhas 20%-87% fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 03:44:15 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 04:53:59 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""]]}, {"id": "1802.08380", "submitter": "Lai Wei", "authors": "Lai Wei and Vaibhav Srivastava", "title": "On Abruptly-Changing and Slowly-Varying Multiarmed Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-stationary stochastic multiarmed bandit (MAB) problem and\npropose two generic algorithms, namely, the limited memory deterministic\nsequencing of exploration and exploitation (LM-DSEE) and the Sliding-Window\nUpper Confidence Bound# (SW-UCB#). We rigorously analyze these algorithms in\nabruptly-changing and slowly-varying environments and characterize their\nperformance. We show that the expected cumulative regret for these algorithms\nunder either of the environments is upper bounded by sublinear functions of\ntime, i.e., the time average of the regret asymptotically converges to zero. We\ncomplement our analytic results with numerical illustrations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 04:14:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 18:13:15 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Wei", "Lai", ""], ["Srivastava", "Vaibhav", ""]]}, {"id": "1802.08397", "submitter": "Yudong Chen", "authors": "Yudong Chen and Yuejie Chi", "title": "Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix\n  Estimation", "comments": "To appear in IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling plays a pivotal role in signal processing and machine\nlearning, with applications ranging from collaborative filtering, video\nsurveillance, medical imaging, to dimensionality reduction and adaptive\nfiltering. Many modern high-dimensional data and interactions thereof can be\nmodeled as lying approximately in a low-dimensional subspace or manifold,\npossibly with additional structures, and its proper exploitations lead to\nsignificant reduction of costs in sensing, computation and storage. In recent\nyears, there is a plethora of progress in understanding how to exploit low-rank\nstructures using computationally efficient procedures in a provable manner,\nincluding both convex and nonconvex approaches. On one side, convex relaxations\nsuch as nuclear norm minimization often lead to statistically optimal\nprocedures for estimating low-rank matrices, where first-order methods are\ndeveloped to address the computational challenges; on the other side, there is\nemerging evidence that properly designed nonconvex procedures, such as\nprojected gradient descent, often provide globally optimal solutions with a\nmuch lower computational cost in many problems. This survey article will\nprovide a unified overview of these recent advances on low-rank matrix\nestimation from incomplete measurements. Attention is paid to rigorous\ncharacterization of the performance of these algorithms, and to problems where\nthe low-rank matrix have additional structural properties that require new\nalgorithmic designs and theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 05:41:37 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 01:48:22 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 18:17:50 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Chen", "Yudong", ""], ["Chi", "Yuejie", ""]]}, {"id": "1802.08404", "submitter": "Takafumi Kajihara", "authors": "Takafumi Kajihara, Motonobu Kanagawa, Keisuke Yamazaki, Kenji Fukumizu", "title": "Kernel Recursive ABC: Point Estimation with Intractable Likelihood", "comments": "to appear in ICML 2018. 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to parameter estimation for simulator-based\nstatistical models with intractable likelihood. Our proposed method involves\nrecursive application of kernel ABC and kernel herding to the same observed\ndata. We provide a theoretical explanation regarding why the approach works,\nshowing (for the population setting) that, under a certain assumption, point\nestimates obtained with this method converge to the true parameter, as\nrecursion proceeds. We have conducted a variety of numerical experiments,\nincluding parameter estimation for a real-world pedestrian flow simulator, and\nshow that in most cases our method outperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:32:06 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 07:40:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kajihara", "Takafumi", ""], ["Kanagawa", "Motonobu", ""], ["Yamazaki", "Keisuke", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1802.08406", "submitter": "Viraj Jayminkumar Shah", "authors": "Viraj Shah and Chinmay Hegde", "title": "Solving Linear Inverse Problems Using GAN Priors: An Algorithm with\n  Provable Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent works, both sparsity-based methods as well as learning-based\nmethods have proven to be successful in solving several challenging linear\ninverse problems. However, sparsity priors for natural signals and images\nsuffer from poor discriminative capability, while learning-based methods seldom\nprovide concrete theoretical guarantees. In this work, we advocate the idea of\nreplacing hand-crafted priors, such as sparsity, with a Generative Adversarial\nNetwork (GAN) to solve linear inverse problems such as compressive sensing. In\nparticular, we propose a projected gradient descent (PGD) algorithm for\neffective use of GAN priors for linear inverse problems, and also provide\ntheoretical guarantees on the rate of convergence of this algorithm. Moreover,\nwe show empirically that our algorithm demonstrates superior performance over\nan existing method of leveraging GANs for compressive sensing.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:40:58 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shah", "Viraj", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1802.08407", "submitter": "Shengyu Zhu", "authors": "Shengyu Zhu, Biao Chen, Zhitang Chen", "title": "Exponentially Consistent Kernel Two-Sample Tests", "comments": "17 pages. Added application to off-line change detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of independent samples from unknown distributions $P$ and $Q$,\na two-sample test decides whether to reject the null hypothesis that $P=Q$.\nRecent attention has focused on kernel two-sample tests as the test statistics\nare easy to compute, converge fast, and have low bias with their finite sample\nestimates. However, there still lacks an exact characterization on the\nasymptotic performance of such tests, and in particular, the rate at which the\ntype-II error probability decays to zero in the large sample limit. In this\nwork, we establish that a class of kernel two-sample tests are exponentially\nconsistent with Polish, locally compact Hausdorff sample space, e.g., $\\mathbb\nR^d$. The obtained exponential decay rate is further shown to be optimal among\nall two-sample tests satisfying the level constraint, and is independent of\nparticular kernels provided that they are bounded continuous and\ncharacteristic. Our results gain new insights into related issues such as fair\nalternative for testing and kernel selection strategy. Finally, as an\napplication, we show that a kernel based test achieves the optimal detection\nfor off-line change detection in the nonparametric setting.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:45:05 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 13:22:09 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhu", "Shengyu", ""], ["Chen", "Biao", ""], ["Chen", "Zhitang", ""]]}, {"id": "1802.08429", "submitter": "Claire Launay", "authors": "Claire Launay, Bruno Galerne, Agn\\`es Desolneux", "title": "Exact Sampling of Determinantal Point Processes without\n  Eigendecomposition", "comments": "Last update: Correction of typos and journal reference", "journal-ref": "Journal of Applied Probability, 57(4), 1198-1221 (2020)", "doi": "10.1017/jpr.2020.56", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) enable the modeling of repulsion: they\nprovide diverse sets of points. The repulsion is encoded in a kernel $K$ that\ncan be seen as a matrix storing the similarity between points. The diversity\ncomes from the fact that the inclusion probability of a subset is equal to the\ndeterminant of a submatrice of $K$. The exact algorithm to sample DPPs uses the\nspectral decomposition of $K$, a computation that becomes costly when dealing\nwith a high number of points. Here, we present an alternative exact algorithm\nin the discrete setting that avoids the eigenvalues and the eigenvectors\ncomputation. Instead, it relies on Cholesky decompositions. This is a two steps\nstrategy: first, it samples a Bernoulli point process with an appropriate\ndistribution, then it samples the target DPP distribution through a thinning\nprocedure. Not only is the method used here innovative, but this algorithm can\nbe competitive with the original algorithm or even faster for some applications\nspecified here.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 08:09:42 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 07:29:17 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 09:09:57 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 06:57:25 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 22:03:23 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Launay", "Claire", ""], ["Galerne", "Bruno", ""], ["Desolneux", "Agn\u00e8s", ""]]}, {"id": "1802.08471", "submitter": "Simon Barthelm\\'e", "authors": "Nicolas Tremblay, Simon Barthelme and Pierre-Olivier Amblard", "title": "Optimized Algorithms to Sample Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we discuss several sampling algorithms for\nDeterminantal Point Processes (DPP). DPPs have recently gained a broad interest\nin the machine learning and statistics literature as random point processes\nwith negative correlation, i.e., ones that can generate a \"diverse\" sample from\na set of items. They are parametrized by a matrix $\\mathbf{L}$, called\n$L$-ensemble, that encodes the correlations between items. The standard\nsampling algorithm is separated in three phases: 1/~eigendecomposition of\n$\\mathbf{L}$, 2/~an eigenvector sampling phase where $\\mathbf{L}$'s\neigenvectors are sampled independently via a Bernoulli variable parametrized by\ntheir associated eigenvalue, 3/~a Gram-Schmidt-type orthogonalisation procedure\nof the sampled eigenvectors.\n  In a naive implementation, the computational cost of the third step is on\naverage $\\mathcal{O}(N\\mu^3)$ where $\\mu$ is the average number of samples of\nthe DPP. We give an algorithm which runs in $\\mathcal{O}(N\\mu^2)$ and is\nextremely simple to implement. If memory is a constraint, we also describe a\ndual variant with reduced memory costs. In addition, we discuss implementation\ndetails often missing in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:30:41 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Barthelme", "Simon", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "1802.08526", "submitter": "Yunlong Jiao", "authors": "Yunlong Jiao and Jean-Philippe Vert", "title": "The Weighted Kendall and High-order Kernels for Permutations", "comments": "Published in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new positive definite kernels for permutations. First we introduce\na weighted version of the Kendall kernel, which allows to weight unequally the\ncontributions of different item pairs in the permutations depending on their\nranks. Like the Kendall kernel, we show that the weighted version is invariant\nto relabeling of items and can be computed efficiently in $O(n \\ln(n))$\noperations, where $n$ is the number of items in the permutation. Second, we\npropose a supervised approach to learn the weights by jointly optimizing them\nwith the function estimated by a kernel machine. Third, while the Kendall\nkernel considers pairwise comparison between items, we extend it by considering\nhigher-order comparisons among tuples of items and show that the supervised\napproach of learning the weights can be systematically generalized to\nhigher-order permutation kernels.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:50:08 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 08:40:50 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Jiao", "Yunlong", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "1802.08530", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell", "title": "Training wide residual networks for deployment using a single bit for\n  each weight", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": "ICLR 2018 - International Conference on Learning Representations,\n  Apr 2018, Vancouver, Canada. 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:54:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["McDonnell", "Mark D.", ""]]}, {"id": "1802.08539", "submitter": "Stephan Eckstein", "authors": "Stephan Eckstein and Michael Kupper", "title": "Computation of optimal transport and related hedging problems via\n  penalization and neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a widely applicable approach to solving (multi-marginal,\nmartingale) optimal transport and related problems via neural networks. The\ncore idea is to penalize the optimization problem in its dual formulation and\nreduce it to a finite dimensional one which corresponds to optimizing a neural\nnetwork with smooth objective function. We present numerical examples from\noptimal transport, martingale optimal transport, portfolio optimization under\nuncertainty and generative adversarial networks that showcase the generality\nand effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 14:19:51 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 15:46:02 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Eckstein", "Stephan", ""], ["Kupper", "Michael", ""]]}, {"id": "1802.08567", "submitter": "Alireza Bagheri", "authors": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "title": "Adversarial Training for Probabilistic Spiking Neural Networks", "comments": "Submitted for possible publication. arXiv admin note: text overlap\n  with arXiv:1710.10704", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers trained using conventional empirical risk minimization or maximum\nlikelihood methods are known to suffer dramatic performance degradations when\ntested over examples adversarially selected based on knowledge of the\nclassifier's decision rule. Due to the prominence of Artificial Neural Networks\n(ANNs) as classifiers, their sensitivity to adversarial examples, as well as\nrobust training schemes, have been recently the subject of intense\ninvestigation. In this paper, for the first time, the sensitivity of spiking\nneural networks (SNNs), or third-generation neural networks, to adversarial\nexamples is studied. The study considers rate and time encoding, as well as\nrate and first-to-spike decoding. Furthermore, a robust training mechanism is\nproposed that is demonstrated to enhance the performance of SNNs under\nwhite-box attacks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 04:33:32 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 19:26:30 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Bagheri", "Alireza", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1802.08598", "submitter": "Fredrik D. Johansson", "authors": "Fredrik D. Johansson, Nathan Kallus, Uri Shalit, David Sontag", "title": "Learning Weighted Representations for Generalization Across Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models that generalize well under distributional shift are often\ndesirable and sometimes crucial to building robust and reliable machine\nlearning applications. We focus on distributional shift that arises in causal\ninference from observational data and in unsupervised domain adaptation. We\npose both of these problems as prediction under a shift in design. Popular\nmethods for overcoming distributional shift make unrealistic assumptions such\nas having a well-specified model or knowing the policy that gave rise to the\nobserved data. Other methods are hindered by their need for a pre-specified\nmetric for comparing observations, or by poor asymptotic properties. We devise\na bound on the generalization error under design shift, incorporating both\nrepresentation learning and sample re-weighting. Based on the bound, we propose\nan algorithmic framework that does not require any of the above assumptions and\nwhich is asymptotically consistent. We empirically study the new framework\nusing two synthetic datasets, and demonstrate its effectiveness compared to\nprevious methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:30:18 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 18:59:29 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Johansson", "Fredrik D.", ""], ["Kallus", "Nathan", ""], ["Shalit", "Uri", ""], ["Sontag", "David", ""]]}, {"id": "1802.08626", "submitter": "Michele Donini", "authors": "Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor,\n  Massimiliano Pontil", "title": "Empirical Risk Minimization under Fairness Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of algorithmic fairness: ensuring that sensitive\nvariables do not unfairly influence the outcome of a classifier. We present an\napproach based on empirical risk minimization, which incorporates a fairness\nconstraint into the learning problem. It encourages the conditional risk of the\nlearned classifier to be approximately constant with respect to the sensitive\nvariable. We derive both risk and fairness bounds that support the statistical\nconsistency of our approach. We specify our approach to kernel methods and\nobserve that the fairness requirement implies an orthogonality constraint which\ncan be easily added to these methods. We further observe that for linear models\nthe constraint translates into a simple data preprocessing step. Experiments\nindicate that the method is empirically effective and performs favorably\nagainst state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:31:52 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 18:11:19 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 08:15:29 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Donini", "Michele", ""], ["Oneto", "Luca", ""], ["Ben-David", "Shai", ""], ["Shawe-Taylor", "John", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1802.08665", "submitter": "Gonzalo Mena E", "authors": "Gonzalo Mena, David Belanger, Scott Linderman, Jasper Snoek", "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks", "comments": null, "journal-ref": "ICLR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutations and matchings are core building blocks in a variety of latent\nvariable models, as they allow us to align, canonicalize, and sort data.\nLearning in such models is difficult, however, because exact marginalization\nover these combinatorial objects is intractable. In response, this paper\nintroduces a collection of new methods for end-to-end learning in such models\nthat approximate discrete maximum-weight matching using the continuous Sinkhorn\noperator. Sinkhorn iteration is attractive because it functions as a simple,\neasy-to-implement analog of the softmax operator. With this, we can define the\nGumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al.\n2016, Maddison2016 et al. 2016) to distributions over latent matchings. We\ndemonstrate the effectiveness of our method by outperforming competitive\nbaselines on a range of qualitatively different tasks: sorting numbers, solving\njigsaw puzzles, and identifying neural signals in worms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:15:13 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Mena", "Gonzalo", ""], ["Belanger", "David", ""], ["Linderman", "Scott", ""], ["Snoek", "Jasper", ""]]}, {"id": "1802.08667", "submitter": "Rahul Singh", "authors": "Victor Chernozhukov, Whitney Newey, Rahul Singh", "title": "De-Biased Machine Learning of Global and Local Parameters Using\n  Regularized Riesz Representers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We provide adaptive inference methods, based on $\\ell_1$ regularization, for\nregular (semiparametric) and non-regular (nonparametric) linear functionals of\nthe conditional expectation function. Examples of regular functionals include\naverage treatment effects, policy effects, and derivatives. Examples of\nnon-regular functionals include average treatment effects, policy effects, and\nderivatives conditional on a covariate subvector fixed at a point. We construct\na Neyman orthogonal equation for the target parameter that is approximately\ninvariant to small perturbations of the nuisance parameters. To achieve this\nproperty, we include the Riesz representer for the functional as an additional\nnuisance parameter. Our analysis yields weak \"double sparsity robustness\":\neither the approximation to the regression or the approximation to the\nrepresenter can be \"completely dense\" as long as the other is sufficiently\n\"sparse\". Our main results are non-asymptotic and imply asymptotic uniform\nvalidity over large classes of models, translating into honest confidence bands\nfor both global and local parameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:17:42 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 00:35:09 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 23:16:24 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 21:10:20 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 17:12:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney", ""], ["Singh", "Rahul", ""]]}, {"id": "1802.08678", "submitter": "Felix Berkenkamp", "authors": "Shromona Ghosh, Felix Berkenkamp, Gireeja Ranade, Shaz Qadeer, Ashish\n  Kapoor", "title": "Verifying Controllers Against Adversarial Examples with Bayesian\n  Optimization", "comments": "Proc. of the IEEE International Conference on Robotics and\n  Automation, 2018", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460635", "report-no": null, "categories": "cs.SY cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in reinforcement learning have lead to the development of\ncomplex controllers for real-world robots. As these robots are deployed in\nsafety-critical applications and interact with humans, it becomes critical to\nensure safety in order to avoid causing harm. A first step in this direction is\nto test the controllers in simulation. To be able to do this, we need to\ncapture what we mean by safety and then efficiently search the space of all\nbehaviors to see if they are safe. In this paper, we present an active-testing\nframework based on Bayesian Optimization. We specify safety constraints using\nlogic and exploit structure in the problem in order to test the system for\nadversarial counter examples that violate the safety specifications. These\nspecifications are defined as complex boolean combinations of smooth functions\non the trajectories and, unlike reward functions in reinforcement learning, are\nexpressive and impose hard constraints on the system. In our framework, we\nexploit regularity assumptions on individual functions in form of a Gaussian\nProcess (GP) prior. We combine these into a coherent optimization framework\nusing problem structure. The resulting algorithm is able to provably verify\ncomplex safety specifications or alternatively find counter examples.\nExperimental results show that the proposed method is able to find adversarial\nexamples quickly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:53:44 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 11:03:21 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ghosh", "Shromona", ""], ["Berkenkamp", "Felix", ""], ["Ranade", "Gireeja", ""], ["Qadeer", "Shaz", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1802.08679", "submitter": "Onur Atan", "authors": "Onur Atan, William R. Zame, M van der Schaar", "title": "Learning Optimal Policies from Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing optimal (or at least better) policies is an important problem in\ndomains from medicine to education to finance and many others. One approach to\nthis problem is through controlled experiments/trials - but controlled\nexperiments are expensive. Hence it is important to choose the best policies on\nthe basis of observational data. This presents two difficult challenges: (i)\nmissing counterfactuals, and (ii) selection bias. This paper presents\ntheoretical bounds on estimation errors of counterfactuals from observational\ndata by making connections to domain adaptation theory. It also presents a\nprincipled way of choosing optimal policies using domain adversarial neural\nnetworks. We illustrate the effectiveness of domain adversarial training\ntogether with various features of our algorithm on a semi-synthetic breast\ncancer dataset and a supervised UCI dataset (Statlog).\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:56:32 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Atan", "Onur", ""], ["Zame", "William R.", ""], ["van der Schaar", "M", ""]]}, {"id": "1802.08680", "submitter": "Aleksander Kubica", "authors": "Nishad Maskara, Aleksander Kubica, Tomas Jochym-O'Connor", "title": "Advantages of versatile neural-network decoding for topological codes", "comments": "11 pages, 6 figures, 2 tables", "journal-ref": "Phys. Rev. A 99, 052351 (2019)", "doi": "10.1103/PhysRevA.99.052351", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding optimal correction of errors in generic stabilizer codes is a\ncomputationally hard problem, even for simple noise models. While this task can\nbe simplified for codes with some structure, such as topological stabilizer\ncodes, developing good and efficient decoders still remains a challenge. In our\nwork, we systematically study a very versatile class of decoders based on\nfeedforward neural networks. To demonstrate adaptability, we apply neural\ndecoders to the triangular color and toric codes under various noise models\nwith realistic features, such as spatially-correlated errors. We report that\nneural decoders provide significant improvement over leading efficient decoders\nin terms of the error-correction threshold. Using neural networks simplifies\nthe process of designing well-performing decoders, and does not require prior\nknowledge of the underlying noise model.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:57:58 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Maskara", "Nishad", ""], ["Kubica", "Aleksander", ""], ["Jochym-O'Connor", "Tomas", ""]]}, {"id": "1802.08686", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Hamza Fawzi, Omar Fawzi", "title": "Adversarial vulnerability for any classifier", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite achieving impressive performance, state-of-the-art classifiers remain\nhighly vulnerable to small, imperceptible, adversarial perturbations. This\nvulnerability has proven empirically to be very intricate to address. In this\npaper, we study the phenomenon of adversarial perturbations under the\nassumption that the data is generated with a smooth generative model. We derive\nfundamental upper bounds on the robustness to perturbations of any\nclassification function, and prove the existence of adversarial perturbations\nthat transfer well across different classifiers with small risk. Our analysis\nof the robustness also provides insights onto key properties of generative\nmodels, such as their smoothness and dimensionality of latent space. We\nconclude with numerical experimental results showing that our bounds provide\ninformative baselines to the maximal achievable robustness on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:46:05 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:44:52 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Fawzi", "Hamza", ""], ["Fawzi", "Omar", ""]]}, {"id": "1802.08714", "submitter": "Huaxiu Yao", "authors": "Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu,\n  Pinghua Gong, Jieping Ye, Zhenhui Li", "title": "Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction", "comments": "AAAI 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxi demand prediction is an important building block to enabling intelligent\ntransportation systems in a smart city. An accurate prediction model can help\nthe city pre-allocate resources to meet travel demand and to reduce empty taxis\non streets which waste energy and worsen the traffic congestion. With the\nincreasing popularity of taxi requesting services such as Uber and Didi Chuxing\n(in China), we are able to collect large-scale taxi demand data continuously.\nHow to utilize such big data to improve the demand prediction is an interesting\nand critical real-world problem. Traditional demand prediction methods mostly\nrely on time series forecasting techniques, which fail to model the complex\nnon-linear spatial and temporal relations. Recent advances in deep learning\nhave shown superior performance on traditionally challenging tasks such as\nimage classification by learning the complex features and correlations from\nlarge-scale data. This breakthrough has inspired researchers to explore deep\nlearning techniques on traffic prediction problems. However, existing methods\non traffic prediction have only considered spatial relation (e.g., using CNN)\nor temporal relation (e.g., using LSTM) independently. We propose a Deep\nMulti-View Spatial-Temporal Network (DMVST-Net) framework to model both spatial\nand temporal relations. Specifically, our proposed model consists of three\nviews: temporal view (modeling correlations between future demand values with\nnear time points via LSTM), spatial view (modeling local spatial correlation\nvia local CNN), and semantic view (modeling correlations among regions sharing\nsimilar temporal patterns). Experiments on large-scale real taxi demand data\ndemonstrate effectiveness of our approach over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 19:53:13 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 01:42:08 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Yao", "Huaxiu", ""], ["Wu", "Fei", ""], ["Ke", "Jintao", ""], ["Tang", "Xianfeng", ""], ["Jia", "Yitian", ""], ["Lu", "Siyu", ""], ["Gong", "Pinghua", ""], ["Ye", "Jieping", ""], ["Li", "Zhenhui", ""]]}, {"id": "1802.08717", "submitter": "Maciej Mazurowski", "authors": "Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir", "title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 04:00:55 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mazurowski", "Maciej A.", ""], ["Buda", "Mateusz", ""], ["Saha", "Ashirbani", ""], ["Bashir", "Mustafa R.", ""]]}, {"id": "1802.08718", "submitter": "Sven Schmit", "authors": "Ramesh Johari and Sven Schmit", "title": "Learning with Abandonment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a platform that wants to learn a personalized policy for each user,\nbut the platform faces the risk of a user abandoning the platform if she is\ndissatisfied with the actions of the platform. For example, a platform is\ninterested in personalizing the number of newsletters it sends, but faces the\nrisk that the user unsubscribes forever. We propose a general thresholded\nlearning model for scenarios like this, and discuss the structure of optimal\npolicies. We describe salient features of optimal personalization algorithms\nand how feedback the platform receives impacts the results. Furthermore, we\ninvestigate how the platform can efficiently learn the heterogeneity across\nusers by interacting with a population and provide performance guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 19:59:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Johari", "Ramesh", ""], ["Schmit", "Sven", ""]]}, {"id": "1802.08735", "submitter": "Rui Shu", "authors": "Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon", "title": "A DIRT-T Approach to Unsupervised Domain Adaptation", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation refers to the problem of leveraging labeled data in a\nsource domain to learn an accurate model in a target domain where labels are\nscarce or unavailable. A recent approach for finding a common representation of\nthe two domains is via domain adversarial training (Ganin & Lempitsky, 2015),\nwhich attempts to induce a feature extractor that matches the source and target\nfeature distributions in some feature space. However, domain adversarial\ntraining faces two critical limitations: 1) if the feature extraction function\nhas high-capacity, then feature distribution matching is a weak constraint, 2)\nin non-conservative domain adaptation (where no single classifier can perform\nwell in both the source and target domains), training the model to do well on\nthe source domain hurts performance on the target domain. In this paper, we\naddress these issues through the lens of the cluster assumption, i.e., decision\nboundaries should not cross high-density data regions. We propose two novel and\nrelated models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,\nwhich combines domain adversarial training with a penalty term that punishes\nthe violation the cluster assumption; 2) the Decision-boundary Iterative\nRefinement Training with a Teacher (DIRT-T) model, which takes the VADA model\nas initialization and employs natural gradient steps to further minimize the\ncluster assumption violation. Extensive empirical results demonstrate that the\ncombination of these two models significantly improve the state-of-the-art\nperformance on the digit, traffic sign, and Wi-Fi recognition domain adaptation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:57:28 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 13:45:46 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Shu", "Rui", ""], ["Bui", "Hung H.", ""], ["Narui", "Hirokazu", ""], ["Ermon", "Stefano", ""]]}, {"id": "1802.08736", "submitter": "Dmitry Shemetov", "authors": "Kirill Paramonov, Dmitry Shemetov, James Sharpnack", "title": "Estimating Graphlet Statistics via Lifting", "comments": null, "journal-ref": "KDD '19 Proceedings of the 25th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining, July 2019, Pages 587-595", "doi": "10.1145/3292500.3330995", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory analysis over network data is often limited by the ability to\nefficiently calculate graph statistics, which can provide a model-free\nunderstanding of the macroscopic properties of a network. We introduce a\nframework for estimating the graphlet count---the number of occurrences of a\nsmall subgraph motif (e.g. a wedge or a triangle) in the network. For massive\ngraphs, where accessing the whole graph is not possible, the only viable\nalgorithms are those that make a limited number of vertex neighborhood queries.\nWe introduce a Monte Carlo sampling technique for graphlet counts, called {\\em\nLifting}, which can simultaneously sample all graphlets of size up to $k$\nvertices for arbitrary $k$. This is the first graphlet sampling method that can\nprovably sample every graphlet with positive probability and can sample\ngraphlets of arbitrary size $k$. We outline variants of lifted graphlet counts,\nincluding the ordered, unordered, and shotgun estimators, random walk starts,\nand parallel vertex starts. We prove that our graphlet count updates are\nunbiased for the true graphlet count and have a controlled variance for all\ngraphlets. We compare the experimental performance of lifted graphlet counts to\nthe state-of-the art graphlet sampling procedures: Waddling and the pairwise\nsubgraph random walk.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:57:28 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 20:39:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Paramonov", "Kirill", ""], ["Shemetov", "Dmitry", ""], ["Sharpnack", "James", ""]]}, {"id": "1802.08737", "submitter": "Rajat Sen", "authors": "Rajat Sen, Karthikeyan Shanmugam, Nihal Sharma, Sanjay Shakkottai", "title": "Contextual Bandits with Stochastic Experts", "comments": "20 pages, 2 Figures, Accepted for publication in AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of contextual bandits with stochastic experts, which\nis a variation of the traditional stochastic contextual bandit with experts\nproblem. In our problem setting, we assume access to a class of stochastic\nexperts, where each expert is a conditional distribution over the arms given a\ncontext. We propose upper-confidence bound (UCB) algorithms for this problem,\nwhich employ two different importance sampling based estimators for the mean\nreward for each expert. Both these estimators leverage information leakage\namong the experts, thus using samples collected under all the experts to\nestimate the mean reward of any given expert. This leads to instance dependent\nregret bounds of $\\mathcal{O}\\left(\\lambda(\\pmb{\\mu})\\mathcal{M}\\log T/\\Delta\n\\right)$, where $\\lambda(\\pmb{\\mu})$ is a term that depends on the mean rewards\nof the experts, $\\Delta$ is the smallest gap between the mean reward of the\noptimal expert and the rest, and $\\mathcal{M}$ quantifies the information\nleakage among the experts. We show that under some assumptions\n$\\lambda(\\pmb{\\mu})$ is typically $\\mathcal{O}(\\log N)$, where $N$ is the\nnumber of experts. We implement our algorithm with stochastic experts generated\nfrom cost-sensitive classification oracles and show superior empirical\nperformance on real-world datasets, when compared to other state of the art\ncontextual bandit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 21:03:49 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 04:58:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Sharma", "Nihal", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1802.08757", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Ba\\c{s}ar", "title": "Fully Decentralized Multi-Agent Reinforcement Learning with Networked\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of \\emph{fully decentralized} multi-agent\nreinforcement learning (MARL), where the agents are located at the nodes of a\ntime-varying communication network. Specifically, we assume that the reward\nfunctions of the agents might correspond to different tasks, and are only known\nto the corresponding agent. Moreover, each agent makes individual decisions\nbased on both the information observed locally and the messages received from\nits neighbors over the network. Within this setting, the collective goal of the\nagents is to maximize the globally averaged return over the network through\nexchanging information with their neighbors. To this end, we propose two\ndecentralized actor-critic algorithms with function approximation, which are\napplicable to large-scale MARL problems where both the number of states and the\nnumber of agents are massively large. Under the decentralized structure, the\nactor step is performed individually by each agent with no need to infer the\npolicies of others. For the critic step, we propose a consensus update via\ncommunication over the network. Our algorithms are fully incremental and can be\nimplemented in an online fashion. Convergence analyses of the algorithms are\nprovided when the value functions are approximated within the class of linear\nfunctions. Extensive simulation results with both linear and nonlinear function\napproximations are presented to validate the proposed algorithms. Our work\nappears to be the first study of fully decentralized MARL algorithms for\nnetworked agents with function approximation, with provable convergence\nguarantees.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 22:53:32 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 02:15:35 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Yang", "Zhuoran", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1802.08760", "submitter": "Roman Novak", "authors": "Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington,\n  Jascha Sohl-Dickstein", "title": "Sensitivity and Generalization in Neural Networks: an Empirical Study", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice it is often found that large over-parameterized neural networks\ngeneralize better than their smaller counterparts, an observation that appears\nto conflict with classical notions of function complexity, which typically\nfavor smaller models. In this work, we investigate this tension between\ncomplexity and generalization through an extensive empirical exploration of two\nnatural metrics of complexity related to sensitivity to input perturbations.\nOur experiments survey thousands of models with various fully-connected\narchitectures, optimizers, and other hyper-parameters, as well as four\ndifferent image classification datasets.\n  We find that trained neural networks are more robust to input perturbations\nin the vicinity of the training data manifold, as measured by the norm of the\ninput-output Jacobian of the network, and that it correlates well with\ngeneralization. We further establish that factors associated with poor\ngeneralization $-$ such as full-batch training or using random labels $-$\ncorrespond to lower robustness, while factors associated with good\ngeneralization $-$ such as data augmentation and ReLU non-linearities $-$ give\nrise to more robust functions. Finally, we demonstrate how the input-output\nJacobian norm can be predictive of generalization at the level of individual\ntest points.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:11:07 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 23:45:21 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 18:01:43 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Novak", "Roman", ""], ["Bahri", "Yasaman", ""], ["Abolafia", "Daniel A.", ""], ["Pennington", "Jeffrey", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1802.08761", "submitter": "Matthew Levine", "authors": "Matthew E. Levine, David J. Albers, Marissa Burgermaster, Patricia G.\n  Davidson, Arlene M. Smaldone, Lena Mamykina", "title": "Behavioral-clinical phenotyping with type 2 diabetes self-monitoring\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To evaluate unsupervised clustering methods for identifying\nindividual-level behavioral-clinical phenotypes that relate personal biomarkers\nand behavioral traits in type 2 diabetes (T2DM) self-monitoring data. Materials\nand Methods: We used hierarchical clustering (HC) to identify groups of meals\nwith similar nutrition and glycemic impact for 6 individuals with T2DM who\ncollected self-monitoring data. We evaluated clusters on: 1) correspondence to\ngold standards generated by certified diabetes educators (CDEs) for 3\nparticipants; 2) face validity, rated by CDEs, and 3) impact on CDEs' ability\nto identify patterns for another 3 participants. Results: Gold standard (GS)\nincluded 9 patterns across 3 participants. Of these, all 9 were re-discovered\nusing HC: 4 GS patterns were consistent with patterns identified by HC (over\n50% of meals in a cluster followed the pattern); another 5 were included as\nsub-groups in broader clusers. 50% (9/18) of clusters were rated over 3 on\n5-point Likert scale for validity, significance, and being actionable. After\nreviewing clusters, CDEs identified patterns that were more consistent with\ndata (70% reduction in contradictions between patterns and participants'\nrecords). Discussion: Hierarchical clustering of blood glucose and\nmacronutrient consumption appears suitable for discovering behavioral-clinical\nphenotypes in T2DM. Most clusters corresponded to gold standard and were rated\npositively by CDEs for face validity. Cluster visualizations helped CDEs\nidentify more robust patterns in nutrition and glycemic impact, creating new\npossibilities for visual analytic solutions. Conclusion: Machine learning\nmethods can use diabetes self-monitoring data to create personalized\nbehavioral-clinical phenotypes, which may prove useful for delivering\npersonalized medicine.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:11:22 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Levine", "Matthew E.", ""], ["Albers", "David J.", ""], ["Burgermaster", "Marissa", ""], ["Davidson", "Patricia G.", ""], ["Smaldone", "Arlene M.", ""], ["Mamykina", "Lena", ""]]}, {"id": "1802.08762", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Lionel Mathelin and Steven L. Brunton and J.\n  Nathan Kutz", "title": "Diffusion Maps meet Nystr\\\"om", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion maps are an emerging data-driven technique for non-linear\ndimensionality reduction, which are especially useful for the analysis of\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\ncomputational complexity of the diffusion maps algorithm scales with the number\nof observations. Thus, long time-series data presents a significant challenge\nfor fast and efficient embedding. We propose integrating the Nystr\\\"om method\nwith diffusion maps in order to ease the computational demand. We achieve a\nspeedup of roughly two to four times when approximating the dominant diffusion\nmap components.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:33:28 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Mathelin", "Lionel", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1802.08768", "submitter": "Augustus Odena", "authors": "Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown,\n  Christopher Olah, Colin Raffel, Ian Goodfellow", "title": "Is Generator Conditioning Causally Related to GAN Performance?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work (Pennington et al, 2017) suggests that controlling the entire\ndistribution of Jacobian singular values is an important design consideration\nin deep learning. Motivated by this, we study the distribution of singular\nvalues of the Jacobian of the generator in Generative Adversarial Networks\n(GANs). We find that this Jacobian generally becomes ill-conditioned at the\nbeginning of training. Moreover, we find that the average (with z from p(z))\nconditioning of the generator is highly predictive of two other ad-hoc metrics\nfor measuring the 'quality' of trained GANs: the Inception Score and the\nFrechet Inception Distance (FID). We test the hypothesis that this relationship\nis causal by proposing a 'regularization' technique (called Jacobian Clamping)\nthat softly penalizes the condition number of the generator Jacobian. Jacobian\nClamping improves the mean Inception Score and the mean FID for GANs trained on\nseveral datasets. It also greatly reduces inter-run variance of the\naforementioned scores, addressing (at least partially) one of the main\ncriticisms of GANs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:48:01 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 00:06:57 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Odena", "Augustus", ""], ["Buckman", "Jacob", ""], ["Olsson", "Catherine", ""], ["Brown", "Tom B.", ""], ["Olah", "Christopher", ""], ["Raffel", "Colin", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1802.08770", "submitter": "Devansh Arpit", "authors": "Chen Xing, Devansh Arpit, Christos Tsirigotis, Yoshua Bengio", "title": "A Walk with SGD", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel empirical observations regarding how stochastic gradient\ndescent (SGD) navigates the loss landscape of over-parametrized deep neural\nnetworks (DNNs). These observations expose the qualitatively different roles of\nlearning rate and batch-size in DNN optimization and generalization.\nSpecifically we study the DNN loss surface along the trajectory of SGD by\ninterpolating the loss surface between parameters from consecutive\n\\textit{iterations} and tracking various metrics during training. We find that\nthe loss interpolation between parameters before and after each training\niteration's update is roughly convex with a minimum (\\textit{valley floor}) in\nbetween for most of the training. Based on this and other metrics, we deduce\nthat for most of the training update steps, SGD moves in valley like regions of\nthe loss surface by jumping from one valley wall to another at a height above\nthe valley floor. This 'bouncing between walls at a height' mechanism helps SGD\ntraverse larger distance for small batch sizes and large learning rates which\nwe find play qualitatively different roles in the dynamics. While a large\nlearning rate maintains a large height from the valley floor, a small batch\nsize injects noise facilitating exploration. We find this mechanism is crucial\nfor generalization because the valley floor has barriers and this exploration\nabove the valley floor allows SGD to quickly travel far away from the\ninitialization point (without being affected by barriers) and find flatter\nregions, corresponding to better generalization.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 00:21:10 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 16:39:29 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 11:42:41 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 01:15:02 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Xing", "Chen", ""], ["Arpit", "Devansh", ""], ["Tsirigotis", "Christos", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1802.08780", "submitter": "Chaitanya Manapragada", "authors": "Chaitanya Manapragada, Geoff Webb, Mahsa Salehi", "title": "Extremely Fast Decision Tree", "comments": "Submitted to KDD'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel incremental decision tree learning algorithm, Hoeffding\nAnytime Tree, that is statistically more efficient than the current\nstate-of-the-art, Hoeffding Tree. We demonstrate that an implementation of\nHoeffding Anytime Tree---\"Extremely Fast Decision Tree\", a minor modification\nto the MOA implementation of Hoeffding Tree---obtains significantly superior\nprequential accuracy on most of the largest classification datasets from the\nUCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in\nthe limit, is naturally resilient to concept drift, and can be used as a higher\naccuracy replacement for Hoeffding Tree in most scenarios, at a small\nadditional computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 01:44:47 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Manapragada", "Chaitanya", ""], ["Webb", "Geoff", ""], ["Salehi", "Mahsa", ""]]}, {"id": "1802.08855", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos", "title": "Minimax Distribution Estimation in Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein metric is an important measure of distance between\nprobability distributions, with applications in machine learning, statistics,\nprobability theory, and data analysis. This paper provides upper and lower\nbounds on statistical minimax rates for the problem of estimating a probability\ndistribution under Wasserstein loss, using only metric properties, such as\ncovering and packing numbers, of the sample space, and weak moment assumptions\non the probability distributions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 14:42:43 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 03:43:27 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 02:24:49 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1802.08888", "submitter": "Sami Abu-El-Haija", "authors": "Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, Joonseok Lee", "title": "N-GCN: Multi-scale Graph Convolution for Semi-supervised Node\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have shown significant improvements in\nsemi-supervised learning on graph-structured data. Concurrently, unsupervised\nlearning of graph embeddings has benefited from the information contained in\nrandom walks. In this paper, we propose a model: Network of GCNs (N-GCN), which\nmarries these two lines of work. At its core, N-GCN trains multiple instances\nof GCNs over node pairs discovered at different distances in random walks, and\nlearns a combination of the instance outputs which optimizes the classification\nobjective. Our experiments show that our proposed N-GCN model improves\nstate-of-the-art baselines on all of the challenging node classification tasks\nwe consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method\nhas other desirable properties, including generalization to recently proposed\nsemi-supervised learning methods such as GraphSAGE, allowing us to propose\nN-SAGE, and resilience to adversarial input perturbations.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 18:30:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Abu-El-Haija", "Sami", ""], ["Kapoor", "Amol", ""], ["Perozzi", "Bryan", ""], ["Lee", "Joonseok", ""]]}, {"id": "1802.08898", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from\nhigh-dimensional distributions in Statistics and Machine learning. HMC is known\nto run very efficiently in practice and its popular second-order \"leapfrog\"\nimplementation has long been conjectured to run in $d^{1/4}$ gradient\nevaluations. Here we show that this conjecture is true when sampling from\nstrongly log-concave target distributions that satisfy a weak third-order\nregularity property associated with the input data. Our regularity condition is\nweaker than the Lipschitz Hessian property and allows us to show faster\nconvergence bounds for a much larger class of distributions than would be\npossible with the usual Lipschitz Hessian constant alone. Important\ndistributions that satisfy our regularity condition include posterior\ndistributions used in Bayesian logistic regression for which the data satisfies\nan \"incoherence\" property. Our result compares favorably with the best\navailable bounds for the class of strongly log-concave distributions, which\ngrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our\nsimulations on synthetic data suggest that, when our regularity condition is\nsatisfied, leapfrog HMC performs better than its competitors -- both in terms\nof accuracy and in terms of the number of gradient evaluations it requires.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:23:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 00:27:58 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 16:26:27 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 15:31:10 GMT"}, {"version": "v5", "created": "Thu, 9 Aug 2018 18:24:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.08903", "submitter": "Jacob Gardner", "authors": "Jacob R. Gardner, Geoff Pleiss, Ruihan Wu, Kilian Q. Weinberger,\n  Andrew Gordon Wilson", "title": "Product Kernel Interpolation for Scalable Gaussian Processes", "comments": "Appears in Artificial Intelligence and Statistics (AISTATS) 21, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work shows that inference for Gaussian processes can be performed\nefficiently using iterative methods that rely only on matrix-vector\nmultiplications (MVMs). Structured Kernel Interpolation (SKI) exploits these\ntechniques by deriving approximate kernels with very fast MVMs. Unfortunately,\nsuch strategies suffer badly from the curse of dimensionality. We develop a new\ntechnique for MVM based learning that exploits product kernel structure. We\ndemonstrate that this technique is broadly applicable, resulting in linear\nrather than exponential runtime with dimension for SKI, as well as\nstate-of-the-art asymptotic complexity for multi-task GPs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:45:38 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Gardner", "Jacob R.", ""], ["Pleiss", "Geoff", ""], ["Wu", "Ruihan", ""], ["Weinberger", "Kilian Q.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1802.08908", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal\n  Talwar, \\'Ulfar Erlingsson", "title": "Scalable Private Learning with PATE", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid adoption of machine learning has increased concerns about the\nprivacy implications of machine learning models trained on sensitive data, such\nas medical records or other personal information. To address those concerns,\none promising approach is Private Aggregation of Teacher Ensembles, or PATE,\nwhich transfers to a \"student\" model the knowledge of an ensemble of \"teacher\"\nmodels, with intuitive privacy provided by training teachers on disjoint data\nand strong privacy guaranteed by noisy aggregation of teachers' answers.\nHowever, PATE has so far been evaluated only on simple classification tasks\nlike MNIST, leaving unclear its utility when applied to larger-scale learning\ntasks and real-world datasets.\n  In this work, we show how PATE can scale to learning tasks with large numbers\nof output classes and uncurated, imbalanced training data with errors. For\nthis, we introduce new noisy aggregation mechanisms for teacher ensembles that\nare more selective and add less noise, and prove their tighter\ndifferential-privacy guarantees. Our new mechanisms build on two insights: the\nchance of teacher consensus is increased by using more concentrated noise and,\nlacking consensus, no answer need be given to a student. The consensus answers\nused are more likely to be correct, offer better intuitive privacy, and incur\nlower-differential privacy cost. Our evaluation shows our mechanisms improve on\nthe original PATE on all measures, and scale to larger tasks with both high\nutility and very strong privacy ($\\varepsilon$ < 1.0).\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 20:39:51 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Papernot", "Nicolas", ""], ["Song", "Shuang", ""], ["Mironov", "Ilya", ""], ["Raghunathan", "Ananth", ""], ["Talwar", "Kunal", ""], ["Erlingsson", "\u00dalfar", ""]]}, {"id": "1802.08925", "submitter": "Aaron Lee", "authors": "Cecilia S. Lee, Ariel J. Tyring, Yue Wu, Sa Xiao, Ariel S. Rokem,\n  Nicolaas P. Deruyter, Qinqin Zhang, Adnan Tufail, Ruikang K. Wang, Aaron Y.\n  Lee", "title": "Generating retinal flow maps from structural optical coherence\n  tomography with artificial intelligence", "comments": "Under revision at Nature Communications. Submitted on June 5th 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in artificial intelligence (AI) for computer\nvision, its application in medical imaging has been limited by the burden and\nlimits of expert-generated labels. We used images from optical coherence\ntomography angiography (OCTA), a relatively new imaging modality that measures\nperfusion of the retinal vasculature, to train an AI algorithm to generate\nvasculature maps from standard structural optical coherence tomography (OCT)\nimages of the same retinae, both exceeding the ability and bypassing the need\nfor expert labeling. Deep learning was able to infer perfusion of\nmicrovasculature from structural OCT images with similar fidelity to OCTA and\nsignificantly better than expert clinicians (P < 0.00001). OCTA suffers from\nneed of specialized hardware, laborious acquisition protocols, and motion\nartifacts; whereas our model works directly from standard OCT which are\nubiquitous and quick to obtain, and allows unlocking of large volumes of\npreviously collected standard OCT data both in existing clinical trials and\nclinical practice. This finding demonstrates a novel application of AI to\nmedical imaging, whereby subtle regularities between different modalities are\nused to image the same body part and AI is used to generate detailed and\naccurate inferences of tissue function from structure imaging.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 22:51:43 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Lee", "Cecilia S.", ""], ["Tyring", "Ariel J.", ""], ["Wu", "Yue", ""], ["Xiao", "Sa", ""], ["Rokem", "Ariel S.", ""], ["Deruyter", "Nicolaas P.", ""], ["Zhang", "Qinqin", ""], ["Tufail", "Adnan", ""], ["Wang", "Ruikang K.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1802.08938", "submitter": "Tianxiang Gao", "authors": "Tianxiang Gao, Chris Chu", "title": "DID: Distributed Incremental Block Coordinate Descent for Nonnegative\n  Matrix Factorization", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has attracted much attention in the\nlast decade as a dimension reduction method in many applications. Due to the\nexplosion in the size of data, naturally the samples are collected and stored\ndistributively in local computational nodes. Thus, there is a growing need to\ndevelop algorithms in a distributed memory architecture. We propose a novel\ndistributed algorithm, called \\textit{distributed incremental block coordinate\ndescent} (DID), to solve the problem. By adapting the block coordinate descent\nframework, closed-form update rules are obtained in DID. Moreover, DID performs\nupdates incrementally based on the most recently updated residual matrix. As a\nresult, only one communication step per iteration is required. The correctness,\nefficiency, and scalability of the proposed algorithm are verified in a series\nof numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 01:23:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Gao", "Tianxiang", ""], ["Chu", "Chris", ""]]}, {"id": "1802.08946", "submitter": "Yuzhe Ma", "authors": "Yuzhe Ma, Robert Nowak, Philippe Rigollet, Xuezhou Zhang, Xiaojin Zhu", "title": "Teacher Improves Learning by Selecting a Training Subset", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call a learner super-teachable if a teacher can trim down an iid training\nset while making the learner learn even better. We provide sharp super-teaching\nguarantees on two learners: the maximum likelihood estimator for the mean of a\nGaussian, and the large margin classifier in 1D. For general learners, we\nprovide a mixed-integer nonlinear programming-based algorithm to find a super\nteaching set. Empirical experiments show that our algorithm is able to find\ngood super-teaching sets for both regression and classification problems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 02:47:46 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ma", "Yuzhe", ""], ["Nowak", "Robert", ""], ["Rigollet", "Philippe", ""], ["Zhang", "Xuezhou", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1802.08976", "submitter": "Yingfei Wang", "authors": "Yingfei Wang, Juliana Martins Do Nascimento, Warren Powell", "title": "Reinforcement Learning for Dynamic Bidding in Truckload Markets: an\n  Application to Large-Scale Fleet Management with Advance Commitments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truckload brokerages, a $100 billion/year industry in the U.S., plays the\ncritical role of matching shippers with carriers, often to move loads several\ndays into the future. Brokerages not only have to find companies that will\nagree to move a load, the brokerage often has to find a price that both the\nshipper and carrier will agree to. The price not only varies by shipper and\ncarrier, but also by the traffic lanes and other variables such as commodity\ntype. Brokerages have to learn about shipper and carrier response functions by\noffering a price and observing whether each accepts the quote. We propose a\nknowledge gradient policy with bootstrap aggregation for high-dimensional\ncontextual settings to guide price experimentation by maximizing the value of\ninformation. The learning policy is tested using a carefully calibrated fleet\nsimulator that includes a stochastic lookahead policy that simulates fleet\nmovements, as well as the stochastic modeling of driver assignments and the\ncarrier's load commitment policies with advance booking.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 09:48:59 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 05:51:25 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wang", "Yingfei", ""], ["Nascimento", "Juliana Martins Do", ""], ["Powell", "Warren", ""]]}, {"id": "1802.09030", "submitter": "Uri Patish", "authors": "Uri Patish, Shimon Ullman", "title": "Cakewalk Sampling", "comments": "Accepted as a conference paper by AAAI-2020 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of finding good local optima in combinatorial optimization\nproblems. Although combinatorial optimization is NP-hard in general, locally\noptimal solutions are frequently used in practice. Local search methods however\ntypically converge to a limited set of optima that depend on their\ninitialization. Sampling methods on the other hand can access any valid\nsolution, and thus can be used either directly or alongside methods of the\nformer type as a way for finding good local optima. Since the effectiveness of\nthis strategy depends on the sampling distribution, we derive a robust learning\nalgorithm that adapts sampling distributions towards good local optima of\narbitrary objective functions. As a first use case, we empirically study the\nefficiency in which sampling methods can recover locally maximal cliques in\nundirected graphs. Not only do we show how our adaptive sampler outperforms\nrelated methods, we also show how it can even approach the performance of\nestablished clique algorithms. As a second use case, we consider how greedy\nalgorithms can be combined with our adaptive sampler, and we demonstrate how\nthis leads to superior performance in k-medoid clustering. Together, these\nfindings suggest that our adaptive sampler can provide an effective strategy to\ncombinatorial optimization problems that arise in practice.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 16:15:32 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 11:07:41 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Patish", "Uri", ""], ["Ullman", "Shimon", ""]]}, {"id": "1802.09031", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda, Taiji Suzuki", "title": "Functional Gradient Boosting based on Residual Network Perception", "comments": "22 pages, 1 figure, 1 table. An extended version of ICML 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual Networks (ResNets) have become state-of-the-art models in deep\nlearning and several theoretical studies have been devoted to understanding why\nResNet works so well. One attractive viewpoint on ResNet is that it is\noptimizing the risk in a functional space by combining an ensemble of effective\nfeatures. In this paper, we adopt this viewpoint to construct a new gradient\nboosting method, which is known to be very powerful in data analysis. To do so,\nwe formalize the gradient boosting perspective of ResNet mathematically using\nthe notion of functional gradients and propose a new method called ResFGB for\nclassification tasks by leveraging ResNet perception. Two types of\ngeneralization guarantees are provided from the optimization perspective: one\nis the margin bound and the other is the expected risk bound by the\nsample-splitting technique. Experimental results show superior performance of\nthe proposed method over state-of-the-art methods such as LightGBM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 16:15:57 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 02:23:08 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nitanda", "Atsushi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1802.09052", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Yifan Sun and Brian Eriksson and Wenlin Wang and Vaneet\n  Aggarwal", "title": "Wide Compression: Tensor Ring Nets", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated state-of-the-art performance in a\nvariety of real-world applications. In order to obtain performance gains, these\nnetworks have grown larger and deeper, containing millions or even billions of\nparameters and over a thousand layers. The trade-off is that these large\narchitectures require an enormous amount of memory, storage, and computation,\nthus limiting their usability. Inspired by the recent tensor ring\nfactorization, we introduce Tensor Ring Networks (TR-Nets), which significantly\ncompress both the fully connected layers and the convolutional layers of deep\nneural networks. Our results show that our TR-Nets approach {is able to\ncompress LeNet-5 by $11\\times$ without losing accuracy}, and can compress the\nstate-of-the-art Wide ResNet by $243\\times$ with only 2.3\\% degradation in\n{Cifar10 image classification}. Overall, this compression scheme shows promise\nin scientific computing and deep learning, especially for emerging\nresource-constrained devices such as smartphones, wearables, and IoT devices.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:09:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wang", "Wenqi", ""], ["Sun", "Yifan", ""], ["Eriksson", "Brian", ""], ["Wang", "Wenlin", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1802.09059", "submitter": "Ahmad Pesaranghader", "authors": "Ahmad Pesaranghader, Ali Pesaranghader, Stan Matwin, Marina Sokolova", "title": "One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation\n  of Text Data", "comments": "12 pages, 1 figure, to appear in the Proceedings of the 31st Canadian\n  Conference on Artificial Intelligence, 8-11 May, 2018, Toronto, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent technical and scientific advances, we have a wealth of\ninformation hidden in unstructured text data such as offline/online narratives,\nresearch articles, and clinical reports. To mine these data properly,\nattributable to their innate ambiguity, a Word Sense Disambiguation (WSD)\nalgorithm can avoid numbers of difficulties in Natural Language Processing\n(NLP) pipeline. However, considering a large number of ambiguous words in one\nlanguage or technical domain, we may encounter limiting constraints for proper\ndeployment of existing WSD models. This paper attempts to address the problem\nof one-classifier-per-one-word WSD algorithms by proposing a single\nBidirectional Long Short-Term Memory (BLSTM) network which by considering\nsenses and context sequences works on all ambiguous words collectively.\nEvaluated on SensEval-3 benchmark, we show the result of our model is\ncomparable with top-performing WSD algorithms. We also discuss how applying\nadditional modifications alleviates the model fault and the need for more\ntraining data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:51:53 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Pesaranghader", "Ahmad", ""], ["Pesaranghader", "Ali", ""], ["Matwin", "Stan", ""], ["Sokolova", "Marina", ""]]}, {"id": "1802.09064", "submitter": "Dennis Shen", "authors": "Anish Agarwal, Muhammad Jehangir Amjad, Devavrat Shah and Dennis Shen", "title": "Model Agnostic Time Series Analysis via Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to impute and forecast a time series by transforming\nthe observed time series into a matrix, utilizing matrix estimation to recover\nmissing values and de-noise observed entries, and performing linear regression\nto make predictions. At the core of our analysis is a representation result,\nwhich states that for a large model class, the transformed time series matrix\nis (approximately) low-rank. In effect, this generalizes the widely used\nSingular Spectrum Analysis (SSA) in time series literature, and allows us to\nestablish a rigorous link between time series analysis and matrix estimation.\nThe key to establishing this link is constructing a Page matrix with\nnon-overlapping entries rather than a Hankel matrix as is commonly done in the\nliterature (e.g., SSA). This particular matrix structure allows us to provide\nfinite sample analysis for imputation and prediction, and prove the asymptotic\nconsistency of our method. Another salient feature of our algorithm is that it\nis model agnostic with respect to both the underlying time dynamics and the\nnoise distribution in the observations. The noise agnostic property of our\napproach allows us to recover the latent states when only given access to noisy\nand partial observations a la a Hidden Markov Model; e.g., recovering the\ntime-varying parameter of a Poisson process without knowing that the underlying\nprocess is Poisson. Furthermore, since our forecasting algorithm requires\nregression with noisy features, our approach suggests a matrix estimation based\nmethod - coupled with a novel, non-standard matrix estimation error metric - to\nsolve the error-in-variable regression problem, which could be of interest in\nits own right. Through synthetic and real-world datasets, we demonstrate that\nour algorithm outperforms standard software packages (including R libraries) in\nthe presence of missing data as well as high levels of noise.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 19:06:06 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 02:50:28 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 17:45:32 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2018 21:17:18 GMT"}, {"version": "v5", "created": "Sun, 18 Nov 2018 20:23:03 GMT"}, {"version": "v6", "created": "Fri, 26 Apr 2019 17:03:36 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Agarwal", "Anish", ""], ["Amjad", "Muhammad Jehangir", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""]]}, {"id": "1802.09069", "submitter": "Songbai Yan", "authors": "Songbai Yan, Kamalika Chaudhuri, Tara Javidi", "title": "Active Learning with Logged Data", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider active learning with logged data, where labeled examples are\ndrawn conditioned on a predetermined logging policy, and the goal is to learn a\nclassifier on the entire population, not just conditioned on the logging\npolicy. Prior work addresses this problem either when only logged data is\navailable, or purely in a controlled random experimentation setting where the\nlogged data is ignored. In this work, we combine both approaches to provide an\nalgorithm that uses logged data to bootstrap and inform experimentation, thus\nachieving the best of both worlds. Our work is inspired by a connection between\ncontrolled random experimentation and active learning, and modifies existing\ndisagreement-based active learning algorithms to exploit logged data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 19:37:58 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 04:14:13 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 17:06:59 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Yan", "Songbai", ""], ["Chaudhuri", "Kamalika", ""], ["Javidi", "Tara", ""]]}, {"id": "1802.09086", "submitter": "Jalil Taghia", "authors": "Jalil Taghia, Thomas B. Sch\\\"on", "title": "Conditionally Independent Multiresolution Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiresolution Gaussian process (GP) has gained increasing attention as\na viable approach towards improving the quality of approximations in GPs that\nscale well to large-scale data. Most of the current constructions assume full\nindependence across resolutions. This assumption simplifies the inference, but\nit underestimates the uncertainties in transitioning from one resolution to\nanother. This in turn results in models which are prone to overfitting in the\nsense of excessive sensitivity to the chosen resolution, and predictions which\nare non-smooth at the boundaries. Our contribution is a new construction which\ninstead assumes conditional independence among GPs across resolutions. We show\nthat relaxing the full independence assumption enables robustness against\noverfitting, and that it delivers predictions that are smooth at the\nboundaries. Our new model is compared against current state of the art on 2\nsynthetic and 9 real-world datasets. In most cases, our new conditionally\nindependent construction performed favorably when compared against models based\non the full independence assumption. In particular, it exhibits little to no\nsigns of overfitting.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 21:32:08 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 09:52:18 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 20:55:37 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Taghia", "Jalil", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1802.09117", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Jianqing Fan, Yinchu Zhu", "title": "Testability of high-dimensional linear models with non-sparse structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding statistical inference under possibly non-sparse\nhigh-dimensional models has gained much interest recently. For a given\ncomponent of the regression coefficient, we show that the difficulty of the\nproblem depends on the sparsity of the corresponding row of the precision\nmatrix of the covariates, not the sparsity of the regression coefficients. We\ndevelop new concepts of uniform and essentially uniform non-testability that\nallow the study of limitations of tests across a broad set of alternatives.\nUniform non-testability identifies a collection of alternatives such that the\npower of any test, against any alternative in the group, is asymptotically at\nmost equal to the nominal size. Implications of the new constructions include\nnew minimax testability results that, in sharp contrast to the current results,\ndo not depend on the sparsity of the regression parameters. We identify new\ntradeoffs between testability and feature correlation. In particular, we show\nthat, in models with weak feature correlations, minimax lower bound can be\nattained by a test whose power has the $\\sqrt{n}$ rate, regardless of the size\nof the model sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 01:00:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 18:49:17 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 02:15:24 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Bradic", "Jelena", ""], ["Fan", "Jianqing", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1802.09127", "submitter": "Carlos Riquelme Ruiz", "authors": "Carlos Riquelme, George Tucker, Jasper Snoek", "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep\n  Networks for Thompson Sampling", "comments": "Sixth International Conference on Learning Representations, ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep reinforcement learning have made significant strides\nin performance on applications such as Go and Atari games. However, developing\npractical methods to balance exploration and exploitation in complex domains\nremains largely unsolved. Thompson Sampling and its extension to reinforcement\nlearning provide an elegant approach to exploration that only requires access\nto posterior samples of the model. At the same time, advances in approximate\nBayesian methods have made posterior approximation for flexible neural network\nmodels practical. Thus, it is attractive to consider approximate Bayesian\nneural networks in a Thompson Sampling framework. To understand the impact of\nusing an approximate posterior on Thompson Sampling, we benchmark\nwell-established and recently developed methods for approximate posterior\nsampling combined with Thompson Sampling over a series of contextual bandit\nproblems. We found that many approaches that have been successful in the\nsupervised learning setting underperformed in the sequential decision-making\nscenario. In particular, we highlight the challenge of adapting slowly\nconverging uncertainty estimates to the online setting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 02:04:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Riquelme", "Carlos", ""], ["Tucker", "George", ""], ["Snoek", "Jasper", ""]]}, {"id": "1802.09128", "submitter": "Nilesh Tripuraneni", "authors": "Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, Michael I.\n  Jordan", "title": "Averaging Stochastic Gradient Descent on Riemannian Manifolds", "comments": "COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of a function defined on a Riemannian manifold\n$\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We\ndevelop a geometric framework to transform a sequence of slowly converging\niterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to\nan averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate.\nWe then present an application of our framework to geodesically-strongly-convex\n(and possibly Euclidean non-convex) problems. Finally, we demonstrate how these\nideas apply to the case of streaming $k$-PCA, where we show how to accelerate\nthe slow rate of the randomized power method (without requiring knowledge of\nthe eigengap) into a robust algorithm achieving the optimal rate of\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 02:05:06 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 14:17:09 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Tripuraneni", "Nilesh", ""], ["Flammarion", "Nicolas", ""], ["Bach", "Francis", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1802.09129", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Sibei Yang, Yizhou Yu", "title": "Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning", "comments": "accepted by IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 02:07:19 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yang", "Sibei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1802.09184", "submitter": "Lin Yang", "authors": "Sham Kakade, Mengdi Wang, Lin F. Yang", "title": "Variance Reduction Methods for Sublinear Reinforcement Learning", "comments": "There is a technical issue in the analysis that is not easily fixable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a technical issue in the analysis that is not easily fixable. We,\ntherefore, withdraw the submission. Sorry for the inconvenience.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 07:01:24 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 19:53:50 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2018 05:22:45 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 16:25:37 GMT"}, {"version": "v5", "created": "Sat, 27 Jun 2020 04:14:36 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kakade", "Sham", ""], ["Wang", "Mengdi", ""], ["Yang", "Lin F.", ""]]}, {"id": "1802.09188", "submitter": "Alain Durmus", "authors": "Alain Durmus, Szymon Majewski, B{\\l}a\\.zej Miasojedow", "title": "Analysis of Langevin Monte Carlo via convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide new insights on the Unadjusted Langevin Algorithm.\nWe show that this method can be formulated as a first order optimization\nalgorithm of an objective functional defined on the Wasserstein space of order\n$2$. Using this interpretation and techniques borrowed from convex\noptimization, we give a non-asymptotic analysis of this method to sample from\nlogconcave smooth target distribution on $\\mathbb{R}^d$. Based on this\ninterpretation, we propose two new methods for sampling from a non-smooth\ntarget distribution, which we analyze as well. Besides, these new algorithms\nare natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD)\nalgorithm, which is a popular extension of the Unadjusted Langevin Algorithm.\nSimilar to SGLD, they only rely on approximations of the gradient of the target\nlog density and can be used for large-scale Bayesian inference.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 07:50:57 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 22:15:40 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Durmus", "Alain", ""], ["Majewski", "Szymon", ""], ["Miasojedow", "B\u0142a\u017cej", ""]]}, {"id": "1802.09197", "submitter": "Woo Yong Choi", "authors": "Woo Yong Choi, Kyu Ye Song, Chan Woo Lee", "title": "AI4AI: Quantitative Methods for Classifying Host Species from Avian\n  Influenza DNA Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avian Influenza breakouts cause millions of dollars in damage each year\nglobally, especially in Asian countries such as China and South Korea. The\nimpact magnitude of a breakout directly correlates to time required to fully\nunderstand the influenza virus, particularly the interspecies pathogenicity.\nThe procedure requires laboratory tests that require resources typically\nlacking in a breakout emergency. In this study, we propose new quantitative\nmethods utilizing machine learning and deep learning to correctly classify host\nspecies given raw DNA sequence data of the influenza virus, and provide\nprobabilities for each classification. The best deep learning models achieve\ntop-1 classification accuracy of 47%, and top-3 classification accuracy of 82%,\non a dataset of 11 host species classes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 08:28:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Choi", "Woo Yong", ""], ["Song", "Kyu Ye", ""], ["Lee", "Chan Woo", ""]]}, {"id": "1802.09210", "submitter": "Michael Unser", "authors": "Michael Unser", "title": "A representer theorem for deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to optimize the activation functions of a deep neural network by\nadding a corresponding functional regularization to the cost function. We\njustify the use of a second-order total-variation criterion. This allows us to\nderive a general representer theorem for deep neural networks that makes a\ndirect connection with splines and sparsity. Specifically, we show that the\noptimal network configuration can be achieved with activation functions that\nare nonuniform linear splines with adaptive knots. The bottom line is that the\naction of each neuron is encoded by a spline whose parameters (including the\nnumber of knots) are optimized during the training procedure. The scheme\nresults in a computational structure that is compatible with the existing\ndeep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut\narchitectures. It also suggests novel optimization challenges, while making the\nlink with $\\ell_1$ minimization and sparsity-promoting techniques explicit.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 09:14:48 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 13:15:18 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Unser", "Michael", ""]]}, {"id": "1802.09225", "submitter": "Michael Viderman", "authors": "Noa Avigdor-Elgrabli, Alex Libov, Michael Viderman, Ran Wolff", "title": "Interpreting Complex Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of a machine learning induced models is critical for feature\nengineering, debugging, and, arguably, compliance. Yet, best of breed machine\nlearning models tend to be very complex. This paper presents a method for model\ninterpretation which has the main benefit that the simple interpretations it\nprovides are always grounded in actual sets of learning examples. The method is\nvalidated on the task of interpreting a complex regression model in the context\nof both an academic problem -- predicting the year in which a song was recorded\nand an industrial one -- predicting mail user churn.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 09:56:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Avigdor-Elgrabli", "Noa", ""], ["Libov", "Alex", ""], ["Viderman", "Michael", ""], ["Wolff", "Ran", ""]]}, {"id": "1802.09246", "submitter": "Xin He", "authors": "Xin He and Junhui Wang and Shaogao Lv", "title": "Efficient kernel-based variable selection with sparsistency", "comments": "27 pages, 5 figures", "journal-ref": "Statistica Sinica, 2022", "doi": "10.5705/ss.202019.0401", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is central to high-dimensional data analysis, and various\nalgorithms have been developed. Ideally, a variable selection algorithm shall\nbe flexible, scalable, and with theoretical guarantee, yet most existing\nalgorithms cannot attain these properties at the same time. In this article, a\nthree-step variable selection algorithm is developed, involving kernel-based\nestimation of the regression function and its gradient functions as well as a\nhard thresholding. Its key advantage is that it assumes no explicit model\nassumption, admits general predictor effects, allows for scalable computation,\nand attains desirable asymptotic sparsistency. The proposed algorithm can be\nadapted to any reproducing kernel Hilbert space (RKHS) with different kernel\nfunctions, and can be extended to interaction selection with slight\nmodification. Its computational cost is only linear in the data dimension, and\ncan be further improved through parallel computing. The sparsistency of the\nproposed algorithm is established for general RKHS under mild conditions,\nincluding linear and Gaussian kernels as special cases. Its effectiveness is\nalso supported by a variety of simulated and real examples.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 11:09:18 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 13:14:23 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 05:01:23 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["He", "Xin", ""], ["Wang", "Junhui", ""], ["Lv", "Shaogao", ""]]}, {"id": "1802.09301", "submitter": "Ya-Ping Hsieh", "authors": "Ya-Ping Hsieh and Volkan Cevher", "title": "Dimension-free Information Concentration via Exp-Concavity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information concentration of probability measures have important implications\nin learning theory. Recently, it is discovered that the information content of\na log-concave distribution concentrates around their differential entropy,\nalbeit with an unpleasant dependence on the ambient dimension. In this work, we\nprove that if the potentials of the log-concave distribution are exp-concave,\nwhich is a central notion for fast rates in online and statistical learning,\nthen the concentration of information can be further improved to depend only on\nthe exp-concavity parameter, and hence, it can be dimension independent.\nCentral to our proof is a novel yet simple application of the variance\nBrascamp-Lieb inequality. In the context of learning theory, our\nconcentration-of-information result immediately implies high-probability\nresults to many of the previous bounds that only hold in expectation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 13:59:25 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Hsieh", "Ya-Ping", ""], ["Cevher", "Volkan", ""]]}, {"id": "1802.09371", "submitter": "Thierry Dumas", "authors": "Thierry Dumas (Sirocco), Aline Roumy (Sirocco), Christine Guillemot\n  (Sirocco)", "title": "Autoencoder based image compression: can the learning be quantization\n  independent?", "comments": "International Conference on Acoustics, Speech and Signal Processing\n  ICASSP, Apr 2018, Calgary, Canada. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of learning transforms for image compression\nvia autoencoders. Usually, the rate-distortion performances of image\ncompression are tuned by varying the quantization step size. In the case of\nautoen-coders, this in principle would require learning one transform per\nrate-distortion point at a given quantization step size. Here, we show that\ncomparable performances can be obtained with a unique learned transform. The\ndifferent rate-distortion points are then reached by varying the quantization\nstep size at test time. This approach saves a lot of training time.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 08:31:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Dumas", "Thierry", "", "Sirocco"], ["Roumy", "Aline", "", "Sirocco"], ["Guillemot", "Christine", "", "Sirocco"]]}, {"id": "1802.09381", "submitter": "Jean-Philippe Vert", "authors": "Beyrem Khalfaoui (CBIO), Jean-Philippe Vert (CBIO, DMA)", "title": "DropLasso: A robust variant of Lasso for single cell RNA-seq data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell RNA sequencing (scRNA-seq) is a fast growing approach to measure\nthe genome-wide transcriptome of many individual cells in parallel, but results\nin noisy data with many dropout events. Existing methods to learn molecular\nsignatures from bulk transcriptomic data may therefore not be adapted to\nscRNA-seq data, in order to automatically classify individual cells into\npredefined classes. We propose a new method called DropLasso to learn a\nmolecular signature from scRNA-seq data. DropLasso extends the dropout\nregularisation technique, popular in neural network training, to esti- mate\nsparse linear models. It is well adapted to data corrupted by dropout noise,\nsuch as scRNA-seq data, and we clarify how it relates to elastic net\nregularisation. We provide promising results on simulated and real scRNA-seq\ndata, suggesting that DropLasso may be better adapted than standard regularisa-\ntions to infer molecular signatures from scRNA-seq data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:10:44 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Khalfaoui", "Beyrem", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO, DMA"]]}, {"id": "1802.09386", "submitter": "Pablo Piantanida", "authors": "Cl\\'ement Feutry and Pablo Piantanida and Yoshua Bengio and Pierre\n  Duhamel", "title": "Learning Anonymized Representations with Adversarial Neural Networks", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods protecting sensitive information or the identity of the\ndata owner have become critical to ensure privacy of individuals as well as of\norganizations. This paper investigates anonymization methods based on\nrepresentation learning and deep neural networks, and motivated by novel\ninformation theoretical bounds. We introduce a novel training objective for\nsimultaneously training a predictor over target variables of interest (the\nregular labels) while preventing an intermediate representation to be\npredictive of the private labels. The architecture is based on three\nsub-networks: one going from input to representation, one from representation\nto predicted regular labels, and one from representation to predicted private\nlabels. The training procedure aims at learning representations that preserve\nthe relevant part of the information (about regular labels) while dismissing\ninformation about the private labels which correspond to the identity of a\nperson. We demonstrate the success of this approach for two distinct\nclassification versus anonymization tasks (handwritten digits and sentiment\nanalysis).\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:14:51 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Feutry", "Cl\u00e9ment", ""], ["Piantanida", "Pablo", ""], ["Bengio", "Yoshua", ""], ["Duhamel", "Pierre", ""]]}, {"id": "1802.09405", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, Aurelio\n  Uncini", "title": "Improving Graph Convolutional Networks with Non-Parametric Activation\n  Functions", "comments": "Submitted to EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a class of neural networks that allow to\nefficiently perform inference on data that is associated to a graph structure,\nsuch as, e.g., citation networks or knowledge graphs. While several variants of\nGNNs have been proposed, they only consider simple nonlinear activation\nfunctions in their layers, such as rectifiers or squashing functions. In this\npaper, we investigate the use of graph convolutional networks (GCNs) when\ncombined with more complex activation functions, able to adapt from the\ntraining data. More specifically, we extend the recently proposed kernel\nactivation function, a non-parametric model which can be implemented easily,\ncan be regularized with standard $\\ell_p$-norms techniques, and is smooth over\nits entire domain. Our experimental evaluation shows that the proposed\narchitecture can significantly improve over its baseline, while similar\nimprovements cannot be obtained by simply increasing the depth or size of the\noriginal GCN.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:36:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Comminiello", "Danilo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1802.09477", "submitter": "Scott Fujimoto", "authors": "Scott Fujimoto, Herke van Hoof, David Meger", "title": "Addressing Function Approximation Error in Actor-Critic Methods", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In value-based reinforcement learning methods such as deep Q-learning,\nfunction approximation errors are known to lead to overestimated value\nestimates and suboptimal policies. We show that this problem persists in an\nactor-critic setting and propose novel mechanisms to minimize its effects on\nboth the actor and the critic. Our algorithm builds on Double Q-learning, by\ntaking the minimum value between a pair of critics to limit overestimation. We\ndraw the connection between target networks and overestimation bias, and\nsuggest delaying policy updates to reduce per-update error and further improve\nperformance. We evaluate our method on the suite of OpenAI gym tasks,\noutperforming the state of the art in every environment tested.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:54:49 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 18:21:26 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 17:37:07 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Fujimoto", "Scott", ""], ["van Hoof", "Herke", ""], ["Meger", "David", ""]]}, {"id": "1802.09484", "submitter": "William Fedus", "authors": "Valentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard,\n  Philippe Beaudoin, Hugo Larochelle, Joelle Pineau, Doina Precup, Yoshua\n  Bengio", "title": "Disentangling the independently controllable factors of variation by\n  interacting with the world", "comments": "Presented at NIPS 2017 Learning Disentangling Representations\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors, and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:03:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Thomas", "Valentin", ""], ["Bengio", "Emmanuel", ""], ["Fedus", "William", ""], ["Pondard", "Jules", ""], ["Beaudoin", "Philippe", ""], ["Larochelle", "Hugo", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1802.09511", "submitter": "Amin Jalali", "authors": "Amin Jalali, Rebecca Willett", "title": "Missing Data in Sparse Transition Matrix Estimation for Sub-Gaussian\n  Vector Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time series data exist in numerous areas such as finance,\ngenomics, healthcare, and neuroscience. An unavoidable aspect of all such\ndatasets is missing data, and dealing with this issue has been an important\nfocus in statistics, control, and machine learning. In this work, we consider a\nhigh-dimensional estimation problem where a dynamical system, governed by a\nstable vector autoregressive model, is randomly and only partially observed at\neach time point. Our task amounts to estimating the transition matrix, which is\nassumed to be sparse. In such a scenario, where covariates are highly\ninterdependent and partially missing, new theoretical challenges arise. While\ntransition matrix estimation in vector autoregressive models has been studied\npreviously, the missing data scenario requires separate efforts. Moreover,\nwhile transition matrix estimation can be studied from a high-dimensional\nsparse linear regression perspective, the covariates are highly dependent and\nexisting results on regularized estimation with missing data from\ni.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a\nnovel concentration result when the innovation noise satisfies the convex\nconcentration property, as well as 2) a new quantity for characterizing the\ninteractions of the time-varying observation process with the underlying\ndynamical system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:54:19 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jalali", "Amin", ""], ["Willett", "Rebecca", ""]]}, {"id": "1802.09514", "submitter": "Jason Altschuler", "authors": "Jason Altschuler, Victor-Emmanuel Brunel, Alan Malek", "title": "Best Arm Identification for Contaminated Bandits", "comments": "to appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies active learning in the context of robust statistics.\nSpecifically, we propose a variant of the Best Arm Identification problem for\n\\emph{contaminated bandits}, where each arm pull has probability $\\varepsilon$\nof generating a sample from an arbitrary contamination distribution instead of\nthe true underlying distribution. The goal is to identify the best (or\napproximately best) true distribution with high probability, with a secondary\ngoal of providing guarantees on the quality of this distribution. The primary\nchallenge of the contaminated bandit setting is that the true distributions are\nonly partially identifiable, even with infinite samples. To address this, we\ndevelop tight, non-asymptotic sample complexity bounds for high-probability\nestimation of the first two robust moments (median and median absolute\ndeviation) from contaminated samples. These concentration inequalities are the\nmain technical contributions of the paper and may be of independent interest.\nUsing these results, we adapt several classical Best Arm Identification\nalgorithms to the contaminated bandit setting and derive sample complexity\nupper bounds for our problem. Finally, we provide matching\ninformation-theoretic lower bounds on the sample complexity (up to a small\nlogarithmic factor).\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:59:30 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 22:23:06 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 00:15:37 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 15:31:29 GMT"}, {"version": "v5", "created": "Wed, 15 May 2019 15:32:00 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Altschuler", "Jason", ""], ["Brunel", "Victor-Emmanuel", ""], ["Malek", "Alan", ""]]}, {"id": "1802.09548", "submitter": "Nina Grgi\\'c-Hla\\v{c}a", "authors": "Nina Grgi\\'c-Hla\\v{c}a, Elissa M. Redmiles, Krishna P. Gummadi, Adrian\n  Weller", "title": "Human Perceptions of Fairness in Algorithmic Decision Making: A Case\n  Study of Criminal Risk Prediction", "comments": "To appear in the Proceedings of the Web Conference (WWW 2018). Code\n  available at https://fate-computing.mpi-sws.org/procedural_fairness/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As algorithms are increasingly used to make important decisions that affect\nhuman lives, ranging from social benefit assignment to predicting risk of\ncriminal recidivism, concerns have been raised about the fairness of\nalgorithmic decision making. Most prior works on algorithmic fairness\nnormatively prescribe how fair decisions ought to be made. In contrast, here,\nwe descriptively survey users for how they perceive and reason about fairness\nin algorithmic decision making.\n  A key contribution of this work is the framework we propose to understand why\npeople perceive certain features as fair or unfair to be used in algorithms.\nOur framework identifies eight properties of features, such as relevance,\nvolitionality and reliability, as latent considerations that inform people's\nmoral judgments about the fairness of feature use in decision-making\nalgorithms. We validate our framework through a series of scenario-based\nsurveys with 576 people. We find that, based on a person's assessment of the\neight latent properties of a feature in our exemplar scenario, we can\naccurately (> 85%) predict if the person will judge the use of the feature as\nfair.\n  Our findings have important implications. At a high-level, we show that\npeople's unfairness concerns are multi-dimensional and argue that future\nstudies need to address unfairness concerns beyond discrimination. At a\nlow-level, we find considerable disagreements in people's fairness judgments.\nWe identify root causes of the disagreements, and note possible pathways to\nresolve them.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 19:00:15 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Grgi\u0107-Hla\u010da", "Nina", ""], ["Redmiles", "Elissa M.", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""]]}, {"id": "1802.09568", "submitter": "Tomer Koren", "authors": "Vineet Gupta, Tomer Koren, Yoram Singer", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preconditioned gradient methods are among the most general and powerful tools\nin optimization. However, preconditioning requires storing and manipulating\nprohibitively large matrices. We describe and analyze a new structure-aware\npreconditioning algorithm, called Shampoo, for stochastic optimization over\ntensor spaces. Shampoo maintains a set of preconditioning matrices, each of\nwhich operates on a single dimension, contracting over the remaining\ndimensions. We establish convergence guarantees in the stochastic convex\nsetting, the proof of which builds upon matrix trace inequalities. Our\nexperiments with state-of-the-art deep learning models show that Shampoo is\ncapable of converging considerably faster than commonly used optimizers.\nAlthough it involves a more complex update rule, Shampoo's runtime per step is\ncomparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 19:36:41 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 00:12:58 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Gupta", "Vineet", ""], ["Koren", "Tomer", ""], ["Singer", "Yoram", ""]]}, {"id": "1802.09578", "submitter": "Yining Wang", "authors": "Yining Wang, Yi Wu, Simon S. Du", "title": "Near-Linear Time Local Polynomial Nonparametric Estimation with Box\n  Kernels", "comments": "Accepted to INFORMS Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local polynomial regression (Fan and Gijbels 1996) is an important class of\nmethods for nonparametric density estimation and regression problems. However,\nstraightforward implementation of local polynomial regression has quadratic\ntime complexity which hinders its applicability in large-scale data analysis.\nIn this paper, we significantly accelerate the computation of local polynomial\nestimates by novel applications of multi-dimensional binary indexed trees\n(Fenwick 1994). Both time and space complexity of our proposed algorithm is\nnearly linear in the number of input data points. Simulation results confirm\nthe efficiency and effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:04:58 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:42:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Yining", ""], ["Wu", "Yi", ""], ["Du", "Simon S.", ""]]}, {"id": "1802.09583", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite, Daniel M. Roy", "title": "Data-dependent PAC-Bayes priors via differential privacy", "comments": "18 pages, 2 figures; equivalent to camera ready, but includes\n  supplementary materials; subsumes and extends some results first reported in\n  arXiv:1712.09376", "journal-ref": "Advances in Neural Information Processing Systems, 31 (2018), pp.\n  8430-8441", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999)\ncan incorporate knowledge about the learning algorithm and (data) distribution\nthrough the use of distribution-dependent priors, yielding tighter\ngeneralization bounds on data-dependent posteriors. Using this flexibility,\nhowever, is difficult, especially when the data distribution is presumed to be\nunknown. We show how an {\\epsilon}-differentially private data-dependent prior\nyields a valid PAC-Bayes bound, and then show how non-private mechanisms for\nchoosing priors can also yield generalization bounds. As an application of this\nresult, we show that a Gaussian prior mean chosen via stochastic gradient\nLangevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes\nbound given control of the 2-Wasserstein distance to an\n{\\epsilon}-differentially private stationary distribution. We study our\ndata-dependent bounds empirically, and show that they can be nonvacuous even\nwhen other distribution-dependent bounds are vacuous.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:14:03 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 17:06:45 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1802.09596", "submitter": "Philipp Probst", "authors": "Philipp Probst, Bernd Bischl and Anne-Laure Boulesteix", "title": "Tunability: Importance of Hyperparameters of Machine Learning Algorithms", "comments": "22 pages, 10 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern supervised machine learning algorithms involve hyperparameters that\nhave to be set before running them. Options for setting hyperparameters are\ndefault values from the software package, manual configuration by the user or\nconfiguring them for optimal predictive performance by a tuning procedure. The\ngoal of this paper is two-fold. Firstly, we formalize the problem of tuning\nfrom a statistical point of view, define data-based defaults and suggest\ngeneral measures quantifying the tunability of hyperparameters of algorithms.\nSecondly, we conduct a large-scale benchmarking study based on 38 datasets from\nthe OpenML platform and six common machine learning algorithms. We apply our\nmeasures to assess the tunability of their parameters. Our results yield\ndefault values for hyperparameters and enable users to decide whether it is\nworth conducting a possibly time consuming tuning strategy, to focus on the\nmost important hyperparameters and to chose adequate hyperparameter spaces for\ntuning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:41:17 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 12:01:07 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 08:43:01 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Probst", "Philipp", ""], ["Bischl", "Bernd", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "1802.09631", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi, Ian H. Jermyn, Jochen Einbeck", "title": "Bayesian shape modelling of cross-sectional geological data", "comments": "4 pages, 1 figure, In proceedings 29th International Workshop on\n  Statistical Modelling, 14-18 July 2014, Gottingen, Germany. Amsterdam:\n  Statistical Modelling Society, pp. 161-164", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape information is of great importance in many applications. For example,\nthe oil-bearing capacity of sand bodies, the subterranean remnants of ancient\nrivers, is related to their cross-sectional shapes. The analysis of these\nshapes is therefore of some interest, but current classifications are\nsimplistic and ad hoc. In this paper, we describe the first steps towards a\ncoherent statistical analysis of these shapes by deriving the integrated\nlikelihood for data shapes given class parameters. The result is of interest\nbeyond this particular application.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 22:28:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Tsiftsi", "Thomai", ""], ["Jermyn", "Ian H.", ""], ["Einbeck", "Jochen", ""]]}, {"id": "1802.09646", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Yasin Abbasi-Yadkori, Mohammad Ghavamzadeh, Nikos\n  Vlassis", "title": "Optimizing over a Restricted Policy Class in Markov Decision Processes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding an optimal policy in a Markov decision\nprocess under a restricted policy class defined by the convex hull of a set of\nbase policies. This problem is of great interest in applications in which a\nnumber of reasonably good (or safe) policies are already known and we are only\ninterested in optimizing in their convex hull. We show that this problem is\nNP-hard to solve exactly as well as to approximate to arbitrary accuracy.\nHowever, under a condition that is akin to the occupancy measures of the base\npolicies having large overlap, we show that there exists an efficient algorithm\nthat finds a policy that is almost as good as the best convex combination of\nthe base policies. The running time of the proposed algorithm is linear in the\nnumber of states and polynomial in the number of base policies. In practice, we\ndemonstrate an efficient implementation for large state problems. Compared to\ntraditional policy gradient methods, the proposed approach has the advantage\nthat, apart from the computation of occupancy measures of some base policies,\nthe iterative method need not interact with the environment during the\noptimization process. This is especially important in complex systems where\nestimating the value of a policy can be a time consuming process.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 23:51:57 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Banijamali", "Ershad", ""], ["Abbasi-Yadkori", "Yasin", ""], ["Ghavamzadeh", "Mohammad", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1802.09650", "submitter": "Scott Sisson", "authors": "Y. Fan and S. A. Sisson", "title": "ABC Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"ABC Samplers\", is to appear in the forthcoming Handbook of\nApproximate Bayesian Computation (2018). It details the main ideas and\nalgorithms used to sample from the ABC approximation to the posterior\ndistribution, including methods based on rejection/importance sampling, MCMC\nand sequential Monte Carlo.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 23:57:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.09656", "submitter": "Ariel Jaffe", "authors": "Ariel Jaffe, Roi Weiss, Shai Carmi, Yuval Kluger and Boaz Nadler", "title": "Learning Binary Latent Variable Models: A Tensor Eigenpair Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models with hidden binary units appear in various\napplications. Learning such models, in particular in the presence of noise, is\na challenging computational problem. In this paper we propose a novel spectral\napproach to this problem, based on the eigenvectors of both the second order\nmoment matrix and third order moment tensor of the observed data. We prove that\nunder mild non-degeneracy conditions, our method consistently estimates the\nmodel parameters at the optimal parametric rate. Our tensor-based method\ngeneralizes previous orthogonal tensor decomposition approaches, where the\nhidden units were assumed to be either statistically independent or mutually\nexclusive. We illustrate the consistency of our method on simulated data and\ndemonstrate its usefulness in learning a common model for population mixtures\nin genetics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:11:01 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Jaffe", "Ariel", ""], ["Weiss", "Roi", ""], ["Carmi", "Shai", ""], ["Kluger", "Yuval", ""], ["Nadler", "Boaz", ""]]}, {"id": "1802.09691", "submitter": "Muhan Zhang", "authors": "Muhan Zhang, Yixin Chen", "title": "Link Prediction Based on Graph Neural Networks", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a key problem for network-structured data. Link prediction\nheuristics use some score functions, such as common neighbors and Katz index,\nto measure the likelihood of links. They have obtained wide practical uses due\nto their simplicity, interpretability, and for some of them, scalability.\nHowever, every heuristic has a strong assumption on when two nodes are likely\nto link, which limits their effectiveness on networks where these assumptions\nfail. In this regard, a more reasonable way should be learning a suitable\nheuristic from a given network instead of using predefined ones. By extracting\na local subgraph around each target link, we aim to learn a function mapping\nthe subgraph patterns to link existence, thus automatically learning a\n`heuristic' that suits the current network. In this paper, we study this\nheuristic learning paradigm for link prediction. First, we develop a novel\n$\\gamma$-decaying heuristic theory. The theory unifies a wide range of\nheuristics in a single framework, and proves that all these heuristics can be\nwell approximated from local subgraphs. Our results show that local subgraphs\nreserve rich information related to link existence. Second, based on the\n$\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from\nlocal subgraphs using a graph neural network (GNN). Its experimental results\nshow unprecedented performance, working consistently well on a wide range of\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 02:35:52 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 20:22:42 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 05:52:00 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Zhang", "Muhan", ""], ["Chen", "Yixin", ""]]}, {"id": "1802.09700", "submitter": "Zhi Xu", "authors": "Zhi Xu, Chengtao Li, Stefanie Jegelka", "title": "Robust GANs against Dishonest Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of deep learning models is a property that has recently gained\nincreasing attention. We explore a notion of robustness for generative\nadversarial models that is pertinent to their internal interactive structure,\nand show that, perhaps surprisingly, the GAN in its original form is not\nrobust. Our notion of robustness relies on a perturbed discriminator, or noisy,\nadversarial interference with its feedback. We explore, theoretically and\nempirically, the effect of model and training properties on this robustness. In\nparticular, we show theoretical conditions for robustness that are supported by\nempirical evidence. We also test the effect of regularization. Our results\nsuggest variations of GANs that are indeed more robust to noisy attacks and\nhave more stable training behavior, requiring less regularization in general.\nInspired by our theoretical results, we further extend our framework to obtain\na class of models related to WGAN, with good empirical performance. Overall,\nour results suggest a new perspective on understanding and designing GAN models\nfrom the viewpoint of their internal robustness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 03:21:44 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 19:40:43 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 03:40:46 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Xu", "Zhi", ""], ["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1802.09707", "submitter": "Lei Wu", "authors": "Lei Wu, Zhanxing Zhu, Cheng Tai, Weinan E", "title": "Understanding and Enhancing the Transferability of Adversarial Examples", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks are known to be vulnerable to\nadversarial examples, formed by applying small but malicious perturbations to\nthe original inputs. Moreover, the perturbations can \\textit{transfer across\nmodels}: adversarial examples generated for a specific model will often mislead\nother unseen models. Consequently the adversary can leverage it to attack\ndeployed systems without any query, which severely hinder the application of\ndeep learning, especially in the areas where security is crucial. In this work,\nwe systematically study how two classes of factors that might influence the\ntransferability of adversarial examples. One is about model-specific factors,\nincluding network architecture, model capacity and test accuracy. The other is\nthe local smoothness of loss function for constructing adversarial examples.\nBased on these understanding, a simple but effective strategy is proposed to\nenhance transferability. We call it variance-reduced attack, since it utilizes\nthe variance-reduced gradient to generate adversarial example. The\neffectiveness is confirmed by a variety of experiments on both CIFAR-10 and\nImageNet datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 03:46:38 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Wu", "Lei", ""], ["Zhu", "Zhanxing", ""], ["Tai", "Cheng", ""], ["E", "Weinan", ""]]}, {"id": "1802.09720", "submitter": "Scott Sisson", "authors": "S. A. Sisson and Y. Fan and M. A. Beaumont", "title": "Overview of Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"Overview of Approximate Bayesian Computation\", is to appear as\nthe first chapter in the forthcoming Handbook of Approximate Bayesian\nComputation (2018). It details the main ideas and concepts behind ABC methods\nwith many examples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:18:12 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Sisson", "S. A.", ""], ["Fan", "Y.", ""], ["Beaumont", "M. A.", ""]]}, {"id": "1802.09725", "submitter": "Scott Sisson", "authors": "D. J. Nott and V. M.-H. Ong and Y. Fan and S. A. Sisson", "title": "High-dimensional ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"High-dimensional ABC\", is to appear in the forthcoming\nHandbook of Approximate Bayesian Computation (2018). It details the main ideas\nand concepts behind extending ABC methods to higher dimensions, with supporting\nexamples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:47:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Nott", "D. J.", ""], ["Ong", "V. M. -H.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.09732", "submitter": "Niladri Chatterji", "authors": "Aldo Pacchiano, Niladri S. Chatterji, Peter L. Bartlett", "title": "Online learning with kernel losses", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the adversarial linear bandits framework,\nwhere the underlying losses are kernel functions (with an associated\nreproducing kernel Hilbert space) rather than linear functions. We study a\nversion of the exponential weights algorithm and bound its regret in this\nsetting. Under conditions on the eigendecay of the kernel we provide a sharp\ncharacterization of the regret for this algorithm. When we have polynomial\neigendecay $\\mu_j \\le \\mathcal{O}(j^{-\\beta})$, we find that the regret is\nbounded by $\\mathcal{R}_n \\le \\mathcal{O}(n^{\\beta/(2(\\beta-1))})$; while under\nthe assumption of exponential eigendecay $\\mu_j \\le \\mathcal{O}(e^{-\\beta j\n})$, we get an even tighter bound on the regret $\\mathcal{R}_n \\le\n\\mathcal{O}(n^{1/2}\\log(n)^{1/2})$. We also study the full information setting\nwhen the underlying losses are kernel functions and present an adapted\nexponential weights algorithm and a conditional gradient descent algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:07:54 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Pacchiano", "Aldo", ""], ["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1802.09736", "submitter": "Kumar Vijay Mishra", "authors": "Ahmet M. Elbir, Kumar Vijay Mishra and Yonina C. Eldar", "title": "Cognitive Radar Antenna Selection via Deep Learning", "comments": "10 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direction of arrival (DoA) estimation of targets improves with the number of\nelements employed by a phased array radar antenna. Since larger arrays have\nhigh associated cost, area and computational load, there is recent interest in\nthinning the antenna arrays without loss of far-field DoA accuracy. In this\ncontext, a cognitive radar may deploy a full array and then select an optimal\nsubarray to transmit and receive the signals in response to changes in the\ntarget environment. Prior works have used optimization and greedy search\nmethods to pick the best subarrays cognitively. In this paper, we leverage deep\nlearning to address the antenna selection problem. Specifically, we construct a\nconvolutional neural network (CNN) as a multi-class classification framework\nwhere each class designates a different subarray. The proposed network\ndetermines a new array every time data is received by the radar, thereby making\nantenna selection a cognitive operation. Our numerical experiments show that\n{the proposed CNN structure provides 22% better classification performance than\na Support Vector Machine and the resulting subarrays yield 72% more accurate\nDoA estimates than random array selections.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:23:13 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 14:56:23 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 16:03:22 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Elbir", "Ahmet M.", ""], ["Mishra", "Kumar Vijay", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1802.09750", "submitter": "Huishuai Zhang", "authors": "Huishuai Zhang, Wei Chen, Tie-Yan Liu", "title": "Train Feedfoward Neural Network with Layer-wise Adaptive Rate via\n  Approximating Back-matching Propagation", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) has achieved great success in training deep\nneural network, where the gradient is computed through back-propagation.\nHowever, the back-propagated values of different layers vary dramatically. This\ninconsistence of gradient magnitude across different layers renders\noptimization of deep neural network with a single learning rate problematic. We\nintroduce the back-matching propagation which computes the backward values on\nthe layer's parameter and the input by matching backward values on the layer's\noutput. This leads to solving a bunch of least-squares problems, which requires\nhigh computational cost. We then reduce the back-matching propagation with\napproximations and propose an algorithm that turns to be the regular SGD with a\nlayer-wise adaptive learning rate strategy. This allows an easy implementation\nof our algorithm in current machine learning frameworks equipped with\nauto-differentiation. We apply our algorithm in training modern deep neural\nnetworks and achieve favorable results over SGD.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 07:25:32 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1802.09756", "submitter": "Junqi Jin", "authors": "Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, Weinan Zhang", "title": "Real-Time Bidding with Multi-Agent Reinforcement Learning in Display\n  Advertising", "comments": null, "journal-ref": "CIKM 2018, Turin, Italy", "doi": "10.1145/3269206.3272021", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time advertising allows advertisers to bid for each impression for a\nvisiting user. To optimize specific goals such as maximizing revenue and return\non investment (ROI) led by ad placements, advertisers not only need to estimate\nthe relevance between the ads and user's interests, but most importantly\nrequire a strategic response with respect to other advertisers bidding in the\nmarket. In this paper, we formulate bidding optimization with multi-agent\nreinforcement learning. To deal with a large number of advertisers, we propose\na clustering method and assign each cluster with a strategic bidding agent. A\npractical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed\nand implemented to balance the tradeoff between the competition and cooperation\namong advertisers. The empirical study on our industry-scaled real-world data\nhas demonstrated the effectiveness of our methods. Our results show\ncluster-based bidding would largely outperform single-agent and bandit\napproaches, and the coordinated bidding achieves better overall objectives than\npurely self-interested bidding agents.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 07:52:35 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 13:53:10 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Jin", "Junqi", ""], ["Song", "Chengru", ""], ["Li", "Han", ""], ["Gai", "Kun", ""], ["Wang", "Jun", ""], ["Zhang", "Weinan", ""]]}, {"id": "1802.09777", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer and Anna Silnova and Lukas Burget and Themos Stafylakis", "title": "Gaussian meta-embeddings for efficient scoring of a heavy-tailed PLDA\n  model", "comments": "submittted to Odyssey 2018: The Speaker and Language Recognition\n  Workshop, Les Sables d'Olonne, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings in machine learning are low-dimensional representations of complex\ninput patterns, with the property that simple geometric operations like\nEuclidean distances and dot products can be used for classification and\ncomparison tasks. The proposed meta-embeddings are special embeddings that live\nin more general inner product spaces. They are designed to propagate\nuncertainty to the final output in speaker recognition and similar\napplications. The familiar Gaussian PLDA model (GPLDA) can be re-formulated as\nan extractor for Gaussian meta-embeddings (GMEs), such that likelihood ratio\nscores are given by Hilbert space inner products between Gaussian likelihood\nfunctions. GMEs extracted by the GPLDA model have fixed precisions and do not\npropagate uncertainty. We show that a generalization to heavy-tailed PLDA gives\nGMEs with variable precisions, which do propagate uncertainty. Experiments on\nNIST SRE 2010 and 2016 show that the proposed method applied to i-vectors\nwithout length normalization is up to 20% more accurate than GPLDA applied to\nlength-normalized ivectors.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 08:55:05 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Brummer", "Niko", ""], ["Silnova", "Anna", ""], ["Burget", "Lukas", ""], ["Stafylakis", "Themos", ""]]}, {"id": "1802.09791", "submitter": "Davide Bacciu", "authors": "Davide Bacciu, Paulo J.G. Lisboa, Jos\\'e D. Mart\\'in, Ruxandra Stoean,\n  Alfredo Vellido", "title": "Bioinformatics and Medicine in the Era of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current scientific advances in the life sciences have their\norigin in the intensive use of data for knowledge discovery. In no area this is\nso clear as in bioinformatics, led by technological breakthroughs in data\nacquisition technologies. It has been argued that bioinformatics could quickly\nbecome the field of research generating the largest data repositories, beating\nother data-intensive areas such as high-energy physics or astroinformatics.\nOver the last decade, deep learning has become a disruptive advance in machine\nlearning, giving new live to the long-standing connectionist paradigm in\nartificial intelligence. Deep learning methods are ideally suited to\nlarge-scale data and, therefore, they should be ideally suited to knowledge\ndiscovery in bioinformatics and biomedicine at large. In this brief paper, we\nreview key aspects of the application of deep learning in bioinformatics and\nmedicine, drawing from the themes covered by the contributions to an ESANN 2018\nspecial session devoted to this topic.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 09:41:44 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Bacciu", "Davide", ""], ["Lisboa", "Paulo J. G.", ""], ["Mart\u00edn", "Jos\u00e9 D.", ""], ["Stoean", "Ruxandra", ""], ["Vellido", "Alfredo", ""]]}, {"id": "1802.09794", "submitter": "Fredrik Bagge Carlson", "authors": "Fredrik Bagge Carlson, Anders Robertsson, Rolf Johansson", "title": "Identification of LTV Dynamical Models with Smooth or Discontinuous Time\n  Evolution by means of Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a connection between trend filtering and system identification\nwhich results in a family of new identification methods for linear,\ntime-varying (LTV) dynamical models based on convex optimization. We\ndemonstrate how the design of the cost function promotes a model with either a\ncontinuous change in dynamics over time, or causes discontinuous changes in\nmodel coefficients occurring at a finite (sparse) set of time instances. We\nfurther discuss the introduction of priors on the model parameters for\nsituations where excitation is insufficient for identification. The\nidentification problems are cast as convex optimization problems and are\napplicable to, e.g., ARX models and state-space models with time-varying\nparameters. We illustrate usage of the methods in simulations of jump-linear\nsystems, a nonlinear robot arm with non-smooth friction and stiff contacts as\nwell as in model-based, trajectory centric reinforcement learning on a smooth\nnonlinear system.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 09:49:59 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Carlson", "Fredrik Bagge", ""], ["Robertsson", "Anders", ""], ["Johansson", "Rolf", ""]]}, {"id": "1802.09802", "submitter": "Carlos Eduardo Rosar Kos Lassance", "authors": "Carlos Eduardo Rosar Kos Lassance, Jean-Charles Vialatte, Vincent\n  Gripon", "title": "Matching Convolutional Neural Networks without Priors about Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of Convolutional Neural Networks (CNNs) to\ngraph-structured data, including strided convolutions and data augmentation on\ngraphs.\n  Our method matches the accuracy of state-of-the-art CNNs when applied on\nimages, without any prior about their 2D regular structure.\n  On fMRI data, we obtain a significant gain in accuracy compared with existing\ngraph-based alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 10:01:55 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Lassance", "Carlos Eduardo Rosar Kos", ""], ["Vialatte", "Jean-Charles", ""], ["Gripon", "Vincent", ""]]}, {"id": "1802.09816", "submitter": "Guillaume Charpiat", "authors": "Armand Zampieri (TITANE), Guillaume Charpiat (TAU), Yuliya Tarabalka\n  (TITANE)", "title": "Coarse to fine non-rigid registration: a chain of scale-specific neural\n  networks for multimodal image alignment with application to remote sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle here the problem of multimodal image non-rigid registration, which\nis of prime importance in remote sensing and medical imaging. The difficulties\nencountered by classical registration approaches include feature design and\nslow optimization by gradient descent. By analyzing these methods, we note the\nsignificance of the notion of scale. We design easy-to-train,\nfully-convolutional neural networks able to learn scale-specific features. Once\nchained appropriately, they perform global registration in linear time, getting\nrid of gradient descent schemes by predicting directly the deformation.We show\ntheir performance in terms of quality and speed through various tasks of remote\nsensing multimodal image alignment. In particular, we are able to register\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\nimages, and outperform current keypoint matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 10:47:06 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zampieri", "Armand", "", "TITANE"], ["Charpiat", "Guillaume", "", "TAU"], ["Tarabalka", "Yuliya", "", "TITANE"]]}, {"id": "1802.09841", "submitter": "M\\'elanie Ducoffe", "authors": "Melanie Ducoffe, Frederic Precioso", "title": "Adversarial Active Learning for Deep Networks: a Margin Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning strategy designed for deep neural networks.\nThe goal is to minimize the number of data annotation queried from an oracle\nduring training. Previous active learning strategies scalable for deep networks\nwere mostly based on uncertain sample selection. In this work, we focus on\nexamples lying close to the decision boundary. Based on theoretical works on\nmargin theory for active learning, we know that such examples may help to\nconsiderably decrease the number of annotations. While measuring the exact\ndistance to the decision boundaries is intractable, we propose to rely on\nadversarial examples. We do not consider anymore them as a threat instead we\nexploit the information they provide on the distribution of the input space in\norder to approximate the distance to decision boundaries. We demonstrate\nempirically that adversarial active queries yield faster convergence of CNNs\ntrained on MNIST, the Shoe-Bag and the Quick-Draw datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 12:02:33 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Ducoffe", "Melanie", ""], ["Precioso", "Frederic", ""]]}, {"id": "1802.09901", "submitter": "Quentin Debard", "authors": "Quentin Debard, Christian Wolf, St\\'ephane Canu and Julien Arn\\'e", "title": "Learning to recognize touch gestures: recurrent vs. convolutional\n  features and dynamic sampling", "comments": "9 pages, 4 figures, accepted at the 13th IEEE Conference on Automatic\n  Face and Gesture Recognition (FG2018). Dataset available at\n  http://itekube7.itekube.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fully automatic method for learning gestures on big touch\ndevices in a potentially multi-user context. The goal is to learn general\nmodels capable of adapting to different gestures, user styles and hardware\nvariations (e.g. device sizes, sampling frequencies and regularities).\n  Based on deep neural networks, our method features a novel dynamic sampling\nand temporal normalization component, transforming variable length gestures\ninto fixed length representations while preserving finger/surface contact\ntransitions, that is, the topology of the signal. This sequential\nrepresentation is then processed with a convolutional model capable, unlike\nrecurrent networks, of learning hierarchical representations with different\nlevels of abstraction.\n  To demonstrate the interest of the proposed method, we introduce a new touch\ngestures dataset with 6591 gestures performed by 27 people, which is, up to our\nknowledge, the first of its kind: a publicly available multi-touch gesture\ndataset for interaction.\n  We also tested our method on a standard dataset of symbolic touch gesture\nrecognition, the MMG dataset, outperforming the state of the art and reporting\nclose to perfect performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 10:24:48 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Debard", "Quentin", ""], ["Wolf", "Christian", ""], ["Canu", "St\u00e9phane", ""], ["Arn\u00e9", "Julien", ""]]}, {"id": "1802.09902", "submitter": "Amirsina Torfi", "authors": "Amirsina Torfi, Rouzbeh A. Shirvani, Sobhan Soleymani, Nasser M.\n  Nasrabadi", "title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is aimed at imposing sparsity in a neural network\narchitecture by increasing the portion of zero-valued weights for reducing its\nsize regarding energy-efficiency consideration and increasing evaluation speed.\nIn most of the conducted research efforts, the sparsity is enforced for network\npruning without any attention to the internal network characteristics such as\nunbalanced outputs of the neurons or more specifically the distribution of the\nweights and outputs of the neurons. That may cause severe accuracy drop due to\nuncontrolled sparsity. In this work, we propose an attention mechanism that\nsimultaneously controls the sparsity intensity and supervised network pruning\nby keeping important information bottlenecks of the network to be active. On\nCIFAR-10, the proposed method outperforms the best baseline method by 6% and\nreduced the accuracy drop by 2.6x at the same level of sparsity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 04:24:49 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 18:24:53 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 17:58:27 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 17:47:49 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Torfi", "Amirsina", ""], ["Shirvani", "Rouzbeh A.", ""], ["Soleymani", "Sobhan", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1802.09913", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Sebastian Ruder, Anders S{\\o}gaard", "title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over\n  Disparate Label Spaces", "comments": "To appear at NAACL 2018 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine multi-task learning and semi-supervised learning by inducing a\njoint embedding space between disparate label spaces and learning transfer\nfunctions between label embeddings, enabling us to jointly leverage unlabelled\ndata and auxiliary, annotated datasets. We evaluate our approach on a variety\nof sequence classification tasks with disparate label spaces. We outperform\nstrong single and multi-task baselines and achieve a new state-of-the-art for\ntopic-based sentiment analysis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 14:38:43 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 07:34:47 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Ruder", "Sebastian", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1802.09914", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "High-Dimensional Vector Semantics", "comments": "12 pages, 5 figures, Int. J. Mod. Phys. C, 2018", "journal-ref": null, "doi": "10.1142/S0129183118500158", "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the \"vector semantics\" problem from the perspective\nof \"almost orthogonal\" property of high-dimensional random vectors. We show\nthat this intriguing property can be used to \"memorize\" random vectors by\nsimply adding them, and we provide an efficient probabilistic solution to the\nset membership problem. Also, we discuss several applications to word context\nvector embeddings, document sentences similarity, and spam filtering.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:50:16 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1802.09932", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Kaiwen Zhou, Hongying Liu, James Cheng, Ivor W. Tsang,\n  Lijun Zhang, Dacheng Tao, Licheng Jiao", "title": "VR-SGD: A Simple Stochastic Variance Reduction Method for Machine\n  Learning", "comments": "46 pages, 25 figures. IEEE Transactions on Knowledge and Data\n  Engineering, accepted in October, 2018. arXiv admin note: substantial text\n  overlap with arXiv:1704.04966", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple variant of the original SVRG, called\nvariance reduced stochastic gradient descent (VR-SGD). Unlike the choices of\nsnapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the\ntwo vectors of VR-SGD are set to the average and last iterate of the previous\nepoch, respectively. The settings allow us to use much larger learning rates,\nand also make our convergence analysis more challenging. We also design two\ndifferent update rules for smooth and non-smooth objective functions,\nrespectively, which means that VR-SGD can tackle non-smooth and/or non-strongly\nconvex problems directly without any reduction techniques. Moreover, we analyze\nthe convergence properties of VR-SGD for strongly convex problems, which show\nthat VR-SGD attains linear convergence. Different from its counterparts that\nhave no convergence guarantees for non-strongly convex problems, we also\nprovide the convergence guarantees of VR-SGD for this case, and empirically\nverify that VR-SGD with varying learning rates achieves similar performance to\nits momentum accelerated variant that has the optimal convergence rate\n$\\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine\nlearning problems, such as convex and non-convex empirical risk minimization,\nand leading eigenvalue computation. Experimental results show that VR-SGD\nconverges significantly faster than SVRG and Prox-SVRG, and usually outperforms\nstate-of-the-art accelerated methods, e.g., Katyusha.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 12:56:07 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 15:03:47 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Shang", "Fanhua", ""], ["Zhou", "Kaiwen", ""], ["Liu", "Hongying", ""], ["Cheng", "James", ""], ["Tsang", "Ivor W.", ""], ["Zhang", "Lijun", ""], ["Tao", "Dacheng", ""], ["Jiao", "Licheng", ""]]}, {"id": "1802.09933", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, Kaiwen Zhou, James Cheng, Kelvin K.W. Ng,\n  Yuichi Yoshida", "title": "Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient\n  Optimization", "comments": "24 pages, 10 figures, AISTATS 2018. arXiv admin note: text overlap\n  with arXiv:1703.06807", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sufficient decrease technique for\nstochastic variance reduced gradient descent methods such as SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nstochastic variance reduction algorithms such as SVRG-SD and SAGA-SD as a\nbyproduct. We introduce a coefficient to scale current iterate and to satisfy\nthe sufficient decrease property, which takes the decisions to shrink, expand\nor even move in the opposite direction, and then give two specific update rules\nof the coefficient for Lasso and ridge regression. Moreover, we analyze the\nconvergence properties of our algorithms for strongly convex problems, which\nshow that our algorithms attain linear convergence rates. We also provide the\nconvergence guarantees of our algorithms for non-strongly convex problems. Our\nexperimental results further verify that our algorithms achieve significantly\nbetter performance than their counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 03:04:50 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Zhou", "Kaiwen", ""], ["Cheng", "James", ""], ["Ng", "Kelvin K. W.", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1802.09963", "submitter": "Cheng Mao", "authors": "Cheng Mao, Ashwin Pananjady, Martin J. Wainwright", "title": "Breaking the $1/\\sqrt{n}$ Barrier: Faster Rates for Permutation-based\n  Models in Polynomial Time", "comments": "30 pages, 1 figure. Accepted for presentation at Conference on\n  Learning Theory (COLT) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications, including rank aggregation and crowd-labeling, can be\nmodeled in terms of a bivariate isotonic matrix with unknown permutations\nacting on its rows and columns. We consider the problem of estimating such a\nmatrix based on noisy observations of a subset of its entries, and design and\nanalyze a polynomial-time algorithm that improves upon the state of the art. In\nparticular, our results imply that any such $n \\times n$ matrix can be\nestimated efficiently in the normalized Frobenius norm at rate\n$\\widetilde{\\mathcal O}(n^{-3/4})$, thus narrowing the gap between\n$\\widetilde{\\mathcal O}(n^{-1})$ and $\\widetilde{\\mathcal O}(n^{-1/2})$, which\nwere hitherto the rates of the most statistically and computationally efficient\nmethods, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:26:15 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 04:19:26 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 15:52:59 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Mao", "Cheng", ""], ["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1802.09979", "submitter": "Jeffrey Pennington", "authors": "Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli", "title": "The Emergence of Spectral Universality in Deep Networks", "comments": "17 pages, 4 figures. Appearing at the 21st International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that tight concentration of the entire spectrum of\nsingular values of a deep network's input-output Jacobian around one at\ninitialization can speed up learning by orders of magnitude. Therefore, to\nguide important design choices, it is important to build a full theoretical\nunderstanding of the spectra of Jacobians at initialization. To this end, we\nleverage powerful tools from free probability theory to provide a detailed\nanalytic understanding of how a deep network's Jacobian spectrum depends on\nvarious hyperparameters including the nonlinearity, the weight and bias\ndistributions, and the depth. For a variety of nonlinearities, our work reveals\nthe emergence of new universal limiting spectral distributions that remain\nconcentrated around one even as the depth goes to infinity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:54:57 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Pennington", "Jeffrey", ""], ["Schoenholz", "Samuel S.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1802.10026", "submitter": "Andrew Wilson", "authors": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov,\n  Andrew Gordon Wilson", "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs", "comments": "Appears at Advances in Neural Information Processing Systems (NIPS),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss functions of deep neural networks are complex and their geometric\nproperties are not well understood. We show that the optima of these complex\nloss functions are in fact connected by simple curves over which training and\ntest accuracy are nearly constant. We introduce a training procedure to\ndiscover these high-accuracy pathways between modes. Inspired by this new\ngeometric insight, we also propose a new ensembling method entitled Fast\nGeometric Ensembling (FGE). Using FGE we can train high-performing ensembles in\nthe time required to train a single model. We achieve improved performance\ncompared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,\nCIFAR-100, and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:13:28 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 13:53:29 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 00:16:34 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 11:39:49 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Garipov", "Timur", ""], ["Izmailov", "Pavel", ""], ["Podoprikhin", "Dmitrii", ""], ["Vetrov", "Dmitry", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1802.10031", "submitter": "George Tucker", "authors": "George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E. Turner,\n  Zoubin Ghahramani, Sergey Levine", "title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "comments": "Updated to ICML final submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods are a widely used class of model-free reinforcement\nlearning algorithms where a state-dependent baseline is used to reduce gradient\nestimator variance. Several recent papers extend the baseline to depend on both\nthe state and action and suggest that this significantly reduces variance and\nimproves sample efficiency without introducing bias into the gradient\nestimates. To better understand this development, we decompose the variance of\nthe policy gradient estimator and numerically show that learned\nstate-action-dependent baselines do not in fact reduce variance over a\nstate-dependent baseline in commonly tested benchmark domains. We confirm this\nunexpected result by reviewing the open-source code accompanying these prior\npapers, and show that subtle implementation decisions cause deviations from the\nmethods presented in the papers and explain the source of the previously\nobserved empirical gains. Furthermore, the variance decomposition highlights\nareas for improvement, which we demonstrate by illustrating a simple change to\nthe typical value function parameterization that can significantly improve\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:16:48 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 17:51:12 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 18:57:02 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Tucker", "George", ""], ["Bhupatiraju", "Surya", ""], ["Gu", "Shixiang", ""], ["Turner", "Richard E.", ""], ["Ghahramani", "Zoubin", ""], ["Levine", "Sergey", ""]]}, {"id": "1802.10116", "submitter": "Cong Xie", "authors": "Cong Xie, Oluwasanmi Koyejo, Indranil Gupta", "title": "Generalized Byzantine-tolerant SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three new robust aggregation rules for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience properties of these aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 19:06:15 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 21:20:17 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 20:08:04 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1802.10168", "submitter": "Hamza Anwar", "authors": "Hamza Anwar and Quanyan Zhu", "title": "ADMM-based Networked Stochastic Variational Inference", "comments": "to be submitted for publishing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the recent advances in \"Big Data\" modeling and prediction tasks,\nvariational Bayesian estimation has gained popularity due to their ability to\nprovide exact solutions to approximate posteriors. One key technique for\napproximate inference is stochastic variational inference (SVI). SVI poses\nvariational inference as a stochastic optimization problem and solves it\niteratively using noisy gradient estimates. It aims to handle massive data for\npredictive and classification tasks by applying complex Bayesian models that\nhave observed as well as latent variables. This paper aims to decentralize it\nallowing parallel computation, secure learning and robustness benefits. We use\nAlternating Direction Method of Multipliers in a top-down setting to develop a\ndistributed SVI algorithm such that independent learners running inference\nalgorithms only require sharing the estimated model parameters instead of their\nprivate datasets. Our work extends the distributed SVI-ADMM algorithm that we\nfirst propose, to an ADMM-based networked SVI algorithm in which not only are\nthe learners working distributively but they share information according to\nrules of a graph by which they form a network. This kind of work lies under the\numbrella of `deep learning over networks' and we verify our algorithm for a\ntopic-modeling problem for corpus of Wikipedia articles. We illustrate the\nresults on latent Dirichlet allocation (LDA) topic model in large document\nclassification, compare performance with the centralized algorithm, and use\nnumerical experiments to corroborate the analytical results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 21:11:56 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Anwar", "Hamza", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1802.10172", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Herve Glotin, Richard Baraniuk", "title": "Semi-Supervised Learning Enabled by Multiscale Deep Neural Network\n  Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) provide state-of-the-art solutions in several\ndifficult machine perceptual tasks. However, their performance relies on the\navailability of a large set of labeled training data, which limits the breadth\nof their applicability. Hence, there is a need for new {\\em semi-supervised\nlearning} methods for DNNs that can leverage both (a small amount of) labeled\nand unlabeled training data. In this paper, we develop a general loss function\nenabling DNNs of any topology to be trained in a semi-supervised manner without\nextra hyper-parameters. As opposed to current semi-supervised techniques based\non topology-specific or unstable approaches, ours is both robust and general.\nWe demonstrate that our approach reaches state-of-the-art performance on the\nSVHN ($9.82\\%$ test error, with $500$ labels and wide Resnet) and CIFAR10\n(16.38% test error, with 8000 labels and sigmoid convolutional neural network)\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 21:27:01 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Balestriero", "Randall", ""], ["Glotin", "Herve", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1802.10238", "submitter": "Benjamin Shickel", "authors": "Benjamin Shickel, Tyler J. Loftus, Lasith Adhikari, Tezcan\n  Ozrazgat-Baslanti, Azra Bihorac, and Parisa Rashidi", "title": "DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using\n  Clinically Interpretable Deep Learning", "comments": null, "journal-ref": "Scientific Reports (2019) 9:1879", "doi": "10.1038/s41598-019-38491-0", "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for assessing illness severity and predicting in-hospital\nmortality among critically ill patients require time-consuming, error-prone\ncalculations using static variable thresholds. These methods do not capitalize\non the emerging availability of streaming electronic health record data or\ncapture time-sensitive individual physiological patterns, a critical task in\nthe intensive care unit. We propose a novel acuity score framework (DeepSOFA)\nthat leverages temporal measurements and interpretable deep learning models to\nassess illness severity at any point during an ICU stay. We compare DeepSOFA\nwith SOFA (Sequential Organ Failure Assessment) baseline models using the same\nmodel inputs and find that at any point during an ICU admission, DeepSOFA\nyields significantly more accurate predictions of in-hospital mortality. A\nDeepSOFA model developed in a public database and validated in a single\ninstitutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI\n0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI\n0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify\nICU patients in need of life-saving interventions prior to the occurrence of an\nunexpected adverse event and inform shared decision-making processes among\npatients, providers, and families regarding goals of care and optimal resource\nutilization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 02:39:02 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 02:04:16 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 01:33:57 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 18:16:07 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Shickel", "Benjamin", ""], ["Loftus", "Tyler J.", ""], ["Adhikari", "Lasith", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1802.10254", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Yoshiyuki Kabashima", "title": "Semi-Analytic Resampling in Lasso", "comments": "33 pages, 10 figures, MATLAB codes implementing the proposed method\n  are distributed in https://github.com/T-Obuchi/AMPR_lasso_matlab", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate method for conducting resampling in Lasso, the $\\ell_1$\npenalized linear regression, in a semi-analytic manner is developed, whereby\nthe average over the resampled datasets is directly computed without repeated\nnumerical sampling, thus enabling an inference free of the statistical\nfluctuations due to sampling finiteness, as well as a significant reduction of\ncomputational time. The proposed method is based on a message passing type\nalgorithm, and its fast convergence is guaranteed by the state evolution\nanalysis, when covariates are provided as zero-mean independently and\nidentically distributed Gaussian random variables. It is employed to implement\nbootstrapped Lasso (Bolasso) and stability selection, both of which are\nvariable selection methods using resampling in conjunction with Lasso, and\nresolves their disadvantage regarding computational cost. To examine\napproximation accuracy and efficiency, numerical experiments were carried out\nusing simulated datasets. Moreover, an application to a real-world dataset, the\nwine quality dataset, is presented. To process such real-world datasets, an\nobjective criterion for determining the relevance of selected variables is also\nintroduced by the addition of noise variables and resampling.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:49:34 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 06:46:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1802.10264", "submitter": "Eric Jang", "authors": "Deirdre Quillen, Eric Jang, Ofir Nachum, Chelsea Finn, Julian Ibarz,\n  Sergey Levine", "title": "Deep Reinforcement Learning for Vision-Based Robotic Grasping: A\n  Simulated Comparative Evaluation of Off-Policy Methods", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore deep reinforcement learning algorithms for\nvision-based robotic grasping. Model-free deep reinforcement learning (RL) has\nbeen successfully applied to a range of challenging environments, but the\nproliferation of algorithms makes it difficult to discern which particular\napproach would be best suited for a rich, diverse task like grasping. To answer\nthis question, we propose a simulated benchmark for robotic grasping that\nemphasizes off-policy learning and generalization to unseen objects. Off-policy\nlearning enables utilization of grasping data over a wide variety of objects,\nand diversity is important to enable the method to generalize to new objects\nthat were not seen during training. We evaluate the benchmark tasks against a\nvariety of Q-function estimation methods, a method previously proposed for\nrobotic grasping with deep neural network models, and a novel approach based on\na combination of Monte Carlo return estimation and an off-policy correction.\nOur results indicate that several simple methods provide a surprisingly strong\ncompetitor to popular algorithms such as double Q-learning, and our analysis of\nstability sheds light on the relative tradeoffs between the algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 05:11:38 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 23:28:14 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Quillen", "Deirdre", ""], ["Jang", "Eric", ""], ["Nachum", "Ofir", ""], ["Finn", "Chelsea", ""], ["Ibarz", "Julian", ""], ["Levine", "Sergey", ""]]}, {"id": "1802.10275", "submitter": "Yuehaw Khoo", "authors": "Yuehaw Khoo, Jianfeng Lu, Lexing Ying", "title": "Solving for high dimensional committor functions using artificial neural\n  networks", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we propose a method based on artificial neural network to study\nthe transition between states governed by stochastic processes. In particular,\nwe aim for numerical schemes for the committor function, the central object of\ntransition path theory, which satisfies a high-dimensional Fokker-Planck\nequation. By working with the variational formulation of such partial\ndifferential equation and parameterizing the committor function in terms of a\nneural network, approximations can be obtained via optimizing the neural\nnetwork weights using stochastic algorithms. The numerical examples show that\nmoderate accuracy can be achieved for high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 06:16:33 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Khoo", "Yuehaw", ""], ["Lu", "Jianfeng", ""], ["Ying", "Lexing", ""]]}, {"id": "1802.10311", "submitter": "Maksym Byshkin", "authors": "Maksym Byshkin, Alex Stivala, Antonietta Mira, Garry Robins and\n  Alessandro Lomi", "title": "Fast Maximum Likelihood estimation via Equilibrium Expectation for Large\n  Network Data", "comments": "Final version", "journal-ref": "Scientific Reports | (2018) 8:11509\n  https://www.nature.com/articles/s41598-018-29725-8", "doi": "10.1038/s41598-018-29725-8", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major line of contemporary research on complex networks is based on the\ndevelopment of statistical models that specify the local motifs associated with\nmacro-structural properties observed in actual networks. This statistical\napproach becomes increasingly problematic as network size increases. In the\ncontext of current research on efficient estimation of models for large network\ndata sets, we propose a fast algorithm for maximum likelihood estimation (MLE)\nthat afords a signifcant increase in the size of networks amenable to direct\nempirical analysis. The algorithm we propose in this paper relies on properties\nof Markov chains at equilibrium, and for this reason it is called equilibrium\nexpectation (EE). We demonstrate the performance of the EE algorithm in the\ncontext of exponential random graphmodels (ERGMs) a family of statistical\nmodels commonly used in empirical research based on network data observed at a\nsingle period in time. Thus far, the lack of efcient computational strategies\nhas limited the empirical scope of ERGMs to relatively small networks with a\nfew thousand nodes. The approach we propose allows a dramatic increase in the\nsize of networks that may be analyzed using ERGMs. This is illustrated in an\nanalysis of several biological networks and one social network with 104,103\nnodes\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:02:26 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:03:41 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Byshkin", "Maksym", ""], ["Stivala", "Alex", ""], ["Mira", "Antonietta", ""], ["Robins", "Garry", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1802.10418", "submitter": "Songtao Lu", "authors": "Songtao Lu and Mingyi Hong and Zhengdao Wang", "title": "On the Sublinear Convergence of Randomly Perturbed Alternating Gradient\n  Descent to Second Order Stationary Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating gradient descent (AGD) is a simple but popular algorithm\nwhich has been applied to problems in optimization, machine learning, data\nming, and signal processing, etc. The algorithm updates two blocks of variables\nin an alternating manner, in which a gradient step is taken on one block, while\nkeeping the remaining block fixed. When the objective function is nonconvex, it\nis well-known the AGD converges to the first-order stationary solution with a\nglobal sublinear rate.\n  In this paper, we show that a variant of AGD-type algorithms will not be\ntrapped by \"bad\" stationary solutions such as saddle points and local maximum\npoints. In particular, we consider a smooth unconstrained optimization problem,\nand propose a perturbed AGD (PA-GD) which converges (with high probability) to\nthe set of second-order stationary solutions (SS2) with a global sublinear\nrate. To the best of our knowledge, this is the first alternating type\nalgorithm which takes $\\mathcal{O}(\\text{polylog}(d)/\\epsilon^{7/3})$\niterations to achieve SS2 with high probability [where polylog$(d)$ is\npolynomial of the logarithm of dimension $d$ of the problem].\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 14:04:35 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Lu", "Songtao", ""], ["Hong", "Mingyi", ""], ["Wang", "Zhengdao", ""]]}, {"id": "1802.10463", "submitter": "Phaniteja S", "authors": "Parijat Dewangan, S Phaniteja, K Madhava Krishna, Abhishek Sarkar,\n  Balaraman Ravindran", "title": "DiGrad: Multi-Task Reinforcement Learning with Shared Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most reinforcement learning algorithms are inefficient for learning multiple\ntasks in complex robotic systems, where different tasks share a set of actions.\nIn such environments a compound policy may be learnt with shared neural network\nparameters, which performs multiple tasks concurrently. However such compound\npolicy may get biased towards a task or the gradients from different tasks\nnegate each other, making the learning unstable and sometimes less data\nefficient. In this paper, we propose a new approach for simultaneous training\nof multiple tasks sharing a set of common actions in continuous action spaces,\nwhich we call as DiGrad (Differential Policy Gradient). The proposed framework\nis based on differential policy gradients and can accommodate multi-task\nlearning in a single actor-critic network. We also propose a simple heuristic\nin the differential policy gradient update to further improve the learning. The\nproposed architecture was tested on 8 link planar manipulator and 27 degrees of\nfreedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2\nend effectors respectively. We show that our approach supports efficient\nmulti-task learning in complex robotic systems, outperforming related methods\nin continuous action spaces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 10:26:08 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Dewangan", "Parijat", ""], ["Phaniteja", "S", ""], ["Krishna", "K Madhava", ""], ["Sarkar", "Abhishek", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1802.10489", "submitter": "Andrew Massimino", "authors": "Andrew K. Massimino and Mark A. Davenport", "title": "As you like it: Localization via paired comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we wish to estimate a vector $\\mathbf{x}$ from a set of binary\npaired comparisons of the form \"$\\mathbf{x}$ is closer to $\\mathbf{p}$ than to\n$\\mathbf{q}$\" for various choices of vectors $\\mathbf{p}$ and $\\mathbf{q}$. The\nproblem of estimating $\\mathbf{x}$ from this type of observation arises in a\nvariety of contexts, including nonmetric multidimensional scaling, \"unfolding,\"\nand ranking problems, often because it provides a powerful and flexible model\nof preference. We describe theoretical bounds for how well we can expect to\nestimate $\\mathbf{x}$ under a randomized model for $\\mathbf{p}$ and\n$\\mathbf{q}$. We also present results for the case where the comparisons are\nnoisy and subject to some degree of error. Additionally, we show that under a\nrandomized model for $\\mathbf{p}$ and $\\mathbf{q}$, a suitable number of binary\npaired comparisons yield a stable embedding of the space of target vectors.\nFinally, we also that we can achieve significant gains by adaptively changing\nthe distribution for choosing $\\mathbf{p}$ and $\\mathbf{q}$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 18:53:00 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Massimino", "Andrew K.", ""], ["Davenport", "Mark A.", ""]]}, {"id": "1802.10497", "submitter": "Elif Vural", "authors": "Jeremy Aghaei Mazaheri, Elif Vural, Claude Labit and Christine\n  Guillemot", "title": "Learning Discriminative Multilevel Structured Dictionaries for\n  Supervised Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations using overcomplete dictionaries have proved to be a\npowerful tool in many signal processing applications such as denoising,\nsuper-resolution, inpainting, compression or classification. The sparsity of\nthe representation very much depends on how well the dictionary is adapted to\nthe data at hand. In this paper, we propose a method for learning structured\nmultilevel dictionaries with discriminative constraints to make them well\nsuited for the supervised pixelwise classification of images. A multilevel\ntree-structured discriminative dictionary is learnt for each class, with a\nlearning objective concerning the reconstruction errors of the image patches\naround the pixels over each class-representative dictionary. After the initial\nassignment of the class labels to image pixels based on their sparse\nrepresentations over the learnt dictionaries, the final classification is\nachieved by smoothing the label image with a graph cut method and an erosion\nmethod. Applied to a common set of texture images, our supervised\nclassification method shows competitive results with the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:03:45 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Mazaheri", "Jeremy Aghaei", ""], ["Vural", "Elif", ""], ["Labit", "Claude", ""], ["Guillemot", "Christine", ""]]}, {"id": "1802.10501", "submitter": "Andrey Malinin", "authors": "Andrey Malinin and Mark Gales", "title": "Predictive Uncertainty Estimation via Prior Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating how uncertain an AI system is in its predictions is important to\nimprove the safety of such systems. Uncertainty in predictive can result from\nuncertainty in model parameters, irreducible data uncertainty and uncertainty\ndue to distributional mismatch between the test and training data\ndistributions. Different actions might be taken depending on the source of the\nuncertainty so it is important to be able to distinguish between them.\nRecently, baseline tasks and metrics have been defined and several practical\nmethods to estimate uncertainty developed. These methods, however, attempt to\nmodel uncertainty due to distributional mismatch either implicitly through\nmodel uncertainty or as data uncertainty. This work proposes a new framework\nfor modeling predictive uncertainty called Prior Networks (PNs) which\nexplicitly models distributional uncertainty. PNs do this by parameterizing a\nprior distribution over predictive distributions. This work focuses on\nuncertainty for classification and evaluates PNs on the tasks of identifying\nout-of-distribution (OOD) samples and detecting misclassification on the MNIST\ndataset, where they are found to outperform previous methods. Experiments on\nsynthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian\nmethods PNs are able to distinguish between data and distributional\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:06:19 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 19:44:18 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 10:53:15 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 23:42:18 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Malinin", "Andrey", ""], ["Gales", "Mark", ""]]}, {"id": "1802.10510", "submitter": "Mohammad Sultan", "authors": "Mohammad M. Sultan, Vijay S. Pande", "title": "Automated design of collective variables using supervised machine\n  learning", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selection of appropriate collective variables for enhancing sampling of\nmolecular simulations remains an unsolved problem in computational biophysics.\nIn particular, picking initial collective variables (CVs) is particularly\nchallenging in higher dimensions. Which atomic coordinates or transforms there\nof from a list of thousands should one pick for enhanced sampling runs? How\ndoes a modeler even begin to pick starting coordinates for investigation? This\nremains true even in the case of simple two state systems and only increases in\ndifficulty for multi-state systems. In this work, we solve the initial CV\nproblem using a data-driven approach inspired by the filed of supervised\nmachine learning. In particular, we show how the decision functions in\nsupervised machine learning (SML) algorithms can be used as initial CVs\n(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and\nChignolin mini-protein as our test cases, we illustrate how the distance to the\nSupport Vector Machines' decision hyperplane, the output probability estimates\nfrom Logistic Regression, the outputs from deep neural network classifiers, and\nother classifiers may be used to reversibly sample slow structural transitions.\nWe discuss the utility of other SML algorithms that might be useful for\nidentifying CVs for accelerating molecular simulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:22:14 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 21:52:50 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sultan", "Mohammad M.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1802.10515", "submitter": "Trisha Lawrence Ms.", "authors": "Trisha Lawrence", "title": "Stochastic Dynamic Programming Heuristics for Influence\n  Maximization-Revenue Optimization", "comments": null, "journal-ref": null, "doi": "10.1007/s41060-018-0155-5", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The well-known Influence Maximization (IM) problem has been actively studied\nby researchers over the past decade, with emphasis on marketing and social\nnetworks. Existing research have obtained solutions to the IM problem by\nobtaining the influence spread and utilizing the property of submodularity.\nThis paper is based on a novel approach to the IM problem geared towards\noptimizing clicks and consequently revenue within anOnline Social Network\n(OSN). Our approach diverts from existing approaches by adopting a novel,\ndecision-making perspective through implementing Stochastic Dynamic Programming\n(SDP). Thus, we define a new problem Influence Maximization-Revenue\nOptimization (IM-RO) and propose SDP as a method in which this problem can be\nsolved. The SDP method has lucrative gains for an advertiser in terms of\noptimizing clicks and generating revenue however, one drawback to the method is\nits associated \"curse of dimensionality\" particularly for problems involving a\nlarge state space. Thus, we introduce the Lawrence Degree Heuristic (LDH),\nAdaptive Hill-Climbing (AHC) and Multistage Particle Swarm Optimization (MPSO)\nheuristics as methods which are orders of magnitude faster than the SDP method\nwhilst achieving near-optimal results. Through a comparative analysis on\nvarious synthetic and real-world networks we present the AHC and LDH as\nheuristics well suited to to the IM-RO problem in terms of their accuracy,\nrunning times and scalability under ideal model parameters. In this paper we\npresent a compelling survey on the SDP method as a practical and lucrative\nmethod for spreading information and optimizing revenue within the context of\nOSNs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:26:06 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 17:19:24 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lawrence", "Trisha", ""]]}, {"id": "1802.10526", "submitter": "Sergei Koltcov", "authors": "Koltcov Sergei", "title": "Application of R\\'enyi and Tsallis Entropies to Topic Modeling\n  Optimization", "comments": "no comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is full length article (draft version) where problem number of topics in\nTopic Modeling is discussed. We proposed idea that Renyi and Tsallis entropy\ncan be used for identification of optimal number in large textual collections.\nWe also report results of numerical experiments of Semantic stability for 4\ntopic models, which shows that semantic stability play very important role in\nproblem topic number. The calculation of Renyi and Tsallis entropy based on\nthermodynamics approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:41:14 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Sergei", "Koltcov", ""]]}, {"id": "1802.10529", "submitter": "Maurits Kaptein", "authors": "Maurits Kaptein and Paul Ketelaar", "title": "Maximum likelihood estimation of a finite mixture of logistic regression\n  models in a continuous data stream", "comments": "1 figure. Working paper including [R] package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In marketing we are often confronted with a continuous stream of responses to\nmarketing messages. Such streaming data provide invaluable information\nregarding message effectiveness and segmentation. However, streaming data are\nhard to analyze using conventional methods: their high volume and the fact that\nthey are continuously augmented means that it takes considerable time to\nanalyze them. We propose a method for estimating a finite mixture of logistic\nregression models which can be used to cluster customers based on a continuous\nstream of responses. This method, which we coin oFMLR, allows segments to be\nidentified in data streams or extremely large static datasets. Contrary to\nblack box algorithms, oFMLR provides model estimates that are directly\ninterpretable. We first introduce oFMLR, explaining in passing general topics\nsuch as online estimation and the EM algorithm, making this paper a high level\noverview of possible methods of dealing with large data streams in marketing\npractice. Next, we discuss model convergence, identifiability, and relations to\nalternative, Bayesian, methods; we also identify more general issues that arise\nfrom dealing with continuously augmented data sets. Finally, we introduce the\noFMLR [R] package and evaluate the method by numerical simulation and by\nanalyzing a large customer clickstream dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:43:16 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kaptein", "Maurits", ""], ["Ketelaar", "Paul", ""]]}, {"id": "1802.10542", "submitter": "Siddhant M. Jayakumar", "authors": "Pablo Sprechmann, Siddhant M. Jayakumar, Jack W. Rae, Alexander\n  Pritzel, Adri\\`a Puigdom\\`enech Badia, Benigno Uria, Oriol Vinyals, Demis\n  Hassabis, Razvan Pascanu, Charles Blundell", "title": "Memory-based Parameter Adaptation", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have excelled on a wide range of problems, from vision\nto language and game playing. Neural networks very gradually incorporate\ninformation into weights as they process data, requiring very low learning\nrates. If the training distribution shifts, the network is slow to adapt, and\nwhen it does adapt, it typically performs badly on the training distribution\nbefore the shift. Our method, Memory-based Parameter Adaptation, stores\nexamples in memory and then uses a context-based lookup to directly modify the\nweights of a neural network. Much higher learning rates can be used for this\nlocal adaptation, reneging the need for many iterations over similar data\nbefore good predictions can be made. As our method is memory-based, it\nalleviates several shortcomings of neural networks, such as catastrophic\nforgetting, fast, stable acquisition of new knowledge, learning with an\nimbalanced class labels, and fast learning during evaluation. We demonstrate\nthis on a range of supervised tasks: large-scale image classification and\nlanguage modelling.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:21:44 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Sprechmann", "Pablo", ""], ["Jayakumar", "Siddhant M.", ""], ["Rae", "Jack W.", ""], ["Pritzel", "Alexander", ""], ["Badia", "Adri\u00e0 Puigdom\u00e8nech", ""], ["Uria", "Benigno", ""], ["Vinyals", "Oriol", ""], ["Hassabis", "Demis", ""], ["Pascanu", "Razvan", ""], ["Blundell", "Charles", ""]]}, {"id": "1802.10549", "submitter": "Alex Rodriguez", "authors": "Maria d'Errico, Elena Facco, Alessandro Laio, and Alex Rodriguez", "title": "Automatic topography of high-dimensional data sets by non-parametric\n  Density Peak clustering", "comments": "There is a Supplementary Information document in the ancillary files\n  folder", "journal-ref": "Information Sciences Volume 560, June 2021, Pages 476-492", "doi": "10.1016/j.ins.2021.01.010", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis in high-dimensional spaces aims at obtaining a synthetic\ndescription of a data set, revealing its main structure and its salient\nfeatures. We here introduce an approach providing this description in the form\nof a topography of the data, namely a human-readable chart of the probability\ndensity from which the data are harvested. The approach is based on an\nunsupervised extension of Density Peak clustering and a non-parametric density\nestimator that measures the probability density in the manifold containing the\ndata. This allows finding automatically the number and the height of the peaks\nof the probability density, and the depth of the \"valleys\" separating them.\nImportantly, the density estimator provides a measure of the error, which\nallows distinguishing genuine density peaks from density fluctuations due to\nfinite sampling. The approach thus provides robust and visual information about\nthe density peaks' height, their statistical reliability, and their\nhierarchical organization, offering a conceptually powerful extension of the\nstandard clustering partitions. We show that this framework is particularly\nuseful in the analysis of complex data sets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:32:07 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:21:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["d'Errico", "Maria", ""], ["Facco", "Elena", ""], ["Laio", "Alessandro", ""], ["Rodriguez", "Alex", ""]]}, {"id": "1802.10551", "submitter": "Gauthier Gidel", "authors": "Gauthier Gidel, Hugo Berard, Ga\\\"etan Vignoud, Pascal Vincent and\n  Simon Lacoste-Julien", "title": "A Variational Inequality Perspective on Generative Adversarial Networks", "comments": "Appears in: Proceedings of the Seventh International Conference on\n  Learning Representations (ICLR 2019). Minor modifications with respect to the\n  ICLR version (First paragraph of page 2 and section 3.3): New reference\n  [Popov 1980] and discussion with regards to the novelty of extrapolation from\n  the past. 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:40:05 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 17:30:55 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 17:53:52 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 04:47:40 GMT"}, {"version": "v5", "created": "Fri, 28 Aug 2020 20:51:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gidel", "Gauthier", ""], ["Berard", "Hugo", ""], ["Vignoud", "Ga\u00ebtan", ""], ["Vincent", "Pascal", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1802.10558", "submitter": "Jicong Fan", "authors": "Jicong Fan and Tommy W.S. Chow", "title": "Exactly Robust Kernel Principal Component Analysis", "comments": "The paper was accepted by IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2909686", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) can recover low-rank matrices when\nthey are corrupted by sparse noises. In practice, many matrices are, however,\nof high-rank and hence cannot be recovered by RPCA. We propose a novel method\ncalled robust kernel principal component analysis (RKPCA) to decompose a\npartially corrupted matrix as a sparse matrix plus a high or full-rank matrix\nwith low latent dimensionality. RKPCA can be applied to many problems such as\nnoise removal and subspace clustering and is still the only unsupervised\nnonlinear method robust to sparse noises. Our theoretical analysis shows that,\nwith high probability, RKPCA can provide high recovery accuracy. The\noptimization of RKPCA involves nonconvex and indifferentiable problems. We\npropose two nonconvex optimization algorithms for RKPCA. They are alternating\ndirection method of multipliers with backtracking line search and proximal\nlinearized minimization with adaptive step size. Comparative studies in noise\nremoval and robust subspace clustering corroborate the effectiveness and\nsuperiority of RKPCA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:49:48 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 19:04:45 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Fan", "Jicong", ""], ["Chow", "Tommy W. S.", ""]]}, {"id": "1802.10567", "submitter": "Jost Tobias Springenberg", "authors": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas\n  Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, Jost Tobias\n  Springenberg", "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch", "comments": "A video of the rich set of learned behaviours can be found at\n  https://youtu.be/mPKyvocNe_M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in\nthe context of Reinforcement Learning (RL). SAC-X enables learning of complex\nbehaviors - from scratch - in the presence of multiple sparse reward signals.\nTo this end, the agent is equipped with a set of general auxiliary tasks, that\nit attempts to learn simultaneously via off-policy RL. The key idea behind our\nmethod is that active (learned) scheduling and execution of auxiliary policies\nallows the agent to efficiently explore its environment - enabling it to excel\nat sparse reward RL. Our experiments in several challenging robotic\nmanipulation settings demonstrate the power of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:15:49 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Riedmiller", "Martin", ""], ["Hafner", "Roland", ""], ["Lampe", "Thomas", ""], ["Neunert", "Michael", ""], ["Degrave", "Jonas", ""], ["Van de Wiele", "Tom", ""], ["Mnih", "Volodymyr", ""], ["Heess", "Nicolas", ""], ["Springenberg", "Jost Tobias", ""]]}, {"id": "1802.10570", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi", "title": "Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions", "comments": "6 pages, 1 figure", "journal-ref": "In proceedings 31st International Workshop on Statistical\n  Modelling, 4-8 July 2016, Rennes, France. Amsterdam: Statistical Modelling\n  Society, pp. 309-314", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:20:12 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tsiftsi", "Thomai", ""]]}, {"id": "1802.10576", "submitter": "Martin Treppner", "authors": "Martin Treppner, Stefan Lenz, Harald Binder and Daniela Z\\\"oller", "title": "Modeling Activity Tracker Data Using Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial activity trackers are set to become an essential tool in health\nresearch, due to increasing availability in the general population. The\ncorresponding vast amounts of mostly unlabeled data pose a challenge to\nstatistical modeling approaches. To investigate the feasibility of deep\nlearning approaches for unsupervised learning with such data, we examine weekly\nusage patterns of Fitbit activity trackers with deep Boltzmann machines (DBMs).\nThis method is particularly suitable for modeling complex joint distributions\nvia latent variables. We also chose this specific procedure because it is a\ngenerative approach, i.e., artificial samples can be generated to explore the\nlearned structure. We describe how the data can be preprocessed to be\ncompatible with binary DBMs. The results reveal two distinct usage patterns in\nwhich one group frequently uses trackers on Mondays and Tuesdays, whereas the\nother uses trackers during the entire week. This exemplary result shows that\nDBMs are feasible and can be useful for modeling activity tracker data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:33:49 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Treppner", "Martin", ""], ["Lenz", "Stefan", ""], ["Binder", "Harald", ""], ["Z\u00f6ller", "Daniela", ""]]}, {"id": "1802.10582", "submitter": "Amir Ghasemian", "authors": "Amir Ghasemian, Homa Hosseinmardi, Aaron Clauset", "title": "Evaluating Overfit and Underfit in Models of Network Community Structure", "comments": "22 pages, 13 figures, 3 tables", "journal-ref": "IEEE Trans. Knowledge and Data Engineering 32(9), 1722-1735 (2019)", "doi": "10.1109/TKDE.2019.2911585", "report-no": null, "categories": "stat.ML cs.SI physics.data-an q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common data mining task on networks is community detection, which seeks an\nunsupervised decomposition of a network into structural groups based on\nstatistical regularities in the network's connectivity. Although many methods\nexist, the No Free Lunch theorem for community detection implies that each\nmakes some kind of tradeoff, and no algorithm can be optimal on all inputs.\nThus, different algorithms will over or underfit on different inputs, finding\nmore, fewer, or just different communities than is optimal, and evaluation\nmethods that use a metadata partition as a ground truth will produce misleading\nconclusions about general accuracy. Here, we present a broad evaluation of over\nand underfitting in community detection, comparing the behavior of 16\nstate-of-the-art community detection algorithms on a novel and structurally\ndiverse corpus of 406 real-world networks. We find that (i) algorithms vary\nwidely both in the number of communities they find and in their corresponding\ncomposition, given the same input, (ii) algorithms can be clustered into\ndistinct high-level groups based on similarities of their outputs on real-world\nnetworks, and (iii) these differences induce wide variation in accuracy on link\nprediction and link description tasks. We introduce a new diagnostic for\nevaluating overfitting and underfitting in practice, and use it to roughly\ndivide community detection methods into general and specialized learning\nalgorithms. Across methods and inputs, Bayesian techniques based on the\nstochastic block model and a minimum description length approach to\nregularization represent the best general learning approach, but can be\noutperformed under specific circumstances. These results introduce both a\ntheoretically principled approach to evaluate over and underfitting in models\nof network community structure and a realistic benchmark by which new methods\nmay be evaluated and compared.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:42:17 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:14:41 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 17:44:42 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ghasemian", "Amir", ""], ["Hosseinmardi", "Homa", ""], ["Clauset", "Aaron", ""]]}]