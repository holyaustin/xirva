[{"id": "1608.00027", "submitter": "Rhiannon Rose", "authors": "Rhiannon V. Rose, Daniel J. Lizotte", "title": "gLOP: the global and Local Penalty for Capturing Predictive\n  Heterogeneity", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with a supervised learning problem, we hope to have rich enough\ndata to build a model that predicts future instances well. However, in\npractice, problems can exhibit predictive heterogeneity: most instances might\nbe relatively easy to predict, while others might be predictive outliers for\nwhich a model trained on the entire dataset does not perform well. Identifying\nthese can help focus future data collection. We present gLOP, the global and\nLocal Penalty, a framework for capturing predictive heterogeneity and\nidentifying predictive outliers. gLOP is based on penalized regression for\nmultitask learning, which improves learning by leveraging training signal\ninformation from related tasks. We give two optimization algorithms for gLOP,\none space-efficient, and another giving the full regularization path. We also\ncharacterize uniqueness in terms of the data and tuning parameters, and present\nempirical results on synthetic data and on two health research problems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 20:57:06 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Rose", "Rhiannon V.", ""], ["Lizotte", "Daniel J.", ""]]}, {"id": "1608.00060", "submitter": "Christian Hansen", "authors": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo,\n  Christian Hansen, Whitney Newey, and James Robins", "title": "Double/Debiased Machine Learning for Treatment and Causal Parameters", "comments": "71 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern supervised statistical/machine learning (ML) methods are\nexplicitly designed to solve prediction problems very well. Achieving this goal\ndoes not imply that these methods automatically deliver good estimators of\ncausal parameters. Examples of such parameters include individual regression\ncoefficients, average treatment effects, average lifts, and demand or supply\nelasticities. In fact, estimates of such causal parameters obtained via naively\nplugging ML estimators into estimating equations for such parameters can behave\nvery poorly due to the regularization bias. Fortunately, this regularization\nbias can be removed by solving auxiliary prediction problems via ML tools.\nSpecifically, we can form an orthogonal score for the target low-dimensional\nparameter by combining auxiliary and main ML predictions. The score is then\nused to build a de-biased estimator of the target parameter which typically\nwill converge at the fastest possible 1/root(n) rate and be approximately\nunbiased and normal, and from which valid confidence intervals for these\nparameters of interest may be constructed. The resulting method thus could be\ncalled a \"double ML\" method because it relies on estimating primary and\nauxiliary predictive models. In order to avoid overfitting, our construction\nalso makes use of the K-fold sample splitting, which we call cross-fitting.\nThis allows us to use a very broad set of ML predictive methods in solving the\nauxiliary and main prediction problems, such as random forest, lasso, ridge,\ndeep neural nets, boosted trees, as well as various hybrids and aggregators of\nthese methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 01:58:04 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 05:48:45 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 10:45:11 GMT"}, {"version": "v4", "created": "Fri, 30 Dec 2016 20:18:36 GMT"}, {"version": "v5", "created": "Wed, 21 Jun 2017 06:27:04 GMT"}, {"version": "v6", "created": "Tue, 12 Dec 2017 20:13:33 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Demirer", "Mert", ""], ["Duflo", "Esther", ""], ["Hansen", "Christian", ""], ["Newey", "Whitney", ""], ["Robins", "James", ""]]}, {"id": "1608.00075", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, Vincent Y. F. Tan, Huan Xu", "title": "Online Nonnegative Matrix Factorization with General Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified and systematic framework for performing online\nnonnegative matrix factorization under a wide variety of important divergences.\nThe online nature of our algorithm makes it particularly amenable to\nlarge-scale data. We prove that the sequence of learned dictionaries converges\nalmost surely to the set of critical points of the expected loss function. We\ndo so by leveraging the theory of stochastic approximations and projected\ndynamical systems. This result substantially generalizes the previous results\nobtained only for the squared-$\\ell_2$ loss. Moreover, the novel techniques\ninvolved in our analysis open new avenues for analyzing similar matrix\nfactorization problems. The computational efficiency and the quality of the\nlearned dictionary of our algorithm are verified empirically on both synthetic\nand real datasets. In particular, on the tasks of topic learning, shadow\nremoval and image denoising, our algorithm achieves superior trade-offs between\nthe quality of learned dictionary and running time over the batch and other\nonline NMF algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 06:07:38 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 02:36:50 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""], ["Xu", "Huan", ""]]}, {"id": "1608.00092", "submitter": "Truyen Tran", "authors": "Hoa Khanh Dam, Truyen Tran, John Grundy, Aditya Ghose", "title": "DeepSoft: A vision for a deep model of software", "comments": "FSE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although software analytics has experienced rapid growth as a research area,\nit has not yet reached its full potential for wide industrial adoption. Most of\nthe existing work in software analytics still relies heavily on costly manual\nfeature engineering processes, and they mainly address the traditional\nclassification problems, as opposed to predicting future events. We present a\nvision for \\emph{DeepSoft}, an \\emph{end-to-end} generic framework for modeling\nsoftware and its development process to predict future risks and recommend\ninterventions. DeepSoft, partly inspired by human memory, is built upon the\npowerful deep learning-based Long Short Term Memory architecture that is\ncapable of learning long-term temporal dependencies that occur in software\nevolution. Such deep learned patterns of software can be used to address a\nrange of challenging problems such as code and task recommendation and\nprediction. DeepSoft provides a new approach for research into modeling of\nsource code, risk prediction and mitigation, developer modeling, and\nautomatically generating code patches from bug reports.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 08:38:15 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Dam", "Hoa Khanh", ""], ["Tran", "Truyen", ""], ["Grundy", "John", ""], ["Ghose", "Aditya", ""]]}, {"id": "1608.00159", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Benjamin M. Marlin", "title": "Learning Tree-Structured Detection Cascades for Heterogeneous Networks\n  of Embedded Devices", "comments": "arXiv admin note: substantial text overlap with arXiv:1607.03730", "journal-ref": null, "doi": "10.1145/3097983.3098169", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach to learning cascaded classifiers for\nuse in computing environments that involve networks of heterogeneous and\nresource-constrained, low-power embedded compute and sensing nodes. We present\na generalization of the classical linear detection cascade to the case of\ntree-structured cascades where different branches of the tree execute on\ndifferent physical compute nodes in the network. Different nodes have access to\ndifferent features, as well as access to potentially different computation and\nenergy resources. We concentrate on the problem of jointly learning the\nparameters for all of the classifiers in the cascade given a fixed cascade\narchitecture and a known set of costs required to carry out the computation at\neach node.To accomplish the objective of joint learning of all detectors, we\npropose a novel approach to combining classifier outputs during training that\nbetter matches the hard cascade setting in which the learned system will be\ndeployed. This work is motivated by research in the area of mobile health where\nenergy efficient real time detectors integrating information from multiple\nwireless on-body sensors and a smart phone are needed for real-time monitoring\nand delivering just- in-time adaptive interventions. We apply our framework to\ntwo activity recognition datasets as well as the problem of cigarette smoking\ndetection from a combination of wrist-worn actigraphy data and respiration\nchest band data.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 19:52:56 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 19:12:43 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 10:10:40 GMT"}, {"version": "v4", "created": "Sat, 24 Jun 2017 23:30:59 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "1608.00218", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Jiashi Feng", "title": "Hyperparameter Transfer Learning through Surrogate Alignment for\n  Efficient Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several optimization methods have been successfully applied to the\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\nmodeling the joint distribution of hyperparameter values and corresponding\nerror. Those methods become less practical when applied to modern DNNs whose\ntraining may take a few days and thus one cannot collect sufficient\nobservations to accurately model the distribution. To address this challenging\nissue, we propose a method that learns to transfer optimal hyperparameter\nvalues for a small source dataset to hyperparameter values with comparable\nperformance on a dataset of interest. As opposed to existing transfer learning\nmethods, our proposed method does not use hand-designed features. Instead, it\nuses surrogates to model the hyperparameter-error distributions of the two\ndatasets and trains a neural network to learn the transfer function. Extensive\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:09:17 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ilievski", "Ilija", ""], ["Feng", "Jiashi", ""]]}, {"id": "1608.00250", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw and Marco Loog", "title": "On Regularization Parameter Estimation under Covariate Shift", "comments": "6 pages, 2 figures, 2 tables. Accepted to ICPR 2016", "journal-ref": "23rd International Conference on Pattern Recognition (ICPR),\n  Cancun, 2016, pp. 426-431", "doi": "10.1109/ICPR.2016.7899671", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies a problem with the usual procedure for\nL2-regularization parameter estimation in a domain adaptation setting. In such\na setting, there are differences between the distributions generating the\ntraining data (source domain) and the test data (target domain). The usual\ncross-validation procedure requires validation data, which can not be obtained\nfrom the unlabeled target data. The problem is that if one decides to use\nsource validation data, the regularization parameter is underestimated. One\npossible solution is to scale the source validation data through importance\nweighting, but we show that this correction is not sufficient. We conclude the\npaper with an empirical analysis of the effect of several importance weight\nestimators on the estimation of the regularization parameter.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 19:02:39 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""]]}, {"id": "1608.00354", "submitter": "Martin Spindler", "authors": "Victor Chernozhukov, Chris Hansen, Martin Spindler", "title": "hdm: High-Dimensional Metrics", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.01700", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article the package High-dimensional Metrics (\\texttt{hdm}) is\nintroduced. It is a collection of statistical methods for estimation and\nquantification of uncertainty in high-dimensional approximately sparse models.\nIt focuses on providing confidence intervals and significance testing for\n(possibly many) low-dimensional subcomponents of the high-dimensional parameter\nvector. Efficient estimators and uniformly valid confidence intervals for\nregression coefficients on target variables (e.g., treatment or policy\nvariable) in a high-dimensional approximately sparse regression model, for\naverage treatment effect (ATE) and average treatment effect for the treated\n(ATET), as well for extensions of these parameters to the endogenous setting\nare provided. Theory grounded, data-driven methods for selecting the\npenalization parameter in Lasso regressions under heteroscedastic and\nnon-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence\nintervals for regression coefficients of a high-dimensional sparse regression\nare implemented. Data sets which have been used in the literature and might be\nuseful for classroom demonstration and for testing new estimators are included.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 08:51:31 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Chris", ""], ["Spindler", "Martin", ""]]}, {"id": "1608.00441", "submitter": "Badong Chen", "authors": "Badong Chen, Lei Xing, Bin Xu, Haiquan Zhao, Nanning Zheng, Jose C.\n  Principe", "title": "Kernel Risk-Sensitive Loss: Definition, Properties and Application to\n  Robust Adaptive Filtering", "comments": "12 pages,8 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2669903", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear similarity measures defined in kernel space, such as correntropy,\ncan extract higher-order statistics of data and offer potentially significant\nperformance improvement over their linear counterparts especially in\nnon-Gaussian signal processing and machine learning. In this work, we propose a\nnew similarity measure in kernel space, called the kernel risk-sensitive loss\n(KRSL), and provide some important properties. We apply the KRSL to adaptive\nfiltering and investigate the robustness, and then develop the MKRSL algorithm\nand analyze the mean square convergence performance. Compared with correntropy,\nthe KRSL can offer a more efficient performance surface, thereby enabling a\ngradient based method to achieve faster convergence speed and higher accuracy\nwhile still maintaining the robustness to outliers. Theoretical analysis\nresults and superior performance of the new algorithm are confirmed by\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 14:19:32 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Badong", ""], ["Xing", "Lei", ""], ["Xu", "Bin", ""], ["Zhao", "Haiquan", ""], ["Zheng", "Nanning", ""], ["Principe", "Jose C.", ""]]}, {"id": "1608.00554", "submitter": "Tarun Kathuria", "authors": "L. Elisa Celis and Amit Deshpande and Tarun Kathuria and Damian\n  Straszak and Nisheeth K. Vishnoi", "title": "On the Complexity of Constrained Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal Point Processes (DPPs) are probabilistic models that arise in\nquantum physics and random matrix theory and have recently found numerous\napplications in computer science. DPPs define distributions over subsets of a\ngiven ground set, they exhibit interesting properties such as negative\ncorrelation, and, unlike other models, have efficient algorithms for sampling.\nWhen applied to kernel methods in machine learning, DPPs favor subsets of the\ngiven data with more diverse features. However, many real-world applications\nrequire efficient algorithms to sample from DPPs with additional constraints on\nthe subset, e.g., partition or matroid constraints that are important to ensure\npriors, resource or fairness constraints on the sampled subset. Whether one can\nefficiently sample from DPPs in such constrained settings is an important\nproblem that was first raised in a survey of DPPs by \\cite{KuleszaTaskar12} and\nstudied in some recent works in the machine learning literature.\n  The main contribution of our paper is the first resolution of the complexity\nof sampling from DPPs with constraints. We give exact efficient algorithms for\nsampling from constrained DPPs when their description is in unary. Furthermore,\nwe prove that when the constraints are specified in binary, this problem is\n#P-hard via a reduction from the problem of computing mixed discriminants\nimplying that it may be unlikely that there is an FPRAS. Our results benefit\nfrom viewing the constrained sampling problem via the lens of polynomials.\nConsequently, we obtain a few algorithms of independent interest: 1) to count\nover the base polytope of regular matroids when there are additional (succinct)\nbudget constraints and, 2) to evaluate and compute the mixed characteristic\npolynomials, that played a central role in the resolution of the Kadison-Singer\nproblem, for certain special cases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:58:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 19:12:46 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 12:38:48 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Celis", "L. Elisa", ""], ["Deshpande", "Amit", ""], ["Kathuria", "Tarun", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1608.00619", "submitter": "Bo-Wei Chen", "authors": "Bo-Wei Chen", "title": "Recursion-Free Online Multiple Incremental/Decremental Analysis Based on\n  Ridge Support Vector Learning", "comments": "Ridge support vector machine (Ridge SVM), Ridge support vector\n  regression (Ridge SVR), multiple incremental learning, multiple decremental\n  learning, online learning, batch learning, cloud computing, big data\n  analysis, data analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a rapid multiple incremental and decremental mechanism\nbased on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free\ncomputation is proposed for predicting the Lagrangian multipliers of new\nsamples. This study examines Ridge Support Vector Models, subsequently devising\na recursion-free function derived from WECs. With the proposed function, all\nthe new Lagrangian multipliers can be computed at once without using any\ngradual step sizes. Moreover, such a function relaxes a constraint, where the\nincrement of new multiple Lagrangian multipliers should be the same in the\nprevious work, thereby easily satisfying the requirement of KKT conditions. The\nproposed mechanism no longer requires typical bookkeeping strategies, which\ncompute the step size by checking all the training samples in each incremental\nround.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:13:12 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 19:55:58 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Chen", "Bo-Wei", ""]]}, {"id": "1608.00621", "submitter": "Bo-Wei Chen", "authors": "Bo-Wei Chen, Nik Nailah Binti Abdullah, and Sangoh Park", "title": "Efficient Multiple Incremental Computation for Kernel Ridge Regression\n  with Bayesian Uncertainty Modeling", "comments": "Multiple incremental analysis, multiple decremental analysis,\n  incremental learning, kernel ridge regression (KRR), recursive KRR,\n  uncertainty analysis, kernelized Bayesian regression, Gaussian process, batch\n  learning, online learning, edge computing, fog computing, regression,\n  classification", "journal-ref": null, "doi": "10.1016/j.future.2017.08.053", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents an efficient incremental/decremental approach for big\nstreams based on Kernel Ridge Regression (KRR), a frequently used data analysis\nin cloud centers. To avoid reanalyzing the whole dataset whenever sensors\nreceive new training data, typical incremental KRR used a single-instance\nmechanism for updating an existing system. However, this inevitably increased\nredundant computational time, not to mention applicability to big streams. To\nthis end, the proposed mechanism supports incremental/decremental processing\nfor both single and multiple samples (i.e., batch processing). A large scale of\ndata can be divided into batches, processed by a machine, without sacrificing\nthe accuracy. Moreover, incremental/decremental analyses in empirical and\nintrinsic space are also proposed in this study to handle different types of\ndata either with a large number of samples or high feature dimensions, whereas\ntypical methods focused only on one type. At the end of this study, we further\nthe proposed mechanism to statistical Kernelized Bayesian Regression, so that\nuncertainty modeling with incremental/decremental computation becomes\napplicable. Experimental results showed that computational time was\nsignificantly reduced, better than the original nonincremental design and the\ntypical single incremental method. Furthermore, the accuracy of the proposed\nmethod remained the same as the baselines. This implied that the system\nenhanced efficiency without sacrificing the accuracy. These findings proved\nthat the proposed method was appropriate for variable streaming data analysis,\nthereby demonstrating the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:21:07 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 04:15:19 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 03:14:27 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Chen", "Bo-Wei", ""], ["Abdullah", "Nik Nailah Binti", ""], ["Park", "Sangoh", ""]]}, {"id": "1608.00624", "submitter": "Johannes Lederer", "authors": "Johannes Lederer, Lu Yu, Irina Gaynanova", "title": "Oracle Inequalities for High-dimensional Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of high-dimensional data in the modern sciences has generated\ntremendous interest in penalized estimators such as the lasso, scaled lasso,\nsquare-root lasso, elastic net, and many others. In this paper, we establish a\ngeneral oracle inequality for prediction in high-dimensional linear regression\nwith such methods. Since the proof relies only on convexity and continuity\narguments, the result holds irrespective of the design matrix and applies to a\nwide range of penalized estimators. Overall, the bound demonstrates that\ngeneric estimators can provide consistent prediction with any design matrix.\nFrom a practical point of view, the bound can help to identify the potential of\nspecific estimators, and they can help to get a sense of the prediction\naccuracy in a given application.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:42:55 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 12:58:57 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Lederer", "Johannes", ""], ["Yu", "Lu", ""], ["Gaynanova", "Irina", ""]]}, {"id": "1608.00686", "submitter": "David Sontag", "authors": "Yoni Halpern and Steven Horng and David Sontag", "title": "Clinical Tagging with Joint Probabilistic Models", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for parameter estimation in bipartite probabilistic\ngraphical models for joint prediction of clinical conditions from the\nelectronic medical record. The method does not rely on the availability of\ngold-standard labels, but rather uses noisy labels, called anchors, for\nlearning. We provide a likelihood-based objective and a moments-based\ninitialization that are effective at learning the model parameters. The learned\nmodel is evaluated in a task of assigning a heldout clinical condition to\npatients based on retrospective analysis of the records, and outperforms\nbaselines which do not account for the noisiness in the labels or do not model\nthe conditions jointly.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 03:09:59 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 15:36:13 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 00:37:40 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Halpern", "Yoni", ""], ["Horng", "Steven", ""], ["Sontag", "David", ""]]}, {"id": "1608.00696", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui and Elizabeth Purdom", "title": "Can we trust the bootstrap in high-dimension?", "comments": null, "journal-ref": null, "doi": null, "report-no": "Tech-report 824", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the performance of the bootstrap in high-dimensions for the\nsetting of linear regression, where $p<n$ but $p/n$ is not close to zero. We\nconsider ordinary least-squares as well as robust regression methods and adopt\na minimalist performance requirement: can the bootstrap give us good confidence\nintervals for a single coordinate of $\\beta$? (where $\\beta$ is the true\nregression vector).\n  We show through a mix of numerical and theoretical work that the bootstrap is\nfraught with problems. Both of the most commonly used methods of bootstrapping\nfor regression -- residual bootstrap and pairs bootstrap -- give very poor\ninference on $\\beta$ as the ratio $p/n$ grows. We find that the residuals\nbootstrap tend to give anti-conservative estimates (inflated Type I error),\nwhile the pairs bootstrap gives very conservative estimates (severe loss of\npower) as the ratio $p/n$ grows. We also show that the jackknife resampling\ntechnique for estimating the variance of $\\hat{\\beta}$ severely overestimates\nthe variance in high dimensions.\n  We contribute alternative bootstrap procedures based on our theoretical\nresults that mitigate these problems. However, the corrections depend on\nassumptions regarding the underlying data-generation model, suggesting that in\nhigh-dimensions it may be difficult to have universal, robust bootstrapping\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 05:10:39 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Purdom", "Elizabeth", ""]]}, {"id": "1608.00704", "submitter": "Shalmali Joshi", "authors": "Shalmali Joshi, Suriya Gunasekar, David Sontag, Joydeep Ghosh", "title": "Identifiable Phenotyping using Constrained Non-Negative Matrix\n  Factorization", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new algorithm for automated and simultaneous phenotyping\nof multiple co-occurring medical conditions, also referred as comorbidities,\nusing clinical notes from the electronic health records (EHRs). A basic latent\nfactor estimation technique of non-negative matrix factorization (NMF) is\naugmented with domain specific constraints to obtain sparse latent factors that\nare anchored to a fixed set of chronic conditions. The proposed anchoring\nmechanism ensures a one-to-one identifiable and interpretable mapping between\nthe latent factors and the target comorbidities. Qualitative assessment of the\nempirical results by clinical experts suggests that the proposed model learns\nclinically interpretable phenotypes while being predictive of 30 day mortality.\nThe proposed method can be readily adapted to any non-negative EHR data across\nvarious healthcare institutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 06:03:53 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 18:02:34 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 13:01:04 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Joshi", "Shalmali", ""], ["Gunasekar", "Suriya", ""], ["Sontag", "David", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1608.00778", "submitter": "Maja Rudolph", "authors": "Maja R. Rudolph, Francisco J. R. Ruiz, Stephan Mandt, David M. Blei", "title": "Exponential Family Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful approach for capturing semantic similarity\namong terms in a vocabulary. In this paper, we develop exponential family\nembeddings, a class of methods that extends the idea of word embeddings to\nother types of high-dimensional data. As examples, we studied neural data with\nreal-valued observations, count data from a market basket analysis, and ratings\ndata from a movie recommendation system. The main idea is to model each\nobservation conditioned on a set of other observations. This set is called the\ncontext, and the way the context is defined is a modeling choice that depends\non the problem. In language the context is the surrounding words; in\nneuroscience the context is close-by neurons; in market basket data the context\nis other items in the shopping cart. Each type of embedding model defines the\ncontext, the exponential family of conditional distributions, and how the\nlatent embedding vectors are shared across data. We infer the embeddings with a\nscalable algorithm based on stochastic gradient descent. On all three\napplications - neural activity of zebrafish, users' shopping behavior, and\nmovie ratings - we found exponential family embedding models to be more\neffective than other types of dimension reduction. They better reconstruct\nheld-out data and find interesting qualitative structure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:44:19 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 15:12:54 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Rudolph", "Maja R.", ""], ["Ruiz", "Francisco J. R.", ""], ["Mandt", "Stephan", ""], ["Blei", "David M.", ""]]}, {"id": "1608.00860", "submitter": "Jie Chen", "authors": "Jie Chen, Haim Avron, Vikas Sindhwani", "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning", "comments": "Journal of Machine Learning Research, vol 18, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of kernels to alleviate the high computational cost\nof large-scale nonparametric learning with kernel methods. The proposed kernel\nis defined based on a hierarchical partitioning of the underlying data domain,\nwhere the Nystr\\\"om method (a globally low-rank approximation) is married with\na locally lossless approximation in a hierarchical fashion. The kernel\nmaintains (strict) positive-definiteness. The corresponding kernel matrix\nadmits a recursively off-diagonal low-rank structure, which allows for fast\nlinear algebra computations. Suppressing the factor of data dimension, the\nmemory and arithmetic complexities for training a regression or a classifier\nare reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively,\nwhere $n$ is the number of training examples and $r$ is the rank on each level\nof the hierarchy. Although other randomized approximate kernels entail a\nsimilar complexity, empirical results show that the proposed kernel achieves a\nmatching performance with a smaller $r$. We demonstrate comprehensive\nexperiments to show the effective use of the proposed kernel on data sizes up\nto the order of millions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:07:25 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:11:25 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Jie", ""], ["Avron", "Haim", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1608.00874", "submitter": "Fabrizio Leisen", "authors": "Jim Griffin and Fabrizio Leisen", "title": "Modelling and computation using NCoRM mixtures for density regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:47:42 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 10:41:56 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 10:21:58 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Griffin", "Jim", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1608.00876", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed", "title": "Relational Similarity Machines", "comments": "MLG16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Relational Similarity Machines (RSM): a fast, accurate,\nand flexible relational learning framework for supervised and semi-supervised\nlearning tasks. Despite the importance of relational learning, most existing\nmethods are hard to adapt to different settings, due to issues with efficiency,\nscalability, accuracy, and flexibility for handling a wide variety of\nclassification problems, data, constraints, and tasks. For instance, many\nexisting methods perform poorly for multi-class classification problems, graphs\nthat are sparsely labeled or network data with low relational autocorrelation.\nIn contrast, the proposed relational learning framework is designed to be (i)\nfast for learning and inference at real-time interactive rates, and (ii)\nflexible for a variety of learning settings (multi-class problems), constraints\n(few labeled instances), and application domains. The experiments demonstrate\nthe effectiveness of RSM for a variety of tasks and data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:48:58 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1608.00945", "submitter": "Xin Zhang", "authors": "Xin Zhang and Scott A. Sisson", "title": "Blocking Collapsed Gibbs Sampler for Latent Dirichlet Allocation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent Dirichlet allocation (LDA) model is a widely-used latent variable\nmodel in machine learning for text analysis. Inference for this model typically\ninvolves a single-site collapsed Gibbs sampling step for latent variables\nassociated with observations. The efficiency of the sampling is critical to the\nsuccess of the model in practical large scale applications. In this article, we\nintroduce a blocking scheme to the collapsed Gibbs sampler for the LDA model\nwhich can, with a theoretical guarantee, improve chain mixing efficiency. We\ndevelop two procedures, an O(K)-step backward simulation and an O(log K)-step\nnested simulation, to directly sample the latent variables within each block.\nWe demonstrate that the blocking scheme achieves substantial improvements in\nchain mixing compared to the state of the art single-site collapsed Gibbs\nsampler. We also show that when the number of topics is over hundreds, the\nnested-simulation blocking scheme can achieve a significant reduction in\ncomputation time compared to the single-site sampler.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:24:50 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Zhang", "Xin", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1608.01008", "submitter": "Chengtao Li", "authors": "Chengtao Li, Stefanie Jegelka, Suvrit Sra", "title": "Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and\n  Constrained Sampling", "comments": "The present version subsumes arXiv:1607.03559", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probability measures induced by set functions with constraints. Such\nmeasures arise in a variety of real-world settings, where prior knowledge,\nresource limitations, or other pragmatic considerations impose constraints. We\nconsider the task of rapidly sampling from such constrained measures, and\ndevelop fast Markov chain samplers for them. Our first main result is for MCMC\nsampling from Strongly Rayleigh (SR) measures, for which we present sharp\npolynomial bounds on the mixing time. As a corollary, this result yields a fast\nmixing sampler for Determinantal Point Processes (DPPs), yielding (to our\nknowledge) the first provably fast MCMC sampler for DPPs since their inception\nover four decades ago. Beyond SR measures, we develop MCMC samplers for\nprobabilistic models with hard constraints and identify sufficient conditions\nunder which their chains mix rapidly. We illustrate our claims by empirically\nverifying the dependence of mixing times on the key factors governing our\ntheoretical bounds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 21:15:30 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 20:02:36 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 19:02:20 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1608.01118", "submitter": "Julien Bect", "authors": "Julien Bect (L2S, GdR MASCOT-NUM), Fran\\c{c}ois Bachoc (IMT), David\n  Ginsbourger (IMSV)", "title": "A supermartingale approach to Gaussian process based sequential design\n  of experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) models have become a well-established frameworkfor the\nadaptive design of costly experiments, and notably of computerexperiments.\nGP-based sequential designs have been found practicallyefficient for various\nobjectives, such as global optimization(estimating the global maximum or\nmaximizer(s) of a function),reliability analysis (estimating a probability of\nfailure) or theestimation of level sets and excursion sets. In this paper, we\nstudythe consistency of an important class of sequential designs, known\nasstepwise uncertainty reduction (SUR) strategies. Our approach relieson the\nkey observation that the sequence of residual uncertaintymeasures, in SUR\nstrategies, is generally a supermartingale withrespect to the filtration\ngenerated by the observations. Thisobservation enables us to establish generic\nconsistency results for abroad class of SUR strategies. The consistency of\nseveral popularsequential design strategies is then obtained by means of this\ngeneralresult. Notably, we establish the consistency of two SUR\nstrategiesproposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat.\nComp.,2012)---to the best of our knowledge, these are the first proofs\nofconsistency for GP-based sequential design algorithms dedicated to\ntheestimation of excursion sets and their measure. We also establish anew, more\ngeneral proof of consistency for the expected improvementalgorithm for global\noptimization which, unlike previous results inthe literature, applies to any GP\nwith continuous sample paths.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 09:01:07 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 08:03:25 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 13:13:49 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1608.01230", "submitter": "Eder Santana", "authors": "Eder Santana, George Hotz", "title": "Learning a Driving Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 15:49:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Santana", "Eder", ""], ["Hotz", "George", ""]]}, {"id": "1608.01234", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani and Chinmay Hegde", "title": "Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2706181", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of demixing a pair of sparse signals from noisy,\nnonlinear observations of their superposition. Mathematically, we consider a\nnonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \\ i=1,\\ldots,m$,\nwhere $x = \\Phi w+\\Psi z$ denotes the superposition signal, $\\Phi$ and $\\Psi$\nare orthonormal bases in $\\mathbb{R}^n$, and $w, z\\in\\mathbb{R}^n$ are sparse\ncoefficient vectors of the constituent signals, and $e_i$ represents the noise.\nMoreover, $g$ represents a nonlinear link function, and $a_i\\in\\mathbb{R}^n$ is\nthe $i$-th row of the measurement matrix, $A\\in\\mathbb{R}^{m\\times n}$.\nProblems of this nature arise in several applications ranging from astronomy,\ncomputer vision, and machine learning. In this paper, we make some concrete\nalgorithmic progress for the above demixing problem. Specifically, we consider\ntwo scenarios: (i) the case when the demixing procedure has no knowledge of the\nlink function, and (ii) the case when the demixing algorithm has perfect\nknowledge of the link function. In both cases, we provide fast algorithms for\nrecovery of the constituents $w$ and $z$ from the observations. Moreover, we\nsupport these algorithms with a rigorous theoretical analysis, and derive\n(nearly) tight upper bounds on the sample complexity of the proposed algorithms\nfor achieving stable recovery of the component signals. We also provide a range\nof numerical simulations to illustrate the performance of the proposed\nalgorithms on both real and synthetic signals and images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:03:25 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 18:55:54 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 17:55:33 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1608.01264", "submitter": "Niao He", "authors": "Niao He, Zaid Harchaoui, Yichen Wang, Le Song", "title": "Fast and Simple Optimization for Poisson Likelihood Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson likelihood models have been prevalently used in imaging, social\nnetworks, and time series analysis. We propose fast, simple,\ntheoretically-grounded, and versatile, optimization algorithms for Poisson\nlikelihood modeling. The Poisson log-likelihood is concave but not\nLipschitz-continuous. Since almost all gradient-based optimization algorithms\nrely on Lipschitz-continuity, optimizing Poisson likelihood models with a\nguarantee of convergence can be challenging, especially for large-scale\nproblems.\n  We present a new perspective allowing to efficiently optimize a wide range of\npenalized Poisson likelihood objectives. We show that an appropriate saddle\npoint reformulation enjoys a favorable geometry and a smooth structure.\nTherefore, we can design a new gradient-based optimization algorithm with\n$O(1/t)$ convergence rate, in contrast to the usual $O(1/\\sqrt{t})$ rate of\nnon-smooth minimization alternatives. Furthermore, in order to tackle problems\nwith large samples, we also develop a randomized block-decomposition variant\nthat enjoys the same convergence rate yet more efficient iteration cost.\n  Experimental results on several point process applications including social\nnetwork estimation and temporal recommendation show that the proposed algorithm\nand its randomized block variant outperform existing methods both on synthetic\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 17:33:16 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["He", "Niao", ""], ["Harchaoui", "Zaid", ""], ["Wang", "Yichen", ""], ["Song", "Le", ""]]}, {"id": "1608.01282", "submitter": "Triet Le", "authors": "Triet M Le", "title": "A Multivariate Hawkes Process with Gaps in Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.DS math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of entities (or nodes) in a network and our intermittent\nobservations of activities from each entity, an important problem is to learn\nthe hidden edges depicting directional relationships among these entities.\nHere, we study causal relationships (excitations) that are realized by a\nmultivariate Hawkes process. The multivariate Hawkes process (MHP) and its\nvariations (spatio-temporal point processes) have been used to study contagion\nin earthquakes, crimes, neural spiking activities, the stock and foreign\nexchange markets, etc. In this paper, we consider the multivariate Hawkes\nprocess with gaps in observations (MHPG). We propose a variational problem for\ndetecting sparsely hidden relationships with a multivariate Hawkes process that\ntakes into account the gaps from each entity. We bypass the problem of dealing\nwith a large amount of missing events by introducing a small number of unknown\nboundary conditions. In the case where our observations are sparse (e.g. from\n10% to 30%), we show through numerical simulations that robust recovery with\nMHPG is still possible even if the lengths of the observed intervals are small\nbut they are chosen accordingly. The numerical results also show that the\nknowledge of gaps and imposing the right boundary conditions are very crucial\nin discovering the underlying patterns and hidden relationships.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:36:37 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 20:56:32 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 02:46:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Le", "Triet M", ""]]}, {"id": "1608.01298", "submitter": "Peter Wittek", "authors": "S\\'andor Dar\\'anyi, Peter Wittek, Konstantinos Konstantinidis, Symeon\n  Papadopoulos, Efstratios Kontopoulos", "title": "A Physical Metaphor to Study Semantic Drift", "comments": "8 pages, 4 figures, to appear in Proceedings of SuCCESS-16, 1st\n  International Workshop on Semantic Change & Evolving Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In accessibility tests for digital preservation, over time we experience\ndrifts of localized and labelled content in statistical models of evolving\nsemantics represented as a vector field. This articulates the need to detect,\nmeasure, interpret and model outcomes of knowledge dynamics. To this end we\nemploy a high-performance machine learning algorithm for the training of\nextremely large emergent self-organizing maps for exploratory data analysis.\nThe working hypothesis we present here is that the dynamics of semantic drifts\ncan be modeled on a relaxed version of Newtonian mechanics called social\nmechanics. By using term distances as a measure of semantic relatedness vs.\ntheir PageRank values indicating social importance and applied as variable\n`term mass', gravitation as a metaphor to express changes in the semantic\ncontent of a vector field lends a new perspective for experimentation. From\n`term gravitation' over time, one can compute its generating potential whose\nfluctuations manifest modifications in pairwise term similarity vs. social\nimportance, thereby updating Osgood's semantic differential. The dataset\nexamined is the public catalog metadata of Tate Galleries, London.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 19:34:13 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Wittek", "Peter", ""], ["Konstantinidis", "Konstantinos", ""], ["Papadopoulos", "Symeon", ""], ["Kontopoulos", "Efstratios", ""]]}, {"id": "1608.01398", "submitter": "Kevin Keys", "authors": "Kevin L. Keys, Gary K. Chen and Kenneth Lange", "title": "Iterative Hard Thresholding for Model Selection in Genome-Wide\n  Association Studies", "comments": "13 pages, 1 figure, 4 tables", "journal-ref": "Genetic Epidemiology 2017:41(8), 756--768", "doi": "10.1002/gepi.22068", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A genome-wide association study (GWAS) correlates marker variation with trait\nvariation in a sample of individuals. Each study subject is genotyped at a\nmultitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here\nwe assume that subjects are unrelated and collected at random and that trait\nvalues are normally distributed or transformed to normality. Over the past\ndecade, researchers have been remarkably successful in applying GWAS analysis\nto hundreds of traits. The massive amount of data produced in these studies\npresent unique computational challenges. Penalized regression with LASSO or MCP\npenalties is capable of selecting a handful of associated SNPs from millions of\npotential SNPs. Unfortunately, model selection can be corrupted by false\npositives and false negatives, obscuring the genetic underpinning of a trait.\nThis paper introduces the iterative hard thresholding (IHT) algorithm to the\nGWAS analysis of continuous traits. Our parallel implementation of IHT\naccommodates SNP genotype compression and exploits multiple CPU cores and\ngraphics processing units (GPUs). This allows statistical geneticists to\nleverage commodity desktop computers in GWAS analysis and to avoid\nsupercomputing. We evaluate IHT performance on both simulated and real GWAS\ndata and conclude that it reduces false positive and false negative rates while\nremaining competitive in computational time with penalized regression. Source\ncode is freely available at https://github.com/klkeys/IHT.jl.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:05:24 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:07:56 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 00:49:55 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Keys", "Kevin L.", ""], ["Chen", "Gary K.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1608.01410", "submitter": "Hyun-Chul Kim", "authors": "Hyun-Chul Kim", "title": "Bayesian Kernel and Mutual $k$-Nearest Neighbor Regression", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bayesian extensions of two nonparametric regression methods which\nare kernel and mutual $k$-nearest neighbor regression methods. Derived based on\nGaussian process models for regression, the extensions provide distributions\nfor target value estimates and the framework to select the hyperparameters. It\nis shown that both the proposed methods asymptotically converge to kernel and\nmutual $k$-nearest neighbor regression methods, respectively. The simulation\nresults show that the proposed methods can select proper hyperparameters and\nare better than or comparable to the former methods for an artificial data set\nand a real world data set.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 01:33:34 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Kim", "Hyun-Chul", ""]]}, {"id": "1608.01747", "submitter": "Yukun Chen", "authors": "Yukun Chen, Jianbo Ye, and Jia Li", "title": "A Distance for HMMs based on Aggregated Wasserstein Metric and State\n  Registration", "comments": "submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time spot follows a Gaussian mixture distribution, a fact\nexploited to softly match, aka register, the states in two HMMs. We refer to\nsuch HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states\nis inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of the states. This distance quantifies\nthe dissimilarity of GMM-HMMs by measuring both the difference between the two\nmarginal GMMs and the difference between the two transition matrices. Our new\ndistance is tested on the tasks of retrieval and classification of time series.\nExperiments on both synthetic data and real data have demonstrated its\nadvantages in terms of accuracy as well as efficiency in comparison with\nexisting distances based on the Kullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 03:37:46 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Chen", "Yukun", ""], ["Ye", "Jianbo", ""], ["Li", "Jia", ""]]}, {"id": "1608.01771", "submitter": "Mert Ozer", "authors": "Mert Ozer, Nyunsu Kim, Hasan Davulcu", "title": "Community Detection in Political Twitter Networks using Nonnegative\n  Matrix Factorization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental task in social network analysis. In this\npaper, first we develop an endorsement filtered user connectivity network by\nutilizing Heider's structural balance theory and certain Twitter triad\npatterns. Next, we develop three Nonnegative Matrix Factorization frameworks to\ninvestigate the contributions of different types of user connectivity and\ncontent information in community detection. We show that user content and\nendorsement filtered connectivity information are complementary to each other\nin clustering politically motivated users into pure political communities. Word\nusage is the strongest indicator of users' political orientation among all\ncontent categories. Incorporating user-word matrix and word similarity\nregularizer provides the missing link in connectivity only methods which suffer\nfrom detection of artificially large number of clusters for Twitter networks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 06:12:07 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Ozer", "Mert", ""], ["Kim", "Nyunsu", ""], ["Davulcu", "Hasan", ""]]}, {"id": "1608.01976", "submitter": "Rashish Tandon", "authors": "Rashish Tandon, Si Si, Pradeep Ravikumar, Inderjit Dhillon", "title": "Kernel Ridge Regression via Partitioning", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 19:02:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Tandon", "Rashish", ""], ["Si", "Si", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1608.02060", "submitter": "Mehdi Korki", "authors": "H. Zayyani, M. Korki", "title": "Weighted diffusion LMP algorithm for distributed estimation in\n  non-uniform noise conditions", "comments": "2 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents an improved version of diffusion least mean ppower (LMP)\nalgorithm for distributed estimation. Instead of sum of mean square errors, a\nweighted sum of mean square error is defined as the cost function for global\nand local cost functions of a network of sensors. The weight coefficients are\nupdated by a simple steepest-descent recursion to minimize the error signal of\nthe global and local adaptive algorithm. Simulation results show the advantages\nof the proposed weighted diffusion LMP over the diffusion LMP algorithm\nspecially in the non-uniform noise conditions in a sensor network.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 03:37:52 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Zayyani", "H.", ""], ["Korki", "M.", ""]]}, {"id": "1608.02158", "submitter": "Adler Perotte", "authors": "Rajesh Ranganath and Adler Perotte and No\\'emie Elhadad and David Blei", "title": "Deep Survival Analysis", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electronic health record (EHR) provides an unprecedented opportunity to\nbuild actionable tools to support physicians at the point of care. In this\npaper, we investigate survival analysis in the context of EHR data. We\nintroduce deep survival analysis, a hierarchical generative approach to\nsurvival analysis. It departs from previous approaches in two primary ways: (1)\nall observations, including covariates, are modeled jointly conditioned on a\nrich latent structure; and (2) the observations are aligned by their failure\ntime, rather than by an arbitrary time zero as in traditional survival\nanalysis. Further, it (3) scalably handles heterogeneous (continuous and\ndiscrete) data types that occur in the EHR. We validate deep survival analysis\nmodel by stratifying patients according to risk of developing coronary heart\ndisease (CHD). Specifically, we study a dataset of 313,000 patients\ncorresponding to 5.5 million months of observations. When compared to the\nclinically validated Framingham CHD risk score, deep survival analysis is\nsignificantly superior in stratifying patients according to their risk.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 22:18:18 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 14:08:02 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Perotte", "Adler", ""], ["Elhadad", "No\u00e9mie", ""], ["Blei", "David", ""]]}, {"id": "1608.02198", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "A General Characterization of the Statistical Query Complexity", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical query (SQ) algorithms are algorithms that have access to an {\\em\nSQ oracle} for the input distribution $D$ instead of i.i.d.~ samples from $D$.\nGiven a query function $\\phi:X \\rightarrow [-1,1]$, the oracle returns an\nestimate of ${\\bf E}_{ x\\sim D}[\\phi(x)]$ within some tolerance $\\tau_\\phi$\nthat roughly corresponds to the number of samples.\n  In this work we demonstrate that the complexity of solving general problems\nover distributions using SQ algorithms can be captured by a relatively simple\nnotion of statistical dimension that we introduce. SQ algorithms capture a\nbroad spectrum of algorithmic approaches used in theory and practice, most\nnotably, convex optimization techniques. Hence our statistical dimension allows\nto investigate the power of a variety of algorithmic approaches by analyzing a\nsingle linear-algebraic parameter. Such characterizations were investigated\nover the past 20 years in learning theory but prior characterizations are\nrestricted to the much simpler setting of classification problems relative to a\nfixed distribution on the domain (Blum et al., 1994; Bshouty and Feldman, 2002;\nYang, 2001; Simon, 2007; Feldman, 2012; Szorenyi, 2009). Our characterization\nis also the first to precisely characterize the necessary tolerance of queries.\nWe give applications of our techniques to two open problems in learning theory\nand to algorithms that are subject to memory and communication constraints.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 09:35:44 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 19:55:31 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 06:12:23 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1608.02209", "submitter": "Daniele Durante", "authors": "Daniele Durante, Nabanita Mukherjee, Rebecca C. Steorts", "title": "Bayesian Learning of Dynamic Multilayer Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research (2017). 18, 1-29", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of networks is being collected in a growing number of fields,\nincluding disease transmission, international relations, social interactions,\nand others. As data streams continue to grow, the complexity associated with\nthese highly multidimensional connectivity data presents novel challenges. In\nthis paper, we focus on the time-varying interconnections among a set of actors\nin multiple contexts, called layers. Current literature lacks flexible\nstatistical models for dynamic multilayer networks, which can enhance quality\nin inference and prediction by efficiently borrowing information within each\nnetwork, across time, and between layers. Motivated by this gap, we develop a\nBayesian nonparametric model leveraging latent space representations. Our\nformulation characterizes the edge probabilities as a function of shared and\nlayer-specific actors positions in a latent space, with these positions\nchanging in time via Gaussian processes. This representation facilitates\ndimensionality reduction and incorporates different sources of information in\nthe observed data. In addition, we obtain tractable procedures for posterior\ncomputation, inference, and prediction. We provide theoretical results on the\nflexibility of our model. Our methods are tested on simulations and infection\nstudies monitoring dynamic face-to-face contacts among individuals in multiple\ndays, where we perform better than current methods in inference and prediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 12:34:47 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 19:04:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Mukherjee", "Nabanita", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1608.02257", "submitter": "Chang Liu", "authors": "Chang Liu, Bo Li, Yevgeniy Vorobeychik, Alina Oprea", "title": "Robust High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of supervised learning techniques has made them ubiquitous\nin research and practice. In high-dimensional settings, supervised learning\ncommonly relies on dimensionality reduction to improve performance and identify\nthe most important factors in predicting outcomes. However, the economic\nimportance of learning has made it a natural target for adversarial\nmanipulation of training data, which we term poisoning attacks. Prior\napproaches to dealing with robust supervised learning rely on strong\nassumptions about the nature of the feature matrix, such as feature\nindependence and sub-Gaussian noise with low variance. We propose an integrated\nmethod for robust regression that relaxes these assumptions, assuming only that\nthe feature matrix can be well approximated by a low-rank matrix. Our\ntechniques integrate improved robust low-rank matrix approximation and robust\nprinciple component regression, and yield strong performance guarantees.\nMoreover, we experimentally show that our methods significantly outperform\nstate of the art both in running time and prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 19:03:52 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 20:20:17 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Liu", "Chang", ""], ["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Oprea", "Alina", ""]]}, {"id": "1608.02280", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and W. D. Brinda", "title": "Statistical Guarantees for Estimating the Centers of a Two-component\n  Gaussian Mixture by EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a general method for analyzing the statistical accuracy of the EM\nalgorithm has been developed and applied to some simple latent variable models\n[Balakrishnan et al. 2016]. In that method, the basin of attraction for valid\ninitialization is required to be a ball around the truth. Using Stein's Lemma,\nwe extend these results in the case of estimating the centers of a\ntwo-component Gaussian mixture in $d$ dimensions. In particular, we\nsignificantly expand the basin of attraction to be the intersection of a half\nspace and a ball around the origin. If the signal-to-noise ratio is at least a\nconstant multiple of $ \\sqrt{d\\log d} $, we show that a random initialization\nstrategy is feasible.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 22:53:55 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Brinda", "W. D.", ""]]}, {"id": "1608.02315", "submitter": "Federico Schl\\\"uter", "authors": "Federico Schl\\\"uter, Yanela Strappa, Diego H. Milone, Facundo Bromberg", "title": "Blankets Joint Posterior score for learning Markov network structures", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.ijar.2017.10.018", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are extensively used to model complex sequential, spatial,\nand relational interactions in a wide range of fields. By learning the\nstructure of independences of a domain, more accurate joint probability\ndistributions can be obtained for inference tasks or, more directly, for\ninterpreting the most significant relations among the variables. Recently,\nseveral researchers have investigated techniques for automatically learning the\nstructure from data by obtaining the probabilistic maximum-a-posteriori\nstructure given the available data. However, all the approximations proposed\ndecompose the posterior of the whole structure into local sub-problems, by\nassuming that the posteriors of the Markov blankets of all the variables are\nmutually independent. In this work, we propose a scoring function for relaxing\nsuch assumption. The Blankets Joint Posterior score computes the joint\nposterior of structures as a joint distribution of the collection of its Markov\nblankets. Essentially, the whole posterior is obtained by computing the\nposterior of the blanket of each variable as a conditional distribution that\ntakes into account information from other blankets in the network. We show in\nour experimental results that the proposed approximation can improve the sample\ncomplexity of state-of-the-art scores when learning complex networks, where the\nindependence assumption between blanket variables is clearly incorrect.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 04:59:40 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 21:30:40 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Schl\u00fcter", "Federico", ""], ["Strappa", "Yanela", ""], ["Milone", "Diego H.", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1608.02341", "submitter": "Nicola Di Mauro", "authors": "Antonio Vergari and Nicola Di Mauro and Floriana Esposito", "title": "Towards Representation Learning with Tractable Probabilistic Models", "comments": "10 pages, submitted to ECML-PKDD 2016 Doctoral Consortium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic models learned as density estimators can be exploited in\nrepresentation learning beside being toolboxes used to answer inference queries\nonly. However, how to extract useful representations highly depends on the\nparticular model involved. We argue that tractable inference, i.e. inference\nthat can be computed in polynomial time, can enable general schemes to extract\nfeatures from black box models. We plan to investigate how Tractable\nProbabilistic Models (TPMs) can be exploited to generate embeddings by random\nquery evaluations. We devise two experimental designs to assess and compare\ndifferent TPMs as feature extractors in an unsupervised representation learning\nframework. We show some experimental results on standard image datasets by\napplying such a method to Sum-Product Networks and Mixture of Trees as\ntractable models generating embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 07:44:24 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Vergari", "Antonio", ""], ["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1608.02485", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Giulio Bottegal, Gianluigi Pillonetto", "title": "Boosting as a kernel-based method", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting combines weak (biased) learners to obtain effective learning\nalgorithms for classification and prediction. In this paper, we show a\nconnection between boosting and kernel-based methods, highlighting both\ntheoretical and practical applications. In the context of $\\ell_2$ boosting, we\nstart with a weak linear learner defined by a kernel $K$. We show that boosting\nwith this learner is equivalent to estimation with a special {\\it boosting\nkernel} that depends on $K$, as well as on the regression matrix, noise\nvariance, and hyperparameters. The number of boosting iterations is modeled as\na continuous hyperparameter, and fit along with other parameters using standard\ntechniques.\n  We then generalize the boosting kernel to a broad new class of boosting\napproaches for more general weak learners, including those based on the\n$\\ell_1$, hinge and Vapnik losses. The approach allows fast hyperparameter\ntuning for this general class, and has a wide range of applications, including\nrobust regression and classification. We illustrate some of these applications\nwith numerical examples on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 15:23:37 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 13:00:45 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Bottegal", "Giulio", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1608.02549", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi and Haris Vikalo", "title": "Sampling Requirements and Accelerated Schemes for Sparse Linear\n  Regression with Orthogonal Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of inferring a sparse vector from random linear\ncombinations of its components. We propose the Accelerated Orthogonal\nLeast-Squares (AOLS) algorithm that improves performance of the well-known\nOrthogonal Least-Squares (OLS) algorithm while requiring significantly lower\ncomputational costs. While OLS greedily selects columns of the coefficient\nmatrix that correspond to non-zero components of the sparse vector, AOLS\nemploys a novel computationally efficient procedure that speeds up the search\nby anticipating future selections via choosing $L$ columns in each step, where\n$L$ is an adjustable hyper-parameter. We analyze the performance of AOLS and\nestablish lower bounds on the probability of exact recovery for both noiseless\nand noisy random linear measurements. In the noiseless scenario, it is shown\nthat when the coefficients are samples from a Gaussian distribution, AOLS with\nhigh probability recovers a $k$-sparse $m$-dimensional sparse vector using\n${\\cal O}(k\\log \\frac{m}{k+L-1})$ measurements. Similar result is established\nfor the bounded-noise scenario where an additional condition on the smallest\nnonzero element of the unknown vector is required. The asymptotic sampling\ncomplexity of AOLS is lower than the asymptotic sampling complexity of the\nexisting sparse reconstruction algorithms. In simulations, AOLS is compared to\nstate-of-the-art sparse recovery techniques and shown to provide better\nperformance in terms of accuracy, running time, or both. Finally, we consider\nan application of AOLS to clustering high-dimensional data lying on the union\nof low-dimensional subspaces and demonstrate its superiority over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 18:36:30 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 21:46:31 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1608.02554", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi and Haris Vikalo", "title": "Sparse recovery via Orthogonal Least-Squares under presence of Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Orthogonal Least-Squares (OLS) algorithm for the recovery of\na $m$-dimensional $k$-sparse signal from a low number of noisy linear\nmeasurements. The Exact Recovery Condition (ERC) in bounded noisy scenario is\nestablished for OLS under certain condition on nonzero elements of the signal.\nThe new result also improves the existing guarantees for Orthogonal Matching\nPursuit (OMP) algorithm. In addition, This framework is employed to provide\nprobabilistic guarantees for the case that the coefficient matrix is drawn at\nrandom according to Gaussian or Bernoulli distribution where we exploit some\nconcentration properties. It is shown that under certain conditions, OLS\nrecovers the true support in $k$ iterations with high probability. This in turn\ndemonstrates that ${\\cal O}\\left(k\\log m\\right)$ measurements is sufficient for\nexact recovery of sparse signals via OLS.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 18:52:33 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1608.02658", "submitter": "Abbas Shojaee", "authors": "Abbas Shojaee, Isuru Ranasinghe, Alireza Ani", "title": "Revisiting Causality Inference in Memory-less Transition Networks", "comments": "This edition is improved with further details in the discussion\n  section and Figure 1. Other authors will be added in final revision; For\n  feedback, opinions, or questions please contact: abbas.shojaee@gmail.com OR\n  abbas.shojaee@yale.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods exist to infer causal networks from massive volumes of\nobservational data. However, almost all existing methods require a considerable\nlength of time series data to capture cause and effect relationships. In\ncontrast, memory-less transition networks or Markov Chain data, which refers to\none-step transitions to and from an event, have not been explored for causality\ninference even though such data is widely available. We find that causal\nnetwork can be inferred from characteristics of four unique distribution zones\naround each event. We call this Composition of Transitions and show that cause,\neffect, and random events exhibit different behavior in their compositions. We\napplied machine learning models to learn these different behaviors and to infer\ncausality. We name this new method Causality Inference using Composition of\nTransitions (CICT). To evaluate CICT, we used an administrative inpatient\nhealthcare dataset to set up a network of patients transitions between\ndifferent diagnoses. We show that CICT is highly accurate in inferring whether\nthe transition between a pair of events is causal or random and performs well\nin identifying the direction of causality in a bi-directional association.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:46:59 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 21:38:17 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 16:33:44 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Shojaee", "Abbas", ""], ["Ranasinghe", "Isuru", ""], ["Ani", "Alireza", ""]]}, {"id": "1608.02715", "submitter": "Truyen Tran", "authors": "Hoa Khanh Dam, Truyen Tran, Trang Pham", "title": "A deep language model for software code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing language models such as n-grams for software code often fail to\ncapture a long context where dependent code elements scatter far apart. In this\npaper, we propose a novel approach to build a language model for software code\nto address this particular issue. Our language model, partly inspired by human\nmemory, is built upon the powerful deep learning-based Long Short Term Memory\narchitecture that is capable of learning long-term dependencies which occur\nfrequently in software code. Results from our intrinsic evaluation on a corpus\nof Java projects have demonstrated the effectiveness of our language model.\nThis work contributes to realizing our vision for DeepSoft, an end-to-end,\ngeneric deep learning-based framework for modeling software and its development\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:16:42 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Dam", "Hoa Khanh", ""], ["Tran", "Truyen", ""], ["Pham", "Trang", ""]]}, {"id": "1608.02731", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Posterior Sampling for Reinforcement Learning Without Episodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a brief technical note to clarify some of the issues with applying\nthe application of the algorithm posterior sampling for reinforcement learning\n(PSRL) in environments without fixed episodes. In particular, this paper aims\nto:\n  - Review some of results which have been proven for finite horizon MDPs\n(Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic\nstructure (Gopalan et al 2014).\n  - Review similar results for optimistic algorithms in infinite horizon\nproblems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and\nSzepesvari 2011), with particular attention to the dynamic episode growth.\n  - Highlight the delicate technical issue which has led to a fault in the\nproof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We\npresent an explicit counterexample to this style of argument. Therefore, we\nsuggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead\nconsidered a conjecture, as it has no rigorous proof.\n  - Present pragmatic approaches to apply PSRL in infinite horizon problems. We\nconjecture that, under some additional assumptions, it will be possible to\nobtain bounds $O( \\sqrt{T} )$ even without episodic reset.\n  We hope that this note serves to clarify existing results in the field of\nreinforcement learning and provides interesting motivation for future work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 09:01:13 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1608.02732", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "On Lower Bounds for Regret in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a brief technical note to clarify the state of lower bounds on regret\nfor reinforcement learning. In particular, this paper:\n  - Reproduces a lower bound on regret for reinforcement learning, similar to\nthe result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).\n  - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett\nand Tewari 2009) does not hold using the standard techniques without further\nwork. We suggest that this result should instead be considered a conjecture as\nit has no rigorous proof.\n  - Suggests that the conjectured lower bound given by (Bartlett and Tewari\n2009) is incorrect and, in fact, it is possible to improve the scaling of the\nupper bound to match the weaker lower bounds presented in this paper.\n  We hope that this note serves to clarify existing results in the field of\nreinforcement learning and provides interesting motivation for future work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 09:02:01 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1608.02861", "submitter": "Oleksii Pokotylo", "authors": "Oleksii Pokotylo and Karl Mosler", "title": "Classification with the pot-pot plot", "comments": "The Online Appendix is available at\n  https://dl.dropboxusercontent.com/u/61551177/Classification_with_the_pot-pot_plot_Online_Appendix.pdf", "journal-ref": "Statistical Papers (2016)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for supervised classification that is based on\npotential functions. The potential of a class is defined as a kernel density\nestimate multiplied by the class's prior probability. The method transforms the\ndata to a potential-potential (pot-pot) plot, where each data point is mapped\nto a vector of potentials. Separation of the classes, as well as classification\nof new data points, is performed on this plot. For this, either the\n$\\alpha$-procedure ($\\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed.\nFor data that are generated from continuous distributions, these classifiers\nprove to be strongly Bayes-consistent. The potentials depend on the kernel and\nits bandwidth used in the density estimate. We investigate several variants of\nbandwidth selection, including joint and separate pre-scaling and a bandwidth\nregression approach. The new method is applied to benchmark data from the\nliterature, including simulated data sets as well as 50 sets of real data. It\ncompares favorably to known classification methods such as LDA, QDA, max kernel\ndensity estimates, $k$-NN, and $DD$-plot classification using depth functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 16:46:53 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Pokotylo", "Oleksii", ""], ["Mosler", "Karl", ""]]}, {"id": "1608.02902", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Martin J. Wainwright, Thomas A. Courtade", "title": "Linear Regression with an Unknown Permutation: Statistical and\n  Computational Limits", "comments": "To appear in part at the 2016 Allerton Conference on Control,\n  Communication and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a noisy linear observation model with an unknown permutation, based\non observing $y = \\Pi^* A x^* + w$, where $x^* \\in \\mathbb{R}^d$ is an unknown\nvector, $\\Pi^*$ is an unknown $n \\times n$ permutation matrix, and $w \\in\n\\mathbb{R}^n$ is additive Gaussian noise. We analyze the problem of permutation\nrecovery in a random design setting in which the entries of the matrix $A$ are\ndrawn i.i.d. from a standard Gaussian distribution, and establish sharp\nconditions on the SNR, sample size $n$, and dimension $d$ under which $\\Pi^*$\nis exactly and approximately recoverable. On the computational front, we show\nthat the maximum likelihood estimate of $\\Pi^*$ is NP-hard to compute, while\nalso providing a polynomial time algorithm when $d =1$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:24:50 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "1608.03022", "submitter": "Oleg Melnikov", "authors": "Oleg Melnikov, Loren H. Raun, Katherine B. Ensor", "title": "Dynamic Principal Component Analysis: Identifying the Relationship\n  between Multiple Air Pollutants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic nature of air quality chemistry and transport makes it difficult\nto identify the mixture of air pollutants for a region. In this study of air\nquality in the Houston metropolitan area we apply dynamic principal component\nanalysis (DPCA) to a normalized multivariate time series of daily concentration\nmeasurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009\nthrough December 31, 2011 for each of the 24 hours in a day. The resulting\ndynamic components are examined by hour across days for the 3 year period.\nDiurnal and seasonal patterns are revealed underlining times when DPCA performs\nbest and two principal components (PCs) explain most variability in the\nmultivariate series. DPCA is shown to be superior to static principal component\nanalysis (PCA) in discovery of linear relations among transformed pollutant\nmeasurements. DPCA captures the time-dependent correlation structure of the\nunderlying pollutants recorded at up to 34 monitoring sites in the region. In\nwinter mornings the first principal component (PC1) (mainly CO and NO2)\nexplains up to 70% of variability. Augmenting with the second principal\ncomponent (PC2) (mainly driven by SO2) the explained variability rises to 90%.\nIn the afternoon, O3 gains prominence in the second principal component. The\nseasonal profile of PCs' contribution to variance loses its distinction in the\nafternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability\nin ambient air data. DPCA provides a strategy for identifying the changing air\nquality profile for the region studied.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:51:17 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Melnikov", "Oleg", ""], ["Raun", "Loren H.", ""], ["Ensor", "Katherine B.", ""]]}, {"id": "1608.03023", "submitter": "Branislav Kveton", "authors": "Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade,\n  and Zheng Wen", "title": "Stochastic Rank-1 Bandits", "comments": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose stochastic rank-$1$ bandits, a class of online learning problems\nwhere at each step a learning agent chooses a pair of row and column arms, and\nreceives the product of their values as a reward. The main challenge of the\nproblem is that the individual values of the row and column are unobserved. We\nassume that these values are stochastic and drawn independently. We propose a\ncomputationally-efficient algorithm for solving our problem, which we call\nRank1Elim. We derive a $O((K + L) (1 / \\Delta) \\log n)$ upper bound on its\n$n$-step regret, where $K$ is the number of rows, $L$ is the number of columns,\nand $\\Delta$ is the minimum of the row and column gaps; under the assumption\nthat the mean row and column rewards are bounded away from zero. To the best of\nour knowledge, we present the first bandit algorithm that finds the maximum\nentry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \\Delta$, and\n$\\log n$. We also derive a nearly matching lower bound. Finally, we evaluate\nRank1Elim empirically on multiple problems. We observe that it leverages the\nstructure of our problems and can learn near-optimal solutions even if our\nmodeling assumptions are mildly violated.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:51:36 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 06:56:24 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 07:58:32 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Vernade", "Claire", ""], ["Wen", "Zheng", ""]]}, {"id": "1608.03045", "submitter": "Matey Neykov", "authors": "Matey Neykov, Junwei Lu, Han Liu", "title": "Combinatorial Inference for Graphical Models", "comments": "78 pages, 18 figures, 2 tables; to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of combinatorial inference problems for graphical\nmodels. Unlike classical statistical inference where the main interest is point\nestimation or parameter testing, combinatorial inference aims at testing the\nglobal structure of the underlying graph. Examples include testing the graph\nconnectivity, the presence of a cycle of certain size, or the maximum degree of\nthe graph. To begin with, we develop a unified theory for the fundamental\nlimits of a large family of combinatorial inference problems. We propose new\nconcepts including structural packing and buffer entropies to characterize how\nthe complexity of combinatorial graph structures impacts the corresponding\nminimax lower bounds. On the other hand, we propose a family of novel and\npractical structural testing algorithms to match the lower bounds. We provide\nthorough numerical results on both synthetic graphical models and brain\nnetworks to illustrate the usefulness of these proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 04:47:29 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 02:24:27 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 00:57:32 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Neykov", "Matey", ""], ["Lu", "Junwei", ""], ["Liu", "Han", ""]]}, {"id": "1608.03100", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Roy Frostig, John Duchi, Percy Liang", "title": "Estimation from Indirect Supervision with Linear Moments", "comments": "12 pages, 7 figures, extended and updated version of our paper\n  appearing in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In structured prediction problems where we have indirect supervision of the\noutput, maximum marginal likelihood faces two computational obstacles:\nnon-convexity of the objective and intractability of even a single gradient\ncomputation. In this paper, we bypass both obstacles for a class of what we\ncall linear indirectly-supervised problems. Our approach is simple: we solve a\nlinear system to estimate sufficient statistics of the model, which we then use\nto estimate parameters via convex optimization. We analyze the statistical\nproperties of our approach and show empirically that it is effective in two\nsettings: learning with local privacy constraints and learning from low-cost\ncount-based annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 09:19:07 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Frostig", "Roy", ""], ["Duchi", "John", ""], ["Liang", "Percy", ""]]}, {"id": "1608.03333", "submitter": "Kuan Liu", "authors": "Kuan Liu, Xing Shi, Anoop Kumar, Linhong Zhu, Prem Natarajan", "title": "Temporal Learning and Sequence Modeling for a Job Recommender System", "comments": "a shorter version in proceedings of RecSys Challenge 2016", "journal-ref": null, "doi": "10.1145/2987538.2987540", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our solution to the job recommendation task for RecSys Challenge\n2016. The main contribution of our work is to combine temporal learning with\nsequence modeling to capture complex user-item activity patterns to improve job\nrecommendations. First, we propose a time-based ranking model applied to\nhistorical observations and a hybrid matrix factorization over time re-weighted\ninteractions. Second, we exploit sequence properties in user-items activities\nand develop a RNN-based recommendation model. Our solution achieved 5$^{th}$\nplace in the challenge among more than 100 participants. Notably, the strong\nperformance of our RNN approach shows a promising new direction in employing\nsequence modeling for recommendation systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:48:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Liu", "Kuan", ""], ["Shi", "Xing", ""], ["Kumar", "Anoop", ""], ["Zhu", "Linhong", ""], ["Natarajan", "Prem", ""]]}, {"id": "1608.03339", "submitter": "Shaobo Lin", "authors": "Shao-Bo Lin, Xin Guo, Ding-Xuan Zhou", "title": "Distributed learning with regularized least squares", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed learning with the least squares regularization scheme in\na reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach,\nthe algorithm partitions a data set into disjoint data subsets, applies the\nleast squares regularization scheme to each data subset to produce an output\nfunction, and then takes an average of the individual output functions as a\nfinal global estimator or predictor. We show with error bounds in expectation\nin both the $L^2$-metric and RKHS-metric that the global output function of\nthis distributed learning is a good approximation to the algorithm processing\nthe whole data in one single machine. Our error bounds are sharp and stated in\na general setting without any eigenfunction assumption. The analysis is\nachieved by a novel second order decomposition of operator differences in our\nintegral operator approach. Even for the classical least squares regularization\nscheme in the RKHS associated with a general kernel, we give the best learning\nrate in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 01:20:23 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 07:45:26 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Lin", "Shao-Bo", ""], ["Guo", "Xin", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1608.03465", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok and B. T. Thomas Yeo", "title": "The Future of Data Analysis in the Neurosciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience is undergoing faster changes than ever before. Over 100 years\nour field qualitatively described and invasively manipulated single or few\norganisms to gain anatomical, physiological, and pharmacological insights. In\nthe last 10 years neuroscience spawned quantitative big-sample datasets on\nmicroanatomy, synaptic connections, optogenetic brain-behavior assays, and\nhigh-level cognition. While growing data availability and information\ngranularity have been amply discussed, we direct attention to a routinely\nneglected question: How will the unprecedented data richness shape data\nanalysis practices? Statistical reasoning is becoming more central to distill\nneurobiological knowledge from healthy and pathological brain recordings. We\nbelieve that large-scale data analysis will use more models that are\nnon-parametric, generative, mixing frequentist and Bayesian aspects, and\ngrounded in different statistical inferences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:43:21 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bzdok", "Danilo", ""], ["Yeo", "B. T. Thomas", ""]]}, {"id": "1608.03487", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Qihang Lin, Lijun Zhang", "title": "A Richer Theory of Convex Constrained Optimization with Reduced\n  Projections and Improved Rates", "comments": "This is the long version of our ICML 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on convex constrained optimization problems, where the\nsolution is subject to a convex inequality constraint. In particular, we aim at\nchallenging problems for which both projection into the constrained domain and\na linear optimization under the inequality constraint are time-consuming, which\nrender both projected gradient methods and conditional gradient methods (a.k.a.\nthe Frank-Wolfe algorithm) expensive. In this paper, we develop projection\nreduced optimization algorithms for both smooth and non-smooth optimization\nwith improved convergence rates under a certain regularity condition of the\nconstraint function. We first present a general theory of optimization with\nonly one projection. Its application to smooth optimization with only one\nprojection yields $O(1/\\epsilon)$ iteration complexity, which improves over the\n$O(1/\\epsilon^2)$ iteration complexity established before for non-smooth\noptimization and can be further reduced under strong convexity. Then we\nintroduce a local error bound condition and develop faster algorithms for\nnon-strongly convex optimization at the price of a logarithmic number of\nprojections. In particular, we achieve an iteration complexity of $\\widetilde\nO(1/\\epsilon^{2(1-\\theta)})$ for non-smooth optimization and $\\widetilde\nO(1/\\epsilon^{1-\\theta})$ for smooth optimization, where $\\theta\\in(0,1]$\nappearing the local error bound condition characterizes the functional local\ngrowth rate around the optimal solutions. Novel applications in solving the\nconstrained $\\ell_1$ minimization problem and a positive semi-definite\nconstrained distance metric learning problem demonstrate that the proposed\nalgorithms achieve significant speed-up compared with previous algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 14:48:18 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 14:10:23 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Yang", "Tianbao", ""], ["Lin", "Qihang", ""], ["Zhang", "Lijun", ""]]}, {"id": "1608.03530", "submitter": "Jason T. L. Wang", "authors": "Nihir Patel and Jason T. L. Wang", "title": "Semi-Supervised Prediction of Gene Regulatory Networks Using Machine\n  Learning Algorithms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of computational methods to predict gene regulatory networks (GRNs) from\ngene expression data is a challenging task. Many studies have been conducted\nusing unsupervised methods to fulfill the task; however, such methods usually\nyield low prediction accuracies due to the lack of training data. In this\narticle, we propose semi-supervised methods for GRN prediction by utilizing two\nmachine learning algorithms, namely support vector machines (SVM) and random\nforests (RF). The semi-supervised methods make use of unlabeled data for\ntraining. We investigate inductive and transductive learning approaches, both\nof which adopt an iterative procedure to obtain reliable negative training data\nfrom the unlabeled data. We then apply our semi-supervised methods to gene\nexpression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate\nthe performance of our methods using the expression data. Our analysis\nindicated that the transductive learning approach outperformed the inductive\nlearning approach for both organisms. However, there was no conclusive\ndifference identified in the performance of SVM and RF. Experimental results\nalso showed that the proposed semi-supervised methods performed better than\nexisting supervised methods for both organisms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 16:52:03 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Patel", "Nihir", ""], ["Wang", "Jason T. L.", ""]]}, {"id": "1608.03532", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati, Rade Stanojevic", "title": "QPass: a Merit-based Evaluation of Soccer Passes", "comments": "2016 ACM KDD Workshop on Large-Scale Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of soccer players' passing ability focuses on\ndescriptive statistics without considering the players' real contribution to\nthe passing and ball possession strategy of their team. Which player is able to\nhelp the build-up of an attack, or to maintain the possession of the ball? We\nintroduce a novel methodology called QPass to answer questions like these\nquantitatively. Based on the analysis of an entire season, we rank the players\nbased on the intrinsic value of their passes using QPass. We derive an album of\npass trajectories for different gaming styles. Our methodology reveals a quite\ncounterintuitive paradigm: losing the ball possession could lead to better\nchances to win a game.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:54:57 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Stanojevic", "Rade", ""]]}, {"id": "1608.03533", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Samaneh Ebrahimi and Kamran Paynabar", "title": "Sequence Graph Transform (SGT): A Feature Embedding Function for\n  Sequence Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence feature embedding is a challenging task due to un-structuredness of\nsequences -- arbitrary strings of arbitrary length. Existing methods are\nefficient in extracting short-term dependencies but typically suffer from\ncomputation issues for the long-term. Sequence Graph Transform (SGT), a feature\nembedding function, that can extract varying amount of short- to long-term\ndependencies without increasing the computation is proposed. SGT's properties\nare analytically proved for interpretation under normal and uniform\ndistribution assumptions. SGT features yield significantly superior results in\nsequence clustering and classification with higher accuracy and lower\ncomputation as compared to the existing methods, including the state-of-the-art\nsequence/string Kernels and LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 16:59:19 GMT"}, {"version": "v10", "created": "Thu, 27 Feb 2020 19:47:52 GMT"}, {"version": "v11", "created": "Wed, 4 Mar 2020 14:54:16 GMT"}, {"version": "v12", "created": "Fri, 8 May 2020 20:03:02 GMT"}, {"version": "v13", "created": "Thu, 29 Oct 2020 11:49:41 GMT"}, {"version": "v14", "created": "Tue, 18 May 2021 00:03:21 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 14:01:03 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 20:03:41 GMT"}, {"version": "v4", "created": "Wed, 28 Sep 2016 00:20:49 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 05:04:59 GMT"}, {"version": "v6", "created": "Sun, 27 Nov 2016 01:43:12 GMT"}, {"version": "v7", "created": "Wed, 30 Nov 2016 06:35:26 GMT"}, {"version": "v8", "created": "Tue, 31 Jan 2017 03:50:58 GMT"}, {"version": "v9", "created": "Sun, 30 Apr 2017 07:21:43 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ranjan", "Chitta", ""], ["Ebrahimi", "Samaneh", ""], ["Paynabar", "Kamran", ""]]}, {"id": "1608.03544", "submitter": "Claudio Gentile", "authors": "Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou,\n  Evans Etrue, Giovanni Zappella", "title": "On Context-Dependent Clustering of Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel cluster-of-bandit algorithm CAB for collaborative\nrecommendation tasks that implements the underlying feedback sharing mechanism\nby estimating the neighborhood of users in a context-dependent manner. CAB\nmakes sharp departures from the state of the art by incorporating collaborative\neffects into inference as well as learning processes in a manner that\nseamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\nprove regret bounds under various assumptions on the data, which exhibit a\ncrisp dependence on the expected number of clusters over the users, a natural\nmeasure of the statistical difficulty of the learning task. Experiments on\nproduction and real-world datasets show that CAB offers significantly increased\nprediction performance against a representative pool of state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 14:13:28 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:16:22 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Kar", "Purushottam", ""], ["Karatzoglou", "Alexandros", ""], ["Etrue", "Evans", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1608.03585", "submitter": "Matthias Poloczek", "authors": "Matthias Poloczek, Jialei Wang, and Peter I. Frazier", "title": "Warm Starting Bayesian Optimization", "comments": "To Appear in the Proc. of the 2016 Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for warm-starting Bayesian optimization, that reduces\nthe solution time required to solve an optimization problem that is one in a\nsequence of related problems. This is useful when optimizing the output of a\nstochastic simulator that fails to provide derivative information, for which\nBayesian optimization methods are well-suited. Solving sequences of related\noptimization problems arises when making several business decisions using one\noptimization model and input data collected over different time periods or\nmarkets. While many gradient-based methods can be warm started by initiating\noptimization at the solution to the previous problem, this warm start approach\ndoes not apply to Bayesian optimization methods, which carry a full metamodel\nof the objective function from iteration to iteration. Our approach builds a\njoint statistical model of the entire collection of related objective\nfunctions, and uses a value of information calculation to recommend points to\nevaluate.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:56:27 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Poloczek", "Matthias", ""], ["Wang", "Jialei", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1608.03639", "submitter": "Truyen Tran", "authors": "Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Faster Training of Very Deep Networks Via p-Norm Gates", "comments": "To appear in ICPR'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major contributing factor to the recent advances in deep neural networks is\nstructural units that let sensory information and gradients to propagate\neasily. Gating is one such structure that acts as a flow control. Gates are\nemployed in many recent state-of-the-art recurrent models such as LSTM and GRU,\nand feedforward models such as Residual Nets and Highway Networks. This enables\nlearning in very deep networks with hundred layers and helps achieve\nrecord-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP\n(e.g., machine translation with GRU). However, there is limited work in\nanalysing the role of gating in the learning process. In this paper, we propose\na flexible $p$-norm gating scheme, which allows user-controllable flow and as a\nconsequence, improve the learning speed. This scheme subsumes other existing\ngating schemes, including those in GRU, Highway Networks and Residual Nets as\nspecial cases. Experiments on large sequence and vector datasets demonstrate\nthat the proposed gating scheme helps improve the learning speed significantly\nwithout extra overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 23:48:44 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1608.03643", "submitter": "Santosh Vempala", "authors": "Ravi Kannan and Santosh Vempala", "title": "Chi-squared Amplification: Identifying Hidden Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general hidden hubs model: an $n \\times n$ random\nmatrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows\noutside $S$ are generated from the probability distribution $p_0 \\sim\nN(0,\\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated\nfrom $p_1 \\sim N(0,\\sigma_1^2)$, $\\sigma_1>\\sigma_0$, and the rest of the\nentries from $p_0$. The problem is to identify the high-degree hubs\nefficiently. This model includes and significantly generalizes the planted\nGaussian Submatrix Model, where the special entries are all in a $k \\times k$\nsubmatrix. There are two well-known barriers: if $k\\geq c\\sqrt{n\\ln n}$, just\nthe row sums are sufficient to find $S$ in the general model. For the submatrix\nproblem, this can be improved by a $\\sqrt{\\ln n}$ factor to $k \\ge c\\sqrt{n}$\nby spectral methods or combinatorial methods. In the variant with $p_0=\\pm 1$\n(with probability $1/2$ each) and $p_1\\equiv 1$, neither barrier has been\nbroken.\n  We give a polynomial-time algorithm to identify all the hidden hubs with high\nprobability for $k \\ge n^{0.5-\\delta}$ for some $\\delta >0$, when\n$\\sigma_1^2>2\\sigma_0^2$. The algorithm extends to the setting where planted\nentries might have different variances each at least as large as $\\sigma_1^2$.\nWe also show a nearly matching lower bound: for $\\sigma_1^2 \\le 2\\sigma_0^2$,\nthere is no polynomial-time Statistical Query algorithm for distinguishing\nbetween a matrix whose entries are all from $N(0,\\sigma_0^2)$ and a matrix with\n$k=n^{0.5-\\delta}$ hidden hubs for any $\\delta >0$. The lower bound as well as\nthe algorithm are related to whether the chi-squared distance of the two\ndistributions diverges. At the critical value $\\sigma_1^2=2\\sigma_0^2$, we show\nthat the general hidden hubs problem can be solved for $k\\geq c\\sqrt n(\\ln\nn)^{1/4}$, improving on the naive row sum-based method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:36:42 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 16:31:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kannan", "Ravi", ""], ["Vempala", "Santosh", ""]]}, {"id": "1608.03665", "submitter": "Wei Wen", "authors": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li", "title": "Learning Structured Sparsity in Deep Neural Networks", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High demand for computation resources severely hinders deployment of\nlarge-scale Deep Neural Networks (DNN) in resource constrained devices. In this\nwork, we propose a Structured Sparsity Learning (SSL) method to regularize the\nstructures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.\nSSL can: (1) learn a compact structure from a bigger DNN to reduce computation\ncost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently\naccelerate the DNNs evaluation. Experimental results show that SSL achieves on\naverage 5.1x and 3.1x speedups of convolutional layer computation of AlexNet\nagainst CPU and GPU, respectively, with off-the-shelf libraries. These speedups\nare about twice speedups of non-structured sparsity; (3) regularize the DNN\nstructure to improve classification accuracy. The results show that for\nCIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual\nNetwork (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,\nwhich is still slightly higher than that of original ResNet with 32 layers. For\nAlexNet, structure regularization by SSL also reduces the error by around ~1%.\nOpen source code is in https://github.com/wenwei202/caffe/tree/scnn\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 03:20:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 20:46:15 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 05:58:48 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 04:03:41 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Wen", "Wei", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1608.03811", "submitter": "Joani Mitro", "authors": "Joani Mitro", "title": "Content-based image retrieval tutorial", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper functions as a tutorial for individuals interested to enter the\nfield of information retrieval but wouldn't know where to begin from. It\ndescribes two fundamental yet efficient image retrieval techniques, the first\nbeing k - nearest neighbors (knn) and the second support vector machines(svm).\nThe goal is to provide the reader with both the theoretical and practical\naspects in order to acquire a better understanding. Along with this tutorial we\nhave also developed the equivalent software1 using the MATLAB environment in\norder to illustrate the techniques, so that the reader can have a hands-on\nexperience.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 14:40:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Mitro", "Joani", ""]]}, {"id": "1608.03817", "submitter": "Yin Cheng Ng", "authors": "Yin Cheng Ng, Pawel Chilinski, Ricardo Silva", "title": "Scaling Factorial Hidden Markov Models: Stochastic Variational Inference\n  without Messages", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial Hidden Markov Models (FHMMs) are powerful models for sequential\ndata but they do not scale well with long sequences. We propose a scalable\ninference and learning algorithm for FHMMs that draws on ideas from the\nstochastic variational inference, neural network and copula literatures. Unlike\nexisting approaches, the proposed algorithm requires no message passing\nprocedure among latent variables and can be distributed to a network of\ncomputers to speed up learning. Our experiments corroborate that the proposed\nalgorithm does not introduce further approximation bias compared to the proven\nstructured mean-field algorithm, and achieves better performance with long\nsequences and large FHMMs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 15:02:10 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 16:51:52 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 15:42:25 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Ng", "Yin Cheng", ""], ["Chilinski", "Pawel", ""], ["Silva", "Ricardo", ""]]}, {"id": "1608.03928", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Hybrid Jacobian and Gauss-Seidel proximal block coordinate update\n  methods for linearly constrained convex programming", "comments": "Accepted in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid development of block coordinate update\n(BCU) methods, which are particularly suitable for problems involving\nlarge-sized data and/or variables. In optimization, BCU first appears as the\ncoordinate descent method that works well for smooth problems or those with\nseparable nonsmooth terms and/or separable constraints. As nonseparable\nconstraints exist, BCU can be applied under primal-dual settings.\n  In the literature, it has been shown that for weakly convex problems with\nnonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may\nfail to converge and that with fully Jacobian rule can converge sublinearly.\nHowever, empirically the method with Jacobian update is usually slower than\nthat with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid\nJacobian and Gauss-Seidel BCU method for solving linearly constrained\nmulti-block structured convex programming, where the objective may have a\nnonseparable quadratic term and separable nonsmooth terms. At each primal block\nvariable update, the method approximates the augmented Lagrangian function at\nan affine combination of the previous two iterates, and the affinely mixing\nmatrix with desired nice properties can be chosen through solving a\nsemidefinite programming. We show that the hybrid method enjoys the theoretical\nconvergence guarantee as Jacobian BCU. In addition, we numerically demonstrate\nthat the method can perform as well as Gauss-Seidel method and better than a\nrecently proposed randomized primal-dual BCU method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 00:30:32 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 13:48:07 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1608.03974", "submitter": "Giovanni Montana", "authors": "Rudra P K Poudel and Pablo Lamata and Giovanni Montana", "title": "Recurrent Fully Convolutional Neural Networks for Multi-slice MRI\n  Cardiac Segmentation", "comments": "MICCAI Workshop RAMBO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cardiac magnetic resonance imaging, fully-automatic segmentation of the\nheart enables precise structural and functional measurements to be taken, e.g.\nfrom short-axis MR images of the left-ventricle. In this work we propose a\nrecurrent fully-convolutional network (RFCN) that learns image representations\nfrom the full stack of 2D slices and has the ability to leverage inter-slice\nspatial dependences through internal memory units. RFCN combines anatomical\ndetection and segmentation into a single architecture that is trained\nend-to-end thus significantly reducing computational time, simplifying the\nsegmentation pipeline, and potentially enabling real-time applications. We\nreport on an investigation of RFCN using two datasets, including the publicly\navailable MICCAI 2009 Challenge dataset. Comparisons have been carried out\nbetween fully convolutional networks and deep restricted Boltzmann machines,\nincluding a recurrent version that leverages inter-slice spatial correlation.\nOur studies suggest that RFCN produces state-of-the-art results and can\nsubstantially improve the delineation of contours near the apex of the heart.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 11:19:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Poudel", "Rudra P K", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1608.04037", "submitter": "Davi Frossard", "authors": "Davi E. N. Frossard, Igor O. Nunes, Renato A. Krohling", "title": "An approach to dealing with missing values in heterogeneous data using\n  k-nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques such as clusterization, neural networks and decision making\nusually rely on algorithms that are not well suited to deal with missing\nvalues. However, real world data frequently contains such cases. The simplest\nsolution is to either substitute them by a best guess value or completely\ndisregard the missing values. Unfortunately, both approaches can lead to biased\nresults. In this paper, we propose a technique for dealing with missing values\nin heterogeneous data using imputation based on the k-nearest neighbors\nalgorithm. It can handle real (which we refer to as crisp henceforward),\ninterval and fuzzy data. The effectiveness of the algorithm is tested on\nseveral datasets and the numerical results are promising.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 23:45:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Frossard", "Davi E. N.", ""], ["Nunes", "Igor O.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1608.04048", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Jiliang Tang, Jose Lugo-Martinez, Ermin Hodzic, Raunak\n  Shrestha, Avishek Saha, Hua Ouyang, Dawei Yin, Hiroshi Mamitsuka, Cenk\n  Sahinalp, Predrag Radivojac, Filippo Menczer, Yi Chang", "title": "Ultra High-Dimensional Nonlinear Feature Selection for Big Biological\n  Data", "comments": "Substantially improved version of arXiv:1411.2331", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods are used to discover complex nonlinear relationships\nin biological and medical data. However, sophisticated learning models are\ncomputationally unfeasible for data with millions of features. Here we\nintroduce the first feature selection method for nonlinear learning problems\nthat can scale up to large, ultra-high dimensional biological data. More\nspecifically, we scale up the novel Hilbert-Schmidt Independence Criterion\nLasso (HSIC Lasso) to handle millions of features with tens of thousand\nsamples. The proposed method is guaranteed to find an optimal subset of\nmaximally predictive features with minimal redundancy, yielding higher\npredictive power and improved interpretability. Its effectiveness is\ndemonstrated through applications to classify phenotypes based on module\nexpression in human prostate cancer patients and to detect enzymes among\nprotein structures. We achieve high accuracy with as few as 20 out of one\nmillion features --- a dimensionality reduction of 99.998%. Our algorithm can\nbe implemented on commodity cloud computing platforms. The dramatic reduction\nof features may lead to the ubiquitous deployment of sophisticated prediction\nmodels in mobile health care applications.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 01:56:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Yamada", "Makoto", ""], ["Tang", "Jiliang", ""], ["Lugo-Martinez", "Jose", ""], ["Hodzic", "Ermin", ""], ["Shrestha", "Raunak", ""], ["Saha", "Avishek", ""], ["Ouyang", "Hua", ""], ["Yin", "Dawei", ""], ["Mamitsuka", "Hiroshi", ""], ["Sahinalp", "Cenk", ""], ["Radivojac", "Predrag", ""], ["Menczer", "Filippo", ""], ["Chang", "Yi", ""]]}, {"id": "1608.04063", "submitter": "Hyun-Chul Kim", "authors": "Hyun-Chul Kim", "title": "Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest\n  Neighbor Classification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-nearest neighbor classification method ($k$-NNC) is one of the\nsimplest nonparametric classification methods. The mutual $k$-NN classification\nmethod (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We\npropose another variant of $k$-NNC, the symmetric $k$-NN classification method\n(S$k$NNC) based on both mutual neighborship and one-sided neighborship. The\nperformance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of\n$k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be\nperformed based on Bayesian mutual and symmetric $k$-NN regression methods with\nthe selection schemes for the parameter $k$. Bayesian mutual and symmetric\n$k$-NN regression methods are based on Gaussian process models, and it turns\nout that they can do M$k$NN and S$k$NN classification with new encodings of\ntarget values (class labels). The simulation results show that the proposed\nmethods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the\nparameter $k$ selected by the leave-one-out cross validation method not only\nfor an artificial data set but also for real world data sets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 06:01:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Kim", "Hyun-Chul", ""]]}, {"id": "1608.04089", "submitter": "Kerry Zhang", "authors": "Kerry Zhang, Jussi Karlgren, Cheng Zhang, Jens Lagergren", "title": "Viewpoint and Topic Modeling of Current Events", "comments": "16 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple sides to every story, and while statistical topic models\nhave been highly successful at topically summarizing the stories in corpora of\ntext documents, they do not explicitly address the issue of learning the\ndifferent sides, the viewpoints, expressed in the documents. In this paper, we\nshow how these viewpoints can be learned completely unsupervised and\nrepresented in a human interpretable form. We use a novel approach of applying\nCorrLDA2 for this purpose, which learns topic-viewpoint relations that can be\nused to form groups of topics, where each group represents a viewpoint. A\ncorpus of documents about the Israeli-Palestinian conflict is then used to\ndemonstrate how a Palestinian and an Israeli viewpoint can be learned. By\nleveraging the magnitudes and signs of the feature weights of a linear SVM, we\nintroduce a principled method to evaluate associations between topics and\nviewpoints. With this, we demonstrate, both quantitatively and qualitatively,\nthat the learned topic groups are contextually coherent, and form consistently\ncorrect topic-viewpoint associations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 11:36:52 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Zhang", "Kerry", ""], ["Karlgren", "Jussi", ""], ["Zhang", "Cheng", ""], ["Lagergren", "Jens", ""]]}, {"id": "1608.04109", "submitter": "Pavlo Mozharovskyi", "authors": "Oleksii Pokotylo, Pavlo Mozharovskyi, Rainer Dyckerhoff", "title": "Depth and depth-based classification with R-package ddalpha", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the seminal idea of Tukey, data depth is a function that measures\nhow close an arbitrary point of the space is located to an implicitly defined\ncenter of a data cloud. Having undergone theoretical and computational\ndevelopments, it is now employed in numerous applications with classification\nbeing the most popular one. The R-package ddalpha is a software directed to\nfuse experience of the applicant with recent achievements in the area of data\ndepth and depth-based classification.\n  ddalpha provides an implementation for exact and approximate computation of\nmost reasonable and widely applied notions of data depth. These can be further\nused in the depth-based multivariate and functional classifiers implemented in\nthe package, where the $DD\\alpha$-procedure is in the main focus. The package\nis expandable with user-defined custom depth methods and separators. The\nimplemented functions for depth visualization and the built-in benchmark\nprocedures may also serve to provide insights into the geometry of the data and\nthe quality of pattern recognition.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 14:44:17 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Pokotylo", "Oleksii", ""], ["Mozharovskyi", "Pavlo", ""], ["Dyckerhoff", "Rainer", ""]]}, {"id": "1608.04123", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Mark A. van de Wiel, Wessel N. van Wieringen", "title": "The Spectral Condition Number Plot for Regularization Parameter\n  Determination", "comments": "41 pages, 7 figures, includes supplementary material", "journal-ref": "Computational Statistics, 35(2):629-646, 2020", "doi": "10.1007/s00180-019-00912-z", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications ask for the estimation of a covariance\n(or precision) matrix in settings where the number of variables is larger than\nthe number of observations. There exists a broad class of ridge-type estimators\nthat employs regularization to cope with the subsequent singularity of the\nsample covariance matrix. These estimators depend on a penalty parameter and\nchoosing its value can be hard, in terms of being computationally unfeasible or\ntenable only for a restricted set of ridge-type estimators. Here we introduce a\nsimple graphical tool, the spectral condition number plot, for informed\nheuristic penalty parameter selection. The proposed tool is computationally\nfriendly and can be employed for the full class of ridge-type covariance\n(precision) estimators.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 18:50:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["van de Wiel", "Mark A.", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "1608.04236", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:14:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 08:06:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1608.04245", "submitter": "Mike Gartrell", "authors": "Mike Gartrell, Ulrich Paquet, Noam Koenigstein", "title": "The Bayesian Low-Rank Determinantal Point Process Mixture Model", "comments": "9 pages, 6 figures. This article draws heavily from arXiv:1602.05436", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are an elegant model for encoding\nprobabilities over subsets, such as shopping baskets, of a ground set, such as\nan item catalog. They are useful for a number of machine learning tasks,\nincluding product recommendation. DPPs are parametrized by a positive\nsemi-definite kernel matrix. Recent work has shown that using a low-rank\nfactorization of this kernel provides remarkable scalability improvements that\nopen the door to training on large-scale datasets and computing online\nrecommendations, both of which are infeasible with standard DPP models that use\na full-rank kernel. In this paper we present a low-rank DPP mixture model that\nallows us to represent the latent structure present in observed subsets as a\nmixture of a number of component low-rank DPPs, where each component DPP is\nresponsible for representing a portion of the observed data. The mixture model\nallows us to effectively address the capacity constraints of the low-rank DPP\nmodel. We present an efficient and scalable Markov Chain Monte Carlo (MCMC)\nlearning algorithm for our model that uses Gibbs sampling and stochastic\ngradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several\nreal-world product recommendation datasets, we show that our low-rank DPP\nmixture model provides substantially better predictive performance than is\npossible with a single low-rank or full-rank DPP, and significantly better\nperformance than several other competing recommendation methods in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:42:51 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 10:41:32 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Gartrell", "Mike", ""], ["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1608.04290", "submitter": "Xiao Fu", "authors": "Xiao Fu, Kejun Huang, Bo Yang, Wing-Kin Ma, Nicholas D. Sidiropoulos", "title": "Robust Volume Minimization-Based Matrix Factorization for Remote Sensing\n  and Document Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2602800", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers \\emph{volume minimization} (VolMin)-based structured\nmatrix factorization (SMF). VolMin is a factorization criterion that decomposes\na given data matrix into a basis matrix times a structured coefficient matrix\nvia finding the minimum-volume simplex that encloses all the columns of the\ndata matrix. Recent work showed that VolMin guarantees the identifiability of\nthe factor matrices under mild conditions that are realistic in a wide variety\nof applications. This paper focuses on both theoretical and practical aspects\nof VolMin. On the theory side, exact equivalence of two independently developed\nsufficient conditions for VolMin identifiability is proven here, thereby\nproviding a more comprehensive understanding of this aspect of VolMin. On the\nalgorithm side, computational complexity and sensitivity to outliers are two\nkey challenges associated with real-world applications of VolMin. These are\naddressed here via a new VolMin algorithm that handles volume regularization in\na computationally simple way, and automatically detects and {iteratively\ndownweights} outliers, simultaneously. Simulations and real-data experiments\nusing a remotely sensed hyperspectral image and the Reuters document corpus are\nemployed to showcase the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 14:51:10 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Yang", "Bo", ""], ["Ma", "Wing-Kin", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1608.04331", "submitter": "Jared Culbertson", "authors": "Jared Culbertson, Dan P. Guralnik, Jakob Hansen, Peter F. Stiller", "title": "Consistency constraints for overlapping data clustering", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine overlapping clustering schemes with functorial constraints, in the\nspirit of Carlsson--Memoli. This avoids issues arising from the chaining\nrequired by partition-based methods. Our principal result shows that any\nclustering functor is naturally constrained to refine single-linkage clusters\nand be refined by maximal-linkage clusters. We work in the context of metric\nspaces with non-expansive maps, which is appropriate for modeling data\nprocessing which does not increase information content.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:12:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Culbertson", "Jared", ""], ["Guralnik", "Dan P.", ""], ["Hansen", "Jakob", ""], ["Stiller", "Peter F.", ""]]}, {"id": "1608.04374", "submitter": "Anthony Caterini", "authors": "Anthony L. Caterini, Dong Eui Chang", "title": "A Geometric Framework for Convolutional Neural Networks", "comments": "Added proofs and algorithms that were missing from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a geometric framework for neural networks is proposed. This\nframework uses the inner product space structure underlying the parameter set\nto perform gradient descent not in a component-based form, but in a\ncoordinate-free manner. Convolutional neural networks are described in this\nframework in a compact form, with the gradients of standard --- and\nhigher-order --- loss functions calculated for each layer of the network. This\napproach can be applied to other network structures and provides a basis on\nwhich to create new networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 19:38:35 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 17:45:06 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Caterini", "Anthony L.", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1608.04414", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension\n  Strikes Back", "comments": "Added illustrations of functions used in some of the constructions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic convex optimization the goal is to minimize a convex function\n$F(x) \\doteq {\\mathbf E}_{{\\mathbf f}\\sim D}[{\\mathbf f}(x)]$ over a convex set\n$\\cal K \\subset {\\mathbb R}^d$ where $D$ is some unknown distribution and each\n$f(\\cdot)$ in the support of $D$ is convex over $\\cal K$. The optimization is\ncommonly based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $D$. A standard\napproach to such problems is empirical risk minimization (ERM) that optimizes\n$F_S(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$. Here we consider the question\nof how many samples are necessary for ERM to succeed and the closely related\nquestion of uniform convergence of $F_S$ to $F$ over $\\cal K$. We demonstrate\nthat in the standard $\\ell_p/\\ell_q$ setting of Lipschitz-bounded functions\nover a $\\cal K$ of bounded radius, ERM requires sample size that scales\nlinearly with the dimension $d$. This nearly matches standard upper bounds and\nimproves on $\\Omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting by\nShalev-Shwartz et al. (2009). In stark contrast, these problems can be solved\nusing dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and\n$\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches. We\nfurther show that our lower bound applies even if the functions in the support\nof $D$ are smooth and efficiently computable and even if an $\\ell_1$\nregularization term is added. Finally, we demonstrate that for a more general\nclass of bounded-range (but not Lipschitz-bounded) stochastic convex programs\nan infinite gap appears already in dimension 2.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 21:19:51 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 00:46:58 GMT"}, {"version": "v3", "created": "Mon, 26 Dec 2016 06:37:48 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1608.04471", "submitter": "Dilin Wang", "authors": "Qiang Liu and Dilin Wang", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference\n  Algorithm", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose variational inference algorithm that forms a\nnatural counterpart of gradient descent for optimization. Our method\niteratively transports a set of particles to match the target distribution, by\napplying a form of functional gradient descent that minimizes the KL\ndivergence. Empirical studies are performed on various real world models and\ndatasets, on which our method is competitive with existing state-of-the-art\nmethods. The derivation of our method is based on a new theoretical result that\nconnects the derivative of KL divergence under smooth transforms with Stein's\nidentity and a recently proposed kernelized Stein discrepancy, which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 03:24:20 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 05:13:47 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 17:31:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Qiang", ""], ["Wang", "Dilin", ""]]}, {"id": "1608.04478", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke", "title": "A Geometrical Approach to Topic Model Estimation", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 04:31:52 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Ke", "Zheng Tracy", ""]]}, {"id": "1608.04481", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Lecture Notes on Randomized Linear Algebra", "comments": "188 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are lecture notes that are based on the lectures from a class I taught\non the topic of Randomized Linear Algebra (RLA) at UC Berkeley during the Fall\n2013 semester.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 04:55:26 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1608.04550", "submitter": "Joachim Van Der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Dirk Deschrijver and Tom\n  Dhaene", "title": "Fast Calculation of the Knowledge Gradient for Optimization of\n  Deterministic Engineering Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel efficient method for computing the Knowledge-Gradient policy for\nContinuous Parameters (KGCP) for deterministic optimization is derived. The\ndifferences with Expected Improvement (EI), a popular choice for Bayesian\noptimization of deterministic engineering simulations, are explored. Both\npolicies and the Upper Confidence Bound (UCB) policy are compared on a number\nof benchmark functions including a problem from structural dynamics. It is\nempirically shown that KGCP has similar performance as the EI policy for many\nproblems, but has better convergence properties for complex (multi-modal)\noptimization problems as it emphasizes more on exploration when the model is\nconfident about the shape of optimal regions. In addition, the relationship\nbetween Maximum Likelihood Estimation (MLE) and slice sampling for estimation\nof the hyperparameters of the underlying models, and the complexity of the\nproblem at hand, is studied.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 11:26:25 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Deschrijver", "Dirk", ""], ["Dhaene", "Tom", ""]]}, {"id": "1608.04581", "submitter": "Jim Jing-Yan Wang", "authors": "Ru-Ze Liang, Wei Xie, Weizhi Li, Hongqi Wang, Jim Jing-Yan Wang, Lisa\n  Taylor", "title": "A novel transfer learning method based on common space mapping and\n  weighted domain matching", "comments": "arXiv admin note: text overlap with arXiv:1605.06673", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning framework for the problem of\ndomain transfer learning. We map the data of two domains to one single common\nspace, and learn a classifier in this common space. Then we adapt the common\nclassifier to the two domains by adding two adaptive functions to it\nrespectively. In the common space, the target domain data points are weighted\nand matched to the target domain in term of distributions. The weighting terms\nof source domain data points and the target domain classification responses are\nalso regularized by the local reconstruction coefficients. The novel transfer\nlearning framework is evaluated over some benchmark cross-domain data sets, and\nit outperforms the existing state-of-the-art transfer learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 13:17:51 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Liang", "Ru-Ze", ""], ["Xie", "Wei", ""], ["Li", "Weizhi", ""], ["Wang", "Hongqi", ""], ["Wang", "Jim Jing-Yan", ""], ["Taylor", "Lisa", ""]]}, {"id": "1608.04585", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Vladislav Ishimtsev", "title": "Conformalized density- and distance-based anomaly detection in\n  time-series data", "comments": "9 pages, 3 figures, conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies (unusual patterns) in time-series data give essential, and often\nactionable information in critical situations. Examples can be found in such\nfields as healthcare, intrusion detection, finance, security and flight safety.\nIn this paper we propose new conformalized density- and distance-based anomaly\ndetection algorithms for a one-dimensional time-series data. The algorithms use\na combination of a feature extraction method, an approach to assess a score\nwhether a new observation differs significantly from a previously observed\ndata, and a probabilistic interpretation of this score based on the conformal\nparadigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 13:32:05 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Ishimtsev", "Vladislav", ""]]}, {"id": "1608.04615", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Mark Sendak, C. Blake Cameron, Katherine Heller", "title": "Scalable Modeling of Multivariate Longitudinal Data for Prediction of\n  Chronic Kidney Disease Progression", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the future trajectory of a disease is an important challenge\nfor personalized medicine and population health management. However, many\ncomplex chronic diseases exhibit large degrees of heterogeneity, and\nfurthermore there is not always a single readily available biomarker to\nquantify disease severity. Even when such a clinical variable exists, there are\noften additional related biomarkers routinely measured for patients that may\nbetter inform the predictions of their future disease state. To this end, we\npropose a novel probabilistic generative model for multivariate longitudinal\ndata that captures dependencies between multivariate trajectories. We use a\nGaussian process based regression model for each individual trajectory, and\nbuild off ideas from latent class models to induce dependence between their\nmean functions. We fit our method using a scalable variational inference\nalgorithm to a large dataset of longitudinal electronic patient health records,\nand find that it improves dynamic predictions compared to a recent state of the\nart method. Our local accountable care organization then uses the model\npredictions during chart reviews of high risk patients with chronic kidney\ndisease.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:30:07 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Futoma", "Joseph", ""], ["Sendak", "Mark", ""], ["Cameron", "C. Blake", ""], ["Heller", "Katherine", ""]]}, {"id": "1608.04636", "submitter": "Julie Nutini", "authors": "Hamed Karimi, Julie Nutini and Mark Schmidt", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the\n  Polyak-\\L{}ojasiewicz Condition", "comments": "[v4]: Fixes a constant factor in the PL-->QG proof, which also\n  simplifies the PL-->EB proof and implies that PL implies EB and QB with the\n  same constant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1963, Polyak proposed a simple condition that is sufficient to show a\nglobal linear convergence rate for gradient descent. This condition is a\nspecial case of the \\L{}ojasiewicz inequality proposed in the same year, and it\ndoes not require strong convexity (or even convexity). In this work, we show\nthat this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker\nthan the main conditions that have been explored to show linear convergence\nrates without strong convexity over the last 25 years. We also use the PL\ninequality to give new analyses of randomized and greedy coordinate descent\nmethods, sign-based gradient descent methods, and stochastic gradient methods\nin the classic setting (with decreasing or constant step-sizes) as well as the\nvariance-reduced setting. We further propose a generalization that applies to\nproximal-gradient methods for non-smooth optimization, leading to simple proofs\nof linear convergence of these methods. Along the way, we give simple\nconvergence results for a wide variety of problems in machine learning: least\nsquares, logistic regression, boosting, resilient backpropagation,\nL1-regularization, support vector machines, stochastic dual coordinate ascent,\nand stochastic variance-reduced gradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:28:24 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 04:08:30 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 22:47:30 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 23:03:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Karimi", "Hamed", ""], ["Nutini", "Julie", ""], ["Schmidt", "Mark", ""]]}, {"id": "1608.04647", "submitter": "Mihai Capot\\u{a}", "authors": "Michael J. Anderson, Mihai Capot\\u{a}, Javier S. Turek, Xia Zhu,\n  Theodore L. Willke, Yida Wang, Po-Hsuan Chen, Jeremy R. Manning, Peter J.\n  Ramadge, Kenneth A. Norman", "title": "Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2016.7840719", "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of functional magnetic resonance image data is rapidly increasing\nas large multi-subject datasets are becoming widely available and\nhigh-resolution scanners are adopted. The inherent low-dimensionality of the\ninformation in this data has led neuroscientists to consider factor analysis\nmethods to extract and analyze the underlying brain activity. In this work, we\nconsider two recent multi-subject factor analysis methods: the Shared Response\nModel and Hierarchical Topographic Factor Analysis. We perform analytical,\nalgorithmic, and code optimization to enable multi-node parallel\nimplementations to scale. Single-node improvements result in 99x and 1812x\nspeedups on these two methods, and enables the processing of larger datasets.\nOur distributed implementations show strong scaling of 3.3x and 5.5x\nrespectively with 20 nodes on real datasets. We also demonstrate weak scaling\non a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768\ncores.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:05:14 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 02:30:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Anderson", "Michael J.", ""], ["Capot\u0103", "Mihai", ""], ["Turek", "Javier S.", ""], ["Zhu", "Xia", ""], ["Willke", "Theodore L.", ""], ["Wang", "Yida", ""], ["Chen", "Po-Hsuan", ""], ["Manning", "Jeremy R.", ""], ["Ramadge", "Peter J.", ""], ["Norman", "Kenneth A.", ""]]}, {"id": "1608.04664", "submitter": "Stefanos Eleftheriadis", "authors": "Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja\n  Pantic", "title": "Variational Gaussian Process Auto-Encoder for Ordinal Prediction of\n  Facial Action Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of simultaneous feature fusion and modeling of discrete\nordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling\napproach. In particular, we introduce GP encoders to project multiple observed\nfeatures onto a latent space, while GP decoders are responsible for\nreconstructing the original features. Inference is performed in a novel\nvariational framework, where the recovered latent representations are further\nconstrained by the ordinal output labels. In this way, we seamlessly integrate\nthe ordinal structure in the learned manifold, while attaining robust fusion of\nthe input features. We demonstrate the representation abilities of our model on\nbenchmark datasets from machine learning and affect analysis. We further\nevaluate the model on the tasks of feature fusion and joint ordinal prediction\nof facial action units. Our experiments demonstrate the benefits of the\nproposed approach compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:31:39 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 21:25:48 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Rudovic", "Ognjen", ""], ["Deisenroth", "Marc P.", ""], ["Pantic", "Maja", ""]]}, {"id": "1608.04667", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Medical image denoising using convolutional denoising autoencoders", "comments": "To appear: 6 pages, paper to be published at the Fourth Workshop on\n  Data Mining in Biomedical Informatics and Healthcare at ICDM, 2016", "journal-ref": null, "doi": "10.1109/ICDMW.2016.0041", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is an important pre-processing step in medical image\nanalysis. Different algorithms have been proposed in past three decades with\nvarying denoising performances. More recently, having outperformed all\nconventional methods, deep learning based models have shown a great promise.\nThese methods are however limited for requirement of large training sample size\nand high computational costs. In this paper we show that using small sample\nsize, denoising autoencoders constructed using convolutional layers can be used\nfor efficient denoising of medical images. Heterogeneous images can be combined\nto boost sample size for increased denoising performance. Simplest of networks\ncan reconstruct images with corruption levels so high that noise and signal are\nnot differentiable to human eye.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:39:20 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 01:09:57 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1608.04674", "submitter": "Eric Chi", "authors": "Bethany Lusch, Eric C. Chi, J. Nathan Kutz", "title": "Shape Constrained Tensor Decompositions using Sparse Representations in\n  Over-Complete Libraries", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $N$-way data arrays and low-rank tensor factorizations where the\ntime mode is coded as a sparse linear combination of temporal elements from an\nover-complete library. Our method, Shape Constrained Tensor Decomposition\n(SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces\n$r$-rank approximations of data tensors via outer products of vectors in each\ndimension of the data. By constraining the vector in the temporal dimension to\nknown analytic forms which are selected from a large set of candidate\nfunctions, more readily interpretable decompositions are achieved and analytic\ntime dependencies discovered. The SCTD method circumvents traditional {\\em\nflattening} techniques where an $N$-way array is reshaped into a matrix in\norder to perform a singular value decomposition. A clear advantage of the SCTD\nalgorithm is its ability to extract transient and intermittent phenomena which\nis often difficult for SVD-based methods. We motivate the SCTD method using\nseveral intuitively appealing results before applying it on a number of\nhigh-dimensional, real-world data sets in order to illustrate the efficiency of\nthe algorithm in extracting interpretable spatio-temporal modes. With the rise\nof data-driven discovery methods, the decomposition proposed provides a viable\ntechnique for analyzing multitudes of data in a more comprehensible fashion.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:00:48 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Lusch", "Bethany", ""], ["Chi", "Eric C.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.04689", "submitter": "Hongyu Guo", "authors": "Martin Renqiang Min, Hongyu Guo, Dongjin Song", "title": "A Shallow High-Order Parametric Approach to Data Visualization and\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit high-order feature interactions efficiently capture essential\nstructural knowledge about the data of interest and have been used for\nconstructing generative models. We present a supervised discriminative\nHigh-Order Parametric Embedding (HOPE) approach to data visualization and\ncompression. Compared to deep embedding models with complicated deep\narchitectures, HOPE generates more effective high-order feature mapping through\nan embarrassingly simple shallow model. Furthermore, two approaches to\ngenerating a small number of exemplars conveying high-order interactions to\nrepresent large-scale data sets are proposed. These exemplars in combination\nwith the feature mapping learned by HOPE effectively capture essential data\nvariations. Moreover, through HOPE, these exemplars are employed to increase\nthe computational efficiency of kNN classification for fast information\nretrieval by thousands of times. For classification in two-dimensional\nembedding space on MNIST and USPS datasets, our shallow method HOPE with simple\nSigmoid transformations significantly outperforms state-of-the-art supervised\ndeep embedding models based on deep neural networks, and even achieved\nhistorically low test error rate of 0.65% in two-dimensional space on MNIST,\nwhich demonstrates the representational efficiency and power of supervised\nshallow models with high-order feature interactions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:54:40 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Guo", "Hongyu", ""], ["Song", "Dongjin", ""]]}, {"id": "1608.04738", "submitter": "Shenjian Zhao", "authors": "Shenjian Zhao, Zhihua Zhang", "title": "An Efficient Character-Level Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation aims at building a single large neural network\nthat can be trained to maximize translation performance. The encoder-decoder\narchitecture with an attention mechanism achieves a translation performance\ncomparable to the existing state-of-the-art phrase-based systems on the task of\nEnglish-to-French translation. However, the use of large vocabulary becomes the\nbottleneck in both training and improving the performance. In this paper, we\npropose an efficient architecture to train a deep character-level neural\nmachine translation by introducing a decimator and an interpolator. The\ndecimator is used to sample the source sequence before encoding while the\ninterpolator is used to resample after decoding. Such a deep model has two\nmajor advantages. It avoids the large vocabulary issue radically; at the same\ntime, it is much faster and more memory-efficient in training than conventional\ncharacter-based models. More interestingly, our model is able to translate the\nmisspelled word like human beings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 07:44:02 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 06:49:32 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhao", "Shenjian", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1608.04773", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev\n  Approximation", "comments": "title changed and minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve principal component regression (PCR), up to a multiplicative\naccuracy $1+\\gamma$, by reducing the problem to $\\tilde{O}(\\gamma^{-1})$\nblack-box calls of ridge regression. Therefore, our algorithm does not require\nany explicit construction of the top principal components, and is suitable for\nlarge-scale PCR instances. In contrast, previous result requires\n$\\tilde{O}(\\gamma^{-2})$ such black-box calls.\n  We obtain this result by developing a general stable recurrence formula for\nmatrix Chebyshev polynomials, and a degree-optimal polynomial approximation to\nthe matrix sign function. Our techniques may be of independent interests,\nespecially when designing iterative methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:48:02 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 19:35:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1608.04783", "submitter": "Aileme Omogbai Aileme Omogbai", "authors": "Aileme Omogbai", "title": "Application of multiview techniques to NHANES dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease prediction or classification using health datasets involve using\nwell-known predictors associated with the disease as features for the models.\nThis study considers multiple data components of an individual's health, using\nthe relationship between variables to generate features that may improve the\nperformance of disease classification models. In order to capture information\nfrom different aspects of the data, this project uses a multiview learning\napproach, using Canonical Correlation Analysis (CCA), a technique that finds\nprojections with maximum correlations between two data views. Data categories\ncollected from the NHANES survey (1999-2014) are used as views to learn the\nmultiview representations. The usefulness of the representations is\ndemonstrated by applying them as features in a Diabetes classification task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 21:20:30 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Omogbai", "Aileme", ""]]}, {"id": "1608.04802", "submitter": "Elad Eban", "authors": "Elad ET. Eban, Mariano Schain, Alan Mackey, Ariel Gordon, Rif A.\n  Saurous, Gal Elidan", "title": "Scalable Learning of Non-Decomposable Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern retrieval systems are often driven by an underlying machine learning\nmodel. The goal of such systems is to identify and possibly rank the few most\nrelevant items for a given query or context. Thus, such systems are typically\nevaluated using a ranking-based performance metric such as the area under the\nprecision-recall curve, the $F_\\beta$ score, precision at fixed recall, etc.\nObviously, it is desirable to train such systems to optimize the metric of\ninterest.\n  In practice, due to the scalability limitations of existing approaches for\noptimizing such objectives, large-scale retrieval systems are instead trained\nto maximize classification accuracy, in the hope that performance as measured\nvia the true objective will also be favorable. In this work we present a\nunified framework that, using straightforward building block bounds, allows for\nhighly scalable optimization of a wide range of ranking-based objectives. We\ndemonstrate the advantage of our approach on several real-life retrieval\nproblems that are significantly larger than those considered in the literature,\nwhile achieving substantial improvement in performance over the\naccuracy-objective baseline.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 23:11:14 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 07:54:51 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Eban", "Elad ET.", ""], ["Schain", "Mariano", ""], ["Mackey", "Alan", ""], ["Gordon", "Ariel", ""], ["Saurous", "Rif A.", ""], ["Elidan", "Gal", ""]]}, {"id": "1608.04830", "submitter": "Truyen Tran", "authors": "Kien Do, Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Outlier Detection on Mixed-Type Data: An Energy-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection amounts to finding data points that differ significantly\nfrom the norm. Classic outlier detection methods are largely designed for\nsingle data type such as continuous or discrete. However, real world data is\nincreasingly heterogeneous, where a data point can have both discrete and\ncontinuous attributes. Handling mixed-type data in a disciplined way remains a\ngreat challenge. In this paper, we propose a new unsupervised outlier detection\nmethod for mixed-type data based on Mixed-variate Restricted Boltzmann Machine\n(Mv.RBM). The Mv.RBM is a principled probabilistic method that models data\ndensity. We propose to use \\emph{free-energy} derived from Mv.RBM as outlier\nscore to detect outliers as those data points lying in low density regions. The\nmethod is fast to learn and compute, is scalable to massive datasets. At the\nsame time, the outlier score is identical to data negative log-density up-to an\nadditive constant. We evaluate the proposed method on synthetic and real-world\ndatasets and demonstrate that (a) a proper handling mixed-types is necessary in\noutlier detection, and (b) free-energy of Mv.RBM is a powerful and efficient\noutlier scoring method, which is highly competitive against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 01:41:40 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1608.04839", "submitter": "Ghassen Jerfel", "authors": "Ghassen Jerfel, Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Dynamic Collaborative Filtering with Compound Poisson Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based collaborative filtering analyzes user-item interactions to infer\nlatent factors that represent user preferences and item characteristics in\norder to predict future interactions. Most collaborative filtering algorithms\nassume that these latent factors are static, although it has been shown that\nuser preferences and item perceptions drift over time. In this paper, we\npropose a conjugate and numerically stable dynamic matrix factorization (DCPF)\nbased on compound Poisson matrix factorization that models the smoothly\ndrifting latent factors using Gamma-Markov chains. We propose a numerically\nstable Gamma chain construction, and then present a stochastic variational\ninference approach to estimate the parameters of our model. We apply our model\nto time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF\nachieves a higher predictive accuracy than state-of-the-art static and dynamic\nfactorization models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 02:38:44 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:19:24 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 07:23:16 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Jerfel", "Ghassen", ""], ["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1608.04845", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Lecture Notes on Spectral Graph Methods", "comments": "257 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are lecture notes that are based on the lectures from a class I taught\non the topic of Spectral Graph Methods at UC Berkeley during the Spring 2015\nsemester.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:38:37 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1608.04846", "submitter": "Po-Hsuan Chen", "authors": "Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S. Turek, Janice Chen,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most effective way to aggregate multi-subject fMRI data is a\nlong-standing and challenging problem. It is of increasing interest in\ncontemporary fMRI studies of human cognition due to the scarcity of data per\nsubject and the variability of brain anatomy and functional response across\nsubjects. Recent work on latent factor models shows promising results in this\ntask but this approach does not preserve spatial locality in the brain. We\nexamine two ways to combine the ideas of a factor model and a searchlight based\nanalysis to aggregate multi-subject fMRI data while preserving spatial\nlocality. We first do this directly by combining a recent factor method known\nas a shared response model with searchlight analysis. Then we design a\nmulti-view convolutional autoencoder for the same task. Both approaches\npreserve spatial locality and have competitive or better performance compared\nwith standard searchlight analysis and the shared response model applied across\nthe whole brain. We also report a system design to handle the computational\nchallenge of training the convolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:49:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chen", "Po-Hsuan", ""], ["Zhu", "Xia", ""], ["Zhang", "Hejia", ""], ["Turek", "Javier S.", ""], ["Chen", "Janice", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.04961", "submitter": "Rajiv Sambasivan", "authors": "Rajiv Sambasivan and Sourish Das", "title": "Clustering Mixed Datasets Using Homogeneity Analysis with Applications\n  to Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with a mixture of numerical and categorical attributes are routinely\nencountered in many application domains. In this work we examine an approach to\nclustering such datasets using homogeneity analysis. Homogeneity analysis\ndetermines a euclidean representation of the data. This can be analyzed by\nleveraging the large body of tools and techniques for data with a euclidean\nrepresentation. Experiments conducted as part of this study suggest that this\napproach can be useful in the analysis and exploration of big datasets with a\nmixture of numerical and categorical attributes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:40:31 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 15:44:21 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 07:01:20 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""]]}, {"id": "1608.04972", "submitter": "Pablo A. Alvarado", "authors": "Pablo A. Alvarado, Mauricio A. \\'Alvarez, \\'Alvaro A. Orozco", "title": "A Three Spatial Dimension Wave Latent Force Model for Describing\n  Excitation Sources and Electric Potentials Produced by Deep Brain Stimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep brain stimulation (DBS) is a surgical treatment for Parkinson's Disease.\nStatic models based on quasi-static approximation are common approaches for DBS\nmodeling. While this simplification has been validated for bioelectric sources,\nits application to rapid stimulation pulses, which contain more high-frequency\npower, may not be appropriate, as DBS therapeutic results depend on stimulus\nparameters such as frequency and pulse width, which are related to time\nvariations of the electric field. We propose an alternative hybrid approach\nbased on probabilistic models and differential equations, by using Gaussian\nprocesses and wave equation. Our model avoids quasi-static approximation,\nmoreover, it is able to describe dynamic behavior of DBS. Therefore, the\nproposed model may be used to obtain a more realistic phenomenon description.\nThe proposed model can also solve inverse problems, i.e. to recover the\ncorresponding source of excitation, given electric potential distribution. The\nelectric potential produced by a time-varying source was predicted using\nproposed model. For static sources, the electric potential produced by\ndifferent electrode configurations were modeled. Four different sources of\nexcitation were recovered by solving the inverse problem. We compare our\noutcomes with the electric potential obtained by solving Poisson's equation\nusing the Finite Element Method (FEM). Our approach is able to take into\naccount time variations of the source and the produced field. Also, inverse\nproblem can be addressed using the proposed model. The electric potential\ncalculated with the proposed model is close to the potential obtained by\nsolving Poisson's equation using FEM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 14:19:35 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Alvarado", "Pablo A.", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Orozco", "\u00c1lvaro A.", ""]]}, {"id": "1608.05081", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed,\n  Li Deng", "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 20:00:04 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 18:20:55 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:01:04 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 10:24:17 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Ahmed", "Faisal", ""], ["Deng", "Li", ""]]}, {"id": "1608.05127", "submitter": "Vikas Chawla", "authors": "Vikas Chawla, Hsiang Sing Naik, Adedotun Akintayo, Dermot Hayes,\n  Patrick Schnable, Baskar Ganapathysubramanian, Soumik Sarkar", "title": "A Bayesian Network approach to County-Level Corn Yield Prediction using\n  historical data and expert knowledge", "comments": "8 pages, In Proceedings of the 22nd ACM SIGKDD Workshop on Data\n  Science for Food, Energy and Water , 2016 (San Francisco, CA, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop yield forecasting is the methodology of predicting crop yields prior to\nharvest. The availability of accurate yield prediction frameworks have enormous\nimplications from multiple standpoints, including impact on the crop commodity\nfutures markets, formulation of agricultural policy, as well as crop insurance\nrating. The focus of this work is to construct a corn yield predictor at the\ncounty scale. Corn yield (forecasting) depends on a complex, interconnected set\nof variables that include economic, agricultural, management and meteorological\nfactors. Conventional forecasting is either knowledge-based computer programs\n(that simulate plant-weather-soil-management interactions) coupled with\ntargeted surveys or statistical model based. The former is limited by the need\nfor painstaking calibration, while the latter is limited to univariate analysis\nor similar simplifying assumptions that fail to capture the complex\ninterdependencies affecting yield. In this paper, we propose a data-driven\napproach that is \"gray box\" i.e. that seamlessly utilizes expert knowledge in\nconstructing a statistical network model for corn yield forecasting. Our\nmultivariate gray box model is developed on Bayesian network analysis to build\na Directed Acyclic Graph (DAG) between predictors and yield. Starting from a\ncomplete graph connecting various carefully chosen variables and yield, expert\nknowledge is used to prune or strengthen edges connecting variables.\nSubsequently the structure (connectivity and edge weights) of the DAG that\nmaximizes the likelihood of observing the training data is identified via\noptimization. We curated an extensive set of historical data (1948-2012) for\neach of the 99 counties in Iowa as data to train the model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 23:30:04 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chawla", "Vikas", ""], ["Naik", "Hsiang Sing", ""], ["Akintayo", "Adedotun", ""], ["Hayes", "Dermot", ""], ["Schnable", "Patrick", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1608.05138", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi and Rong Zhou", "title": "Hybrid CPU-GPU Framework for Network Motifs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel architectures such as the GPU are becoming increasingly\nimportant due to the recent proliferation of data. In this paper, we propose a\nkey class of hybrid parallel graphlet algorithms that leverages multiple CPUs\nand GPUs simultaneously for computing k-vertex induced subgraph statistics\n(called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we\nalso investigate single GPU methods (using multiple cores) and multi-GPU\nmethods that leverage all available GPUs simultaneously for computing induced\nsubgraph statistics. Both methods leverage GPU devices only, whereas the hybrid\nmulti-core CPU-GPU framework leverages all available multi-core CPUs and\nmultiple GPUs for computing graphlets in large networks. Compared to recent\napproaches, our methods are orders of magnitude faster, while also more cost\neffective enjoying superior performance per capita and per watt. In particular,\nthe methods are up to 300 times faster than the recent state-of-the-art method.\nTo the best of our knowledge, this is the first work to leverage multiple CPUs\nand GPUs simultaneously for computing induced subgraph statistics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 00:29:21 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 01:55:07 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""]]}, {"id": "1608.05152", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Conditional Sparse Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and statistics typically focus on building models that\ncapture the vast majority of the data, possibly ignoring a small subset of data\nas \"noise\" or \"outliers.\" By contrast, here we consider the problem of jointly\nidentifying a significant (but perhaps small) segment of a population in which\nthere is a highly sparse linear regression fit, together with the coefficients\nfor the linear fit. We contend that such tasks are of interest both because the\nmodels themselves may be able to achieve better predictions in such special\ncases, but also because they may aid our understanding of the data. We give\nalgorithms for such problems under the sup norm, when this unknown segment of\nthe population is described by a k-DNF condition and the regression fit is\ns-sparse for constant k and s. For the variants of this problem when the\nregression fit is not so sparse or using expected error, we also give a\npreliminary algorithm and highlight the question as a challenge for future\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 01:30:49 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1608.05182", "submitter": "Yanbo Xu", "authors": "Yanbo Xu, Yanxun Xu and Suchi Saria", "title": "A Bayesian Nonparametric Approach for Estimating Individualized\n  Treatment-Response Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the continuous response over time to\ninterventions using observational time series---a retrospective dataset where\nthe policy by which the data are generated is unknown to the learner. We are\nmotivated by applications where response varies by individuals and therefore,\nestimating responses at the individual-level is valuable for personalizing\ndecision-making. We refer to this as the problem of estimating individualized\ntreatment response (ITR) curves. In statistics, G-computation formula (Robins,\n1986) has been commonly used for estimating treatment responses from\nobservational data containing sequential treatment assignments. However, past\nstudies have focused predominantly on obtaining point-in-time estimates at the\npopulation level. We leverage the G-computation formula and develop a novel\nBayesian nonparametric (BNP) method that can flexibly model functional data and\nprovide posterior inference over the treatment response curves at both the\nindividual and population level. On a challenging dataset containing time\nseries from patients admitted to a hospital, we estimate responses to\ntreatments used in managing kidney function and show that the resulting fits\nare more accurate than alternative approaches. Accurate methods for obtaining\nITRs from observational data can dramatically accelerate the pace at which\npersonalized treatment plans become possible.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 05:31:53 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:44:14 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Xu", "Yanbo", ""], ["Xu", "Yanxun", ""], ["Saria", "Suchi", ""]]}, {"id": "1608.05225", "submitter": "Joachim van der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Dirk Deschrijver and Tom\n  Dhaene", "title": "Active Learning for Approximation of Expensive Functions with Normal\n  Distributed Output Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When approximating a black-box function, sampling with active learning\nfocussing on regions with non-linear responses tends to improve accuracy. We\npresent the FLOLA-Voronoi method introduced previously for deterministic\nresponses, and theoretically derive the impact of output uncertainty. The\nalgorithm automatically puts more emphasis on exploration to provide more\ninformation to the models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 10:15:54 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Deschrijver", "Dirk", ""], ["Dhaene", "Tom", ""]]}, {"id": "1608.05258", "submitter": "Tatiana Shpakova", "authors": "Tatiana Shpakova and Francis Bach", "title": "Parameter Learning for Log-supermodular Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider log-supermodular models on binary variables, which are\nprobabilistic models with negative log-densities which are submodular. These\nmodels provide probabilistic interpretations of common combinatorial\noptimization tasks such as image segmentation. In this paper, we focus\nprimarily on parameter estimation in the models from known upper-bounds on the\nintractable log-partition function. We show that the bound based on separable\noptimization on the base polytope of the submodular function is always inferior\nto a bound based on \"perturb-and-MAP\" ideas. Then, to learn parameters, given\nthat our approximation of the log-partition function is an expectation (over\nour own randomization), we use a stochastic subgradient technique to maximize a\nlower-bound on the log-likelihood. This can also be extended to conditional\nmaximum likelihood. We illustrate our new results in a set of experiments in\nbinary image denoising, where we highlight the flexibility of a probabilistic\nmodel to learn with missing data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 13:55:41 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Shpakova", "Tatiana", ""], ["Bach", "Francis", ""]]}, {"id": "1608.05275", "submitter": "Elad Mezuman", "authors": "Elad Mezuman and Yair Weiss", "title": "A Tight Convex Upper Bound on the Likelihood of a Finite Mixture", "comments": "icpr 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function of a finite mixture model is a non-convex function\nwith multiple local maxima and commonly used iterative algorithms such as EM\nwill converge to different solutions depending on initial conditions. In this\npaper we ask: is it possible to assess how far we are from the global maximum\nof the likelihood? Since the likelihood of a finite mixture model can grow\nunboundedly by centering a Gaussian on a single datapoint and shrinking the\ncovariance, we constrain the problem by assuming that the parameters of the\nindividual models are members of a large discrete set (e.g. estimating a\nmixture of two Gaussians where the means and variances of both Gaussians are\nmembers of a set of a million possible means and variances). For this setting\nwe show that a simple upper bound on the likelihood can be computed using\nconvex optimization and we analyze conditions under which the bound is\nguaranteed to be tight. This bound can then be used to assess the quality of\nsolutions found by EM (where the final result is projected on the discrete set)\nor any other mixture estimation algorithm. For any dataset our method allows us\nto find a finite mixture model together with a dataset-specific bound on how\nfar the likelihood of this mixture is from the global optimum of the likelihood\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 14:27:45 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Mezuman", "Elad", ""], ["Weiss", "Yair", ""]]}, {"id": "1608.05347", "submitter": "Feras Saad", "authors": "Feras Saad, Vikash Mansinghka", "title": "Probabilistic Data Analysis with Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic techniques are central to data analysis, but different\napproaches can be difficult to apply, combine, and compare. This paper\nintroduces composable generative population models (CGPMs), a computational\nabstraction that extends directed graphical models and can be used to describe\nand compose a broad class of probabilistic data analysis techniques. Examples\ninclude hierarchical Bayesian models, multivariate kernel methods,\ndiscriminative machine learning, clustering algorithms, dimensionality\nreduction, and arbitrary probabilistic programs. We also demonstrate the\nintegration of CGPMs into BayesDB, a probabilistic programming platform that\ncan express data analysis tasks using a modeling language and a structured\nquery language. The practical value is illustrated in two ways. First, CGPMs\nare used in an analysis that identifies satellite data records which probably\nviolate Kepler's Third Law, by composing causal probabilistic programs with\nnon-parametric Bayes in under 50 lines of probabilistic code. Second, for\nseveral representative data analysis tasks, we report on lines of code and\naccuracy measurements of various CGPMs, plus comparisons with standard baseline\nsolutions from Python and MATLAB libraries.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:47:53 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Saad", "Feras", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1608.05493", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Wolfgang Kellerer and Martin Kleinsteuber", "title": "Network Volume Anomaly Detection and Identification in Large-scale\n  Networks based on Online Time-structured Traffic Tensor Tracking", "comments": "IEEE Transactions on Network and Service Management", "journal-ref": "IEEE Transactions on Network and Service Management, vol.13, no.3,\n  pp.636-650, 2016", "doi": "10.1109/TNSM.2016.2598788", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses network anomography, that is, the problem of inferring\nnetwork-level anomalies from indirect link measurements. This problem is cast\nas a low-rank subspace tracking problem for normal flows under incomplete\nobservations, and an outlier detection problem for abnormal flows. Since\ntraffic data is large-scale time-structured data accompanied with noise and\noutliers under partial observations, an efficient modeling method is essential.\nTo this end, this paper proposes an online subspace tracking of a Hankelized\ntime-structured traffic tensor for normal flows based on the Candecomp/PARAFAC\ndecomposition exploiting the recursive least squares (RLS) algorithm. We\nestimate abnormal flows as outlier sparse flows via sparsity maximization in\nthe underlying under-constrained linear-inverse problem. A major advantage is\nthat our algorithm estimates normal flows by low-dimensional matrices with\ntime-directional features as well as the spatial correlation of multiple links\nwithout using the past observed measurements and the past model parameters.\nExtensive numerical evaluations show that the proposed algorithm achieves\nfaster convergence per iteration of model approximation, and better volume\nanomaly detection performance compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 05:06:58 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Kellerer", "Wolfgang", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1608.05560", "submitter": "Yang Wang", "authors": "Yang Wang, Wenjie Zhang, Lin Wu, Xuemin Lin, Meng Fang, Shirui Pan", "title": "Iterative Views Agreement: An Iterative Low-Rank based Structured\n  Optimization Method to Multi-View Spectral Clustering", "comments": "Accepted to appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view spectral clustering, which aims at yielding an agreement or\nconsensus data objects grouping across multi-views with their graph laplacian\nmatrices, is a fundamental clustering problem. Among the existing methods,\nLow-Rank Representation (LRR) based method is quite superior in terms of its\neffectiveness, intuitiveness and robustness to noise corruptions. However, it\naggressively tries to learn a common low-dimensional subspace for multi-view\ndata, while inattentively ignoring the local manifold structure in each view,\nwhich is critically important to the spectral clustering; worse still, the\nlow-rank minimization is enforced to achieve the data correlation consensus\namong all views, failing to flexibly preserve the local manifold structure for\neach view. In this paper, 1) we propose a multi-graph laplacian regularized LRR\nwith each graph laplacian corresponding to one view to characterize its local\nmanifold structure. 2) Instead of directly enforcing the low-rank minimization\namong all views for correlation consensus, we separately impose low-rank\nconstraint on each view, coupled with a mutual structural consensus constraint,\nwhere it is able to not only well preserve the local manifold structure but\nalso serve as a constraint for that from other views, which iteratively makes\nthe views more agreeable. Extensive experiments on real-world multi-view data\nsets demonstrate its superiority.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 10:25:46 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Wang", "Yang", ""], ["Zhang", "Wenjie", ""], ["Wu", "Lin", ""], ["Lin", "Xuemin", ""], ["Fang", "Meng", ""], ["Pan", "Shirui", ""]]}, {"id": "1608.05581", "submitter": "Jean Golay", "authors": "Jean Golay and Mikhail Kanevski", "title": "Unsupervised Feature Selection Based on the Morisita Estimator of\n  Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a new filter algorithm for selecting the smallest\nsubset of features carrying all the information content of a data set (i.e. for\nremoving redundant features). It is an advanced version of the fractal\ndimension reduction technique, and it relies on the recently introduced\nMorisita estimator of Intrinsic Dimension (ID). Here, the ID is used to\nquantify dependencies between subsets of features, which allows the effective\nprocessing of highly non-linear data. The proposed algorithm is successfully\ntested on simulated and real world case studies. Different levels of sample\nsize and noise are examined along with the variability of the results. In\naddition, a comprehensive procedure based on random forests shows that the data\ndimensionality is significantly reduced by the algorithm without loss of\nrelevant information. And finally, comparisons with benchmark feature selection\ntechniques demonstrate the promising performance of this new filter.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 12:28:21 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 16:02:12 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 15:56:35 GMT"}, {"version": "v4", "created": "Sun, 5 Mar 2017 16:54:00 GMT"}, {"version": "v5", "created": "Fri, 2 Jun 2017 19:07:02 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Golay", "Jean", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1608.05610", "submitter": "Yevgeny Seldin", "authors": "Niklas Thiemann and Christian Igel and Olivier Wintenberger and\n  Yevgeny Seldin", "title": "A Strongly Quasiconvex PAC-Bayesian Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new PAC-Bayesian bound and a way of constructing a hypothesis\nspace, so that the bound is convex in the posterior distribution and also\nconvex in a trade-off parameter between empirical performance of the posterior\ndistribution and its complexity. The complexity is measured by the\nKullback-Leibler divergence to a prior. We derive an alternating procedure for\nminimizing the bound. We show that the bound can be rewritten as a\none-dimensional function of the trade-off parameter and provide sufficient\nconditions under which the function has a single global minimum. When the\nconditions are satisfied the alternating minimization is guaranteed to converge\nto the global minimum of the bound. We provide experimental results\ndemonstrating that rigorous minimization of the bound is competitive with\ncross-validation in tuning the trade-off between complexity and empirical\nperformance. In all our experiments the trade-off turned to be quasiconvex even\nwhen the sufficient conditions were violated.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:21:18 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 09:45:07 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Thiemann", "Niklas", ""], ["Igel", "Christian", ""], ["Wintenberger", "Olivier", ""], ["Seldin", "Yevgeny", ""]]}, {"id": "1608.05747", "submitter": "Edward Pyzer-Knapp", "authors": "Dipti Jasrasaria, Edward O. Pyzer-Knapp, Dmitrij Rappoport, and Alan\n  Aspuru-Guzik", "title": "Space-Filling Curves as a Novel Crystal Structure Representation for\n  Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in applying machine learning techniques for chemical\nproblems is to find suitable representations for molecular and crystal\nstructures. While the structure representations based on atom connectivities\nare prevalent for molecules, two-dimensional descriptors are not suitable for\ndescribing molecular crystals. In this work, we introduce the SFC-M family of\nfeature representations, which are based on Morton space-filling curves, as an\nalternative means of representing crystal structures. Latent Semantic Indexing\n(LSI) was employed in a novel setting to reduce sparsity of feature\nrepresentations. The quality of the SFC-M representations were assessed by\nusing them in combination with artificial neural networks to predict Density\nFunctional Theory (DFT) single point, Ewald summed, lattice, and many-body\ndispersion energies of 839 organic molecular crystal unit cells from the\nCambridge Structural Database that consist of the elements C, H, N, and O.\nPromising initial results suggest that the SFC-M representations merit further\nexploration to improve its ability to predict solid-state properties of organic\ncrystal structures\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 22:01:43 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Jasrasaria", "Dipti", ""], ["Pyzer-Knapp", "Edward O.", ""], ["Rappoport", "Dmitrij", ""], ["Aspuru-Guzik", "Alan", ""]]}, {"id": "1608.05749", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Constantine Caramanis, Sujay Sanghavi", "title": "Solving a Mixture of Many Random Linear Equations by Tensor\n  Decomposition and Alternating Minimization", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving mixed random linear equations with $k$\ncomponents. This is the noiseless setting of mixed linear regression. The goal\nis to estimate multiple linear models from mixed samples in the case where the\nlabels (which sample corresponds to which model) are not observed. We give a\ntractable algorithm for the mixed linear equation problem, and show that under\nsome technical conditions, our algorithm is guaranteed to solve the problem\nexactly with sample complexity linear in the dimension, and polynomial in $k$,\nthe number of components. Previous approaches have required either exponential\ndependence on $k$, or super-linear dependence on the dimension. The proposed\nalgorithm is a combination of tensor decomposition and alternating\nminimization. Our analysis involves proving that the initialization provided by\nthe tensor method allows alternating minimization, which is equivalent to EM in\nour setting, to converge to the global optimum at a linear rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 22:10:46 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1608.05806", "submitter": "Alexey Rogozhnikov", "authors": "A. Rogozhnikov", "title": "Reweighting with Boosted Decision Trees", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/762/1/012036", "report-no": null, "categories": "physics.data-an hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning tools are commonly used in modern high energy physics (HEP)\nexperiments. Different models, such as boosted decision trees (BDT) and\nartificial neural networks (ANN), are widely used in analyses and even in the\nsoftware triggers.\n  In most cases, these are classification models used to select the \"signal\"\nevents from data. Monte Carlo simulated events typically take part in training\nof these models. While the results of the simulation are expected to be close\nto real data, in practical cases there is notable disagreement between\nsimulated and observed data. In order to use available simulation in training,\ncorrections must be introduced to generated data. One common approach is\nreweighting - assigning weights to the simulated events. We present a novel\nmethod of event reweighting based on boosted decision trees. The problem of\nchecking the quality of reweighting step in analyses is also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 09:30:29 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Rogozhnikov", "A.", ""]]}, {"id": "1608.05878", "submitter": "Daniel Larremore", "authors": "Leto Peel, Daniel B. Larremore, and Aaron Clauset", "title": "The ground truth about metadata and community detection in networks", "comments": "27 pages, 10 figures, 11 tables", "journal-ref": "Science Advances 3(5) e1602548, 2017", "doi": "10.1126/sciadv.1602548", "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across many scientific domains, there is a common need to automatically\nextract a simplified view or coarse-graining of how a complex system's\ncomponents interact. This general task is called community detection in\nnetworks and is analogous to searching for clusters in independent vector data.\nIt is common to evaluate the performance of community detection algorithms by\ntheir ability to find so-called \"ground truth\" communities. This works well in\nsynthetic networks with planted communities because such networks' links are\nformed explicitly based on those known communities. However, there are no\nplanted communities in real world networks. Instead, it is standard practice to\ntreat some observed discrete-valued node attributes, or metadata, as ground\ntruth. Here, we show that metadata are not the same as ground truth, and that\ntreating them as such induces severe theoretical and practical problems. We\nprove that no algorithm can uniquely solve community detection, and we prove a\ngeneral No Free Lunch theorem for community detection, which implies that there\ncan be no algorithm that is optimal for all possible community detection tasks.\nHowever, community detection remains a powerful tool and node metadata still\nhave value so a careful exploration of their relationship with network\nstructure can yield insights of genuine worth. We illustrate this point by\nintroducing two statistical techniques that can quantify the relationship\nbetween metadata and community structure for a broad class of models. We\ndemonstrate these techniques using both synthetic and real-world networks, and\nfor multiple types of metadata and community structure.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 23:57:12 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 14:25:47 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Peel", "Leto", ""], ["Larremore", "Daniel B.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1608.05889", "submitter": "Jing Wang", "authors": "Jing Wang and Meng Wang and Peipei Li and Luoqi Liu and Zhongqiu Zhao\n  and Xuegang Hu and Xindong Wu", "title": "Online Feature Selection with Group Structure Analysis", "comments": "IEEE Transactions on Knowledge and Data Engineering,2015", "journal-ref": null, "doi": "10.1109/TKDE.2015.2441716", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 02:39:48 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Jing", ""], ["Wang", "Meng", ""], ["Li", "Peipei", ""], ["Liu", "Luoqi", ""], ["Zhao", "Zhongqiu", ""], ["Hu", "Xuegang", ""], ["Wu", "Xindong", ""]]}, {"id": "1608.05910", "submitter": "Aditya Ramadona", "authors": "Aditya L. Ramadona, Rendra Agusta, Sulistyawati, Lutfan Lazuardi,\n  Anwar D. Cahyono, {\\AA}sa Holmner, Fatwa S.T. Dewi, Hari Kusnanto, Joacim\n  R\\\"ocklov", "title": "Mining of health and disease events on Twitter: validating search\n  protocols within the setting of Indonesia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study seeks to validate a search protocol of ill health-related terms\nusing Twitter data which can later be used to understand if, and how, Twitter\ncan reveal information on the current health situation. We extracted\nconversations related to health and disease postings on Twitter using a set of\npre-defined keywords, assessed the prevalence, frequency, and timing of such\ncontent in these conversations, and validated how this search protocol was able\nto detect relevant disease tweets. Classification and Regression Trees (CART)\nalgorithm was used to train and test search protocols of disease and health\nhits comparing to those identified by our team. The accuracy of predictions\nshowed a good validity with AUC beyond 0.8. Our study shows that monitoring of\npublic sentiment on Twitter can be used as a real-time proxy for health events.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 09:57:12 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 14:08:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Ramadona", "Aditya L.", ""], ["Agusta", "Rendra", ""], ["Sulistyawati", "", ""], ["Lazuardi", "Lutfan", ""], ["Cahyono", "Anwar D.", ""], ["Holmner", "\u00c5sa", ""], ["Dewi", "Fatwa S. T.", ""], ["Kusnanto", "Hari", ""], ["R\u00f6cklov", "Joacim", ""]]}, {"id": "1608.05921", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Lexing Xie, Cheng Soon Ong", "title": "Probabilistic Knowledge Graph Construction: Compositional and\n  Incremental Approaches", "comments": "The 25th ACM International Conference on Information and Knowledge\n  Management (CIKM 2016)", "journal-ref": null, "doi": "10.1145/2983323.2983677", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph construction consists of two tasks: extracting information\nfrom external resources (knowledge population) and inferring missing\ninformation through a statistical analysis on the extracted information\n(knowledge completion). In many cases, insufficient external resources in the\nknowledge population hinder the subsequent statistical inference. The gap\nbetween these two processes can be reduced by an incremental population\napproach. We propose a new probabilistic knowledge graph factorisation method\nthat benefits from the path structure of existing knowledge (e.g. syllogism)\nand enables a common modelling approach to be used for both incremental\npopulation and knowledge completion tasks. More specifically, the probabilistic\nformulation allows us to develop an incremental population algorithm that\ntrades off exploitation-exploration. Experiments on three benchmark datasets\nshow that the balanced exploitation-exploration helps the incremental\npopulation, and the additional path structure helps to predict missing\ninformation in knowledge completion.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 11:49:53 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 04:52:33 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Kim", "Dongwoo", ""], ["Xie", "Lexing", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1608.05934", "submitter": "Mohammadreza Ashouri", "authors": "Nouraddin Misagh, Mohammadreza Ashouri", "title": "Spatial Modeling of Oil Exploration Areas Using Neural Networks and\n  ANFIS in GIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration of hydrocarbon resources is a highly complicated and expensive\nprocess where various geological, geochemical and geophysical factors are\ndeveloped then combined together. It is highly significant how to design the\nseismic data acquisition survey and locate the exploratory wells since\nincorrect or imprecise locations lead to waste of time and money during the\noperation. The objective of this study is to locate high-potential oil and gas\nfield in 1: 250,000 sheet of Ahwaz including 20 oil fields to reduce both time\nand costs in exploration and production processes. In this regard, 17 maps were\ndeveloped using GIS functions for factors including: minimum and maximum of\ntotal organic carbon (TOC), yield potential for hydrocarbons production (PP),\nTmax peak, production index (PI), oxygen index (OI), hydrogen index (HI) as\nwell as presence or proximity to high residual Bouguer gravity anomalies,\nproximity to anticline axis and faults, topography and curvature maps obtained\nfrom Asmari Formation subsurface contours. To model and to integrate maps, this\nstudy employed artificial neural network and adaptive neuro-fuzzy inference\nsystem (ANFIS) methods. The results obtained from model validation demonstrated\nthat the 17x10x5 neural network with R=0.8948, RMS=0.0267, and kappa=0.9079 can\nbe trained better than other models such as ANFIS and predicts the potential\nareas more accurately. However, this method failed to predict some oil fields\nand wrongly predict some areas as potential zones.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 13:00:45 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Misagh", "Nouraddin", ""], ["Ashouri", "Mohammadreza", ""]]}, {"id": "1608.05983", "submitter": "Ian Gemp", "authors": "Ian Gemp, Ishan Durugkar, Mario Parente, M. Darby Dyar, Sridhar\n  Mahadevan", "title": "Inverting Variational Autoencoders for Improved Generative Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 19:02:27 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 14:20:27 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Gemp", "Ian", ""], ["Durugkar", "Ishan", ""], ["Parente", "Mario", ""], ["Dyar", "M. Darby", ""], ["Mahadevan", "Sridhar", ""]]}, {"id": "1608.05995", "submitter": "Ming Lin", "authors": "Ming Lin and Jieping Ye", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine\n  and Rank-One Matrix Sensing", "comments": "accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient alternating framework for learning a generalized\nversion of Factorization Machine (gFM) on steaming data with provable\nguarantees. When the instances are sampled from $d$ dimensional random Gaussian\nvectors and the target second order coefficient matrix in gFM is of rank $k$,\nour algorithm converges linearly, achieves $O(\\epsilon)$ recovery error after\nretrieving $O(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $O(kd)$\nmemory in one-pass of dataset and only requires matrix-vector product\noperations in each iteration. The key ingredient of our framework is a\nconstruction of an estimation sequence endowed with a so-called Conditionally\nIndependent RIP condition (CI-RIP). As special cases of gFM, our framework can\nbe applied to symmetric or asymmetric rank-one matrix sensing problems, such as\ninductive matrix completion and phase retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 20:28:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 17:54:50 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 21:43:05 GMT"}, {"version": "v4", "created": "Wed, 14 Sep 2016 02:24:22 GMT"}, {"version": "v5", "created": "Tue, 25 Oct 2016 21:23:23 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Lin", "Ming", ""], ["Ye", "Jieping", ""]]}, {"id": "1608.06010", "submitter": "Yun Wang", "authors": "Yun Wang, Xu Chen and Peter J. Ramadge", "title": "Feedback-Controlled Sequential Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:40:56 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:52:30 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Chen", "Xu", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06014", "submitter": "Yun Wang", "authors": "Yun Wang and Peter J. Ramadge", "title": "The Symmetry of a Simple Optimization Problem in Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently dictionary screening has been proposed as an effective way to\nimprove the computational efficiency of solving the lasso problem, which is one\nof the most commonly used method for learning sparse representations. To\naddress today's ever increasing large dataset, effective screening relies on a\ntight region bound on the solution to the dual lasso. Typical region bounds are\nin the form of an intersection of a sphere and multiple half spaces. One way to\ntighten the region bound is using more half spaces, which however, adds to the\noverhead of solving the high dimensional optimization problem in lasso\nscreening. This paper reveals the interesting property that the optimization\nproblem only depends on the projection of features onto the subspace spanned by\nthe normals of the half spaces. This property converts an optimization problem\nin high dimension to much lower dimension, and thus sheds light on reducing the\ncomputation overhead of lasso screening based on tighter region bounds.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:48:43 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:05:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06031", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Jian Li, Mingda Qiao", "title": "Towards Instance Optimal Bounds for Best Arm Identification", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical best arm identification (Best-$1$-Arm) problem, we are given\n$n$ stochastic bandit arms, each associated with a reward distribution with an\nunknown mean. We would like to identify the arm with the largest mean with\nprobability at least $1-\\delta$, using as few samples as possible.\nUnderstanding the sample complexity of Best-$1$-Arm has attracted significant\nattention since the last decade. However, the exact sample complexity of the\nproblem is still unknown.\n  Recently, Chen and Li made the gap-entropy conjecture concerning the instance\nsample complexity of Best-$1$-Arm. Given an instance $I$, let $\\mu_{[i]}$ be\nthe $i$th largest mean and $\\Delta_{[i]}=\\mu_{[1]}-\\mu_{[i]}$ be the\ncorresponding gap. $H(I)=\\sum_{i=2}^n\\Delta_{[i]}^{-2}$ is the complexity of\nthe instance. The gap-entropy conjecture states that\n$\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)\\right)$ is an\ninstance lower bound, where $\\mathsf{Ent}(I)$ is an entropy-like term\ndetermined by the gaps, and there is a $\\delta$-correct algorithm for\nBest-$1$-Arm with sample complexity\n$O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\right)$.\nIf the conjecture is true, we would have a complete understanding of the\ninstance-wise sample complexity of Best-$1$-Arm.\n  We make significant progress towards the resolution of the gap-entropy\nconjecture. For the upper bound, we provide a highly nontrivial algorithm which\nrequires \\[O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}\n+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\mathrm{polylog}(n,\\delta^{-1})\\right)\\]\nsamples in expectation. For the lower bound, we show that for any Gaussian\nBest-$1$-Arm instance with gaps of the form $2^{-k}$, any $\\delta$-correct\nmonotone algorithm requires $\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1} +\n\\mathsf{Ent}(I)\\right)\\right)$ samples in expectation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:05:10 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 02:19:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1608.06048", "submitter": "Ajinkya More", "authors": "Ajinkya More", "title": "Survey of resampling techniques for improving classification performance\n  in unbalanced datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of classification problems need to deal with data imbalance between\nclasses. Often it is desired to have a high recall on the minority class while\nmaintaining a high precision on the majority class. In this paper, we review a\nnumber of resampling techniques proposed in literature to handle unbalanced\ndatasets and study their effect on classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 04:27:28 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["More", "Ajinkya", ""]]}, {"id": "1608.06072", "submitter": "Ibrahim Alabdulmohsin", "authors": "Ibrahim Alabdulmohsin", "title": "Uniform Generalization, Concentration, and Adaptive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental goal in any learning algorithm is to mitigate its risk for\noverfitting. Mathematically, this requires that the learning algorithm enjoys a\nsmall generalization risk, which is defined either in expectation or in\nprobability. Both types of generalization are commonly used in the literature.\nFor instance, generalization in expectation has been used to analyze\nalgorithms, such as ridge regression and SGD, whereas generalization in\nprobability is used in the VC theory, among others. Recently, a third notion of\ngeneralization has been studied, called uniform generalization, which requires\nthat the generalization risk vanishes uniformly in expectation across all\nbounded parametric losses. It has been shown that uniform generalization is, in\nfact, equivalent to an information-theoretic stability constraint, and that it\nrecovers classical results in learning theory. It is achievable under various\nsettings, such as sample compression schemes, finite hypothesis spaces, finite\ndomains, and differential privacy. However, the relationship between uniform\ngeneralization and concentration remained unknown. In this paper, we answer\nthis question by proving that, while a generalization in expectation does not\nimply a generalization in probability, a uniform generalization in expectation\ndoes imply concentration. We establish a chain rule for the uniform\ngeneralization risk of the composition of hypotheses and use it to derive a\nlarge deviation bound. Finally, we prove that the bound is tight.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 07:47:56 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 16:36:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""]]}, {"id": "1608.06203", "submitter": "Sewoong Oh", "authors": "Ashish Khetan, Sewoong Oh", "title": "Computational and Statistical Tradeoffs in Learning to Rank", "comments": "30 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive and heterogeneous modern datasets, it is of fundamental interest\nto provide guarantees on the accuracy of estimation when computational\nresources are limited. In the application of learning to rank, we provide a\nhierarchy of rank-breaking mechanisms ordered by the complexity in thus\ngenerated sketch of the data. This allows the number of data points collected\nto be gracefully traded off against computational resources available, while\nguaranteeing the desired level of accuracy. Theoretical guarantees on the\nproposed generalized rank-breaking implicitly provide such trade-offs, which\ncan be explicitly characterized under certain canonical scenarios on the\nstructure of the data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:58:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1608.06238", "submitter": "Pantita Palittapongarnpim", "authors": "Pantita Palittapongarnpim, Peter Wittek, Barry C. Sanders", "title": "Single-shot Adaptive Measurement for Quantum-enhanced Metrology", "comments": "11 pages, 4 figures, paper accepted for Quantum Communications and\n  Quantum Imaging XIV conference at SPIE Optics + Photonics 2016", "journal-ref": "Proc. SPIE 9980, Quantum Communications and Quantum Imaging XIV,\n  (2016) 99800H (11pp)", "doi": "10.1117/12.2237355", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum-enhanced metrology aims to estimate an unknown parameter such that\nthe precision scales better than the shot-noise bound. Single-shot adaptive\nquantum-enhanced metrology (AQEM) is a promising approach that uses feedback to\ntweak the quantum process according to previous measurement outcomes.\nTechniques and formalism for the adaptive case are quite different from the\nusual non-adaptive quantum metrology approach due to the causal relationship\nbetween measurements and outcomes. We construct a formal framework for AQEM by\nmodeling the procedure as a decision-making process, and we derive the\nimprecision and the Cram\\'{e}r-Rao lower bound with explicit dependence on the\nfeedback policy. We also explain the reinforcement learning approach for\ngenerating quantum control policies, which is adopted due to the optimal policy\nbeing non-trivial to devise. Applying a learning algorithm based on\ndifferential evolution enables us to attain imprecision for adaptive\ninterferometric phase estimation, which turns out to be SQL when non-entangled\nparticles are used in the scheme.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 17:52:28 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Palittapongarnpim", "Pantita", ""], ["Wittek", "Peter", ""], ["Sanders", "Barry C.", ""]]}, {"id": "1608.06253", "submitter": "Christina Lioma Assoc. Prof", "authors": "Brian Brost and Yevgeny Seldin and Ingemar J. Cox and Christina Lioma", "title": "Multi-Dueling Bandits and Their Application to Online Ranker Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 18:20:18 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Brost", "Brian", ""], ["Seldin", "Yevgeny", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1608.06296", "submitter": "Jennifer N. Wei", "authors": "Jennifer N. Wei, David Duvenaud, Al\\'an Aspuru-Guzik", "title": "Neural networks for the prediction organic chemistry reactions", "comments": "21 pages, 5 figures", "journal-ref": "ACS.Cent.Sci. 2 (2016) 725-732", "doi": "10.1021/acscentsci.6b00219", "report-no": null, "categories": "physics.chem-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reaction prediction remains one of the major challenges for organic\nchemistry, and is a pre-requisite for efficient synthetic planning. It is\ndesirable to develop algorithms that, like humans, \"learn\" from being exposed\nto examples of the application of the rules of organic chemistry. We explore\nthe use of neural networks for predicting reaction types, using a new reaction\nfingerprinting method. We combine this predictor with SMARTS transformations to\nbuild a system which, given a set of reagents and re- actants, predicts the\nlikely products. We test this method on problems from a popular organic\nchemistry textbook.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 20:01:05 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 21:31:23 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Wei", "Jennifer N.", ""], ["Duvenaud", "David", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1608.06315", "submitter": "David Sussillo", "authors": "David Sussillo, Rafal Jozefowicz, L. F. Abbott, Chethan Pandarinath", "title": "LFADS - Latent Factor Analysis via Dynamical Systems", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience is experiencing a data revolution in which many hundreds or\nthousands of neurons are recorded simultaneously. Currently, there is little\nconsensus on how such data should be analyzed. Here we introduce LFADS (Latent\nFactor Analysis via Dynamical Systems), a method to infer latent dynamics from\nsimultaneously recorded, single-trial, high-dimensional neural spiking data.\nLFADS is a sequential model based on a variational auto-encoder. By making a\ndynamical systems hypothesis regarding the generation of the observed data,\nLFADS reduces observed spiking to a set of low-dimensional temporal factors,\nper-trial initial conditions, and inferred inputs. We compare LFADS to existing\nmethods on synthetic data and show that it significantly out-performs them in\ninferring neural firing rates and latent dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 21:15:00 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Sussillo", "David", ""], ["Jozefowicz", "Rafal", ""], ["Abbott", "L. F.", ""], ["Pandarinath", "Chethan", ""]]}, {"id": "1608.06383", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Softplus Regressions and Convex Polytopes", "comments": "33 pages + 12 page appendix, 15 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To construct flexible nonlinear predictive distributions, the paper\nintroduces a family of softplus function based regression models that convolve,\nstack, or combine both operations by convolving countably infinite stacked\ngamma distributions, whose scales depend on the covariates. Generalizing\nlogistic regression that uses a single hyperplane to partition the covariate\nspace into two halves, softplus regressions employ multiple hyperplanes to\nconstruct a confined space, related to a single convex polytope defined by the\nintersection of multiple half-spaces or a union of multiple convex polytopes,\nto separate one class from the other. The gamma process is introduced to\nsupport the convolution of countably infinite (stacked) covariate-dependent\ngamma distributions. For Bayesian inference, Gibbs sampling derived via novel\ndata augmentation and marginalization techniques is used to deconvolve and/or\ndemix the highly complex nonlinear predictive distribution. Example results\ndemonstrate that softplus regressions provide flexible nonlinear decision\nboundaries, achieving classification accuracies comparable to that of kernel\nsupport vector machine while requiring significant less computation for\nout-of-sample prediction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 05:15:25 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1608.06412", "submitter": "Benjamin Guedj", "authors": "Alain Celisse and Benjamin Guedj", "title": "Stability revisited: new generalisation bounds for the Leave-one-Out", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present paper provides a new generic strategy leading to non-asymptotic\ntheoretical guarantees on the Leave-one-Out procedure applied to a broad class\nof learning algorithms. This strategy relies on two main ingredients: the new\nnotion of $L^q$ stability, and the strong use of moment inequalities. $L^q$\nstability extends the ongoing notion of hypothesis stability while remaining\nweaker than the uniform stability. It leads to new PAC exponential\ngeneralisation bounds for Leave-one-Out under mild assumptions. In the\nliterature, such bounds are available only for uniform stable algorithms under\nboundedness for instance. Our generic strategy is applied to the Ridge\nregression algorithm as a first step.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 08:11:29 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Celisse", "Alain", ""], ["Guedj", "Benjamin", ""]]}, {"id": "1608.06582", "submitter": "David Schnoerr", "authors": "David Schnoerr, Guido Sanguinetti and Ramon Grima", "title": "Approximation and inference methods for stochastic biochemical kinetics\n  - a tutorial review", "comments": "73 pages, 12 figures in J. Phys. A: Math. Theor. (2016)", "journal-ref": null, "doi": "10.1088/1751-8121/aa54d9", "report-no": null, "categories": "q-bio.QM cond-mat.stat-mech physics.bio-ph q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic fluctuations of molecule numbers are ubiquitous in biological\nsystems. Important examples include gene expression and enzymatic processes in\nliving cells. Such systems are typically modelled as chemical reaction networks\nwhose dynamics are governed by the Chemical Master Equation. Despite its simple\nstructure, no analytic solutions to the Chemical Master Equation are known for\nmost systems. Moreover, stochastic simulations are computationally expensive,\nmaking systematic analysis and statistical inference a challenging task.\nConsequently, significant effort has been spent in recent decades on the\ndevelopment of efficient approximation and inference methods. This article\ngives an introduction to basic modelling concepts as well as an overview of\nstate of the art methods. First, we motivate and introduce deterministic and\nstochastic methods for modelling chemical networks, and give an overview of\nsimulation and exact solution methods. Next, we discuss several approximation\nmethods, including the chemical Langevin equation, the system size expansion,\nmoment closure approximations, time-scale separation approximations and hybrid\nmethods. We discuss their various properties and review recent advances and\nremaining challenges for these methods. We present a comparison of several of\nthese methods by means of a numerical case study and highlight some of their\nrespective advantages and disadvantages. Finally, we discuss the problem of\ninference from experimental data in the Bayesian framework and review recent\nmethods developed the literature. In summary, this review gives a\nself-contained introduction to modelling, approximations and inference methods\nfor stochastic chemical kinetics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 17:13:05 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 13:05:27 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Schnoerr", "David", ""], ["Sanguinetti", "Guido", ""], ["Grima", "Ramon", ""]]}, {"id": "1608.06622", "submitter": "David Brandman", "authors": "Michael C. Burkhart, David M. Brandman, Carlos E. Vargas-Irwin,\n  Matthew T. Harrison", "title": "The discriminative Kalman filter for nonlinear and non-Gaussian\n  sequential Bayesian filtering", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter (KF) is used in a variety of applications for computing the\nposterior distribution of latent states in a state space model. The model\nrequires a linear relationship between states and observations. Extensions to\nthe Kalman filter have been proposed that incorporate linear approximations to\nnonlinear models, such as the extended Kalman filter (EKF) and the unscented\nKalman filter (UKF). However, we argue that in cases where the dimensionality\nof observed variables greatly exceeds the dimensionality of state variables, a\nmodel for $p(\\text{state}|\\text{observation})$ proves both easier to learn and\nmore accurate for latent space estimation. We derive and validate what we call\nthe discriminative Kalman filter (DKF): a closed-form discriminative version of\nBayesian filtering that readily incorporates off-the-shelf discriminative\nlearning techniques. Further, we demonstrate that given mild assumptions,\nhighly non-linear models for $p(\\text{state}|\\text{observation})$ can be\nspecified. We motivate and validate on synthetic datasets and in neural\ndecoding from non-human primates, showing substantial increases in decoding\nperformance versus the standard Kalman filter.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 19:53:20 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:49:27 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Burkhart", "Michael C.", ""], ["Brandman", "David M.", ""], ["Vargas-Irwin", "Carlos E.", ""], ["Harrison", "Matthew T.", ""]]}, {"id": "1608.06863", "submitter": "Victoria Peterson Mrs", "authors": "Victoria Peterson, Hugo Leonardo Rufiner, Ruben Daniel Spies", "title": "Kullback-Leibler Penalized Sparse Discriminant Analysis for\n  Event-Related Potential Classification", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain computer interface (BCI) is a system which provides direct\ncommunication between the mind of a person and the outside world by using only\nbrain activity (EEG). The event-related potential (ERP)-based BCI problem\nconsists of a binary pattern recognition. Linear discriminant analysis (LDA) is\nwidely used to solve this type of classification problems, but it fails when\nthe number of features is large relative to the number of observations. In this\nwork we propose a penalized version of the sparse discriminant analysis (SDA),\ncalled Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This\nmethod inherits both the discriminative feature selection and classification\nproperties of SDA and it also improves SDA performance through the addition of\nKullback-Leibler class discrepancy information. The KLSDA method is design to\nautomatically select the optimal regularization parameters. Numerical\nexperiments with two real ERP-EEG datasets show that this new method\noutperforms standard SDA.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 15:32:51 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Peterson", "Victoria", ""], ["Rufiner", "Hugo Leonardo", ""], ["Spies", "Ruben Daniel", ""]]}, {"id": "1608.06879", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Jakub Kone\\v{c}n\\'y, Peter Richt\\'arik, Barnab\\'as\n  P\\'ocz\\'os, Alex Smola", "title": "AIDE: Fast and Communication Efficient Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two new communication-efficient methods for\ndistributed minimization of an average of functions. The first algorithm is an\ninexact variant of the DANE algorithm that allows any local algorithm to return\nan approximate solution to a local subproblem. We show that such a strategy\ndoes not affect the theoretical guarantees of DANE significantly. In fact, our\napproach can be viewed as a robustification strategy since the method is\nsubstantially better behaved than DANE on data partition arising in practice.\nIt is well known that DANE algorithm does not match the communication\ncomplexity lower bounds. To bridge this gap, we propose an accelerated variant\nof the first method, called AIDE, that not only matches the communication lower\nbounds but can also be implemented using a purely first-order oracle. Our\nempirical results show that AIDE is superior to other communication efficient\nalgorithms in settings that naturally arise in machine learning applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:04:12 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""], ["P\u00f3cz\u00f3s", "Barnab\u00e1s", ""], ["Smola", "Alex", ""]]}, {"id": "1608.06884", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "Towards Bayesian Deep Learning: A Framework and Some Existing Methods", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 2016. This is a slightly shorter version of the survey\n  arXiv:1604.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While perception tasks such as visual object recognition and text\nunderstanding play an important role in human intelligence, the subsequent\ntasks that involve inference, reasoning and planning require an even higher\nlevel of intelligence. The past few years have seen major advances in many\nperception tasks using deep learning models. For higher-level inference,\nhowever, probabilistic graphical models with their Bayesian nature are still\nmore powerful and flexible. To achieve integrated intelligence that involves\nboth perception and inference, it is naturally desirable to tightly integrate\ndeep learning and Bayesian models within a principled probabilistic framework,\nwhich we call Bayesian deep learning. In this unified framework, the perception\nof text or images using deep learning can boost the performance of higher-level\ninference and in return, the feedback from the inference process is able to\nenhance the perception of text or images. This paper proposes a general\nframework for Bayesian deep learning and reviews its recent applications on\nrecommender systems, topic models, and control. In this paper, we also discuss\nthe relationship and differences between Bayesian deep learning and other\nrelated topics like Bayesian treatment of neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:15:22 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 15:32:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1608.07001", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen, Xiaoli Li", "title": "Incremental Minimax Optimization based Fuzzy Clustering for Large\n  Multi-view Data", "comments": "32 pages, 1 figures, submitted to Fuzzy Sets and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental clustering approaches have been proposed for handling large data\nwhen given data set is too large to be stored. The key idea of these approaches\nis to find representatives to represent each cluster in each data chunk and\nfinal data analysis is carried out based on those identified representatives\nfrom all the chunks. However, most of the incremental approaches are used for\nsingle view data. As large multi-view data generated from multiple sources\nbecomes prevalent nowadays, there is a need for incremental clustering\napproaches to handle both large and multi-view data. In this paper we propose a\nnew incremental clustering approach called incremental minimax optimization\nbased fuzzy clustering (IminimaxFCM) to handle large multi-view data. In\nIminimaxFCM, representatives with multiple views are identified to represent\neach cluster by integrating multiple complementary views using minimax\noptimization. The detailed problem formulation, updating rules derivation, and\nthe in-depth analysis of the proposed IminimaxFCM are provided. Experimental\nstudies on several real world multi-view data sets have been conducted. We\nobserved that IminimaxFCM outperforms related incremental fuzzy clustering in\nterms of clustering accuracy, demonstrating the great potential of IminimaxFCM\nfor large multi-view data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 01:56:20 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""], ["Li", "Xiaoli", ""]]}, {"id": "1608.07005", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen", "title": "Multi-View Fuzzy Clustering with Minimax Optimization for Effective\n  Clustering of Data from Multiple Sources", "comments": "34 pages, submitted to Expert Systems with Applications. arXiv admin\n  note: text overlap with arXiv:1608.07001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data clustering refers to categorizing a data set by making good\nuse of related information from multiple representations of the data. It\nbecomes important nowadays because more and more data can be collected in a\nvariety of ways, in different settings and from different sources, so each data\nset can be represented by different sets of features to form different views of\nit. Many approaches have been proposed to improve clustering performance by\nexploring and integrating heterogeneous information underlying different views.\nIn this paper, we propose a new multi-view fuzzy clustering approach called\nMinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In\nMinimaxFCM the consensus clustering results are generated based on minimax\noptimization in which the maximum disagreements of different weighted views are\nminimized. Moreover, the weight of each view can be learned automatically in\nthe clustering process. In addition, there is only one parameter to be set\nbesides the fuzzifier. The detailed problem formulation, updating rules\nderivation, and the in-depth analysis of the proposed MinimaxFCM are provided\nhere. Experimental studies on nine multi-view data sets including real world\nimage and document data sets have been conducted. We observed that MinimaxFCM\noutperforms related multi-view clustering approaches in terms of clustering\naccuracy, demonstrating the great potential of MinimaxFCM for multi-view data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 02:15:37 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""]]}, {"id": "1608.07019", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Jie Li, Qiaosheng Zhang and Yadong Wang", "title": "Comparison among dimensionality reduction techniques based on Random\n  Projection for cancer classification", "comments": null, "journal-ref": "Computational biology and chemistry, 65: 165-172, 2016", "doi": "10.1016/j.compbiolchem.2016.09.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Projection (RP) technique has been widely applied in many scenarios\nbecause it can reduce high-dimensional features into low-dimensional space\nwithin short time and meet the need of real-time analysis of massive data.\nThere is an urgent need of dimensionality reduction with fast increase of big\ngenomics data. However, the performance of RP is usually lower. We attempt to\nimprove classification accuracy of RP through combining other reduction\ndimension methods such as Principle Component Analysis (PCA), Linear\nDiscriminant Analysis (LDA), and Feature Selection (FS). We compared\nclassification accuracy and running time of different combination methods on\nthree microarray datasets and a simulation dataset. Experimental results show a\nremarkable improvement of 14.77% in classification accuracy of FS followed by\nRP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield\na more discriminative subspace with an increase of 13.65% on classification\naccuracy on the same dataset. FS followed by RP outperforms other combination\nmethods in classification accuracy on most of the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 05:14:57 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 13:56:03 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 02:52:17 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 01:59:19 GMT"}, {"version": "v5", "created": "Sat, 17 Jun 2017 04:12:57 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xie", "Haozhe", ""], ["Li", "Jie", ""], ["Zhang", "Qiaosheng", ""], ["Wang", "Yadong", ""]]}, {"id": "1608.07179", "submitter": "Yuichi Yoshida", "authors": "Kohei Hayashi, Yuichi Yoshida", "title": "Minimizing Quadratic Functions in Constant Time", "comments": "An extended abstract will appear in the proceedings of NIPS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sampling-based optimization method for quadratic functions is proposed. Our\nmethod approximately solves the following $n$-dimensional quadratic\nminimization problem in constant time, which is independent of $n$:\n$z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle +\nn\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle +\nn\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$\nis a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our\ntheoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such\nthat the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with\nprobability $1-\\delta$. The empirical performance (accuracy and runtime) is\npositively confirmed by numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:43:17 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Hayashi", "Kohei", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1608.07241", "submitter": "Roman Ilin", "authors": "Roman Ilin, Barbara A. Han", "title": "Formal Concept Analysis of Rodent Carriers of Zoonotic Disease", "comments": "5 pages, presented at 2016 ICML Workshop on #Data4Good: Machine\n  Learning in Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of Formal Concept Analysis is applied to a dataset describing\nthe traits of rodents, with the goal of identifying zoonotic disease\ncarriers,or those species carrying infections that can spillover to cause human\ndisease. The concepts identified among these species together provide\nrules-of-thumb about the intrinsic biological features of rodents that carry\nzoonotic diseases, and offer utility for better targeting field surveillance\nefforts in the search for novel disease carriers in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:29:23 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Ilin", "Roman", ""], ["Han", "Barbara A.", ""]]}, {"id": "1608.07251", "submitter": "Qingyang Li", "authors": "Qingyang Li, Tao Yang, Liang Zhan, Derrek Paul Hibar, Neda Jahanshad,\n  Yalin Wang, Jieping Ye, Paul M. Thompson, Jie Wang", "title": "Large-scale Collaborative Imaging Genetics Studies of Risk Genetic\n  Factors for Alzheimer's Disease Across Multiple Institutions", "comments": "Published on the 19th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI). 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) offer new opportunities to identify\ngenetic risk factors for Alzheimer's disease (AD). Recently, collaborative\nefforts across different institutions emerged that enhance the power of many\nexisting techniques on individual institution data. However, a major barrier to\ncollaborative studies of GWAS is that many institutions need to preserve\nindividual data privacy. To address this challenge, we propose a novel\ndistributed framework, termed Local Query Model (LQM) to detect risk SNPs for\nAD across multiple research institutions. To accelerate the learning process,\nwe propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening\nrule to identify irrelevant features and remove them from the optimization. To\nthe best of our knowledge, this is the first successful run of the\ncomputationally intensive model selection procedure to learn a consistent model\nacross different institutions without compromising their privacy while ranking\nthe SNPs that may collectively affect AD. Empirical studies are conducted on\n809 subjects with 5.9 million SNP features which are distributed across three\nindividual institutions. D-EDPP achieved a 66-fold speed-up by effectively\nidentifying irrelevant features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 23:21:49 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Li", "Qingyang", ""], ["Yang", "Tao", ""], ["Zhan", "Liang", ""], ["Hibar", "Derrek Paul", ""], ["Jahanshad", "Neda", ""], ["Wang", "Yalin", ""], ["Ye", "Jieping", ""], ["Thompson", "Paul M.", ""], ["Wang", "Jie", ""]]}, {"id": "1608.07441", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "comments": null, "journal-ref": "ECCV 16 WS TASK-CV: Transferring and Adapting Source Knowledge in\n  Computer Vision, Oct 2016, Amsterdam, Netherlands. ECCV 16 WS TASK-CV:\n  Transferring and Adapting Source Knowledge in Computer Vision", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot learning has been shown to be an efficient strategy for domain\nadaptation. In this context, this paper builds on the recent work of Bucher et\nal. [1], which proposed an approach to solve Zero-Shot classification problems\n(ZSC) by introducing a novel metric learning based objective function. This\nobjective function allows to learn an optimal embedding of the attributes\njointly with a measure of similarity between images and attributes. This paper\nextends their approach by proposing several schemes to control the generation\nof the negative pairs, resulting in a significant improvement of the\nperformance and giving above state-of-the-art results on three challenging ZSC\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:42:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1608.07494", "submitter": "Jonas Haslbeck", "authors": "Jonas M. B. Haslbeck, Dirk U. Wulff", "title": "Estimating the Number of Clusters via Normalized Cluster Instability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve current instability-based methods for the selection of the number\nof clusters $k$ in cluster analysis by developing a normalized cluster\ninstability measure that corrects for the distribution of cluster sizes, a\npreviously unaccounted driver of cluster instability. We show that our\nnormalized instability measure outperforms current instability-based measures\nacross the whole sequence of possible $k$ and especially overcomes limitations\nin the context of large $k$. We also compare, for the first time, model-based\nand model-free approaches to determine cluster-instability and find their\nperformance to be comparable. We make our method available in the R-package\n\\verb+cstab+.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 15:26:01 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 23:52:11 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 18:17:26 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 21:38:02 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Haslbeck", "Jonas M. B.", ""], ["Wulff", "Dirk U.", ""]]}, {"id": "1608.07502", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen, Lu-An Tang, Yizhou Sun, Zhengzhang Chen, Kai Zhang", "title": "Entity Embedding-based Anomaly Detection for Heterogeneous Categorical\n  Events", "comments": "Published as a conference paper in IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection plays an important role in modern data-driven security\napplications, such as detecting suspicious access to a socket from a process.\nIn many cases, such events can be described as a collection of categorical\nvalues that are considered as entities of different types, which we call\nheterogeneous categorical events. Due to the lack of intrinsic distance\nmeasures among entities, and the exponentially large event space, most existing\nwork relies heavily on heuristics to calculate abnormal scores for events.\nDifferent from previous work, we propose a principled and unified probabilistic\nmodel APE (Anomaly detection via Probabilistic pairwise interaction and Entity\nembedding) that directly models the likelihood of events. In this model, we\nembed entities into a common latent space using their observed co-occurrence in\ndifferent events. More specifically, we first model the compatibility of each\npair of entities according to their embeddings. Then we utilize the weighted\npairwise interactions of different entity types to define the event\nprobability. Using Noise-Contrastive Estimation with \"context-dependent\" noise\ndistribution, our model can be learned efficiently regardless of the large\nevent space. Experimental results on real enterprise surveillance data show\nthat our methods can accurately detect abnormal events compared to other\nstate-of-the-art abnormal detection techniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 16:15:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Chen", "Ting", ""], ["Tang", "Lu-An", ""], ["Sun", "Yizhou", ""], ["Chen", "Zhengzhang", ""], ["Zhang", "Kai", ""]]}, {"id": "1608.07526", "submitter": "Badong Chen", "authors": "Xi Liu, Badong Chen, Bin Xu, Zongze Wu and Paul Honeine", "title": "Maximum Correntropy Unscented Filter", "comments": "9 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unscented transformation (UT) is an efficient method to solve the state\nestimation problem for a non-linear dynamic system, utilizing a derivative-free\nhigher-order approximation by approximating a Gaussian distribution rather than\napproximating a non-linear function. Applying the UT to a Kalman filter type\nestimator leads to the well-known unscented Kalman filter (UKF). Although the\nUKF works very well in Gaussian noises, its performance may deteriorate\nsignificantly when the noises are non-Gaussian, especially when the system is\ndisturbed by some heavy-tailed impulsive noises. To improve the robustness of\nthe UKF against impulsive noises, a new filter for nonlinear systems is\nproposed in this work, namely the maximum correntropy unscented filter (MCUF).\nIn MCUF, the UT is applied to obtain the prior estimates of the state and\ncovariance matrix, and a robust statistical linearization regression based on\nthe maximum correntropy criterion (MCC) is then used to obtain the posterior\nestimates of the state and covariance. The satisfying performance of the new\nalgorithm is confirmed by two illustrative examples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 17:19:14 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Liu", "Xi", ""], ["Chen", "Badong", ""], ["Xu", "Bin", ""], ["Wu", "Zongze", ""], ["Honeine", "Paul", ""]]}, {"id": "1608.07536", "submitter": "Valentina Gregori", "authors": "Valentina Gregori and Barbara Caputo", "title": "Leveraging over intact priors for boosting control and dexterity of\n  prosthetic hands by amputees", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive myoelectric prostheses require a long training time to obtain\nsatisfactory control dexterity. These training times could possibly be reduced\nby leveraging over training efforts by previous subjects. So-called domain\nadaptation algorithms formalize this strategy and have indeed been shown to\nsignificantly reduce the amount of required training data for intact subjects\nfor myoelectric movements classification. It is not clear, however, whether\nthese results extend also to amputees and, if so, whether prior information\nfrom amputees and intact subjects is equally useful. To overcome this problem,\nwe evaluated several domain adaptation algorithms on data coming from both\namputees and intact subjects. Our findings indicate that: (1) the use of\nprevious experience from other subjects allows us to reduce the training time\nby about an order of magnitude; (2) this improvement holds regardless of\nwhether an amputee exploits previous information from other amputees or from\nintact subjects.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 17:47:58 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Gregori", "Valentina", ""], ["Caputo", "Barbara", ""]]}, {"id": "1608.07597", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker", "title": "A Randomized Approach to Efficient Kernel Clustering", "comments": "To appear in IEEE GlobalSIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based K-means clustering has gained popularity due to its simplicity\nand the power of its implicit non-linear representation of the data. A dominant\nconcern is the memory requirement since memory scales as the square of the\nnumber of data points. We provide a new analysis of a class of approximate\nkernel methods that have more modest memory requirements, and propose a\nspecific one-pass randomized kernel approximation followed by standard K-means\non the transformed data. The analysis and experiments suggest the method is\naccurate, while requiring drastically less memory than standard kernel K-means\nand significantly less memory than Nystrom based approximations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:26:31 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:08:40 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 17:51:12 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1608.07605", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar, Jing Qian, Venkatesh Saligrama", "title": "Clustering and Community Detection with Imbalanced Clusters", "comments": "Extended version of arXiv:1309.2303 with new applications. Accepted\n  to IEEE TSIPN", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2601022", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering methods which are frequently used in clustering and\ncommunity detection applications are sensitive to the specific graph\nconstructions particularly when imbalanced clusters are present. We show that\nratio cut (RCut) or normalized cut (NCut) objectives are not tailored to\nimbalanced cluster sizes since they tend to emphasize cut sizes over cut\nvalues. We propose a graph partitioning problem that seeks minimum cut\npartitions under minimum size constraints on partitions to deal with imbalanced\ncluster sizes. Our approach parameterizes a family of graphs by adaptively\nmodulating node degrees on a fixed node set, yielding a set of parameter\ndependent cuts reflecting varying levels of imbalance. The solution to our\nproblem is then obtained by optimizing over these parameters. We present\nrigorous limit cut analysis results to justify our approach and demonstrate the\nsuperiority of our method through experiments on synthetic and real datasets\nfor data clustering, semi-supervised learning and community detection.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:49:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Aksoylar", "Cem", ""], ["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1608.07630", "submitter": "Daniel Hsu", "authors": "Ji Xu, Daniel Hsu, Arian Maleki", "title": "Global analysis of Expectation Maximization for mixtures of two\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Maximization (EM) is among the most popular algorithms for\nestimating parameters of statistical models. However, EM, which is an iterative\nalgorithm based on the maximum likelihood principle, is generally only\nguaranteed to find stationary points of the likelihood objective, and these\npoints may be far from any maximizer. This article addresses this disconnect\nbetween the statistical principles behind EM and its algorithmic properties.\nSpecifically, it provides a global analysis of EM for specific models in which\nthe observations comprise an i.i.d. sample from a mixture of two Gaussians.\nThis is achieved by (i) studying the sequence of parameters from idealized\nexecution of EM in the infinite sample limit, and fully characterizing the\nlimit points of the sequence in terms of the initial parameters; and then (ii)\nbased on this convergence analysis, establishing statistical consistency (or\nlack thereof) for the actual sequence of parameters produced by EM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 23:53:43 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Xu", "Ji", ""], ["Hsu", "Daniel", ""], ["Maleki", "Arian", ""]]}, {"id": "1608.07636", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Sreeram Kannan, Baosen Zhang and Radha Poovendran", "title": "Learning Temporal Dependence from Time-Series Data with Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting where a collection of time series, modeled as random\nprocesses, evolve in a causal manner, and one is interested in learning the\ngraph governing the relationships of these processes. A special case of wide\ninterest and applicability is the setting where the noise is Gaussian and\nrelationships are Markov and linear. We study this setting with two additional\nfeatures: firstly, each random process has a hidden (latent) state, which we\nuse to model the internal memory possessed by the variables (similar to hidden\nMarkov models). Secondly, each variable can depend on its latent memory state\nthrough a random lag (rather than a fixed lag), thus modeling memory recall\nwith differing lags at distinct times. Under this setting, we develop an\nestimator and prove that under a genericity assumption, the parameters of the\nmodel can be learned consistently. We also propose a practical adaption of this\nestimator, which demonstrates significant performance gains in both synthetic\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:25:54 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Hosseini", "Hossein", ""], ["Kannan", "Sreeram", ""], ["Zhang", "Baosen", ""], ["Poovendran", "Radha", ""]]}, {"id": "1608.07690", "submitter": "Thomas Tanay", "authors": "Thomas Tanay and Lewis Griffin", "title": "A Boundary Tilting Persepective on the Phenomenon of Adversarial\n  Examples", "comments": "arXiv admin note: text overlap with arXiv:1412.6572 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to suffer from a surprising weakness:\ntheir classification outputs can be changed by small, non-random perturbations\nof their inputs. This adversarial example phenomenon has been explained as\noriginating from deep networks being \"too linear\" (Goodfellow et al., 2014). We\nshow here that the linear explanation of adversarial examples presents a number\nof limitations: the formal argument is not convincing, linear classifiers do\nnot always suffer from the phenomenon, and when they do their adversarial\nexamples are different from the ones affecting deep networks.\n  We propose a new perspective on the phenomenon. We argue that adversarial\nexamples exist when the classification boundary lies close to the submanifold\nof sampled data, and present a mathematical analysis of this new perspective in\nthe linear case. We define the notion of adversarial strength and show that it\ncan be reduced to the deviation angle between the classifier considered and the\nnearest centroid classifier. Then, we show that the adversarial strength can be\nmade arbitrarily high independently of the classification performance due to a\nmechanism that we call boundary tilting. This result leads us to defining a new\ntaxonomy of adversarial examples. Finally, we show that the adversarial\nstrength observed in practice is directly dependent on the level of\nregularisation used and the strongest adversarial examples, symptomatic of\noverfitting, can be avoided by using a proper level of regularisation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 10:44:54 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Tanay", "Thomas", ""], ["Griffin", "Lewis", ""]]}, {"id": "1608.07710", "submitter": "Yangming Zhou", "authors": "Yangming Zhou and Guoping Qiu", "title": "Random Forest for Label Ranking", "comments": "28 pages, 4 figures,accepted to Expert Systems With Applications in\n  June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label ranking aims to learn a mapping from instances to rankings over a\nfinite number of predefined labels. Random forest is a powerful and one of the\nmost successful general-purpose machine learning algorithms of modern times. In\nthis paper, we present a powerful random forest label ranking method which uses\nrandom decision trees to retrieve nearest neighbors. We have developed a novel\ntwo-step rank aggregation strategy to effectively aggregate neighboring\nrankings discovered by the random forest into a final predicted ranking.\nCompared with existing methods, the new random forest method has many\nadvantages including its intrinsically scalable tree data structure, highly\nparallel-able computational architecture and much superior performance. We\npresent extensive experimental results to demonstrate that our new method\nachieves the highly competitive performance compared with state-of-the-art\nmethods for datasets with complete ranking and datasets with only partial\nranking information.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 13:32:42 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 11:05:45 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 03:22:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhou", "Yangming", ""], ["Qiu", "Guoping", ""]]}, {"id": "1608.07734", "submitter": "Tameem Adel", "authors": "Tameem Adel, Cassio P. de Campos", "title": "Learning Bayesian Networks with Incomplete Data by Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for learning Bayesian networks from data with\nmissing values using a data augmentation approach. An exact Bayesian network\nlearning algorithm is obtained by recasting the problem into a standard\nBayesian network learning problem without missing data. To the best of our\nknowledge, this is the first exact algorithm for this problem. As expected, the\nexact algorithm does not scale to large domains. We build on the exact method\nto create an approximate algorithm using a hill-climbing technique. This\nalgorithm scales to large domains so long as a suitable standard structure\nlearning method for complete data is available. We perform a wide range of\nexperiments to demonstrate the benefits of learning Bayesian networks with such\nnew approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 18:41:47 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 01:50:25 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Adel", "Tameem", ""], ["de Campos", "Cassio P.", ""]]}, {"id": "1608.07739", "submitter": "Jordan Frecon", "authors": "Jordan Frecon, Nelly Pustelnik, Nicolas Dobigeon, Herwig Wendt and\n  Patrice Abry", "title": "Bayesian selection for the l2-Potts model regularization parameter: 1D\n  piecewise constant signal denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2715000", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise constant denoising can be solved either by deterministic\noptimization approaches, based on the Potts model, or by stochastic Bayesian\nprocedures. The former lead to low computational time but require the selection\nof a regularization parameter, whose value significantly impacts the achieved\nsolution, and whose automated selection remains an involved and challenging\nproblem. Conversely, fully Bayesian formalisms encapsulate the regularization\nparameter selection into hierarchical models, at the price of high\ncomputational costs. This contribution proposes an operational strategy that\ncombines hierarchical Bayesian and Potts model formulations, with the double\naim of automatically tuning the regularization parameter and of maintaining\ncomputational effciency. The proposed procedure relies on formally connecting a\nBayesian framework to a l2-Potts functional. Behaviors and performance for the\nproposed piecewise constant denoising and regularization parameter tuning\ntechniques are studied qualitatively and assessed quantitatively, and shown to\ncompare favorably against those of a fully Bayesian hierarchical procedure,\nboth in accuracy and in computational load.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 19:59:29 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 13:56:44 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Frecon", "Jordan", ""], ["Pustelnik", "Nelly", ""], ["Dobigeon", "Nicolas", ""], ["Wendt", "Herwig", ""], ["Abry", "Patrice", ""]]}, {"id": "1608.07892", "submitter": "Junqi Jin", "authors": "Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang", "title": "Optimizing Recurrent Neural Networks Architectures under Time\n  Constraints", "comments": "Withdrawn due to incompleteness and some overlaps with existing\n  literatures, I will resubmit adding further results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN)'s architecture is a key factor influencing its\nperformance. We propose algorithms to optimize hidden sizes under running time\nconstraint. We convert the discrete optimization into a subset selection\nproblem. By novel transformations, the objective function becomes submodular\nand constraint becomes supermodular. A greedy algorithm with bounds is\nsuggested to solve the transformed problem. And we show how transformations\ninfluence the bounds. To speed up optimization, surrogate functions are\nproposed which balance exploration and exploitation. Experiments show that our\nalgorithms can find more accurate models or faster models than manually tuned\nstate-of-the-art and random search. We also compare popular RNN architectures\nusing our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 02:14:48 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 13:37:52 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 03:45:44 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jin", "Junqi", ""], ["Yan", "Ziang", ""], ["Fu", "Kun", ""], ["Jiang", "Nan", ""], ["Zhang", "Changshui", ""]]}, {"id": "1608.07929", "submitter": "Fabrice Rossi", "authors": "Romain Guigour\\`es, Marc Boull\\'e, Fabrice Rossi (SAMM)", "title": "Discovering Patterns in Time-Varying Graphs: A Triclustering Approach", "comments": "Advances in Data Analysis and Classification, Springer Verlag, 2015,\n  Online First", "journal-ref": null, "doi": "10.1007/s11634-015-0218-6", "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel technique to track structures in time varying\ngraphs. The method uses a maximum a posteriori approach for adjusting a\nthree-dimensional co-clustering of the source vertices, the destination\nvertices and the time, to the data under study, in a way that does not require\nany hyper-parameter tuning. The three dimensions are simultaneously segmented\nin order to build clusters of source vertices, destination vertices and time\nsegments where the edge distributions across clusters of vertices follow the\nsame evolution over the time segments. The main novelty of this approach lies\nin that the time segments are directly inferred from the evolution of the edge\ndistribution between the vertices, thus not requiring the user to make any a\npriori quantization. Experiments conducted on artificial data illustrate the\ngood behavior of the technique, and a study of a real-life data set shows the\npotential of the proposed approach for exploratory data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 07:06:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Guigour\u00e8s", "Romain", "", "SAMM"], ["Boull\u00e9", "Marc", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1608.07934", "submitter": "Hadi Zare", "authors": "Hadi Zare and Mojtaba Niazi", "title": "Relevant based structure learning for feature selection", "comments": "29 pages, 11 figures", "journal-ref": "Eng. Appl. Artif. Intel. 55 (2016) 93-102", "doi": "10.1016/j.engappai.2016.06.001", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important task in many problems occurring in pattern\nrecognition, bioinformatics, machine learning and data mining applications. The\nfeature selection approach enables us to reduce the computation burden and the\nfalling accuracy effect of dealing with huge number of features in typical\nlearning problems. There is a variety of techniques for feature selection in\nsupervised learning problems based on different selection metrics. In this\npaper, we propose a novel unified framework for feature selection built on the\ngraphical models and information theoretic tools. The proposed approach\nexploits the structure learning among features to select more relevant and less\nredundant features to the predictive modeling problem according to a primary\nnovel likelihood based criterion. In line with the selection of the optimal\nsubset of features through the proposed method, it provides us the Bayesian\nnetwork classifier without the additional cost of model training on the\nselected subset of features. The optimal properties of our method are\nestablished through empirical studies and computational complexity analysis.\nFurthermore the proposed approach is evaluated on a bunch of benchmark datasets\nbased on the well-known classification algorithms. Extensive experiments\nconfirm the significant improvement of the proposed approach compared to the\nearlier works.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 07:21:20 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Zare", "Hadi", ""], ["Niazi", "Mojtaba", ""]]}, {"id": "1608.07986", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou and Alexey Lindo and Eric B. Ford", "title": "Geometric adaptive Monte Carlo in random environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold Markov chain Monte Carlo algorithms have been introduced to sample\nmore effectively from challenging target densities exhibiting multiple modes or\nstrong correlations. Such algorithms exploit the local geometry of the\nparameter space, thus enabling chains to achieve a faster convergence rate when\nmeasured in number of steps. However, acquiring local geometric information can\noften increase computational complexity per step to the extent that sampling\nfrom high-dimensional targets becomes inefficient in terms of total\ncomputational time. This paper analyzes the computational complexity of\nmanifold Langevin Monte Carlo and proposes a geometric adaptive Monte Carlo\nsampler aimed at balancing the benefits of exploiting local geometry with\ncomputational cost to achieve a high effective sample size for a given\ncomputational cost. The suggested sampler is a discrete-time stochastic process\nin random environment. The random environment allows to switch between local\ngeometric and adaptive proposal kernels with the help of a schedule. An\nexponential schedule is put forward that enables more frequent use of geometric\ninformation in early transient phases of the chain, while saving computational\ntime in late stationary phases. The average complexity can be manually set\ndepending on the need for geometric exploitation posed by the underlying model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 10:32:15 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 18:56:51 GMT"}, {"version": "v3", "created": "Sat, 5 May 2018 20:21:14 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 05:50:54 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Lindo", "Alexey", ""], ["Ford", "Eric B.", ""]]}, {"id": "1608.08052", "submitter": "Balamurugan P", "authors": "Nicolas Flammarion and Balamurugan Palaniappan and Francis Bach", "title": "Robust Discriminative Clustering with Sparse Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering high-dimensional data often requires some form of dimensionality\nreduction, where clustered variables are separated from \"noise-looking\"\nvariables. We cast this problem as finding a low-dimensional projection of the\ndata which is well-clustered. This yields a one-dimensional projection in the\nsimplest situation with two clusters, and extends naturally to a multi-label\nscenario for more than two clusters. In this paper, (a) we first show that this\njoint clustering and dimension reduction formulation is equivalent to\npreviously proposed discriminative clustering frameworks, thus leading to\nconvex relaxations of the problem, (b) we propose a novel sparse extension,\nwhich is still cast as a convex relaxation and allows estimation in higher\ndimensions, (c) we propose a natural extension for the multi-label scenario,\n(d) we provide a new theoretical analysis of the performance of these\nformulations with a simple probabilistic model, leading to scalings over the\nform $d=O(\\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse\ncase, where $n$ is the number of examples and $d$ the ambient dimension, and\nfinally, (e) we propose an efficient iterative algorithm with running-time\ncomplexity proportional to $O(nd^2)$, improving on earlier algorithms which had\nquadratic complexity in the number of examples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:00:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Flammarion", "Nicolas", ""], ["Palaniappan", "Balamurugan", ""], ["Bach", "Francis", ""]]}, {"id": "1608.08063", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary, Marco Cuturi, Nicolas Courty, Alain Rakotomamonjy", "title": "Wasserstein Discriminant Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5717-1", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Discriminant Analysis (WDA) is a new supervised method that can\nimprove classification of high-dimensional data by computing a suitable linear\nmap onto a lower dimensional subspace. Following the blueprint of classical\nLinear Discriminant Analysis (LDA), WDA selects the projection matrix that\nmaximizes the ratio of two quantities: the dispersion of projected points\ncoming from different classes, divided by the dispersion of projected points\ncoming from the same class. To quantify dispersion, WDA uses regularized\nWasserstein distances, rather than cross-variance measures which have been\nusually considered, notably in LDA. Thanks to the the underlying principles of\noptimal transport, WDA is able to capture both global (at distribution scale)\nand local (at samples scale) interactions between classes. Regularized\nWasserstein distances can be computed using the Sinkhorn matrix scaling\nalgorithm; We show that the optimization of WDA can be tackled using automatic\ndifferentiation of Sinkhorn iterations. Numerical experiments show promising\nresults both in terms of prediction and visualization on toy examples and real\nlife datasets such as MNIST and on deep features obtained from a subset of the\nCaltech dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:18:40 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 08:42:15 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Flamary", "R\u00e9mi", ""], ["Cuturi", "Marco", ""], ["Courty", "Nicolas", ""], ["Rakotomamonjy", "Alain", ""]]}, {"id": "1608.08225", "submitter": "Max Tegmark", "authors": "Henry W. Lin (Harvard), Max Tegmark (MIT), David Rolnick (MIT)", "title": "Why does deep and cheap learning work so well?", "comments": "Replaced to match version published in Journal of Statistical\n  Physics: https://link.springer.com/article/10.1007/s10955-017-1836-5 Improved\n  refs & discussion, typos fixed. 16 pages, 3 figs", "journal-ref": null, "doi": "10.1007/s10955-017-1836-5", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the success of deep learning could depend not only on mathematics\nbut also on physics: although well-known mathematical theorems guarantee that\nneural networks can approximate arbitrary functions well, the class of\nfunctions of practical interest can frequently be approximated through \"cheap\nlearning\" with exponentially fewer parameters than generic ones. We explore how\nproperties frequently encountered in physics such as symmetry, locality,\ncompositionality, and polynomial log-probability translate into exceptionally\nsimple neural networks. We further argue that when the statistical process\ngenerating the data is of a certain hierarchical form prevalent in physics and\nmachine-learning, a deep neural network can be more efficient than a shallow\none. We formalize these claims using information theory and discuss the\nrelation to the renormalization group. We prove various \"no-flattening\ntheorems\" showing when efficient linear deep networks cannot be accurately\napproximated by shallow ones without efficiency loss, for example, we show that\n$n$ variables cannot be multiplied using fewer than 2^n neurons in a single\nhidden layer.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 20:00:14 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 00:38:45 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 02:15:43 GMT"}, {"version": "v4", "created": "Thu, 3 Aug 2017 18:32:53 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lin", "Henry W.", "", "Harvard"], ["Tegmark", "Max", "", "MIT"], ["Rolnick", "David", "", "MIT"]]}, {"id": "1608.08266", "submitter": "Nicola Di Mauro", "authors": "Antonio Vergari and Nicola Di Mauro and Floriana Esposito", "title": "Visualizing and Understanding Sum-Product Networks", "comments": "Machine Learning Journal paper (First Online), 24 pages", "journal-ref": null, "doi": "10.1007/s10994-018-5760-y", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sum-Product Networks (SPNs) are recently introduced deep tractable\nprobabilistic models by which several kinds of inference queries can be\nanswered exactly and in a tractable time. Up to now, they have been largely\nused as black box density estimators, assessed only by comparing their\nlikelihood scores only. In this paper we explore and exploit the inner\nrepresentations learned by SPNs. We do this with a threefold aim: first we want\nto get a better understanding of the inner workings of SPNs; secondly, we seek\nadditional ways to evaluate one SPN model and compare it against other\nprobabilistic models, providing diagnostic tools to practitioners; lastly, we\nwant to empirically evaluate how good and meaningful the extracted\nrepresentations are, as in a classic Representation Learning framework. In\norder to do so we revise their interpretation as deep neural networks and we\npropose to exploit several visualization techniques on their node activations\nand network outputs under different types of inference queries. To investigate\nthese models as feature extractors, we plug some SPNs, learned in a greedy\nunsupervised fashion on image datasets, in supervised classification learning\ntasks. We extract several embedding types from node activations by filtering\nnodes by their type, by their associated feature abstraction level and by their\nscope. In a thorough empirical comparison we prove them to be competitive\nagainst those generated from popular feature extractors as Restricted Boltzmann\nMachines. Finally, we investigate embeddings generated from random\nprobabilistic marginal queries as means to compare other tractable\nprobabilistic models on a common ground, extending our experiments to Mixtures\nof Trees.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 22:10:17 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 16:27:28 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Vergari", "Antonio", ""], ["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1608.08306", "submitter": "Faris B Mismar", "authors": "Faris B. Mismar, Brian L. Evans", "title": "Machine Learning in Downlink Coordinated Multipoint in Heterogeneous\n  Networks", "comments": "5 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1812.03421", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for downlink coordinated multipoint (DL CoMP) in\nheterogeneous fifth generation New Radio (NR) networks. The primary\ncontribution of our paper is an algorithm to enhance the trigger of DL CoMP\nusing online machine learning. We use support vector machine (SVM) classifiers\nto enhance the user downlink throughput in a realistic frequency division\nduplex network environment. Our simulation results show improvement in both the\nmacro and pico base station downlink throughputs due to the informed triggering\nof the multiple radio streams as learned by the SVM classifier.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 02:36:41 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 00:14:23 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 01:24:01 GMT"}, {"version": "v4", "created": "Mon, 5 Mar 2018 14:39:19 GMT"}, {"version": "v5", "created": "Sun, 2 Sep 2018 03:39:34 GMT"}, {"version": "v6", "created": "Fri, 1 Feb 2019 23:21:12 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Mismar", "Faris B.", ""], ["Evans", "Brian L.", ""]]}, {"id": "1608.08337", "submitter": "Avleen Bijral", "authors": "Avleen S. Bijral", "title": "Data Dependent Convergence for Distributed Stochastic Optimization", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation we propose alternative analysis of distributed\nstochastic gradient descent (SGD) algorithms that rely on spectral properties\nof the data covariance. As a consequence we can relate questions pertaining to\nspeedups and convergence rates for distributed SGD to the data distribution\ninstead of the regularity properties of the objective functions. More precisely\nwe show that this rate depends on the spectral norm of the sample covariance\nmatrix. An estimate of this norm can provide practitioners with guidance\ntowards a potential gain in algorithm performance. For example many sparse\ndatasets with low spectral norm prove to be amenable to gains in distributed\nsettings. Towards establishing this data dependence we first study a\ndistributed consensus-based SGD algorithm and show that the rate of convergence\ninvolves the spectral norm of the sample covariance matrix when the underlying\ndata is assumed to be independent and identically distributed (homogenous).\nThis dependence allows us to identify network regimes that prove to be\nbeneficial for datasets with low sample covariance spectral norm. Existing\nconsensus based analyses prove to be sub-optimal in the homogenous setting. Our\nanalysis method also allows us to find data-dependent convergence rates as we\nlimit the amount of communication. Spreading a fixed amount of data across more\nnodes slows convergence; in the asymptotic regime we show that adding more\nmachines can help when minimizing twice-differentiable losses. Since the\nmini-batch results don't follow from the consensus results we propose a\ndifferent data dependent analysis thereby providing theoretical validation for\nwhy certain datasets are more amenable to mini-batching. We also provide\nempirical evidence for results in this thesis.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 05:58:38 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Bijral", "Avleen S.", ""]]}, {"id": "1608.08362", "submitter": "Vahid Bastani", "authors": "Vahid Bastani, Lucio Marcenaro, Carlo Regazzoni", "title": "Incremental Nonlinear System Identification and Adaptive Particle\n  Filtering Using Gaussian Process", "comments": "submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An incremental/online state dynamic learning method is proposed for\nidentification of the nonlinear Gaussian state space models. The method embeds\nthe stochastic variational sparse Gaussian process as the probabilistic state\ndynamic model inside a particle filter framework. Model updating is done at\nmeasurement sample rate using stochastic gradient descent based optimization\nimplemented in the state estimation filtering loop. The performance of the\nproposed method is compared with state-of-the-art Gaussian process based batch\nlearning methods. Finally, it is shown that the state estimation performance\nsignificantly improves due to the online learning of state dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 08:32:13 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Bastani", "Vahid", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1608.08659", "submitter": "Yuying Xie", "authors": "Yuying Xie, Yufeng Liu and William Valdar", "title": "Joint Estimation of Multiple Dependent Gaussian Graphical Models with\n  Applications to Mouse Genomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are widely used to represent conditional dependence\namong random variables. In this paper, we propose a novel estimator for data\narising from a group of Gaussian graphical models that are themselves\ndependent. A motivating example is that of modeling gene expression collected\non multiple tissues from the same individual: here the multivariate outcome is\naffected by dependencies acting not only at the level of the specific tissues,\nbut also at the level of the whole body; existing methods that assume\nindependence among graphs are not applicable in this case. To estimate multiple\ndependent graphs, we decompose the problem into two graphical layers: the\nsystemic layer, which affects all outcomes and thereby induces cross- graph\ndependence, and the category-specific layer, which represents graph-specific\nvariation. We propose a graphical EM technique that estimates both layers\njointly, establish estimation consistency and selection sparsistency of the\nproposed estimator, and confirm by simulation that the EM method is superior to\na simple one-step method. We apply our technique to mouse genomics data and\nobtain biologically plausible results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 21:12:07 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Xie", "Yuying", ""], ["Liu", "Yufeng", ""], ["Valdar", "William", ""]]}, {"id": "1608.08698", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov", "title": "Reconstructing parameters of spreading models from partial observations", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.dis-nn physics.soc-ph q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreading processes are often modelled as a stochastic dynamics occurring on\ntop of a given network with edge weights corresponding to the transmission\nprobabilities. Knowledge of veracious transmission probabilities is essential\nfor prediction, optimization, and control of diffusion dynamics. Unfortunately,\nin most cases the transmission rates are unknown and need to be reconstructed\nfrom the spreading data. Moreover, in realistic settings it is impossible to\nmonitor the state of each node at every time, and thus the data is highly\nincomplete. We introduce an efficient dynamic message-passing algorithm, which\nis able to reconstruct parameters of the spreading model given only partial\ninformation on the activation times of nodes in the network. The method is\ngeneralizable to a large class of dynamic models, as well to the case of\ntemporal graphs.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 00:59:54 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Lokhov", "Andrey Y.", ""]]}, {"id": "1608.08761", "submitter": "Tingting Xie", "authors": "Tingting Xie, Yuxing Peng, Changjian Wang", "title": "hi-RF: Incremental Learning Random Forest for large-scale multi-class\n  Data Classification", "comments": "Accepted by AIIE2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, dynamically growing data and incrementally growing number of\nclasses pose new challenges to large-scale data classification research. Most\ntraditional methods struggle to balance the precision and computational burden\nwhen data and its number of classes increased. However, some methods are with\nweak precision, and the others are time-consuming. In this paper, we propose an\nincremental learning method, namely, heterogeneous incremental Nearest Class\nMean Random Forest (hi-RF), to handle this issue. It is a heterogeneous method\nthat either replaces trees or updates trees leaves in the random forest\nadaptively, to reduce the computational time in comparable performance, when\ndata of new classes arrive. Specifically, to keep the accuracy, one proportion\nof trees are replaced by new NCM decision trees; to reduce the computational\nload, the rest trees are updated their leaves probabilities only. Most of all,\nout-of-bag estimation and out-of-bag boosting are proposed to balance the\naccuracy and the computational efficiency. Fair experiments were conducted and\ndemonstrated its comparable precision with much less computational time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 08:18:48 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:34:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Xie", "Tingting", ""], ["Peng", "Yuxing", ""], ["Wang", "Changjian", ""]]}, {"id": "1608.08852", "submitter": "Martin Genzel", "authors": "Martin Genzel and Gitta Kutyniok", "title": "A Mathematical Framework for Feature Selection from Real-World Data with\n  Non-Linear Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenge of feature selection based on a\nrelatively small collection of sample pairs $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$.\nThe observations $y_i \\in \\mathbb{R}$ are thereby supposed to follow a noisy\nsingle-index model, depending on a certain set of signal variables. A major\ndifficulty is that these variables usually cannot be observed directly, but\nrather arise as hidden factors in the actual data vectors $x_i \\in\n\\mathbb{R}^d$ (feature variables). We will prove that a successful variable\nselection is still possible in this setup, even when the applied estimator does\nnot have any knowledge of the underlying model parameters and only takes the\n'raw' samples $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$ as input. The model\nassumptions of our results will be fairly general, allowing for non-linear\nobservations, arbitrary convex signal structures as well as strictly convex\nloss functions. This is particularly appealing for practical purposes, since in\nmany applications, already standard methods, e.g., the Lasso or logistic\nregression, yield surprisingly good outcomes. Apart from a general discussion\nof the practical scope of our theoretical findings, we will also derive a\nrigorous guarantee for a specific real-world problem, namely sparse feature\nextraction from (proteomics-based) mass spectrometry data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 13:53:09 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Genzel", "Martin", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "1608.08925", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Recursive Partitioning for Personalization using Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to choose from m discrete treatment options\n(e.g., news item or medical drug) the one with best causal effect for a\nparticular instance (e.g., user or patient) where the training data consists of\npassive observations of covariates, treatment, and the outcome of the\ntreatment. The standard approach to this problem is regress and compare: split\nthe training data by treatment, fit a regression model in each split, and, for\na new instance, predict all m outcomes and pick the best. By reformulating the\nproblem as a single learning task rather than m separate ones, we propose a new\napproach based on recursively partitioning the data into regimes where\ndifferent treatments are optimal. We extend this approach to an optimal\npartitioning approach that finds a globally optimal partition, achieving a\ncompact, interpretable, and impactful personalization model. We develop new\ntools for validating and evaluating personalization models on observational\ndata and use these to demonstrate the power of our novel approaches in a\npersonalized medicine and a job training application.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:20:59 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 22:59:40 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 13:12:19 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1608.08967", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "Robustness of classifiers: from adversarial to random noise", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown that state-of-the-art classifiers are\nvulnerable to worst-case (i.e., adversarial) perturbations of the datapoints.\nOn the other hand, it has been empirically observed that these same classifiers\nare relatively robust to random noise. In this paper, we propose to study a\n\\textit{semi-random} noise regime that generalizes both the random and\nworst-case noise regimes. We propose the first quantitative analysis of the\nrobustness of nonlinear classifiers in this general noise regime. We establish\nprecise theoretical bounds on the robustness of classifiers in this general\nregime, which depend on the curvature of the classifier's decision boundary.\nOur bounds confirm and quantify the empirical observations that classifiers\nsatisfying curvature constraints are robust to random noise. Moreover, we\nquantify the robustness of classifiers in terms of the subspace dimension in\nthe semi-random noise regime, and show that our bounds remarkably interpolate\nbetween the worst-case and random noise regimes. We perform experiments and\nshow that the derived bounds provide very accurate estimates when applied to\nvarious state-of-the-art deep neural networks and datasets. This result\nsuggests bounds on the curvature of the classifiers' decision boundaries that\nwe support experimentally, and more generally offers important insights onto\nthe geometry of high dimensional classification problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:54:34 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1608.08968", "submitter": "Amir Sepehri", "authors": "Amir Sepehri", "title": "The Bayesian SLOPE", "comments": "19 pages, 4 figures, R software available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SLOPE estimates regression coefficients by minimizing a regularized\nresidual sum of squares using a sorted-$\\ell_1$-norm penalty. The SLOPE\ncombines testing and estimation in regression problems. It exhibits suitable\nvariable selection and prediction properties, as well as minimax optimality.\nThis paper introduces the Bayesian SLOPE procedure for linear regression. The\nclassical SLOPE estimate is the posterior mode in the normal regression problem\nwith an appropriate prior on the coefficients. The Bayesian SLOPE considers the\nfull Bayesian model and has the advantage of offering credible sets and\nstandard error estimates for the parameters. Moreover, the hierarchical\nBayesian framework allows for full Bayesian and empirical Bayes treatment of\nthe penalty coefficients; whereas it is not clear how to choose these\ncoefficients when using the SLOPE on a general design matrix. A direct\ncharacterization of the posterior is provided which suggests a Gibbs sampler\nthat does not involve latent variables. An efficient hybrid Gibbs sampler for\nthe Bayesian SLOPE is introduced. Point estimation using the posterior mean is\nhighlighted, which automatically facilitates the Bayesian prediction of future\nobservations. These are demonstrated on real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:55:52 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 07:55:12 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Sepehri", "Amir", ""]]}, {"id": "1608.08984", "submitter": "Jonathan Ortigosa-Hern\\'andez", "authors": "Jonathan Ortigosa-Hern\\'andez, I\\~naki Inza, Jose A. Lozano", "title": "Towards Competitive Classifiers for Unbalanced Classification Problems:\n  A Study on the Performance Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a great methodological effort has been invested in proposing\ncompetitive solutions to the class-imbalance problem, little effort has been\nmade in pursuing a theoretical understanding of this matter.\n  In order to shed some light on this topic, we perform, through a novel\nframework, an exhaustive analysis of the adequateness of the most commonly used\nperformance scores to assess this complex scenario. We conclude that using\nunweighted H\\\"older means with exponent $p \\leq 1$ to average the recalls of\nall the classes produces adequate scores which are capable of determining\nwhether a classifier is competitive.\n  Then, we review the major solutions presented in the class-imbalance\nliterature. Since any learning task can be defined as an optimisation problem\nwhere a loss function, usually connected to a particular score, is minimised,\nour goal, here, is to find whether the learning tasks found in the literature\nare also oriented to maximise the previously detected adequate scores. We\nconclude that they usually maximise the unweighted H\\\"older mean with $p = 1$\n(a-mean).\n  Finally, we provide bounds on the values of the studied performance scores\nwhich guarantee a classifier with a higher recall than the random classifier in\neach and every class.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:34:51 GMT"}], "update_date": "2016-09-04", "authors_parsed": [["Ortigosa-Hern\u00e1ndez", "Jonathan", ""], ["Inza", "I\u00f1aki", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1608.09014", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "A Tutorial on Online Supervised Learning with Applications to Node\n  Classification in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the elegant observation of T. Cover '65 which, perhaps, is not as\nwell-known to the broader community as it should be. The first goal of the\ntutorial is to explain---through the prism of this elementary result---how to\nsolve certain sequence prediction problems by modeling sets of solutions rather\nthan the unknown data-generating mechanism. We extend Cover's observation in\nseveral directions and focus on computational aspects of the proposed\nalgorithms. The applicability of the methods is illustrated on several\nexamples, including node classification in a network.\n  The second aim of this tutorial is to demonstrate the following phenomenon:\nit is possible to predict as well as a combinatorial \"benchmark\" for which we\nhave a certain multiplicative approximation algorithm, even if the exact\ncomputation of the benchmark given all the data is NP-hard. The proposed\nprediction methods, therefore, circumvent some of the computational\ndifficulties associated with finding the best model given the data. These\ndifficulties arise rather quickly when one attempts to develop a probabilistic\nmodel for graph-based or other problems with a combinatorial structure.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:55:35 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}]