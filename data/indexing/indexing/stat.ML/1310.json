[{"id": "1310.0154", "submitter": "Yudong Chen", "authors": "Yudong Chen", "title": "Incoherence-Optimal Matrix Completion", "comments": "Fixed a minor error in Theorem 3 for matrix decomposition. To appear\n  in the IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2015.2415195", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the matrix completion problem. We show that it is not\nnecessary to assume joint incoherence, which is a standard but unintuitive and\nrestrictive condition that is imposed by previous studies. This leads to a\nsample complexity bound that is order-wise optimal with respect to the\nincoherence parameter (as well as to the rank $r$ and the matrix dimension $n$\nup to a log factor). As a consequence, we improve the sample complexity of\nrecovering a semidefinite matrix from $O(nr^{2}\\log^{2}n)$ to $O(nr\\log^{2}n)$,\nand the highest allowable rank from $\\Theta(\\sqrt{n}/\\log n)$ to\n$\\Theta(n/\\log^{2}n)$. The key step in proof is to obtain new bounds on the\n$\\ell_{\\infty,2}$-norm, defined as the maximum of the row and column norms of a\nmatrix. To illustrate the applicability of our techniques, we discuss\nextensions to SVD projection, structured matrix completion and semi-supervised\nclustering, for which we provide order-wise improvements over existing results.\nFinally, we turn to the closely-related problem of low-rank-plus-sparse matrix\ndecomposition. We show that the joint incoherence condition is unavoidable here\nfor polynomial-time algorithms conditioned on the Planted Clique conjecture.\nThis means it is intractable in general to separate a rank-$\\omega(\\sqrt{n})$\npositive semidefinite matrix and a sparse matrix. Interestingly, our results\nshow that the standard and joint incoherence conditions are associated\nrespectively with the information (statistical) and computational aspects of\nthe matrix decomposition problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 06:37:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 04:25:30 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2013 06:36:42 GMT"}, {"version": "v4", "created": "Fri, 13 Feb 2015 11:18:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Yudong", ""]]}, {"id": "1310.0188", "submitter": "Hau-tieng Wu", "authors": "Noureddine El Karoui, Hau-tieng Wu", "title": "Graph connection Laplacian and random matrices with random blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.SP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph connection Laplacian (GCL) is a modern data analysis technique that is\nstarting to be applied for the analysis of high dimensional and massive\ndatasets. Motivated by this technique, we study matrices that are akin to the\nones appearing in the null case of GCL, i.e the case where there is no\nstructure in the dataset under investigation. Developing this understanding is\nimportant in making sense of the output of the algorithms based on GCL. We\nhence develop a theory explaining the behavior of the spectral distribution of\na large class of random matrices, in particular random matrices with random\nblock entries of fixed size. Part of the theory covers the case where there is\nsignificant dependence between the blocks. Numerical work shows that the\nagreement between our theoretical predictions and numerical simulations is\ngenerally very good.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 08:38:13 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 00:27:44 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1310.0376", "submitter": "Nicolas Dobigeon", "authors": "Olivier Besson and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Joint Bayesian estimation of close subspaces from noisy measurements", "comments": "Submitted for publication in IEEE Signal Process. Letters", "journal-ref": null, "doi": "10.1109/LSP.2013.2296138", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we consider two sets of observations defined as subspace\nsignals embedded in noise and we wish to analyze the distance between these two\nsubspaces. The latter entails evaluating the angles between the subspaces, an\nissue reminiscent of the well-known Procrustes problem. A Bayesian approach is\ninvestigated where the subspaces of interest are considered as random with a\njoint prior distribution (namely a Bingham distribution), which allows the\ncloseness of the two subspaces to be adjusted. Within this framework, the\nminimum mean-square distance estimator of both subspaces is formulated and\nimplemented via a Gibbs sampler. A simpler scheme based on alternative maximum\na posteriori estimation is also presented. The new schemes are shown to provide\nmore accurate estimates of the angles between the subspaces, compared to\nsingular value decomposition based independent estimation of the two subspaces.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 16:28:59 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Besson", "Olivier", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1310.0423", "submitter": "Prakash Balachandran", "authors": "Prakash Balachandran, Edoardo Airoldi and Eric Kolaczyk", "title": "Inference of Network Summary Statistics Through Network Denoising", "comments": "47 pages, 3 figures, submitted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observing an undirected network that is `noisy' in the sense that\nthere are Type I and Type II errors in the observation of edges. Such errors\ncan arise, for example, in the context of inferring gene regulatory networks in\ngenomics or functional connectivity networks in neuroscience. Given a single\nobserved network then, to what extent are summary statistics for that network\nrepresentative of their analogues for the true underlying network? Can we infer\nsuch statistics more accurately by taking into account the noise in the\nobserved network edges?\n  In this paper, we answer both of these questions. In particular, we develop a\nspectral-based methodology using the adjacency matrix to `denoise' the observed\nnetwork data and produce more accurate inference of the summary statistics of\nthe true network. We characterize performance of our methodology through bounds\non appropriate notions of risk in the $L^2$ sense, and conclude by illustrating\nthe practical impact of this work on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 18:54:14 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 20:51:33 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 11:27:10 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Balachandran", "Prakash", ""], ["Airoldi", "Edoardo", ""], ["Kolaczyk", "Eric", ""]]}, {"id": "1310.0432", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie", "title": "Online Learning of Dynamic Parameters in Social Networks", "comments": "12 pages, To appear in Neural Information Processing Systems (NIPS)\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 19:08:04 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rakhlin", "Alexander", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1310.0509", "submitter": "Isik Baris Fidaner", "authors": "I\\c{s}{\\i}k Bar{\\i}\\c{s} Fidaner and Ali Taylan Cemgil", "title": "Summary Statistics for Partitionings and Feature Allocations", "comments": "Accepted to NIPS 2013:\n  https://nips.cc/Conferences/2013/Program/event.php?ID=3763", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Infinite mixture models are commonly used for clustering. One can sample from\nthe posterior of mixture assignments by Monte Carlo methods or find its maximum\na posteriori solution by optimization. However, in some problems the posterior\nis diffuse and it is hard to interpret the sampled partitionings. In this\npaper, we introduce novel statistics based on block sizes for representing\nsample sets of partitionings and feature allocations. We develop an\nelement-based definition of entropy to quantify segmentation among their\nelements. Then we propose a simple algorithm called entropy agglomeration (EA)\nto summarize and visualize this information. Experiments on various infinite\nmixture posteriors as well as a feature allocation dataset demonstrate that the\nproposed statistics are useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 22:34:18 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 06:28:18 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2013 18:26:44 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2013 08:43:59 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Fidaner", "I\u015f\u0131k Bar\u0131\u015f", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1310.0512", "submitter": "Rui Wu", "authors": "Jiaming Xu, Rui Wu, Kai Zhu, Bruce Hajek, R. Srikant, Lei Ying", "title": "Jointly Clustering Rows and Columns of Binary Matrices: Algorithms and\n  Trade-offs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard clustering problems, data points are represented by vectors, and\nby stacking them together, one forms a data matrix with row or column cluster\nstructure. In this paper, we consider a class of binary matrices, arising in\nmany applications, which exhibit both row and column cluster structure, and our\ngoal is to exactly recover the underlying row and column clusters by observing\nonly a small fraction of noisy entries. We first derive a lower bound on the\nminimum number of observations needed for exact cluster recovery. Then, we\npropose three algorithms with different running time and compare the number of\nobservations needed by them for successful cluster recovery. Our analytical\nresults show smooth time-data trade-offs: one can gradually reduce the\ncomputational complexity when increasingly more observations are available.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 22:46:06 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 21:59:39 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Xu", "Jiaming", ""], ["Wu", "Rui", ""], ["Zhu", "Kai", ""], ["Hajek", "Bruce", ""], ["Srikant", "R.", ""], ["Ying", "Lei", ""]]}, {"id": "1310.0532", "submitter": "Avanti Athreya", "authors": "Vince Lyzinski, Daniel Sussman, Minh Tang, Avanti Athreya, Carey\n  Priebe", "title": "Perfect Clustering for Stochastic Blockmodel Graphs via Adjacency\n  Spectral Embedding", "comments": "22 pages, including references; 2 figures", "journal-ref": "Electronic Journal of Statistics, 8 (2014) 2905--2922", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex clustering in a stochastic blockmodel graph has wide applicability and\nhas been the subject of extensive research. In thispaper, we provide a short\nproof that the adjacency spectral embedding can be used to obtain perfect\nclustering for the stochastic blockmodel and the degree-corrected stochastic\nblockmodel. We also show an analogous result for the more general random dot\nproduct graph model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 00:33:34 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 23:35:36 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 20:58:05 GMT"}, {"version": "v4", "created": "Thu, 15 Jan 2015 21:43:45 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Lyzinski", "Vince", ""], ["Sussman", "Daniel", ""], ["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Priebe", "Carey", ""]]}, {"id": "1310.0740", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Mark Girolami", "title": "Pseudo-Marginal Bayesian Inference for Gaussian Processes", "comments": "14 pages double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenges that arise when adopting Gaussian Process priors in\nprobabilistic modeling are how to carry out exact Bayesian inference and how to\naccount for uncertainty on model parameters when making model-based predictions\non out-of-sample data. Using probit regression as an illustrative working\nexample, this paper presents a general and effective methodology based on the\npseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses\nboth of these issues. The results presented in this paper show improvements\nover existing sampling methods to simulate from the posterior distribution over\nthe parameters defining the covariance function of the Gaussian Process prior.\nThis is particularly important as it offers a powerful tool to carry out full\nBayesian inference of Gaussian Process based hierarchic statistical models in\ngeneral. The results also demonstrate that Monte Carlo based integration of all\nmodel parameters is actually feasible in this class of models providing a\nsuperior quantification of uncertainty in predictions. Extensive comparisons\nwith respect to state-of-the-art probabilistic classifiers confirm this\nassertion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 15:29:28 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 10:41:39 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 08:53:34 GMT"}, {"version": "v4", "created": "Mon, 7 Apr 2014 09:42:58 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Filippone", "Maurizio", ""], ["Girolami", "Mark", ""]]}, {"id": "1310.0807", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Yuejie Chi and Andrea Goldsmith", "title": "Exact and Stable Covariance Estimation from Quadratic Sampling via\n  Convex Programming", "comments": "accepted to IEEE Transactions on Information Theory, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference and information processing of high-dimensional data\noften require efficient and accurate estimation of their second-order\nstatistics. With rapidly changing data, limited processing power and storage at\nthe acquisition devices, it is desirable to extract the covariance structure\nfrom a single pass over the data and a small number of stored measurements. In\nthis paper, we explore a quadratic (or rank-one) measurement model which\nimposes minimal memory requirements and low computational complexity during the\nsampling process, and is shown to be optimal in preserving various\nlow-dimensional covariance structures. Specifically, four popular structural\nassumptions of covariance matrices, namely low rank, Toeplitz low rank,\nsparsity, jointly rank-one and sparse structure, are investigated, while\nrecovery is achieved via convex relaxation paradigms for the respective\nstructure.\n  The proposed quadratic sampling framework has a variety of potential\napplications including streaming data processing, high-frequency wireless\ncommunication, phase space tomography and phase retrieval in optics, and\nnon-coherent subspace detection. Our method admits universally accurate\ncovariance estimation in the absence of noise, as soon as the number of\nmeasurements exceeds the information theoretic limits. We also demonstrate the\nrobustness of this approach against noise and imperfect structural assumptions.\nOur analysis is established upon a novel notion called the mixed-norm\nrestricted isometry property (RIP-$\\ell_{2}/\\ell_{1}$), as well as the\nconventional RIP-$\\ell_{2}/\\ell_{2}$ for near-isotropic and bounded\nmeasurements. In addition, our results improve upon the best-known phase\nretrieval (including both dense and sparse signals) guarantees using PhaseLift\nwith a significantly simpler approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 19:52:57 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 19:32:50 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2013 17:25:57 GMT"}, {"version": "v4", "created": "Sat, 21 Dec 2013 02:07:28 GMT"}, {"version": "v5", "created": "Thu, 19 Mar 2015 21:23:21 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1310.0865", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos and Yu Zhang and Georgios B. Giannakis", "title": "Electricity Market Forecasting via Low-Rank Multi-Kernel Learning", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/JSTSP.2014.2336611", "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smart grid vision entails advanced information technology and data\nanalytics to enhance the efficiency, sustainability, and economics of the power\ngrid infrastructure. Aligned to this end, modern statistical learning tools are\nleveraged here for electricity market inference. Day-ahead price forecasting is\ncast as a low-rank kernel learning problem. Uniquely exploiting the market\nclearing process, congestion patterns are modeled as rank-one components in the\nmatrix of spatio-temporally varying prices. Through a novel nuclear norm-based\nregularization, kernels across pricing nodes and hours can be systematically\nselected. Even though market-wide forecasting is beneficial from a learning\nperspective, it involves processing high-dimensional market data. The latter\nbecomes possible after devising a block-coordinate descent algorithm for\nsolving the non-convex optimization problem involved. The algorithm utilizes\nresults from block-sparse vector recovery and is guaranteed to converge to a\nstationary point. Numerical tests on real data from the Midwest ISO (MISO)\nmarket corroborate the prediction accuracy, computational efficiency, and the\ninterpretative merits of the developed approach over existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 23:51:38 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 17:33:35 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Zhang", "Yu", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1310.1022", "submitter": "Peter K\\\"oves\\'arki", "authors": "Peter Kovesarki, Ian C. Brock", "title": "Multivariate regression and fit function uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a multivariate polynomial regression method where the\nuncertainty of the input parameters are approximated with Gaussian\ndistributions, derived from the central limit theorem for large weighted sums,\ndirectly from the training sample. The estimated uncertainties can be\npropagated into the optimal fit function, as an alternative to the statistical\nbootstrap method. This uncertainty can be propagated further into a loss\nfunction like quantity, with which it is possible to calculate the expected\nloss function, and allows to select the optimal polynomial degree with\nstatistical significance. Combined with simple phase space splitting methods,\nit is possible to model most features of the training data even with low degree\npolynomials or constants.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 16:16:35 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Kovesarki", "Peter", ""], ["Brock", "Ian C.", ""]]}, {"id": "1310.1147", "submitter": "Xiliang Lu", "authors": "Jian Huang, Yuling Jiao, Bangti Jin, Jin Liu, Xiliang Lu, Can Yang", "title": "A Unified Primal Dual Active Set Algorithm for Nonconvex Sparse Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of recovering a sparse signal based on\npenalized least squares formulations. We develop a novel algorithm of\nprimal-dual active set type for a class of nonconvex sparsity-promoting\npenalties, including $\\ell^0$, bridge, smoothly clipped absolute deviation,\ncapped $\\ell^1$ and minimax concavity penalty. First we establish the existence\nof a global minimizer for the related optimization problems. Then we derive a\nnovel necessary optimality condition for the global minimizer using the\nassociated thresholding operator. The solutions to the optimality system are\ncoordinate-wise minimizers, and under minor conditions, they are also local\nminimizers. Upon introducing the dual variable, the active set can be\ndetermined using the primal and dual variables together. Further, this relation\nlends itself to an iterative algorithm of active set type which at each step\ninvolves first updating the primal variable only on the active set and then\nupdating the dual variable explicitly. When combined with a continuation\nstrategy on the regularization parameter, the primal dual active set method is\nshown to converge globally to the underlying regression target under certain\nregularity conditions. Extensive numerical experiments with both simulated and\nreal data demonstrate its superior performance in efficiency and accuracy\ncompared with the existing sparse recovery methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 02:06:00 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 03:37:38 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2016 15:22:43 GMT"}, {"version": "v4", "created": "Sat, 6 Jan 2018 16:12:16 GMT"}, {"version": "v5", "created": "Wed, 27 Feb 2019 02:27:57 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Jin", "Bangti", ""], ["Liu", "Jin", ""], ["Lu", "Xiliang", ""], ["Yang", "Can", ""]]}, {"id": "1310.1187", "submitter": "Johan Pensar", "authors": "Johan Pensar, Henrik Nyman, Timo Koski and Jukka Corander", "title": "Labeled Directed Acyclic Graphs: a generalization of context-specific\n  independence in directed graphical models", "comments": "26 pages, 17 figures", "journal-ref": null, "doi": "10.1007/s10618-014-0355-0", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of labeled directed acyclic graph (LDAG) models\nfor finite sets of discrete variables. LDAGs generalize earlier proposals for\nallowing local structures in the conditional probability distribution of a\nnode, such that unrestricted label sets determine which edges can be deleted\nfrom the underlying directed acyclic graph (DAG) for a given context. Several\nproperties of these models are derived, including a generalization of the\nconcept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is\nenabled by introducing an LDAG-based factorization of the Dirichlet prior for\nthe model parameters, such that the marginal likelihood can be calculated\nanalytically. In addition, we develop a novel prior distribution for the model\nstructures that can appropriately penalize a model for its labeling complexity.\nA non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill\nclimbing approach is used for illustrating the useful properties of LDAG models\nfor both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 07:29:08 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Koski", "Timo", ""], ["Corander", "Jukka", ""]]}, {"id": "1310.1297", "submitter": "Vincent Lyzinski", "authors": "Vince Lyzinski, Daniel L. Sussman, Donniell E. Fishkind, Henry Pao, Li\n  Chen, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe", "title": "Spectral Clustering for Divide-and-Conquer Graph Matching", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallelized bijective graph matching algorithm that leverages\nseeds and is designed to match very large graphs. Our algorithm combines\nspectral graph embedding with existing state-of-the-art seeded graph matching\nprocedures. We justify our approach by proving that modestly correlated, large\nstochastic block model random graphs are correctly matched utilizing very few\nseeds through our divide-and-conquer procedure. We also demonstrate the\neffectiveness of our approach in matching very large graphs in simulated and\nreal data examples, showing up to a factor of 8 improvement in runtime with\nminimal sacrifice in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 14:40:30 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2014 00:23:34 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 02:40:51 GMT"}, {"version": "v4", "created": "Wed, 22 Oct 2014 02:42:45 GMT"}, {"version": "v5", "created": "Thu, 12 Mar 2015 19:12:03 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lyzinski", "Vince", ""], ["Sussman", "Daniel L.", ""], ["Fishkind", "Donniell E.", ""], ["Pao", "Henry", ""], ["Chen", "Li", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1310.1363", "submitter": "Stefan Wager", "authors": "Stefan Wager, Alexander Blocker, Niall Cardin", "title": "Weakly supervised clustering: Learning fine-grained signals from coarse\n  labels", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS812 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 801-820", "doi": "10.1214/15-AOAS812", "report-no": "IMS-AOAS-AOAS812", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a classification problem where we do not have access to labels for\nindividual training examples, but only have average labels over subpopulations.\nWe give practical examples of this setup and show how such a classification\ntask can usefully be analyzed as a weakly supervised clustering problem. We\npropose three approaches to solving the weakly supervised clustering problem,\nincluding a latent variables model that performs well in our experiments. We\nillustrate our methods on an analysis of aggregated elections data and an\nindustry data set that was the original motivation for this research.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 18:34:54 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 18:45:35 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 07:57:08 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Wager", "Stefan", ""], ["Blocker", "Alexander", ""], ["Cardin", "Niall", ""]]}, {"id": "1310.1404", "submitter": "Luke Bornn", "authors": "Michael Cherkassky and Luke Bornn", "title": "Sequential Monte Carlo Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a flexible and efficient framework for handling\nmulti-armed bandits, combining sequential Monte Carlo algorithms with\nhierarchical Bayesian modeling techniques. The framework naturally encompasses\nrestless bandits, contextual bandits, and other bandit variants under a single\ninferential model. Despite the model's generality, we propose efficient Monte\nCarlo algorithms to make inference scalable, based on recent developments in\nsequential Monte Carlo methods. Through two simulation studies, the framework\nis shown to outperform other empirical methods, while also naturally scaling to\nmore complex problems for which existing approaches can not cope. Additionally,\nwe successfully apply our framework to online video-based advertising\nrecommendation, and show its increased efficacy as compared to current state of\nthe art bandit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 20:19:56 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Cherkassky", "Michael", ""], ["Bornn", "Luke", ""]]}, {"id": "1310.1415", "submitter": "Misha Denil", "authors": "Misha Denil, David Matheson, Nando de Freitas", "title": "Narrowing the Gap: Random Forests In Theory and In Practice", "comments": "Under review by the International Conference on Machine Learning\n  (ICML) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread interest and practical use, the theoretical properties of\nrandom forests are still not well understood. In this paper we contribute to\nthis understanding in two ways. We present a new theoretically tractable\nvariant of random regression forests and prove that our algorithm is\nconsistent. We also provide an empirical evaluation, comparing our algorithm\nand other theoretically tractable random forest models to the random forest\nalgorithm used in practice. Our experiments provide insight into the relative\nimportance of different simplifications that theoreticians have made to obtain\ntractable models for analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 22:33:35 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Denil", "Misha", ""], ["Matheson", "David", ""], ["de Freitas", "Nando", ""]]}, {"id": "1310.1495", "submitter": "Purnamrita Sarkar", "authors": "Purnamrita Sarkar, Peter J. Bickel", "title": "Role of normalization in spectral clustering for stochastic blockmodels", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1285 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 962-990", "doi": "10.1214/14-AOS1285", "report-no": "IMS-AOS-AOS1285", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a technique that clusters elements using the top few\neigenvectors of their (possibly normalized) similarity matrix. The quality of\nspectral clustering is closely tied to the convergence properties of these\nprincipal eigenvectors. This rate of convergence has been shown to be identical\nfor both the normalized and unnormalized variants in recent random matrix\ntheory literature. However, normalization for spectral clustering is commonly\nbelieved to be beneficial [Stat. Comput. 17 (2007) 395-416]. Indeed, our\nexperiments show that normalization improves prediction accuracy. In this\npaper, for the popular stochastic blockmodel, we theoretically show that\nnormalization shrinks the spread of points in a class by a constant fraction\nunder a broad parameter regime. As a byproduct of our work, we also obtain\nsharp deviation bounds of empirical principal eigenvalues of graphs generated\nfrom a stochastic blockmodel.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 17:07:28 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 12:21:07 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1310.1502", "submitter": "John T. Holodnak", "authors": "John T. Holodnak, Ilse C. F. Ipsen", "title": "Randomized Approximation of the Gram Matrix: Exact Computation and\n  Probabilistic Bounds", "comments": "Update to title in third version. Major revisions in second version\n  including new bounds and a more detailed experimental section. Submitted to\n  SIMAX", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a real matrix A with n columns, the problem is to approximate the Gram\nproduct AA^T by c << n weighted outer products of columns of A. Necessary and\nsufficient conditions for the exact computation of AA^T (in exact arithmetic)\nfrom c >= rank(A) columns depend on the right singular vector matrix of A. For\na Monte-Carlo matrix multiplication algorithm by Drineas et al. that samples\nouter products, we present probabilistic bounds for the 2-norm relative error\ndue to randomization. The bounds depend on the stable rank or the rank of A,\nbut not on the matrix dimensions. Numerical experiments illustrate that the\nbounds are informative, even for stringent success probabilities and matrices\nof small dimension. We also derive bounds for the smallest singular value and\nthe condition number of matrices obtained by sampling rows from orthonormal\nmatrices.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 18:09:50 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 19:46:17 GMT"}, {"version": "v3", "created": "Thu, 15 May 2014 16:32:14 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Holodnak", "John T.", ""], ["Ipsen", "Ilse C. F.", ""]]}, {"id": "1310.1518", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra, Amit Kumar Mishra", "title": "Contraction Principle based Robust Iterative Algorithms for Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative algorithms are ubiquitous in the field of data mining. Widely known\nexamples of such algorithms are the least mean square algorithm,\nbackpropagation algorithm of neural networks. Our contribution in this paper is\nan improvement upon this iterative algorithms in terms of their respective\nperformance metrics and robustness. This improvement is achieved by a new\nscaling factor which is multiplied to the error term. Our analysis shows that\nin essence, we are minimizing the corresponding LASSO cost function, which is\nthe reason of its increased robustness. We also give closed form expressions\nfor the number of iterations for convergence and the MSE floor of the original\ncost function for a minimum targeted value of the L1 norm. As a concluding\ntheme based on the stochastic subgradient algorithm, we give a comparison\nbetween the well known Dantzig selector and our algorithm based on contraction\nprinciple. By these simulations we attempt to show the optimality of our\napproach for any widely used parent iterative optimization problem.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 20:49:37 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Mitra", "Rangeet", ""], ["Mishra", "Amit Kumar", ""]]}, {"id": "1310.1519", "submitter": "Amin Zollanvari", "authors": "Amin Zollanvari and Edward R. Dougherty", "title": "Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of\n  Classification Error in the Gaussian Model", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important aspect of any classifier is its error rate, because this\nquantifies its predictive capacity. Thus, the accuracy of error estimation is\ncritical. Error estimation is problematic in small-sample classifier design\nbecause the error must be estimated using the same data from which the\nclassifier has been designed. Use of prior knowledge, in the form of a prior\ndistribution on an uncertainty class of feature-label distributions to which\nthe true, but unknown, feature-distribution belongs, can facilitate accurate\nerror estimation (in the mean-square sense) in circumstances where accurate\ncompletely model-free error estimation is impossible. This paper provides\nanalytic asymptotically exact finite-sample approximations for various\nperformance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE)\nerror estimator in the case of linear discriminant analysis (LDA) in the\nmultivariate Gaussian model. These performance metrics include the first,\nsecond, and cross moments of the Bayesian MMSE error estimator with the true\nerror of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator.\nWe lay down the theoretical groundwork for Kolmogorov double-asymptotics in a\nBayesian setting, which enables us to derive asymptotic expressions of the\ndesired performance metrics. From these we produce analytic finite-sample\napproximations and demonstrate their accuracy via numerical examples. Various\nexamples illustrate the behavior of these approximations and their use in\ndetermining the necessary sample size to achieve a desired RMS. The\nSupplementary Material contains derivations for some equations and added\nfigures.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 21:57:02 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 18:18:34 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Zollanvari", "Amin", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1310.1533", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann, Jonas Peters, Jan Ernest", "title": "CAM: Causal additive models, high-dimensional order search and penalized\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1260 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2526-2556", "doi": "10.1214/14-AOS1260", "report-no": "IMS-AOS-AOS1260", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 03:12:34 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 12:31:45 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""], ["Ernest", "Jan", ""]]}, {"id": "1310.1545", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Richard Yi Da Xu, Longbing Cao, Yin Song", "title": "Learning Hidden Structures with Relational Models by Adequately\n  Involving Rich Information in A Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively modelling hidden structures in a network is very practical but\ntheoretically challenging. Existing relational models only involve very limited\ninformation, namely the binary directional link data, embedded in a network to\nlearn hidden networking structures. There is other rich and meaningful\ninformation (e.g., various attributes of entities and more granular information\nthan binary elements such as \"like\" or \"dislike\") missed, which play a critical\nrole in forming and understanding relations in a network. In this work, we\npropose an informative relational model (InfRM) framework to adequately involve\nrich information and its granularity in a network, including metadata\ninformation about each entity and various forms of link data. Firstly, an\neffective metadata information incorporation method is employed on the prior\ninformation from relational models MMSB and LFRM. This is to encourage the\nentities with similar metadata information to have similar hidden structures.\nSecondly, we propose various solutions to cater for alternative forms of link\ndata. Substantial efforts have been made towards modelling appropriateness and\nefficiency, for example, using conjugate priors. We evaluate our framework and\nits inference algorithms in different datasets, which shows the generality and\neffectiveness of our models in capturing implicit structures in networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 05:47:50 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Fan", "Xuhui", ""], ["Da Xu", "Richard Yi", ""], ["Cao", "Longbing", ""], ["Song", "Yin", ""]]}, {"id": "1310.1562", "submitter": "Hangjin Jiang", "authors": "Hangjin Jiang, Yiming Ding", "title": "Dependence Measure for non-additive model", "comments": "This paper has been withdrawn by the author due to change of the main\n  content", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a new statistical dependency measure called Copula Dependency\nCoefficient(CDC) for two sets of variables based on copula. It is robust to\noutliers, easy to implement, powerful and appropriate to high-dimensional\nvariables. These properties are important in many applications. Experimental\nresults show that CDC can detect the dependence between variables in both\nadditive and non-additive models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 09:36:55 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 10:25:25 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 05:58:33 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 04:18:06 GMT"}, {"version": "v5", "created": "Sun, 25 Mar 2018 13:03:24 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Jiang", "Hangjin", ""], ["Ding", "Yiming", ""]]}, {"id": "1310.1757", "submitter": "Iain Murray", "authors": "Benigno Uria, Iain Murray, Hugo Larochelle", "title": "A Deep and Tractable Density Estimator", "comments": "9 pages, 4 tables, 1 algorithm, 5 figures. To appear ICML 2014, JMLR\n  W&CP volume 32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Autoregressive Distribution Estimator (NADE) and its real-valued\nversion RNADE are competitive density models of multidimensional data across a\nvariety of domains. These models use a fixed, arbitrary ordering of the data\ndimensions. One can easily condition on variables at the beginning of the\nordering, and marginalize out variables at the end of the ordering, however\nother inference tasks require approximate inference. In this work we introduce\nan efficient procedure to simultaneously train a NADE model for each possible\nordering of the variables, by sharing parameters across all these models. We\ncan thus use the most convenient model for each inference task at hand, and\nensembles of such models with different orderings are immediately available.\nMoreover, unlike the original NADE, our training procedure scales to deep\nmodels. Empirically, ensembles of Deep NADE models obtain state of the art\ndensity estimation performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 12:42:41 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 17:13:56 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Uria", "Benigno", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1310.1800", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Generalized Negative Binomial Processes and the Representation of\n  Cluster Structures", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the concept of a cluster structure to define a joint\ndistribution of the sample size and its exchangeable random partitions. The\ncluster structure allows the probability distribution of the random partitions\nof a subset of the sample to be dependent on the sample size, a feature not\npresented in a partition structure. A generalized negative binomial process\ncount-mixture model is proposed to generate a cluster structure, where in the\nprior the number of clusters is finite and Poisson distributed and the cluster\nsizes follow a truncated negative binomial distribution. The number and sizes\nof clusters can be controlled to exhibit distinct asymptotic behaviors. Unique\nmodel properties are illustrated with example clustering results using a\ngeneralized Polya urn sampling scheme. The paper provides new methods to\ngenerate exchangeable random partitions and to control both the cluster-number\nand cluster-size distributions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 14:35:45 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1310.1803", "submitter": "Robin Scheibler", "authors": "Robin Scheibler, Saeid Haghighatshoar and Martin Vetterli", "title": "A Fast Hadamard Transform for Signals with Sub-linear Sparsity in the\n  Transform Domain", "comments": "17 pages. 11 figures. A shorter version was submitted to the 51st\n  Allerton Conference on Communication, Control and Computing (2013)", "journal-ref": null, "doi": "10.1109/TIT.2015.2404441", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new iterative low complexity algorithm has been presented for computing the\nWalsh-Hadamard transform (WHT) of an $N$ dimensional signal with a $K$-sparse\nWHT, where $N$ is a power of two and $K = O(N^\\alpha)$, scales sub-linearly in\n$N$ for some $0 < \\alpha < 1$. Assuming a random support model for the non-zero\ntransform domain components, the algorithm reconstructs the WHT of the signal\nwith a sample complexity $O(K \\log_2(\\frac{N}{K}))$, a computational complexity\n$O(K\\log_2(K)\\log_2(\\frac{N}{K}))$ and with a very high probability\nasymptotically tending to 1.\n  The approach is based on the subsampling (aliasing) property of the WHT,\nwhere by a carefully designed subsampling of the time domain signal, one can\ninduce a suitable aliasing pattern in the transform domain. By treating the\naliasing patterns as parity-check constraints and borrowing ideas from erasure\ncorrecting sparse-graph codes, the recovery of the non-zero spectral values has\nbeen formulated as a belief propagation (BP) algorithm (peeling decoding) over\na sparse-graph code for the binary erasure channel (BEC). Tools from coding\ntheory are used to analyze the asymptotic performance of the algorithm in the\nvery sparse ($\\alpha\\in(0,\\frac{1}{3}]$) and the less sparse\n($\\alpha\\in(\\frac{1}{3},1)$) regime.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 14:46:51 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2013 14:54:51 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Scheibler", "Robin", ""], ["Haghighatshoar", "Saeid", ""], ["Vetterli", "Martin", ""]]}, {"id": "1310.1826", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi and Volkan Cevher", "title": "Learning Non-Parametric Basis Independent Models from Point Queries via\n  Low-Rank Methods", "comments": "27 pages, minor corrections in the proof of Proposition 2 (appendix\n  H), modified the statement of Proposition 2, typos corrected in appendix E", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning multi-ridge functions of the form f(x) =\ng(Ax) from point evaluations of f. We assume that the function f is defined on\nan l_2-ball in R^d, g is twice continuously differentiable almost everywhere,\nand A \\in R^{k \\times d} is a rank k matrix, where k << d. We propose a\nrandomized, polynomial-complexity sampling scheme for estimating such\nfunctions. Our theoretical developments leverage recent techniques from low\nrank matrix recovery, which enables us to derive a polynomial time estimator of\nthe function f along with uniform approximation guarantees. We prove that our\nscheme can also be applied for learning functions of the form: f(x) =\n\\sum_{i=1}^{k} g_i(a_i^T x), provided f satisfies certain smoothness conditions\nin a neighborhood around the origin. We also characterize the noise robustness\nof the scheme. Finally, we present numerical examples to illustrate the\ntheoretical bounds in action.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 15:46:26 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 16:18:54 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Tyagi", "Hemant", ""], ["Cevher", "Volkan", ""]]}, {"id": "1310.1840", "submitter": "Olivier Fercoq", "authors": "Olivier Fercoq", "title": "Parallel coordinate descent for the Adaboost problem", "comments": "7 pages, 3 figures, extended version of the paper presented to\n  ICMLA'13", "journal-ref": null, "doi": "10.1109/ICMLA.2013.72", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a randomised parallel version of Adaboost based on previous studies\non parallel coordinate descent. The algorithm uses the fact that the logarithm\nof the exponential loss is a function with coordinate-wise Lipschitz continuous\ngradient, in order to define the step lengths. We provide the proof of\nconvergence for this randomised Adaboost algorithm and a theoretical\nparallelisation speedup factor. We finally provide numerical examples on\nlearning problems of various sizes that show that the algorithm is competitive\nwith concurrent approaches, especially for large scale problems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 16:04:28 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Fercoq", "Olivier", ""]]}, {"id": "1310.1867", "submitter": "Daniel Soudry", "authors": "Daniel Soudry, Ron Meir", "title": "Mean Field Bayes Backpropagation: scalable training of multilayer neural\n  networks with binary weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant success has been reported recently using deep neural networks for\nclassification. Such large networks can be computationally intensive, even\nafter training is over. Implementing these trained networks in hardware chips\nwith a limited precision of synaptic weights may improve their speed and energy\nefficiency by several orders of magnitude, thus enabling their integration into\nsmall and low-power electronic devices. With this motivation, we develop a\ncomputationally efficient learning algorithm for multilayer neural networks\nwith binary weights, assuming all the hidden neurons have a fan-out of one.\nThis algorithm, derived within a Bayesian probabilistic online setting, is\nshown to work well for both synthetic and real-world problems, performing\ncomparably to algorithms with real-valued weights, while retaining\ncomputational tractability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 17:32:37 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 10:58:18 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 05:49:13 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2013 12:37:54 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Soudry", "Daniel", ""], ["Meir", "Ron", ""]]}, {"id": "1310.1934", "submitter": "Nikos Karampatziakis", "authors": "Nikos Karampatziakis, Paul Mineiro", "title": "Discriminative Features via Generalized Eigenvectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing examples in a way that is compatible with the underlying\nclassifier can greatly enhance the performance of a learning system. In this\npaper we investigate scalable techniques for inducing discriminative features\nby taking advantage of simple second order structure in the data. We focus on\nmulticlass classification and show that features extracted from the generalized\neigenvectors of the class conditional second moments lead to classifiers with\nexcellent empirical performance. Moreover, these features have attractive\ntheoretical properties, such as inducing representations that are invariant to\nlinear transformations of the input. We evaluate classifiers built from these\nfeatures on three different tasks, obtaining state of the art results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:05:52 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Karampatziakis", "Nikos", ""], ["Mineiro", "Paul", ""]]}, {"id": "1310.1947", "submitter": "Frank Hutter", "authors": "Frank Hutter and Holger Hoos and Kevin Leyton-Brown", "title": "Bayesian Optimization With Censored Response Data", "comments": "Extended version of NIPS 2011 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) aims to minimize a given blackbox function using a\nmodel that is updated whenever new evidence about the function becomes\navailable. Here, we address the problem of BO under partially right-censored\nresponse data, where in some evaluations we only obtain a lower bound on the\nfunction value. The ability to handle such response data allows us to\nadaptively censor costly function evaluations in minimization problems where\nthe cost of a function evaluation corresponds to the function value. One\nimportant application giving rise to such censored data is the\nruntime-minimizing variant of the algorithm configuration problem: finding\nsettings of a given parametric algorithm that minimize the runtime required for\nsolving problem instances from a given distribution. We demonstrate that\nterminating slow algorithm runs prematurely and handling the resulting\nright-censored observations can substantially improve the state of the art in\nmodel-based algorithm configuration.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:43:16 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Hutter", "Frank", ""], ["Hoos", "Holger", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1310.1949", "submitter": "Nikos Karampatziakis", "authors": "Alekh Agarwal, Sham M. Kakade, Nikos Karampatziakis, Le Song, Gregory\n  Valiant", "title": "Least Squares Revisited: Scalable Approaches for Multi-class Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides simple algorithms for multi-class (and multi-label)\nprediction in settings where both the number of examples n and the data\ndimension d are relatively large. These robust and parameter free algorithms\nare essentially iterative least-squares updates and very versatile both in\ntheory and in practice. On the theoretical front, we present several variants\nwith convergence guarantees. Owing to their effective use of second-order\nstructure, these algorithms are substantially better than first-order methods\nin many practical scenarios. On the empirical side, we present a scalable\nstagewise variant of our approach, which achieves dramatic computational\nspeedups over popular optimization packages such as Liblinear and Vowpal Wabbit\non standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art\naccuracies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 15:18:37 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Agarwal", "Alekh", ""], ["Kakade", "Sham M.", ""], ["Karampatziakis", "Nikos", ""], ["Song", "Le", ""], ["Valiant", "Gregory", ""]]}, {"id": "1310.2059", "submitter": "Peter Richtarik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Distributed Coordinate Descent Method for Learning with Big Data", "comments": "11 two-column pages, 1 algorithm, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method\nfor solving loss minimization problems with big data. We initially partition\nthe coordinates (features) and assign each partition to a different node of a\ncluster. At every iteration, each node picks a random subset of the coordinates\nfrom those it owns, independently from the other computers, and in parallel\ncomputes and applies updates to the selected coordinates based on a simple\nclosed-form formula. We give bounds on the number of iterations sufficient to\napproximately solve the problem with high probability, and show how it depends\non the data and on the partitioning. We perform numerical experiments with a\nLASSO instance described by a 3TB matrix.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 09:31:27 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1310.2125", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Sohan Seth and Samuel Kaski", "title": "Retrieval of Experiments with Sequential Dirichlet Process Mixtures in\n  Model Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving relevant experiments given a query\nexperiment, motivated by the public databases of datasets in molecular biology\nand other experimental sciences, and the need of scientists to relate to\nearlier work on the level of actual measurement data. Since experiments are\ninherently noisy and databases ever accumulating, we argue that a retrieval\nengine should possess two particular characteristics. First, it should compare\nmodels learnt from the experiments rather than the raw measurements themselves:\nthis allows incorporating experiment-specific prior knowledge to suppress noise\neffects and focus on what is important. Second, it should be updated\nsequentially from newly published experiments, without explicitly storing\neither the measurements or the models, which is critical for saving storage\nspace and protecting data privacy: this promotes life long learning. We\nformulate the retrieval as a ``supermodelling'' problem, of sequentially\nlearning a model of the set of posterior distributions, represented as sets of\nMCMC samples, and suggest the use of Particle-Learning-based sequential\nDirichlet process mixture (DPM) for this purpose. The relevance measure for\nretrieval is derived from the supermodel through the mixture representation. We\ndemonstrate the performance of the proposed retrieval method on simulated data\nand molecular biological experiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:10:26 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 22:04:33 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Kaski", "Samuel", ""]]}, {"id": "1310.2273", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Stephen A. Vavasis", "title": "Semidefinite Programming Based Preconditioning for More Robust\n  Near-Separable Nonnegative Matrix Factorization", "comments": "25 pages, 6 figures, 4 tables. New numerical experiments, additional\n  remarks and comments", "journal-ref": "SIAM Journal on Optimization 25 (1), pp. 677-698, 2015", "doi": "10.1137/130940670", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) under the separability assumption can\nprovably be solved efficiently, even in the presence of noise, and has been\nshown to be a powerful technique in document classification and hyperspectral\nunmixing. This problem is referred to as near-separable NMF and requires that\nthere exists a cone spanned by a small subset of the columns of the input\nnonnegative matrix approximately containing all columns. In this paper, we\npropose a preconditioning based on semidefinite programming making the input\nmatrix well-conditioned. This in turn can improve significantly the performance\nof near-separable NMF algorithms which is illustrated on the popular successive\nprojection algorithm (SPA). The new preconditioned SPA is provably more robust\nto noise, and outperforms SPA on several synthetic data sets. We also show how\nan active-set method allow us to apply the preconditioning on large-scale\nreal-world hyperspectral images.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 20:30:38 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 09:11:30 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1310.2408", "submitter": "Jun Zhu", "authors": "Jun Zhu, Xun Zheng, Bo Zhang", "title": "Improved Bayesian Logistic Supervised Topic Models with Data\n  Augmentation", "comments": "9 pages, ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models with a logistic likelihood have two issues that\npotentially limit their practical use: 1) response variables are usually\nover-weighted by document word counts; and 2) existing variational inference\nmethods make strict mean-field assumptions. We address these issues by: 1)\nintroducing a regularization constant to better balance the two parts based on\nan optimization formulation of Bayesian inference; and 2) developing a simple\nGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and\ncollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm\nhas analytical forms of each conditional distribution without making any\nrestricting assumptions and can be easily parallelized. Empirical results\ndemonstrate significant improvements on prediction performance and time\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:23:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhu", "Jun", ""], ["Zheng", "Xun", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2409", "submitter": "Ning Chen", "authors": "Ning Chen, Jun Zhu, Fei Xia, Bo Zhang", "title": "Discriminative Relational Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:32:56 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Xia", "Fei", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2451", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (LIF), Hachem Kadri (LIF)", "title": "M-Power Regularized Least Squares Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is used to find a solution that both fits the data and is\nsufficiently smooth, and thereby is very effective for designing and refining\nlearning algorithms. But the influence of its exponent remains poorly\nunderstood. In particular, it is unclear how the exponent of the reproducing\nkernel Hilbert space~(RKHS) regularization term affects the accuracy and the\nefficiency of kernel-based learning algorithms. Here we consider regularized\nleast squares regression (RLSR) with an RKHS regularization raised to the power\nof m, where m is a variable real exponent. We design an efficient algorithm for\nsolving the associated minimization problem, we provide a theoretical analysis\nof its stability, and we compare its advantage with respect to computational\ncomplexity, speed of convergence and prediction accuracy to the classical\nkernel ridge regression algorithm where the regularization exponent m is fixed\nat 2. Our results show that the m-power RLSR problem can be solved efficiently,\nand support the suggestion that one can use a regularization term that grows\nsignificantly slower than the standard quadratic growth in the RKHS norm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 12:18:29 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 13:45:18 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Audiffren", "Julien", "", "LIF"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1310.2627", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Bryan R. Routledge and Noah A. Smith", "title": "A Sparse and Adaptive Prior for Time-Dependent Model Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the scenario where the parameters of a probabilistic model are\nexpected to vary over time. We construct a novel prior distribution that\npromotes sparsity and adapts the strength of correlation between parameters at\nsuccessive timesteps, based on the data. We derive approximate variational\ninference procedures for learning and prediction with this prior. We test the\napproach on two tasks: forecasting financial quantities from relevant text, and\nmodeling language contingent on time-varying financial measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 20:39:08 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 05:11:48 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Yogatama", "Dani", ""], ["Routledge", "Bryan R.", ""], ["Smith", "Noah A.", ""]]}, {"id": "1310.2641", "submitter": "Benjamin Rolfs", "authors": "Dhafer Malouche, Bala Rajaratnam, Benjamin T. Rolfs", "title": "Duality in Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have proven to be powerful tools for representing\nhigh-dimensional systems of random variables. One example of such a model is\nthe undirected graph, in which lack of an edge represents conditional\nindependence between two random variables given the rest. Another example is\nthe bidirected graph, in which absence of edges encodes pairwise marginal\nindependence. Both of these classes of graphical models have been extensively\nstudied, and while they are considered to be dual to one another, except in a\nfew instances this duality has not been thoroughly investigated. In this paper,\nwe demonstrate how duality between undirected and bidirected models can be used\nto transport results for one class of graphical models to the dual model in a\ntransparent manner. We proceed to apply this technique to extend previously\nexisting results as well as to prove new ones, in three important domains.\nFirst, we discuss the pairwise and global Markov properties for undirected and\nbidirected models, using the pseudographoid and reverse-pseudographoid rules\nwhich are weaker conditions than the typically used intersection and\ncomposition rules. Second, we investigate these pseudographoid and reverse\npseudographoid rules in the context of probability distributions, using the\nconcept of duality in the process. Duality allows us to quickly relate them to\nthe more familiar intersection and composition properties. Third and finally,\nwe apply the dualization method to understand the implications of faithfulness,\nwhich in turn leads to a more general form of an existing result.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 21:54:14 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Malouche", "Dhafer", ""], ["Rajaratnam", "Bala", ""], ["Rolfs", "Benjamin T.", ""]]}, {"id": "1310.2816", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang", "title": "Gibbs Max-margin Topic Models with Data Augmentation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-margin learning is a powerful approach to building classifiers and\nstructured output predictors. Recent work on max-margin supervised topic models\nhas successfully integrated it with Bayesian topic models to discover\ndiscriminative latent semantic structures and make accurate predictions for\nunseen testing data. However, the resulting learning problems are usually hard\nto solve because of the non-smoothness of the margin loss. Existing approaches\nto building max-margin supervised topic models rely on an iterative procedure\nto solve multiple latent SVM subproblems with additional mean-field assumptions\non the desired posterior distributions. This paper presents an alternative\napproach by defining a new max-margin loss. Namely, we present Gibbs max-margin\nsupervised topic models, a latent variable Gibbs classifier to discover hidden\ntopic representations for various tasks, including classification, regression\nand multi-task learning. Gibbs max-margin supervised topic models minimize an\nexpected margin loss, which is an upper bound of the existing margin loss\nderived from an expected prediction rule. By introducing augmented variables\nand integrating out the Dirichlet variables analytically by conjugacy, we\ndevelop simple Gibbs sampling algorithms with no restricting assumptions and no\nneed to solve SVM subproblems. Furthermore, each step of the\n\"augment-and-collapse\" Gibbs sampling algorithms has an analytical conditional\ndistribution, from which samples can be easily drawn. Experimental results\ndemonstrate significant improvements on time efficiency. The classification\nperformance is also significantly improved over competitors on binary,\nmulti-class and multi-label classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:47:40 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Perkins", "Hugh", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2880", "submitter": "Adrian Barbu", "authors": "Adrian Barbu, Yiyuan She, Liangjing Ding, Gary Gramajo", "title": "Feature Selection with Annealing for Computer Vision and Big Data\n  Learning", "comments": "18 pages, 9 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 39, no 2, pp 272 - 286, 2017", "doi": "10.1109/TPAMI.2016.2544315", "report-no": null, "categories": "stat.ML cs.CV cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and medical imaging problems are faced with learning\nfrom large-scale datasets, with millions of observations and features. In this\npaper we propose a novel efficient learning scheme that tightens a sparsity\nconstraint by gradually removing variables based on a criterion and a schedule.\nThe attractive fact that the problem size keeps dropping throughout the\niterations makes it particularly suitable for big data learning. Our approach\napplies generically to the optimization of any differentiable loss function,\nand finds applications in regression, classification and ranking. The resultant\nalgorithms build variable screening into estimation and are extremely simple to\nimplement. We provide theoretical guarantees of convergence and selection\nconsistency. In addition, one dimensional piecewise linear response functions\nare used to account for nonlinearity and a second order prior is imposed on\nthese functions to avoid overfitting. Experiments on real and synthetic data\nshow that the proposed method compares very well with other state of the art\nmethods in regression, classification and ranking while being computationally\nvery efficient and scalable.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 16:47:22 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 22:42:51 GMT"}, {"version": "v3", "created": "Tue, 30 Sep 2014 00:33:36 GMT"}, {"version": "v4", "created": "Wed, 1 Oct 2014 20:03:42 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2015 13:20:26 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 02:02:20 GMT"}, {"version": "v7", "created": "Thu, 17 Mar 2016 14:55:09 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Barbu", "Adrian", ""], ["She", "Yiyuan", ""], ["Ding", "Liangjing", ""], ["Gramajo", "Gary", ""]]}, {"id": "1310.2905", "submitter": "Christian P. Robert", "authors": "E. Moreno, F.-J. Vazquez-Polo, and C.P. Robert", "title": "Two discussions of the paper \"Bayesian measures of model complexity and\n  fit\" by D. Spiegelhalter et al., Read before The Royal Statistical Society at\n  a meeting organized by the Research Section on Wednesday, March 13th, 2002", "comments": "4 pages, to appear in the Journal of the Royal Statistical Society,\n  Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the written discussions of the paper \"Bayesian measures of model\ncomplexity and fit\" by D. Spiegelhalter et al. (2002), following the\ndiscussions given at the Annual Meeting of the Royal Statistical Society in\nNewcastle-upon-Tyne on September 3rd, 2013.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 18:22:37 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2013 08:14:52 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Moreno", "E.", ""], ["Vazquez-Polo", "F. -J.", ""], ["Robert", "C. P.", ""]]}, {"id": "1310.2931", "submitter": "Stefan Wager", "authors": "Stefan Wager, Nick Chamandy, Omkar Muralidharan, and Amir Najmi", "title": "Feedback Detection for Live Predictors", "comments": "Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictor that is deployed in a live production system may perturb the\nfeatures it uses to make predictions. Such a feedback loop can occur, for\nexample, when a model that predicts a certain type of behavior ends up causing\nthe behavior it predicts, thus creating a self-fulfilling prophecy. In this\npaper we analyze predictor feedback detection as a causal inference problem,\nand introduce a local randomization scheme that can be used to detect\nnon-linear feedback in real-world problems. We conduct a pilot study for our\nproposed methodology using a predictive system currently deployed as a part of\na search engine.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 19:57:45 GMT"}, {"version": "v2", "created": "Sat, 1 Nov 2014 01:48:35 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Chamandy", "Nick", ""], ["Muralidharan", "Omkar", ""], ["Najmi", "Amir", ""]]}, {"id": "1310.3003", "submitter": "Xingye Qiao", "authors": "Xingye Qiao and Lingsong Zhang", "title": "Distance-weighted Support Vector Machine", "comments": "31 pages, 5 figures", "journal-ref": "Statistics and Its Interface, 8, 3, pp. 331-345 (2015)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel linear classification method that possesses the merits of both the\nSupport Vector Machine (SVM) and the Distance-weighted Discrimination (DWD) is\nproposed in this article. The proposed Distance-weighted Support Vector Machine\nmethod can be viewed as a hybrid of SVM and DWD that finds the classification\ndirection by minimizing mainly the DWD loss, and determines the intercept term\nin the SVM manner. We show that our method inheres the merit of DWD, and hence,\novercomes the data-piling and overfitting issue of SVM. On the other hand, the\nnew method is not subject to imbalanced data issue which was a main advantage\nof SVM over DWD. It uses an unusual loss which combines the Hinge loss (of SVM)\nand the DWD loss through a trick of axillary hyperplane. Several theoretical\nproperties, including Fisher consistency and asymptotic normality of the DWSVM\nsolution are developed. We use some simulated examples to show that the new\nmethod can compete DWD and SVM on both classification performance and\ninterpretability. A real data application further establishes the usefulness of\nour approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 02:21:49 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2014 21:53:56 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2015 15:45:52 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Qiao", "Xingye", ""], ["Zhang", "Lingsong", ""]]}, {"id": "1310.3004", "submitter": "Xingye Qiao", "authors": "Xingye Qiao (1), Lingsong Zhang (2) ((1) State University of New York\n  at Binghamton, (2) Purdue University)", "title": "Flexible High-dimensional Classification Machines and Their Asymptotic\n  Properties", "comments": "49 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important topic in statistics and machine learning with\ngreat potential in many real applications. In this paper, we investigate two\npopular large margin classification methods, Support Vector Machine (SVM) and\nDistance Weighted Discrimination (DWD), under two contexts: the\nhigh-dimensional, low-sample size data and the imbalanced data. A unified\nfamily of classification machines, the FLexible Assortment MachinE (FLAME) is\nproposed, within which DWD and SVM are special cases. The FLAME family helps to\nidentify the similarities and differences between SVM and DWD. It is well known\nthat many classifiers overfit the data in the high-dimensional setting; and\nothers are sensitive to the imbalanced data, that is, the class with a larger\nsample size overly influences the classifier and pushes the decision boundary\ntowards the minority class. SVM is resistant to the imbalanced data issue, but\nit overfits high-dimensional data sets by showing the undesired data-piling\nphenomena. The DWD method was proposed to improve SVM in the high-dimensional\nsetting, but its decision boundary is sensitive to the imbalanced ratio of\nsample sizes. Our FLAME family helps to understand an intrinsic connection\nbetween SVM and DWD, and improves both methods by providing a better trade-off\nbetween sensitivity to the imbalanced data and overfitting the high-dimensional\ndata. Several asymptotic properties of the FLAME classifiers are studied.\nSimulations and real data applications are investigated to illustrate the\nusefulness of the FLAME classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 02:22:20 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Qiao", "Xingye", ""], ["Zhang", "Lingsong", ""]]}, {"id": "1310.3099", "submitter": "Roland Maas", "authors": "Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann", "title": "A Bayesian Network View on Acoustic Model-Based Techniques for Robust\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a unifying Bayesian network view on various approaches\nfor acoustic model adaptation, missing feature, and uncertainty decoding that\nare well-known in the literature of robust automatic speech recognition. The\nrepresentatives of these classes can often be deduced from a Bayesian network\nthat extends the conventional hidden Markov models used in speech recognition.\nThese extensions, in turn, can in many cases be motivated from an underlying\nobservation model that relates clean and distorted feature vectors. By\nconverting the observation models into a Bayesian network representation, we\nformulate the corresponding compensation rules leading to a unified view on\nknown derivations as well as to new formulations for certain approaches. The\ngeneric Bayesian perspective provided in this contribution thus highlights\nstructural differences and similarities between the analyzed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:07:57 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 13:52:44 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Maas", "Roland", ""], ["Huemmer", "Christian", ""], ["Sehr", "Armin", ""], ["Kellermann", "Walter", ""]]}, {"id": "1310.3101", "submitter": "Eric Strobl", "authors": "Eric Strobl, Shyam Visweswaran", "title": "Deep Multiple Kernel Learning", "comments": "4 pages, 1 figure, 1 table, conference paper", "journal-ref": "IEEE 12th International Conference on Machine Learning and\n  Applications (ICMLA 2013)", "doi": "10.1109/ICMLA.2013.84", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Deep learning methods have predominantly been applied to large artificial\nneural networks. Despite their state-of-the-art performance, these large\nnetworks typically do not generalize well to datasets with limited sample\nsizes. In this paper, we take a different approach by learning multiple layers\nof kernels. We combine kernels at each layer and then optimize over an estimate\nof the support vector machine leave-one-out error rather than the dual\nobjective function. Our experiments on a variety of datasets show that each\nlayer successively increases performance with only a few base kernels.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:14:00 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Strobl", "Eric", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1310.3438", "submitter": "Peter Richtarik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "On Optimal Probabilities in Stochastic Coordinate Descent Methods", "comments": "5 pages, 1 algorithm (`NSync), 2 theorems, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new parallel coordinate descent method---`NSync---in\nwhich at each iteration a random subset of coordinates is updated, in parallel,\nallowing for the subsets to be chosen non-uniformly. We derive convergence\nrates under a strong convexity assumption, and comment on how to assign\nprobabilities to the sets to optimize the bound. The complexity and practical\nperformance of the method can outperform its uniform variant by an order of\nmagnitude. Surprisingly, the strategy of updating a single randomly selected\ncoordinate per iteration---with optimal probabilities---may require less\niterations, both in theory and practice, than the strategy of updating all\ncoordinates at every iteration.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 00:12:25 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1310.3556", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Srinivas Nambirajan, Petros Drineas", "title": "Identifying Influential Entries in a Matrix", "comments": "There is a bug in the proof of Lemma 5, which we are currently\n  working to fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any matrix A in R^(m x n) of rank \\rho, we present a probability\ndistribution over the entries of A (the element-wise leverage scores of\nequation (2)) that reveals the most influential entries in the matrix. From a\ntheoretical perspective, we prove that sampling at most s = O ((m + n) \\rho^2\nln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)\nwith respect to these scores and solving the nuclear norm minimization problem\non the sampled entries, reconstructs A exactly. To the best of our knowledge,\nthese are the strongest theoretical guarantees on matrix completion without any\nincoherence assumptions on the matrix A. From an experimental perspective, we\nshow that entries corresponding to high element-wise leverage scores reveal\nstructural properties of the data matrix that are of interest to domain\nscientists.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 03:49:02 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 12:13:32 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Kundu", "Abhisek", ""], ["Nambirajan", "Srinivas", ""], ["Drineas", "Petros", ""]]}, {"id": "1310.3561", "submitter": "Fang Han", "authors": "Fang Han, Han Liu", "title": "ECA: High Dimensional Elliptical Component Analysis in non-Gaussian\n  Distributions", "comments": "to appear in JASA (T&M)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust alternative to principal component analysis (PCA) ---\ncalled elliptical component analysis (ECA) --- for analyzing high dimensional,\nelliptically distributed data. ECA estimates the eigenspace of the covariance\nmatrix of the elliptical data. To cope with heavy-tailed elliptical\ndistributions, a multivariate rank statistic is exploited. At the model-level,\nwe consider two settings: either that the leading eigenvectors of the\ncovariance matrix are non-sparse or that they are sparse. Methodologically, we\npropose ECA procedures for both non-sparse and sparse settings. Theoretically,\nwe provide both non-asymptotic and asymptotic analyses quantifying the\ntheoretical performances of ECA. In the non-sparse setting, we show that ECA's\nperformance is highly related to the effective rank of the covariance matrix.\nIn the sparse setting, the results are twofold: (i) We show that the sparse ECA\nestimator based on a combinatoric program attains the optimal rate of\nconvergence; (ii) Based on some recent developments in estimating sparse\nleading eigenvectors, we show that a computationally efficient sparse ECA\nestimator attains the optimal rate of convergence under a suboptimal scaling.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 04:40:12 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 15:10:59 GMT"}, {"version": "v3", "created": "Sun, 16 Nov 2014 04:41:49 GMT"}, {"version": "v4", "created": "Mon, 3 Oct 2016 17:41:01 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Han", "Fang", ""], ["Liu", "Han", ""]]}, {"id": "1310.3697", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Shie Mannor", "title": "Variance Adjusted Actor Critic Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an actor-critic framework for MDPs where the objective is the\nvariance-adjusted expected return. Our critic uses linear function\napproximation, and we extend the concept of compatible features to the\nvariance-adjusted setting. We present an episodic actor-critic algorithm and\nshow that it converges almost surely to a locally optimal point of the\nobjective function.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 14:36:22 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1310.3745", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Constantine Caramanis, Sujay Sanghavi", "title": "Alternating Minimization for Mixed Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed linear regression involves the recovery of two (or more) unknown\nvectors from unlabeled linear measurements; that is, where each sample comes\nfrom exactly one of the vectors, but we do not know which one. It is a classic\nproblem, and the natural and empirically most popular approach to its solution\nhas been the EM algorithm. As in other settings, this is prone to bad local\nminima; however, each iteration is very fast (alternating between guessing\nlabels, and solving with those labels).\n  In this paper we provide a new initialization procedure for EM, based on\nfinding the leading two eigenvectors of an appropriate matrix. We then show\nthat with this, a re-sampled version of the EM algorithm provably converges to\nthe correct vectors, under natural assumptions on the sampling distribution,\nand with nearly optimal (unimprovable) sample complexity. This provides not\nonly the first characterization of EM's performance, but also much lower sample\ncomplexity as compared to both standard (randomly initialized) EM, and other\nmethods for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 16:48:15 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 19:39:55 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1310.3863", "submitter": "Giovanni Montana", "authors": "Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech,\n  Christoforos Anagnostopoulos, Giovanni Montana", "title": "Estimating Time-varying Brain Connectivity Networks from Functional MRI\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the functional architecture of the brain in terms of networks\nis becoming increasingly common. In most fMRI applications functional networks\nare assumed to be stationary, resulting in a single network estimated for the\nentire time course. However recent results suggest that the connectivity\nbetween brain regions is highly non-stationary even at rest. As a result, there\nis a need for new brain imaging methodologies that comprehensively account for\nthe dynamic (i.e., non-stationary) nature of the fMRI data. In this work we\npropose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithm\nwhich estimates dynamic brain networks from fMRI data. We apply the SINGLE\nalgorithm to functional MRI data from 24 healthy patients performing a\nchoice-response task to demonstrate the dynamic changes in network structure\nthat accompany a simple but attentionally demanding cognitive task. Using graph\ntheoretic measures we show that the Right Inferior Frontal Gyrus, frequently\nreported as playing an important role in cognitive control, dynamically changes\nwith the task. Our results suggest that the Right Inferior Frontal Gyrus plays\na fundamental role in the attention and executive function during cognitively\ndemanding tasks and may play a key role in regulating the balance between other\nbrain regions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 21:37:55 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 08:50:01 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Hellyer", "Peter", ""], ["Sharp", "David", ""], ["Leech", "Robert", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1310.3892", "submitter": "Bradley Price", "authors": "Bradley S. Price, Charles J. Geyer, and Adam J. Rothman", "title": "Ridge Fusion in Statistical Learning", "comments": "24 pages and 9 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to jointly estimate multiple\nprecision matrices for use in quadratic discriminant analysis and model based\nclustering. A ridge penalty and a ridge fusion penalty are used to introduce\nshrinkage and promote similarity between precision matrix estimates. Block-wise\ncoordinate descent is used for optimization, and validation likelihood is used\nfor tuning parameter selection. Our method is applied in quadratic discriminant\nanalysis and semi-supervised model based clustering.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 01:27:14 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 21:36:52 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 13:10:03 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Price", "Bradley S.", ""], ["Geyer", "Charles J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1310.4210", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo", "title": "Demystifying Information-Theoretic Clustering", "comments": "Proceedings of The 31st International Conference on Machine Learning\n  (ICML), 2014. 11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for clustering data which is grounded in\ninformation-theoretic principles and requires no parametric assumptions.\nPrevious attempts to use information theory to define clusters in an\nassumption-free way are based on maximizing mutual information between data and\ncluster labels. We demonstrate that this intuition suffers from a fundamental\nconceptual flaw that causes clustering performance to deteriorate as the amount\nof data increases. Instead, we return to the axiomatic foundations of\ninformation theory to define a meaningful clustering measure based on the\nnotion of consistency under coarse-graining for finite data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 21:19:22 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 22:21:06 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""], ["Sha", "Fei", ""], ["DeDeo", "Simon", ""]]}, {"id": "1310.4249", "submitter": "Gordon Berman", "authors": "Gordon J. Berman, Daniel M. Choi, William Bialek, and Joshua W.\n  Shaevitz", "title": "Mapping the stereotyped behaviour of freely-moving fruit flies", "comments": "21 pages, 17 figures. Email GJB (gberman@princeton.edu) to see\n  supplementary movies, Journal of the Royal Society Interface, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most animals possess the ability to actuate a vast diversity of movements,\nostensibly constrained only by morphology and physics. In practice, however, a\nfrequent assumption in behavioral science is that most of an animal's\nactivities can be described in terms of a small set of stereotyped motifs. Here\nwe introduce a method for mapping the behavioral space of organisms, relying\nonly upon the underlying structure of postural movement data to organize and\nclassify behaviors. We find that six different drosophilid species each perform\na mix of non-stereotyped actions and over one hundred hierarchically-organized,\nstereotyped behaviors. Moreover, we use this approach to compare these species'\nbehavioral spaces, systematically identifying subtle behavioral differences\nbetween closely-related species.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 02:44:56 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 02:26:51 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Berman", "Gordon J.", ""], ["Choi", "Daniel M.", ""], ["Bialek", "William", ""], ["Shaevitz", "Joshua W.", ""]]}, {"id": "1310.4252", "submitter": "Sihong Xie", "authors": "Sihong Xie and Xiangnan Kong and Jing Gao and Wei Fan and Philip S.Yu", "title": "Multilabel Consensus Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, a large amount of noisy and incomplete data can be\ncollected from multiple sources for prediction tasks. Combining multiple models\nor data sources helps to counteract the effects of low data quality and the\nbias of any single model or data source, and thus can improve the robustness\nand the performance of predictive models. Out of privacy, storage and bandwidth\nconsiderations, in certain circumstances one has to combine the predictions\nfrom multiple models or data sources to obtain the final predictions without\naccessing the raw data. Consensus-based prediction combination algorithms are\neffective for such situations. However, current research on prediction\ncombination focuses on the single label setting, where an instance can have one\nand only one label. Nonetheless, data nowadays are usually multilabeled, such\nthat more than one label have to be predicted at the same time. Direct\napplications of existing prediction combination methods to multilabel settings\ncan lead to degenerated performance. In this paper, we address the challenges\nof combining predictions from multiple multilabel classifiers and propose two\nnovel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and\nMLCM-a (MLCM for microAUC). These algorithms can capture label correlations\nthat are common in multilabel classifications, and optimize corresponding\nperformance metrics. Experimental results on popular multilabel classification\ntasks verify the theoretical analysis and effectiveness of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 03:04:47 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Xie", "Sihong", ""], ["Kong", "Xiangnan", ""], ["Gao", "Jing", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1310.4362", "submitter": "Jussi Gillberg Mr.", "authors": "Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi\n  Soininen, Marjo-Riitta J\\\"arvelin, Mika Ala-Korpela, Samuel Kaski", "title": "Bayesian Information Sharing Between Noise And Regression Models\n  Improves Prediction of Weak Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the prediction of weak effects in a multiple-output regression\nsetup, when covariates are expected to explain a small amount, less than\n$\\approx 1%$, of the variance of the target variables. To facilitate the\nprediction of the weak effects, we constrain our model structure by introducing\na novel Bayesian approach of sharing information between the regression model\nand the noise model. Further reduction of the effective number of parameters is\nachieved by introducing an infinite shrinkage prior and group sparsity in the\ncontext of the Bayesian reduced rank regression, and using the Bayesian\ninfinite factor model as a flexible low-rank noise model. In our experiments\nthe model incorporating the novelties outperformed alternatives in genomic\nprediction of rich phenotype data. In particular, the information sharing\nbetween the noise and regression models led to significant improvement in\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:13:45 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Gillberg", "Jussi", ""], ["Marttinen", "Pekka", ""], ["Pirinen", "Matti", ""], ["Kangas", "Antti J", ""], ["Soininen", "Pasi", ""], ["J\u00e4rvelin", "Marjo-Riitta", ""], ["Ala-Korpela", "Mika", ""], ["Kaski", "Samuel", ""]]}, {"id": "1310.4366", "submitter": "Dmitry Ignatov", "authors": "Elena Nenova and Dmitry I. Ignatov and Andrey V. Konstantinov", "title": "An FCA-based Boolean Matrix Factorisation for Collaborative Filtering", "comments": "http://ceur-ws.org/Vol-977/paper8.pdf", "journal-ref": "In: C. Carpineto, A. Napoli, S.O. Kuznetsov (eds), FCA Meets IR\n  2013, Vol. 977, CEUR Workshop Proceeding, 2013. P. 57-73", "doi": null, "report-no": null, "categories": "cs.IR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for Collaborative Filtering which is based on\nBoolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series of\nexperiments on real data (Movielens dataset) we compare the approach with the\nSVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of the\nexperimental consequences is that it is enough to have a binary-scaled rating\ndata to obtain almost the same quality in terms of MAE by BMF than for the\nSVD-based algorithm in case of non-scaled data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:17:37 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Nenova", "Elena", ""], ["Ignatov", "Dmitry I.", ""], ["Konstantinov", "Andrey V.", ""]]}, {"id": "1310.4375", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, Arnaud Doucet", "title": "Fast Computation of Wasserstein Barycenters", "comments": "9 pages, 4 figures", "journal-ref": "Proceedings of the 31st International Conference on Machine\n  Learning, JMLR W&CP 32 (2) 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms to compute the mean of a set of empirical\nprobability measures under the optimal transport metric. This mean, known as\nthe Wasserstein barycenter, is the measure that minimizes the sum of its\nWasserstein distances to each element in that set. We propose two original\nalgorithms to compute Wasserstein barycenters that build upon the subgradient\nmethod. A direct implementation of these algorithms is, however, too costly\nbecause it would require the repeated resolution of large primal and dual\noptimal transport problems to compute subgradients. Extending the work of\nCuturi (2013), we propose to smooth the Wasserstein distance used in the\ndefinition of Wasserstein barycenters with an entropic regularizer and recover\nin doing so a strictly convex objective whose gradients can be computed for a\nconsiderably cheaper computational cost using matrix scaling algorithms. We use\nthese algorithms to visualize a large family of images and to solve a\nconstrained clustering problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:47:14 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 11:10:01 GMT"}, {"version": "v3", "created": "Tue, 17 Jun 2014 14:08:44 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Cuturi", "Marco", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1310.4377", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Hierarchical Block Structures and High-resolution Model Selection in\n  Large Networks", "comments": "18 pages, 9 figures + Supplemental Material", "journal-ref": "Phys. Rev. X 4, 011047 (2014)", "doi": "10.1103/PhysRevX.4.011047", "report-no": null, "categories": "physics.data-an cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Discovering and characterizing the large-scale topological features in\nempirical networks are crucial steps in understanding how complex systems\nfunction. However, most existing methods used to obtain the modular structure\nof networks suffer from serious problems, such as being oblivious to the\nstatistical evidence supporting the discovered patterns, which results in the\ninability to separate actual structure from noise. In addition to this, one\nalso observes a resolution limit on the size of communities, where smaller but\nwell-defined clusters are not detectable when the network becomes large. This\nphenomenon occurs not only for the very popular approach of modularity\noptimization, which lacks built-in statistical validation, but also for more\nprincipled methods based on statistical inference and model selection, which do\nincorporate statistical validation in a formally correct way. Here we construct\na nested generative model that, through a complete description of the entire\nnetwork hierarchy at multiple scales, is capable of avoiding this limitation,\nand enables the detection of modular structure at levels far beyond those\npossible with current approaches. Even with this increased resolution, the\nmethod is based on the principle of parsimony, and is capable of separating\nsignal from noise, and thus will not lead to the identification of spurious\nmodules even on sparse networks. Furthermore, it fully generalizes other\napproaches in that it is not restricted to purely assortative mixing patterns,\ndirected or undirected graphs, and ad hoc hierarchical structures such as\nbinary trees. Despite its general character, the approach is tractable, and can\nbe combined with advanced techniques of community detection to yield an\nefficient algorithm that scales well for very large networks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:50:05 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 10:53:47 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2013 09:02:22 GMT"}, {"version": "v4", "created": "Thu, 26 Dec 2013 21:18:52 GMT"}, {"version": "v5", "created": "Mon, 3 Mar 2014 15:23:15 GMT"}, {"version": "v6", "created": "Tue, 25 Mar 2014 10:30:23 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1310.4378", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Efficient Monte Carlo and greedy heuristic for the inference of\n  stochastic block models", "comments": "9 pages, 9 figures", "journal-ref": "Phys. Rev. E 89, 012804 (2014)", "doi": "10.1103/PhysRevE.89.012804", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.SI physics.comp-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present an efficient algorithm for the inference of stochastic block\nmodels in large networks. The algorithm can be used as an optimized Markov\nchain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced\nsusceptibility to getting trapped in metastable states, or as a greedy\nagglomerative heuristic, with an almost linear $O(N\\ln^2N)$ complexity, where\n$N$ is the number of nodes in the network, independent on the number of blocks\nbeing inferred. We show that the heuristic is capable of delivering results\nwhich are indistinguishable from the more exact and numerically expensive MCMC\nmethod in many artificial and empirical networks, despite being much faster.\nThe method is entirely unbiased towards any specific mixing pattern, and in\nparticular it does not favor assortative community structures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:50:15 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 10:46:19 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2014 16:16:28 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1310.4456", "submitter": "Stefan Webb", "authors": "Stefan Douglas Webb", "title": "Inference, Sampling, and Learning in Copula Cumulative Distribution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative distribution network (CDN) is a recently developed class of\nprobabilistic graphical models (PGMs) permitting a copula factorization, in\nwhich the CDF, rather than the density, is factored. Despite there being much\nrecent interest within the machine learning community about copula\nrepresentations, there has been scarce research into the CDN, its amalgamation\nwith copula theory, and no evaluation of its performance. Algorithms for\ninference, sampling, and learning in these models are underdeveloped compared\nthose of other PGMs, hindering widerspread use.\n  One advantage of the CDN is that it allows the factors to be parameterized as\ncopulae, combining the benefits of graphical models with those of copula\ntheory. In brief, the use of a copula parameterization enables greater\nmodelling flexibility by separating representation of the marginals from the\ndependence structure, permitting more efficient and robust learning. Another\nadvantage is that the CDN permits the representation of implicit latent\nvariables, whose parameterization and connectivity are not required to be\nspecified. Unfortunately, that the model can encode only latent relationships\nbetween variables severely limits its utility.\n  In this thesis, we present inference, learning, and sampling for CDNs, and\nfurther the state-of-the-art. First, we explain the basics of copula theory and\nthe representation of copula CDNs. Then, we discuss inference in the models,\nand develop the first sampling algorithm. We explain standard learning methods,\npropose an algorithm for learning from data missing completely at random\n(MCAR), and develop a novel algorithm for learning models of arbitrary\ntreewidth and size. Properties of the models and algorithms are investigated\nthrough Monte Carlo simulations. We conclude with further discussion of the\nadvantages and limitations of CDNs, and suggest future work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 17:33:34 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Webb", "Stefan Douglas", ""]]}, {"id": "1310.4546", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean", "title": "Distributed Representations of Words and Phrases and their\n  Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 23:28:53 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Mikolov", "Tomas", ""], ["Sutskever", "Ilya", ""], ["Chen", "Kai", ""], ["Corrado", "Greg", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1310.4794", "submitter": "Irina Holmes", "authors": "Irina Holmes and Ambar Sengupta", "title": "The Gaussian Radon Transform and Machine Learning", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing recent interest in probabilistic interpretations of\nkernel-based methods as well as learning in Banach spaces. The absence of a\nuseful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbert\nspace is a serious obstacle for such stochastic models. We propose an\nestimation model for the ridge regression problem within the framework of\nabstract Wiener spaces and show how the support vector machine solution to such\nproblems can be interpreted in terms of the Gaussian Radon transform.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 18:43:40 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 02:49:09 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Holmes", "Irina", ""], ["Sengupta", "Ambar", ""]]}, {"id": "1310.4849", "submitter": "Willem Waegeman", "authors": "Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei\n  Cheng, Eyke Hullermeier", "title": "On the Bayes-optimality of F-measure maximizers", "comments": null, "journal-ref": "JMLR 15 (2014) 3333-3388", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure, which has originally been introduced in information retrieval,\nis nowadays routinely used as a performance metric for problems such as binary\nclassification, multi-label classification, and structured output prediction.\nOptimizing this measure is a statistically and computationally challenging\nproblem, since no closed-form solution exists. Adopting a decision-theoretic\nperspective, this article provides a formal and experimental analysis of\ndifferent approaches for maximizing the F-measure. We start with a Bayes-risk\nanalysis of related loss functions, such as Hamming loss and subset zero-one\nloss, showing that optimizing such losses as a surrogate of the F-measure leads\nto a high worst-case regret. Subsequently, we perform a similar type of\nanalysis for F-measure maximizing algorithms, showing that such algorithms are\napproximate, while relying on additional assumptions regarding the statistical\ndistribution of the binary response variables. Furthermore, we present a new\nalgorithm which is not only computationally efficient but also Bayes-optimal,\nregardless of the underlying distribution. To this end, the algorithm requires\nonly a quadratic (with respect to the number of binary responses) number of\nparameters of the joint distribution. We illustrate the practical performance\nof all analyzed methods by means of experiments with multi-label classification\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 20:34:04 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 15:58:38 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 15:58:09 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Waegeman", "Willem", ""], ["Dembczynski", "Krzysztof", ""], ["Jachnik", "Arkadiusz", ""], ["Cheng", "Weiwei", ""], ["Hullermeier", "Eyke", ""]]}, {"id": "1310.4945", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "A novel sparsity and clustering regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel SPARsity and Clustering (SPARC) regularizer, which is a\nmodified version of the previous octagonal shrinkage and clustering algorithm\nfor regression (OSCAR), where, the proposed regularizer consists of a\n$K$-sparse constraint and a pair-wise $\\ell_{\\infty}$ norm restricted on the\n$K$ largest components in magnitude. The proposed regularizer is able to\nseparably enforce $K$-sparsity and encourage the non-zeros to be equal in\nmagnitude. Moreover, it can accurately group the features without shrinking\ntheir magnitude. In fact, SPARC is closely related to OSCAR, so that the\nproximity operator of the former can be efficiently computed based on that of\nthe latter, allowing using proximal splitting algorithms to solve problems with\nSPARC regularization. Experiments on synthetic data and with benchmark breast\ncancer data show that SPARC is a competitive group-sparsity inducing\nregularizer for regression and classification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:31:54 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 16:41:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1310.5007", "submitter": "Tianbing Xu", "authors": "Tianbing Xu, Jianfeng Gao, Lin Xiao, Amelia Regan", "title": "Online Classification Using a Voted RDA Method", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a voted dual averaging method for online classification problems\nwith explicit regularization. This method employs the update rule of the\nregularized dual averaging (RDA) method, but only on the subsequence of\ntraining examples where a classification error is made. We derive a bound on\nthe number of mistakes made by this method on the training set, as well as its\ngeneralization error rate. We also introduce the concept of relative strength\nof regularization, and show how it affects the mistake bound and generalization\nperformance. We experimented with the method using $\\ell_1$ regularization on a\nlarge-scale natural language processing task, and obtained state-of-the-art\nclassification performance with fairly sparse models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 04:01:25 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Xu", "Tianbing", ""], ["Gao", "Jianfeng", ""], ["Xiao", "Lin", ""], ["Regan", "Amelia", ""]]}, {"id": "1310.5034", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Kathrin Bujna, and Daniel Kuntze", "title": "A Theoretical and Experimental Comparison of the EM and SEM Algorithm", "comments": "This paper is a preprint of a paper submitted to and accepted for\n  publication in ICPR 2014 and is subject to IEEE copyright", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a new analysis of the SEM algorithm. Unlike previous\nwork, we focus on the analysis of a single run of the algorithm. First, we\ndiscuss the algorithm for general mixture distributions. Second, we consider\nGaussian mixture models and show that with high probability the update\nequations of the EM algorithm and its stochastic variant are almost the same,\ngiven that the input set is sufficiently large. Our experiments confirm that\nthis still holds for a large number of successive update steps. In particular,\nfor Gaussian mixture models, we show that the stochastic variant runs nearly\ntwice as fast.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:31:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 14:05:49 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Bujna", "Kathrin", ""], ["Kuntze", "Daniel", ""]]}, {"id": "1310.5035", "submitter": "Zhouchen Lin", "authors": "Zhouchen Lin, Risheng Liu, Huan Li", "title": "Linearized Alternating Direction Method with Parallel Splitting and\n  Adaptive Penalty for Separable Convex Programs in Machine Learning", "comments": "Preliminary version published on Asian Conference on Machine Learning\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and other fields can be (re)for-mulated as\nlinearly constrained separable convex programs. In most of the cases, there are\nmultiple blocks of variables. However, the traditional alternating direction\nmethod (ADM) and its linearized version (LADM, obtained by linearizing the\nquadratic penalty term) are for the two-block case and cannot be naively\ngeneralized to solve the multi-block case. So there is great demand on\nextending the ADM based methods for the multi-block case. In this paper, we\npropose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve\nmulti-block separable convex programs efficiently. When all the component\nobjective functions have bounded subgradients, we obtain convergence results\nthat are stronger than those of ADM and LADM, e.g., allowing the penalty\nparameter to be unbounded and proving the sufficient and necessary conditions}\nfor global convergence. We further propose a simple optimality measure and\nreveal the convergence rate of LADMPSAP in an ergodic sense. For programs with\nextra convex set constraints, with refined parameter estimation we devise a\npractical version of LADMPSAP for faster convergence. Finally, we generalize\nLADMPSAP to handle programs with more difficult objective functions by\nlinearizing part of the objective function as well. LADMPSAP is particularly\nsuitable for sparse representation and low-rank recovery problems because its\nsubproblems have closed form solutions and the sparsity and low-rankness of the\niterates can be preserved during the iteration. It is also highly\nparallelizable and hence fits for parallel or distributed computing. Numerical\nexperiments testify to the advantages of LADMPSAP in speed and numerical\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:31:08 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 02:14:13 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Lin", "Zhouchen", ""], ["Liu", "Risheng", ""], ["Li", "Huan", ""]]}, {"id": "1310.5082", "submitter": "Gustavo Camps-Valls", "authors": "Gustavo Camps-Valls, Juan Guti\\'errez, Gabriel G\\'omez-P\\'erez,\n  Jes\\'us Malo", "title": "On the Suitable Domain for SVM Training in Image Coding", "comments": null, "journal-ref": "Journal of Machine Learning Research, JMLR, 9(1), 49-66, 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional SVM-based image coding methods are founded on independently\nrestricting the distortion in every image coefficient at some particular image\nrepresentation. Geometrically, this implies allowing arbitrary signal\ndistortions in an $n$-dimensional rectangle defined by the\n$\\varepsilon$-insensitivity zone in each dimension of the selected image\nrepresentation domain. Unfortunately, not every image representation domain is\nwell-suited for such a simple, scalar-wise, approach because statistical and/or\nperceptual interactions between the coefficients may exist. These interactions\nimply that scalar approaches may induce distortions that do not follow the\nimage statistics and/or are perceptually annoying. Taking into account these\nrelations would imply using non-rectangular $\\varepsilon$-insensitivity regions\n(allowing coupled distortions in different coefficients), which is beyond the\nconventional SVM formulation.\n  In this paper, we report a condition on the suitable domain for developing\nefficient SVM image coding schemes. We analytically demonstrate that no linear\ndomain fulfills this condition because of the statistical and perceptual\ninter-coefficient relations that exist in these domains. This theoretical\nresult is experimentally confirmed by comparing SVM learning in previously\nreported linear domains and in a recently proposed non-linear perceptual domain\nthat simultaneously reduces the statistical and perceptual relations (so it is\ncloser to fulfilling the proposed condition). These results highlight the\nrelevance of an appropriate choice of the image representation before SVM\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 16:34:04 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Camps-Valls", "Gustavo", ""], ["Guti\u00e9rrez", "Juan", ""], ["G\u00f3mez-P\u00e9rez", "Gabriel", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1310.5089", "submitter": "Gustavo Camps-Valls", "authors": "Jer\\'onimo Arenas-Garc\\'ia, Kaare Brandt Petersen, Gustavo\n  Camps-Valls, Lars Kai Hansen", "title": "Kernel Multivariate Analysis Framework for Supervised Subspace Learning:\n  A Tutorial on Linear and Kernel Multivariate Methods", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, 30(4), 16-29, 2013", "doi": "10.1109/MSP.2013.2250591", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and dimensionality reduction are important tasks in many\nfields of science dealing with signal processing and analysis. The relevance of\nthese techniques is increasing as current sensory devices are developed with\never higher resolution, and problems involving multimodal data sources become\nmore common. A plethora of feature extraction methods are available in the\nliterature collectively grouped under the field of Multivariate Analysis (MVA).\nThis paper provides a uniform treatment of several methods: Principal Component\nAnalysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis\n(CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensions\nderived by means of the theory of reproducing kernel Hilbert spaces. We also\nreview their connections to other methods for classification and statistical\ndependence estimation, and introduce some recent developments to deal with the\nextreme cases of large-scale and low-sized problems. To illustrate the wide\napplicability of these methods in both classification and regression problems,\nwe analyze their performance in a benchmark of publicly available data sets,\nand pay special attention to specific real applications involving audio\nprocessing for music genre prediction and hyperspectral satellite images for\nEarth and climate monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 16:44:05 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""], ["Petersen", "Kaare Brandt", ""], ["Camps-Valls", "Gustavo", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1310.5095", "submitter": "Fabrice Rossi", "authors": "Martin Riedel, Marika K\\\"astner, Fabrice Rossi (SAMM), Thomas Villmann", "title": "Regularization in Relevance Learning Vector Quantization Using l one\n  Norms", "comments": null, "journal-ref": "21-th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN 2013), Bruges :\n  Belgium (2013)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this contribution a method for l one regularization in\nprototype based relevance learning vector quantization (LVQ) for sparse\nrelevance profiles. Sparse relevance profiles in hyperspectral data analysis\nfade down those spectral bands which are not necessary for classification. In\nparticular, we consider the sparsity in the relevance profile enforced by LASSO\noptimization. The latter one is obtained by a gradient learning scheme using a\ndifferentiable parametrized approximation of the $l_{1}$-norm, which has an\nupper error bound. We extend this regularization idea also to the matrix\nlearning variant of LVQ as the natural generalization of relevance learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 17:00:34 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Riedel", "Martin", "", "SAMM"], ["K\u00e4stner", "Marika", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Villmann", "Thomas", ""]]}, {"id": "1310.5288", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham", "title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian\n  Processes", "comments": "13 Pages, 9 Figures, 1 Table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are typically used for smoothing and interpolation on\nsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --\nenabling automatic pattern extrapolation with Gaussian processes on large\nmultidimensional datasets. GPatt unifies and extends highly expressive kernels\nand fast exact inference techniques. Without human intervention -- no hand\ncrafting of kernel features, and no sophisticated initialisation procedures --\nwe show that GPatt can solve large scale pattern extrapolation, inpainting, and\nkernel discovery problems, including a problem with 383400 training points. We\nfind that GPatt significantly outperforms popular alternative scalable Gaussian\nprocess methods in speed and accuracy. Moreover, we discover profound\ndifferences between each of these methods, suggesting expressive kernels,\nnonparametric representations, and exact inference are useful for modelling\nlarge scale multidimensional patterns.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 01:26:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 16:58:35 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 14:10:34 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Gilboa", "Elad", ""], ["Nehorai", "Arye", ""], ["Cunningham", "John P.", ""]]}, {"id": "1310.5347", "submitter": "Il Memming Park", "authors": "Il Memming Park, Sohan Seth, Steven Van Vaerenbergh", "title": "Bayesian Extensions of Kernel Least Mean Squares", "comments": "7 pages, 4 fiures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel least mean squares (KLMS) algorithm is a computationally efficient\nnonlinear adaptive filtering method that \"kernelizes\" the celebrated (linear)\nleast mean squares algorithm. We demonstrate that the least mean squares\nalgorithm is closely related to the Kalman filtering, and thus, the KLMS can be\ninterpreted as an approximate Bayesian filtering method. This allows us to\nsystematically develop extensions of the KLMS by modifying the underlying\nstate-space and observation models. The resulting extensions introduce many\ndesirable properties such as \"forgetting\", and the ability to learn from\ndiscrete data, while retaining the computational simplicity and time complexity\nof the original algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 16:58:57 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Park", "Il Memming", ""], ["Seth", "Sohan", ""], ["Van Vaerenbergh", "Steven", ""]]}, {"id": "1310.5415", "submitter": "Takanori Watanabe", "authors": "Takanori Watanabe, Daniel Kessler, Clayton Scott, Michael Angstadt,\n  Chandra Sripada", "title": "Disease Prediction based on Functional Connectomes using a Scalable and\n  Spatially-Informed Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial evidence indicates that major psychiatric disorders are\nassociated with distributed neural dysconnectivity, leading to strong interest\nin using neuroimaging methods to accurately predict disorder status. In this\nwork, we are specifically interested in a multivariate approach that uses\nfeatures derived from whole-brain resting state functional connectomes.\nHowever, functional connectomes reside in a high dimensional space, which\ncomplicates model interpretation and introduces numerous statistical and\ncomputational challenges. Traditional feature selection techniques are used to\nreduce data dimensionality, but are blind to the spatial structure of the\nconnectomes. We propose a regularization framework where the 6-D structure of\nthe functional connectome is explicitly taken into account via the fused Lasso\nor the GraphNet regularizer. Our method only restricts the loss function to be\nconvex and margin-based, allowing non-differentiable loss functions such as the\nhinge-loss to be used. Using the fused Lasso or GraphNet regularizer with the\nhinge-loss leads to a structured sparse support vector machine (SVM) with\nembedded feature selection. We introduce a novel efficient optimization\nalgorithm based on the augmented Lagrangian and the classical alternating\ndirection method, which can solve both fused Lasso and GraphNet regularized SVM\nwith very little modification. We also demonstrate that the inner subproblems\nof the algorithm can be solved efficiently in analytic form by coupling the\nvariable splitting strategy with a data augmentation scheme. Experiments on\nsimulated data and resting state scans from a large schizophrenia dataset show\nthat our proposed approach can identify predictive regions that are spatially\ncontiguous in the 6-D \"connectome space,\" offering an additional layer of\ninterpretability that could provide new insights about various disease\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 04:03:46 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 01:45:43 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Watanabe", "Takanori", ""], ["Kessler", "Daniel", ""], ["Scott", "Clayton", ""], ["Angstadt", "Michael", ""], ["Sripada", "Chandra", ""]]}, {"id": "1310.5426", "submitter": "Evan Sparks", "authors": "Evan R. Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao\n  Pan, Joseph Gonzalez, Michael J. Franklin, Michael I. Jordan, Tim Kraska", "title": "MLI: An API for Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLI is an Application Programming Interface designed to address the\nchallenges of building Machine Learn- ing algorithms in a distributed setting\nbased on data-centric computing. Its primary goal is to simplify the\ndevelopment of high-performance, scalable, distributed algorithms. Our initial\nresults show that, relative to existing systems, this interface can be used to\nbuild distributed implementations of a wide variety of common Machine Learning\nalgorithms with minimal complexity and highly competitive performance and\nscalability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 04:58:11 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 22:08:12 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Sparks", "Evan R.", ""], ["Talwalkar", "Ameet", ""], ["Smith", "Virginia", ""], ["Kottalam", "Jey", ""], ["Pan", "Xinghao", ""], ["Gonzalez", "Joseph", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Kraska", "Tim", ""]]}, {"id": "1310.5438", "submitter": "Jan Drugowitsch", "authors": "Jan Drugowitsch", "title": "Variational Bayesian inference for linear and logistic regression", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describe the model, derivation, and implementation of variational\nBayesian inference for linear and logistic regression, both with and without\nautomatic relevance determination. It has the dual function of acting as a\ntutorial for the derivation of variational Bayesian inference for simple\nmodels, as well as documenting, and providing brief examples for the\nMATLAB/Octave functions that implement this inference. These functions are\nfreely available online.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 07:10:51 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 15:40:34 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 21:34:00 GMT"}, {"version": "v4", "created": "Sat, 29 Jun 2019 00:31:44 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Drugowitsch", "Jan", ""]]}, {"id": "1310.5543", "submitter": "Haizhang Zhang", "authors": "Benxun Wang, Haizhang Zhang", "title": "Universalities of Reproducing Kernels Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have been widely applied to machine learning and other\nquestions of approximating an unknown function from its finite sample data. To\nensure arbitrary accuracy of such approximation, various denseness conditions\nare imposed on the selected kernel. This note contributes to the study of\nuniversal, characteristic, and $C_0$-universal kernels. We first give simple\nand direct description of the difference and relation among these three kinds\nof universalities of kernels. We then focus on translation-invariant and\nweighted polynomial kernels. A simple and shorter proof of the known\ncharacterization of characteristic translation-invariant kernels will be\npresented. The main purpose of the note is to give a delicate discussion on the\nuniversalities of weighted polynomial kernels.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 13:51:16 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2013 03:45:51 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Wang", "Benxun", ""], ["Zhang", "Haizhang", ""]]}, {"id": "1310.5666", "submitter": "Helene Massam", "authors": "Helene Massam and Nanwei Wang", "title": "Distributed parameter estimation of discrete hierarchical models via\n  marginal likelihoods", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider discrete graphical models Markov with respect to a graph $G$ and\npropose two distributed marginal methods to estimate the maximum likelihood\nestimate of the canonical parameter of the model. Both methods are based on a\nrelaxation of the marginal likelihood obtained by considering the density of\nthe variables represented by a vertex $v$ of $G$ and a neighborhood. The two\nmethods differ by the size of the neighborhood of $v$. We show that the\nestimates are consistent and that those obtained with the larger neighborhood\nhave smaller asymptotic variance than the ones obtained through the smaller\nneighborhood.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 18:29:12 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Massam", "Helene", ""], ["Wang", "Nanwei", ""]]}, {"id": "1310.5715", "submitter": "Deanna Needell", "authors": "Deanna Needell, Nathan Srebro, Rachel Ward", "title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized\n  Kaczmarz algorithm", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain an improved finite-sample guarantee on the linear convergence of\nstochastic gradient descent for smooth and strongly convex objectives,\nimproving from a quadratic dependence on the conditioning $(L/\\mu)^2$ (where\n$L$ is a bound on the smoothness and $\\mu$ on the strong convexity) to a linear\ndependence on $L/\\mu$. Furthermore, we show how reweighting the sampling\ndistribution (i.e. importance sampling) is necessary in order to further\nimprove convergence, and obtain a linear dependence in the average smoothness,\ndominating previous results. We also discuss importance sampling for SGD more\nbroadly and show how it can improve convergence also in other scenarios. Our\nresults are based on a connection we make between SGD and the randomized\nKaczmarz algorithm, which allows us to transfer ideas between the separate\nbodies of literature studying each of the two methods. In particular, we recast\nthe randomized Kaczmarz algorithm as an instance of SGD, and apply our results\nto prove its exponential convergence, but to the solution of a weighted least\nsquares problem rather than the original least squares problem. We then present\na modified Kaczmarz algorithm with partially biased sampling which does\nconverge to the original least squares solution with the same exponential\nconvergence rate.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 20:15:44 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2014 01:43:32 GMT"}, {"version": "v3", "created": "Thu, 20 Mar 2014 16:51:23 GMT"}, {"version": "v4", "created": "Thu, 27 Nov 2014 05:10:09 GMT"}, {"version": "v5", "created": "Fri, 16 Jan 2015 17:11:24 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Needell", "Deanna", ""], ["Srebro", "Nathan", ""], ["Ward", "Rachel", ""]]}, {"id": "1310.5738", "submitter": "Frank Hutter", "authors": "Frank Hutter and Michael A. Osborne", "title": "A Kernel for Hierarchical Parameter Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a family of kernels for mixed continuous/discrete hierarchical\nparameter spaces and show that they are positive definite.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 22:02:17 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Hutter", "Frank", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1310.5764", "submitter": "Chao Gao", "authors": "Chao Gao and Dengyong Zhou", "title": "Minimax Optimal Convergence Rates for Estimating Ground Truth from\n  Crowdsourced Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has become a primary means for label collection in many\nreal-world machine learning applications. A classical method for inferring the\ntrue labels from the noisy labels provided by crowdsourcing workers is\nDawid-Skene estimator. In this paper, we prove convergence rates of a projected\nEM algorithm for the Dawid-Skene estimator. The revealed exponent in the rate\nof convergence is shown to be optimal via a lower bound argument. Our work\nresolves the long standing issue of whether Dawid-Skene estimator has sound\ntheoretical guarantees besides its good performance observed in practice. In\naddition, a comparative study with majority voting illustrates both advantages\nand pitfalls of the Dawid-Skene estimator.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 00:12:56 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 21:47:14 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2014 00:47:36 GMT"}, {"version": "v4", "created": "Fri, 27 Jun 2014 22:28:20 GMT"}, {"version": "v5", "created": "Sat, 7 Feb 2015 19:51:54 GMT"}, {"version": "v6", "created": "Mon, 30 May 2016 17:50:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Gao", "Chao", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1310.5791", "submitter": "T. Tony Cai", "authors": "T. Tony Cai, Anru Zhang", "title": "ROP: Matrix recovery via rank-one projections", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1267 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 102-138", "doi": "10.1214/14-AOS1267", "report-no": "IMS-AOS-AOS1267", "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of low-rank matrices is of significant interest in a range of\ncontemporary applications. In this paper, we introduce a rank-one projection\nmodel for low-rank matrix recovery and propose a constrained nuclear norm\nminimization method for stable recovery of low-rank matrices in the noisy case.\nThe procedure is adaptive to the rank and robust against small perturbations.\nBoth upper and lower bounds for the estimation accuracy under the Frobenius\nnorm loss are obtained. The proposed estimator is shown to be rate-optimal\nunder certain conditions. The estimator is easy to implement via convex\nprogramming and performs well numerically. The techniques and main results\ndeveloped in the paper also have implications to other related statistical\nproblems. An application to estimation of spiked covariance matrices from\none-dimensional random projections is considered. The results demonstrate that\nit is still possible to accurately estimate the covariance matrix of a\nhigh-dimensional distribution based only on one-dimensional projections.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 03:30:29 GMT"}, {"version": "v2", "created": "Sun, 17 Aug 2014 13:20:54 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 10:24:46 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1310.6062", "submitter": "Piotr Pokarowski", "authors": "Piotr Pokarowski and Jan Mielniczuk", "title": "Combined l_1 and greedy l_0 penalized least squares for linear model\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a computationally effective algorithm for a linear model\nselection consisting of three steps: screening--ordering--selection (SOS).\nScreening of predictors is based on the thresholded Lasso that is l_1 penalized\nleast squares. The screened predictors are then fitted using least squares (LS)\nand ordered with respect to their t statistics. Finally, a model is selected\nusing greedy generalized information criterion (GIC) that is l_0 penalized LS\nin a nested family induced by the ordering. We give non-asymptotic upper bounds\non error probability of each step of the SOS algorithm in terms of both\npenalties. Then we obtain selection consistency for different (n, p) scenarios\nunder conditions which are needed for screening consistency of the Lasso. For\nthe traditional setting (n >p) we give Sanov-type bounds on the error\nprobabilities of the ordering--selection algorithm. Its surprising consequence\nis that the selection error of greedy GIC is asymptotically not larger than of\nexhaustive GIC. We also obtain new bounds on prediction and estimation errors\nfor the Lasso which are proved in parallel for the algorithm used in practice\nand its formal version.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 21:27:18 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Pokarowski", "Piotr", ""], ["Mielniczuk", "Jan", ""]]}, {"id": "1310.6067", "submitter": "Wojciech Samek", "authors": "Wojciech Samek and Alexander Binder and Klaus-Robert M\\\"uller", "title": "Multiple Kernel Learning for Brain-Computer Interfacing", "comments": "Corrected manuscript", "journal-ref": "W. Samek, A. Binder, K.-R. M\\\"uller. Multiple Kernel Learning for\n  Brain-Computer Interfacing. Proceedings of 35th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),\n  7048-51, 2013", "doi": "10.1109/EMBC.2013.6611181", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining information from different sources is a common way to improve\nclassification accuracy in Brain-Computer Interfacing (BCI). For instance, in\nsmall sample settings it is useful to integrate data from other subjects or\nsessions in order to improve the estimation quality of the spatial filters or\nthe classifier. Since data from different subjects may show large variability,\nit is crucial to weight the contributions according to importance. Many\nmulti-subject learning algorithms determine the optimal weighting in a separate\nstep by using heuristics, however, without ensuring that the selected weights\nare optimal with respect to classification. In this work we apply Multiple\nKernel Learning (MKL) to this problem. MKL has been widely used for feature\nfusion in computer vision and allows to simultaneously learn the classifier and\nthe optimal weighting. We compare the MKL method to two baseline approaches and\ninvestigate the reasons for performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 21:46:02 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Samek", "Wojciech", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1310.6288", "submitter": "Hao Zhang", "authors": "Hao Zhang and Liqing Zhang", "title": "Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery\n  EEG in Rehabilitation Training", "comments": "10 pages,3 figures", "journal-ref": null, "doi": "10.3233/978-1-61499-419-0-537", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current studies about motor imagery based rehabilitation training systems for\nstroke subjects lack an appropriate analytic method, which can achieve a\nconsiderable classification accuracy, at the same time detects gradual changes\nof imagery patterns during rehabilitation process and disinters potential\nmechanisms about motor function recovery. In this study, we propose an adaptive\nboosting algorithm based on the cortex plasticity and spectral band shifts.\nThis approach models the usually predetermined spatial-spectral configurations\nin EEG study into variable preconditions, and introduces a new heuristic of\nstochastic gradient boost for training base learners under these preconditions.\nWe compare our proposed algorithm with commonly used methods on datasets\ncollected from 2 months' clinical experiments. The simulation results\ndemonstrate the effectiveness of the method in detecting the variations of\nstroke patients' EEG patterns. By chronologically reorganizing the weight\nparameters of the learned additive model, we verify the spatial compensatory\nmechanism on impaired cortex and detect the changes of accentuation bands in\nspectral domain, which may contribute important prior knowledge for\nrehabilitation practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 16:43:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Zhang", "Hao", ""], ["Zhang", "Liqing", ""]]}, {"id": "1310.6319", "submitter": "Steven Reece", "authors": "Steven Reece, Stephen Roberts, Siddhartha Ghosh, Alex Rogers and\n  Nicholas Jennings", "title": "Efficient State-Space Inference of Periodic Latent Force Models", "comments": "61 pages, 13 figures, accepted for publication in JMLR. Updates from\n  earlier version occur throughout article in response to JMLR reviews", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent force models (LFM) are principled approaches to incorporating\nsolutions to differential equations within non-parametric inference methods.\nUnfortunately, the development and application of LFMs can be inhibited by\ntheir computational cost, especially when closed-form solutions for the LFM are\nunavailable, as is the case in many real world problems where these latent\nforces exhibit periodic behaviour. Given this, we develop a new sparse\nrepresentation of LFMs which considerably improves their computational\nefficiency, as well as broadening their applicability, in a principled way, to\ndomains with periodic or near periodic latent forces. Our approach uses a\nlinear basis model to approximate one generative model for each periodic force.\nWe assume that the latent forces are generated from Gaussian process priors and\ndevelop a linear basis model which fully expresses these priors. We apply our\napproach to model the thermal dynamics of domestic buildings and show that it\nis effective at predicting day-ahead temperatures within the homes. We also\napply our approach within queueing theory in which quasi-periodic arrival rates\nare modelled as latent forces. In both cases, we demonstrate that our approach\ncan be implemented efficiently using state-space methods which encode the\nlinear dynamic systems via LFMs. Further, we show that state estimates obtained\nusing periodic latent force models can reduce the root mean squared error to\n17% of that from non-periodic models and 27% of the nearest rival approach\nwhich is the resonator model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 18:27:42 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 19:20:14 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Reece", "Steven", ""], ["Roberts", "Stephen", ""], ["Ghosh", "Siddhartha", ""], ["Rogers", "Alex", ""], ["Jennings", "Nicholas", ""]]}, {"id": "1310.6343", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora and Aditya Bhaskara and Rong Ge and Tengyu Ma", "title": "Provable Bounds for Learning Some Deep Representations", "comments": "The first 18 pages serve as an extended abstract and a 36 pages long\n  technical appendix follows", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms with provable guarantees that learn a class of deep nets\nin the generative model view popularized by Hinton and others. Our generative\nmodel is an $n$ node multilayer neural net that has degree at most $n^{\\gamma}$\nfor some $\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our\nalgorithm learns {\\em almost all} networks in this class with polynomial\nrunning time. The sample complexity is quadratic or cubic depending upon the\ndetails of the model.\n  The algorithm uses layerwise learning. It is based upon a novel idea of\nobserving correlations among features and using these to infer the underlying\nedge structure via a global graph recovery procedure. The analysis of the\nalgorithm reveals interesting structure of neural networks with random edge\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 19:49:32 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Arora", "Sanjeev", ""], ["Bhaskara", "Aditya", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1310.6536", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Randomized co-training: from cortical neurons to machine learning and\n  back again", "comments": "NIPS workshop: Randomized methods for machine learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its size and complexity, the human cortex exhibits striking\nanatomical regularities, suggesting there may simple meta-algorithms underlying\ncortical learning and computation. We expect such meta-algorithms to be of\ninterest since they need to operate quickly, scalably and effectively with\nlittle-to-no specialized assumptions.\n  This note focuses on a specific question: How can neurons use vast quantities\nof unlabeled data to speed up learning from the comparatively rare labels\nprovided by reward systems? As a partial answer, we propose randomized\nco-training as a biologically plausible meta-algorithm satisfying the above\nrequirements. As evidence, we describe a biologically-inspired algorithm,\nCorrelated Nystrom Views (XNV) that achieves state-of-the-art performance in\nsemi-supervised learning, and sketch work in progress on a neuronal\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 09:33:17 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1310.6547", "submitter": "Giuseppe Jurman", "authors": "Tommaso Furlanello and Marco Cristoforetti and Cesare Furlanello and\n  Giuseppe Jurman", "title": "Sparse Predictive Structure of Deconvolved Functional Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional and structural representation of the brain as a complex\nnetwork is marked by the fact that the comparison of noisy and intrinsically\ncorrelated high-dimensional structures between experimental conditions or\ngroups shuns typical mass univariate methods. Furthermore most network\nestimation methods cannot distinguish between real and spurious correlation\narising from the convolution due to nodes' interaction, which thus introduces\nadditional noise in the data. We propose a machine learning pipeline aimed at\nidentifying multivariate differences between brain networks associated to\ndifferent experimental conditions. The pipeline (1) leverages the deconvolved\nindividual contribution of each edge and (2) maps the task into a sparse\nclassification problem in order to construct the associated \"sparse deconvolved\npredictive network\", i.e., a graph with the same nodes of those compared but\nwhose edge weights are defined by their relevance for out of sample predictions\nin classification. We present an application of the proposed method by decoding\nthe covert attention direction (left or right) based on the single-trial\nfunctional connectivity matrix extracted from high-frequency\nmagnetoencephalography (MEG) data. Our results demonstrate how network\ndeconvolution matched with sparse classification methods outperforms typical\napproaches for MEG decoding.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 10:13:20 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Furlanello", "Tommaso", ""], ["Cristoforetti", "Marco", ""], ["Furlanello", "Cesare", ""], ["Jurman", "Giuseppe", ""]]}, {"id": "1310.6740", "submitter": "Roman Garnett", "authors": "Roman Garnett and Michael A. Osborne and Philipp Hennig", "title": "Active Learning of Linear Embeddings for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active learning method for discovering low-dimensional\nstructure in high-dimensional Gaussian process (GP) tasks. Such problems are\nincreasingly frequent and important, but have hitherto presented severe\npractical difficulties. We further introduce a novel technique for\napproximately marginalizing GP hyperparameters, yielding marginal predictions\nrobust to hyperparameter mis-specification. Our method offers an efficient\nmeans of performing GP regression, quadrature, or Bayesian optimization in\nhigh-dimensional spaces.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 14:15:39 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Garnett", "Roman", ""], ["Osborne", "Michael A.", ""], ["Hennig", "Philipp", ""]]}, {"id": "1310.6778", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu and Kenneth Bollen", "title": "Bayesian estimation of possible causal direction in the presence of\n  latent confounders using a linear non-Gaussian acyclic structural equation\n  model with individual-specific effects", "comments": "21 pages, 4 figures. A revised version was accepted at Journal of\n  Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning the possible causal direction of two observed variables\nin the presence of latent confounding variables. Several existing methods have\nbeen shown to consistently estimate causal direction assuming linear or some\ntype of nonlinear relationship and no latent confounders. However, the\nestimation results could be distorted if either assumption is actually\nviolated. In this paper, we first propose a new linear non-Gaussian acyclic\nstructural equation model with individual-specific effects that allows latent\nconfounders to be considered. We then propose an empirical Bayesian approach\nfor estimating possible causal direction using the new model. We demonstrate\nthe effectiveness of our method using artificial and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 21:27:57 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 00:38:08 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Shimizu", "Shohei", ""], ["Bollen", "Kenneth", ""]]}, {"id": "1310.6998", "submitter": "Shiladitya Sinha", "authors": "Shiladitya Sinha, Chris Dyer, Kevin Gimpel, and Noah A. Smith", "title": "Predicting the NFL using Twitter", "comments": "Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\n  Mining for Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between social media output and National Football\nLeague (NFL) games, using a dataset containing messages from Twitter and NFL\ngame statistics. Specifically, we consider tweets pertaining to specific teams\nand games in the NFL season and use them alongside statistical game data to\nbuild predictive models for future game outcomes (which team will win?) and\nsports betting outcomes (which team will win with the point spread? will the\ntotal points be over/under the line?). We experiment with several feature sets\nand find that simple features using large volumes of tweets can match or exceed\nthe performance of more traditional features that use game statistics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 18:35:22 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Sinha", "Shiladitya", ""], ["Dyer", "Chris", ""], ["Gimpel", "Kevin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1310.7033", "submitter": "Yue Wang", "authors": "Niya Wang, Eric P. Hoffman, Robert Clarke, Zhen Zhang, David M.\n  Herrington, Ie-Ming Shih, Douglas A. Levine, Guoqiang Yu, Jianhua Xuan and\n  Yue Wang", "title": "A feasible roadmap for unsupervised deconvolution of two-source mixed\n  gene expressions", "comments": "5 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue heterogeneity is a major confounding factor in studying individual\npopulations that cannot be resolved directly by global profiling. Experimental\nsolutions to mitigate tissue heterogeneity are expensive, time consuming,\ninapplicable to existing data, and may alter the original gene expression\npatterns. Here we ask whether it is possible to deconvolute two-source mixed\nexpressions (estimating both proportions and cell-specific profiles) from two\nor more heterogeneous samples without requiring any prior knowledge. Supported\nby a well-grounded mathematical framework, we argue that both constituent\nproportions and cell-specific expressions can be estimated in a completely\nunsupervised mode when cell-specific marker genes exist, which do not have to\nbe known a priori, for each of constituent cell types. We demonstrate the\nperformance of unsupervised deconvolution on both simulation and real gene\nexpression data, together with perspective discussions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 20:10:08 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Wang", "Niya", ""], ["Hoffman", "Eric P.", ""], ["Clarke", "Robert", ""], ["Zhang", "Zhen", ""], ["Herrington", "David M.", ""], ["Shih", "Ie-Ming", ""], ["Levine", "Douglas A.", ""], ["Yu", "Guoqiang", ""], ["Xuan", "Jianhua", ""], ["Wang", "Yue", ""]]}, {"id": "1310.7048", "submitter": "Jie  Wang", "authors": "Jie Wang and Peter Wonka and Jieping Ye", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is a widely used method for classification.\nAlthough many efforts have been devoted to develop efficient solvers, it\nremains challenging to apply SVM to large-scale problems. A nice property of\nSVM is that the non-support vectors have no effect on the resulting classifier.\nMotivated by this observation, we present fast and efficient screening rules to\ndiscard non-support vectors by analyzing the dual problem of SVM via\nvariational inequalities (DVI). As a result, the number of data instances to be\nentered into the optimization can be substantially reduced. Some appealing\nfeatures of our screening method are: (1) DVI is safe in the sense that the\nvectors discarded by DVI are guaranteed to be non-support vectors; (2) the data\nset needs to be scanned only once to run the screening, whose computational\ncost is negligible compared to that of solving the SVM problem; (3) DVI is\nindependent of the solvers and can be integrated with any existing efficient\nsolvers. We also show that the DVI technique can be extended to detect\nnon-support vectors in the least absolute deviations regression (LAD). To the\nbest of our knowledge, there are currently no screening methods for LAD. We\nhave evaluated DVI on both synthetic and real data sets. Experiments indicate\nthat DVI significantly outperforms the existing state-of-the-art screening\nrules for SVM, and is very effective in discarding non-support vectors for LAD.\nThe speedup gained by DVI rules can be up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 23:01:52 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Wang", "Jie", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1310.7163", "submitter": "Lihong Li", "authors": "Lihong Li", "title": "Generalized Thompson Sampling for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling, one of the oldest heuristics for solving multi-armed\nbandits, has recently been shown to demonstrate state-of-the-art performance.\nThe empirical success has led to great interests in theoretical understanding\nof this heuristic. In this paper, we approach this problem in a way very\ndifferent from existing efforts. In particular, motivated by the connection\nbetween Thompson Sampling and exponentiated updates, we propose a new family of\nalgorithms called Generalized Thompson Sampling in the expert-learning\nframework, which includes Thompson Sampling as a special case. Similar to most\nexpert-learning algorithms, Generalized Thompson Sampling uses a loss function\nto adjust the experts' weights. General regret bounds are derived, which are\nalso instantiated to two important loss functions: square loss and logarithmic\nloss. In contrast to existing bounds, our results apply to quite general\ncontextual bandits. More importantly, they quantify the effect of the \"prior\"\ndistribution on the regret bounds.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 06:29:55 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Li", "Lihong", ""]]}, {"id": "1310.7300", "submitter": "Maxim Raginsky", "authors": "Peng Guan, Maxim Raginsky, Rebecca Willett", "title": "Relax but stay in control: from value to algorithms for online Markov\n  decision processes", "comments": "40 pages; additional results in the convex-analytic framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms are designed to perform in non-stationary\nenvironments, but generally there is no notion of a dynamic state to model\nconstraints on current and future actions as a function of past actions.\nState-based models are common in stochastic control settings, but commonly used\nframeworks such as Markov Decision Processes (MDPs) assume a known stationary\nenvironment. In recent years, there has been a growing interest in combining\nthe above two frameworks and considering an MDP setting in which the cost\nfunction is allowed to change arbitrarily after each time step. However, most\nof the work in this area has been algorithmic: given a problem, one would\ndevelop an algorithm almost from scratch. Moreover, the presence of the state\nand the assumption of an arbitrarily varying environment complicate both the\ntheoretical analysis and the development of computationally efficient methods.\nThis paper describes a broad extension of the ideas proposed by Rakhlin et al.\nto give a general framework for deriving algorithms in an MDP setting with\narbitrarily changing costs. This framework leads to a unifying view of existing\nmethods and provides a general procedure for constructing new ones. Several new\nmethods are presented, and one of them is shown to have important advantages\nover a similar method developed from scratch via an online version of\napproximate dynamic programming.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 03:08:48 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 18:14:36 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Guan", "Peng", ""], ["Raginsky", "Maxim", ""], ["Willett", "Rebecca", ""]]}, {"id": "1310.7529", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind\n  Source Separation", "comments": "31 pages, 7 figures, 4 tables. Main changes: new numerical\n  experiments on column-rank-deficient matrices, typos corrected, discussion on\n  the comparison with XRAY", "journal-ref": "SIAM J. on Imaging Sciences 7 (2), pp. 1420-1450, 2014", "doi": "10.1137/130946782", "report-no": null, "categories": "stat.ML cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new fast and robust recursive algorithm for\nnear-separable nonnegative matrix factorization, a particular nonnegative blind\nsource separation problem. This algorithm, which we refer to as the successive\nnonnegative projection algorithm (SNPA), is closely related to the popular\nsuccessive projection algorithm (SPA), but takes advantage of the nonnegativity\nconstraint in the decomposition. We prove that SNPA is more robust than SPA and\ncan be applied to a broader class of nonnegative matrices. This is illustrated\non some synthetic data sets, and on a real-world hyperspectral image.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 18:41:48 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 09:06:07 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 08:47:07 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1310.7637", "submitter": "Salvador Flores M.", "authors": "Salvador Flores and Luis M. Briceno-Arias", "title": "Regularization of $\\ell_1$ minimization for dealing with outliers and\n  noise in Statistics and Signal Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness properties of $\\ell_1$ norm minimization for the\nclassical linear regression problem with a given design matrix and\ncontamination restricted to the dependent variable. We perform a fine error\nanalysis of the $\\ell_1$ estimator for measurements errors consisting of\noutliers coupled with noise. We introduce a new estimation technique resulting\nfrom a regularization of $\\ell_1$ minimization by inf-convolution with the\n$\\ell_2$ norm. Concerning robustness to large outliers, the proposed estimator\nkeeps the breakdown point of the $\\ell_1$ estimator, and reduces to least\nsquares when there are not outliers. We present a globally convergent\nforward-backward algorithm for computing our estimator and some numerical\nexperiments confirming its theoretical properties.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 22:08:22 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 14:51:44 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Flores", "Salvador", ""], ["Briceno-Arias", "Luis M.", ""]]}, {"id": "1310.7679", "submitter": "Ni  Ding Miss", "authors": "Ni Ding, Parastoo Sadeghi and Rodney A. Kennedy", "title": "Structured Optimal Transmission Control in Network-coded Two-way Relay\n  Channels", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a transmission control problem in network-coded two-way\nrelay channels (NC-TWRC), where the relay buffers random symbol arrivals from\ntwo users, and the channels are assumed to be fading. The problem is modeled by\na discounted infinite horizon Markov decision process (MDP). The objective is\nto find a transmission control policy that minimizes the symbol delay, buffer\noverflow and transmission power consumption and error rate simultaneously and\nin the long run. By using the concepts of submodularity, multimodularity and\nL-natural convexity, we study the structure of the optimal policy searched by\ndynamic programming (DP) algorithm. We show that the optimal transmission\npolicy is nondecreasing in queue occupancies or/and channel states under\ncertain conditions such as the chosen values of parameters in the MDP model,\nchannel modeling method, modulation scheme and the preservation of stochastic\ndominance in the transitions of system states. The results derived in this\npaper can be used to relieve the high complexity of DP and facilitate real-time\ncontrol.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 04:17:22 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Ding", "Ni", ""], ["Sadeghi", "Parastoo", ""], ["Kennedy", "Rodney A.", ""]]}, {"id": "1310.7780", "submitter": "Garvesh Raskutti", "authors": "Garvesh Raskutti and Sayan Mukherjee", "title": "The Information Geometry of Mirror Descent", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information geometry applies concepts in differential geometry to probability\nand statistics and is especially useful for parameter estimation in exponential\nfamilies where parameters are known to lie on a Riemannian manifold.\nConnections between the geometric properties of the induced manifold and\nstatistical properties of the estimation problem are well-established. However\ndeveloping first-order methods that scale to larger problems has been less of a\nfocus in the information geometry community. The best known algorithm that\nincorporates manifold structure is the second-order natural gradient descent\nalgorithm introduced by Amari. On the other hand, stochastic approximation\nmethods have led to the development of first-order methods for optimizing noisy\nobjective functions. A recent generalization of the Robbins-Monro algorithm\nknown as mirror descent, developed by Nemirovski and Yudin is a first order\nmethod that induces non-Euclidean geometries. However current analysis of\nmirror descent does not precisely characterize the induced non-Euclidean\ngeometry nor does it consider performance in terms of statistical relative\nefficiency. In this paper, we prove that mirror descent induced by Bregman\ndivergences is equivalent to the natural gradient descent algorithm on the dual\nRiemannian manifold. Using this equivalence, it follows that (1) mirror descent\nis the steepest descent direction along the Riemannian manifold of the\nexponential family; (2) mirror descent with log-likelihood loss applied to\nparameter estimation in exponential families asymptotically achieves the\nclassical Cram\\'er-Rao lower bound and (3) natural gradient descent for\nmanifolds corresponding to exponential families can be implemented as a\nfirst-order method through mirror descent.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 12:21:12 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 20:40:42 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1310.7855", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on and Pablo Monfort", "title": "A comparison of bandwidth selectors for mean shift clustering", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the performance of several automatic bandwidth selectors,\noriginally designed for density gradient estimation, as data-based procedures\nfor nonparametric, modal clustering. The key tool to obtain a clustering from\ndensity gradient estimators is the mean shift algorithm, which allows to obtain\na partition not only of the data sample, but also of the whole space. The\nresults of our simulation study suggest that most of the methods considered\nhere, like cross validation and plug in bandwidth selectors, are useful for\ncluster analysis via the mean shift algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 16:05:44 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""], ["Monfort", "Pablo", ""]]}, {"id": "1310.7868", "submitter": "Karim Pichara Baksai", "authors": "Karim Pichara and Pavlos Protopapas", "title": "Automatic Classification of Variable Stars in Catalogs with missing data", "comments": null, "journal-ref": "2013 ApJ 777 83", "doi": "10.1088/0004-637X/777/2/83", "report-no": null, "categories": "astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automatic classification method for astronomical catalogs with\nmissing data. We use Bayesian networks, a probabilistic graphical model, that\nallows us to perform inference to pre- dict missing values given observed data\nand dependency relationships between variables. To learn a Bayesian network\nfrom incomplete data, we use an iterative algorithm that utilises sampling\nmethods and expectation maximization to estimate the distributions and\nprobabilistic dependencies of variables from data with missing values. To test\nour model we use three catalogs with missing data (SAGE, 2MASS and UBVI) and\none complete catalog (MACHO). We examine how classification accuracy changes\nwhen information from missing data catalogs is included, how our method\ncompares to traditional missing data approaches and at what computational cost.\nIntegrating these catalogs with missing data we find that classification of\nvariable objects improves by few percent and by 15% for quasar detection while\nkeeping the computational cost the same.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 16:37:13 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1310.7991", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth\n  Netrapalli", "title": "Learning Sparsely Used Overcomplete Dictionaries via Alternating\n  Minimization", "comments": "Local linear convergence now holds under RIP and also more general\n  restricted eigenvalue conditions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse coding, where each sample consists of a\nsparse linear combination of a set of dictionary atoms, and the task is to\nlearn both the dictionary elements and the mixing coefficients. Alternating\nminimization is a popular heuristic for sparse coding, where the dictionary and\nthe coefficients are estimated in alternate steps, keeping the other fixed.\nTypically, the coefficients are estimated via $\\ell_1$ minimization, keeping\nthe dictionary fixed, and the dictionary is estimated through least squares,\nkeeping the coefficients fixed. In this paper, we establish local linear\nconvergence for this variant of alternating minimization and establish that the\nbasin of attraction for the global optimum (corresponding to the true\ndictionary and the coefficients) is $\\order{1/s^2}$, where $s$ is the sparsity\nlevel in each sample and the dictionary satisfies RIP. Combined with the recent\nresults of approximate dictionary estimation, this yields provable guarantees\nfor exact recovery of both the dictionary elements and the coefficients, when\nthe dictionary elements are incoherent.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 01:12:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 22:55:12 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Agarwal", "Alekh", ""], ["Anandkumar", "Animashree", ""], ["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1310.7994", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions for Novel Word Detection in\n  Separable Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplicial condition and other stronger conditions that imply it have\nrecently played a central role in developing polynomial time algorithms with\nprovable asymptotic consistency and sample complexity guarantees for topic\nestimation in separable topic models. Of these algorithms, those that rely\nsolely on the simplicial condition are impractical while the practical ones\nneed stronger conditions. In this paper, we demonstrate, for the first time,\nthat the simplicial condition is a fundamental, algorithm-independent,\ninformation-theoretic necessary condition for consistent separable topic\nestimation. Furthermore, under solely the simplicial condition, we present a\npractical quadratic-complexity algorithm based on random projections which\nconsistently detects all novel words of all topics using only up to\nsecond-order empirical word moments. This algorithm is amenable to distributed\nimplementation making it attractive for 'big-data' scenarios involving a\nnetwork of large distributed databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 01:19:26 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Rohban", "Mohammad H.", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1310.8004", "submitter": "Boyu Wang", "authors": "Boyu Wang, Joelle Pineau", "title": "Online Ensemble Learning for Imbalanced Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While both cost-sensitive learning and online learning have been studied\nextensively, the effort in simultaneously dealing with these two issues is\nlimited. Aiming at this challenge task, a novel learning framework is proposed\nin this paper. The key idea is based on the fusion of online ensemble\nalgorithms and the state of the art batch mode cost-sensitive bagging/boosting\nalgorithms. Within this framework, two separately developed research areas are\nbridged together, and a batch of theoretically sound online cost-sensitive\nbagging and online cost-sensitive boosting algorithms are first proposed.\nUnlike other online cost-sensitive learning algorithms lacking theoretical\nanalysis of asymptotic properties, the convergence of the proposed algorithms\nis guaranteed under certain conditions, and the experimental evidence with\nbenchmark data sets also validates the effectiveness and efficiency of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 02:11:48 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Wang", "Boyu", ""], ["Pineau", "Joelle", ""]]}, {"id": "1310.8203", "submitter": "Mathias Fuchs", "authors": "Mathias Fuchs, Roman Hornung, Riccardo De Bin and Anne-Laure\n  Boulesteix", "title": "A U-statistic estimator for the variance of resampling-based error\n  estimators", "comments": "15 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit resampling procedures for error estimation in binary\nclassification in terms of U-statistics. In particular, we exploit the fact\nthat the error rate estimator involving all learning-testing splits is a\nU-statistic. Thus, it has minimal variance among all unbiased estimators and is\nasymptotically normally distributed. Moreover, there is an unbiased estimator\nfor this minimal variance if the total sample size is at least the double\nlearning set size plus two. In this case, we exhibit such an estimator which is\nanother U-statistic. It enjoys, again, various optimality properties and yields\nan asymptotically exact hypothesis test of the equality of error rates when two\nlearning algorithms are compared. Our statements apply to any deterministic\nlearning algorithms under weak non-degeneracy assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 15:44:14 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 16:19:02 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fuchs", "Mathias", ""], ["Hornung", "Roman", ""], ["De Bin", "Riccardo", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "1310.8243", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Leon Bottou, Miroslav Dudik, John Langford", "title": "Para-active learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training examples are not all equally informative. Active learning strategies\nleverage this observation in order to massively reduce the number of examples\nthat need to be labeled. We leverage the same observation to build a generic\nstrategy for parallelizing learning algorithms. This strategy is effective\nbecause the search for informative examples is highly parallelizable and\nbecause we show that its performance does not deteriorate when the sifting\nprocess relies on a slightly outdated model. Parallel active learning is\nparticularly attractive to train nonlinear models with non-linear\nrepresentations because there are few practical parallel learning algorithms\nfor such models. We report preliminary experiments using both kernel SVMs and\nSGD-trained neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 17:49:11 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bottou", "Leon", ""], ["Dudik", "Miroslav", ""], ["Langford", "John", ""]]}, {"id": "1310.8320", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Jun Liu", "title": "Safe and Efficient Screening For Sparse Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening is an effective technique for speeding up the training process of a\nsparse learning model by removing the features that are guaranteed to be\ninactive the process. In this paper, we present a efficient screening technique\nfor sparse support vector machine based on variational inequality. The\ntechnique is both efficient and safe.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 20:56:50 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Zhao", "Zheng", ""], ["Liu", "Jun", ""]]}, {"id": "1310.8499", "submitter": "Karol Gregor", "authors": "Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan\n  Wierstra", "title": "Deep AutoRegressive Networks", "comments": "Appears in Proceedings of the 31st International Conference on\n  Machine Learning (ICML), Beijing, China, 2014", "journal-ref": "Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan\n  Wierstra. Deep AutoRegressive Networks. In Proceedings of the 31st\n  International Conference on Machine Learning (ICML), JMLR: W&CP volume 32,\n  2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep, generative autoencoder capable of learning hierarchies\nof distributed representations from data. Successive deep stochastic hidden\nlayers are equipped with autoregressive connections, which enable the model to\nbe sampled from quickly and exactly via ancestral sampling. We derive an\nefficient approximate parameter estimation method based on the minimum\ndescription length (MDL) principle, which can be seen as maximising a\nvariational lower bound on the log-likelihood, with a feedforward neural\nnetwork implementing approximate inference. We demonstrate state-of-the-art\ngenerative performance on a number of classic data sets: several UCI data sets,\nMNIST and Atari 2600 games.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 13:47:30 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 16:22:43 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Gregor", "Karol", ""], ["Danihelka", "Ivo", ""], ["Mnih", "Andriy", ""], ["Blundell", "Charles", ""], ["Wierstra", "Daan", ""]]}, {"id": "1310.8574", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy, Michael Habeck, Bernhard Sch\\\"olkopf", "title": "Spatial statistics, image analysis and percolation theory", "comments": null, "journal-ref": "The Joint Statistical Meetings Proceedings (2011), Time Series and\n  Network Section, American Statistical Association, pp. 5571 - 5581", "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method for detection of signals and reconstruction of\nimages in the presence of random noise. The method uses results from\npercolation theory. We specifically address the problem of detection of\nmultiple objects of unknown shapes in the case of nonparametric noise. The\nnoise density is unknown and can be heavy-tailed. The objects of interest have\nunknown varying intensities. No boundary shape constraints are imposed on the\nobjects, only a set of weak bulk conditions is required. We view the object\ndetection problem as a multiple hypothesis testing for discrete statistical\ninverse problems. We present an algorithm that allows to detect greyscale\nobjects of various shapes in noisy images. We prove results on consistency and\nalgorithmic complexity of our procedures. Applications to cryo-electron\nmicroscopy are presented.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 16:26:09 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Habeck", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1310.8612", "submitter": "Jie Chen", "authors": "Jie Chen and C\\'edric Richard and Alfred O. Hero III", "title": "Nonlinear unmixing of hyperspectral images using a semiparametric model\n  and spatial regularization", "comments": "5 pages, 1 figure, submitted to ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating spatial information into hyperspectral unmixing procedures has\nbeen shown to have positive effects, due to the inherent spatial-spectral\nduality in hyperspectral scenes. Current research works that consider spatial\ninformation are mainly focused on the linear mixing model. In this paper, we\ninvestigate a variational approach to incorporating spatial correlation into a\nnonlinear unmixing procedure. A nonlinear algorithm operating in reproducing\nkernel Hilbert spaces, associated with an $\\ell_1$ local variation norm as the\nspatial regularizer, is derived. Experimental results, with both synthetic and\nreal data, illustrate the effectiveness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 17:40:20 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1310.8618", "submitter": "Jie Chen", "authors": "Jie Chen and Wei Gao and C\\'edric Richard and Jose-Carlos M. Bermudez", "title": "Convergence analysis of kernel LMS algorithm with pre-tuned dictionary", "comments": "5 pages, 2 figures, submitted to ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel least-mean-square (KLMS) algorithm is an appealing tool for online\nidentification of nonlinear systems due to its simplicity and robustness. In\naddition to choosing a reproducing kernel and setting filter parameters,\ndesigning a KLMS adaptive filter requires to select a so-called dictionary in\norder to get a finite-order model. This dictionary has a significant impact on\nperformance, and requires careful consideration. Theoretical analysis of KLMS\nas a function of dictionary setting has rarely, if ever, been addressed in the\nliterature. In an analysis previously published by the authors, the dictionary\nelements were assumed to be governed by the same probability density function\nof the input data. In this paper, we modify this study by considering the\ndictionary as part of the filter parameters to be set. This theoretical\nanalysis paves the way for future investigations on KLMS dictionary design.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 17:53:06 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Chen", "Jie", ""], ["Gao", "Wei", ""], ["Richard", "C\u00e9dric", ""], ["Bermudez", "Jose-Carlos M.", ""]]}]