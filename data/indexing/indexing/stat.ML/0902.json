[{"id": "0902.0392", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Tree Exploration for Bayesian RL Exploration", "comments": "13 pages, 1 figure. Slightly extended and corrected version (notation\n  errors and lower bound calculation) of homonymous paper presented at the\n  conference of Computational Intelligence for Modelling, Control and\n  Automation 2008 (CIMCA'08)", "journal-ref": null, "doi": null, "report-no": "IAS-08-04", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in reinforcement learning has produced algorithms for optimal\ndecision making under uncertainty that fall within two main types. The first\nemploys a Bayesian framework, where optimality improves with increased\ncomputational time. This is because the resulting planning task takes the form\nof a dynamic programming problem on a belief tree with an infinite number of\nstates. The second type employs relatively simple algorithm which are shown to\nsuffer small regret within a distribution-free framework. This paper presents a\nlower bound and a high probability upper bound on the optimal value function\nfor the nodes in the Bayesian belief tree, which are analogous to similar\nbounds in POMDPs. The bounds are then used to create more efficient strategies\nfor exploring the tree. The resulting algorithms are compared with the\ndistribution-free algorithm UCB1, as well as a simpler baseline algorithm on\nmulti-armed bandit problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2009 22:37:23 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2011 08:13:36 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "0902.0600", "submitter": "Nicolas Brodu", "authors": "Nicolas Brodu", "title": "Reconstruction of Epsilon-Machines in Predictive Frameworks and\n  Decisional States", "comments": "Minor revision, final version. Free/libre code at\n  http://nicolas.brodu.numerimoire.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces both a new algorithm for reconstructing\nepsilon-machines from data, as well as the decisional states. These are defined\nas the internal states of a system that lead to the same decision, based on a\nuser-provided utility or pay-off function. The utility function encodes some a\npriori knowledge external to the system, it quantifies how bad it is to make\nmistakes. The intrinsic underlying structure of the system is modeled by an\nepsilon-machine and its causal states. The decisional states form a partition\nof the lower-level causal states that is defined according to the higher-level\nuser's knowledge. In a complex systems perspective, the decisional states are\nthus the \"emerging\" patterns corresponding to the utility function. The\ntransitions between these decisional states correspond to events that lead to a\nchange of decision. The new REMAPF algorithm estimates both the epsilon-machine\nand the decisional states from data. Application examples are given for hidden\nmodel reconstruction, cellular automata filtering, and edge detection in\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2009 20:48:24 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2009 09:12:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2010 09:32:37 GMT"}, {"version": "v4", "created": "Fri, 21 Jan 2011 11:13:41 GMT"}, {"version": "v5", "created": "Mon, 6 Jun 2011 16:22:31 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Brodu", "Nicolas", ""]]}, {"id": "0902.1323", "submitter": "Giovanni Montana", "authors": "Brian McWilliams, Giovanni Montana", "title": "Sparse partial least squares for on-line variable selection in\n  multivariate data streams", "comments": "26 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a computationally efficient algorithm for on-line\nvariable selection in multivariate regression problems involving high\ndimensional data streams. The algorithm recursively extracts all the latent\nfactors of a partial least squares solution and selects the most important\nvariables for each factor. This is achieved by means of only one sparse\nsingular value decomposition which can be efficiently updated on-line and in an\nadaptive fashion. Simulation results based on artificial data streams\ndemonstrate that the algorithm is able to select important variables in dynamic\nsettings where the correlation structure among the observed streams is governed\nby a few hidden components and the importance of each variable changes over\ntime. We also report on an application of our algorithm to a multivariate\nversion of the \"enhanced index tracking\" problem using financial data streams.\nThe application consists of performing on-line asset allocation with the\nobjective of overperforming two benchmark indices simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2009 17:41:11 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["McWilliams", "Brian", ""], ["Montana", "Giovanni", ""]]}, {"id": "0902.1733", "submitter": "Jean-Yves Audibert", "authors": "Jean-Yves Audibert (Imagine, INRIA Rocquencourt), Olivier Catoni (DMA)", "title": "Risk bounds in linear regression through PAC-Bayesian truncation", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting as well as the best linear combination\nof d given functions in least squares regression, and variants of this problem\nincluding constraints on the parameters of the linear combination. When the\ninput distribution is known, there already exists an algorithm having an\nexpected excess risk of order d/n, where n is the size of the training data.\nWithout this strong assumption, standard results often contain a multiplicative\nlog n factor, and require some additional assumptions like uniform boundedness\nof the d-dimensional input representation and exponential moments of the\noutput. This work provides new risk bounds for the ridge estimator and the\nordinary least squares estimator, and their variants. It also provides\nshrinkage procedures with convergence rate d/n (i.e., without the logarithmic\nfactor) in expectation and in deviations, under various assumptions. The key\ncommon surprising factor of these results is the absence of exponential moment\ncondition on the output distribution while achieving exponential deviations.\nAll risk bounds are obtained through a PAC-Bayesian analysis on truncated\ndifferences of losses. Finally, we show that some of these results are not\nparticular to the least squares loss, but can be generalized to similar\nstrongly convex loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2009 20:30:55 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2010 19:50:40 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Audibert", "Jean-Yves", "", "Imagine, INRIA Rocquencourt"], ["Catoni", "Olivier", "", "DMA"]]}, {"id": "0902.1970", "submitter": "Mohamed Hebiri", "authors": "Mohamed Hebiri (PMA)", "title": "Sparse Conformal Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal predictors, introduced by Vovk et al. (2005), serve to build\nprediction intervals by exploiting a notion of conformity of the new data point\nwith previously observed data. In the present paper, we propose a novel method\nfor constructing prediction intervals for the response variable in multivariate\nlinear models. The main emphasis is on sparse linear models, where only few of\nthe covariates have significant influence on the response variable even if\ntheir number is very large. Our approach is based on combining the principle of\nconformal prediction with the $\\ell_1$ penalized least squares estimator\n(LASSO). The resulting confidence set depends on a parameter $\\epsilon>0$ and\nhas a coverage probability larger than or equal to $1-\\epsilon$. The numerical\nexperiments reported in the paper show that the length of the confidence set is\nsmall. Furthermore, as a by-product of the proposed approach, we provide a\ndata-driven procedure for choosing the LASSO penalty. The selection power of\nthe method is illustrated on simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2009 19:29:30 GMT"}], "update_date": "2009-02-12", "authors_parsed": [["Hebiri", "Mohamed", "", "PMA"]]}, {"id": "0902.2808", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh, Michael Spagat and Jorge A. Restrepo", "title": "Ultrametric Wavelet Regression of Multivariate Time Series: Application\n  to Colombian Conflict Analysis", "comments": "36 pages, 13 figures", "journal-ref": "IEEE Transactions on Systems, Man, and Cybernetics - Part A,\n  Systems and Humans, 2011", "doi": "10.1109/TSMCA.2010.2064301", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first pursue the study of how hierarchy provides a well-adapted tool for\nthe analysis of change. Then, using a time sequence-constrained hierarchical\nclustering, we develop the practical aspects of a new approach to wavelet\nregression. This provides a new way to link hierarchical relationships in a\nmultivariate time series data set with external signals. Violence data from the\nColombian conflict in the years 1990 to 2004 is used throughout. We conclude\nwith some proposals for further study on the relationship between social\nviolence and market forces, viz. between the Colombian conflict and the US\nnarcotics market.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2009 23:59:47 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Murtagh", "Fionn", ""], ["Spagat", "Michael", ""], ["Restrepo", "Jorge A.", ""]]}, {"id": "0902.3130", "submitter": "Servane Gey", "authors": "Servane Gey (MAP5)", "title": "Risk Bounds for CART Classifiers under a Margin Condition", "comments": null, "journal-ref": "Pattern Recognition 45 (2012) 3523-3534", "doi": "10.1016/j.patcog.2012.02.021", "report-no": "MAP5 2009-04", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk bounds for Classification and Regression Trees (CART, Breiman et. al.\n1984) classifiers are obtained under a margin condition in the binary\nsupervised classification framework. These risk bounds are obtained\nconditionally on the construction of the maximal deep binary tree and permit to\nprove that the linear penalty used in the CART pruning algorithm is valid under\na margin condition. It is also shown that, conditionally on the construction of\nthe maximal tree, the final selection by test sample does not alter\ndramatically the estimation accuracy of the Bayes classifier. In the two-class\nclassification framework, the risk bounds that are proved, obtained by using\npenalized model selection, validate the CART algorithm which is used in many\ndata mining applications such as Biology, Medicine or Image Coding.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 13:20:10 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2009 14:34:13 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2009 19:29:08 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2010 14:38:40 GMT"}, {"version": "v5", "created": "Thu, 1 Mar 2012 14:11:49 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Gey", "Servane", "", "MAP5"]]}, {"id": "0902.3347", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer, Masashi Sugiyama, Mikio Braun", "title": "Lanczos Approximations for the Speedup of Kernel Partial Least Squares\n  Regression", "comments": "to appear in Proceedings of the 12th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 09)", "journal-ref": "JMLR Workshop and Conference Proceedings 5 (AISTATS 2009), p\n  288-295, 2009", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is\nquadratic in the number of examples. However, the necessity of obtaining\nsensitivity measures as degrees of freedom for model selection or confidence\nintervals for more detailed analysis requires cubic runtime, and thus\nconstitutes a computational bottleneck in real-world data analysis. We propose\na novel algorithm for KPLS which not only computes (a) the fit, but also (b)\nits approximate degrees of freedom and (c) error bars in quadratic runtime. The\nalgorithm exploits a close connection between Kernel PLS and the Lanczos\nalgorithm for approximating the eigenvalues of symmetric matrices, and uses\nthis approximation to compute the trace of powers of the kernel matrix in\nquadratic runtime.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 11:28:20 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Kraemer", "Nicole", ""], ["Sugiyama", "Masashi", ""], ["Braun", "Mikio", ""]]}, {"id": "0902.3453", "submitter": "Samory Kpotufe", "authors": "Samory Kpotufe", "title": "Escaping the curse of dimensionality with a tree-based regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first tree-based regressor whose convergence rate depends only\non the intrinsic dimension of the data, namely its Assouad dimension. The\nregressor uses the RPtree partitioning procedure, a simple randomized variant\nof k-d trees.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 20:50:53 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Kpotufe", "Samory", ""]]}, {"id": "0902.3526", "submitter": "Gilles Stoltz", "authors": "Gabor Lugosi, Omiros Papaspiliopoulos, Gilles Stoltz (DMA, GREGH)", "title": "Online Multi-task Learning with Hard Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss multi-task online learning when a decision maker has to deal\nsimultaneously with M tasks. The tasks are related, which is modeled by\nimposing that the M-tuple of actions taken by the decision maker needs to\nsatisfy certain constraints. We give natural examples of such restrictions and\nthen discuss a general class of tractable constraints, for which we introduce\ncomputationally efficient ways of selecting actions, essentially by reducing to\nan on-line shortest path problem. We briefly discuss \"tracking\" and \"bandit\"\nversions of the problem and extend the model in various ways, including\nnon-additive global losses and uncountably infinite sets of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 07:39:13 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2009 14:50:53 GMT"}], "update_date": "2009-03-27", "authors_parsed": [["Lugosi", "Gabor", "", "DMA, GREGH"], ["Papaspiliopoulos", "Omiros", "", "DMA, GREGH"], ["Stoltz", "Gilles", "", "DMA, GREGH"]]}, {"id": "0902.3619", "submitter": "Antonio Galves", "authors": "Antonio Galves, Charlotte Galves, Jes\\'us E. Garc\\'ia, Nancy L.\n  Garcia, Florencia Leonardi", "title": "Context tree selection and linguistic rhythm retrieval from written\n  texts", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS511 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 186-209", "doi": "10.1214/11-AOAS511", "report-no": "IMS-AOAS-AOAS511", "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The starting point of this article is the question \"How to retrieve\nfingerprints of rhythm in written texts?\" We address this problem in the case\nof Brazilian and European Portuguese. These two dialects of Modern Portuguese\nshare the same lexicon and most of the sentences they produce are superficially\nidentical. Yet they are conjectured, on linguistic grounds, to implement\ndifferent rhythms. We show that this linguistic question can be formulated as a\nproblem of model selection in the class of variable length Markov chains. To\ncarry on this approach, we compare texts from European and Brazilian\nPortuguese. These texts are previously encoded according to some basic rhythmic\nfeatures of the sentences which can be automatically retrieved. This is an\nentirely new approach from the linguistic point of view. Our statistical\ncontribution is the introduction of the smallest maximizer criterion which is a\nconstant free procedure for model selection. As a by-product, this provides a\nsolution for the problem of optimal choice of the penalty constant when using\nthe BIC to select a variable length Markov chain. Besides proving the\nconsistency of the smallest maximizer criterion when the sample size diverges,\nwe also make a simulation study comparing our approach with both the standard\nBIC selection and the Peres-Shields order estimation. Applied to the linguistic\nsample constituted for our case study, the smallest maximizer criterion assigns\ndifferent context-tree models to the two dialects of Portuguese. The features\nof the selected models are compatible with current conjectures discussed in the\nlinguistic literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 16:45:58 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2009 15:57:51 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2011 13:05:08 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2012 09:08:43 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Galves", "Antonio", ""], ["Galves", "Charlotte", ""], ["Garc\u00eda", "Jes\u00fas E.", ""], ["Garcia", "Nancy L.", ""], ["Leonardi", "Florencia", ""]]}, {"id": "0902.4380", "submitter": "Nicole Kraemer", "authors": "Gilles Blanchard, Nicole Kraemer", "title": "Kernel Partial Least Squares is Universally Consistent", "comments": "18 pages, no figures", "journal-ref": "JMLR Workshop and Conference Proceedings 9 (AISTATS 2010) 57-64,\n  2010", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the statistical consistency of kernel Partial Least Squares\nRegression applied to a bounded regression learning problem on a reproducing\nkernel Hilbert space. Partial Least Squares stands out of well-known classical\napproaches as e.g. Ridge Regression or Principal Components Regression, as it\nis not defined as the solution of a global cost minimization procedure over a\nfixed model nor is it a linear estimator. Instead, approximate solutions are\nconstructed by projections onto a nested set of data-dependent subspaces. To\nprove consistency, we exploit the known fact that Partial Least Squares is\nequivalent to the conjugate gradient algorithm in combination with early\nstopping. The choice of the stopping rule (number of iterations) is a crucial\npoint. We study two empirical stopping rules. The first one monitors the\nestimation error in each iteration step of Partial Least Squares, and the\nsecond one estimates the empirical complexity in terms of a condition number.\nBoth stopping rules lead to universally consistent estimators provided the\nkernel is universal.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2009 14:25:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2010 17:32:09 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kraemer", "Nicole", ""]]}, {"id": "0902.4389", "submitter": "Alexander G. Ramm", "authors": "A.G.Ramm", "title": "Dimension reduction in representation of the data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose the data consist of a set $S$ of points $x_j$, $1\\leq j \\leq J$,\ndistributed in a bounded domain $D\\subset R^N$, where $N$ is a large number. An\nalgorithm is given for finding the sets $L_k$ of dimension $k\\ll N$,\n$k=1,2,...K$, in a neighborhood of which maximal amount of points $x_j\\in S$\nlie. The algorithm is different from PCA (principal component analysis)\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2009 15:14:31 GMT"}], "update_date": "2009-02-26", "authors_parsed": [["Ramm", "A. G.", ""]]}]