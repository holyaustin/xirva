[{"id": "1303.0073", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "A Method for Comparing Hedge Funds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system to\nidentify behavioral similarities among time-series representing monthly returns\nof 11,312 hedge funds operated during approximately one decade (2000 - 2010).\nThe presented approach of cross-category and cross-location classification\nassists the investor to identify alternative investments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:38:35 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:20:08 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0076", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Bio-Signals-based Situation Comparison Approach to Predict Pain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a time-series-based classification approach to identify\nsimilarities between bio-medical-based situations. The proposed approach allows\nclassifying collections of time-series representing bio-medical measurements,\ni.e., situations, regardless of the type, the length and the quantity of the\ntime-series a situation comprised of.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:49:11 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:19:06 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0140", "submitter": "Edward Moroshko", "authors": "Nina Vaits, Edward Moroshko, Koby Crammer", "title": "Second-Order Non-Stationary Online Learning for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a learner, in standard online learning, is to have the cumulative\nloss not much larger compared with the best-performing function from some fixed\nclass. Numerous algorithms were shown to have this gap arbitrarily close to\nzero, compared with the best function that is chosen off-line. Nevertheless,\nmany real-world applications, such as adaptive filtering, are non-stationary in\nnature, and the best prediction function may drift over time. We introduce two\nnovel algorithms for online regression, designed to work well in non-stationary\nenvironment. Our first algorithm performs adaptive resets to forget the\nhistory, while the second is last-step min-max optimal in context of a drift.\nWe analyze both algorithms in the worst-case regret framework and show that\nthey maintain an average loss close to that of the best slowly changing\nsequence of linear functions, as long as the cumulative drift is sublinear. In\naddition, in the stationary case, when no drift occurs, our algorithms suffer\nlogarithmic regret, as for previous algorithms. Our bounds improve over the\nexisting ones, and simulations demonstrate the usefulness of these algorithms\ncompared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 10:50:46 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Vaits", "Nina", ""], ["Moroshko", "Edward", ""], ["Crammer", "Koby", ""]]}, {"id": "1303.0166", "submitter": "Stefan Harmeling", "authors": "Stefan Harmeling, Michael Hirsch, Bernhard Sch\\\"olkopf", "title": "On a link between kernel mean maps and Fraunhofer diffraction, with an\n  application to super-resolution beyond the diffraction limit", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), Portland, 2013", "journal-ref": null, "doi": "10.1109/CVPR.2013.144", "report-no": null, "categories": "physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a link between Fourier optics and a recent construction from the\nmachine learning community termed the kernel mean map. Using the Fraunhofer\napproximation, it identifies the kernel with the squared Fourier transform of\nthe aperture. This allows us to use results about the invertibility of the\nkernel mean map to provide a statement about the invertibility of Fraunhofer\ndiffraction, showing that imaging processes with arbitrarily small apertures\ncan in principle be invertible, i.e., do not lose information, provided the\nobjects to be imaged satisfy a generic condition. A real world experiment shows\nthat we can super-resolve beyond the Rayleigh limit.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 13:41:53 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Harmeling", "Stefan", ""], ["Hirsch", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1303.0268", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Johannes Rauh, Nihat Ay", "title": "Maximal Information Divergence from Statistical Models defined by Neural\n  Networks", "comments": "8 pages, 1 figure", "journal-ref": "Geometric science of information : first international conference,\n  GSI 2013, Paris, France, August 28-30, 2013. Proceedings / F. Nielsen...\n  (eds.). Springer, 2013. - P. 759-766", "doi": "10.1007/978-3-642-40020-9_85", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review recent results about the maximal values of the Kullback-Leibler\ninformation divergence from statistical models defined by neural networks,\nincluding naive Bayes models, restricted Boltzmann machines, deep belief\nnetworks, and various classes of exponential families. We illustrate approaches\nto compute the maximal divergence from a given model starting from simple sub-\nor super-models. We give a new result for deep and narrow belief networks with\nfinite-valued units.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 20:21:32 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""], ["Ay", "Nihat", ""]]}, {"id": "1303.0283", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Inverse Signal Classification for Financial Instruments", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.0073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system using a\ncollection of 7,881 financial instruments traded during 2011 to identify\ninverse behavior among the time-series.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:45:42 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:17:56 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0309", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet and Bernhard Sch\\\"olkopf", "title": "One-Class Support Measure Machines for Group Anomaly Detection", "comments": "Conference on Uncertainty in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose one-class support measure machines (OCSMMs) for group anomaly\ndetection which aims at recognizing anomalous aggregate behaviors of data\npoints. The OCSMMs generalize well-known one-class support vector machines\n(OCSVMs) to a space of probability measures. By formulating the problem as\nquantile estimation on distributions, we can establish an interesting\nconnection to the OCSVMs and variable kernel density estimators (VKDEs) over\nthe input space on which the distributions are defined, bridging the gap\nbetween large-margin methods and kernel density estimators. In particular, we\nshow that various types of VKDEs can be considered as solutions to a class of\nregularization problems studied in this paper. Experiments on Sloan Digital Sky\nSurvey dataset and High Energy Particle Physics dataset demonstrate the\nbenefits of the proposed framework in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 21:50:09 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 13:42:46 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Muandet", "Krikamol", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1303.0341", "submitter": "Wen-Xin Zhou", "authors": "T. Tony Cai, Wen-Xin Zhou", "title": "Matrix Completion via Max-Norm Constrained Optimization", "comments": "33 pages", "journal-ref": null, "doi": "10.1214/16-EJS1147", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has been well studied under the uniform sampling model and\nthe trace-norm regularized methods perform well both theoretically and\nnumerically in such a setting. However, the uniform sampling model is\nunrealistic for a range of applications and the standard trace-norm relaxation\ncan behave very poorly when the underlying sampling scheme is non-uniform.\n  In this paper we propose and analyze a max-norm constrained empirical risk\nminimization method for noisy matrix completion under a general sampling model.\nThe optimal rate of convergence is established under the Frobenius norm loss in\nthe context of approximately low-rank matrix reconstruction. It is shown that\nthe max-norm constrained method is minimax rate-optimal and yields a unified\nand robust approximate recovery guarantee, with respect to the sampling\ndistributions. The computational effectiveness of this method is also\ndiscussed, based on first-order algorithms for solving convex optimizations\ninvolving max-norm regularization.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 03:22:37 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 06:05:27 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 02:03:30 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1303.0417", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury", "title": "On the convergence of the IRLS algorithm in Non-Local Patch Regression", "comments": null, "journal-ref": "IEEE Signal Processing Letters, vol. 20(8), 815 - 818, 2013", "doi": "10.1109/LSP.2013.2268248", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it was demonstrated in [CS2012,CS2013] that the robustness of the\nclassical Non-Local Means (NLM) algorithm [BCM2005] can be improved by\nincorporating $\\ell^p (0 < p \\leq 2)$ regression into the NLM framework. This\ngeneral optimization framework, called Non-Local Patch Regression (NLPR),\ncontains NLM as a special case. Denoising results on synthetic and natural\nimages show that NLPR consistently performs better than NLM beyond a moderate\nnoise level, and significantly so when $p$ is close to zero. An iteratively\nreweighted least-squares (IRLS) algorithm was proposed for solving the\nregression problem in NLPR, where the NLM output was used to initialize the\niterations. Based on exhaustive numerical experiments, we observe that the IRLS\nalgorithm is globally convergent (for arbitrary initialization) in the convex\nregime $1 \\leq p \\leq 2$, and locally convergent (fails very rarely using NLM\ninitialization) in the non-convex regime $0 < p < 1$. In this letter, we adapt\nthe \"majorize-minimize\" framework introduced in [Voss1980] to explain these\nobservations.\n  [CS2012] Chaudhury et al. (2012), \"Non-local Euclidean medians,\" IEEE Signal\nProcessing Letters.\n  [CS2013] Chaudhury et al. (2013), \"Non-local patch regression: Robust image\ndenoising in patch space,\" IEEE ICASSP.\n  [BCM2005] Buades et al. (2005), \"A review of image denoising algorithms, with\na new one,\" Multiscale Modeling and Simulation.\n  [Voss1980] Voss et al. (1980), \"Linear convergence of generalized Weiszfeld's\nmethod,\" Computing.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 19:06:01 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 12:06:30 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Chaudhury", "Kunal N.", ""]]}, {"id": "1303.0448", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy and Andreas\n  Spanias", "title": "Learning Stable Multilevel Dictionaries for Sparse Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations using learned dictionaries are being increasingly used\nwith success in several data processing and machine learning applications. The\navailability of abundant training data necessitates the development of\nefficient, robust and provably good dictionary learning algorithms. Algorithmic\nstability and generalization are desirable characteristics for dictionary\nlearning algorithms that aim to build global dictionaries which can efficiently\nmodel any test data similar to the training samples. In this paper, we propose\nan algorithm to learn dictionaries for sparse representations from large scale\ndata, and prove that the proposed learning algorithm is stable and\ngeneralizable asymptotically. The algorithm employs a 1-D subspace clustering\nprocedure, the K-hyperline clustering, in order to learn a hierarchical\ndictionary with multiple levels. We also propose an information-theoretic\nscheme to estimate the number of atoms needed in each level of learning and\ndevelop an ensemble approach to learn robust dictionaries. Using the proposed\ndictionaries, the sparse code for novel test data can be computed using a\nlow-complexity pursuit procedure. We demonstrate the stability and\ngeneralization characteristics of the proposed algorithm using simulations. We\nalso evaluate the utility of the multilevel dictionaries in compressed recovery\nand subspace learning applications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:49:56 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 19:21:54 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1303.0551", "submitter": "Dimitris S. Papailiopoulos", "authors": "Dimitris S. Papailiopoulos, Alexandros G. Dimakis, and Stavros\n  Korokythakis", "title": "Sparse PCA through Low-rank Approximations", "comments": "Long version of the ICML 2013 paper:\n  http://jmlr.org/proceedings/papers/v28/papailiopoulos13.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm that computes the $k$-sparse principal\ncomponent of a positive semidefinite matrix $A$. Our algorithm is combinatorial\nand operates by examining a discrete set of special vectors lying in a\nlow-dimensional eigen-subspace of $A$. We obtain provable approximation\nguarantees that depend on the spectral decay profile of the matrix: the faster\nthe eigenvalue decay, the better the quality of our approximation. For example,\nif the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time\napproximation algorithm for any desired accuracy.\n  A key algorithmic component of our scheme is a combinatorial feature\nelimination step that is provably safe and in practice significantly reduces\nthe running complexity of our algorithm. We implement our algorithm and test it\non multiple artificial and real data sets. Due to the feature elimination step,\nit is possible to perform sparse PCA on data sets consisting of millions of\nentries in a few minutes. Our experimental evaluation shows that our scheme is\nnearly optimal while finding very sparse vectors. We compare to the prior state\nof the art and show that our scheme matches or outperforms previous algorithms\nin all tested data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 19:08:55 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 00:30:12 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Papailiopoulos", "Dimitris S.", ""], ["Dimakis", "Alexandros G.", ""], ["Korokythakis", "Stavros", ""]]}, {"id": "1303.0561", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Top-down particle filtering for Bayesian decision trees", "comments": "ICML 2013", "journal-ref": "JMLR W&CP 28(3):280-288, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree learning is a popular approach for classification and\nregression in machine learning and statistics, and Bayesian\nformulations---which introduce a prior distribution over decision trees, and\nformulate learning as posterior inference given data---have been shown to\nproduce competitive performance. Unlike classic decision tree learning\nalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existing\nBayesian algorithms produce an approximation to the posterior distribution by\nevolving a complete tree (or collection thereof) iteratively via local Monte\nCarlo modifications to the structure of the tree, e.g., using Markov chain\nMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that\ninstead works in a top-down manner, mimicking the behavior and speed of classic\nalgorithms. We demonstrate empirically that our approach delivers accuracy\ncomparable to the most popular MCMC method, but operates more than an order of\nmagnitude faster, and thus represents a better computation-accuracy tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 20:36:44 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 23:10:00 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1303.0632", "submitter": "Yangbo He", "authors": "Yangbo He, Jinzhu Jia, Bin Yu", "title": "Supplement to \"Reversible MCMC on Markov equivalence classes of sparse\n  directed acyclic graphs\"", "comments": "14 papges", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This supplementary material includes three parts: some preliminary results,\nfour examples, an experiment, three new algorithms, and all proofs of the\nresults in the paper \"Reversible MCMC on Markov equivalence classes of sparse\ndirected acyclic graphs\".\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 07:59:11 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2013 01:46:02 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["He", "Yangbo", ""], ["Jia", "Jinzhu", ""], ["Yu", "Bin", ""]]}, {"id": "1303.0642", "submitter": "Rajarshi Guhaniyogi", "authors": "Rajarshi Guhaniyogi and David B. Dunson", "title": "Bayesian Compressed Regression", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to variable selection or shrinkage in high dimensional\nregression, we propose to randomly compress the predictors prior to analysis.\nThis dramatically reduces storage and computational bottlenecks, performing\nwell when the predictors can be projected to a low dimensional linear subspace\nwith minimal loss of information about the response. As opposed to existing\nBayesian dimensionality reduction approaches, the exact posterior distribution\nconditional on the compressed data is available analytically, speeding up\ncomputation by many orders of magnitude while also bypassing robustness issues\ndue to convergence and mixing problems with MCMC. Model averaging is used to\nreduce sensitivity to the random projection matrix, while accommodating\nuncertainty in the subspace dimension. Strong theoretical support is provided\nfor the approach by showing near parametric convergence rates for the\npredictive density in the large p small n asymptotic paradigm. Practical\nperformance relative to competitors is illustrated in simulations and real data\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 08:39:34 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2013 20:15:31 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1303.0663", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang and Ji Wu", "title": "Denoising Deep Neural Networks Based Voice Activity Detection", "comments": "This paper has been accepted by IEEE ICASSP-2013, and will be\n  published online after May, 2013", "journal-ref": null, "doi": "10.1109/ICASSP.2013.6637769", "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)\nhas been proposed. It is powerful in fusing the advantages of multiple\nfeatures, and achieves the state-of-the-art performance. However, the deep\nlayers of the DBN-based VAD do not show an apparent superiority to the\nshallower layers. In this paper, we propose a denoising-deep-neural-network\n(DDNN) based VAD to address the aforementioned problem. Specifically, we\npre-train a deep neural network in a special unsupervised denoising greedy\nlayer-wise mode, and then fine-tune the whole network in a supervised way by\nthe common back-propagation algorithm. In the pre-training phase, we take the\nnoisy speech signals as the visible layer and try to extract a new feature that\nminimizes the reconstruction cross-entropy loss between the noisy speech\nsignals and its corresponding clean speech signals. Experimental results show\nthat the proposed DDNN-based VAD not only outperforms the DBN-based VAD but\nalso shows an apparent performance improvement of the deep layers over\nshallower layers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:17:49 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Zhang", "Xiao-Lei", ""], ["Wu", "Ji", ""]]}, {"id": "1303.0665", "submitter": "Florent Garcin", "authors": "Florent Garcin, Christos Dimitrakakis and Boi Faltings", "title": "Personalized News Recommendation with Context Trees", "comments": null, "journal-ref": "Proceedings of the 7th ACM conference on Recommender systems\n  (2013), pp. 105--112", "doi": "10.1145/2507157.2507166", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The profusion of online news articles makes it difficult to find interesting\narticles, a problem that can be assuaged by using a recommender system to bring\nthe most relevant news stories to readers. However, news recommendation is\nchallenging because the most relevant articles are often new content seen by\nfew users. In addition, they are subject to trends and preference changes over\ntime, and in many cases we do not have sufficient information to profile the\nreader.\n  In this paper, we introduce a class of news recommendation systems based on\ncontext trees. They can provide high-quality news recommendation to anonymous\nvisitors based on present browsing behaviour. We show that context-tree\nrecommender systems provide good prediction accuracy and recommendation\nnovelty, and they are sufficiently flexible to capture the unique properties of\nnews articles.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:19:43 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Garcin", "Florent", ""], ["Dimitrakakis", "Christos", ""], ["Faltings", "Boi", ""]]}, {"id": "1303.0691", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Learning AMP Chain Graphs and some Marginal Models Thereof under\n  Faithfulness: Extended Version", "comments": "Changes from v1 to v2: The interpretation of the antecedent of the\n  rule R3 changed, which in turn implied modifying Lemma 6 and Theorem 1.\n  Changes from v2 to v3: Minor improvements in the first 12 pages. A shorter\n  version is to appear in International Journal of Approximate Reasoning, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)\ninterpretation. In particular, we present a constraint based algorithm for\nlearning an AMP chain graph a given probability distribution is faithful to.\nMoreover, we show that the extension of Meek's conjecture to AMP chain graphs\ndoes not hold, which compromises the development of efficient and correct\nscore+search learning algorithms under assumptions weaker than faithfulness.\n  We also introduce a new family of graphical models that consists of\nundirected and bidirected edges. We name this new family maximal\ncovariance-concentration graphs (MCCGs) because it includes both covariance and\nconcentration graphs as subfamilies. However, every MCCG can be seen as the\nresult of marginalizing out some nodes in an AMP CG. We describe global, local\nand pairwise Markov properties for MCCGs and prove their equivalence. We\ncharacterize when two MCCGs are Markov equivalent, and show that every Markov\nequivalence class of MCCGs has a distinguished member. We present a constraint\nbased algorithm for learning a MCCG a given probability distribution is\nfaithful to.\n  Finally, we present a graphical criterion for reading dependencies from a\nMCCG of a probability distribution that satisfies the graphoid properties, weak\ntransitivity and composition. We prove that the criterion is sound and complete\nin certain sense.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 13:02:49 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 11:55:54 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2014 13:07:42 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1303.0727", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Estimating a sharp convergence bound for randomized ensembles", "comments": "This paper extends the earlier work, \"The Convergence Rate of\n  Majority Vote under Exchangeability\" from 2013, as well as \"A Sharp Bound on\n  the Computation-Accuracy Tradeoff for Majority Voting Ensembles\" from 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.SI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When randomized ensembles such as bagging or random forests are used for\nbinary classification, the prediction error of the ensemble tends to decrease\nand stabilize as the number of classifiers increases. However, the precise\nrelationship between prediction error and ensemble size is unknown in practice.\nIn the standard case when classifiers are aggregated by majority vote, the\npresent work offers a way to quantify this convergence in terms of \"algorithmic\nvariance,\" i.e. the variance of prediction error due only to the randomized\ntraining algorithm. Specifically, we study a theoretical upper bound on this\nvariance, and show that it is sharp --- in the sense that it is attained by a\nspecific family of randomized classifiers. Next, we address the problem of\nestimating the unknown value of the bound, which leads to a unique twist on the\nclassical problem of non-parametric density estimation. In particular, we\ndevelop an estimator for the bound and show that its MSE matches optimal\nnon-parametric rates under certain conditions. (Concurrent with this work, some\nclosely related results have also been considered in Cannings and Samworth\n(2017) and Lopes (2019).)\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:16:00 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 10:34:41 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 08:23:09 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1303.0742", "submitter": "Yoann Isaac", "authors": "Quentin Barth\\'elemy, C\\'edric Gouy-Pailler, Yoann Isaac, Antoine\n  Souloumiac, Anthony Larue, J\\'er\\^ome I. Mars", "title": "Multivariate Temporal Dictionary Learning for EEG", "comments": null, "journal-ref": "Published in Journal of Neuroscience Methods, vol. 215, pp. 19-28,\n  2013", "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the issue of representing electroencephalographic\n(EEG) signals in an efficient way. While classical approaches use a fixed Gabor\ndictionary to analyze EEG signals, this article proposes a data-driven method\nto obtain an adapted dictionary. To reach an efficient dictionary learning,\nappropriate spatial and temporal modeling is required. Inter-channels links are\ntaken into account in the spatial multivariate model, and shift-invariance is\nused for the temporal model. Multivariate learned kernels are informative (a\nfew atoms code plentiful energy) and interpretable (the atoms can have a\nphysiological meaning). Using real EEG data, the proposed method is shown to\noutperform the classical multichannel matching pursuit used with a Gabor\ndictionary, as measured by the representative power of the learned dictionary\nand its spatial flexibility. Moreover, dictionary learning can capture\ninterpretable patterns: this ability is illustrated on real data, learning a\nP300 evoked potential.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:58:24 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Barth\u00e9lemy", "Quentin", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Isaac", "Yoann", ""], ["Souloumiac", "Antoine", ""], ["Larue", "Anthony", ""], ["Mars", "J\u00e9r\u00f4me I.", ""]]}, {"id": "1303.0775", "submitter": "Onur Ozdemir", "authors": "Onur Ozdemir, Ruoyu Li, Pramod K. Varshney", "title": "Hybrid Maximum Likelihood Modulation Classification Using Multiple\n  Radios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The performance of a modulation classifier is highly sensitive to channel\nsignal-to-noise ratio (SNR). In this paper, we focus on amplitude-phase\nmodulations and propose a modulation classification framework based on\ncentralized data fusion using multiple radios and the hybrid maximum likelihood\n(ML) approach. In order to alleviate the computational complexity associated\nwith ML estimation, we adopt the Expectation Maximization (EM) algorithm. Due\nto SNR diversity, the proposed multi-radio framework provides robustness to\nchannel SNR. Numerical results show the superiority of the proposed approach\nwith respect to single radio approaches as well as to modulation classifiers\nusing moments based estimators.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 18:28:41 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 01:02:30 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Ozdemir", "Onur", ""], ["Li", "Ruoyu", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1303.1152", "submitter": "Martin Jaggi", "authors": "Martin Jaggi", "title": "An Equivalence between the Lasso and Support Vector Machines", "comments": "Book chapter in Regularization, Optimization, Kernels, and Support\n  Vector Machines, Johan A.K. Suykens, Marco Signoretto, Andreas Argyriou\n  (Editors), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relation of two fundamental tools in machine learning and\nsignal processing, that is the support vector machine (SVM) for classification,\nand the Lasso technique used in regression. We show that the resulting\noptimization problems are equivalent, in the following sense. Given any\ninstance of an $\\ell_2$-loss soft-margin (or hard-margin) SVM, we construct a\nLasso instance having the same optimal solutions, and vice versa.\n  As a consequence, many existing optimization algorithms for both SVMs and\nLasso can also be applied to the respective other problem instances. Also, the\nequivalence allows for many known theoretical insights for SVM and Lasso to be\ntranslated between the two settings. One such implication gives a simple\nkernelized version of the Lasso, analogous to the kernels used in the SVM\nsetting. Another consequence is that the sparsity of a Lasso solution is equal\nto the number of support vectors for the corresponding SVM instance, and that\none can use screening rules to prune the set of support vectors. Furthermore,\nwe can relate sublinear time algorithms for the two problems, and give a new\nsuch algorithm variant for the Lasso. We also study the regularization paths\nfor both methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 19:59:13 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 12:03:24 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Jaggi", "Martin", ""]]}, {"id": "1303.1208", "submitter": "Clayton Scott", "authors": "Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, Clayton\n  Scott", "title": "Classification with Asymmetric Label Noise: Consistency and Maximal\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world classification problems, the labels of training examples\nare randomly corrupted. Most previous theoretical work on classification with\nlabel noise assumes that the two classes are separable, that the label noise is\nindependent of the true class label, or that the noise proportions for each\nclass are known. In this work, we give conditions that are necessary and\nsufficient for the true class-conditional distributions to be identifiable.\nThese conditions are weaker than those analyzed previously, and allow for the\nclasses to be nonseparable and the noise levels to be asymmetric and unknown.\nThe conditions essentially state that a majority of the observed labels are\ncorrect and that the true class-conditional distributions are \"mutually\nirreducible,\" a concept we introduce that limits the similarity of the two\ndistributions. For any label noise problem, there is a unique pair of true\nclass-conditional distributions satisfying the proposed conditions, and we\nargue that this pair corresponds in a certain sense to maximal denoising of the\nobserved distributions.\n  Our results are facilitated by a connection to \"mixture proportion\nestimation,\" which is the problem of estimating the maximal proportion of one\ndistribution that is present in another. We establish a novel rate of\nconvergence result for mixture proportion estimation, and apply this to obtain\nconsistency of a discrimination rule based on surrogate loss minimization.\nExperimental results on benchmark data and a nuclear particle classification\nproblem demonstrate the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 22:23:14 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 22:36:23 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2016 15:38:43 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Blanchard", "Gilles", ""], ["Flaska", "Marek", ""], ["Handy", "Gregory", ""], ["Pozzi", "Sara", ""], ["Scott", "Clayton", ""]]}, {"id": "1303.1217", "submitter": "Marcel  Nassar", "authors": "Jing Lin, Marcel Nassar, Brian L. Evans", "title": "Impulsive Noise Mitigation in Powerline Communications Using Sparse\n  Bayesian Learning", "comments": "To appear in IEEE Journal on Selected Areas of Communications", "journal-ref": null, "doi": "10.1109/JSAC.2013.130702", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive asynchronous and cyclostationary impulsive noise limits\ncommunication performance in OFDM powerline communication (PLC) systems.\nConventional OFDM receivers assume additive white Gaussian noise and hence\nexperience degradation in communication performance in impulsive noise.\nAlternate designs assume a parametric statistical model of impulsive noise and\nuse the model parameters in mitigating impulsive noise. These receivers require\noverhead in training and parameter estimation, and degrade due to model and\nparameter mismatch, especially in highly dynamic environments. In this paper,\nwe model impulsive noise as a sparse vector in the time domain without any\nother assumptions, and apply sparse Bayesian learning methods for estimation\nand mitigation without training. We propose three iterative algorithms with\ndifferent complexity vs. performance trade-offs: (1) we utilize the noise\nprojection onto null and pilot tones to estimate and subtract the noise\nimpulses; (2) we add the information in the data tones to perform joint noise\nestimation and OFDM detection; (3) we embed our algorithm into a decision\nfeedback structure to further enhance the performance of coded systems. When\ncompared to conventional OFDM PLC receivers, the proposed receivers achieve SNR\ngains of up to 9 dB in coded and 10 dB in uncoded systems in the presence of\nimpulsive noise.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 22:58:24 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Lin", "Jing", ""], ["Nassar", "Marcel", ""], ["Evans", "Brian L.", ""]]}, {"id": "1303.1280", "submitter": "Remi Lajugie", "authors": "R\\'emi Lajugie (LIENS), Sylvain Arlot (LIENS), Francis Bach (LIENS)", "title": "Large-Margin Metric Learning for Partitioning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider unsupervised partitioning problems, such as\nclustering, image segmentation, video segmentation and other change-point\ndetection problems. We focus on partitioning problems based explicitly or\nimplicitly on the minimization of Euclidean distortions, which include\nmean-based change-point detection, K-means, spectral clustering and normalized\ncuts. Our main goal is to learn a Mahalanobis metric for these unsupervised\nproblems, leading to feature weighting and/or selection. This is done in a\nsupervised way by assuming the availability of several potentially partially\nlabelled datasets that share the same metric. We cast the metric learning\nproblem as a large-margin structured prediction problem, with proper definition\nof regularizers and losses, leading to a convex optimization problem which can\nbe solved efficiently with iterative techniques. We provide experiments where\nwe show how learning the metric may significantly improve the partitioning\nperformance in synthetic examples, bioinformatics, video segmentation and image\nsegmentation problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 09:23:45 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Lajugie", "R\u00e9mi", "", "LIENS"], ["Arlot", "Sylvain", "", "LIENS"], ["Bach", "Francis", "", "LIENS"]]}, {"id": "1303.1312", "submitter": "Niels Lovmand  Pedersen", "authors": "Niels Lovmand Pedersen and Carles Navarro Manch\\'on Bernard Henri\n  Fleury", "title": "A Fast Iterative Bayesian Inference Algorithm for Sparse Channel\n  Estimation", "comments": "in Proc. IEEE Int. Communications Conf. (ICC), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Bayesian channel estimation algorithm for\nmulticarrier receivers based on pilot symbol observations. The inherent sparse\nnature of wireless multipath channels is exploited by modeling the prior\ndistribution of multipath components' gains with a hierarchical representation\nof the Bessel K probability density function; a highly efficient, fast\niterative Bayesian inference method is then applied to the proposed model. The\nresulting estimator outperforms other state-of-the-art Bayesian and\nnon-Bayesian estimators, either by yielding lower mean squared estimation error\nor by attaining the same accuracy with improved convergence rate, as shown in\nour numerical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 12:17:12 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Pedersen", "Niels Lovmand", ""], ["Fleury", "Carles Navarro Manch\u00f3n Bernard Henri", ""]]}, {"id": "1303.1700", "submitter": "Boris Campillo-Gimenez", "authors": "Boris Campillo-Gimenez, Wassim Jouini, Sahar Bayat, Marc Cuggia", "title": "K-Nearest Neighbour algorithm coupled with logistic regression in\n  medical case-based reasoning systems. Application to prediction of access to\n  the renal transplant waiting list in Brittany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction. Case Based Reasoning (CBR) is an emerg- ing decision making\nparadigm in medical research where new cases are solved relying on previously\nsolved similar cases. Usually, a database of solved cases is provided, and\nevery case is described through a set of attributes (inputs) and a label\n(output). Extracting useful information from this database can help the CBR\nsystem providing more reliable results on the yet to be solved cases.\nObjective. For that purpose we suggest a general frame- work where a CBR\nsystem, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with various\ninformation obtained from a Logistic Regression (LR) model. Methods. LR is\napplied, on the case database, to assign weights to the attributes as well as\nthe solved cases. Thus, five possible decision making systems based on K-NN\nand/or LR were identified: a standalone K-NN, a standalone LR and three soft\nK-NN algorithms that rely on the weights based on the results of the LR. The\nevaluation of the described approaches is performed in the field of renal\ntransplant access waiting list. Results and conclusion. The results show that\nour suggested approach, where the K-NN algorithm relies on both weighted\nattributes and cases, can efficiently deal with non relevant attributes,\nwhereas the four other approaches suffer from this kind of noisy setups. The\nrobustness of this approach suggests interesting perspectives for medical\nproblem solving tools using CBR methodology.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 14:25:23 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Campillo-Gimenez", "Boris", ""], ["Jouini", "Wassim", ""], ["Bayat", "Sahar", ""], ["Cuggia", "Marc", ""]]}, {"id": "1303.1993", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and James V. Burke and Gianluigi Pillonetto", "title": "Optimization viewpoint on Kalman smoothing, with applications to robust\n  and sparse estimation", "comments": "46 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the optimization formulation of the Kalman\nfiltering and smoothing problems, and use this perspective to develop a variety\nof extensions and applications. We first formulate classic Kalman smoothing as\na least squares problem, highlight special structure, and show that the classic\nfiltering and smoothing algorithms are equivalent to a particular algorithm for\nsolving this problem. Once this equivalence is established, we present\nextensions of Kalman smoothing to systems with nonlinear process and\nmeasurement models, systems with linear and nonlinear inequality constraints,\nsystems with outliers in the measurements or sudden changes in the state, and\nsystems where the sparsity of the state sequence must be accounted for. All\nextensions preserve the computational efficiency of the classic algorithms, and\nmost of the extensions are illustrated with numerical examples, which are part\nof an open source Kalman smoothing Matlab/Octave package.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 13:53:40 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 10:35:41 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1303.2184", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis, Sergios Theodoridis, Charalampos Mavroforakis,\n  Leoni Dalla", "title": "Complex Support Vector Machines for Regression and Quaternary\n  Classification", "comments": "Manuscript accepted in IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2014.2336679", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new framework for complex Support Vector Regression as\nwell as Support Vector Machines for quaternary classification. The method\nexploits the notion of widely linear estimation to model the input-out relation\nfor complex-valued data and considers two cases: a) the complex data are split\ninto their real and imaginary parts and a typical real kernel is employed to\nmap the complex data to a complexified feature space and b) a pure complex\nkernel is used to directly map the data to the induced complex feature space.\nThe recently developed Wirtinger's calculus on complex reproducing kernel\nHilbert spaces (RKHS) is employed in order to compute the Lagrangian and derive\nthe dual optimization problem. As one of our major results, we prove that any\ncomplex SVM/SVR task is equivalent with solving two real SVM/SVR tasks\nexploiting a specific real kernel which is generated by the chosen complex\nkernel. In particular, the case of pure complex kernels leads to the generation\nof new kernels, which have not been considered before. In the classification\ncase, the proposed framework inherently splits the complex space into four\nparts. This leads naturally in solving the four class-task (quaternary\nclassification), instead of the typical two classes of the real SVM. In turn,\nthis rationale can be used in a multiclass problem as a split-class scenario\nbased on four classes, as opposed to the one-versus-all method; this can lead\nto significant computational savings. Experiments demonstrate the effectiveness\nof the proposed framework for regression and classification tasks that involve\ncomplex data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 09:09:54 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 06:04:33 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 09:42:04 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""], ["Mavroforakis", "Charalampos", ""], ["Dalla", "Leoni", ""]]}, {"id": "1303.2221", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, Nikolai Nefedov", "title": "Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann\n  Manifolds", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 62, no. 4, pp.\n  905-918, February 2014", "doi": "10.1109/TSP.2013.2295553", "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationships between entities in datasets are often of multiple nature, like\ngeographical distance, social relationships, or common interests among people\nin a social network, for example. This information can naturally be modeled by\na set of weighted and undirected graphs that form a global multilayer graph,\nwhere the common vertex set represents the entities and the edges on different\nlayers capture the similarities of the entities in term of the different\nmodalities. In this paper, we address the problem of analyzing multi-layer\ngraphs and propose methods for clustering the vertices by efficiently merging\nthe information provided by the multiple modalities. To this end, we propose to\ncombine the characteristics of individual graph layers using tools from\nsubspace analysis on a Grassmann manifold. The resulting combination can then\nbe viewed as a low dimensional representation of the original data which\npreserves the most important information from diverse relationships between\nentities. We use this information in new clustering methods and test our\nalgorithm on several synthetic and real world datasets where we demonstrate\nsuperior or competitive performances compared to baseline and state-of-the-art\ntechniques. Our generic framework further extends to numerous analysis and\nlearning problems that involve different types of information on graphs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 15:31:48 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Dong", "Xiaowen", ""], ["Frossard", "Pascal", ""], ["Vandergheynst", "Pierre", ""], ["Nefedov", "Nikolai", ""]]}, {"id": "1303.2378", "submitter": "Hamed Firouzi", "authors": "Hamed Firouzi, Bala Rajaratnam, Alfred Hero", "title": "Predictive Correlation Screening: Application to Two-stage Predictor\n  Design in High Dimension", "comments": "31 pages, 9 figures, Appearing in Proceedings of the 16th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to variable selection, called Predictive\nCorrelation Screening, for predictor design. Predictive Correlation Screening\n(PCS) implements false positive control on the selected variables, is well\nsuited to small sample sizes, and is scalable to high dimensions. We establish\nasymptotic bounds for Familywise Error Rate (FWER), and resultant mean square\nerror of a linear predictor on the selected variables. We apply Predictive\nCorrelation Screening to the following two-stage predictor design problem. An\nexperimenter wants to learn a multivariate predictor of gene expressions based\non successive biological samples assayed on mRNA arrays. She assays the whole\ngenome on a few samples and from these assays she selects a small number of\nvariables using Predictive Correlation Screening. To reduce assay cost, she\nsubsequently assays only the selected variables on the remaining samples, to\nlearn the predictor coefficients. We show superiority of Predictive Correlation\nScreening relative to LASSO and correlation learning (sometimes popularly\nreferred to in the literature as marginal regression or simple thresholding) in\nterms of performance and computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 21:31:37 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 07:05:51 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Firouzi", "Hamed", ""], ["Rajaratnam", "Bala", ""], ["Hero", "Alfred", ""]]}, {"id": "1303.2395", "submitter": "Xu Sun", "authors": "Xu Sun, Jinqiao Duan, Xiaofan Li, Xiangjun Wang", "title": "State estimation under non-Gaussian Levy noise: A modified Kalman\n  filtering method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.IT cs.LG math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter is extensively used for state estimation for linear systems\nunder Gaussian noise. When non-Gaussian L\\'evy noise is present, the\nconventional Kalman filter may fail to be effective due to the fact that the\nnon-Gaussian L\\'evy noise may have infinite variance. A modified Kalman filter\nfor linear systems with non-Gaussian L\\'evy noise is devised. It works\neffectively with reasonable computational cost. Simulation results are\npresented to illustrate this non-Gaussian filtering method.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 23:20:12 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Sun", "Xu", ""], ["Duan", "Jinqiao", ""], ["Li", "Xiaofan", ""], ["Wang", "Xiangjun", ""]]}, {"id": "1303.2417", "submitter": "GuangGang Geng", "authors": "Xiao-Bo Jin and Guang-Gang Geng", "title": "Linear NDCG and Pair-wise Loss", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear NDCG is used for measuring the performance of the Web content quality\nassessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will prove\nthat the DCG error equals a new pair-wise loss.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 03:29:35 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Geng", "Guang-Gang", ""]]}, {"id": "1303.2488", "submitter": "Michel Plantie", "authors": "Michel Crampes, Michel Planti\\'e", "title": "Visualizing and Interacting with Concept Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Concept Hierarchies and Formal Concept Analysis are theoretically well\ngrounded and largely experimented methods. They rely on line diagrams called\nGalois lattices for visualizing and analysing object-attribute sets. Galois\nlattices are visually seducing and conceptually rich for experts. However they\npresent important drawbacks due to their concept oriented overall structure:\nanalysing what they show is difficult for non experts, navigation is\ncumbersome, interaction is poor, and scalability is a deep bottleneck for\nvisual interpretation even for experts. In this paper we introduce semantic\nprobes as a means to overcome many of these problems and extend usability and\napplication possibilities of traditional FCA visualization methods. Semantic\nprobes are visual user centred objects which extract and organize reduced\nGalois sub-hierarchies. They are simpler, clearer, and they provide a better\nnavigation support through a rich set of interaction possibilities. Since probe\ndriven sub-hierarchies are limited to users focus, scalability is under control\nand interpretation is facilitated. After some successful experiments, several\napplications are being developed with the remaining problem of finding a\ncompromise between simplicity and conceptual expressivity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 11:20:23 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Crampes", "Michel", ""], ["Planti\u00e9", "Michel", ""]]}, {"id": "1303.2506", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Monte-Carlo utility estimates for Bayesian reinforcement learning", "comments": "6 pages, 4 figures, 1 table, submitted to IEEE conference on decision\n  and control", "journal-ref": null, "doi": "10.1109/CDC.2013.6761048", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a set of algorithms for Monte-Carlo Bayesian\nreinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the\nBayes-optimal value function is employed to construct an optimistic policy.\nSecondly, gradient-based algorithms for approximate upper and lower bounds are\nintroduced. Finally, we introduce a new class of gradient algorithms for\nBayesian Bellman error minimisation. We theoretically show that the gradient\nmethods are sound. Experimentally, we demonstrate the superiority of the upper\nbound method in terms of reward obtained. However, we also show that the\nBayesian Bellman error method is a close second, despite its significant\ncomputational simplicity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 13:06:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1303.2517", "submitter": "Hamed Masnadi-Shirazi", "authors": "Hamed Masnadi-Shirazi", "title": "Refinement revisited with connections to Bayes error, conditional\n  entropy and calibrated classifiers", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of refinement from probability elicitation is considered for\nproper scoring rules. Taking directions from the axioms of probability,\nrefinement is further clarified using a Hilbert space interpretation and\nreformulated into the underlying data distribution setting where connections to\nmaximal marginal diversity and conditional entropy are considered and used to\nderive measures that provide arbitrarily tight bounds on the Bayes error.\nRefinement is also reformulated into the classifier output setting and its\nconnections to calibrated classifiers and proper margin losses are established.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 13:34:51 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Masnadi-Shirazi", "Hamed", ""]]}, {"id": "1303.2663", "submitter": "Allon G. Percus", "authors": "Laura M. Smith, Kristina Lerman, Cristina Garcia-Cardona, Allon G.\n  Percus, Rumi Ghosh", "title": "Spectral Clustering with Epidemic Diffusion", "comments": "6 pages, to appear in Physical Review E", "journal-ref": null, "doi": "10.1103/PhysRevE.88.042813", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is widely used to partition graphs into distinct modules\nor communities. Existing methods for spectral clustering use the eigenvalues\nand eigenvectors of the graph Laplacian, an operator that is closely associated\nwith random walks on graphs. We propose a new spectral partitioning method that\nexploits the properties of epidemic diffusion. An epidemic is a dynamic process\nthat, unlike the random walk, simultaneously transitions to all the neighbors\nof a given node. We show that the replicator, an operator describing epidemic\ndiffusion, is equivalent to the symmetric normalized Laplacian of a reweighted\ngraph with edges reweighted by the eigenvector centralities of their incident\nnodes. Thus, more weight is given to edges connecting more central nodes. We\ndescribe a method that partitions the nodes based on the componentwise ratio of\nthe replicator's second eigenvector to the first, and compare its performance\nto traditional spectral clustering techniques on synthetic graphs with known\ncommunity structure. We demonstrate that the replicator gives preference to\ndense, clique-like structures, enabling it to more effectively discover\ncommunities that may be obscured by dense intercommunity linking.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 20:00:32 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 05:12:35 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Smith", "Laura M.", ""], ["Lerman", "Kristina", ""], ["Garcia-Cardona", "Cristina", ""], ["Percus", "Allon G.", ""], ["Ghosh", "Rumi", ""]]}, {"id": "1303.2823", "submitter": "Steven Van Vaerenbergh", "authors": "Fernando P\\'erez-Cruz, Steven Van Vaerenbergh, Juan Jos\\'e\n  Murillo-Fuentes, Miguel L\\'azaro-Gredilla and Ignacio Santamaria", "title": "Gaussian Processes for Nonlinear Signal Processing", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, vol.30, no.4, pp.40-50, July 2013", "doi": "10.1109/MSP.2013.2250352", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are versatile tools that have been successfully\nemployed to solve nonlinear estimation problems in machine learning, but that\nare rarely used in signal processing. In this tutorial, we present GPs for\nregression as a natural nonlinear extension to optimal Wiener filtering. After\nestablishing their basic formulation, we discuss several important aspects and\nextensions, including recursive and adaptive algorithms for dealing with\nnon-stationarity, low-complexity solutions, non-Gaussian noise models and\nclassification scenarios. Furthermore, we provide a selection of relevant\napplications to wireless digital communications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:16:29 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 11:07:52 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["P\u00e9rez-Cruz", "Fernando", ""], ["Van Vaerenbergh", "Steven", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["L\u00e1zaro-Gredilla", "Miguel", ""], ["Santamaria", "Ignacio", ""]]}, {"id": "1303.2827", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, James V. Burke and Gianluigi Pillonetto", "title": "Linear system identification using stable spline kernels and PLQ\n  penalties", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to linear system identification is given by parametric\nPrediction Error Methods (PEM). In this context, model complexity is often\nunknown so that a model order selection step is needed to suitably trade-off\nbias and variance. Recently, a different approach to linear system\nidentification has been introduced, where model order determination is avoided\nby using a regularized least squares framework. In particular, the penalty term\non the impulse response is defined by so called stable spline kernels. They\nembed information on regularity and BIBO stability, and depend on a small\nnumber of parameters which can be estimated from data. In this paper, we\nprovide new nonsmooth formulations of the stable spline estimator. In\nparticular, we consider linear system identification problems in a very broad\ncontext, where regularization functionals and data misfits can come from a rich\nset of piecewise linear quadratic functions. Moreover, our anal- ysis includes\npolyhedral inequality constraints on the unknown impulse response. For any\nformulation in this class, we show that interior point methods can be used to\nsolve the system identification problem, with complexity O(n3)+O(mn2) in each\niteration, where n and m are the number of impulse response coefficients and\nmeasurements, respectively. The usefulness of the framework is illustrated via\na numerical experiment where output measurements are contaminated by outliers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:31:29 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1303.2892", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier and Remi Munos", "title": "Toward Optimal Stratification for Stratified Monte-Carlo Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adaptive stratified sampling for Monte Carlo\nintegration of a noisy function, given a finite budget n of noisy evaluations\nto the function. We tackle in this paper the problem of adapting to the\nfunction at the same time the number of samples into each stratum and the\npartition itself. More precisely, it is interesting to refine the partition of\nthe domain in area where the noise to the function, or where the variations of\nthe function, are very heterogeneous. On the other hand, having a (too) refined\nstratification is not optimal. Indeed, the more refined the stratification, the\nmore difficult it is to adjust the allocation of the samples to the\nstratification, i.e. sample more points where the noise or variations of the\nfunction are larger. We provide in this paper an algorithm that selects online,\namong a large class of partitions, the partition that provides the optimal\ntrade-off, and allocates the samples almost optimally on this partition.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 14:20:27 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Munos", "Remi", ""]]}, {"id": "1303.2912", "submitter": "Roger Frigola", "authors": "Roger Frigola and Carl Edward Rasmussen", "title": "Integrated Pre-Processing for Bayesian Nonlinear System Identification\n  with Gaussian Processes", "comments": "Proceedings of the 52th IEEE International Conference on Decision and\n  Control (CDC), Firenze, Italy, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GP-FNARX: a new model for nonlinear system identification based\non a nonlinear autoregressive exogenous model (NARX) with filtered regressors\n(F) where the nonlinear regression problem is tackled using sparse Gaussian\nprocesses (GP). We integrate data pre-processing with system identification\ninto a fully automated procedure that goes from raw data to an identified\nmodel. Both pre-processing parameters and GP hyper-parameters are tuned by\nmaximizing the marginal likelihood of the probabilistic model. We obtain a\nBayesian model of the system's dynamics which is able to report its uncertainty\nin regions where the data is scarce. The automated approach, the modeling of\nuncertainty and its relatively low computational cost make of GP-FNARX a good\ncandidate for applications in robotics and adaptive control.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 15:18:00 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 16:25:57 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 12:43:24 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Frigola", "Roger", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1303.3055", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Peter L. Bartlett and Csaba Szepesvari", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen\n  Transition Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Markov decision processes with finite state\nand action spaces when the transition probability distributions and loss\nfunctions are chosen adversarially and are allowed to change with time. We\nintroduce an algorithm whose regret with respect to any policy in a comparison\nclass grows as the square root of the number of rounds of the game, provided\nthe transition probabilities satisfy a uniform mixing condition. Our approach\nis efficient as long as the comparison class is polynomial and we can compute\nexpectations over sample paths for each policy. Designing an efficient\nalgorithm with small regret for the general case remains an open problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 23:25:37 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Bartlett", "Peter L.", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1303.3128", "submitter": "Chinghway Lim", "authors": "Chinghway Lim and Bin Yu", "title": "Estimation Stability with Cross Validation (ESCV)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is often used to select the regularization parameter in\nhigh dimensional problems. However, when applied to the sparse modeling method\nLasso, CV leads to models that are unstable in high-dimensions, and\nconsequently not suited for reliable interpretation. In this paper, we propose\na model-free criterion ESCV based on a new estimation stability (ES) metric and\nCV. Our proposed ESCV finds a locally ES-optimal model smaller than the CV\nchoice so that the it fits the data and also enjoys estimation stability\nproperty. We demonstrate that ESCV is an effective alternative to CV at a\nsimilar easily parallelizable computational cost. In particular, we compare the\ntwo approaches with respect to several performance measures when applied to the\nLasso on both simulated and real data sets. For dependent predictors common in\npractice, our main finding is that, ESCV cuts down false positive rates often\nby a large margin, while sacrificing little of true positive rates. ESCV\nusually outperforms CV in terms of parameter estimation while giving similar\nperformance as CV in terms of prediction. For the two real data sets from\nneuroscience and cell biology, the models found by ESCV are less than half of\nthe model sizes by CV. Judged based on subject knowledge, they are more\nplausible than those by CV as well. We also discuss some regularization\nparameter alignment issues that come up in both approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 10:47:42 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 09:36:50 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Lim", "Chinghway", ""], ["Yu", "Bin", ""]]}, {"id": "1303.3163", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi and Mauricio Araya", "title": "A Greedy Approximation of Bayesian Reinforcement Learning with Probably\n  Optimistic Transition Model", "comments": "the 13th International Workshop on Adaptive and Learning Agents at\n  AAMAS'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Reinforcement Learning (RL) is capable of not only incorporating\ndomain knowledge, but also solving the exploration-exploitation dilemma in a\nnatural way. As Bayesian RL is intractable except for special cases, previous\nwork has proposed several approximation methods. However, these methods are\nusually too sensitive to parameter values, and finding an acceptable parameter\nsetting is practically impossible in many applications. In this paper, we\npropose a new algorithm that greedily approximates Bayesian RL to achieve\nrobustness in parameter space. We show that for a desired learning behavior,\nour proposed algorithm has a polynomial sample complexity that is lower than\nthose of existing algorithms. We also demonstrate that the proposed algorithm\nnaturally outperforms other existing algorithms when the prior distributions\nare not significantly misleading. On the other hand, the proposed algorithm\ncannot handle greatly misspecified priors as well as the other algorithms can.\nThis is a natural consequence of the fact that the proposed algorithm is\ngreedier than the other algorithms. Accordingly, we discuss a way to select an\nappropriate algorithm for different tasks based on the algorithms' greediness.\nWe also introduce a new way of simplifying Bayesian planning, based on which\nfuture work would be able to derive new algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 14:06:21 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 03:01:40 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 01:04:03 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Araya", "Mauricio", ""]]}, {"id": "1303.3207", "submitter": "Luca Baldassarre", "authors": "Luca Baldassarre and Nirav Bhan and Volkan Cevher and Anastasios\n  Kyrillidis and Siddhartha Satpathi", "title": "Group-Sparse Model Selection: Hardness and Relaxations", "comments": "34 pages. Submitted to IEEE Trans. on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-based sparsity models are proven instrumental in linear regression\nproblems for recovering signals from much fewer measurements than standard\ncompressive sensing. The main promise of these models is the recovery of\n\"interpretable\" signals through the identification of their constituent groups.\nIn this paper, we establish a combinatorial framework for group-model selection\nproblems and highlight the underlying tractability issues. In particular, we\nshow that the group-model selection problem is equivalent to the well-known\nNP-hard weighted maximum coverage problem (WMC). Leveraging a graph-based\nunderstanding of group models, we describe group structures which enable\ncorrect model selection in polynomial time via dynamic programming.\nFurthermore, group structures that lead to totally unimodular constraints have\ntractable discrete as well as convex relaxations. We also present a\ngeneralization of the group-model that allows for within group sparsity, which\ncan be used to model hierarchical sparsity. Finally, we study the Pareto\nfrontier of group-sparse approximations for two tractable models, among which\nthe tree sparsity model, and illustrate selection and computation trade-offs\nbetween our framework and the existing convex relaxations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 16:22:03 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 15:39:29 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2013 07:47:22 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 14:30:21 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Baldassarre", "Luca", ""], ["Bhan", "Nirav", ""], ["Cevher", "Volkan", ""], ["Kyrillidis", "Anastasios", ""], ["Satpathi", "Siddhartha", ""]]}, {"id": "1303.3240", "submitter": "Mihalis A. Nicolaou", "authors": "Mihalis A. Nicolaou, Stefanos Zafeiriou and Maja Pantic", "title": "A Unified Framework for Probabilistic Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying framework which reduces the construction of\nprobabilistic component analysis techniques to a mere selection of the latent\nneighbourhood, thus providing an elegant and principled framework for creating\nnovel component analysis models as well as constructing probabilistic\nequivalents of deterministic component analysis methods. Under our framework,\nwe unify many very popular and well-studied component analysis algorithms, such\nas Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),\nLocality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of\nwhich have no probabilistic equivalents in literature thus far. We firstly\ndefine the Markov Random Fields (MRFs) which encapsulate the latent\nconnectivity of the aforementioned component analysis techniques; subsequently,\nwe show that the projection directions produced by all PCA, LDA, LPP and SFA\nare also produced by the Maximum Likelihood (ML) solution of a single joint\nprobability density function, composed by selecting one of the defined MRF\npriors while utilising a simple observation model. Furthermore, we propose\nnovel Expectation Maximization (EM) algorithms, exploiting the proposed joint\nPDF, while we generalize the proposed methodologies to arbitrary connectivities\nvia parameterizable MRF products. Theoretical analysis and experiments on both\nsimulated and real world data show the usefulness of the proposed framework, by\nderiving methods which well outperform state-of-the-art equivalents.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 18:18:14 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 15:33:29 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Nicolaou", "Mihalis A.", ""], ["Zafeiriou", "Stefanos", ""], ["Pantic", "Maja", ""]]}, {"id": "1303.3257", "submitter": "Francesco Strino Ph.D.", "authors": "Fabio Parisi, Francesco Strino, Boaz Nadler and Yuval Kluger", "title": "Ranking and combining multiple predictors without labeled data", "comments": "Supplementary Information is included at the end of the manuscript.\n  This is a revision of our original submission of the manuscript entitled \"The\n  student's dilemma: ranking and improving prediction at test time without\n  access to training data\", which is now entitled \"Ranking and combining\n  multiple predictors without labeled data\"", "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 111 (2014) 1253-1258", "doi": "10.1073/pnas.1219097111", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad range of classification and decision making problems, one is given\nthe advice or predictions of several classifiers, of unknown reliability, over\nmultiple questions or queries. This scenario is different from the standard\nsupervised setting, where each classifier accuracy can be assessed using\navailable labeled data, and raises two questions: given only the predictions of\nseveral classifiers over a large set of unlabeled test data, is it possible to\na) reliably rank them; and b) construct a meta-classifier more accurate than\nmost classifiers in the ensemble? Here we present a novel spectral approach to\naddress these questions. First, assuming conditional independence between\nclassifiers, we show that the off-diagonal entries of their covariance matrix\ncorrespond to a rank-one matrix. Moreover, the classifiers can be ranked using\nthe leading eigenvector of this covariance matrix, as its entries are\nproportional to their balanced accuracies. Second, via a linear approximation\nto the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML),\na novel ensemble classifier whose weights are equal to this eigenvector\nentries. On both simulated and real data, SML typically achieves a higher\naccuracy than most classifiers in the ensemble and can provide a better\nstarting point than majority voting, for estimating the maximum likelihood\nsolution. Furthermore, SML is robust to the presence of small malicious groups\nof classifiers designed to veer the ensemble prediction away from the (unknown)\nground truth.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 19:45:03 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 16:45:16 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2013 17:22:41 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Parisi", "Fabio", ""], ["Strino", "Francesco", ""], ["Nadler", "Boaz", ""], ["Kluger", "Yuval", ""]]}, {"id": "1303.3265", "submitter": "Konstantina Palla Miss", "authors": "Konstantina Palla, David A. Knowles, Zoubin Ghahramani", "title": "A dependent partition-valued process for multitask clustering and time\n  evolving network modelling", "comments": "9 pages, 7 figures, submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental aim of clustering algorithms is to partition data points. We\nconsider tasks where the discovered partition is allowed to vary with some\ncovariate such as space or time. One approach would be to use\nfragmentation-coagulation processes, but these, being Markov processes, are\nrestricted to linear or tree structured covariate spaces. We define a\npartition-valued process on an arbitrary covariate space using Gaussian\nprocesses. We use the process to construct a multitask clustering model which\npartitions datapoints in a similar way across multiple data sources, and a time\nseries model of network data which allows cluster assignments to vary over\ntime. We describe sampling algorithms for inference and apply our method to\ndefining cancer subtypes based on different types of cellular characteristics,\nfinding regulatory modules from gene expression data from multiple human\npopulations, and discovering time varying community structure in a social\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 13:55:20 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2013 14:08:40 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Palla", "Konstantina", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1303.3664", "submitter": "Weicong Ding", "authors": "Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama", "title": "Topic Discovery through Data Dependent and Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for topic modeling based on the geometry of\ncross-document word-frequency patterns. This perspective gains significance\nunder the so called separability condition. This is a condition on existence of\nnovel-words that are unique to each topic. We present a suite of highly\nefficient algorithms based on data-dependent and random projections of\nword-frequency patterns to identify novel words and associated topics. We will\nalso discuss the statistical guarantees of the data-dependent projections\nmethod based on two mild assumptions on the prior density of topic document\nmatrix. Our key insight here is that the maximum and minimum values of\ncross-document frequency patterns projected along any direction are associated\nwith novel words. While our sample complexity bounds for topic recovery are\nsimilar to the state-of-art, the computational complexity of our random\nprojection scheme scales linearly with the number of documents and the number\nof words per document. We present several experiments on synthetic and\nreal-world datasets to demonstrate qualitative and quantitative merits of our\nscheme.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 02:37:19 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 13:11:02 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Ding", "Weicong", ""], ["Rohban", "Mohammad H.", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1303.3716", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Subspace Clustering via Thresholding and Spectral Clustering", "comments": "ICASSP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering a set of high-dimensional data points\ninto sets of low-dimensional linear subspaces. The number of subspaces, their\ndimensions, and their orientations are unknown. We propose a simple and\nlow-complexity clustering algorithm based on thresholding the correlations\nbetween the data points followed by spectral clustering. A probabilistic\nperformance analysis shows that this algorithm succeeds even when the subspaces\nintersect, and when the dimensions of the subspaces scale (up to a log-factor)\nlinearly in the ambient dimension. Moreover, we prove that the algorithm also\nsucceeds for data points that are subject to erasures with the number of\nerasures scaling (up to a log-factor) linearly in the ambient dimension.\nFinally, we propose a simple scheme that provably detects outliers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 09:52:54 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1303.3866", "submitter": "Se Un Park", "authors": "Se Un Park, Nicolas Dobigeon, Alfred O. Hero", "title": "Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases\n  and its Application to MRFM", "comments": "This work has been submitted to Signal Processing for possible\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variational Bayesian method of joint image reconstruction and\npoint spread function (PSF) estimation when the PSF of the imaging device is\nonly partially known. To solve this semi-blind deconvolution problem, prior\ndistributions are specified for the PSF and the 3D image. Joint image\nreconstruction and PSF estimation is then performed within a Bayesian\nframework, using a variational algorithm to estimate the posterior\ndistribution. The image prior distribution imposes an explicit atomic measure\nthat corresponds to image sparsity. Importantly, the proposed Bayesian\ndeconvolution algorithm does not require hand tuning. Simulation results\nclearly demonstrate that the semi-blind deconvolution algorithm compares\nfavorably with previous Markov chain Monte Carlo (MCMC) version of myopic\nsparse reconstruction. It significantly outperforms mismatched non-blind\nalgorithms that rely on the assumption of the perfect knowledge of the PSF. The\nalgorithm is illustrated on real data from magnetic resonance force microscopy\n(MRFM).\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 19:07:48 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Park", "Se Un", ""], ["Dobigeon", "Nicolas", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1303.3987", "submitter": "Liping  Wang", "authors": "Liping Wang and Songcan Chen", "title": "$l_{2,p}$ Matrix Norm and Its Application in Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such as\ncomputer vision, pattern recognition, biological study and etc. As an extension\nof $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to find\njointly sparse solutions. Moreover, an efficient iterative algorithm has been\ndesigned to solve $l_{2,1}$-norm involved minimizations. Actually,\ncomputational studies have showed that $l_p$-regularization ($0<p<1$) is\nsparser than $l_1$-regularization, but the extension to matrix norm has been\nseldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\\in\n(0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$\nvector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$.\nFortunately, an efficient unified algorithm is proposed to solve the induced\n$l_{2,p}$-norm $(p\\in (0, 1])$ optimization problems. The convergence can also\nbe uniformly demonstrated for all $p\\in (0, 1]$. Typical $p\\in (0,1]$ are\napplied to select features in computational biology and the experimental\nresults show that some choices of $0<p<1$ do improve the sparse pattern of\nusing $p=1$.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 15:06:12 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Wang", "Liping", ""], ["Chen", "Songcan", ""]]}, {"id": "1303.4172", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Margins, Shrinkage, and Boosting", "comments": "To appear, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript shows that AdaBoost and its immediate variants can produce\napproximate maximum margin classifiers simply by scaling step size choices with\na fixed small constant. In this way, when the unscaled step size is an optimal\nchoice, these results provide guarantees for Friedman's empirically successful\n\"shrinkage\" procedure for gradient boosting (Friedman, 2000). Guarantees are\nalso provided for a variety of other step sizes, affirming the intuition that\nincreasingly regularized line searches provide improved margin guarantees. The\nresults hold for the exponential loss and similar losses, most notably the\nlogistic loss.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 07:33:29 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1303.4385", "submitter": "Nabin Malakar", "authors": "N. K. Malakar and D. Gladkov and K. H. Knuth", "title": "Modeling a Sensor to Improve its Efficacy", "comments": "18 pages, 8 figures, submitted to the special issue of \"Sensors for\n  Robotics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots rely on sensors to provide them with information about their\nsurroundings. However, high-quality sensors can be extremely expensive and\ncost-prohibitive. Thus many robotic systems must make due with lower-quality\nsensors. Here we demonstrate via a case study how modeling a sensor can improve\nits efficacy when employed within a Bayesian inferential framework. As a test\nbed we employ a robotic arm that is designed to autonomously take its own\nmeasurements using an inexpensive LEGO light sensor to estimate the position\nand radius of a white circle on a black field. The light sensor integrates the\nlight arriving from a spatially distributed region within its field of view\nweighted by its Spatial Sensitivity Function (SSF). We demonstrate that by\nincorporating an accurate model of the light sensor SSF into the likelihood\nfunction of a Bayesian inference engine, an autonomous system can make improved\ninferences about its surroundings. The method presented here is data-based,\nfairly general, and made with plug-and play in mind so that it could be\nimplemented in similar problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 04:11:06 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Malakar", "N. K.", ""], ["Gladkov", "D.", ""], ["Knuth", "K. H.", ""]]}, {"id": "1303.4431", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "Generalized Thompson Sampling for Sequential Decision-Making and Causal\n  Inference", "comments": "28 pages, 5 figures", "journal-ref": "Complex Adaptive Systems Modeling 2014, 2:2", "doi": "10.1186/2194-3206-2-2", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown how sampling actions from the predictive\ndistribution over the optimal action-sometimes called Thompson sampling-can be\napplied to solve sequential adaptive control problems, when the optimal policy\nis known for each possible environment. The predictive distribution can then be\nconstructed by a Bayesian superposition of the optimal policies weighted by\ntheir posterior probability that is updated by Bayesian inference and causal\ncalculus. Here we discuss three important features of this approach. First, we\ndiscuss in how far such Thompson sampling can be regarded as a natural\nconsequence of the Bayesian modeling of policy uncertainty. Second, we show how\nThompson sampling can be used to study interactions between multiple adaptive\nagents, thus, opening up an avenue of game-theoretic analysis. Third, we show\nhow Thompson sampling can be applied to infer causal relationships when\ninteracting with an environment in a sequential fashion. In summary, our\nresults suggest that Thompson sampling might not merely be a useful heuristic,\nbut a principled method to address problems of adaptive sequential\ndecision-making and causal inference.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:34:06 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1303.4434", "submitter": "Pinghua Gong", "authors": "Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex\n  Regularized Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex sparsity-inducing penalties have recently received considerable\nattentions in sparse learning. Recent theoretical investigations have\ndemonstrated their superiority over the convex counterparts in several sparse\nlearning settings. However, solving the non-convex optimization problems\nassociated with non-convex penalties remains a big challenge. A commonly used\napproach is the Multi-Stage (MS) convex relaxation (or DC programming), which\nrelaxes the original non-convex problem to a sequence of convex problems. This\napproach is usually not very practical for large-scale problems because its\ncomputational cost is a multiple of solving a single convex problem. In this\npaper, we propose a General Iterative Shrinkage and Thresholding (GIST)\nalgorithm to solve the nonconvex optimization problem for a large class of\nnon-convex penalties. The GIST algorithm iteratively solves a proximal operator\nproblem, which in turn has a closed-form solution for many commonly used\npenalties. At each outer iteration of the algorithm, we use a line search\ninitialized by the Barzilai-Borwein (BB) rule that allows finding an\nappropriate step size quickly. The paper also presents a detailed convergence\nanalysis of the GIST algorithm. The efficiency of the proposed algorithm is\ndemonstrated by extensive experiments on large-scale data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:41:53 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Gong", "Pinghua", ""], ["Zhang", "Changshui", ""], ["Lu", "Zhaosong", ""], ["Huang", "Jianhua", ""], ["Ye", "Jieping", ""]]}, {"id": "1303.4694", "submitter": "Jayaraman J. Thiagarajan", "authors": "Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan and Andreas\n  Spanias", "title": "Recovering Non-negative and Combined Sparse Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-negative solution to an underdetermined linear system can be uniquely\nrecovered sometimes, even without imposing any additional sparsity constraints.\nIn this paper, we derive conditions under which a unique non-negative solution\nfor such a system can exist, based on the theory of polytopes. Furthermore, we\ndevelop the paradigm of combined sparse representations, where only a part of\nthe coefficient vector is constrained to be non-negative, and the rest is\nunconstrained (general). We analyze the recovery of the unique, sparsest\nsolution, for combined representations, under three different cases of\ncoefficient support knowledge: (a) the non-zero supports of non-negative and\ngeneral coefficients are known, (b) the non-zero support of general\ncoefficients alone is known, and (c) both the non-zero supports are unknown.\nFor case (c), we propose the combined orthogonal matching pursuit algorithm for\ncoefficient recovery and derive the deterministic sparsity threshold under\nwhich recovery of the unique, sparsest coefficient vector is possible. We\nquantify the order complexity of the algorithms, and examine their performance\nin exact and approximate recovery of coefficients under various conditions of\nnoise. Furthermore, we also obtain their empirical phase transition\ncharacteristics. We show that the basis pursuit algorithm, with partial\nnon-negative constraints, and the proposed greedy algorithm perform better in\nrecovering the unique sparse representation when compared to their\nunconstrained counterparts. Finally, we demonstrate the utility of the proposed\nmethods in recovering images corrupted by saturation noise.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 04:33:14 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 20:22:33 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "1303.4756", "submitter": "Zhaoshi Meng", "authors": "Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred O. Hero III", "title": "Marginal Likelihoods for Distributed Parameter Estimation of Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2350956", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed estimation of the inverse covariance matrix, also\ncalled the concentration or precision matrix, in Gaussian graphical models.\nTraditional centralized estimation often requires global inference of the\ncovariance matrix, which can be computationally intensive in large dimensions.\nApproximate inference based on message-passing algorithms, on the other hand,\ncan lead to unstable and biased estimation in loopy graphical models. In this\npaper, we propose a general framework for distributed estimation based on a\nmaximum marginal likelihood (MML) approach. This approach computes local\nparameter estimates by maximizing marginal likelihoods defined with respect to\ndata collected from local neighborhoods. Due to the non-convexity of the MML\nproblem, we introduce and solve a convex relaxation. The local estimates are\nthen combined into a global estimate without the need for iterative\nmessage-passing between neighborhoods. The proposed algorithm is naturally\nparallelizable and computationally efficient, thereby making it suitable for\nhigh-dimensional problems. In the classical regime where the number of\nvariables $p$ is fixed and the number of samples $T$ increases to infinity, the\nproposed estimator is shown to be asymptotically consistent and to improve\nmonotonically as the local neighborhood size increases. In the high-dimensional\nscaling regime where both $p$ and $T$ increase to infinity, the convergence\nrate to the true parameters is derived and is seen to be comparable to\ncentralized maximum likelihood estimation. Extensive numerical experiments\ndemonstrate the improved performance of the two-hop version of the proposed\nestimator, which suffices to almost close the gap to the centralized maximum\nlikelihood estimator at a reduced computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 20:34:47 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2014 18:58:21 GMT"}, {"version": "v3", "created": "Fri, 16 May 2014 22:14:49 GMT"}, {"version": "v4", "created": "Tue, 3 Jun 2014 21:05:14 GMT"}, {"version": "v5", "created": "Tue, 24 Jun 2014 16:49:58 GMT"}, {"version": "v6", "created": "Wed, 13 Aug 2014 16:16:16 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Meng", "Zhaoshi", ""], ["Wei", "Dennis", ""], ["Wiesel", "Ami", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1303.4778", "submitter": "Eva Dyer", "authors": "Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk", "title": "Greedy Feature Selection for Subspace Clustering", "comments": "32 pages, 7 figures, 1 table", "journal-ref": "Journal of Machine Learning Research, Vol.14, Issue 1, pp.\n  2487-2517, January 2013", "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unions of subspaces provide a powerful generalization to linear subspace\nmodels for collections of high-dimensional data. To learn a union of subspaces\nfrom a collection of data, sets of signals in the collection that belong to the\nsame subspace must be identified in order to obtain accurate estimates of the\nsubspace structures present in the data. Recently, sparse recovery methods have\nbeen shown to provide a provable and robust strategy for exact feature\nselection (EFS)--recovering subsets of points from the ensemble that live in\nthe same subspace. In parallel with recent studies of EFS with L1-minimization,\nin this paper, we develop sufficient conditions for EFS with a greedy method\nfor sparse signal recovery known as orthogonal matching pursuit (OMP).\nFollowing our analysis, we provide an empirical study of feature selection\nstrategies for signals living on unions of subspaces and characterize the gap\nbetween sparse recovery methods and nearest neighbor (NN)-based approaches. In\nparticular, we demonstrate that sparse recovery methods provide significant\nadvantages over NN methods and the gap between the two approaches is\nparticularly pronounced when the sampling of subspaces in the dataset is\nsparse. Our results suggest that OMP may be employed to reliably recover exact\nfeature sets in a number of regimes where NN approaches fail to reveal the\nsubspace membership of points in the ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 22:17:20 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 19:07:34 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Dyer", "Eva L.", ""], ["Sankaranarayanan", "Aswin C.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1303.4805", "submitter": "Jabed Hossain Tomal", "authors": "Jabed H. Tomal, William J. Welch, Ruben H. Zamar", "title": "Ensembling classification models based on phalanxes of variables with\n  applications in drug discovery", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS778 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 69-93", "doi": "10.1214/14-AOAS778", "report-no": "IMS-AOAS-AOAS778", "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical detection of a rare class of objects in a two-class\nclassification problem can pose several challenges. Because the class of\ninterest is rare in the training data, there is relatively little information\nin the known class response labels for model building. At the same time the\navailable explanatory variables are often moderately high dimensional. In the\nfour assays of our drug-discovery application, compounds are active or not\nagainst a specific biological target, such as lung cancer tumor cells, and\nactive compounds are rare. Several sets of chemical descriptor variables from\ncomputational chemistry are available to classify the active versus inactive\nclass; each can have up to thousands of variables characterizing molecular\nstructure of the compounds. The statistical challenge is to make use of the\nrichness of the explanatory variables in the presence of scant response\ninformation. Our algorithm divides the explanatory variables into subsets\nadaptively and passes each subset to a base classifier. The various base\nclassifiers are then ensembled to produce one model to rank new objects by\ntheir estimated probabilities of belonging to the rare class of interest. The\nessence of the algorithm is to choose the subsets such that variables in the\nsame group work well together; we call such groups phalanxes.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 01:23:50 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2013 18:13:27 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2014 17:12:23 GMT"}, {"version": "v4", "created": "Fri, 15 May 2015 04:36:39 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Tomal", "Jabed H.", ""], ["Welch", "William J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1303.4996", "submitter": "Henrik Ohlsson", "authors": "Henrik Ohlsson, Yonina C. Eldar, Allen Y. Yang and S. Shankar Sastry", "title": "Compressive Shift Retrieval", "comments": "Submitted to IEEE Transactions on Signal Processing. Accepted to the\n  38th International Conference on Acoustics, Speech, and Signal Processing\n  (ICASSP), Vancouver, Canada, May 2013", "journal-ref": null, "doi": "10.1109/TSP.2014.2332974", "report-no": null, "categories": "cs.SY cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical shift retrieval problem considers two signals in vector form\nthat are related by a shift. The problem is of great importance in many\napplications and is typically solved by maximizing the cross-correlation\nbetween the two signals. Inspired by compressive sensing, in this paper, we\nseek to estimate the shift directly from compressed signals. We show that under\ncertain conditions, the shift can be recovered using fewer samples and less\ncomputation compared to the classical setup. Of particular interest is shift\nestimation from Fourier coefficients. We show that under rather mild conditions\nonly one Fourier coefficient suffices to recover the true shift.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 17:16:33 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2013 05:00:50 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Ohlsson", "Henrik", ""], ["Eldar", "Yonina C.", ""], ["Yang", "Allen Y.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1303.5145", "submitter": "Karthik Mohan", "authors": "Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee", "title": "Node-Based Learning of Multiple Gaussian Graphical Models", "comments": "42 pages, 16 figures. Accepted to JMLR, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 02:10:10 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 05:31:35 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2013 04:14:09 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2014 21:30:33 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Mohan", "Karthik", ""], ["London", "Palma", ""], ["Fazel", "Maryam", ""], ["Witten", "Daniela", ""], ["Lee", "Su-In", ""]]}, {"id": "1303.5197", "submitter": "Yoann Isaac", "authors": "Yoann Isaac, Quentin Barth\\'elemy, Jamal Atif, C\\'edric Gouy-Pailler,\n  Mich\\`ele Sebag", "title": "Multi-dimensional sparse structured signal approximation using split\n  Bregman iterations", "comments": "5 pages, ICASSP 2013 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the sparse approximation of signals using overcomplete\nrepresentations, such that it preserves the (prior) structure of\nmulti-dimensional signals. The underlying optimization problem is tackled using\na multi-dimensional split Bregman optimization approach. An extensive empirical\nevaluation shows how the proposed approach compares to the state of the art\ndepending on the signal features.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 09:13:23 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 14:45:23 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 12:54:19 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Isaac", "Yoann", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Atif", "Jamal", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Sebag", "Mich\u00e8le", ""]]}, {"id": "1303.5244", "submitter": "Martin Kleinsteuber", "authors": "Simon Hawe, Matthias Seibert, and Martin Kleinsteuber", "title": "Separable Dictionary Learning", "comments": "12 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques in computer vision, machine learning, and statistics rely on\nthe fact that a signal of interest admits a sparse representation over some\ndictionary. Dictionaries are either available analytically, or can be learned\nfrom a suitable training set. While analytic dictionaries permit to capture the\nglobal structure of a signal and allow a fast implementation, learned\ndictionaries often perform better in applications as they are more adapted to\nthe considered class of signals. In imagery, unfortunately, the numerical\nburden for (i) learning a dictionary and for (ii) employing the dictionary for\nreconstruction tasks only allows to deal with relatively small image patches\nthat only capture local image information. The approach presented in this paper\naims at overcoming these drawbacks by allowing a separable structure on the\ndictionary throughout the learning process. On the one hand, this permits\nlarger patch-sizes for the learning phase, on the other hand, the dictionary is\napplied efficiently in reconstruction tasks. The learning procedure is based on\noptimizing over a product of spheres which updates the dictionary as a whole,\nthus enforces basic dictionary properties such as mutual coherence explicitly\nduring the learning procedure. In the special case where no separable structure\nis enforced, our method competes with state-of-the-art dictionary learning\nmethods like K-SVD.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 12:40:05 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Hawe", "Simon", ""], ["Seibert", "Matthias", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1303.5508", "submitter": "George Chen", "authors": "George H. Chen, Christian Wachinger, Polina Golland", "title": "Sparse Projections of Medical Images onto Manifolds", "comments": "International Conference on Information Processing in Medical Imaging\n  (IPMI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning has been successfully applied to a variety of medical\nimaging problems. Its use in real-time applications requires fast projection\nonto the low-dimensional space. To this end, out-of-sample extensions are\napplied by constructing an interpolation function that maps from the input\nspace to the low-dimensional manifold. Commonly used approaches such as the\nNystr\\\"{o}m extension and kernel ridge regression require using all training\npoints. We propose an interpolation function that only depends on a small\nsubset of the input training data. Consequently, in the testing phase each new\npoint only needs to be compared against a small number of input training data\nin order to project the point onto the low-dimensional space. We interpret our\nmethod as an out-of-sample extension that approximates kernel ridge regression.\nOur method involves solving a simple convex optimization problem and has the\nattractive property of guaranteeing an upper bound on the approximation error,\nwhich is crucial for medical applications. Tuning this error bound controls the\nsparsity of the resulting interpolation function. We illustrate our method in\ntwo clinical applications that require fast mapping of input images onto a\nlow-dimensional space.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 03:24:10 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 19:21:33 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Chen", "George H.", ""], ["Wachinger", "Christian", ""], ["Golland", "Polina", ""]]}, {"id": "1303.5588", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, James V. Burke, and Gianluigi Pillonetto", "title": "Robust and Trend Following Student's t Kalman Smoothers", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Kalman smoothing framework based on modeling errors using the\nheavy tailed Student's t distribution, along with algorithms, convergence\ntheory, open-source general implementation, and several important applications.\nThe computational effort per iteration grows linearly with the length of the\ntime series, and all smoothers allow nonlinear process and measurement models.\n  Robust smoothers form an important subclass of smoothers within this\nframework. These smoothers work in situations where measurements are highly\ncontaminated by noise or include data unexplained by the forward model. Highly\nrobust smoothers are developed by modeling measurement errors using the\nStudent's t distribution, and outperform the recently proposed L1-Laplace\nsmoother in extreme situations with data containing 20% or more outliers.\n  A second special application we consider in detail allows tracking sudden\nchanges in the state. It is developed by modeling process noise using the\nStudent's t distribution, and the resulting smoother can track sudden changes\nin the state.\n  These features can be used separately or in tandem, and we present a general\nsmoother algorithm and open source implementation, together with convergence\nanalysis that covers a wide range of smoothers. A key ingredient of our\napproach is a technique to deal with the non-convexity of the Student's t loss\nfunction. Numerical results for linear and nonlinear models illustrate the\nperformance of the new smoothers for robust and tracking applications, as well\nas for mixed problems that have both types of features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 11:36:19 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1303.5613", "submitter": "Steven Smith", "authors": "Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, and\n  Garrett Bernstein", "title": "Network Detection Theory and Performance", "comments": "Submitted to IEEE Trans. Signal Processing", "journal-ref": "IEEE Trans. Signal Process., vol. 62, no. 20, pp. 5324-5338,\n  October 2014", "doi": "10.1109/TSP.2014.2336613", "report-no": null, "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network detection is an important capability in many areas of applied\nresearch in which data can be represented as a graph of entities and\nrelationships. Oftentimes the object of interest is a relatively small subgraph\nin an enormous, potentially uninteresting background. This aspect characterizes\nnetwork detection as a \"big data\" problem. Graph partitioning and network\ndiscovery have been major research areas over the last ten years, driven by\ninterest in internet search, cyber security, social networks, and criminal or\nterrorist activities. The specific problem of network discovery is addressed as\na special case of graph partitioning in which membership in a small subgraph of\ninterest must be determined. Algebraic graph theory is used as the basis to\nanalyze and compare different network detection methods. A new Bayesian network\ndetection framework is introduced that partitions the graph based on prior\ninformation and direct observations. The new approach, called space-time threat\npropagation, is proved to maximize the probability of detection and is\ntherefore optimum in the Neyman-Pearson sense. This optimality criterion is\ncompared to spectral community detection approaches which divide the global\ngraph into subsets or communities with optimal connectivity properties. We also\nexplore a new generative stochastic model for covert networks and analyze using\nreceiver operating characteristics the detection performance of both classes of\noptimal detection techniques.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 13:34:28 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Smith", "Steven T.", ""], ["Senne", "Kenneth D.", ""], ["Philips", "Scott", ""], ["Kao", "Edward K.", ""], ["Bernstein", "Garrett", ""]]}, {"id": "1303.5685", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Andrew E. Waters, Christoph Studer and Richard G.\n  Baraniuk", "title": "Sparse Factor Analysis for Learning and Content Analytics", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 15, pp. 1959-2008,\n  June, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new model and algorithms for machine learning-based learning\nanalytics, which estimate a learner's knowledge of the concepts underlying a\ndomain, and content analytics, which estimate the relationships among a\ncollection of questions and those concepts. Our model represents the\nprobability that a learner provides the correct response to a question in terms\nof three factors: their understanding of a set of underlying concepts, the\nconcepts involved in each question, and each question's intrinsic difficulty.\nWe estimate these factors given the graded responses to a collection of\nquestions. The underlying estimation problem is ill-posed in general,\nespecially when only a subset of the questions are answered. The key\nobservation that enables a well-posed solution is the fact that typical\neducational domains of interest involve only a small number of key concepts.\nLeveraging this observation, we develop both a bi-convex maximum-likelihood and\na Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.\nWe also incorporate user-defined tags on questions to facilitate the\ninterpretability of the estimated factors. Experiments with synthetic and\nreal-world data demonstrate the efficacy of our approach. Finally, we make a\nconnection between SPARFA and noisy, binary-valued (1-bit) dictionary learning\nthat is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 18:44:56 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 20:33:18 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Waters", "Andrew E.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1303.5913", "submitter": "Marcus Chen", "authors": "Marcus Chen, Cham Tat Jen, Pang Sze Kim, Alvina Goh", "title": "A Diffusion Process on Riemannian Manifold for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual tracking for long video sequences is a research area that has\nmany important applications. The main challenges include how the target image\ncan be modeled and how this model can be updated. In this paper, we model the\ntarget using a covariance descriptor, as this descriptor is robust to problems\nsuch as pixel-pixel misalignment, pose and illumination changes, that commonly\noccur in visual tracking. We model the changes in the template using a\ngenerative process. We introduce a new dynamical model for the template update\nusing a random walk on the Riemannian manifold where the covariance descriptors\nlie in. This is done using log-transformed space of the manifold to free the\nconstraints imposed inherently by positive semidefinite matrices. Modeling\ntemplate variations and poses kinetics together in the state space enables us\nto jointly quantify the uncertainties relating to the kinematic states and the\ntemplate in a principled way. Finally, the sequential inference of the\nposterior distribution of the kinematic states and the template is done using a\nparticle filter. Our results shows that this principled approach can be robust\nto changes in illumination, poses and spatial affine transformation. In the\nexperiments, our method outperformed the current state-of-the-art algorithm -\nthe incremental Principal Component Analysis method, particularly when a target\nunderwent fast poses changes and also maintained a comparable performance in\nstable target tracking cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 04:55:40 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Chen", "Marcus", ""], ["Jen", "Cham Tat", ""], ["Kim", "Pang Sze", ""], ["Goh", "Alvina", ""]]}, {"id": "1303.5976", "submitter": "Lorenzo Rosasco", "authors": "Silvia Villa, Lorenzo Rosasco and Tomaso Poggio", "title": "On Learnability, Complexity and Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental question of learnability of a hypotheses class in\nthe supervised learning setting and in the general learning setting introduced\nby Vladimir Vapnik. We survey classic results characterizing learnability in\nterm of suitable notions of complexity, as well as more recent results that\nestablish the connection between learnability and stability of a learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 18:32:38 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Villa", "Silvia", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1303.5984", "submitter": "Adel Javanmard", "authors": "Morteza Ibrahimi and Adel Javanmard and Benjamin Van Roy", "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic\n  Systems", "comments": "16 pages", "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 2012:\n  2645-2653", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of adaptive control of a high dimensional linear\nquadratic (LQ) system. Previous work established the asymptotic convergence to\nan optimal controller for various adaptive control schemes. More recently, for\nthe average cost LQ problem, a regret bound of ${O}(\\sqrt{T})$ was shown, apart\nform logarithmic factors. However, this bound scales exponentially with $p$,\nthe dimension of the state space. In this work we consider the case where the\nmatrices describing the dynamic of the LQ system are sparse and their\ndimensions are large. We present an adaptive control scheme that achieves a\nregret bound of ${O}(p \\sqrt{T})$, apart from logarithmic factors. In\nparticular, our algorithm has an average cost of $(1+\\eps)$ times the optimum\ncost after $T = \\polylog(p) O(1/\\eps^2)$. This is in comparison to previous\nwork on the dense dynamics where the algorithm requires time that scales\nexponentially with dimension in order to achieve regret of $\\eps$ times the\noptimal cost.\n  We believe that our result has prominent applications in the emerging area of\ncomputational advertising, in particular targeted online advertising and\nadvertising in social networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 19:56:49 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ibrahimi", "Morteza", ""], ["Javanmard", "Adel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1303.6001", "submitter": "Bal\\'azs Szalkai", "authors": "Bal\\'azs Szalkai", "title": "Generalizing k-means for an arbitrary distance matrix", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original k-means clustering method works only if the exact vectors\nrepresenting the data points are known. Therefore calculating the distances\nfrom the centroids needs vector operations, since the average of abstract data\npoints is undefined. Existing algorithms can be extended for those cases when\nthe sole input is the distance matrix, and the exact representing vectors are\nunknown. This extension may be named relational k-means after a notation for a\nsimilar algorithm invented for fuzzy clustering. A method is then proposed for\ngeneralizing k-means for scenarios when the data points have absolutely no\nconnection with a Euclidean space.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 22:33:15 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Szalkai", "Bal\u00e1zs", ""]]}, {"id": "1303.6086", "submitter": "Massimiliano Pontil", "authors": "Andreas Argyriou, Luca Baldassarre, Charles A. Micchelli, Massimiliano\n  Pontil", "title": "On Sparsity Inducing Regularization Methods for Machine Learning", "comments": "12 pages. arXiv admin note: text overlap with arXiv:1104.1436", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past years there has been an explosion of interest in learning\nmethods based on sparsity regularization. In this paper, we discuss a general\nclass of such methods, in which the regularizer can be expressed as the\ncomposition of a convex function $\\omega$ with a linear function. This setting\nincludes several methods such the group Lasso, the Fused Lasso, multi-task\nlearning and many more. We present a general approach for solving\nregularization problems of this kind, under the assumption that the proximity\noperator of the function $\\omega$ is available. Furthermore, we comment on the\napplication of this approach to support vector machines, a technique pioneered\nby the groundbreaking work of Vladimir Vapnik.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 11:09:08 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Argyriou", "Andreas", ""], ["Baldassarre", "Luca", ""], ["Micchelli", "Charles A.", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1303.6223", "submitter": "Rajen Shah", "authors": "Rajen Dinesh Shah, Nicolai Meinshausen", "title": "Random Intersection Trees", "comments": "23 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research 15 (2014) 629-654", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 17:29:24 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Shah", "Rajen Dinesh", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1303.6370", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki", "title": "Convex Tensor Decomposition via Structured Schatten Norm Regularization", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss structured Schatten norms for tensor decomposition that includes\ntwo recently proposed norms (\"overlapped\" and \"latent\") for\nconvex-optimization-based tensor decomposition, and connect tensor\ndecomposition with wider literature on structured sparsity. Based on the\nproperties of the structured Schatten norms, we mathematically analyze the\nperformance of \"latent\" approach for tensor decomposition, which was\nempirically found to perform better than the \"overlapped\" approach in some\nsettings. We show theoretically that this is indeed the case. In particular,\nwhen the unknown true tensor is low-rank in a specific mode, this approach\nperforms as good as knowing the mode with the smallest rank. Along the way, we\nshow a novel duality result for structures Schatten norms, establish the\nconsistency, and discuss the identifiability of this approach. We confirm\nthrough numerical simulations that our theoretical prediction can precisely\npredict the scaling behavior of the mean squared error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 02:36:49 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1303.6746", "submitter": "Matthew W. Hoffman", "authors": "Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas", "title": "Exploiting correlation and budget constraints in Bayesian multi-armed\n  bandit optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding the maximizer of a nonlinear smooth\nfunction, that can only be evaluated point-wise, subject to constraints on the\nnumber of permitted function evaluations. This problem is also known as\nfixed-budget best arm identification in the multi-armed bandit literature. We\nintroduce a Bayesian approach for this problem and show that it empirically\noutperforms both the existing frequentist counterpart and other Bayesian\noptimization methods. The Bayesian approach places emphasis on detailed\nmodelling, including the modelling of correlations among the arms. As a result,\nit can perform well in situations where the number of arms is much larger than\nthe number of allowed function evaluation, whereas the frequentist counterpart\nis inapplicable. This feature enables us to develop and deploy practical\napplications, such as automatic machine learning toolboxes. The paper presents\ncomprehensive comparisons of the proposed approach, Thompson sampling,\nclassical Bayesian optimization techniques, more recent Bayesian bandit\napproaches, and state-of-the-art best arm identification methods. This is the\nfirst comparison of many of these methods in the literature and allows us to\nexamine the relative merits of their different features.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 06:17:09 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 21:17:52 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2013 12:10:07 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 10:52:24 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Hoffman", "Matthew W.", ""], ["Shahriari", "Bobak", ""], ["de Freitas", "Nando", ""]]}, {"id": "1303.6750", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "Sequential testing over multiple stages and performance analysis of data\n  fusion", "comments": "SPIE Signal Processing, Sensor Fusion and Target Recognition XXII", "journal-ref": "Proc. SPIE Vol 8745, 87450S (2013)", "doi": "10.1117/12.2017754", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a methodology for modeling the performance of decision-level data\nfusion between different sensor configurations, implemented as part of the\nJIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian network\nformulation of classical probabilistic data fusion, which allows elementary\nfusion structures to be stacked and analyzed efficiently. We then present an\nextension of the Wald sequential test for combining the outputs of the Bayesian\nnetwork over time. We discuss an algorithm to compute its performance\nstatistics and illustrate the approach on some examples. This variant of the\nsequential test involves multiple, distinct stages, where the evidence\naccumulated from each stage is carried over into the next one, and is motivated\nby a need to keep certain sensors in the network inactive unless triggered by\nother sensors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 06:53:26 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1303.6811", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Sparse approximation and recovery by greedy algorithms in Banach spaces", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.3595", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse approximation by greedy algorithms. We prove the\nLebesgue-type inequalities for the Weak Chebyshev Greedy Algorithm (WCGA), a\ngeneralization of the Weak Orthogonal Matching Pursuit to the case of a Banach\nspace. The main novelty of these results is a Banach space setting instead of a\nHilbert space setting. The results are proved for redundant dictionaries\nsatisfying certain conditions. Then we apply these general results to the case\nof bases. In particular, we prove that the WCGA provides almost optimal sparse\napproximation for the trigonometric system in $L_p$, $2\\le p<\\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 12:45:48 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}, {"id": "1303.6935", "submitter": "Xiaocheng  Tang", "authors": "Xiaocheng Tang and Katya Scheinberg", "title": "Efficiently Using Second Order Information in Large l1 Regularization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel general algorithm LHAC that efficiently uses second-order\ninformation to train a class of large-scale l1-regularized problems. Our method\nexecutes cheap iterations while achieving fast local convergence rate by\nexploiting the special structure of a low-rank matrix, constructed via\nquasi-Newton approximation of the Hessian of the smooth loss function. A greedy\nactive-set strategy, based on the largest violations in the dual constraints,\nis employed to maintain a working set that iteratively estimates the complement\nof the optimal active set. This allows for smaller size of subproblems and\neventually identifies the optimal active set. Empirical comparisons confirm\nthat LHAC is highly competitive with several recently proposed state-of-the-art\nspecialized solvers for sparse logistic regression and sparse inverse\ncovariance matrix selection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:34:05 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Tang", "Xiaocheng", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1303.6938", "submitter": "Pasi Jyl\\\"anki", "authors": "Pasi Jyl\\\"anki, Aapo Nummenmaa and Aki Vehtari", "title": "Expectation Propagation for Neural Networks with Sparsity-promoting\n  Priors", "comments": null, "journal-ref": "Journal of Machine Learning Research, 15(May): 1849-1901, 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for nonlinear regression using a two-layer neural\nnetwork (NN) model structure with sparsity-favoring hierarchical priors on the\nnetwork weights. We present an expectation propagation (EP) approach for\napproximate integration over the posterior distribution of the weights, the\nhierarchical scale parameters of the priors, and the residual scale. Using a\nfactorized posterior approximation we derive a computationally efficient\nalgorithm, whose complexity scales similarly to an ensemble of independent\nsparse linear models. The approach enables flexible definition of weight priors\nwith different sparseness properties such as independent Laplace priors with a\ncommon scale parameter or Gaussian automatic relevance determination (ARD)\npriors with different relevance parameters for all inputs. The approach can be\nextended beyond standard activation functions and NN model structures to form\nflexible nonlinear predictors from multiple sparse linear models. The effects\nof the hierarchical priors and the predictive performance of the algorithm are\nassessed using both simulated and real-world data. Comparisons are made to two\nalternative models with ARD priors: a Gaussian process with a NN covariance\nfunction and marginal maximum a posteriori estimates of the relevance\nparameters, and a NN with Markov chain Monte Carlo integration over all the\nunknown model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:40:26 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Jyl\u00e4nki", "Pasi", ""], ["Nummenmaa", "Aapo", ""], ["Vehtari", "Aki", ""]]}, {"id": "1303.6977", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis, Nikolaos Tziortziotis", "title": "ABC Reinforcement Learning", "comments": "Corrected version of paper appearing in ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple, general framework for likelihood-free\nBayesian reinforcement learning, through Approximate Bayesian Computation\n(ABC). The main advantage is that we only require a prior distribution on a\nclass of simulators (generative models). This is useful in domains where an\nanalytical probabilistic model of the underlying process is too complex to\nformulate, but where detailed simulation models are available. ABC-RL allows\nthe use of any Bayesian reinforcement learning technique, even in this case. In\naddition, it can be seen as an extension of rollout algorithms to the case\nwhere we do not know what the correct model to draw rollouts from is. We\nexperimentally demonstrate the potential of this approach in a comparison with\nLSPI. Finally, we introduce a theorem showing that ABC is a sound methodology\nin principle, even when non-sufficient statistics are used.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 20:51:33 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 12:54:53 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 09:42:59 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2013 11:18:26 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Tziortziotis", "Nikolaos", ""]]}, {"id": "1303.7093", "submitter": "Aravind Kota Gopalakrishna", "authors": "Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J.\n  Lukkien", "title": "Relevance As a Metric for Evaluating Machine Learning Algorithms", "comments": "To Appear at International Conference on Machine Learning and Data\n  Mining (MLDM 2013), 14 pages, 6 figures", "journal-ref": null, "doi": "10.1007/978-3-642-39712-7_15", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the choice of a learning algorithm that is suitable for\nthe application domain is critical. The performance metric used to compare\ndifferent algorithms must also reflect the concerns of users in the application\ndomain under consideration. In this work, we propose a novel probability-based\nperformance metric called Relevance Score for evaluating supervised learning\nalgorithms. We evaluate the proposed metric through empirical analysis on a\ndataset gathered from an intelligent lighting pilot installation. In comparison\nto the commonly used Classification Accuracy metric, the Relevance Score proves\nto be more appropriate for a certain class of applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 11:01:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 19:12:06 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 14:26:49 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Gopalakrishna", "Aravind Kota", ""], ["Ozcelebi", "Tanir", ""], ["Liotta", "Antonio", ""], ["Lukkien", "Johan J.", ""]]}, {"id": "1303.7226", "submitter": "Yudong Chen", "authors": "Yudong Chen, Vikas Kawadia, Rahul Urgaonkar", "title": "Detecting Overlapping Temporal Community Structure in Time-Evolving\n  Networks", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled approach for detecting overlapping temporal community\nstructure in dynamic networks. Our method is based on the following framework:\nfind the overlapping temporal community structure that maximizes a quality\nfunction associated with each snapshot of the network subject to a temporal\nsmoothness constraint. A novel quality function and a smoothness constraint are\nproposed to handle overlaps, and a new convex relaxation is used to solve the\nresulting combinatorial optimization problem. We provide theoretical guarantees\nas well as experimental results that reveal community structure in real and\nsynthetic networks. Our main insight is that certain structures can be\nidentified only when temporal correlation is considered and when communities\nare allowed to overlap. In general, discovering such overlapping temporal\ncommunity structure can enhance our understanding of real-world complex\nnetworks by revealing the underlying stability behind their seemingly chaotic\nevolution.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 19:56:39 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Chen", "Yudong", ""], ["Kawadia", "Vikas", ""], ["Urgaonkar", "Rahul", ""]]}, {"id": "1303.7264", "submitter": "Yaojia Zhu", "authors": "Yaojia Zhu, Xiaoran Yan, Lise Getoor and Cristopher Moore", "title": "Scalable Text and Link Analysis with Mixed-Topic Link Models", "comments": "11 pages, 4 figures", "journal-ref": "Proc. 19th SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD) 2013, 473-481", "doi": "10.1145/2487575.2487693", "report-no": null, "categories": "cs.LG cs.IR cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets contain rich information about objects, as well as pairwise\nrelations between them. For instance, in networks of websites, scientific\npapers, and other documents, each node has content consisting of a collection\nof words, as well as hyperlinks or citations to other nodes. In order to\nperform inference on such data sets, and make predictions and recommendations,\nit is useful to have models that are able to capture the processes which\ngenerate the text at each node and the links between them. In this paper, we\ncombine classic ideas in topic modeling with a variant of the mixed-membership\nblock model recently developed in the statistical physics community. The\nresulting model has the advantage that its parameters, including the mixture of\ntopics of each document and the resulting overlapping communities, can be\ninferred with a simple and scalable expectation-maximization algorithm. We test\nour model on three data sets, performing unsupervised topic classification and\nlink prediction. For both tasks, our model outperforms several existing\nstate-of-the-art methods, achieving higher accuracy with significantly less\ncomputation, analyzing a data set with 1.3 million words and 44 thousand links\nin a few minutes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 22:34:51 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Zhu", "Yaojia", ""], ["Yan", "Xiaoran", ""], ["Getoor", "Lise", ""], ["Moore", "Cristopher", ""]]}, {"id": "1303.7286", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "On the symmetrical Kullback-Leibler Jeffreys centroids", "comments": "17 pages, 1 figure, source code in R", "journal-ref": "IEEE Signal Processing Letters (Volume:20 , Issue: 7 ), pp.\n  657-660, 2013", "doi": "10.1109/LSP.2013.2260538", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the success of the bag-of-word modeling paradigm, clustering\nhistograms has become an important ingredient of modern information processing.\nClustering histograms can be performed using the celebrated $k$-means\ncentroid-based algorithm. From the viewpoint of applications, it is usually\nrequired to deal with symmetric distances. In this letter, we consider the\nJeffreys divergence that symmetrizes the Kullback-Leibler divergence, and\ninvestigate the computation of Jeffreys centroids. We first prove that the\nJeffreys centroid can be expressed analytically using the Lambert $W$ function\nfor positive histograms. We then show how to obtain a fast guaranteed\napproximation when dealing with frequency histograms. Finally, we conclude with\nsome remarks on the $k$-means histogram clustering.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 03:11:21 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 06:01:08 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2014 05:35:12 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1303.7297", "submitter": "Tomonari Sei", "authors": "Tomonari Sei", "title": "Infinitely imbalanced binomial regression and deformed exponential\n  families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logistic regression model is known to converge to a Poisson point process\nmodel if the binary response tends to infinitely imbalanced. In this paper, it\nis shown that this phenomenon is universal in a wide class of link functions on\nbinomial regression. The proof relies on the extreme value theory. For the\nlogit, probit and complementary log-log link functions, the intensity measure\nof the point process becomes an exponential family. For some other link\nfunctions, deformed exponential families appear. A penalized maximum likelihood\nestimator for the Poisson point process model is suggested.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 05:37:51 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2013 17:32:52 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Sei", "Tomonari", ""]]}, {"id": "1303.7410", "submitter": "Shohei Shimizu", "authors": "Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio", "title": "ParceLiNGAM: A causal ordering method robust against latent confounders", "comments": "A revised version of this was accepted in Neural Computation. 18\n  pages and 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1204.1795", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning a causal ordering of variables in a linear non-Gaussian\nacyclic model called LiNGAM. Several existing methods have been shown to\nconsistently estimate a causal ordering assuming that all the model assumptions\nare correct. But, the estimation results could be distorted if some assumptions\nactually are violated. In this paper, we propose a new algorithm for learning\ncausal orders that is robust against one typical violation of the model\nassumptions: latent confounders. The key idea is to detect latent confounders\nby testing independence between estimated external influences and find subsets\n(parcels) that include variables that are not affected by latent confounders.\nWe demonstrate the effectiveness of our method using artificial data and\nsimulated brain imaging data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 14:40:24 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 03:11:13 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Tashiro", "Tatsuya", ""], ["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Washio", "Takashi", ""]]}, {"id": "1303.7461", "submitter": "Guido F.  Montufar", "authors": "Guido F. Mont\\'ufar", "title": "Universal Approximation Depth and Errors of Narrow Belief Networks with\n  Discrete Units", "comments": "19 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize recent theoretical work on the minimal number of layers of\nnarrow deep belief networks that can approximate any probability distribution\non the states of their visible units arbitrarily well. We relax the setting of\nbinary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010;\nMont\\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and the\nvanishing approximation error to an arbitrary approximation error tolerance.\nFor example, we show that a $q$-ary deep belief network with $L\\geq\n2+\\frac{q^{\\lceil m-\\delta \\rceil}-1}{q-1}$ layers of width $n \\leq m +\n\\log_q(m) + 1$ for some $m\\in \\mathbb{N}$ can approximate any probability\ndistribution on $\\{0,1,\\ldots,q-1\\}^n$ without exceeding a Kullback-Leibler\ndivergence of $\\delta$. Our analysis covers discrete restricted Boltzmann\nmachines and na\\\"ive Bayes models as special cases.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 19:15:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 21:50:07 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Mont\u00fafar", "Guido F.", ""]]}, {"id": "1303.7474", "submitter": "Matthew Anderson", "authors": "Matthew Anderson, Geng-Shen Fu, Ronald Phlypo, and T\\\"ulay Adal{\\i}", "title": "Independent Vector Analysis: Identification Conditions and Performance\n  Bounds", "comments": "14 pages, 5 figures, in review for IEEE Trans. on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2333554", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an extension of independent component analysis (ICA) from one to\nmultiple datasets, termed independent vector analysis (IVA), has been the\nsubject of significant research interest. IVA has also been shown to be a\ngeneralization of Hotelling's canonical correlation analysis. In this paper, we\nprovide the identification conditions for a general IVA formulation, which\naccounts for linear, nonlinear, and sample-to-sample dependencies. The\nidentification conditions are a generalization of previous results for ICA and\nfor IVA when samples are independently and identically distributed.\nFurthermore, a principal aim of IVA is the identification of dependent sources\nbetween datasets. Thus, we provide the additional conditions for when the\narbitrary ordering of the sources within each dataset is common. Performance\nbounds in terms of the Cramer-Rao lower bound are also provided for the\ndemixing matrices and interference to source ratio. The performance of two IVA\nalgorithms are compared to the theoretical bounds.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 19:52:31 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Anderson", "Matthew", ""], ["Fu", "Geng-Shen", ""], ["Phlypo", "Ronald", ""], ["Adal\u0131", "T\u00fclay", ""]]}]