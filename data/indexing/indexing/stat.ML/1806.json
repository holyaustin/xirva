[{"id": "1806.00003", "submitter": "Rudrasis Chakraborty Mr", "authors": "Rudrasis Chakraborty, Chun-Hao Yang and Baba C. Vemuri", "title": "A mixture model for aggregation of multiple pre-trained weak classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have gained immense popularity in Computer Vision and other\nfields in the past few years due to their remarkable performance on\nrecognition/classification tasks surpassing the state-of-the art. One of the\nkeys to their success lies in the richness of the automatically learned\nfeatures. In order to get very good accuracy, one popular option is to increase\nthe depth of the network. Training such a deep network is however infeasible or\nimpractical with moderate computational resources and budget. The other\nalternative to increase the performance is to learn multiple weak classifiers\nand boost their performance using a boosting algorithm or a variant thereof.\nBut, one of the problems with boosting algorithms is that they require a\nre-training of the networks based on the misclassified samples. Motivated by\nthese problems, in this work we propose an aggregation technique which combines\nthe output of multiple weak classifiers. We formulate the aggregation problem\nusing a mixture model fitted to the trained classifier outputs. Our model does\nnot require any re-training of the `weak' networks and is computationally very\nfast (takes $<30$ seconds to run in our experiments). Thus, using a less\nexpensive training stage and without doing any re-training of networks, we\nexperimentally demonstrate that it is possible to boost the performance by\n$12\\%$. Furthermore, we present experiments using hand-crafted features and\nimproved the classification performance using the proposed aggregation\ntechnique. One of the major advantages of our framework is that our framework\nallows one to combine features that are very likely to be of distinct\ndimensions since they are extracted using different networks/algorithms. Our\nexperimental results demonstrate a significant performance gain from the use of\nour aggregation technique at a very small computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 01:56:41 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Yang", "Chun-Hao", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1806.00007", "submitter": "Zhi-Hua Zhou", "authors": "Ji Feng, Yang Yu, Zhi-Hua Zhou", "title": "Multi-Layered Gradient Boosting Decision Trees", "comments": null, "journal-ref": "NeurIPS 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layered representation is believed to be the key ingredient of deep\nneural networks especially in cognitive tasks like computer vision. While\nnon-differentiable models such as gradient boosting decision trees (GBDTs) are\nthe dominant methods for modeling discrete or tabular data, they are hard to\nincorporate with such representation learning ability. In this work, we propose\nthe multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring\nthe ability to learn hierarchical representations by stacking several layers of\nregression GBDTs as its building block. The model can be jointly trained by a\nvariant of target propagation across layers, without the need to derive\nback-propagation nor differentiability. Experiments and visualizations\nconfirmed the effectiveness of the model in terms of performance and\nrepresentation learning ability.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:06:18 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Feng", "Ji", ""], ["Yu", "Yang", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1806.00035", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet,\n  Sylvain Gelly", "title": "Assessing Generative Models via Precision and Recall", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generative modeling have led to an increased interest in\nthe study of statistical divergences as means of model comparison. Commonly\nused evaluation methods, such as the Frechet Inception Distance (FID),\ncorrelate well with the perceived quality of samples and are sensitive to mode\ndropping. However, these metrics are unable to distinguish between different\nfailure cases since they only yield one-dimensional scores. We propose a novel\ndefinition of precision and recall for distributions which disentangles the\ndivergence into two separate dimensions. The proposed notion is intuitive,\nretains desirable properties, and naturally leads to an efficient algorithm\nthat can be used to evaluate generative models. We relate this notion to total\nvariation as well as to recent evaluation metrics such as Inception Score and\nFID. To demonstrate the practical utility of the proposed approach we perform\nan empirical study on several variants of Generative Adversarial Networks and\nVariational Autoencoders. In an extensive set of experiments we show that the\nproposed metric is able to disentangle the quality of generated samples from\nthe coverage of the target distribution.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:14:41 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 10:43:26 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1806.00040", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Weihao Kong, Alistair Stewart", "title": "Efficient Algorithms and Lower Bounds for Robust Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional linear regression in a robust model\nwhere an $\\epsilon$-fraction of the samples can be adversarially corrupted. We\nfocus on the fundamental setting where the covariates of the uncorrupted\nsamples are drawn from a Gaussian distribution $\\mathcal{N}(0, \\Sigma)$ on\n$\\mathbb{R}^d$. We give nearly tight upper bounds and computational lower\nbounds for this problem. Specifically, our main contributions are as follows:\n  For the case that the covariance matrix is known to be the identity, we give\na sample near-optimal and computationally efficient algorithm that outputs a\ncandidate hypothesis vector $\\widehat{\\beta}$ which approximates the unknown\nregression vector $\\beta$ within $\\ell_2$-norm $O(\\epsilon \\log(1/\\epsilon)\n\\sigma)$, where $\\sigma$ is the standard deviation of the random observation\nnoise. An error of $\\Omega (\\epsilon \\sigma)$ is information-theoretically\nnecessary, even with infinite sample size. Prior work gave an algorithm for\nthis problem with sample complexity $\\tilde{\\Omega}(d^2/\\epsilon^2)$ whose\nerror guarantee scales with the $\\ell_2$-norm of $\\beta$.\n  For the case of unknown covariance, we show that we can efficiently achieve\nthe same error guarantee as in the known covariance case using an additional\n$\\tilde{O}(d^2/\\epsilon^2)$ unlabeled examples. On the other hand, an error of\n$O(\\epsilon \\sigma)$ can be information-theoretically attained with\n$O(d/\\epsilon^2)$ samples. We prove a Statistical Query (SQ) lower bound\nproviding evidence that this quadratic tradeoff in the sample size is inherent.\nMore specifically, we show that any polynomial time SQ learning algorithm for\nrobust linear regression (in Huber's contamination model) with estimation\ncomplexity $O(d^{2-c})$, where $c>0$ is an arbitrarily small constant, must\nincur an error of $\\Omega(\\sqrt{\\epsilon} \\sigma)$.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:23:29 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kong", "Weihao", ""], ["Stewart", "Alistair", ""]]}, {"id": "1806.00050", "submitter": "Heinrich Jiang", "authors": "Andrew Cotter, Maya Gupta, Heinrich Jiang, James Muller, Taman\n  Narayan, Serena Wang, Tao Zhu", "title": "Interpretable Set Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose learning flexible but interpretable functions that aggregate a\nvariable-length set of permutation-invariant feature vectors to predict a\nlabel. We use a deep lattice network model so we can architect the model\nstructure to enhance interpretability, and add monotonicity constraints between\ninputs-and-outputs. We then use the proposed set function to automate the\nengineering of dense, interpretable features from sparse categorical features,\nwhich we call semantic feature engine. Experiments on real-world data show the\nachieved accuracy is similar to deep sets or deep neural networks, and is\neasier to debug and understand.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:53:15 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Cotter", "Andrew", ""], ["Gupta", "Maya", ""], ["Jiang", "Heinrich", ""], ["Muller", "James", ""], ["Narayan", "Taman", ""], ["Wang", "Serena", ""], ["Zhu", "Tao", ""]]}, {"id": "1806.00054", "submitter": "Taesung Lee", "authors": "Taesung Lee, Benjamin Edwards, Ian Molloy, Dong Su", "title": "Defending Against Machine Learning Model Stealing Attacks Using\n  Deceptive Perturbations", "comments": "Under review for a peer review conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to simple model stealing attacks if\nthe adversary can obtain output labels for chosen inputs. To protect against\nthese attacks, it has been proposed to limit the information provided to the\nadversary by omitting probability scores, significantly impacting the utility\nof the provided service. In this work, we illustrate how a service provider can\nstill provide useful, albeit misleading, class probability information, while\nsignificantly limiting the success of the attack. Our defense forces the\nadversary to discard the class probabilities, requiring significantly more\nqueries before they can train a model with comparable performance. We evaluate\nseveral attack strategies, model architectures, and hyperparameters under\nvarying adversarial models, and evaluate the efficacy of our defense against\nthe strongest adversary. Finally, we quantify the amount of noise injected into\nthe class probabilities to mesure the loss in utility, e.g., adding 1.26 nats\nper query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can\ndegrade the accuracy of the stolen model at least 20%, or require up to 64\ntimes more queries while keeping the accuracy of the protected model almost\nintact.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:09:15 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 18:09:42 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 17:13:47 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 16:41:41 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Lee", "Taesung", ""], ["Edwards", "Benjamin", ""], ["Molloy", "Ian", ""], ["Su", "Dong", ""]]}, {"id": "1806.00064", "submitter": "Zhun Liu", "authors": "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu\n  Liang, Amir Zadeh, Louis-Philippe Morency", "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors", "comments": "* Equal contribution. 10 pages. Accepted by ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal research is an emerging field of artificial intelligence, and one\nof the main research problems in this field is multimodal fusion. The fusion of\nmultimodal data is the process of integrating multiple unimodal representations\ninto one compact multimodal representation. Previous research in this field has\nexploited the expressiveness of tensors for multimodal representation. However,\nthese methods often suffer from exponential increase in dimensions and in\ncomputational complexity introduced by transformation of input into tensor. In\nthis paper, we propose the Low-rank Multimodal Fusion method, which performs\nmultimodal fusion using low-rank tensors to improve efficiency. We evaluate our\nmodel on three different tasks: multimodal sentiment analysis, speaker trait\nanalysis, and emotion recognition. Our model achieves competitive results on\nall these tasks while drastically reducing computational complexity. Additional\nexperiments also show that our model can perform robustly for a wide range of\nlow-rank settings, and is indeed much more efficient in both training and\ninference compared to other methods that utilize tensor representations.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:28:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Liu", "Zhun", ""], ["Shen", "Ying", ""], ["Lakshminarasimhan", "Varun Bharadhwaj", ""], ["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1806.00069", "submitter": "Leilani Gilpin", "authors": "Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael\n  Specter, Lalana Kagal", "title": "Explaining Explanations: An Overview of Interpretability of Machine\n  Learning", "comments": "The 5th IEEE International Conference on Data Science and Advanced\n  Analytics (DSAA 2018). [Research Track]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:48:00 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:03:52 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 21:06:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Gilpin", "Leilani H.", ""], ["Bau", "David", ""], ["Yuan", "Ben Z.", ""], ["Bajwa", "Ayesha", ""], ["Specter", "Michael", ""], ["Kagal", "Lalana", ""]]}, {"id": "1806.00080", "submitter": "Mason A. Porter", "authors": "Ryan Flanagan, Lucas Lacasa, Emma K. Towlson, Sang Hoon Lee, and Mason\n  A. Porter", "title": "Effect of antipsychotics on community structure in functional brain\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph nlin.AO physics.soc-ph q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schizophrenia, a mental disorder that is characterized by abnormal social\nbehavior and failure to distinguish one's own thoughts and ideas from reality,\nhas been associated with structural abnormalities in the architecture of\nfunctional brain networks. Using various methods from network analysis, we\nexamine the effect of two classical therapeutic antipsychotics --- Aripiprazole\nand Sulpiride --- on the structure of functional brain networks of healthy\ncontrols and patients who have been diagnosed with schizophrenia. We compare\nthe community structures of functional brain networks of different individuals\nusing mesoscopic response functions, which measure how community structure\nchanges across different scales of a network. We are able to do a reasonably\ngood job of distinguishing patients from controls, and we are most successful\nat this task on people who have been treated with Aripiprazole. We demonstrate\nthat this increased separation between patients and controls is related only to\na change in the control group, as the functional brain networks of the patient\ngroup appear to be predominantly unaffected by this drug. This suggests that\nAripiprazole has a significant and measurable effect on community structure in\nhealthy individuals but not in individuals who are diagnosed with\nschizophrenia. In contrast, we find for individuals are given the drug\nSulpiride that it is more difficult to separate the networks of patients from\nthose of controls. Overall, we observe differences in the effects of the drugs\n(and a placebo) on community structure in patients and controls and also that\nthis effect differs across groups. We thereby demonstrate that different types\nof antipsychotic drugs selectively affect mesoscale structures of brain\nnetworks, providing support that mesoscale structures such as communities are\nmeaningful functional units in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:24:19 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 01:12:44 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Flanagan", "Ryan", ""], ["Lacasa", "Lucas", ""], ["Towlson", "Emma K.", ""], ["Lee", "Sang Hoon", ""], ["Porter", "Mason A.", ""]]}, {"id": "1806.00081", "submitter": "Partha Ghosh", "authors": "Partha Ghosh, Arpan Losalka, Michael J Black", "title": "Resisting Adversarial Attacks using Gaussian Mixture Variational\n  Autoencoders", "comments": "Proc. AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Susceptibility of deep neural networks to adversarial attacks poses a major\ntheoretical and practical challenge. All efforts to harden classifiers against\nsuch attacks have seen limited success. Two distinct categories of samples to\nwhich deep networks are vulnerable, \"adversarial samples\" and \"fooling\nsamples\", have been tackled separately so far due to the difficulty posed when\nconsidered together. In this work, we show how one can address them both under\none unified framework. We tie a discriminative model with a generative model,\nrendering the adversarial objective to entail a conflict. Our model has the\nform of a variational autoencoder, with a Gaussian mixture prior on the latent\nvector. Each mixture component of the prior distribution corresponds to one of\nthe classes in the data. This enables us to perform selective classification,\nleading to the rejection of adversarial samples instead of misclassification.\nOur method inherently provides a way of learning a selective classifier in a\nsemi-supervised scenario as well, which can resist adversarial attacks. We also\nshow how one can reclassify the rejected adversarial samples.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:25:24 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 16:31:52 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Ghosh", "Partha", ""], ["Losalka", "Arpan", ""], ["Black", "Michael J", ""]]}, {"id": "1806.00088", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Jonathan Masci, Federico Monti, Michael M. Bronstein,\n  Leonidas Guibas", "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning systems have become ubiquitous in many aspects of our lives.\nUnfortunately, it has been shown that such systems are vulnerable to\nadversarial attacks, making them prone to potential unlawful uses. Designing\ndeep neural networks that are robust to adversarial attacks is a fundamental\nstep in making such systems safer and deployable in a broader variety of\napplications (e.g. autonomous driving), but more importantly is a necessary\nstep to design novel and more advanced architectures built on new computational\nparadigms rather than marginally building on the existing ones. In this paper\nwe introduce PeerNets, a novel family of convolutional networks alternating\nclassical Euclidean convolutions with graph convolutions to harness information\nfrom a graph of peer samples. This results in a form of non-local forward\npropagation in the model, where latent features are conditioned on the global\nstructure induced by the graph, that is up to 3 times more robust to a variety\nof white- and black-box adversarial attacks compared to conventional\narchitectures with almost no drop in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:33:21 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Svoboda", "Jan", ""], ["Masci", "Jonathan", ""], ["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1806.00101", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, Kai Xu, Michael U. Gutmann and Charles Sutton", "title": "Generative Ratio Matching Networks", "comments": "ICLR 2020; Code: https://github.com/GRAM-nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models can learn to generate realistic-looking images, but\nmany of the most effective methods are adversarial and involve a saddlepoint\noptimization, which requires a careful balancing of training between a\ngenerator network and a critic network. Maximum mean discrepancy networks\n(MMD-nets) avoid this issue by using kernel as a fixed adversary, but\nunfortunately, they have not on their own been able to match the generative\nquality of adversarial training. In this work, we take their insight of using\nkernels as fixed adversaries further and present a novel method for training\ndeep generative models that does not involve saddlepoint optimization. We call\nour method generative ratio matching or GRAM for short. In GRAM, the generator\nand the critic networks do not play a zero-sum game against each other,\ninstead, they do so against a fixed kernel. Thus GRAM networks are not only\nstable to train like MMD-nets but they also match and beat the generative\nquality of adversarially trained generative networks.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 21:06:03 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 23:48:13 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 02:06:36 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Srivastava", "Akash", ""], ["Xu", "Kai", ""], ["Gutmann", "Michael U.", ""], ["Sutton", "Charles", ""]]}, {"id": "1806.00125", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai and Wei Shi and Cesar A. Uribe and Angelia Nedich and Anna\n  Scaglione", "title": "Accelerating Incremental Gradient Optimization with Curvature\n  Information", "comments": "22 pages, 3 figures, 3 tables. Accepted by Computational Optimization\n  and Applications, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies an acceleration technique for incremental aggregated\ngradient ({\\sf IAG}) method through the use of \\emph{curvature} information for\nsolving strongly convex finite sum optimization problems. These optimization\nproblems of interest arise in large-scale learning applications. Our technique\nutilizes a curvature-aided gradient tracking step to produce accurate gradient\nestimates incrementally using Hessian information. We propose and analyze two\nmethods utilizing the new technique, the curvature-aided IAG ({\\sf CIAG})\nmethod and the accelerated CIAG ({\\sf A-CIAG}) method, which are analogous to\ngradient method and Nesterov's accelerated gradient method, respectively.\nSetting $\\kappa$ to be the condition number of the objective function, we prove\nthe $R$ linear convergence rates of $1 - \\frac{4c_0 \\kappa}{(\\kappa+1)^2}$ for\nthe {\\sf CIAG} method, and $1 - \\sqrt{\\frac{c_1}{2\\kappa}}$ for the {\\sf\nA-CIAG} method, where $c_0,c_1 \\leq 1$ are constants inversely proportional to\nthe distance between the initial point and the optimal solution. When the\ninitial iterate is close to the optimal solution, the $R$ linear convergence\nrates match with the gradient and accelerated gradient method, albeit {\\sf\nCIAG} and {\\sf A-CIAG} operate in an incremental setting with strictly lower\ncomputation complexity. Numerical experiments confirm our findings. The source\ncodes used for this paper can be found on\n\\url{http://github.com/hoitowai/ciag/}.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 22:58:02 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 14:53:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Wai", "Hoi-To", ""], ["Shi", "Wei", ""], ["Uribe", "Cesar A.", ""], ["Nedich", "Angelia", ""], ["Scaglione", "Anna", ""]]}, {"id": "1806.00144", "submitter": "Said Ouala", "authors": "Said Ouala, Cedric Herzet, Ronan Fablet", "title": "Sea surface temperature prediction and reconstruction using patch-level\n  neural network representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forecasting and reconstruction of ocean and atmosphere dynamics from\nsatellite observation time series are key challenges. While model-driven\nrepresentations remain the classic approaches, data-driven representations\nbecome more and more appealing to benefit from available large-scale\nobservation and simulation datasets. In this work we investigate the relevance\nof recently introduced bilinear residual neural network representations, which\nmimic numerical integration schemes such as Runge-Kutta, for the forecasting\nand assimilation of geophysical fields from satellite-derived remote sensing\ndata. As a case-study, we consider satellite-derived Sea Surface Temperature\ntime series off South Africa, which involves intense and complex upper ocean\ndynamics. Our numerical experiments demonstrate that the proposed patch-level\nneural-network-based representations outperform other data-driven models,\nincluding analog schemes, both in terms of forecasting and missing data\ninterpolation performance with a relative gain up to 50\\% for highly dynamic\nareas.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:10:04 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ouala", "Said", ""], ["Herzet", "Cedric", ""], ["Fablet", "Ronan", ""]]}, {"id": "1806.00145", "submitter": "Chris Hettinger", "authors": "Chris Hettinger, Tanner Christensen, Jeffrey Humpherys, Tyler J.\n  Jarvis", "title": "Tandem Blocks in Deep Convolutional Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the success of residual networks (resnets) and related architectures,\nshortcut connections have quickly become standard tools for building\nconvolutional neural networks. The explanations in the literature for the\napparent effectiveness of shortcuts are varied and often contradictory. We\nhypothesize that shortcuts work primarily because they act as linear\ncounterparts to nonlinear layers. We test this hypothesis by using several\nvariations on the standard residual block, with different types of linear\nconnections, to build small image classification networks. Our experiments show\nthat other kinds of linear connections can be even more effective than the\nidentity shortcuts. Our results also suggest that the best type of linear\nconnection for a given application may depend on both network width and depth.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:19:05 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hettinger", "Chris", ""], ["Christensen", "Tanner", ""], ["Humpherys", "Jeffrey", ""], ["Jarvis", "Tyler J.", ""]]}, {"id": "1806.00148", "submitter": "Adam Charles", "authors": "Adam S. Charles", "title": "Interpreting Deep Learning: The Machine Learning Rorschach Test?", "comments": "13 pages, 1 figure. Preprint is related to an upcoming Society for\n  Industrial and Applied Mathematics (SIAM) News article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical understanding of deep learning is one of the most important tasks\nfacing the statistics and machine learning communities. While deep neural\nnetworks (DNNs) originated as engineering methods and models of biological\nnetworks in neuroscience and psychology, they have quickly become a centerpiece\nof the machine learning toolbox. Unfortunately, DNN adoption powered by recent\nsuccesses combined with the open-source nature of the machine learning\ncommunity, has outpaced our theoretical understanding. We cannot reliably\nidentify when and why DNNs will make mistakes. In some applications like text\ntranslation these mistakes may be comical and provide for fun fodder in\nresearch talks, a single error can be very costly in tasks like medical\nimaging. As we utilize DNNs in increasingly sensitive applications, a better\nunderstanding of their properties is thus imperative. Recent advances in DNN\ntheory are numerous and include many different sources of intuition, such as\nlearning theory, sparse signal analysis, physics, chemistry, and psychology. An\ninteresting pattern begins to emerge in the breadth of possible\ninterpretations. The seemingly limitless approaches are mostly constrained by\nthe lens with which the mathematical operations are viewed. Ultimately, the\ninterpretation of DNNs appears to mimic a type of Rorschach test --- a\npsychological test wherein subjects interpret a series of seemingly ambiguous\nink-blots. Validation for DNN theory requires a convergence of the literature.\nWe must distinguish between universal results that are invariant to the\nanalysis perspective and those that are specific to a particular network\nconfiguration. Simultaneously we must deal with the fact that many standard\nstatistical tools for quantifying generalization or empirically assessing\nimportant network features are difficult to apply to DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:35:32 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Charles", "Adam S.", ""]]}, {"id": "1806.00149", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative\n  Operators", "comments": "12 pages, 5 figures, 1 table", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2020)", "doi": "10.1109/TNNLS.2020.3005167", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generic type of stochastic neurons, called $q$-neurons, that\nconsiders activation functions based on Jackson's $q$-derivatives with\nstochastic parameters $q$. Our generalization of neural network architectures\nwith $q$-neurons is shown to be both scalable and very easy to implement. We\ndemonstrate experimentally consistently improved performances over\nstate-of-the-art standard activation functions, both on training and testing\nloss functions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:46:29 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 03:47:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1806.00153", "submitter": "Jong Chul Ye", "authors": "Juyoung Lee, Yoseob Han, Jae-Kyun Ryu, Jang-Yeon Park and Jong Chul Ye", "title": "k-Space Deep Learning for Reference-free EPI Ghost Correction", "comments": "To appear in Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": "https://doi.org/10.1002/mrm.27896", "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nyquist ghost artifacts in EPI are originated from phase mismatch between the\neven and odd echoes. However, conventional correction methods using reference\nscans often produce erroneous results especially in high-field MRI due to the\nnon-linear and time-varying local magnetic field changes. Recently, it was\nshown that the problem of ghost correction can be reformulated as k-space\ninterpolation problem that can be solved using structured low-rank Hankel\nmatrix approaches. Another recent work showed that data driven Hankel matrix\ndecomposition can be reformulated to exhibit similar structures as deep\nconvolutional neural network. By synergistically combining these findings, we\npropose a k-space deep learning approach that immediately corrects the phase\nmismatch without a reference scan in both accelerated and non-accelerated EPI\nacquisitions. To take advantage of the even and odd-phase directional\nredundancy, the k-space data is divided into two channels configured with even\nand odd phase encodings. The redundancies between coils are also exploited by\nstacking the multi-coil k-space data into additional input channels. Then, our\nk-space ghost correction network is trained to learn the interpolation kernel\nto estimate the missing virtual k-space data. For the accelerated EPI data, the\nsame neural network is trained to directly estimate the interpolation kernels\nfor missing k-space data from both ghost and subsampling. Reconstruction\nresults using 3T and 7T in-vivo data showed that the proposed method\noutperformed the image quality compared to the existing methods, and the\ncomputing time is much faster.The proposed k-space deep learning for EPI ghost\ncorrection is highly robust and fast, and can be combined with acceleration, so\nthat it can be used as a promising correction tool for high-field MRI without\nchanging the current acquisition protocol.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:01:27 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 07:17:27 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 00:24:07 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lee", "Juyoung", ""], ["Han", "Yoseob", ""], ["Ryu", "Jae-Kyun", ""], ["Park", "Jang-Yeon", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.00159", "submitter": "Zhanxing Zhu", "authors": "Ruosi Wan, Mingjun Zhong, Haoyi Xiong, and Zhanxing Zhu", "title": "Neural Control Variates for Variance Reduction", "comments": "Published as a conference paper at ECML PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics and machine learning, approximation of an intractable\nintegration is often achieved by using the unbiased Monte Carlo estimator, but\nthe variances of the estimation are generally high in many applications.\nControl variates approaches are well-known to reduce the variance of the\nestimation. These control variates are typically constructed by employing\npredefined parametric functions or polynomials, determined by using those\nsamples drawn from the relevant distributions. Instead, we propose to construct\nthose control variates by learning neural networks to handle the cases when\ntest functions are complex. In many applications, obtaining a large number of\nsamples for Monte Carlo estimation is expensive, which may result in\noverfitting when training a neural network. We thus further propose to employ\nauxiliary random variables induced by the original ones to extend data samples\nfor training the neural networks. We apply the proposed control variates with\naugmented variables to thermodynamic integration and reinforcement learning.\nExperimental results demonstrate that our method can achieve significant\nvariance reduction compared with other alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:28:19 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:00:29 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wan", "Ruosi", ""], ["Zhong", "Mingjun", ""], ["Xiong", "Haoyi", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1806.00166", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, Malte Rasch and Wilfried Haensch", "title": "Training LSTM Networks with Resistive Cross-Point Devices", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work we have shown that resistive cross point devices, so\ncalled Resistive Processing Unit (RPU) devices, can provide significant power\nand speed benefits when training deep fully connected networks as well as\nconvolutional neural networks. In this work, we further extend the RPU concept\nfor training recurrent neural networks (RNNs) namely LSTMs. We show that the\nmapping of recurrent layers is very similar to the mapping of fully connected\nlayers and therefore the RPU concept can potentially provide large acceleration\nfactors for RNNs as well. In addition, we study the effect of various device\nimperfections and system parameters on training performance. Symmetry of\nupdates becomes even more crucial for RNNs; already a few percent asymmetry\nresults in an increase in the test error compared to the ideal case trained\nwith floating point numbers. Furthermore, the input signal resolution to device\narrays needs to be at least 7 bits for successful training. However, we show\nthat a stochastic rounding scheme can reduce the input signal resolution back\nto 5 bits. Further, we find that RPU device variations and hardware noise are\nenough to mitigate overfitting, so that there is less need for using dropout.\nWe note that the models trained here are roughly 1500 times larger than the\nfully connected network trained on MNIST dataset in terms of the total number\nof multiplication and summation operations performed per epoch. Thus, here we\nattempt to study the validity of the RPU approach for large scale networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:58:20 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Rasch", "Malte", ""], ["Haensch", "Wilfried", ""]]}, {"id": "1806.00176", "submitter": "Wonyeol Lee", "authors": "Wonyeol Lee, Hangyeol Yu, Hongseok Yang", "title": "Reparameterization Gradient for Non-differentiable Models", "comments": "To appear at Neural Information Processing Systems (NIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:11:02 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 11:06:08 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lee", "Wonyeol", ""], ["Yu", "Hangyeol", ""], ["Yang", "Hongseok", ""]]}, {"id": "1806.00179", "submitter": "George Philipp", "authors": "George Philipp, Jaime G. Carbonell", "title": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural\n  Networks", "comments": "Previous name: The Nonlinearity Coefficient - Predicting Overfitting\n  in Deep Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time, designing neural architectures that exhibit high performance\nwas considered a dark art that required expert hand-tuning. One of the few\nwell-known guidelines for architecture design is the avoidance of exploding\ngradients, though even this guideline has remained relatively vague and\ncircumstantial. We introduce the nonlinearity coefficient (NLC), a measurement\nof the complexity of the function computed by a neural network that is based on\nthe magnitude of the gradient. Via an extensive empirical study, we show that\nthe NLC is a powerful predictor of test error and that attaining a right-sized\nNLC is essential for optimal performance.\n  The NLC exhibits a range of intriguing and important properties. It is\nclosely tied to the amount of information gained from computing a single\nnetwork gradient. It is tied to the error incurred when replacing the\nnonlinearity operations in the network with linear operations. It is not\nsusceptible to the confounders of multiplicative scaling, additive bias and\nlayer width. It is stable from layer to layer. Hence, we argue that the NLC is\nthe first robust predictor of overfitting in deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:58:14 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 02:21:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Philipp", "George", ""], ["Carbonell", "Jaime G.", ""]]}, {"id": "1806.00195", "submitter": "Ian Simon", "authors": "Ian Simon, Adam Roberts, Colin Raffel, Jesse Engel, Curtis Hawthorne,\n  Douglas Eck", "title": "Learning a Latent Space of Multitrack Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering and exploring the underlying structure of multi-instrumental\nmusic using learning-based approaches remains an open problem. We extend the\nrecent MusicVAE model to represent multitrack polyphonic measures as vectors in\na latent space. Our approach enables several useful operations such as\ngenerating plausible measures from scratch, interpolating between measures in a\nmusically meaningful way, and manipulating specific musical attributes. We also\nintroduce chord conditioning, which allows all of these operations to be\nperformed while keeping harmony fixed, and allows chords to be changed while\nmaintaining musical \"style\". By generating a sequence of measures over a\npredefined chord progression, our model can produce music with convincing\nlong-term structure. We demonstrate that our latent space model makes it\npossible to intuitively control and generate musical sequences with rich\ninstrumentation (see https://goo.gl/s2N7dV for generated audio).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:59:05 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Simon", "Ian", ""], ["Roberts", "Adam", ""], ["Raffel", "Colin", ""], ["Engel", "Jesse", ""], ["Hawthorne", "Curtis", ""], ["Eck", "Douglas", ""]]}, {"id": "1806.00201", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Martin Biehl, Nathaniel Virgo, Ryota Kanai", "title": "Being curious about the answers to questions: novelty search with\n  learned attention", "comments": "8 pages, 7 figures, ALife 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of attentional neural network layers in order to learn\na `behavior characterization' which can be used to drive novelty search and\ncuriosity-based policies. The space is structured towards answering a\nparticular distribution of questions, which are used in a supervised way to\ntrain the attentional neural network. We find that in a 2d exploration task,\nthe structure of the space successfully encodes local sensory-motor\ncontingencies such that even a greedy local `do the most novel action' policy\nwith no reinforcement learning or evolution can explore the space quickly. We\nalso apply this to a high/low number guessing game task, and find that guessing\naccording to the learned attention profile performs active inference and can\ndiscover the correct number more quickly than an exact but passive approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 05:32:47 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Virgo", "Nathaniel", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.00250", "submitter": "Roxana Istrate Rox", "authors": "R. Istrate, F. Scheidegger, G. Mariani, D. Nikolopoulos, C. Bekas, A.\n  C. I. Malossi", "title": "TAPAS: Train-less Accuracy Predictor for Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years an increasing number of researchers and practitioners have\nbeen suggesting algorithms for large-scale neural network architecture search:\ngenetic algorithms, reinforcement learning, learning curve extrapolation, and\naccuracy predictors. None of them, however, demonstrated high-performance\nwithout training new experiments in the presence of unseen datasets. We propose\na new deep neural network accuracy predictor, that estimates in fractions of a\nsecond classification performance for unseen input datasets, without training.\nIn contrast to previously proposed approaches, our prediction is not only\ncalibrated on the topological network information, but also on the\ncharacterization of the dataset-difficulty which allows us to re-tune the\nprediction without any training. Our predictor achieves a performance which\nexceeds 100 networks per second on a single GPU, thus creating the opportunity\nto perform large-scale architecture search within a few minutes. We present\nresults of two searches performed in 400 seconds on a single GPU. Our best\ndiscovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for\nCIFAR-100, verified by training. These networks are performance competitive\nwith other automatically discovered state-of-the-art networks however we only\nneeded a small fraction of the time to solution and computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 09:17:15 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Istrate", "R.", ""], ["Scheidegger", "F.", ""], ["Mariani", "G.", ""], ["Nikolopoulos", "D.", ""], ["Bekas", "C.", ""], ["Malossi", "A. C. I.", ""]]}, {"id": "1806.00265", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Philipp Fuernstahl, Orcun Goksel", "title": "Learn the new, keep the old: Extending pretrained models with new\n  anatomy and images", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_42", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely accepted as a promising solution for medical\nimage segmentation, given a sufficiently large representative dataset of images\nwith corresponding annotations. With ever increasing amounts of annotated\nmedical datasets, it is infeasible to train a learning method always with all\ndata from scratch. This is also doomed to hit computational limits, e.g.,\nmemory or runtime feasible for training. Incremental learning can be a\npotential solution, where new information (images or anatomy) is introduced\niteratively. Nevertheless, for the preservation of the collective information,\nit is essential to keep some \"important\" (i.e. representative) images and\nannotations from the past, while adding new information. In this paper, we\nintroduce a framework for applying incremental learning for segmentation and\npropose novel methods for selecting representative data therein. We\ncomparatively evaluate our methods in different scenarios using MR images and\nvalidate the increased learning capacity with using our methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:14:31 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ozdemir", "Firat", ""], ["Fuernstahl", "Philipp", ""], ["Goksel", "Orcun", ""]]}, {"id": "1806.00271", "submitter": "Yunfu Song", "authors": "Yunfu Song, Zhijian Ou", "title": "Generative Modeling by Inclusive Neural Random Fields with Applications\n  in Image Generation and Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural random fields (NRFs), referring to a class of generative models that\nuse neural networks to implement potential functions in random fields (a.k.a.\nenergy-based models), are not new but receive less attention with slow\nprogress. Different from various directed graphical models such as generative\nadversarial networks (GANs), NRFs provide an interesting family of undirected\ngraphical models for generative modeling. In this paper we propose a new\napproach, the inclusive-NRF approach, to learning NRFs for continuous data\n(e.g. images), by introducing inclusive-divergence minimized auxiliary\ngenerators and developing stochastic gradient sampling in an augmented space.\nBased on the new approach, specific inclusive-NRF models are developed and\nthoroughly evaluated in two important generative modeling applications - image\ngeneration and anomaly detection. The proposed models consistently improve over\nstate-of-the-art results in both applications. Remarkably, in addition to\nsuperior sample generation, one additional benefit of our inclusive-NRF\napproach is that, unlike GANs, it can directly provide (unnormalized) density\nestimate for sample evaluation. With these contributions and results, this\npaper significantly advances the learning and applications of NRFs to a new\nlevel, both theoretically and empirically, which have never been obtained\nbefore.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:26:15 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 12:51:15 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 15:41:35 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 15:31:22 GMT"}, {"version": "v5", "created": "Wed, 22 Jul 2020 04:20:23 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Song", "Yunfu", ""], ["Ou", "Zhijian", ""]]}, {"id": "1806.00319", "submitter": "Jack Umenberger", "authors": "Jack Umenberger and Thomas B. Sch\\\"on", "title": "Learning convex bounds for linear quadratic control policy synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to make decisions from observed data in dynamic environments remains\na problem of fundamental importance in a number of fields, from artificial\nintelligence and robotics, to medicine and finance. This paper concerns the\nproblem of learning control policies for unknown linear dynamical systems so as\nto maximize a quadratic reward function. We present a method to optimize the\nexpected value of the reward over the posterior distribution of the unknown\nsystem parameters, given data. The algorithm involves sequential convex\nprograming, and enjoys reliable local convergence and robust stability\nguarantees. Numerical simulations and stabilization of a real-world inverted\npendulum are used to demonstrate the approach, with strong performance and\nrobustness properties observed in both.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 12:46:55 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Umenberger", "Jack", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1806.00336", "submitter": "Elif Tugce Ceran", "authors": "Elif Tu\\u{g}\\c{c}e Ceran, Deniz G\\\"und\\\"uz, and Andr\\'as Gy\\\"orgy", "title": "A Reinforcement Learning Approach to Age of Information in Multi-User\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling the transmission of time-sensitive data to multiple users over\nerror-prone communication channels is studied with the goal of minimizing the\nlong-term average age of information (AoI) at the users under a constraint on\nthe average number of transmissions at the source node. After each\ntransmission, the source receives an instantaneous ACK/NACK feedback from the\nintended receiver and decides on what time and to which user to transmit the\nnext update. The optimal scheduling policy is first studied under different\nfeedback mechanisms when the channel statistics are known; in particular, the\nstandard automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols are\nconsidered. Then a reinforcement learning (RL) approach is introduced, which\ndoes not assume any a priori information on the random processes governing the\nchannel states. Different RL methods are verified and compared through\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 13:42:15 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ceran", "Elif Tu\u011f\u00e7e", ""], ["G\u00fcnd\u00fcz", "Deniz", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""]]}, {"id": "1806.00338", "submitter": "Yuqian Zhang", "authors": "Yuqian Zhang and Han-Wen Kuo and John Wright", "title": "Structured Local Optima in Sparse Blind Deconvolution", "comments": "63 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution is a ubiquitous problem of recovering two unknown signals\nfrom their convolution. Unfortunately, this is an ill-posed problem in general.\nThis paper focuses on the {\\em short and sparse} blind deconvolution problem,\nwhere the one unknown signal is short and the other one is sparsely and\nrandomly supported. This variant captures the structure of the unknown signals\nin several important applications. We assume the short signal to have unit\n$\\ell^2$ norm and cast the blind deconvolution problem as a nonconvex\noptimization problem over the sphere. We demonstrate that (i) in a certain\nregion of the sphere, every local optimum is close to some shift truncation of\nthe ground truth, and (ii) for a generic short signal of length $k$, when the\nsparsity of activation signal $\\theta\\lesssim k^{-2/3}$ and number of\nmeasurements $m\\gtrsim poly(k)$, a simple initialization method together with a\ndescent algorithm which escapes strict saddle points recovers a near shift\ntruncation of the ground truth kernel.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 13:44:02 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 15:26:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhang", "Yuqian", ""], ["Kuo", "Han-Wen", ""], ["Wright", "John", ""]]}, {"id": "1806.00370", "submitter": "Damien Scieur", "authors": "Damien Scieur, Edouard Oyallon, Alexandre d'Aspremont and Francis Bach", "title": "Nonlinear Acceleration of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Regularized Nonlinear Acceleration (RNA) algorithm is an acceleration\nmethod capable of improving the rate of convergence of many optimization\nschemes such as gradient descend, SAGA or SVRG. Until now, its analysis is\nlimited to convex problems, but empirical observations shows that RNA may be\nextended to wider settings. In this paper, we investigate further the benefits\nof RNA when applied to neural networks, in particular for the task of image\nrecognition on CIFAR10 and ImageNet. With very few modifications of exiting\nframeworks, RNA improves slightly the optimization process of CNNs, after\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:26:57 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Scieur", "Damien", ""], ["Oyallon", "Edouard", ""], ["d'Aspremont", "Alexandre", ""], ["Bach", "Francis", ""]]}, {"id": "1806.00381", "submitter": "Ilya Chevyrev", "authors": "Ilya Chevyrev, Vidit Nanda, Harald Oberhauser", "title": "Persistence paths and signature features in topological data analysis", "comments": "Additional experiment and further details. To appear in IEEE\n  Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": "IEEE TPAMI (2020) Volume: 42, Issue: 1, pp. 192 - 202", "doi": "10.1109/TPAMI.2018.2885516", "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new feature map for barcodes that arise in persistent homology\ncomputation. The main idea is to first realize each barcode as a path in a\nconvenient vector space, and to then compute its path signature which takes\nvalues in the tensor algebra of that vector space. The composition of these two\noperations - barcode to path, path to tensor series - results in a feature map\nthat has several desirable properties for statistical learning, such as\nuniversality and characteristicness, and achieves state-of-the-art results on\ncommon classification benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:54:25 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 12:24:04 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chevyrev", "Ilya", ""], ["Nanda", "Vidit", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1806.00388", "submitter": "Tristan Naumann", "authors": "Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L. Beam,\n  Irene Y. Chen, Rajesh Ranganath", "title": "A Review of Challenges and Opportunities in Machine Learning for Health", "comments": "Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern electronic health records (EHRs) provide data to answer clinically\nmeaningful questions. The growing data in EHRs makes healthcare ripe for the\nuse of machine learning. However, learning in a clinical setting presents\nunique challenges that complicate the use of common machine learning\nmethodologies. For example, diseases in EHRs are poorly labeled, conditions can\nencompass multiple underlying endotypes, and healthy individuals are\nunderrepresented. This article serves as a primer to illuminate these\nchallenges and highlights opportunities for members of the machine learning\ncommunity to contribute to healthcare.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:12:20 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 17:11:13 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 03:49:08 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 19:18:59 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Ghassemi", "Marzyeh", ""], ["Naumann", "Tristan", ""], ["Schulam", "Peter", ""], ["Beam", "Andrew L.", ""], ["Chen", "Irene Y.", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "1806.00400", "submitter": "Charlie Nash", "authors": "Charlie Nash, Nate Kushman, Christopher K. I. Williams", "title": "Inverting Supervised Representations with Autoregressive Neural Density\n  Models", "comments": "Accepted for publication by AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for feature interpretation that makes use of recent\nadvances in autoregressive density estimation models to invert model\nrepresentations. We train generative inversion models to express a distribution\nover input features conditioned on intermediate model representations. Insights\ninto the invariances learned by supervised models can be gained by viewing\nsamples from these inversion models. In addition, we can use these inversion\nmodels to estimate the mutual information between a model's inputs and its\nintermediate representations, thus quantifying the amount of information\npreserved by the network at different stages. Using this method we examine the\ntypes of information preserved at different layers of convolutional neural\nnetworks, and explore the invariances induced by different architectural\nchoices. Finally we show that the mutual information between inputs and network\nlayers decreases over the course of training, supporting recent work by\nShwartz-Ziv and Tishby (2017) on the information bottleneck theory of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:38:58 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 12:14:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Nash", "Charlie", ""], ["Kushman", "Nate", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1806.00413", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Sebastian U. Stich, Martin Jaggi", "title": "Global linear convergence of Newton's method without strong-convexity or\n  Lipschitz gradients", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Newton's method converges globally at a linear rate for\nobjective functions whose Hessians are stable. This class of problems includes\nmany functions which are not strongly convex, such as logistic regression. Our\nlinear convergence result is (i) affine-invariant, and holds even if an (ii)\napproximate Hessian is used, and if the subproblems are (iii) only solved\napproximately. Thus we theoretically demonstrate the superiority of Newton's\nmethod over first-order methods, which would only achieve a sublinear\n$O(1/t^2)$ rate under similar conditions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:58:54 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1806.00416", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Georgios Paraskevopoulos and Efthymios Tzinis and Emmanouil-Vasileios\n  Vlatakis-Gkaragkounis and Alexandros Potamianos", "title": "Pattern Search Multidimensional Scaling", "comments": "36 pages, Under review for JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel view of nonlinear manifold learning using derivative-free\noptimization techniques. Specifically, we propose an extension of the classical\nmulti-dimensional scaling (MDS) method, where instead of performing gradient\ndescent, we sample and evaluate possible \"moves\" in a sphere of fixed radius\nfor each point in the embedded space. A fixed-point convergence guarantee can\nbe shown by formulating the proposed algorithm as an instance of General\nPattern Search (GPS) framework. Evaluation on both clean and noisy synthetic\ndatasets shows that pattern search MDS can accurately infer the intrinsic\ngeometry of manifolds embedded in high-dimensional spaces. Additionally,\nexperiments on real data, even under noisy conditions, demonstrate that the\nproposed pattern search MDS yields state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:07:53 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 13:14:46 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 03:49:45 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Paraskevopoulos", "Georgios", ""], ["Tzinis", "Efthymios", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1806.00420", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe", "title": "Whitening and Coloring batch transform for GANs", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is a common technique used to speed-up and stabilize\ntraining. On the other hand, the learnable parameters of BN are commonly used\nin conditional Generative Adversarial Networks (cGANs) for representing\nclass-specific information using conditional Batch Normalization (cBN). In this\npaper we propose to generalize both BN and cBN using a Whitening and Coloring\nbased batch normalization. We show that our conditional Coloring can represent\ncategorical conditioning information which largely helps the cGAN qualitative\nresults. Moreover, we show that full-feature whitening is important in a\ngeneral GAN scenario in which the training process is known to be highly\nunstable. We test our approach on different datasets and using different GAN\nnetworks and training protocols, showing a consistent improvement in all the\ntested frameworks. Our CIFAR-10 conditioned results are higher than all\nprevious works on this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:17:30 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 23:19:40 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "1806.00421", "submitter": "Christian Aristide Nikolai Beck", "authors": "Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and\n  Arnulf Jentzen", "title": "Solving the Kolmogorov PDE by means of deep learning", "comments": "33 pages, 1 figure Accepted for publication in the Journal of\n  Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) and the Kolmogorov partial\ndifferential equations (PDEs) associated to them have been widely used in\nmodels from engineering, finance, and the natural sciences. In particular, SDEs\nand Kolmogorov PDEs, respectively, are highly employed in models for the\napproximative pricing of financial derivatives. Kolmogorov PDEs and SDEs,\nrespectively, can typically not be solved explicitly and it has been and still\nis an active topic of research to design and analyze numerical methods which\nare able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly\nall approximation methods for Kolmogorov PDEs in the literature suffer under\nthe curse of dimensionality or only provide approximations of the solution of\nthe PDE at a single fixed space-time point. In this paper we derive and propose\na numerical approximation method which aims to overcome both of the above\nmentioned drawbacks and intends to deliver a numerical approximation of the\nKolmogorov PDE on an entire region $[a,b]^d$ without suffering from the curse\nof dimensionality. Numerical results on examples including the heat equation,\nthe Black-Scholes model, the stochastic Lorenz equation, and the Heston model\nsuggest that the proposed approximation algorithm is quite effective in high\ndimensions in terms of both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:18:57 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:26:22 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Beck", "Christian", ""], ["Becker", "Sebastian", ""], ["Grohs", "Philipp", ""], ["Jaafari", "Nor", ""], ["Jentzen", "Arnulf", ""]]}, {"id": "1806.00428", "submitter": "Aditya Vora", "authors": "Aditya Vora", "title": "A Classification approach towards Unsupervised Learning of Visual\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a technique for unsupervised learning of visual\nrepresentations. Specifically, we train a model for foreground and background\nclassification task, in the process of which it learns visual representations.\nForeground and background patches for training come af- ter mining for such\npatches from hundreds and thousands of unlabelled videos available on the web\nwhich we ex- tract using a proposed patch extraction algorithm. With- out using\nany supervision, with just using 150, 000 unla- belled videos and the PASCAL\nVOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP\nwhich is close to the best performing unsupervised feature learn- ing technique\nwhereas better than many other proposed al- gorithms. The code for patch\nextraction is implemented in Matlab and available open source at the following\nlink .\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:35:08 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Vora", "Aditya", ""]]}, {"id": "1806.00437", "submitter": "Hyunghoon Cho", "authors": "Hyunghoon Cho, Benjamin DeMeo, Jian Peng, Bonnie Berger", "title": "Large-Margin Classification in Hyperbolic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing data in hyperbolic space can effectively capture latent\nhierarchical relationships. With the goal of enabling accurate classification\nof points in hyperbolic space while respecting their hyperbolic geometry, we\nintroduce hyperbolic SVM, a hyperbolic formulation of support vector machine\nclassifiers, and elucidate through new theoretical work its connection to the\nEuclidean counterpart. We demonstrate the performance improvement of hyperbolic\nSVM for multi-class prediction tasks on real-world complex networks as well as\nsimulated datasets. Our work allows analytic pipelines that take the inherent\nhyperbolic geometry of the data into account in an end-to-end fashion without\nresorting to ill-fitting tools developed for Euclidean space.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:52:16 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Cho", "Hyunghoon", ""], ["DeMeo", "Benjamin", ""], ["Peng", "Jian", ""], ["Berger", "Bonnie", ""]]}, {"id": "1806.00451", "submitter": "Ludwig Schmidt", "authors": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar", "title": "Do CIFAR-10 Classifiers Generalize to CIFAR-10?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to select these models for multiple years now. To understand the\ndanger of overfitting, we measure the accuracy of CIFAR-10 classifiers by\ncreating a new test set of truly unseen images. Although we ensure that the new\ntest set is as close to the original data distribution as possible, we find a\nlarge drop in accuracy (4% to 10%) for a broad range of deep learning models.\nYet more recent models with higher original accuracy show a smaller drop and\nbetter overall performance, indicating that this drop is likely not due to\noverfitting based on adaptivity. Instead, we view our results as evidence that\ncurrent accuracy numbers are brittle and susceptible to even minute natural\nvariations in the data distribution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:16:56 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Recht", "Benjamin", ""], ["Roelofs", "Rebecca", ""], ["Schmidt", "Ludwig", ""], ["Shankar", "Vaishaal", ""]]}, {"id": "1806.00463", "submitter": "Marcello Benedetti", "authors": "Marcello Benedetti, Edward Grant, Leonard Wossnig, Simone Severini", "title": "Adversarial quantum circuit learning for pure state approximation", "comments": "14 pages, 6 figures. Minor revisions. As published in New J. Phys", "journal-ref": "New J. Phys. 21 043023 (2019)", "doi": "10.1088/1367-2630/ab14b5", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning is one of the most successful approaches to modelling\nhigh-dimensional probability distributions from data. The quantum computing\ncommunity has recently begun to generalize this idea and to look for potential\napplications. In this work, we derive an adversarial algorithm for the problem\nof approximating an unknown quantum pure state. Although this could be done on\nuniversal quantum computers, the adversarial formulation enables us to execute\nthe algorithm on near-term quantum computers. Two parametrized circuits are\noptimized in tandem: One tries to approximate the target state, the other tries\nto distinguish between target and approximated state. Supported by numerical\nsimulations, we show that resilient backpropagation algorithms perform\nremarkably well in optimizing the two circuits. We use the bipartite\nentanglement entropy to design an efficient heuristic for the stopping\ncriterion. Our approach may find application in quantum state tomography.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:41:35 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 17:32:51 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 09:40:11 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Benedetti", "Marcello", ""], ["Grant", "Edward", ""], ["Wossnig", "Leonard", ""], ["Severini", "Simone", ""]]}, {"id": "1806.00468", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Jason Lee, Daniel Soudry, Nathan Srebro", "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We show that gradient descent on full-width linear convolutional networks of\ndepth $L$ converges to a linear predictor related to the $\\ell_{2/L}$ bridge\npenalty in the frequency domain. This is in contrast to linearly fully\nconnected networks, where gradient descent converges to the hard margin linear\nsupport vector machine solution, regardless of depth.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:58:58 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 02:51:38 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Lee", "Jason", ""], ["Soudry", "Daniel", ""], ["Srebro", "Nathan", ""]]}, {"id": "1806.00499", "submitter": "Aditya Ramesh", "authors": "Aditya Ramesh and Yann LeCun", "title": "Backpropagation for Implicit Spectral Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most successful machine intelligence systems rely on gradient-based learning,\nwhich is made possible by backpropagation. Some systems are designed to aid us\nin interpreting data when explicit goals cannot be provided. These unsupervised\nsystems are commonly trained by backpropagating through a likelihood function.\nWe introduce a tool that allows us to do this even when the likelihood is not\nexplicitly set, by instead using the implicit likelihood of the model.\nExplicitly defining the likelihood often entails making heavy-handed\nassumptions that impede our ability to solve challenging tasks. On the other\nhand, the implicit likelihood of the model is accessible without the need for\nsuch assumptions. Our tool, which we call spectral backpropagation, allows us\nto optimize it in much greater generality than what has been attempted before.\nGANs can also be viewed as a technique for optimizing implicit likelihoods. We\nstudy them using spectral backpropagation in order to demonstrate robustness\nfor high-dimensional problems, and identify two novel properties of the\ngenerator G: (1) there exist aberrant, nonsensical outputs to which G assigns\nvery high likelihood, and (2) the eigenvectors of the metric induced by G over\nlatent space correspond to quasi-disentangled explanatory factors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 18:28:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Ramesh", "Aditya", ""], ["LeCun", "Yann", ""]]}, {"id": "1806.00509", "submitter": "Mohammad Akbari", "authors": "Mohammad Akbari and Jie Liang", "title": "Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation", "comments": "5 pages, 6 figures, ICASSP 2018", "journal-ref": "2018 IEEE International Conference on Acoustics, Speech and Signal\n  Processing, 2321-2325", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-recurrent hybrid VAE-GAN model for generating sequential data is\nintroduced. In order to consider the spatial correlation of the data in each\nframe of the generated sequence, CNNs are utilized in the encoder, generator,\nand discriminator. The subsequent frames are sampled from the latent\ndistributions obtained by encoding the previous frames. As a result, the\ndependencies between the frames are maintained. Two testing frameworks for\nsynthesizing a sequence with any number of frames are also proposed. The\npromising experimental results on piano music generation indicates the\npotential of the proposed framework in modeling other sequential data such as\nvideo.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 18:59:58 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Akbari", "Mohammad", ""], ["Liang", "Jie", ""]]}, {"id": "1806.00512", "submitter": "Maohua Zhu", "authors": "Maohua Zhu, Jason Clemons, Jeff Pool, Minsoo Rhu, Stephen W. Keckler,\n  Yuan Xie", "title": "Structurally Sparsified Backward Propagation for Faster Long Short-Term\n  Memory Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting sparsity enables hardware systems to run neural networks faster\nand more energy-efficiently. However, most prior sparsity-centric optimization\ntechniques only accelerate the forward pass of neural networks and usually\nrequire an even longer training process with iterative pruning and retraining.\nWe observe that artificially inducing sparsity in the gradients of the gates in\nan LSTM cell has little impact on the training quality. Further, we can enforce\nstructured sparsity in the gate gradients to make the LSTM backward pass up to\n45% faster than the state-of-the-art dense approach and 168% faster than the\nstate-of-the-art sparsifying method on modern GPUs. Though the structured\nsparsifying method can impact the accuracy of a model, this performance gap can\nbe eliminated by mixing our sparse training method and the standard dense\ntraining method. Experimental results show that the mixed method can achieve\ncomparable results in a shorter time span than using purely dense training.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:08:30 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhu", "Maohua", ""], ["Clemons", "Jason", ""], ["Pool", "Jeff", ""], ["Rhu", "Minsoo", ""], ["Keckler", "Stephen W.", ""], ["Xie", "Yuan", ""]]}, {"id": "1806.00530", "submitter": "Carson Eisenach", "authors": "Carson Eisenach, Han Liu", "title": "Efficient, Certifiably Optimal Clustering with Applications to Latent\n  Variable Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the task of clustering either $d$ variables or $d$ points into\n$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)\n$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been\nshown in the literature to have good statistical properties in a variety of\nsettings, but remains intractable to solve in practice. To this end we propose\nFORCE, a new algorithm to solve this SDP relaxation. Compared to the naive\ninterior point method, our method reduces the computational complexity of\nsolving the SDP from $\\tilde{O}(d^7\\log\\epsilon^{-1})$ to\n$\\tilde{O}(d^{6}K^{-2}\\epsilon^{-1})$ arithmetic operations for an\n$\\epsilon$-optimal solution. Our method combines a primal first-order method\nwith a dual optimality certificate search, which when successful, allows for\nearly termination of the primal method. We show for certain variable clustering\nproblems that, with high probability, FORCE is guaranteed to find the optimal\nsolution to the SDP relaxation and provide a certificate of exact optimality.\nAs verified by our numerical experiments, this allows FORCE to solve the P-W\nSDP with dimensions in the hundreds in only tens of seconds. For a variation of\nthe P-W SDP where $K$ is not known a priori a slight modification of FORCE\nreduces the computational complexity of solving this problem as well: from\n$\\tilde{O}(d^7\\log\\epsilon^{-1})$ using a standard SDP solver to\n$\\tilde{O}(d^{4}\\epsilon^{-1})$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 20:01:58 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 16:40:13 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 03:27:26 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Eisenach", "Carson", ""], ["Liu", "Han", ""]]}, {"id": "1806.00534", "submitter": "Anastasios Kyrillidis", "authors": "Tayo Ajayi, David Mildebrath, Anastasios Kyrillidis, Shashanka Ubaru,\n  Georgios Kollias, Kristofer Bouchard", "title": "Provably convergent acceleration in factored gradient descent with\n  applications in matrix sensing", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present theoretical results on the convergence of \\emph{non-convex}\naccelerated gradient descent in matrix factorization models with $\\ell_2$-norm\nloss. The purpose of this work is to study the effects of acceleration in\nnon-convex settings, where provable convergence with acceleration should not be\nconsidered a \\emph{de facto} property. The technique is applied to matrix\nsensing problems, for the estimation of a rank $r$ optimal solution $X^\\star\n\\in \\mathbb{R}^{n \\times n}$. Our contributions can be summarized as follows.\n$i)$ We show that acceleration in factored gradient descent converges at a\nlinear rate; this fact is novel for non-convex matrix factorization settings,\nunder common assumptions. $ii)$ Our proof technique requires the acceleration\nparameter to be carefully selected, based on the properties of the problem,\nsuch as the condition number of $X^\\star$ and the condition number of objective\nfunction. $iii)$ Currently, our proof leads to the same dependence on the\ncondition number(s) in the contraction parameter, similar to recent results on\nnon-accelerated algorithms. $iv)$ Acceleration is observed in practice, both in\nsynthetic examples and in two real applications: neuronal multi-unit activities\nrecovery from single electrode recordings, and quantum state tomography on\nquantum computing simulators.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 20:29:47 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 00:27:55 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 11:46:46 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 18:57:26 GMT"}, {"version": "v5", "created": "Sat, 21 Sep 2019 19:43:17 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ajayi", "Tayo", ""], ["Mildebrath", "David", ""], ["Kyrillidis", "Anastasios", ""], ["Ubaru", "Shashanka", ""], ["Kollias", "Georgios", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "1806.00540", "submitter": "Kenneth Young", "authors": "Kenny J. Young, Richard S. Sutton, Shuo Yang", "title": "Integrating Episodic Memory into a Reinforcement Learning Agent using\n  Reservoir Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic memory is a psychology term which refers to the ability to recall\nspecific events from the past. We suggest one advantage of this particular type\nof memory is the ability to easily assign credit to a specific state when\nremembered information is found to be useful. Inspired by this idea, and the\nincreasing popularity of external memory mechanisms to handle long-term\ndependencies in deep learning systems, we propose a novel algorithm which uses\na reservoir sampling procedure to maintain an external memory consisting of a\nfixed number of past states. The algorithm allows a deep reinforcement learning\nagent to learn online to preferentially remember those states which are found\nto be useful to recall later on. Critically this method allows for efficient\nonline computation of gradient estimates with respect to the write process of\nthe external memory. Thus unlike most prior mechanisms for external memory it\nis feasible to use in an online reinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 20:52:31 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Young", "Kenny J.", ""], ["Sutton", "Richard S.", ""], ["Yang", "Shuo", ""]]}, {"id": "1806.00543", "submitter": "Manish Raghavan", "authors": "Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, Zhiwei\n  Steven Wu", "title": "The Externalities of Exploration and How Data Diversity Helps\n  Exploitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms, widely used to power search and content\noptimization on the web, must balance exploration and exploitation, potentially\nsacrificing the experience of current users for information that will lead to\nbetter decisions in the future. Recently, concerns have been raised about\nwhether the process of exploration could be viewed as unfair, placing too much\nburden on certain individuals or groups. Motivated by these concerns, we\ninitiate the study of the externalities of exploration - the undesirable side\neffects that the presence of one party may impose on another - under the linear\ncontextual bandits model. We introduce the notion of a group externality,\nmeasuring the extent to which the presence of one population of users impacts\nthe rewards of another. We show that this impact can in some cases be negative,\nand that, in a certain sense, no algorithm can avoid it. We then study\nexternalities at the individual level, interpreting the act of exploration as\nan externality imposed on the current user of a system by future users. This\ndrives us to ask under what conditions inherent diversity in the data makes\nexplicit exploration unnecessary. We build on a recent line of work on the\nsmoothed analysis of the greedy algorithm that always chooses the action that\ncurrently looks optimal, improving on prior results to show that a greedy\napproach almost matches the best possible Bayesian regret rate of any other\nalgorithm on the same problem instance whenever the diversity conditions hold,\nand that this regret is at most $\\tilde{O}(T^{1/3})$. Returning to group-level\neffects, we show that under the same conditions, negative group externalities\nessentially vanish under the greedy algorithm. Together, our results uncover a\nsharp contrast between the high externalities that exist in the worst case, and\nthe ability to remove all externalities if the data is sufficiently diverse.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:22:12 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 21:23:13 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Raghavan", "Manish", ""], ["Slivkins", "Aleksandrs", ""], ["Vaughan", "Jennifer Wortman", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1806.00548", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang, Arshdeep Sekhon, Yanjun Qi", "title": "A Fast and Scalable Joint Estimator for Integrating Additional Knowledge\n  in Learning Multiple Related Sparse Gaussian Graphical Models", "comments": "ICML 2018; Proof and Design of W in Appendix; Available as R tool\n  \"jeek\". This updated version correct a few equation errors", "journal-ref": "International Conference on Machine Learning. 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of including additional knowledge in estimating\nsparse Gaussian graphical models (sGGMs) from aggregated samples, arising often\nin bioinformatics and neuroimaging applications. Previous joint sGGM estimators\neither fail to use existing knowledge or cannot scale-up to many tasks (large\n$K$) under a high-dimensional (large $p$) situation. In this paper, we propose\na novel \\underline{J}oint \\underline{E}lementary \\underline{E}stimator\nincorporating additional \\underline{K}nowledge (JEEK) to infer multiple related\nsparse Gaussian Graphical models from large-scale heterogeneous data. Using\ndomain knowledge as weights, we design a novel hybrid norm as the minimization\nobjective to enforce the superposition of two weighted sparsity constraints,\none on the shared interactions and the other on the task-specific structural\npatterns. This enables JEEK to elegantly consider various forms of existing\nknowledge based on the domain at hand and avoid the need to design\nknowledge-specific optimization. JEEK is solved through a fast and entry-wise\nparallelizable solution that largely improves the computational efficiency of\nthe state-of-the-art $O(p^5K^4)$ to $O(p^2K^4)$. We conduct a rigorous\nstatistical analysis showing that JEEK achieves the same convergence rate\n$O(\\log(Kp)/n_{tot})$ as the state-of-the-art estimators that are much harder\nto compute. Empirically, on multiple synthetic datasets and two real-world\ndata, JEEK outperforms the speed of the state-of-arts significantly while\nachieving the same level of prediction accuracy. Available as R tool @\nhttp://jointnets.org/\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:41:34 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 16:56:06 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 10:52:47 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 20:40:40 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Beilun", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1806.00552", "submitter": "L\\'eo Neufcourt", "authors": "L\\'eo Neufcourt, Yuchen Cao, Witold Nazarewicz and Frederi Viens", "title": "Bayesian approach to model-based extrapolation of nuclear observables", "comments": null, "journal-ref": "Phys. Rev. C 98, 034318 (2018)", "doi": "10.1103/PhysRevC.98.034318", "report-no": null, "categories": "nucl-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mass, or binding energy, is the basis property of the atomic nucleus. It\ndetermines its stability, and reaction and decay rates. Quantifying the nuclear\nbinding is important for understanding the origin of elements in the universe.\nThe astrophysical processes responsible for the nucleosynthesis in stars often\ntake place far from the valley of stability, where experimental masses are not\nknown. In such cases, missing nuclear information must be provided by\ntheoretical predictions using extreme extrapolations. Bayesian machine learning\ntechniques can be applied to improve predictions by taking full advantage of\nthe information contained in the deviations between experimental and calculated\nmasses. We consider 10 global models based on nuclear Density Functional Theory\nas well as two more phenomenological mass models. The emulators of S2n\nresiduals and credibility intervals defining theoretical error bars are\nconstructed using Bayesian Gaussian processes and Bayesian neural networks. We\nconsider a large training dataset pertaining to nuclei whose masses were\nmeasured before 2003. For the testing datasets, we considered those exotic\nnuclei whose masses have been determined after 2003. We then carried out\nextrapolations towards the 2n dripline. While both Gaussian processes and\nBayesian neural networks reduce the rms deviation from experiment\nsignificantly, GP offers a better and much more stable performance. The\nincrease in the predictive power is quite astonishing: the resulting rms\ndeviations from experiment on the testing dataset are similar to those of more\nphenomenological models. The empirical coverage probability curves we obtain\nmatch very well the reference values which is highly desirable to ensure\nhonesty of uncertainty quantification, and the estimated credibility intervals\non predictions make it possible to evaluate predictive power of individual\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 22:07:13 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 02:59:38 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 18:14:12 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Neufcourt", "L\u00e9o", ""], ["Cao", "Yuchen", ""], ["Nazarewicz", "Witold", ""], ["Viens", "Frederi", ""]]}, {"id": "1806.00556", "submitter": "Ariel Schwartz", "authors": "Ariel Schwartz and Ronen Talmon", "title": "Intrinsic Isometric Manifold Learning with Application to Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data living on manifolds commonly appear in many applications. Often this\nresults from an inherently latent low-dimensional system being observed through\nhigher dimensional measurements. We show that under certain conditions, it is\npossible to construct an intrinsic and isometric data representation, which\nrespects an underlying latent intrinsic geometry. Namely, we view the observed\ndata only as a proxy and learn the structure of a latent unobserved intrinsic\nmanifold, whereas common practice is to learn the manifold of the observed\ndata. For this purpose, we build a new metric and propose a method for its\nrobust estimation by assuming mild statistical priors and by using artificial\nneural networks as a mechanism for metric regularization and parametrization.\nWe show successful application to unsupervised indoor localization in ad-hoc\nsensor networks. Specifically, we show that our proposed method facilitates\naccurate localization of a moving agent from imaging data it collects.\nImportantly, our method is applied in the same way to two different imaging\nmodalities, thereby demonstrating its intrinsic and modality-invariant\ncapabilities.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 22:37:57 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 15:07:38 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Schwartz", "Ariel", ""], ["Talmon", "Ronen", ""]]}, {"id": "1806.00569", "submitter": "Kota Matsui", "authors": "Kota Matsui, Wataru Kumagai, Kenta Kanamori, Mitsuaki Nishikimi,\n  Takafumi Kanamori", "title": "Variable Selection for Nonparametric Learning with Power Series Kernels", "comments": "24 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a variable selection method for general\nnonparametric kernel-based estimation. The proposed method consists of\ntwo-stage estimation: (1) construct a consistent estimator of the target\nfunction, (2) approximate the estimator using a few variables by l1-type\npenalized estimation. We see that the proposed method can be applied to various\nkernel nonparametric estimation such as kernel ridge regression, kernel-based\ndensity and density-ratio estimation. We prove that the proposed method has the\nproperty of the variable selection consistency when the power series kernel is\nused. This result is regarded as an extension of the variable selection\nconsistency for the non-negative garrote to the kernel-based estimators.\nSeveral experiments including simulation studies and real data applications\nshow the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 01:59:51 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 15:53:26 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Matsui", "Kota", ""], ["Kumagai", "Wataru", ""], ["Kanamori", "Kenta", ""], ["Nishikimi", "Mitsuaki", ""], ["Kanamori", "Takafumi", ""]]}, {"id": "1806.00572", "submitter": "Thanh Nguyen", "authors": "Thanh V. Nguyen, Raymond K. W. Wong and Chinmay Hegde", "title": "Autoencoders Learn Generative Linear Models", "comments": "Experimental study on synthesis data added. Typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a series of results for unsupervised learning with autoencoders.\nSpecifically, we study shallow two-layer autoencoder architectures with shared\nweights. We focus on three generative models for data that are common in\nstatistical machine learning: (i) the mixture-of-gaussians model, (ii) the\nsparse coding model, and (iii) the sparsity model with non-negative\ncoefficients. For each of these models, we prove that under suitable choices of\nhyperparameters, architectures, and initialization, autoencoders learned by\ngradient descent can successfully recover the parameters of the corresponding\nmodel. To our knowledge, this is the first result that rigorously studies the\ndynamics of gradient descent for weight-sharing autoencoders. Our analysis can\nbe viewed as theoretical evidence that shallow autoencoder modules indeed can\nbe used as feature learning mechanisms for a variety of data models, and may\nshed insight on how to train larger stacked architectures with autoencoders as\nbasic building blocks.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 02:24:16 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 06:12:55 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 07:40:13 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Nguyen", "Thanh V.", ""], ["Wong", "Raymond K. W.", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1806.00580", "submitter": "Pinlong Zhao", "authors": "Pinlong Zhao, Zhouyu Fu, Ou wu, Qinghua Hu, and Jun Wang", "title": "Detecting Adversarial Examples via Key-based Network", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural networks have achieved state-of-the-art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. Small and often imperceptible\nperturbations to the input images are sufficient to fool the most powerful deep\nneural networks. Various defense methods have been proposed to address this\nissue. However, they either require knowledge on the process of generating\nadversarial examples, or are not robust against new attacks specifically\ndesigned to penetrate the existing defense. In this work, we introduce\nkey-based network, a new detection-based defense mechanism to distinguish\nadversarial examples from normal ones based on error correcting output codes,\nusing the binary code vectors produced by multiple binary classifiers applied\nto randomly chosen label-sets as signatures to match normal images and reject\nadversarial examples. In contrast to existing defense methods, the proposed\nmethod does not require knowledge of the process for generating adversarial\nexamples and can be applied to defend against different types of attacks. For\nthe practical black-box and gray-box scenarios, where the attacker does not\nknow the encoding scheme, we show empirically that key-based network can\neffectively detect adversarial examples generated by several state-of-the-art\nattacks.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 04:13:02 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhao", "Pinlong", ""], ["Fu", "Zhouyu", ""], ["wu", "Ou", ""], ["Hu", "Qinghua", ""], ["Wang", "Jun", ""]]}, {"id": "1806.00582", "submitter": "Yue Zhao", "authors": "Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, Vikas\n  Chandra", "title": "Federated Learning with Non-IID Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables resource-constrained edge compute devices, such as\nmobile phones and IoT devices, to learn a shared model for prediction, while\nkeeping the training data local. This decentralized approach to train models\nprovides privacy, security, regulatory and economic benefits. In this work, we\nfocus on the statistical challenge of federated learning when local data is\nnon-IID. We first show that the accuracy of federated learning reduces\nsignificantly, by up to 55% for neural networks trained for highly skewed\nnon-IID data, where each client device trains only on a single class of data.\nWe further show that this accuracy reduction can be explained by the weight\ndivergence, which can be quantified by the earth mover's distance (EMD) between\nthe distribution over classes on each device and the population distribution.\nAs a solution, we propose a strategy to improve training on non-IID data by\ncreating a small subset of data which is globally shared between all the edge\ndevices. Experiments show that accuracy can be increased by 30% for the\nCIFAR-10 dataset with only 5% globally shared data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 04:45:58 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhao", "Yue", ""], ["Li", "Meng", ""], ["Lai", "Liangzhen", ""], ["Suda", "Naveen", ""], ["Civin", "Damon", ""], ["Chandra", "Vikas", ""]]}, {"id": "1806.00589", "submitter": "Yiming Zhang", "authors": "Yiming Zhang, Quan Ho Vuong, Kenny Song, Xiao-Yue Gong, Keith W. Ross", "title": "Efficient Entropy for Policy Gradient with Multidimensional Action Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep reinforcement learning has been shown to be adept at\nsolving sequential decision processes with high-dimensional state spaces such\nas in the Atari games. Many reinforcement learning problems, however, involve\nhigh-dimensional discrete action spaces as well as high-dimensional state\nspaces. This paper considers entropy bonus, which is used to encourage\nexploration in policy gradient. In the case of high-dimensional action spaces,\ncalculating the entropy and its gradient requires enumerating all the actions\nin the action space and running forward and backpropagation for each action,\nwhich may be computationally infeasible. We develop several novel unbiased\nestimators for the entropy bonus and its gradient. We apply these estimators to\nseveral models for the parameterized policies, including Independent Sampling,\nCommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM.\nFinally, we test our algorithms on two environments: a multi-hunter\nmulti-rabbit grid game and a multi-agent multi-arm bandit problem. The results\nshow that our entropy estimators substantially improve performance with\nmarginal additional computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 06:25:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhang", "Yiming", ""], ["Vuong", "Quan Ho", ""], ["Song", "Kenny", ""], ["Gong", "Xiao-Yue", ""], ["Ross", "Keith W.", ""]]}, {"id": "1806.00608", "submitter": "Daniel Huang", "authors": "Daniel Huang, Prafulla Dhariwal, Dawn Song, Ilya Sutskever", "title": "GamePad: A Learning Environment for Theorem Proving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a system called GamePad that can be used to\nexplore the application of machine learning methods to theorem proving in the\nCoq proof assistant. Interactive theorem provers such as Coq enable users to\nconstruct machine-checkable proofs in a step-by-step manner. Hence, they\nprovide an opportunity to explore theorem proving with human supervision. We\nuse GamePad to synthesize proofs for a simple algebraic rewrite problem and\ntrain baseline models for a formalization of the Feit-Thompson theorem. We\naddress position evaluation (i.e., predict the number of proof steps left) and\ntactic prediction (i.e., predict the next proof step) tasks, which arise\nnaturally in tactic-based theorem proving.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 09:19:08 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:37:30 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Huang", "Daniel", ""], ["Dhariwal", "Prafulla", ""], ["Song", "Dawn", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1806.00630", "submitter": "Daiki Kimura", "authors": "Daiki Kimura", "title": "DAQN: Deep Auto-encoder and Q-Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep reinforcement learning method usually requires a large number of\ntraining images and executing actions to obtain sufficient results. When it is\nextended a real-task in the real environment with an actual robot, the method\nwill be required more training images due to complexities or noises of the\ninput images, and executing a lot of actions on the real robot also becomes a\nserious problem. Therefore, we propose an extended deep reinforcement learning\nmethod that is applied a generative model to initialize the network for\nreducing the number of training trials. In this paper, we used a deep q-network\nmethod as the deep reinforcement learning method and a deep auto-encoder as the\ngenerative model. We conducted experiments on three different tasks: a\ncart-pole game, an atari game, and a real-game with an actual robot. The\nproposed method trained efficiently on all tasks than the previous method,\nespecially 2.5 times faster on a task with real environment images.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 13:09:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kimura", "Daiki", ""]]}, {"id": "1806.00640", "submitter": "Bowei Yan", "authors": "Bowei Yan, Oluwasanmi Koyejo, Kai Zhong, Pradeep Ravikumar", "title": "Binary Classification with Karmic, Threshold-Quasi-Concave Metrics", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex performance measures, beyond the popular measure of accuracy, are\nincreasingly being used in the context of binary classification. These complex\nperformance measures are typically not even decomposable, that is, the loss\nevaluated on a batch of samples cannot typically be expressed as a sum or\naverage of losses evaluated at individual samples, which in turn requires new\ntheoretical and methodological developments beyond standard treatments of\nsupervised learning. In this paper, we advance this understanding of binary\nclassification for complex performance measures by identifying two key\nproperties: a so-called Karmic property, and a more technical\nthreshold-quasi-concavity property, which we show is milder than existing\nstructural assumptions imposed on performance measures. Under these properties,\nwe show that the Bayes optimal classifier is a threshold function of the\nconditional probability of positive class. We then leverage this result to come\nup with a computationally practical plug-in classifier, via a novel threshold\nestimator, and further, provide a novel statistical analysis of classification\nerror with respect to complex performance measures.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 14:12:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yan", "Bowei", ""], ["Koyejo", "Oluwasanmi", ""], ["Zhong", "Kai", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1806.00650", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil, Sheetal Kalyani", "title": "Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit", "comments": "Accepted in ICML 2018. Significantly improved and extended version of\n  arXiv preprint arXiv:1707.08712", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering\nsparse high dimensional vectors in linear regression models. The optimal\nperformance of OMP requires \\textit{a priori} knowledge of either the sparsity\nof regression vector or noise statistics. Both these statistics are rarely\nknown \\textit{a priori} and are very difficult to estimate. In this paper, we\npresent a novel technique called residual ratio thresholding (RRT) to operate\nOMP without any \\textit{a priori} knowledge of sparsity and noise statistics\nand establish finite sample and large sample support recovery guarantees for\nthe same. Both analytical results and numerical simulations in real and\nsynthetic data sets indicate that RRT has a performance comparable to OMP with\n\\textit{a priori} knowledge of sparsity and noise statistics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 15:11:38 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1806.00656", "submitter": "Ahmad Alzahrani", "authors": "Ahmad Alzahrani and Samira Sadaoui", "title": "Scraping and Preprocessing Commercial Auction Data for Fraud\n  Classification", "comments": null, "journal-ref": null, "doi": "10.6084/m9.figshare.6272342", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last three decades, we have seen a significant increase in trading\ngoods and services through online auctions. However, this business created an\nattractive environment for malicious moneymakers who can commit different types\nof fraud activities, such as Shill Bidding (SB). The latter is predominant\nacross many auctions but this type of fraud is difficult to detect due to its\nsimilarity to normal bidding behaviour. The unavailability of SB datasets makes\nthe development of SB detection and classification models burdensome.\nFurthermore, to implement efficient SB detection models, we should produce SB\ndata from actual auctions of commercial sites. In this study, we first scraped\na large number of eBay auctions of a popular product. After preprocessing the\nraw auction data, we build a high-quality SB dataset based on the most reliable\nSB strategies. The aim of our research is to share the preprocessed auction\ndataset as well as the SB training (unlabelled) dataset, thereby researchers\ncan apply various machine learning techniques by using authentic data of\nauctions and fraud.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 16:06:11 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 14:24:56 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Alzahrani", "Ahmad", ""], ["Sadaoui", "Samira", ""]]}, {"id": "1806.00663", "submitter": "Linwei Hu", "authors": "Linwei Hu, Jie Chen, Vijayan N. Nair, and Agus Sudjianto", "title": "Locally Interpretable Models and Effects based on Supervised\n  Partitioning (LIME-SUP)", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised Machine Learning (SML) algorithms such as Gradient Boosting,\nRandom Forest, and Neural Networks have become popular in recent years due to\ntheir increased predictive performance over traditional statistical methods.\nThis is especially true with large data sets (millions or more observations and\nhundreds to thousands of predictors). However, the complexity of the SML models\nmakes them opaque and hard to interpret without additional tools. There has\nbeen a lot of interest recently in developing global and local diagnostics for\ninterpreting and explaining SML models. In this paper, we propose locally\ninterpretable models and effects based on supervised partitioning (trees)\nreferred to as LIME-SUP. This is in contrast with the KLIME approach that is\nbased on clustering the predictor space. We describe LIME-SUP based on fitting\ntrees to the fitted response (LIM-SUP-R) as well as the derivatives of the\nfitted response (LIME-SUP-D). We compare the results with KLIME and describe\nits advantages using simulation and real data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 16:30:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hu", "Linwei", ""], ["Chen", "Jie", ""], ["Nair", "Vijayan N.", ""], ["Sudjianto", "Agus", ""]]}, {"id": "1806.00667", "submitter": "Yarin Gal", "authors": "Yarin Gal, Lewis Smith", "title": "Sufficient Conditions for Idealised Models to Have No Adversarial\n  Examples: a Theoretical and Empirical Study with Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove, under two sufficient conditions, that idealised models can have no\nadversarial examples. We discuss which idealised models satisfy our conditions,\nand show that idealised Bayesian neural networks (BNNs) satisfy these. We\ncontinue by studying near-idealised BNNs using HMC inference, demonstrating the\ntheoretical ideas in practice. We experiment with HMC on synthetic data derived\nfrom MNIST for which we know the ground-truth image density, showing that\nnear-perfect epistemic uncertainty correlates to density under image manifold,\nand that adversarial images lie off the manifold in our setting. This suggests\nwhy MC dropout, which can be seen as performing approximate inference, has been\nobserved to be an effective defence against adversarial examples in practice;\nWe highlight failure-cases of non-idealised BNNs relying on dropout, suggesting\na new attack for dropout models and a new defence as well. Lastly, we\ndemonstrate the defence on a cats-vs-dogs image classification task with a\nVGG13 variant.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 16:43:17 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 17:15:46 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 21:25:21 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Gal", "Yarin", ""], ["Smith", "Lewis", ""]]}, {"id": "1806.00672", "submitter": "Lori Dalton", "authors": "Lori A. Dalton, Marco E. Benalc\\'azar, and Edward R. Dougherty", "title": "Optimal Clustering under Uncertainty", "comments": "19 pages, 5 eps figures, 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0204627", "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical clustering algorithms typically either lack an underlying\nprobability framework to make them predictive or focus on parameter estimation\nrather than defining and minimizing a notion of error. Recent work addresses\nthese issues by developing a probabilistic framework based on the theory of\nrandom labeled point processes and characterizing a Bayes clusterer that\nminimizes the number of misclustered points. The Bayes clusterer is analogous\nto the Bayes classifier. Whereas determining a Bayes classifier requires full\nknowledge of the feature-label distribution, deriving a Bayes clusterer\nrequires full knowledge of the point process. When uncertain of the point\nprocess, one would like to find a robust clusterer that is optimal over the\nuncertainty, just as one may find optimal robust classifiers with uncertain\nfeature-label distributions. Herein, we derive an optimal robust clusterer by\nfirst finding an effective random point process that incorporates all\nrandomness within its own probabilistic structure and from which a Bayes\nclusterer can be derived that provides an optimal robust clusterer relative to\nthe uncertainty. This is analogous to the use of effective class-conditional\ndistributions in robust classification. After evaluating the performance of\nrobust clusterers in synthetic mixtures of Gaussians models, we apply the\nframework to granular imaging, where we make use of the asymptotic\ngranulometric moment theory for granular images to relate robust clustering\ntheory to the application.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 17:07:22 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Dalton", "Lori A.", ""], ["Benalc\u00e1zar", "Marco E.", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1806.00676", "submitter": "Kav\\'e Salamatian", "authors": "Loqman Salamatian, Dali Kaafar, Kav\\'e Salamatian", "title": "A Geometric Approach for Real-time Monitoring of Dynamic Large Scale\n  Graphs: AS-level graphs illustrated", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SI math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring of large dynamic networks is a major chal- lenge for a wide\nrange of application. The complexity stems from properties of the underlying\ngraphs, in which slight local changes can lead to sizable variations of global\nprop- erties, e.g., under certain conditions, a single link cut that may be\noverlooked during monitoring can result in splitting the graph into two\ndisconnected components. Moreover, it is often difficult to determine whether a\nchange will propagate globally or remain local. Traditional graph theory\nmeasure such as the centrality or the assortativity of the graph are not\nsatisfying to characterize global properties of the graph. In this paper, we\ntackle the problem of real-time monitoring of dynamic large scale graphs by\ndeveloping a geometric approach that leverages notions of geometric curvature\nand recent development in graph embeddings using Ollivier-Ricci curvature [47].\nWe illustrate the use of our method by consid- ering the practical case of\nmonitoring dynamic variations of global Internet using topology changes\ninformation provided by combining several BGP feeds. In particular, we use our\nmethod to detect major events and changes via the geometry of the embedding of\nthe graph.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 17:32:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Salamatian", "Loqman", ""], ["Kaafar", "Dali", ""], ["Salamatian", "Kav\u00e9", ""]]}, {"id": "1806.00681", "submitter": "Yunzhe Tao", "authors": "Yunzhe Tao, Qi Sun, Qiang Du, and Wei Liu", "title": "Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling", "comments": "Accepted by NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal neural networks have been proposed and shown to be effective in\nseveral computer vision tasks, where the nonlocal operations can directly\ncapture long-range dependencies in the feature space. In this paper, we study\nthe nature of diffusion and damping effect of nonlocal networks by doing\nspectrum analysis on the weight matrices of the well-trained networks, and then\npropose a new formulation of the nonlocal block. The new block not only learns\nthe nonlocal interactions but also has stable dynamics, thus allowing deeper\nnonlocal structures. Moreover, we interpret our formulation from the general\nnonlocal modeling perspective, where we make connections between the proposed\nnonlocal network and other nonlocal models, such as nonlocal diffusion process\nand Markov jump process.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:23:48 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 22:00:29 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 17:21:36 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 03:08:46 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Tao", "Yunzhe", ""], ["Sun", "Qi", ""], ["Du", "Qiang", ""], ["Liu", "Wei", ""]]}, {"id": "1806.00685", "submitter": "Yunzhe Tao", "authors": "Yunzhe Tao, Lin Ma, Weizhong Zhang, Jian Liu, Wei Liu, Qiang Du", "title": "Hierarchical Attention-Based Recurrent Highway Networks for Time Series\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series prediction has been studied in a variety of domains. However, it\nis still challenging to predict future series given historical observations and\npast exogenous data. Existing methods either fail to consider the interactions\namong different components of exogenous variables which may affect the\nprediction accuracy, or cannot model the correlations between exogenous data\nand target data. Besides, the inherent temporal dynamics of exogenous data are\nalso related to the target series prediction, and thus should be considered as\nwell. To address these issues, we propose an end-to-end deep learning model,\ni.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which\nincorporates spatio-temporal feature extraction of exogenous variables and\ntemporal dynamics modeling of target variables into a single framework.\nMoreover, by introducing the hierarchical attention mechanism, HRHN can\nadaptively select the relevant exogenous features in different semantic levels.\nWe carry out comprehensive empirical evaluations with various methods over\nseveral datasets, and show that HRHN outperforms the state of the arts in time\nseries prediction, especially in capturing sudden changes and sudden\noscillations of time series.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:46:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tao", "Yunzhe", ""], ["Ma", "Lin", ""], ["Zhang", "Weizhong", ""], ["Liu", "Jian", ""], ["Liu", "Wei", ""], ["Du", "Qiang", ""]]}, {"id": "1806.00701", "submitter": "Jeremias Sulam", "authors": "Jeremias Sulam, Aviad Aberdam, Amir Beck, Michael Elad", "title": "On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsimonious representations are ubiquitous in modeling and processing\ninformation. Motivated by the recent Multi-Layer Convolutional Sparse Coding\n(ML-CSC) model, we herein generalize the traditional Basis Pursuit problem to a\nmulti-layer setting, introducing similar sparse enforcing penalties at\ndifferent representation layers in a symbiotic relation between synthesis and\nanalysis sparse priors. We explore different iterative methods to solve this\nnew problem in practice, and we propose a new Multi-Layer Iterative Soft\nThresholding Algorithm (ML-ISTA), as well as a fast version (ML-FISTA). We show\nthat these nested first order algorithms converge, in the sense that the\nfunction value of near-fixed points can get arbitrarily close to the solution\nof the original problem.\n  We further show how these algorithms effectively implement particular\nrecurrent convolutional neural networks (CNNs) that generalize feed-forward\nones without introducing any parameters. We present and analyze different\narchitectures resulting unfolding the iterations of the proposed pursuit\nalgorithms, including a new Learned ML-ISTA, providing a principled way to\nconstruct deep recurrent CNNs. Unlike other similar constructions, these\narchitectures unfold a global pursuit holistically for the entire network. We\ndemonstrate the emerging constructions in a supervised learning setting,\nconsistently improving the performance of classical CNNs while maintaining the\nnumber of parameters constant.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 20:31:30 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 20:51:21 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 15:49:20 GMT"}, {"version": "v4", "created": "Sun, 18 Nov 2018 17:14:28 GMT"}, {"version": "v5", "created": "Wed, 21 Nov 2018 16:21:28 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Sulam", "Jeremias", ""], ["Aberdam", "Aviad", ""], ["Beck", "Amir", ""], ["Elad", "Michael", ""]]}, {"id": "1806.00711", "submitter": "Boyang Wang", "authors": "Boyang Wang, Zirui Li, Jianwei Gong, Yidi Liu, Huiyan Chen, Chao Lu", "title": "Learning and Generalizing Motion Primitives from Driving Data for\n  Path-Tracking Applications", "comments": "2018 IEEE Intelligent Vehicles Symposium (IV)", "journal-ref": "2018 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2018.8500696", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Considering the driving habits which are learned from the naturalistic\ndriving data in the path-tracking system can significantly improve the\nacceptance of intelligent vehicles. Therefore, the goal of this paper is to\ngenerate the prediction results of lateral commands with confidence regions\naccording to the reference based on the learned motion primitives. We present a\ntwo-level structure for learning and generalizing motion primitives through\ndemonstrations. The lower-level motion primitives are generated under the path\nsegmentation and clustering layer in the upper-level. The Gaussian Mixture\nModel(GMM) is utilized to represent the primitives and Gaussian Mixture\nRegression (GMR) is selected to generalize the motion primitives. We show how\nthe upper-level can help to improve the prediction accuracy and evaluate the\ninfluence of different time scales and the number of Gaussian components. The\nmodel is trained and validated by using the driving data collected from the\nBeijing Institute of Technology (BIT) intelligent vehicle platform. Experiment\nresults show that the proposed method can extract the motion primitives from\nthe driving data and predict the future lateral control commands with high\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 22:29:43 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 13:14:36 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Wang", "Boyang", ""], ["Li", "Zirui", ""], ["Gong", "Jianwei", ""], ["Liu", "Yidi", ""], ["Chen", "Huiyan", ""], ["Lu", "Chao", ""]]}, {"id": "1806.00720", "submitter": "Haitao Liu", "authors": "Haitao Liu, Jianfei Cai, Yi Wang, Yew-Soon Ong", "title": "Generalized Robust Bayesian Committee Machine for Large-scale Gaussian\n  Process Regression", "comments": "paper + supplementary material, appears in Proceedings of ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to scale standard Gaussian process (GP) regression to large-scale\ndatasets, aggregation models employ factorized training process and then\ncombine predictions from distributed experts. The state-of-the-art aggregation\nmodels, however, either provide inconsistent predictions or require\ntime-consuming aggregation process. We first prove the inconsistency of typical\naggregations using disjoint or random data partition, and then present a\nconsistent yet efficient aggregation model for large-scale GP. The proposed\nmodel inherits the advantages of aggregations, e.g., closed-form inference and\naggregation, parallelization and distributed computing. Furthermore,\ntheoretical and empirical analyses reveal that the new aggregation model\nperforms better due to the consistent predictions that converge to the true\nunderlying function when the training size approaches infinity.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 01:08:39 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Liu", "Haitao", ""], ["Cai", "Jianfei", ""], ["Wang", "Yi", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1806.00728", "submitter": "Nisar Ahmed", "authors": "Nisar Ahmed", "title": "Data-Free/Data-Sparse Softmax Parameter Estimation with Structured Class\n  Geometries", "comments": "Final version accepted to IEEE Signal Processing Letters (double\n  column), submitted July 21, 2018", "journal-ref": null, "doi": "10.1109/LSP.2018.2860238", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers softmax parameter estimation when little/no labeled\ntraining data is available, but a priori information about the relative\ngeometry of class label log-odds boundaries is available. It is shown that\n`data-free' softmax model synthesis corresponds to solving a linear system of\nparameter equations, wherein desired dominant class log-odds boundaries are\nencoded via convex polytopes that decompose the input feature space. When\nsolvable, the linear equations yield closed-form softmax parameter solution\nfamilies using class boundary polytope specifications only. This allows softmax\nparameter learning to be implemented without expensive brute force data\nsampling and numerical optimization. The linear equations can also be adapted\nto constrained maximum likelihood estimation in data-sparse settings. Since\nsolutions may also fail to exist for the linear parameter equations derived\nfrom certain polytope specifications, it is thus also shown that there exist\nprobabilistic classification problems over m convexly separable classes for\nwhich the log-odds boundaries cannot be learned using an m-class softmax model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:03:32 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 03:02:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ahmed", "Nisar", ""]]}, {"id": "1806.00730", "submitter": "Yamini Bansal", "authors": "Yamini Bansal, Madhu Advani, David D Cox, Andrew M Saxe", "title": "Minnorm training: an algorithm for training over-parameterized deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new training method for finding minimum weight\nnorm solutions in over-parameterized neural networks (NNs). This method seeks\nto improve training speed and generalization performance by framing NN training\nas a constrained optimization problem wherein the sum of the norm of the\nweights in each layer of the network is minimized, under the constraint of\nexactly fitting training data. It draws inspiration from support vector\nmachines (SVMs), which are able to generalize well, despite often having an\ninfinite number of free parameters in their primal form, and from recent\ntheoretical generalization bounds on NNs which suggest that lower norm\nsolutions generalize better. To solve this constrained optimization problem,\nour method employs Lagrange multipliers that act as integrators of error over\ntraining and identify `support vector'-like examples. The method can be\nimplemented as a wrapper around gradient based methods and uses standard\nback-propagation of gradients from the NN for both regression and\nclassification versions of the algorithm. We provide theoretical justifications\nfor the effectiveness of this algorithm in comparison to early stopping and\n$L_2$-regularization using simple, analytically tractable settings. In\nparticular, we show faster convergence to the max-margin hyperplane in a\nshallow network (compared to vanilla gradient descent); faster convergence to\nthe minimum-norm solution in a linear chain (compared to $L_2$-regularization);\nand initialization-independent generalization performance in a deep linear\nnetwork. Finally, using the MNIST dataset, we demonstrate that this algorithm\ncan boost test accuracy and identify difficult examples in real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:33:01 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 15:26:07 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Bansal", "Yamini", ""], ["Advani", "Madhu", ""], ["Cox", "David D", ""], ["Saxe", "Andrew M", ""]]}, {"id": "1806.00770", "submitter": "Aleksandar Bojchevski", "authors": "Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany,\n  Stephan G\\\"unnemann, Michael M. Bronstein", "title": "Dual-Primal Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a surge of interest in developing deep\nlearning methods for non-Euclidean structured data such as graphs. In this\npaper, we propose Dual-Primal Graph CNN, a graph convolutional architecture\nthat alternates convolution-like operations on the graph and its dual. Our\napproach allows to learn both vertex- and edge features and generalizes the\nprevious graph attention (GAT) model. We provide extensive experimental\nvalidation showing state-of-the-art results on a variety of tasks tested on\nestablished graph benchmarks, including CORA and Citeseer citation networks as\nwell as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:15:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Monti", "Federico", ""], ["Shchur", "Oleksandr", ""], ["Bojchevski", "Aleksandar", ""], ["Litany", "Or", ""], ["G\u00fcnnemann", "Stephan", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1806.00775", "submitter": "Jungseul Ok", "authors": "Jungseul Ok and Alexandre Proutiere and Damianos Tranos", "title": "Exploration in Structured Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address reinforcement learning problems with finite state and action\nspaces where the underlying MDP has some known structure that could be\npotentially exploited to minimize the exploration rates of suboptimal (state,\naction) pairs. For any arbitrary structure, we derive problem-specific regret\nlower bounds satisfied by any learning algorithm. These lower bounds are made\nexplicit for unstructured MDPs and for those whose transition probabilities and\naverage reward functions are Lipschitz continuous w.r.t. the state and action.\nFor Lipschitz MDPs, the bounds are shown not to scale with the sizes $S$ and\n$A$ of the state and action spaces, i.e., they are smaller than $c\\log T$ where\n$T$ is the time horizon and the constant $c$ only depends on the Lipschitz\nstructure, the span of the bias function, and the minimal action sub-optimality\ngap. This contrasts with unstructured MDPs where the regret lower bound\ntypically scales as $SA\\log T$. We devise DEL (Directed Exploration Learning),\nan algorithm that matches our regret lower bounds. We further simplify the\nalgorithm for Lipschitz MDPs, and show that the simplified version is still\nable to efficiently exploit the structure.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:47:45 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 15:33:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ok", "Jungseul", ""], ["Proutiere", "Alexandre", ""], ["Tranos", "Damianos", ""]]}, {"id": "1806.00804", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen and Lior Wolf", "title": "NAM: Non-Adversarial Unsupervised Domain Mapping", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods were recently proposed for the task of translating images\nbetween domains without prior knowledge in the form of correspondences. The\nexisting methods apply adversarial learning to ensure that the distribution of\nthe mapped source domain is indistinguishable from the target domain, which\nsuffers from known stability issues. In addition, most methods rely heavily on\n`cycle' relationships between the domains, which enforce a one-to-one mapping.\nIn this work, we introduce an alternative method: Non-Adversarial Mapping\n(NAM), which separates the task of target domain generative modeling from the\ncross-domain mapping task. NAM relies on a pre-trained generative model of the\ntarget domain, and aligns each source image with an image synthesized from the\ntarget domain, while jointly optimizing the domain mapping function. It has\nseveral key advantages: higher quality and resolution image translations,\nsimpler and more stable training and reusable target models. Extensive\nexperiments are presented validating the advantages of our method.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:53:20 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 08:59:26 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hoshen", "Yedid", ""], ["Wolf", "Lior", ""]]}, {"id": "1806.00806", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Eung Yeop Kim, and Jong Chul Ye", "title": "k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-resolved angiography with interleaved stochastic trajectories (TWIST)\nhas been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve\nhighly accelerated acquisitions, TWIST combines the periphery of the k-space\ndata from several adjacent frames to reconstruct one temporal frame. However,\nthis view-sharing scheme limits the true temporal resolution of TWIST.\nMoreover, the k-space sampling patterns have been specially designed for a\nspecific generalized autocalibrating partial parallel acquisition (GRAPPA)\nfactor so that it is not possible to reduce the number of view-sharing once the\nk-data is acquired. To address these issues, this paper proposes a novel\nk-space deep learning approach for parallel MRI. In particular, we have\ndesigned our neural network so that accurate k-space interpolations are\nperformed simultaneously for multiple coils by exploiting the redundancies\nalong the coils and images. Reconstruction results using in vivo TWIST data set\nconfirm that the proposed method can immediately generate high-quality\nreconstruction results with various choices of view- sharing, allowing us to\nexploit the trade-off between spatial and temporal resolution in time-resolved\nMR angiography.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:56:46 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 06:51:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cha", "Eunju", ""], ["Kim", "Eung Yeop", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.00811", "submitter": "Xiaojie Mao", "authors": "Nathan Kallus, Xiaojie Mao, Madeleine Udell", "title": "Causal Inference with Noisy and Missing Covariates via Matrix\n  Factorization", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valid causal inference in observational studies often requires controlling\nfor confounders. However, in practice measurements of confounders may be noisy,\nand can lead to biased estimates of causal effects. We show that we can reduce\nthe bias caused by measurement noise using a large number of noisy measurements\nof the underlying confounders. We propose the use of matrix factorization to\ninfer the confounders from noisy covariates, a flexible and principled\nframework that adapts to missing values, accommodates a wide variety of data\ntypes, and can augment many causal inference methods. We bound the error for\nthe induced average treatment effect estimator and show it is consistent in a\nlinear regression setting, using Exponential Family Matrix Completion\npreprocessing. We demonstrate the effectiveness of the proposed procedure in\nnumerical experiments with both synthetic data and real clinical data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 15:19:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Udell", "Madeleine", ""]]}, {"id": "1806.00826", "submitter": "Sergiy Pereverzyev Jr.", "authors": "Shuai Lu, Peter Math\\'e, Sergiy Pereverzyev Jr", "title": "Analysis of regularized Nystr\\\"om subsampling for regression functions\n  of low smoothness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a Nystr\\\"om type subsampling approach to large kernel\nlearning methods in the misspecified case, where the target function is not\nassumed to belong to the reproducing kernel Hilbert space generated by the\nunderlying kernel. This case is less understood, in spite of its practical\nimportance. To model such a case, the smoothness of target functions is\ndescribed in terms of general source conditions. It is surprising that almost\nfor the whole range of the source conditions, describing the misspecified case,\nthe corresponding learning rate bounds can be achieved with just one value of\nthe regularization parameter. This observation allows a formulation of mild\nconditions under which the plain Nystr\\\"om subsampling can be realized with\nsubquadratic cost maintaining the guaranteed learning rates.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 16:27:31 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Lu", "Shuai", ""], ["Math\u00e9", "Peter", ""], ["Pereverzyev", "Sergiy", "Jr"]]}, {"id": "1806.00848", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong and Dorina Thanou and Michael Rabbat and Pascal Frossard", "title": "Learning graphs from data: A signal representation perspective", "comments": "corrected several imprecise statements in previous versions of the\n  manuscript as well as in the article of the same title in the May 2019 issue\n  of IEEE Signal Processing Magazine (vol. 36, no. 3, pp. 44-63, May 2019)", "journal-ref": null, "doi": "10.1109/MSP.2018.2887284", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of a meaningful graph topology plays a crucial role in the\neffective representation, processing, analysis and visualization of structured\ndata. When a natural choice of the graph is not readily available from the data\nsets, it is thus desirable to infer or learn a graph topology from the data. In\nthis tutorial overview, we survey solutions to the problem of graph learning,\nincluding classical viewpoints from statistics and physics, and more recent\napproaches that adopt a graph signal processing (GSP) perspective. We further\nemphasize the conceptual similarities and differences between classical and\nGSP-based graph inference methods, and highlight the potential advantage of the\nlatter in a number of theoretical and practical scenarios. We conclude with\nseveral open issues and challenges that are keys to the design of future signal\nprocessing and machine learning algorithms for learning graphs from data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 18:39:36 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 11:04:17 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 15:55:43 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Dong", "Xiaowen", ""], ["Thanou", "Dorina", ""], ["Rabbat", "Michael", ""], ["Frossard", "Pascal", ""]]}, {"id": "1806.00852", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Mohammad Havaei, Gabriel Chartrand, Hassan Chouaib,\n  Thomas Vincent, Andrew Jesson, Nicolas Chapados, Stan Matwin", "title": "On the Importance of Attention in Meta-Learning for Few-Shot Text\n  Classification", "comments": "13 pages, 4 figures, submitted to NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based text classification methods are limited by their\nability to achieve fast learning and generalization when the data is scarce. We\naddress this problem by integrating a meta-learning procedure that uses the\nknowledge learned across many tasks as an inductive bias towards better natural\nlanguage understanding. Based on the Model-Agnostic Meta-Learning framework\n(MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML)\nalgorithm for text classification. The essential difference between MAML and\nATAML is in the separation of task-agnostic representation learning and\ntask-specific attentive adaptation. The proposed ATAML is designed to encourage\ntask-agnostic representation learning by way of task-agnostic parameterization\nand facilitate task-specific adaptation via attention mechanisms. We provide\nevidence to show that the attention mechanism in ATAML has a synergistic effect\non learning performance. In comparisons with models trained from random\ninitialization, pretrained models and meta trained MAML, our proposed ATAML\nmethod generalizes better on single-label and multi-label classification tasks\nin miniRCV1 and miniReuters-21578 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 19:16:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jiang", "Xiang", ""], ["Havaei", "Mohammad", ""], ["Chartrand", "Gabriel", ""], ["Chouaib", "Hassan", ""], ["Vincent", "Thomas", ""], ["Jesson", "Andrew", ""], ["Chapados", "Nicolas", ""], ["Matwin", "Stan", ""]]}, {"id": "1806.00875", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Massoud Pedram", "title": "Deploying Customized Data Representation and Approximate Computing in\n  Machine Learning Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major advancements in building general-purpose and customized hardware have\nbeen one of the key enablers of versatility and pervasiveness of machine\nlearning models such as deep neural networks. To sustain this ubiquitous\ndeployment of machine learning models and cope with their computational and\nstorage complexity, several solutions such as low-precision representation of\nmodel parameters using fixed-point representation and deploying approximate\narithmetic operations have been employed. Studying the potency of such\nsolutions in different applications requires integrating them into existing\nmachine learning frameworks for high-level simulations as well as implementing\nthem in hardware to analyze their effects on power/energy dissipation,\nthroughput, and chip area. Lop is a library for design space exploration that\nbridges the gap between machine learning and efficient hardware realization. It\ncomprises a Python module, which can be integrated with some of the existing\nmachine learning frameworks and implements various customizable data\nrepresentations including fixed-point and floating-point as well as approximate\narithmetic operations.Furthermore, it includes a highly-parameterized Scala\nmodule, which allows synthesizing hardware based on the said data\nrepresentations and arithmetic operations. Lop allows researchers and designers\nto quickly compare quality of their models using various data representations\nand arithmetic operations in Python and contrast the hardware cost of viable\nrepresentations by synthesizing them on their target platforms (e.g., FPGA or\nASIC). To the best of our knowledge, Lop is the first library that allows both\nsoftware simulation and hardware realization using customized data\nrepresentations and approximate computing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:00:01 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Pedram", "Massoud", ""]]}, {"id": "1806.00877", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai and Zhuoran Yang and Zhaoran Wang and Mingyi Hong", "title": "Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual\n  Optimization", "comments": "final version as appeared in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the success of single-agent reinforcement learning, multi-agent\nreinforcement learning (MARL) remains challenging due to complex interactions\nbetween agents. Motivated by decentralized applications such as sensor\nnetworks, swarm robotics, and power grids, we study policy evaluation in MARL,\nwhere agents with jointly observed state-action pairs and private local rewards\ncollaborate to learn the value of a given policy. In this paper, we propose a\ndouble averaging scheme, where each agent iteratively performs averaging over\nboth space and time to incorporate neighboring gradient information and local\nreward information, respectively. We prove that the proposed algorithm\nconverges to the optimal solution at a global geometric rate. In particular,\nsuch an algorithm is built upon a primal-dual reformulation of the mean squared\nprojected Bellman error minimization problem, which gives rise to a\ndecentralized convex-concave saddle-point problem. To the best of our\nknowledge, the proposed double averaging primal-dual optimization algorithm is\nthe first to achieve fast finite-time convergence on decentralized\nconvex-concave saddle-point problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:07:34 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 04:19:15 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 17:12:32 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2019 01:26:29 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Wai", "Hoi-To", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Hong", "Mingyi", ""]]}, {"id": "1806.00880", "submitter": "Mahyar Khayatkhoei", "authors": "Mahyar Khayatkhoei, Ahmed Elgammal, Maneesh Singh", "title": "Disconnected Manifold Learning for Generative Adversarial Networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images may lie on a union of disjoint manifolds rather than one\nglobally connected manifold, and this can cause several difficulties for the\ntraining of common Generative Adversarial Networks (GANs). In this work, we\nfirst show that single generator GANs are unable to correctly model a\ndistribution supported on a disconnected manifold, and investigate how sample\nquality, mode dropping and local convergence are affected by this. Next, we\nshow how using a collection of generators can address this problem, providing\nnew insights into the success of such multi-generator GANs. Finally, we explain\nthe serious issues caused by considering a fixed prior over the collection of\ngenerators and propose a novel approach for learning the prior and inferring\nthe necessary number of generators without any supervision. Our proposed\nmodifications can be applied on top of any other GAN model to enable learning\nof distributions supported on disconnected manifolds. We conduct several\nexperiments to illustrate the aforementioned shortcoming of GANs, its\nconsequences in practice, and the effectiveness of our proposed modifications\nin alleviating these issues.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:19:48 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 21:15:13 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 22:54:41 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Khayatkhoei", "Mahyar", ""], ["Elgammal", "Ahmed", ""], ["Singh", "Maneesh", ""]]}, {"id": "1806.00892", "submitter": "Sumeet Katariya", "authors": "Sumeet Katariya, Branislav Kveton, Zheng Wen, Vamsi K. Potluru", "title": "Conservative Exploration using Interleaving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical problems, a learning agent may want to learn the best\naction in hindsight without ever taking a bad action, which is significantly\nworse than the default production action. In general, this is impossible\nbecause the agent has to explore unknown actions, some of which can be bad, to\nlearn better actions. However, when the actions are combinatorial, this may be\npossible if the unknown action can be evaluated by interleaving it with the\nproduction action. We formalize this concept as learning in stochastic\ncombinatorial semi-bandits with exchangeable actions. We design efficient\nlearning algorithms for this problem, bound their n-step regret, and evaluate\nthem on both synthetic and real-world problems. Our real-world experiments show\nthat our algorithms can learn to recommend K most attractive movies without\never violating a strict production constraint, both overall and subject to a\ndiversity constraint.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 23:20:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Potluru", "Vamsi K.", ""]]}, {"id": "1806.00894", "submitter": "Xiao Chen", "authors": "Barak Oshri, Annie Hu, Peter Adelson, Xiao Chen, Pascaline Dupas,\n  Jeremy Weinstein, Marshall Burke, David Lobell and Stefano Ermon", "title": "Infrastructure Quality Assessment in Africa using Satellite Imagery and\n  Deep Learning", "comments": null, "journal-ref": "KDD 2018 Proceedings of the 24th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining", "doi": "10.1145/3219819.3219924", "report-no": null, "categories": "cs.CY cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The UN Sustainable Development Goals allude to the importance of\ninfrastructure quality in three of its seventeen goals. However, monitoring\ninfrastructure quality in developing regions remains prohibitively expensive\nand impedes efforts to measure progress toward these goals. To this end, we\ninvestigate the use of widely available remote sensing data for the prediction\nof infrastructure quality in Africa. We train a convolutional neural network to\npredict ground truth labels from the Afrobarometer Round 6 survey using Landsat\n8 and Sentinel 1 satellite imagery.\n  Our best models predict infrastructure quality with AUROC scores of 0.881 on\nElectricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using\nLandsat 8. These performances are significantly better than models that\nleverage OpenStreetMap or nighttime light intensity on the same tasks. We also\ndemonstrate that our trained model can accurately make predictions in an unseen\ncountry after fine-tuning on a small sample of images. Furthermore, the model\ncan be deployed in regions with limited samples to predict infrastructure\noutcomes with higher performance than nearest neighbor spatial interpolation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 23:30:01 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Oshri", "Barak", ""], ["Hu", "Annie", ""], ["Adelson", "Peter", ""], ["Chen", "Xiao", ""], ["Dupas", "Pascaline", ""], ["Weinstein", "Jeremy", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1806.00900", "submitter": "Wei Hu", "authors": "Simon S. Du, Wei Hu, Jason D. Lee", "title": "Algorithmic Regularization in Learning Deep Homogeneous Models: Layers\n  are Automatically Balanced", "comments": "In NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the implicit regularization imposed by gradient descent for learning\nmulti-layer homogeneous functions including feed-forward fully connected and\nconvolutional deep neural networks with linear, ReLU or Leaky ReLU activation.\nWe rigorously prove that gradient flow (i.e. gradient descent with\ninfinitesimal step size) effectively enforces the differences between squared\nnorms across different layers to remain invariant without any explicit\nregularization. This result implies that if the weights are initially small,\ngradient flow automatically balances the magnitudes of all layers. Using a\ndiscretization argument, we analyze gradient descent with positive step size\nfor the non-convex low-rank asymmetric matrix factorization problem without any\nregularization. Inspired by our findings for gradient flow, we prove that\ngradient descent with step sizes $\\eta_t = O\\left(t^{-\\left(\n\\frac12+\\delta\\right)} \\right)$ ($0<\\delta\\le\\frac12$) automatically balances\ntwo low-rank factors and converges to a bounded global optimum. Furthermore,\nfor rank-$1$ asymmetric matrix factorization we give a finer analysis showing\ngradient descent with constant step size converges to the global minimum at a\nglobally linear rate. We believe that the idea of examining the invariance\nimposed by first order algorithms in learning homogeneous models could serve as\na fundamental building block for studying optimization for learning deep\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 00:07:38 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 17:02:49 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Lee", "Jason D.", ""]]}, {"id": "1806.00914", "submitter": "Manoj Reddy Dareddy", "authors": "Manoj Reddy Dareddy, Ariyam Das, Junghoo Cho, Carlo Zaniolo", "title": "How Much Are You Willing to Share? A \"Poker-Styled\" Selective Privacy\n  Preserving Framework for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most industrial recommender systems rely on the popular collaborative\nfiltering (CF) technique for providing personalized recommendations to its\nusers. However, the very nature of CF is adversarial to the idea of user\nprivacy, because users need to share their preferences with others in order to\nbe grouped with like-minded people and receive accurate recommendations. While\nprevious privacy preserving approaches have been successful inasmuch as they\nconcealed user preference information to some extent from a centralized\nrecommender system, they have also, nevertheless, incurred significant\ntrade-offs in terms of privacy, scalability, and accuracy. They are also\nvulnerable to privacy breaches by malicious actors. In light of these\nobservations, we propose a novel selective privacy preserving (SP2) paradigm\nthat allows users to custom define the scope and extent of their individual\nprivacies, by marking their personal ratings as either public (which can be\nshared) or private (which are never shared and stored only on the user device).\nOur SP2 framework works in two steps: (i) First, it builds an initial\nrecommendation model based on the sum of all public ratings that have been\nshared by users and (ii) then, this public model is fine-tuned on each user's\ndevice based on the user private ratings, thus eventually learning a more\naccurate model. Furthermore, in this work, we introduce three different\nalgorithms for implementing an end-to-end SP2 framework that can scale\neffectively from thousands to hundreds of millions of items. Our user survey\nshows that an overwhelming fraction of users are likely to rate much more items\nto improve the overall recommendations when they can control what ratings will\nbe publicly shared with others.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:25:06 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dareddy", "Manoj Reddy", ""], ["Das", "Ariyam", ""], ["Cho", "Junghoo", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1806.00919", "submitter": "Yi-Qing Wang", "authors": "Yi-Qing Wang", "title": "Adversarial confidence and smoothness regularizations for scalable\n  unsupervised discriminative learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a generic probabilistic discriminative learner\nfrom the functional viewpoint and argue that, to make it learn well, it is\nnecessary to constrain its hypothesis space to a set of non-trivial piecewise\nconstant functions. To achieve this goal, we present a scalable unsupervised\nregularization framework. On the theoretical front, we prove that this\nframework is conducive to a factually confident and smooth discriminative model\nand connect it to an adversarial Taboo game, spectral clustering and virtual\nadversarial training. Experimentally, we take deep neural networks as our\nlearners and demonstrate that, when trained under our framework in the\nunsupervised setting, they not only achieve state-of-the-art clustering results\nbut also generalize well on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:46:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Wang", "Yi-Qing", ""]]}, {"id": "1806.00931", "submitter": "Tariq Daouda", "authors": "Tariq Daouda (1 and 2 and 3), Jeremie Zumer (1 and 4 and 3), Claude\n  Perreault (1 and 5 and 3) and S\\'ebastien Lemieux (1 and 4 and 3) ((1)\n  Institute for Research in Immunology and Cancer, (2) Department of\n  biochemistry, (3) Universit\\'e de Montr\\'eal, (4) Department of Computer\n  Science and Operations Research, (5) Department of Medicine)", "title": "Holographic Neural Architectures", "comments": "10 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.GN q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is at the heart of what makes deep learning\neffective. In this work, we introduce a new framework for representation\nlearning that we call \"Holographic Neural Architectures\" (HNAs). In the same\nway that an observer can experience the 3D structure of a holographed object by\nlooking at its hologram from several angles, HNAs derive Holographic\nRepresentations from the training set. These representations can then be\nexplored by moving along a continuous bounded single dimension. We show that\nHNAs can be used to make generative networks, state-of-the-art regression\nmodels and that they are inherently highly resistant to noise. Finally, we\nargue that because of their denoising abilities and their capacity to\ngeneralize well from very few examples, models based upon HNAs are particularly\nwell suited for biological applications where training examples are rare or\nnoisy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 02:41:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Daouda", "Tariq", "", "1 and 2 and 3"], ["Zumer", "Jeremie", "", "1 and 4 and 3"], ["Perreault", "Claude", "", "1 and 5 and 3"], ["Lemieux", "S\u00e9bastien", "", "1 and 4 and 3"]]}, {"id": "1806.00949", "submitter": "Shay Moran", "authors": "Noga Alon and Roi Livni and Maryanthe Malliaris and Shay Moran", "title": "Private PAC learning implies finite Littlestone dimension", "comments": "STOC camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR math.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every approximately differentially private learning algorithm\n(possibly improper) for a class $H$ with Littlestone dimension~$d$ requires\n$\\Omega\\bigl(\\log^*(d)\\bigr)$ examples. As a corollary it follows that the\nclass of thresholds over $\\mathbb{N}$ can not be learned in a private manner;\nthis resolves open question due to [Bun et al., 2015, Feldman and Xiao, 2015].\nWe leave as an open question whether every class with a finite Littlestone\ndimension can be learned by an approximately differentially private algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 04:35:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:53:31 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 16:12:06 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Alon", "Noga", ""], ["Livni", "Roi", ""], ["Malliaris", "Maryanthe", ""], ["Moran", "Shay", ""]]}, {"id": "1806.00952", "submitter": "Navid Azizan Ruhi", "authors": "Navid Azizan and Babak Hassibi", "title": "Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic descent methods (of the gradient and mirror varieties) have become\nincreasingly popular in optimization. In fact, it is now widely recognized that\nthe success of deep learning is not only due to the special deep architecture\nof the models, but also due to the behavior of the stochastic descent methods\nused, which play a key role in reaching \"good\" solutions that generalize well\nto unseen data. In an attempt to shed some light on why this is the case, we\nrevisit some minimax properties of stochastic gradient descent (SGD) for the\nsquare loss of linear models---originally developed in the 1990's---and extend\nthem to general stochastic mirror descent (SMD) algorithms for general loss\nfunctions and nonlinear models. In particular, we show that there is a\nfundamental identity which holds for SMD (and SGD) under very general\nconditions, and which implies the minimax optimality of SMD (and SGD) for\nsufficiently small step size, and for a general class of loss functions and\ngeneral nonlinear models. We further show that this identity can be used to\nnaturally establish other properties of SMD (and SGD), namely convergence and\nimplicit regularization for over-parameterized linear models (in what is now\nbeing called the \"interpolating regime\"), some of which have been shown in\ncertain cases in prior literature. We also argue how this identity can be used\nin the so-called \"highly over-parameterized\" nonlinear setting (where the\nnumber of parameters far exceeds the number of data points) to provide insights\ninto why SMD (and SGD) may have similar convergence and implicit regularization\nproperties for deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 04:53:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 04:59:28 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 08:05:37 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 21:53:08 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Azizan", "Navid", ""], ["Hassibi", "Babak", ""]]}, {"id": "1806.00973", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, CNRS, CRIStAL), Wouter Koolen (CWI), Aurelien\n  Garivier (IMT)", "title": "Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the minimum/maximum mean among a finite set of distributions is a\nfundamental sub-task in planning, game tree search and reinforcement learning.\nWe formalize this learning task as the problem of sequentially testing how the\nminimum mean among a finite set of distributions compares to a given threshold.\nWe develop refined non-asymptotic lower bounds, which show that optimality\nmandates very different sampling behavior for a low vs high true minimum. We\nshow that Thompson Sampling and the intuitive Lower Confidence Bounds policy\neach nail only one of these cases. We develop a novel approach that we call\nMurphy Sampling. Even though it entertains exclusively low true minima, we\nprove that MS is optimal for both possibilities. We then design advanced\nself-normalized deviation inequalities, fueling more aggressive stopping rules.\nWe complement our theoretical guarantees by experiments showing that MS works\nbest in practice.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:37:22 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, CNRS, CRIStAL"], ["Koolen", "Wouter", "", "CWI"], ["Garivier", "Aurelien", "", "IMT"]]}, {"id": "1806.00979", "submitter": "Patricio Cerda", "authors": "Patricio Cerda (PARIETAL), Ga\\\"el Varoquaux (PARIETAL), Bal\\'azs\n  K\\'egl (LAL, CNRS)", "title": "Similarity encoding for learning with dirty categorical variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For statistical learning, categorical variables in a table are usually\nconsidered as discrete entities and encoded separately to feature vectors,\ne.g., with one-hot encoding. \"Dirty\" non-curated data gives rise to categorical\nvariables with a very high cardinality but redundancy: several categories\nreflect the same entity. In databases, this issue is typically solved with a\ndeduplication step. We show that a simple approach that exposes the redundancy\nto the learning algorithm brings significant gains. We study a generalization\nof one-hot encoding, similarity encoding, that builds feature vectors from\nsimilarities across categories. We perform a thorough empirical validation on\nnon-curated tables, a problem seldom studied in machine learning. Results on\nseven real-world datasets show that similarity encoding brings significant\ngains in prediction in comparison with known encoding methods for categories or\nstrings, notably one-hot encoding and bag of character n-grams. We draw\npractical recommendations for encoding dirty categories: 3-gram similarity\nappears to be a good choice to capture morphological resemblance. For very\nhigh-cardinality, dimensionality reduction significantly reduces the\ncomputational cost with little loss in performance: random projections or\nchoosing a subset of prototype categories still outperforms classic encoding\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:46:22 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Cerda", "Patricio", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["K\u00e9gl", "Bal\u00e1zs", "", "LAL, CNRS"]]}, {"id": "1806.00981", "submitter": "Franz Pernkopf", "authors": "Tobias Schrank and Franz Pernkopf", "title": "Automatic Clustering of a Network Protocol with Weakly-Supervised\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction is a fundamental part when learning behavioral models of systems.\nUsually the process of abstraction is manually defined by domain experts. This\npaper presents a method to perform automatic abstraction for network protocols.\nIn particular a weakly supervised clustering algorithm is used to build an\nabstraction with a small vocabulary size for the widely used TLS protocol. To\nshow the effectiveness of the proposed method we compare the resultant abstract\nmessages to a manually constructed (reference) abstraction. With a small amount\nof side-information in the form of a few labeled examples this method finds an\nabstraction that matches the reference abstraction perfectly.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:52:08 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Schrank", "Tobias", ""], ["Pernkopf", "Franz", ""]]}, {"id": "1806.00989", "submitter": "Fran\\c{c}ois Portier", "authors": "Bernard Delyon and Fran\\c{c}ois Portier", "title": "Asymptotic optimality of adaptive importance sampling", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance sampling (AIS) uses past samples to update the\n\\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with\ntwo steps : (i) to explore the space with $n_t$ points according to $q_t$ and\n(ii) to exploit the current amount of information to update the sampling\npolicy. The very fundamental question raised in this paper concerns the\nbehavior of empirical sums based on AIS. Without making any assumption on the\nallocation policy $n_t$, the theory developed involves no restriction on the\nsplit of computational resources between the explore (i) and the exploit (ii)\nstep. It is shown that AIS is asymptotically optimal : the asymptotic behavior\nof AIS is the same as some \"oracle\" strategy that knows the targeted sampling\npolicy from the beginning. From a practical perspective, weighted AIS is\nintroduced, a new method that allows to forget poor samples from early stages.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 07:20:07 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:02:54 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Delyon", "Bernard", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1806.01003", "submitter": "Francesco Sasso", "authors": "Francesco Sasso, Angelo Coluccia and Giuseppe Notarstefano", "title": "Distributed Learning from Interactions in Social Networks", "comments": "This submission is a shorter work (for conference publication) of a\n  more comprehensive paper, already submitted as arXiv:1706.04081 (under review\n  for journal publication). In this short submission only one social set-up is\n  considered and only one of the relaxed estimators is proposed. Moreover, the\n  exhaustive analysis, carried out in the longer manuscript, is completely\n  missing in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network scenario in which agents can evaluate each other\naccording to a score graph that models some interactions. The goal is to design\na distributed protocol, run by the agents, that allows them to learn their\nunknown state among a finite set of possible values. We propose a Bayesian\nframework in which scores and states are associated to probabilistic events\nwith unknown parameters and hyperparameters, respectively. We show that each\nagent can learn its state by means of a local Bayesian classifier and a\n(centralized) Maximum-Likelihood (ML) estimator of parameter-hyperparameter\nthat combines plain ML and Empirical Bayes approaches. By using tools from\ngraphical models, which allow us to gain insight on conditional dependencies of\nscores and states, we provide a relaxed probabilistic model that ultimately\nleads to a parameter-hyperparameter estimator amenable to distributed\ncomputation. To highlight the appropriateness of the proposed relaxation, we\ndemonstrate the distributed estimators on a social interaction set-up for user\nprofiling.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:19:27 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Sasso", "Francesco", ""], ["Coluccia", "Angelo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1806.01009", "submitter": "Francesco Ortelli", "authors": "Francesco Ortelli, Sara van de Geer", "title": "On the total variation regularized estimator over a class of tree graphs", "comments": "42 pages", "journal-ref": "Electronic Journal of Statistics, 12, 2018, 4517-4570", "doi": "10.1214/18-EJS1519", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize to tree graphs obtained by connecting path graphs an oracle\nresult obtained for the Fused Lasso over the path graph. Moreover we show that\nit is possible to substitute in the oracle inequality the minimum of the\ndistances between jumps by their harmonic mean. In doing so we prove a lower\nbound on the compatibility constant for the total variation penalty. Our\nanalysis leverages insights obtained for the path graph with one branch to\nunderstand the case of more general tree graphs.\n  As a side result, we get insights into the irrepresentable condition for such\ntree graphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:41:49 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 14:30:27 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 16:14:05 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ortelli", "Francesco", ""], ["van de Geer", "Sara", ""]]}, {"id": "1806.01010", "submitter": "Sung Whan Yoon", "authors": "Sung Whan Yoon, Jun Seo, Jaekyun Moon", "title": "Meta-Learner with Linear Nulling", "comments": "presented at 2018 NeurIPS (NIPS) Workshop on Meta-Learning (Montreal,\n  Canada)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a meta-learning algorithm utilizing a linear transformer that\ncarries out null-space projection of neural network outputs. The main idea is\nto construct an alternative classification space such that the error signals\nduring few-shot learning are quickly zero-forced on that space so that reliable\nclassification on low data is possible. The final decision on a query is\nobtained utilizing a null-space-projected distance measure between the network\noutput and reference vectors, both of which have been trained in the initial\nlearning phase. Among the known methods with a given model size, our\nmeta-learner achieves the best or near-best image classification accuracies\nwith Omniglot and miniImageNet datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:42:09 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 14:14:24 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 14:54:25 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yoon", "Sung Whan", ""], ["Seo", "Jun", ""], ["Moon", "Jaekyun", ""]]}, {"id": "1806.01047", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Andre Marquand", "title": "Normative Modeling of Neuroimaging Data using Scalable Multi-Task\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normative modeling has recently been proposed as an alternative for the\ncase-control approach in modeling heterogeneity within clinical cohorts.\nNormative modeling is based on single-output Gaussian process regression that\nprovides coherent estimates of uncertainty required by the method but does not\nconsider spatial covariance structure. Here, we introduce a scalable multi-task\nGaussian process regression (S-MTGPR) approach to address this problem. To this\nend, we exploit a combination of a low-rank approximation of the spatial\ncovariance matrix with algebraic properties of Kronecker product in order to\nreduce the computational complexity of Gaussian process regression in\nhigh-dimensional output spaces. On a public fMRI dataset, we show that S-MTGPR:\n1) leads to substantial computational improvements that allow us to estimate\nnormative models for high-dimensional fMRI data whilst accounting for spatial\nstructure in data; 2) by modeling both spatial and across-sample variances, it\nprovides higher sensitivity in novelty detection scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:20:49 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 20:10:14 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Marquand", "Andre", ""]]}, {"id": "1806.01052", "submitter": "Farid Khosravikia", "authors": "Farid Khosravikia, Yasaman Zeinali, Zoltan Nagy, Patricia Clayton, and\n  Ellen M. Rathje", "title": "Neural Network-Based Equations for Predicting PGA and PGV in Texas,\n  Oklahoma, and Kansas", "comments": "5th Geotechnical Earthquake Engineering and Soil Dynamics Conference,\n  Austin, TX, USA, June 10-13. (2018)", "journal-ref": null, "doi": "10.1061/9780784481462.052", "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parts of Texas, Oklahoma, and Kansas have experienced increased rates of\nseismicity in recent years, providing new datasets of earthquake recordings to\ndevelop ground motion prediction models for this particular region of the\nCentral and Eastern North America (CENA). This paper outlines a framework for\nusing Artificial Neural Networks (ANNs) to develop attenuation models from the\nground motion recordings in this region. While attenuation models exist for the\nCENA, concerns over the increased rate of seismicity in this region necessitate\ninvestigation of ground motions prediction models particular to these states.\nTo do so, an ANN-based framework is proposed to predict peak ground\nacceleration (PGA) and peak ground velocity (PGV) given magnitude, earthquake\nsource-to-site distance, and shear wave velocity. In this framework,\napproximately 4,500 ground motions with magnitude greater than 3.0 recorded in\nthese three states (Texas, Oklahoma, and Kansas) since 2005 are considered.\nResults from this study suggest that existing ground motion prediction models\ndeveloped for CENA do not accurately predict the ground motion intensity\nmeasures for earthquakes in this region, especially for those with low\nsource-to-site distances or on very soft soil conditions. The proposed ANN\nmodels provide much more accurate prediction of the ground motion intensity\nmeasures at all distances and magnitudes. The proposed ANN models are also\nconverted to relatively simple mathematical equations so that engineers can\neasily use them to predict the ground motion intensity measures for future\nevents. Finally, through a sensitivity analysis, the contributions of the\npredictive parameters to the prediction of the considered intensity measures\nare investigated.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:32:17 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Khosravikia", "Farid", ""], ["Zeinali", "Yasaman", ""], ["Nagy", "Zoltan", ""], ["Clayton", "Patricia", ""], ["Rathje", "Ellen M.", ""]]}, {"id": "1806.01059", "submitter": "Preethi Lahoti", "authors": "Preethi Lahoti, Krishna P. Gummadi, Gerhard Weikum", "title": "iFair: Learning Individually Fair Data Representations for Algorithmic\n  Decision Making", "comments": "Accepted at ICDE 2019. Please cite the ICDE 2019 proceedings version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are rated and ranked, towards algorithmic decision making in an\nincreasing number of applications, typically based on machine learning.\nResearch on how to incorporate fairness into such tasks has prevalently pursued\nthe paradigm of group fairness: giving adequate success rates to specifically\nprotected groups. In contrast, the alternative paradigm of individual fairness\nhas received relatively little attention, and this paper advances this less\nexplored direction. The paper introduces a method for probabilistically mapping\nuser records into a low-rank representation that reconciles individual fairness\nand the utility of classifiers and rankings in downstream applications. Our\nnotion of individual fairness requires that users who are similar in all\ntask-relevant attributes such as job qualification, and disregarding all\npotentially discriminating attributes such as gender, should have similar\noutcomes. We demonstrate the versatility of our method by applying it to\nclassification and learning-to-rank tasks on a variety of real-world datasets.\nOur experiments show substantial improvements over the best prior work for this\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:42:08 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 16:29:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Lahoti", "Preethi", ""], ["Gummadi", "Krishna P.", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1806.01083", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Optimal Balancing of Time-Dependent Confounders for Marginal Structural\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) estimate the causal effect of a\ntime-varying treatment in the presence of time-dependent confounding via\nweighted regression. The standard approach of using inverse probability of\ntreatment weighting (IPTW) can lead to high-variance estimates due to extreme\nweights and be sensitive to model misspecification. Various methods have been\nproposed to partially address this, including truncation and stabilized-IPTW to\ntemper extreme weights and covariate balancing propensity score (CBPS) to\naddress treatment model misspecification. In this paper, we present Kernel\nOptimal Weighting (KOW), a convex-optimization-based approach that finds\nweights for fitting the MSM that optimally balance time-dependent confounders\nwhile simultaneously controlling for precision, directly addressing the above\nlimitations. KOW directly minimizes the error in estimation due to\ntime-dependent confounding via a new decomposition as a functional. We further\nextend KOW to control for informative censoring. We evaluate the performance of\nKOW in a simulation study, comparing it with IPTW, stabilized-IPTW, and CBPS.\nWe demonstrate the use of KOW in studying the effect of treatment initiation on\ntime-to-death among people living with HIV and the effect of negative\nadvertising on elections in the United States.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:51:05 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 14:02:43 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1806.01094", "submitter": "Sebastian Weichwald", "authors": "Niklas Pfister, Sebastian Weichwald, Peter B\\\"uhlmann, Bernhard\n  Sch\\\"olkopf", "title": "Robustifying Independent Component Analysis by Adjusting for Group-Wise\n  Stationary Noise", "comments": "equal contribution between Pfister and Weichwald", "journal-ref": "Journal of Machine Learning Research, 20(147):1-50, 2019. (\n  http://www.jmlr.org/papers/v20/18-399.html )", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce coroICA, confounding-robust independent component analysis, a\nnovel ICA algorithm which decomposes linearly mixed multivariate observations\ninto independent components that are corrupted (and rendered dependent) by\nhidden group-wise stationary confounding. It extends the ordinary ICA model in\na theoretically sound and explicit way to incorporate group-wise (or\nenvironment-wise) confounding. We show that our proposed general noise model\nallows to perform ICA in settings where other noisy ICA procedures fail.\nAdditionally, it can be used for applications with grouped data by adjusting\nfor different stationary noise within each group. Our proposed noise model has\na natural relation to causality and we explain how it can be applied in the\ncontext of causal inference. In addition to our theoretical framework, we\nprovide an efficient estimation procedure and prove identifiability of the\nunmixing matrix under mild assumptions. Finally, we illustrate the performance\nand robustness of our method on simulated data, provide audible and visual\nexamples, and demonstrate the applicability to real-world scenarios by\nexperiments on publicly available Antarctic ice core data as well as two EEG\ndata sets. We provide a scikit-learn compatible pip-installable Python package\ncoroICA as well as R and Matlab implementations accompanied by a documentation\nat https://sweichwald.de/coroICA/\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:17:14 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:02:44 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 14:38:49 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Pfister", "Niklas", ""], ["Weichwald", "Sebastian", ""], ["B\u00fchlmann", "Peter", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1806.01145", "submitter": "Deepak Baby", "authors": "Deepak Baby and Sarah Verhulst", "title": "Machines hear better when they have ears", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-neural-network (DNN) based noise suppression systems yield significant\nimprovements over conventional approaches such as spectral subtraction and\nnon-negative matrix factorization, but do not generalize well to noise\nconditions they were not trained for. In comparison to DNNs, humans show\nremarkable noise suppression capabilities that yield successful speech\nintelligibility under various adverse listening conditions and negative\nsignal-to-noise ratios (SNRs). Motivated by the excellent human performance,\nthis paper explores whether numerical models that simulate human cochlear\nsignal processing can be combined with DNNs to improve the robustness of DNN\nbased noise suppression systems. Five cochlear models were coupled to\nfully-connected and recurrent NN-based noise suppression systems and were\ntrained and evaluated for a variety of noise conditions using objective\nmetrics: perceptual speech quality (PESQ), segmental SNR and cepstral distance.\nThe simulations show that biophysically-inspired cochlear models improve the\ngeneralizability of DNN-based noise suppression systems for unseen noise and\nnegative SNRs. This approach thus leads to robust noise suppression systems\nthat are less sensitive to the noise type and noise level. Because cochlear\nmodels capture the intrinsic nonlinearities and dynamics of peripheral auditory\nprocessing, it is shown here that accounting for their deterministic signal\nprocessing improves machine hearing and avoids overtraining of multi-layer\nDNNs. We hence conclude that machines hear better when realistic cochlear\nmodels are used at the input of DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:44:35 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 16:23:09 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Baby", "Deepak", ""], ["Verhulst", "Sarah", ""]]}, {"id": "1806.01159", "submitter": "Edward Pyzer-Knapp", "authors": "Matthew Groves and Edward O. Pyzer-Knapp", "title": "Efficient and Scalable Batch Bayesian Optimization Using K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present K-Means Batch Bayesian Optimization (KMBBO), a novel batch\nsampling algorithm for Bayesian Optimization (BO). KMBBO uses unsupervised\nlearning to efficiently estimate peaks of the model acquisition function. We\nshow in empirical experiments that our method outperforms the current\nstate-of-the-art batch allocation algorithms on a variety of test problems\nincluding tuning of algorithm hyper-parameters and a challenging drug discovery\nproblem. In order to accommodate the real-world problem of high dimensional\ndata, we propose a modification to KMBBO by combining it with compressed\nsensing to project the optimization into a lower dimensional subspace. We\ndemonstrate empirically that this 2-step method outperforms algorithms where no\ndimensionality reduction has taken place.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 15:24:51 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 08:25:25 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Groves", "Matthew", ""], ["Pyzer-Knapp", "Edward O.", ""]]}, {"id": "1806.01175", "submitter": "Artemij Amiranashvili", "authors": "Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, Thomas Brox", "title": "TD or not TD: Analyzing the Role of Temporal Differencing in Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding of reinforcement learning (RL) has been shaped by\ntheoretical and empirical results that were obtained decades ago using tabular\nrepresentations and linear function approximators. These results suggest that\nRL methods that use temporal differencing (TD) are superior to direct Monte\nCarlo estimation (MC). How do these results hold up in deep RL, which deals\nwith perceptually complex environments and deep nonlinear models? In this\npaper, we re-examine the role of TD in modern deep RL, using specially designed\nenvironments that control for specific factors that affect performance, such as\nreward sparsity, reward delay, and the perceptual complexity of the task. When\ncomparing TD with infinite-horizon MC, we are able to reproduce classic results\nin modern settings. Yet we also find that finite-horizon MC is not inferior to\nTD, even when rewards are sparse or delayed. This makes MC a viable alternative\nto TD in deep RL.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:16:51 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Amiranashvili", "Artemij", ""], ["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""], ["Brox", "Thomas", ""]]}, {"id": "1806.01182", "submitter": "Fabio Vitale Fabio", "authors": "Fabio Vitale, Nikos Parotsidis, Claudio Gentile", "title": "Online Reciprocal Recommendation with Theoretical Performance Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reciprocal recommendation problem is one where the goal of learning is not\njust to predict a user's preference towards a passive item (e.g., a book), but\nto recommend the targeted user on one side another user from the other side\nsuch that a mutual interest between the two exists. The problem thus is sharply\ndifferent from the more traditional items-to-users recommendation, since a good\nmatch requires meeting the preferences of both users. We initiate a rigorous\ntheoretical investigation of the reciprocal recommendation task in a specific\nframework of sequential learning. We point out general limitations, formulate\nreasonable assumptions enabling effective learning and, under these\nassumptions, we design and analyze a computationally efficient algorithm that\nuncovers mutual likes at a pace comparable to those achieved by a clearvoyant\nalgorithm knowing all user preferences in advance. Finally, we validate our\nalgorithm against synthetic and real-world datasets, showing improved empirical\nperformance over simple baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:26:51 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Vitale", "Fabio", ""], ["Parotsidis", "Nikos", ""], ["Gentile", "Claudio", ""]]}, {"id": "1806.01186", "submitter": "Victoria Krakovna", "authors": "Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, Shane\n  Legg", "title": "Penalizing side effects using stepwise relative reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design safe reinforcement learning agents that avoid unnecessary\ndisruptions to their environment? We show that current approaches to penalizing\nside effects can introduce bad incentives, e.g. to prevent any irreversible\nchanges in the environment, including the actions of other agents. To isolate\nthe source of such undesirable incentives, we break down side effects penalties\ninto two components: a baseline state and a measure of deviation from this\nbaseline state. We argue that some of these incentives arise from the choice of\nbaseline, and others arise from the choice of deviation measure. We introduce a\nnew variant of the stepwise inaction baseline and a new deviation measure based\non relative reachability of states. The combination of these design choices\navoids the given undesirable incentives, while simpler baselines and the\nunreachability measure fail. We demonstrate this empirically by comparing\ndifferent combinations of baseline and deviation measure choices on a set of\ngridworld experiments designed to illustrate possible bad incentives.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:30:17 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 09:17:21 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Krakovna", "Victoria", ""], ["Orseau", "Laurent", ""], ["Kumar", "Ramana", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""]]}, {"id": "1806.01203", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R.\n  McKee, Joshua B. Tenenbaum, Peter W. Battaglia", "title": "Relational inductive bias for physical construction in humans and\n  machines", "comments": "In Proceedings of the Annual Meeting of the Cognitive Science Society\n  (CogSci 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current deep learning systems excel at tasks such as object\nclassification, language processing, and gameplay, few can construct or modify\na complex system such as a tower of blocks. We hypothesize that what these\nsystems lack is a \"relational inductive bias\": a capacity for reasoning about\ninter-object relations and making choices over a structured description of a\nscene. To test this hypothesis, we focus on a task that involves gluing pairs\nof blocks together to stabilize a tower, and quantify how well humans perform.\nWe then introduce a deep reinforcement learning agent which uses object- and\nrelation-centric scene and policy representations and apply it to the task. Our\nresults show that these structured representations allow the agent to\noutperform both humans and more naive approaches, suggesting that relational\ninductive bias is an important component in solving structured reasoning\nproblems and for building more intelligent, flexible machines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:45:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Allen", "Kelsey R.", ""], ["Bapst", "Victor", ""], ["Zhu", "Tina", ""], ["McKee", "Kevin R.", ""], ["Tenenbaum", "Joshua B.", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1806.01235", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios and Alex Smola", "title": "Deep Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for deep learning on networks and graphs. It relies\non the notion that many graph algorithms, such as PageRank, Weisfeiler-Lehman,\nor Message Passing can be expressed as iterative vertex updates. Unlike\nprevious methods which rely on the ingenuity of the designer, Deep Graphs are\nadaptive to the estimation problem. Training and deployment are both efficient,\nsince the cost is $O(|E| + |V|)$, where $E$ and $V$ are the sets of edges and\nvertices respectively. In short, we learn the recurrent update functions rather\nthan positing their specific functional form. This yields an algorithm that\nachieves excellent accuracy on both graph labeling and regression tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:24:18 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""], ["Smola", "Alex", ""]]}, {"id": "1806.01240", "submitter": "Laurent Younes", "authors": "Laurent Younes", "title": "Diffeomorphic Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a learning paradigm in which the training data is\ntransformed by a diffeomorphic transformation before prediction. The learning\nalgorithm minimizes a cost function evaluating the prediction error on the\ntraining set penalized by the distance between the diffeomorphism and the\nidentity. The approach borrows ideas from shape analysis where diffeomorphisms\nare estimated for shape and image alignment, and brings them in a previously\nunexplored setting, estimating, in particular diffeomorphisms in much larger\ndimensions. After introducing the concept and describing a learning algorithm,\nwe present diverse applications, mostly with synthetic examples, demonstrating\nthe potential of the approach, as well as some insight on how it can be\nimproved.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:28:04 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 19:11:10 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 20:06:32 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Younes", "Laurent", ""]]}, {"id": "1806.01242", "submitter": "Alvaro Sanchez-Gonzalez", "authors": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh\n  Merel, Martin Riedmiller, Raia Hadsell and Peter Battaglia", "title": "Graph networks as learnable physics engines for inference and control", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interacting with everyday physical scenes requires rich\nknowledge about the structure of the world, represented either implicitly in a\nvalue or policy function, or explicitly in a transition model. Here we\nintroduce a new class of learnable models--based on graph networks--which\nimplement an inductive bias for object- and relation-centric representations of\ncomplex, dynamical systems. Our results show that as a forward model, our\napproach supports accurate predictions from real and simulated data, and\nsurprisingly strong and efficient generalization, across eight distinct\nphysical systems which we varied parametrically and structurally. We also found\nthat our inference model can perform system identification. Our models are also\ndifferentiable, and support online planning via gradient-based trajectory\noptimization, as well as offline policy optimization. Our framework offers new\nopportunities for harnessing and exploiting rich knowledge about the world, and\ntakes a key step toward building machines with more human-like representations\nof the world.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:29:40 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Sanchez-Gonzalez", "Alvaro", ""], ["Heess", "Nicolas", ""], ["Springenberg", "Jost Tobias", ""], ["Merel", "Josh", ""], ["Riedmiller", "Martin", ""], ["Hadsell", "Raia", ""], ["Battaglia", "Peter", ""]]}, {"id": "1806.01248", "submitter": "Jie Zhang", "authors": "Jie Zhang and Xiaolong Wang and Dawei Li and Yalin Wang", "title": "Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent\n  Neural Network on Mobile Devices", "comments": "Accepted by IJCAI-ECAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) achieve cutting-edge performance on a\nvariety of problems. However, due to their high computational and memory\ndemands, deploying RNNs on resource constrained mobile devices is a challenging\ntask. To guarantee minimum accuracy loss with higher compression rate and\ndriven by the mobile resource requirement, we introduce a novel model\ncompression approach DirNet based on an optimized fast dictionary learning\nalgorithm, which 1) dynamically mines the dictionary atoms of the projection\ndictionary matrix within layer to adjust the compression rate 2) adaptively\nchanges the sparsity of sparse codes cross the hierarchical layers.\nExperimental results on language model and an ASR model trained with a 1000h\nspeech dataset demonstrate that our method significantly outperforms prior\napproaches. Evaluated on off-the-shelf mobile devices, we are able to reduce\nthe size of original model by eight times with real-time model inference and\nnegligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:39:58 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 22:01:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Jie", ""], ["Wang", "Xiaolong", ""], ["Li", "Dawei", ""], ["Wang", "Yalin", ""]]}, {"id": "1806.01258", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios", "title": "Agreement-based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is a problem that has occupied machine learning researchers\nfor a long time. Recently, its importance has become evident through\napplications in deep learning. We propose an agreement-based learning framework\nthat prevents many of the pitfalls associated with model selection. It relies\non coupling the training of multiple models by encouraging them to agree on\ntheir predictions while training. In contrast with other model selection and\ncombination approaches used in machine learning, the proposed framework is\ninspired by human learning. We also propose a learning algorithm defined within\nthis framework which manages to significantly outperform alternatives in\npractice, and whose performance improves further with the availability of\nunlabeled data. Finally, we describe a number of potential directions for\ndeveloping more flexible agreement-based learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:55:12 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""]]}, {"id": "1806.01259", "submitter": "Jack Kosaian", "authors": "Jack Kosaian, K.V. Rashmi, and Shivaram Venkataraman", "title": "Learning a Code: Machine Learning for Approximate Non-Linear Coded\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are typically run on large scale, distributed\ncompute infrastructure that routinely face a number of unavailabilities such as\nfailures and temporary slowdowns. Adding redundant computations using\ncoding-theoretic tools called \"codes\" is an emerging technique to alleviate the\nadverse effects of such unavailabilities. A code consists of an encoding\nfunction that proactively introduces redundant computation and a decoding\nfunction that reconstructs unavailable outputs using the available ones. Past\nwork focuses on using codes to provide resilience for linear computations and\nspecific iterative optimization algorithms. However, computations performed for\na variety of applications including inference on state-of-the-art machine\nlearning algorithms, such as neural networks, typically fall outside this\nrealm. In this paper, we propose taking a learning-based approach to designing\ncodes that can handle non-linear computations. We present carefully designed\nneural network architectures and a training methodology for learning encoding\nand decoding functions that produce approximate reconstructions of unavailable\ncomputation results. We present extensive experimental results demonstrating\nthe effectiveness of the proposed approach: we show that the our learned codes\ncan accurately reconstruct $64 - 98\\%$ of the unavailable predictions from\nneural-network based image classifiers on the MNIST, Fashion-MNIST, and\nCIFAR-10 datasets. To the best of our knowledge, this work proposes the first\nlearning-based approach for designing codes, and also presents the first\ncoding-theoretic solution that can provide resilience for any non-linear\n(differentiable) computation. Our results show that learning can be an\neffective technique for designing codes, and that learned codes are a highly\npromising approach for bringing the benefits of coding to non-linear\ncomputations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:56:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kosaian", "Jack", ""], ["Rashmi", "K. V.", ""], ["Venkataraman", "Shivaram", ""]]}, {"id": "1806.01260", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Oisin Mac Aodha, Michael Firman, Gabriel Brostow", "title": "Digging Into Self-Supervised Monocular Depth Estimation", "comments": "ICCV 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Per-pixel ground-truth depth data is challenging to acquire at scale. To\novercome this limitation, self-supervised learning has emerged as a promising\nalternative for training models to perform monocular depth estimation. In this\npaper, we propose a set of improvements, which together result in both\nquantitatively and qualitatively improved depth maps compared to competing\nself-supervised methods.\n  Research on self-supervised monocular training usually explores increasingly\ncomplex architectures, loss functions, and image formation models, all of which\nhave recently helped to close the gap with fully-supervised methods. We show\nthat a surprisingly simple model, and associated design choices, lead to\nsuperior predictions. In particular, we propose (i) a minimum reprojection\nloss, designed to robustly handle occlusions, (ii) a full-resolution\nmulti-scale sampling method that reduces visual artifacts, and (iii) an\nauto-masking loss to ignore training pixels that violate camera motion\nassumptions. We demonstrate the effectiveness of each component in isolation,\nand show high quality, state-of-the-art results on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:58:05 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 19:06:28 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 01:27:58 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2019 22:57:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Mac Aodha", "Oisin", ""], ["Firman", "Michael", ""], ["Brostow", "Gabriel", ""]]}, {"id": "1806.01261", "submitter": "Peter Battaglia", "authors": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro\n  Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti,\n  David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song,\n  Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen,\n  Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra,\n  Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu", "title": "Relational inductive biases, deep learning, and graph networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:58:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:33:54 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 17:51:36 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Battaglia", "Peter W.", ""], ["Hamrick", "Jessica B.", ""], ["Bapst", "Victor", ""], ["Sanchez-Gonzalez", "Alvaro", ""], ["Zambaldi", "Vinicius", ""], ["Malinowski", "Mateusz", ""], ["Tacchetti", "Andrea", ""], ["Raposo", "David", ""], ["Santoro", "Adam", ""], ["Faulkner", "Ryan", ""], ["Gulcehre", "Caglar", ""], ["Song", "Francis", ""], ["Ballard", "Andrew", ""], ["Gilmer", "Justin", ""], ["Dahl", "George", ""], ["Vaswani", "Ashish", ""], ["Allen", "Kelsey", ""], ["Nash", "Charles", ""], ["Langston", "Victoria", ""], ["Dyer", "Chris", ""], ["Heess", "Nicolas", ""], ["Wierstra", "Daan", ""], ["Kohli", "Pushmeet", ""], ["Botvinick", "Matt", ""], ["Vinyals", "Oriol", ""], ["Li", "Yujia", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1806.01264", "submitter": "Subhabrata Mukherjee", "authors": "Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li", "title": "OpenTag: Open Attribute Value Extraction from Product Profiles [Deep\n  Learning, Active Learning, Named Entity Recognition]", "comments": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, London, UK, August 19-23, 2018", "journal-ref": null, "doi": "10.1145/3219819.3219839", "report-no": null, "categories": "cs.CL cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of missing attribute values is to find values describing an\nattribute of interest from a free text input. Most past related work on\nextraction of missing attribute values work with a closed world assumption with\nthe possible set of values known beforehand, or use dictionaries of values and\nhand-crafted features. How can we discover new attribute values that we have\nnever seen before? Can we do this with limited human annotation or supervision?\nWe study this problem in the context of product catalogs that often have\nmissing values for many attributes of interest.\n  In this work, we leverage product profile information such as titles and\ndescriptions to discover missing values of product attributes. We develop a\nnovel deep tagging model OpenTag for this extraction problem with the following\ncontributions: (1) we formalize the problem as a sequence tagging task, and\npropose a joint model exploiting recurrent neural networks (specifically,\nbidirectional LSTM) to capture context and semantics, and Conditional Random\nFields (CRF) to enforce tagging consistency, (2) we develop a novel attention\nmechanism to provide interpretable explanation for our model's decisions, (3)\nwe propose a novel sampling strategy exploring active learning to reduce the\nburden of human annotation. OpenTag does not use any dictionary or hand-crafted\nfeatures as in prior works. Extensive experiments in real-life datasets in\ndifferent domains show that OpenTag with our active learning strategy discovers\nnew attribute values from as few as 150 annotated samples (reduction in 3.3x\namount of annotation effort) with a high F-score of 83%, outperforming\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:41:07 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 17:29:28 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zheng", "Guineng", ""], ["Mukherjee", "Subhabrata", ""], ["Dong", "Xin Luna", ""], ["Li", "Feifei", ""]]}, {"id": "1806.01265", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman", "title": "Equivalence Between Wasserstein and Value-Aware Loss for Model-based\n  Reinforcement Learning", "comments": "Accepted at the FAIM workshop \"Prediction and Generative Modeling in\n  Reinforcement Learning\", Stockholm, Sweden, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a generative model is a key component of model-based reinforcement\nlearning. Though learning a good model in the tabular setting is a simple task,\nlearning a useful model in the approximate setting is challenging. In this\ncontext, an important question is the loss function used for model learning as\nvarying the loss function can have a remarkable impact on effectiveness of\nplanning. Recently Farahmand et al. (2017) proposed a value-aware model\nlearning (VAML) objective that captures the structure of value function during\nmodel learning. Using tools from Asadi et al. (2018), we show that minimizing\nthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.\nThis equivalence improves our understanding of value-aware models, and also\ncreates a theoretical foundation for applications of Wasserstein in model-based\nreinforcement~learning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:54:18 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 12:53:13 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Asadi", "Kavosh", ""], ["Cater", "Evan", ""], ["Misra", "Dipendra", ""], ["Littman", "Michael L.", ""]]}, {"id": "1806.01267", "submitter": "Daiki Kimura", "authors": "Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, Sakyasingha\n  Dasgupta", "title": "Internal Model from Observations for Reward Shaping", "comments": "7 pages, 6 figures, ICML workshop (ALA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning methods require careful design involving a reward\nfunction to obtain the desired action policy for a given task. In the absence\nof hand-crafted reward functions, prior work on the topic has proposed several\nmethods for reward estimation by using expert state trajectories and action\npairs. However, there are cases where complete or good action information\ncannot be obtained from expert demonstrations. We propose a novel reinforcement\nlearning method in which the agent learns an internal model of observation on\nthe basis of expert-demonstrated state trajectories to estimate rewards without\ncompletely learning the dynamics of the external environment from state-action\npairs. The internal model is obtained in the form of a predictive model for the\ngiven expert state distribution. During reinforcement learning, the agent\npredicts the reward as a function of the difference between the actual state\nand the state predicted by the internal model. We conducted multiple\nexperiments in environments of varying complexity, including the Super Mario\nBros and Flappy Bird games. We show our method successfully trains good\npolicies directly from expert game-play videos.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 08:15:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 03:56:36 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 22:43:21 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2018 13:15:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kimura", "Daiki", ""], ["Chaudhury", "Subhajit", ""], ["Tachibana", "Ryuki", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1806.01316", "submitter": "Ryo Karakida", "authors": "Ryo Karakida, Shotaro Akaho, Shun-ichi Amari", "title": "Universal Statistics of Fisher Information in Deep Neural Networks: Mean\n  Field Approach", "comments": "Accepted at AISTATS2019. Main text: 10 pages, 2 figures.\n  Supplementary material: 9 pages, 2 figures, typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) is a fundamental quantity to represent\nthe characteristics of a stochastic model, including deep neural networks\n(DNNs). The present study reveals novel statistics of FIM that are universal\namong a wide class of DNNs. To this end, we use random weights and large width\nlimits, which enables us to utilize mean field theories. We investigate the\nasymptotic statistics of the FIM's eigenvalues and reveal that most of them are\nclose to zero while the maximum eigenvalue takes a huge value. Because the\nlandscape of the parameter space is defined by the FIM, it is locally flat in\nmost dimensions, but strongly distorted in others. Moreover, we demonstrate the\npotential usage of the derived statistics in learning strategies. First, small\neigenvalues that induce flatness can be connected to a norm-based capacity\nmeasure of generalization ability. Second, the maximum eigenvalue that induces\nthe distortion enables us to quantitatively estimate an appropriately sized\nlearning rate for gradient methods to converge.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:34:58 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 15:42:43 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 20:33:27 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Karakida", "Ryo", ""], ["Akaho", "Shotaro", ""], ["Amari", "Shun-ichi", ""]]}, {"id": "1806.01318", "submitter": "Hyang-Won Lee", "authors": "Hyang-Won Lee, Jianan Zhang, Eytan Modiano", "title": "Data-driven Localization and Estimation of Disturbance in the\n  Interconnected Power System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the location of a disturbance and its magnitude is an important\ncomponent for stable operation of power systems. We study the problem of\nlocalizing and estimating a disturbance in the interconnected power system. We\ntake a model-free approach to this problem by using frequency data from\ngenerators. Specifically, we develop a logistic regression based method for\nlocalization and a linear regression based method for estimation of the\nmagnitude of disturbance. Our model-free approach does not require the\nknowledge of system parameters such as inertia constants and topology, and is\nshown to achieve highly accurate localization and estimation performance even\nin the presence of measurement noise and missing data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:38:13 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Lee", "Hyang-Won", ""], ["Zhang", "Jianan", ""], ["Modiano", "Eytan", ""]]}, {"id": "1806.01330", "submitter": "Sunipa Dev", "authors": "Sunipa Dev, Safia Hassan, Jeff M. Phillips", "title": "Closed Form Word Embedding Alignment", "comments": "Accepted ICDM 2019 and KAIS Special Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a family of techniques to align word embeddings which are derived\nfrom different source datasets or created using different mechanisms (e.g.,\nGloVe or word2vec). Our methods are simple and have a closed form to optimally\nrotate, translate, and scale to minimize root mean squared errors or maximize\nthe average cosine similarity between two embeddings of the same vocabulary\ninto the same dimensional space. Our methods extend approaches known as\nAbsolute Orientation, which are popular for aligning objects in\nthree-dimensions, and generalize an approach by Smith etal (ICLR 2017). We\nprove new results for optimal scaling and for maximizing cosine similarity.\nThen we demonstrate how to evaluate the similarity of embeddings from different\nsources or mechanisms, and that certain properties like synonyms and analogies\nare preserved across the embeddings and can be enhanced by simply aligning and\naveraging ensembles of embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:03:52 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 00:07:17 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 18:57:29 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 19:46:43 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Dev", "Sunipa", ""], ["Hassan", "Safia", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1806.01337", "submitter": "Siavash Golkar", "authors": "Siavash Golkar and Kyle Cranmer", "title": "Backdrop: Stochastic Backpropagation", "comments": "11 pages, 9 figures, 2 tables. Source code available at\n  https://github.com/dexgen/backdrop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce backdrop, a flexible and simple-to-implement method, intuitively\ndescribed as dropout acting only along the backpropagation pipeline. Backdrop\nis implemented via one or more masking layers which are inserted at specific\npoints along the network. Each backdrop masking layer acts as the identity in\nthe forward pass, but randomly masks parts of the backward gradient\npropagation. Intuitively, inserting a backdrop layer after any convolutional\nlayer leads to stochastic gradients corresponding to features of that scale.\nTherefore, backdrop is well suited for problems in which the data have a\nmulti-scale, hierarchical structure. Backdrop can also be applied to problems\nwith non-decomposable loss functions where standard SGD methods are not well\nsuited. We perform a number of experiments and demonstrate that backdrop leads\nto significant improvements in generalization.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:15:25 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Golkar", "Siavash", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1806.01347", "submitter": "Josiah Hanna", "authors": "Josiah P. Hanna, Scott Niekum, Peter Stone", "title": "Importance Sampling Policy Evaluation with an Estimated Behavior Policy", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of off-policy evaluation in Markov decision\nprocesses. Off-policy evaluation is the task of evaluating the expected return\nof one policy with data generated by a different, behavior policy. Importance\nsampling is a technique for off-policy evaluation that re-weights off-policy\nreturns to account for differences in the likelihood of the returns between the\ntwo policies. In this paper, we study importance sampling with an estimated\nbehavior policy where the behavior policy estimate comes from the same set of\ndata used to compute the importance sampling estimate. We find that this\nestimator often lowers the mean squared error of off-policy evaluation compared\nto importance sampling with the true behavior policy or using a behavior policy\nthat is estimated from a separate data set. Intuitively, estimating the\nbehavior policy in this way corrects for error due to sampling in the\naction-space. Our empirical results also extend to other popular variants of\nimportance sampling and show that estimating a non-Markovian behavior policy\ncan further lower large-sample mean squared error even when the true behavior\npolicy is Markovian.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:47:24 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 16:58:16 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 20:47:55 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Hanna", "Josiah P.", ""], ["Niekum", "Scott", ""], ["Stone", "Peter", ""]]}, {"id": "1806.01353", "submitter": "Scott Lee", "authors": "Scott Lee", "title": "Natural Language Generation for Electronic Health Records", "comments": null, "journal-ref": null, "doi": "10.1038/s41746-018-0070-0", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of methods existing for generating synthetic electronic health\nrecords (EHRs), but they are not capable of generating unstructured text, like\nemergency department (ED) chief complaints, history of present illness or\nprogress notes. Here, we use the encoder-decoder model, a deep learning\nalgorithm that features in many contemporary machine translation systems, to\ngenerate synthetic chief complaints from discrete variables in EHRs, like age\ngroup, gender, and discharge diagnosis. After being trained end-to-end on\nauthentic records, the model can generate realistic chief complaint text that\npreserves much of the epidemiological information in the original data. As a\nside effect of the model's optimization goal, these synthetic chief complaints\nare also free of relatively uncommon abbreviation and misspellings, and they\ninclude none of the personally-identifiable information (PII) that was in the\ntraining data, suggesting it may be used to support the de-identification of\ntext in EHRs. When combined with algorithms like generative adversarial\nnetworks (GANs), our model could be used to generate fully-synthetic EHRs,\nfacilitating data sharing between healthcare providers and researchers and\nimproving our ability to develop machine learning methods tailored to the\ninformation in healthcare data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 12:01:48 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lee", "Scott", ""]]}, {"id": "1806.01363", "submitter": "Giuseppe Cuccu", "authors": "Giuseppe Cuccu, Julian Togelius, Philippe Cudre-Mauroux", "title": "Playing Atari with Six Neurons", "comments": "Accepted at AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning, applied to vision-based problems like Atari\ngames, maps pixels directly to actions; internally, the deep neural network\nbears the responsibility of both extracting useful information and making\ndecisions based on it. By separating the image processing from decision-making,\none could better understand the complexity of each task, as well as potentially\nfind smaller policy representations that are easier for humans to understand\nand may generalize better. To this end, we propose a new method for learning\npolicies and compact state representations separately but simultaneously for\npolicy approximation in reinforcement learning. State representations are\ngenerated by an encoder based on two novel algorithms: Increasing Dictionary\nVector Quantization makes the encoder capable of growing its dictionary size\nover time, to address new observations as they appear in an open-ended\nonline-learning context; Direct Residuals Sparse Coding encodes observations by\ndisregarding reconstruction error minimization, and aiming instead for highest\ninformation inclusion. The encoder autonomously selects observations online to\ntrain on, in order to maximize code sparsity. As the dictionary size increases,\nthe encoder produces increasingly larger inputs for the neural network: this is\naddressed by a variation of the Exponential Natural Evolution Strategies\nalgorithm which adapts its probability distribution dimensionality along the\nrun. We test our system on a selection of Atari games using tiny neural\nnetworks of only 6 to 18 neurons (depending on the game's controls). These are\nstill capable of achieving results comparable---and occasionally superior---to\nstate-of-the-art techniques which use two orders of magnitude more neurons.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:09:43 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 11:42:42 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cuccu", "Giuseppe", ""], ["Togelius", "Julian", ""], ["Cudre-Mauroux", "Philippe", ""]]}, {"id": "1806.01380", "submitter": "Asaf Cassel", "authors": "Asaf Cassel (1), Shie Mannor (2), Assaf Zeevi (3) ((1) School of\n  Computer Science, Tel Aviv University, (2) Faculty of Electrical Engineering,\n  Technion, Israel Institute of Technology, (3) Graudate School of Business,\n  Columbia University)", "title": "A General Framework for Bandit Problems Beyond Cumulative Objectives", "comments": "Preliminary version accepted for presentation at Conference on\n  Learning Theory (COLT) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic multi-armed bandit (MAB) problem is a common model for\nsequential decision problems. In the standard setup, a decision maker has to\nchoose at every instant between several competing arms, each of them provides a\nscalar random variable, referred to as a \"reward.\" Nearly all research on this\ntopic considers the total cumulative reward as the criterion of interest. This\nwork focuses on other natural objectives that cannot be cast as a sum over\nrewards, but rather more involved functions of the reward stream. Unlike the\ncase of cumulative criteria, in the problems we study here the oracle policy,\nthat knows the problem parameters a priori and is used to \"center\" the regret,\nis not trivial. We provide a systematic approach to such problems, and derive\ngeneral conditions under which the oracle policy is sufficiently tractable to\nfacilitate the design of optimism-based (upper confidence bound) learning\npolicies. These conditions elucidate an interesting interplay between the arm\nreward distributions and the performance metric. Our main findings are\nillustrated for several commonly used objectives such as conditional\nvalue-at-risk, mean-variance trade-offs, Sharpe-ratio, and more.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:48:57 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 12:27:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Cassel", "Asaf", ""], ["Mannor", "Shie", ""], ["Zeevi", "Assaf", ""]]}, {"id": "1806.01426", "submitter": "Zhibing Zhao", "authors": "Zhibing Zhao and Lirong Xia", "title": "Composite Marginal Likelihood Methods for Random Utility Models", "comments": "22 pages, 3 figures, accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and flexible\nrank-breaking-then-composite-marginal-likelihood (RBCML) framework for learning\nrandom utility models (RUMs), which include the Plackett-Luce model. We\ncharacterize conditions for the objective function of RBCML to be strictly\nlog-concave by proving that strict log-concavity is preserved under convolution\nand marginalization. We characterize necessary and sufficient conditions for\nRBCML to satisfy consistency and asymptotic normality. Experiments on synthetic\ndata show that RBCML for Gaussian RUMs achieves better statistical efficiency\nand computational efficiency than the state-of-the-art algorithm and our RBCML\nfor the Plackett-Luce model provides flexible tradeoffs between running time\nand statistical efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:27:58 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Zhao", "Zhibing", ""], ["Xia", "Lirong", ""]]}, {"id": "1806.01427", "submitter": "Cody Coleman", "authors": "Cody Coleman, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\n  Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Re, Matei Zaharia", "title": "Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance\n  Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have proposed hardware, software, and algorithmic optimizations\nto improve the computational performance of deep learning. While some of these\noptimizations perform the same operations faster (e.g., increasing GPU clock\nspeed), many others modify the semantics of the training procedure (e.g.,\nreduced precision), and can impact the final model's accuracy on unseen data.\nDue to a lack of standard evaluation criteria that considers these trade-offs,\nit is difficult to directly compare these optimizations. To address this\nproblem, we recently introduced DAWNBench, a benchmark competition focused on\nend-to-end training time to achieve near-state-of-the-art accuracy on an unseen\ndataset---a combined metric called time-to-accuracy (TTA). In this work, we\nanalyze the entries from DAWNBench, which received optimized submissions from\nmultiple industrial groups, to investigate the behavior of TTA as a metric as\nwell as trends in the best-performing entries. We show that TTA has a low\ncoefficient of variation and that models optimized for TTA generalize nearly as\nwell as those trained using standard methods. Additionally, even though\nDAWNBench entries were able to train ImageNet models in under 3 minutes, we\nfind they still underutilize hardware capabilities such as Tensor Cores.\nFurthermore, we find that distributed entries can spend more than half of their\ntime on communication. We show similar findings with entries to the MLPERF v0.5\nbenchmark.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:29:05 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 22:15:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Coleman", "Cody", ""], ["Kang", "Daniel", ""], ["Narayanan", "Deepak", ""], ["Nardi", "Luigi", ""], ["Zhao", "Tian", ""], ["Zhang", "Jian", ""], ["Bailis", "Peter", ""], ["Olukotun", "Kunle", ""], ["Re", "Chris", ""], ["Zaharia", "Matei", ""]]}, {"id": "1806.01445", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure\n  Leskovec", "title": "Embedding Logical Queries on Knowledge Graphs", "comments": "Published in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning low-dimensional embeddings of knowledge graphs is a powerful\napproach used to predict unobserved or missing edges between entities. However,\nan open challenge in this area is developing techniques that can go beyond\nsimple edge prediction and handle more complex logical queries, which might\ninvolve multiple unobserved edges, entities, and variables. For instance, given\nan incomplete biological knowledge graph, we might want to predict \"em what\ndrugs are likely to target proteins involved with both diseases X and Y?\" -- a\nquery that requires reasoning about all possible proteins that {\\em might}\ninteract with diseases X and Y. Here we introduce a framework to efficiently\nmake predictions about conjunctive logical queries -- a flexible but tractable\nsubset of first-order logic -- on incomplete knowledge graphs. In our approach,\nwe embed graph nodes in a low-dimensional space and represent logical operators\nas learned geometric operations (e.g., translation, rotation) in this embedding\nspace. By performing logical operations within a low-dimensional embedding\nspace, our approach achieves a time complexity that is linear in the number of\nquery variables, compared to the exponential complexity required by a naive\nenumeration-based approach. We demonstrate the utility of this framework in two\napplication studies on real-world datasets with millions of relations:\npredicting logical relationships in a network of drug-gene-disease interactions\nand in a graph-based representation of social interactions derived from a\npopular web forum.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 00:36:13 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 02:56:57 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 17:49:15 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 15:18:35 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hamilton", "William L.", ""], ["Bajaj", "Payal", ""], ["Zitnik", "Marinka", ""], ["Jurafsky", "Dan", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.01455", "submitter": "Jonathan Mei", "authors": "Jonathan Mei, Jos\\'e M.F. Moura", "title": "EigenNetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications donot have the benefit of the laws of physics to derive\nsuccinct descriptive models for observed data. In alternative,\ninterdependencies among $N$ time series $\\{ x_{nk}, k>0 \\}_{n=1}^{N}$ are\nnowadays often captured by a graph or network $G$ that in practice may be very\nlarge. The network itself may change over time as well (i.e., as $G_k$).\nTracking brute force the changes of time varying networks presents major\nchallenges, including the associated computational problems. Further, a large\nset of networks may not lend itself to useful analysis. This paper approximates\nthe time varying networks $\\left\\{G_k\\right\\}$ as weighted linear combinations\nof eigennetworks. The eigennetworks are fixed building blocks that are\nestimated by first learning the time series of graphs $G_k$ from the data $\\{\nx_{nk}, k>0 \\}_{n=1}^{N}$, followed by a Principal Network Analysis procedure.\nThe weights of the eigennetwork representation are eigenfeatures and the time\nvarying networks $\\left\\{G_k\\right\\}$ describe a trajectory in eigennetwork\nspace. These eigentrajectories should be smooth since the networks $G_k$ vary\nat a much slower rate than the data $x_{nk}$, except when structural network\nshifts occur reflecting potentially an abrupt change in the underlying\napplication and sources of the data. Algorithms for learning the time series of\ngraphs $\\left\\{G_k\\right\\}$, deriving the eigennetworks, eigenfeatures and\neigentrajectories, and detecting changepoints are presented. Experiments on\nsimulated data and with two real time series data (a voting record of the US\nsenate and genetic expression data for the \\textit{Drosophila Melanogaster} as\nit goes through its life cycle) demonstrate the performance of the learning and\nprovide interesting interpretations of the eigennetworks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 01:31:01 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 15:12:40 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Mei", "Jonathan", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1806.01466", "submitter": "Yongjin Lu", "authors": "Xin-Guang Yang and Yongjin Lu", "title": "Informative Gene Selection for Microarray Classification via Adaptive\n  Elastic Net with Conditional Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advantage of achieving a better performance under weak\nregularization, elastic net has attracted wide attention in statistics, machine\nlearning, bioinformatics, and other fields. In particular, a variation of the\nelastic net, adaptive elastic net (AEN), integrates the adaptive grouping\neffect. In this paper, we aim to develop a new algorithm: Adaptive Elastic Net\nwith Conditional Mutual Information (AEN-CMI) that further improves AEN by\nincorporating conditional mutual information into the gene selection process.\nWe apply this new algorithm to screen significant genes for two kinds of\ncancers: colon cancer and leukemia. Compared with other algorithms including\nSupport Vector Machine, Classic Elastic Net and Adaptive Elastic Net, the\nproposed algorithm, AEN-CMI, obtains the best classification performance using\nthe least number of genes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 02:18:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 14:19:23 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 13:42:50 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Yang", "Xin-Guang", ""], ["Lu", "Yongjin", ""]]}, {"id": "1806.01468", "submitter": "Yilin Zhang", "authors": "Yilin Zhang and Karl Rohe", "title": "Understanding Regularized Spectral Clustering via Graph Conductance", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper uses the relationship between graph conductance and spectral\nclustering to study (i) the failures of spectral clustering and (ii) the\nbenefits of regularization. The explanation is simple. Sparse and stochastic\ngraphs create a lot of small trees that are connected to the core of the graph\nby only one edge. Graph conductance is sensitive to these noisy `dangling\nsets'. Spectral clustering inherits this sensitivity. The second part of the\npaper starts from a previously proposed form of regularized spectral clustering\nand shows that it is related to the graph conductance on a `regularized graph'.\nWe call the conductance on the regularized graph CoreCut. Based upon previous\narguments that relate graph conductance to spectral clustering (e.g. Cheeger\ninequality), minimizing CoreCut relaxes to regularized spectral clustering.\nSimple inspection of CoreCut reveals why it is less sensitive to small cuts in\nthe graph. Together, these results show that unbalanced partitions from\nspectral clustering can be understood as overfitting to noise in the periphery\nof a sparse and stochastic graph. Regularization fixes this overfitting. In\naddition to this statistical benefit, these results also demonstrate how\nregularization can improve the computational speed of spectral clustering. We\nprovide simulations and data examples to illustrate these results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 02:41:44 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 18:28:56 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 04:42:19 GMT"}, {"version": "v4", "created": "Sat, 1 Dec 2018 05:46:00 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhang", "Yilin", ""], ["Rohe", "Karl", ""]]}, {"id": "1806.01471", "submitter": "Arjun Nitin Bhagoji", "authors": "Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal", "title": "PAC-learning in the presence of evasion adversaries", "comments": "14 pages, 2 figures (minor changes to biblatex output)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of evasion attacks during the test phase of machine learning\nalgorithms represents a significant challenge to both their deployment and\nunderstanding. These attacks can be carried out by adding imperceptible\nperturbations to inputs to generate adversarial examples and finding effective\ndefenses and detectors has proven to be difficult. In this paper, we step away\nfrom the attack-defense arms race and seek to understand the limits of what can\nbe learned in the presence of an evasion adversary. In particular, we extend\nthe Probably Approximately Correct (PAC)-learning framework to account for the\npresence of an adversary. We first define corrupted hypothesis classes which\narise from standard binary hypothesis classes in the presence of an evasion\nadversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted\nas the adversarial VC-dimension. We then show that sample complexity upper\nbounds from the Fundamental Theorem of Statistical learning can be extended to\nthe case of evasion adversaries, where the sample complexity is controlled by\nthe adversarial VC-dimension. We then explicitly derive the adversarial\nVC-dimension for halfspace classifiers in the presence of a sample-wise\nnorm-constrained adversary of the type commonly studied for evasion attacks and\nshow that it is the same as the standard VC-dimension, closing an open\nquestion. Finally, we prove that the adversarial VC-dimension can be either\nlarger or smaller than the standard VC-dimension depending on the hypothesis\nclass and adversary, making it an interesting object of study in its own right.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 02:55:56 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 14:39:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Cullina", "Daniel", ""], ["Bhagoji", "Arjun Nitin", ""], ["Mittal", "Prateek", ""]]}, {"id": "1806.01477", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Bo Dong, Dan Schonfeld, Anthony Hoogs", "title": "An Explainable Adversarial Robustness Metric for Deep Learning Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks(DNN) have excessively advanced the field of computer\nvision by achieving state of the art performance in various vision tasks. These\nresults are not limited to the field of vision but can also be seen in speech\nrecognition and machine translation tasks. Recently, DNNs are found to poorly\nfail when tested with samples that are crafted by making imperceptible changes\nto the original input images. This causes a gap between the validation and\nadversarial performance of a DNN. An effective and generalizable robustness\nmetric for evaluating the performance of DNN on these adversarial inputs is\nstill missing from the literature. In this paper, we propose Noise Sensitivity\nScore (NSS), a metric that quantifies the performance of a DNN on a specific\ninput under different forms of fix-directional attacks. An insightful\nmathematical explanation is provided for deeply understanding the proposed\nmetric. By leveraging the NSS, we also proposed a skewness based dataset\nrobustness metric for evaluating a DNN's adversarial performance on a given\ndataset. Extensive experiments using widely used state of the art architectures\nalong with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100,\nand ImageNet, are used to validate the effectiveness and generalization of our\nproposed metrics. Instead of simply measuring a DNN's adversarial robustness in\nthe input domain, as previous works, the proposed NSS is built on top of\ninsightful mathematical understanding of the adversarial attack and gives a\nmore explicit explanation of the robustness.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 03:07:56 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 04:08:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Agarwal", "Chirag", ""], ["Dong", "Bo", ""], ["Schonfeld", "Dan", ""], ["Hoogs", "Anthony", ""]]}, {"id": "1806.01486", "submitter": "Alexander Stec", "authors": "Alexander Stec and Diego Klabjan", "title": "Forecasting Crime with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to take advantage of deep neural networks in\norder to make next day crime count predictions in a fine-grain city partition.\nWe make predictions using Chicago and Portland crime data, which is augmented\nwith additional datasets covering weather, census data, and public\ntransportation. The crime counts are broken into 10 bins and our model predicts\nthe most likely bin for a each spatial region at a daily level. We train this\ndata using increasingly complex neural network structures, including variations\nthat are suited to the spatial and temporal aspects of the crime prediction\nproblem. With our best model we are able to predict the correct bin for overall\ncrime count with 75.6% and 65.3% accuracy for Chicago and Portland,\nrespectively. The results show the efficacy of neural networks for the\nprediction problem and the value of using external datasets in addition to\nstandard crime data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 04:08:12 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Stec", "Alexander", ""], ["Klabjan", "Diego", ""]]}, {"id": "1806.01488", "submitter": "Cheng Soon Ong", "authors": "Finnian Lattimore and Cheng Soon Ong", "title": "A Primer on Causal Analysis", "comments": "Parts of this document are copied verbatim from Finnian Lattimore's\n  PhD thesis, ANU 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a conceptual map to navigate causal analysis problems. Focusing on\nthe case of discrete random variables, we consider the case of causal effect\nestimation from observational data. The presented approaches apply also to\ncontinuous variables, but the issue of estimation becomes more complex. We then\nintroduce the four schools of thought for causal analysis\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 04:19:22 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Lattimore", "Finnian", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1806.01502", "submitter": "Yen Yu", "authors": "Yen Yu, Acer Y.C. Chang, Ryota Kanai", "title": "Boredom-driven curious learning by Homeo-Heterostatic Value Gradients", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": "10.3389/fnbot.2018.00088", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Homeo-Heterostatic Value Gradients (HHVG) algorithm\nas a formal account on the constructive interplay between boredom and curiosity\nwhich gives rise to effective exploration and superior forward model learning.\nWe envisaged actions as instrumental in agent's own epistemic disclosure. This\nmotivated two central algorithmic ingredients: devaluation and devaluation\nprogress, both underpin agent's cognition concerning intrinsically generated\nrewards. The two serve as an instantiation of homeostatic and heterostatic\nintrinsic motivation. A key insight from our algorithm is that the two\nseemingly opposite motivations can be reconciled---without which exploration\nand information-gathering cannot be effectively carried out. We supported this\nclaim with empirical evidence, showing that boredom-enabled agents consistently\noutperformed other curious or explorative agent variants in model building\nbenchmarks based on self-assisted experience accumulation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 05:34:46 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Yu", "Yen", ""], ["Chang", "Acer Y. C.", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.01528", "submitter": "Dmytro Perekrestenko", "authors": "Dmytro Perekrestenko, Philipp Grohs, Dennis Elbr\\\"achter, Helmut\n  B\\\"olcskei", "title": "The universal approximation power of finite-width deep ReLU networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that finite-width deep ReLU neural networks yield rate-distortion\noptimal approximation (B\\\"olcskei et al., 2018) of polynomials, windowed\nsinusoidal functions, one-dimensional oscillatory textures, and the Weierstrass\nfunction, a fractal function which is continuous but nowhere differentiable.\nTogether with their recently established universal approximation property of\naffine function systems (B\\\"olcskei et al., 2018), this shows that deep neural\nnetworks approximate vastly different signal structures generated by the affine\ngroup, the Weyl-Heisenberg group, or through warping, and even certain\nfractals, all with approximation error decaying exponentially in the number of\nneurons. We also prove that in the approximation of sufficiently smooth\nfunctions finite-width deep networks require strictly smaller connectivity than\nfinite-depth wide networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 07:37:59 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Perekrestenko", "Dmytro", ""], ["Grohs", "Philipp", ""], ["Elbr\u00e4chter", "Dennis", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1806.01540", "submitter": "Antonio Farias", "authors": "Valdigleis S. Costaa, Antonio Diego S. Farias, Benjam\\'in Bedregal,\n  Regivan H. N. Santiago, Anne Magaly de P. Canuto", "title": "Combining Multiple Algorithms in Classifier Ensembles using Generalized\n  Mixture Functions", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.06.021", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier ensembles are pattern recognition structures composed of a set of\nclassification algorithms (members), organized in a parallel way, and a\ncombination method with the aim of increasing the classification accuracy of a\nclassification system. In this study, we investigate the application of a\ngeneralized mixture (GM) functions as a new approach for providing an efficient\ncombination procedure for these systems through the use of dynamic weights in\nthe combination process. Therefore, we present three GM functions to be applied\nas a combination method. The main advantage of these functions is that they can\ndefine dynamic weights at the member outputs, making the combination process\nmore efficient. In order to evaluate the feasibility of the proposed approach,\nan empirical analysis is conducted, applying classifier ensembles to 25\ndifferent classification data sets. In this analysis, we compare the use of the\nproposed approaches to ensembles using traditional combination methods as well\nas the state-of-the-art ensemble methods. Our findings indicated gains in terms\nof performance when comparing the proposed approaches to the traditional ones\nas well as comparable results with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:11:16 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Costaa", "Valdigleis S.", ""], ["Farias", "Antonio Diego S.", ""], ["Bedregal", "Benjam\u00edn", ""], ["Santiago", "Regivan H. N.", ""], ["Canuto", "Anne Magaly de P.", ""]]}, {"id": "1806.01547", "submitter": "Gullal Singh Cheema", "authors": "Ankita Shukla, Gullal Singh Cheema, Saket Anand", "title": "Semi-Supervised Clustering with Neural Networks", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering using neural networks has recently demonstrated promising\nperformance in machine learning and computer vision applications. However, the\nperformance of current approaches is limited either by unsupervised learning or\ntheir dependence on large set of labeled data samples. In this paper, we\npropose ClusterNet that uses pairwise semantic constraints from very few\nlabeled data samples (<5% of total data) and exploits the abundant unlabeled\ndata to drive the clustering approach. We define a new loss function that uses\npairwise semantic similarity between objects combined with constrained k-means\nclustering to efficiently utilize both labeled and unlabeled data in the same\nframework. The proposed network uses convolution autoencoder to learn a latent\nrepresentation that groups data into k specified clusters, while also learning\nthe cluster centers simultaneously. We evaluate and compare the performance of\nClusterNet on several datasets and state of the art deep clustering approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:23:42 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 09:10:35 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Shukla", "Ankita", ""], ["Cheema", "Gullal Singh", ""], ["Anand", "Saket", ""]]}, {"id": "1806.01551", "submitter": "Ingyo Chung", "authors": "Ingyo Chung, Saehoon Kim, Juho Lee, Kwang Joon Kim, Sung Ju Hwang,\n  Eunho Yang", "title": "Deep Mixed Effect Model using Gaussian Processes: A Personalized and\n  Reliable Prediction for Healthcare", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a personalized and reliable prediction model for healthcare, which\ncan provide individually tailored medical services such as diagnosis, disease\ntreatment, and prevention. Our proposed framework targets at making\npersonalized and reliable predictions from time-series data, such as Electronic\nHealth Records (EHR), by modeling two complementary components: i) a shared\ncomponent that captures global trend across diverse patients and ii) a\npatient-specific component that models idiosyncratic variability for each\npatient. To this end, we propose a composite model of a deep neural network to\nlearn complex global trends from the large number of patients, and Gaussian\nProcesses (GP) to probabilistically model individual time-series given\nrelatively small number of visits per patient. We evaluate our model on diverse\nand heterogeneous tasks from EHR datasets and show practical advantages over\nstandard time-series deep models such as pure Recurrent Neural Network (RNN).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:26:53 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 09:44:42 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 03:50:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chung", "Ingyo", ""], ["Kim", "Saehoon", ""], ["Lee", "Juho", ""], ["Kim", "Kwang Joon", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "1806.01552", "submitter": "Jerome Darmont", "authors": "Ayb\\\"uk\\\"e Ozt\\\"urk (ERIC, ArAr), St\\'ephane Lallich (ERIC),\n  J\\'er\\^ome Darmont (ERIC)", "title": "A Visual Quality Index for Fuzzy C-Means", "comments": null, "journal-ref": "14th International Conference on Artificial Intelligence\n  Applications and Innovations (AIAI 2018), May 2018, Rhodes, Greece. Springer,\n  IFIP Advances in Information and Communication Technology, 519, pp.546-555,\n  2018, http://easyconferences.eu/aiai2018/", "doi": "10.1007/978-3-319-92007-8", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is widely used in the areas of machine learning and data\nmining. Fuzzy clustering is a particular method that considers that a data\npoint can belong to more than one cluster. Fuzzy clustering helps obtain\nflexible clusters, as needed in such applications as text categorization. The\nperformance of a clustering algorithm critically depends on the number of\nclusters, and estimating the optimal number of clusters is a challenging task.\nQuality indices help estimate the optimal number of clusters. However, there is\nno quality index that can obtain an accurate number of clusters for different\ndatasets. Thence, in this paper, we propose a new cluster quality index\nassociated with a visual, graph-based solution that helps choose the optimal\nnumber of clusters in fuzzy partitions. Moreover, we validate our theoretical\nresults through extensive comparison experiments against state-of-the-art\nquality indices on a variety of numerical real-world and artificial datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:27:40 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Ozt\u00fcrk", "Ayb\u00fck\u00eb", "", "ERIC, ArAr"], ["Lallich", "St\u00e9phane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1806.01579", "submitter": "Adam Gudy\\'s", "authors": "Marek Sikora, {\\L}ukasz Wr\\'obel, Adam Gudy\\'s", "title": "GuideR: a guided separate-and-conquer rule learning in classification,\n  regression, and survival settings", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2019.02.019", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents GuideR, a user-guided rule induction algorithm, which\novercomes the largest limitation of the existing methods-the lack of the\npossibility to introduce user's preferences or domain knowledge to the rule\nlearning process. Automatic selection of attributes and attribute ranges often\nleads to the situation in which resulting rules do not contain interesting\ninformation. We propose an induction algorithm which takes into account user's\nrequirements. Our method uses the sequential covering approach and is suitable\nfor classification, regression, and survival analysis problems. The\neffectiveness of the algorithm in all these tasks has been verified\nexperimentally, confirming guided rule induction to be a powerful data analysis\ntool.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 09:34:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sikora", "Marek", ""], ["Wr\u00f3bel", "\u0141ukasz", ""], ["Gudy\u015b", "Adam", ""]]}, {"id": "1806.01584", "submitter": "Thomas Dietterich", "authors": "Thomas G. Dietterich and George Trimponias and Zhitang Chen", "title": "Discovering and Removing Exogenous State Variables and Rewards for\n  Reinforcement Learning", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exogenous state variables and rewards can slow down reinforcement learning by\ninjecting uncontrolled variation into the reward signal. We formalize exogenous\nstate variables and rewards and identify conditions under which an MDP with\nexogenous state can be decomposed into an exogenous Markov Reward Process\ninvolving only the exogenous state+reward and an endogenous Markov Decision\nProcess defined with respect to only the endogenous rewards. We also derive a\nvariance-covariance condition under which Monte Carlo policy evaluation on the\nendogenous MDP is accelerated compared to using the full MDP. Similar speedups\nare likely to carry over to all RL algorithms. We develop two algorithms for\ndiscovering the exogenous variables and test them on several MDPs. Results show\nthat the algorithms are practical and can significantly speed up reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 09:44:41 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Dietterich", "Thomas G.", ""], ["Trimponias", "George", ""], ["Chen", "Zhitang", ""]]}, {"id": "1806.01600", "submitter": "Akshita Bhandari", "authors": "Akshita Bhandari and Chandramani Singh", "title": "Accelerated Randomized Coordinate Descent Algorithms for Stochastic\n  Optimization and Online Learning", "comments": "20 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose accelerated randomized coordinate descent algorithms for\nstochastic optimization and online learning. Our algorithms have significantly\nless per-iteration complexity than the known accelerated gradient algorithms.\nThe proposed algorithms for online learning have better regret performance than\nthe known randomized online coordinate descent algorithms. Furthermore, the\nproposed algorithms for stochastic optimization exhibit as good convergence\nrates as the best known randomized coordinate descent algorithms. We also show\nsimulation results to demonstrate performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:25:58 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 17:36:22 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Bhandari", "Akshita", ""], ["Singh", "Chandramani", ""]]}, {"id": "1806.01603", "submitter": "Simon Carbonnelle", "authors": "Simon Carbonnelle and Christophe De Vleeschouwer", "title": "Layer rotation: a surprisingly powerful indicator of generalization in\n  deep networks?", "comments": "Extended version of paper presented at ICML workshop \"Identifying and\n  Understanding Deep Learning Phenomena\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work presents extensive empirical evidence that layer rotation, i.e. the\nevolution across training of the cosine distance between each layer's weight\nvector and its initialization, constitutes an impressively consistent indicator\nof generalization performance. In particular, larger cosine distances between\nfinal and initial weights of each layer consistently translate into better\ngeneralization performance of the final model. Interestingly, this relation\nadmits a network independent optimum: training procedures during which all\nlayers' weights reach a cosine distance of 1 from their initialization\nconsistently outperform other configurations -by up to 30% test accuracy.\nMoreover, we show that layer rotations are easily monitored and controlled\n(helpful for hyperparameter tuning) and potentially provide a unified framework\nto explain the impact of learning rate tuning, weight decay, learning rate\nwarmups and adaptive gradient methods on generalization and training speed. In\nan attempt to explain the surprising properties of layer rotation, we show on a\n1-layer MLP trained on MNIST that layer rotation correlates with the degree to\nwhich features of intermediate layers have been trained.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:39:21 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 16:01:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Carbonnelle", "Simon", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1806.01610", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Patryk Chrab\\k{a}szcz, Frank Hutter, Tonio\n  Ball", "title": "Training Generative Reversible Networks", "comments": "Source code for this study is at\n  https://github.com/robintibor/generative-reversible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models with an encoding component such as autoencoders currently\nreceive great interest. However, training of autoencoders is typically\ncomplicated by the need to train a separate encoder and decoder model that have\nto be enforced to be reciprocal to each other. To overcome this problem,\nby-design reversible neural networks (RevNets) had been previously used as\ngenerative models either directly optimizing the likelihood of the data under\nthe model or using an adversarial approach on the generated data. Here, we\ninstead investigate their performance using an adversary on the latent space in\nthe adversarial autoencoder framework. We investigate the generative\nperformance of RevNets on the CelebA dataset, showing that generative RevNets\ncan generate coherent faces with similar quality as Variational Autoencoders.\nThis first attempt to use RevNets inside the adversarial autoencoder framework\nslightly underperformed relative to recent advanced generative models using an\nautoencoder component on CelebA, but this gap may diminish with further\noptimization of the training setup of generative RevNets. In addition to the\nexperiments on CelebA, we show a proof-of-principle experiment on the MNIST\ndataset suggesting that adversary-free trained RevNets can discover meaningful\nlatent dimensions without pre-specifying the number of dimensions of the latent\nsampling distribution. In summary, this study shows that RevNets can be\nemployed in different generative training settings.\n  Source code for this study is at\nhttps://github.com/robintibor/generative-reversible\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 11:16:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 08:40:34 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 23:22:27 GMT"}, {"version": "v4", "created": "Thu, 23 Aug 2018 12:07:40 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Chrab\u0105szcz", "Patryk", ""], ["Hutter", "Frank", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.01619", "submitter": "Changyong Oh", "authors": "ChangYong Oh, Efstratios Gavves, Max Welling", "title": "BOCK : Bayesian Optimization with Cylindrical Kernels", "comments": "10 pages, 5 figures, 5 tables, 1 algorithm", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, Stockholm, Sweden, PMLR 80, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in Bayesian Optimization is the boundary issue (Swersky,\n2017) where an algorithm spends too many evaluations near the boundary of its\nsearch space. In this paper, we propose BOCK, Bayesian Optimization with\nCylindrical Kernels, whose basic idea is to transform the ball geometry of the\nsearch space using a cylindrical transformation. Because of the transformed\ngeometry, the Gaussian Process-based surrogate model spends less budget\nsearching near the boundary, while concentrating its efforts relatively more\nnear the center of the search region, where we expect the solution to be\nlocated. We evaluate BOCK extensively, showing that it is not only more\naccurate and efficient, but it also scales successfully to problems with a\ndimensionality as high as 500. We show that the better accuracy and scalability\nof BOCK even allows optimizing modestly sized neural network layers, as well as\nneural network hyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 11:57:03 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 20:35:46 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Oh", "ChangYong", ""], ["Gavves", "Efstratios", ""], ["Welling", "Max", ""]]}, {"id": "1806.01655", "submitter": "P.K. Srijith", "authors": "Vinayak Kumar, Vaibhav Singh, P. K. Srijith, Andreas Damianou", "title": "Deep Gaussian Processes with Convolutional Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative\nto standard parametric deep learning models. A DGP is formed by stacking\nmultiple GPs resulting in a well-regularized composition of functions. The\nBayesian framework that equips the model with attractive properties, such as\nimplicit capacity control and predictive uncertainty, makes it at the same time\nchallenging to combine with a convolutional structure. This has hindered the\napplication of DGPs in computer vision tasks, an area where deep parametric\nmodels (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such\nas radial basis functions (RBFs) are insufficient for handling pixel\nvariability in raw images. In this paper, we build on the recent convolutional\nGP to develop Convolutional DGP (CDGP) models which effectively capture image\nlevel features through the use of convolution kernels, therefore opening up the\nway for applying DGPs to computer vision tasks. Our model learns local spatial\ninfluence and outperforms strong GP based baselines on multi-class image\nclassification. We also consider various constructions of convolution kernel\nover the image patches, analyze the computational trade-offs and provide an\nefficient framework for convolutional DGP models. The experimental results on\nimage data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate\nthe effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:41:14 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kumar", "Vinayak", ""], ["Singh", "Vaibhav", ""], ["Srijith", "P. K.", ""], ["Damianou", "Andreas", ""]]}, {"id": "1806.01660", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, Tuo Zhao", "title": "Towards Understanding Acceleration Tradeoff between Momentum and\n  Asynchrony in Nonconvex Stochastic Optimization", "comments": "arXiv admin note: text overlap with arXiv:1802.05155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) is\none of the most popular algorithms in distributed machine learning. However,\nits convergence properties for these complicated nonconvex problems is still\nlargely unknown, because of the current technical limit. Therefore, in this\npaper, we propose to analyze the algorithm through a simpler but nontrivial\nnonconvex problem - streaming PCA, which helps us to understand Aync-MSGD\nbetter even for more general problems. Specifically, we establish the\nasymptotic rate of convergence of Async-MSGD for streaming PCA by diffusion\napproximation. Our results indicate a fundamental tradeoff between asynchrony\nand momentum: To ensure convergence and acceleration through asynchrony, we\nhave to reduce the momentum (compared with Sync-MSGD). To the best of our\nknowledge, this is the first theoretical attempt on understanding Async-MSGD\nfor distributed nonconvex stochastic optimization. Numerical experiments on\nboth streaming PCA and training deep neural networks are provided to support\nour findings for Async-MSGD.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:49:47 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 13:10:51 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 14:23:12 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2018 17:58:20 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 21:23:34 GMT"}, {"version": "v6", "created": "Wed, 13 Jan 2021 14:50:06 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Tianyi", ""], ["Li", "Shiyang", ""], ["Shi", "Jianping", ""], ["Zhou", "Enlu", ""], ["Zhao", "Tuo", ""]]}, {"id": "1806.01670", "submitter": "Igor Sieradzki", "authors": "Damian Le\\'sniak, Igor Sieradzki, Igor Podolak", "title": "On Latent Distributions Without Finite Mean in Generative Models", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the properties of multidimensional probability distributions\nin the context of latent space prior distributions of implicit generative\nmodels. Our work revolves around the phenomena arising while decoding linear\ninterpolations between two random latent vectors -- regions of latent space in\nclose proximity to the origin of the space are sampled causing distribution\nmismatch. We show that due to the Central Limit Theorem, this region is almost\nnever sampled during the training process. As a result, linear interpolations\nmay generate unrealistic data and their usage as a tool to check quality of the\ntrained model is questionable. We propose to use multidimensional Cauchy\ndistribution as the latent prior. Cauchy distribution does not satisfy the\nassumptions of the CLT and has a number of properties that allow it to work\nwell in conjunction with linear interpolations. We also provide two general\nmethods of creating non-linear interpolations that are easily applicable to a\nlarge family of common latent distributions. Finally we empirically analyze the\nquality of data generated from low-probability-mass regions for the DCGAN model\non the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:07:16 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Le\u015bniak", "Damian", ""], ["Sieradzki", "Igor", ""], ["Podolak", "Igor", ""]]}, {"id": "1806.01678", "submitter": "Nate Veldt", "authors": "Nate Veldt and David Gleich and Anthony Wirth and James Saunderson", "title": "A Projection Method for Metric-Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:25:30 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Veldt", "Nate", ""], ["Gleich", "David", ""], ["Wirth", "Anthony", ""], ["Saunderson", "James", ""]]}, {"id": "1806.01731", "submitter": "Ali Fathi Baghbadorani", "authors": "Greg Kirczenow, Ali Fathi, Matt Davison", "title": "Machine Learning for Yield Curve Feature Extraction: Application to\n  Illiquid Corporate Bonds (Preliminary Draft)", "comments": "5 figures, Presented in CAIMS 2018-Toronto, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the application of machine learning in extracting the\nmarket implied features from historical risk neutral corporate bond yields. We\nconsider the example of a hypothetical illiquid fixed income market. After\nchoosing a surrogate liquid market, we apply the Denoising Autoencoder\nalgorithm from the field of computer vision and pattern recognition to learn\nthe features of the missing yield parameters from the historically implied data\nof the instruments traded in the chosen liquid market. The results of the\ntrained machine learning algorithm are compared with the outputs of a point in-\ntime 2 dimensional interpolation algorithm known as the Thin Plate Spline.\nFinally, the performances of the two algorithms are compared.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:01:17 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kirczenow", "Greg", ""], ["Fathi", "Ali", ""], ["Davison", "Matt", ""]]}, {"id": "1806.01738", "submitter": "Sarah Parisot", "authors": "Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo\n  Guerrero, Ben Glocker, Daniel Rueckert", "title": "Disease Prediction using Graph Convolutional Networks: Application to\n  Autism Spectrum Disorder and Alzheimer's Disease", "comments": "in Press at Medical Image Analysis, MICCAI 2017 Special Issue", "journal-ref": null, "doi": "10.1016/j.media.2018.06.001", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are widely used as a natural framework that captures interactions\nbetween individual elements represented as nodes in a graph. In medical\napplications, specifically, nodes can represent individuals within a\npotentially large population (patients or healthy controls) accompanied by a\nset of features, while the graph edges incorporate associations between\nsubjects in an intuitive manner. This representation allows to incorporate the\nwealth of imaging and non-imaging information as well as individual subject\nfeatures simultaneously in disease classification tasks. Previous graph-based\napproaches for supervised or unsupervised learning in the context of disease\nprediction solely focus on pairwise similarities between subjects, disregarding\nindividual characteristics and features, or rather rely on subject-specific\nimaging feature vectors and fail to model interactions between them. In this\npaper, we present a thorough evaluation of a generic framework that leverages\nboth imaging and non-imaging information and can be used for brain analysis in\nlarge populations. This framework exploits Graph Convolutional Networks (GCNs)\nand involves representing populations as a sparse graph, where its nodes are\nassociated with imaging-based feature vectors, while phenotypic information is\nintegrated as edge weights. The extensive evaluation explores the effect of\neach individual component of this framework on disease prediction performance\nand further compares it to different baselines. The framework performance is\ntested on two large datasets with diverse underlying data, ABIDE and ADNI, for\nthe prediction of Autism Spectrum Disorder and conversion to Alzheimer's\ndisease, respectively. Our analysis shows that our novel framework can improve\nover state-of-the-art results on both databases, with 70.4% classification\naccuracy for ABIDE and 80.0% for ADNI.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:10:53 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Parisot", "Sarah", ""], ["Ktena", "Sofia Ira", ""], ["Ferrante", "Enzo", ""], ["Lee", "Matthew", ""], ["Guerrero", "Ricardo", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.01743", "submitter": "Xingyu Fu", "authors": "XingYu Fu and JinHong Du and YiFeng Guo and MingWen Liu and Tao Dong\n  and XiuWen Duan", "title": "A Machine Learning Framework for Stock Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates how to apply machine learning algorithms to\ndistinguish good stocks from the bad stocks. To this end, we construct 244\ntechnical and fundamental features to characterize each stock, and label stocks\naccording to their ranking with respect to the return-to-volatility ratio.\nAlgorithms ranging from traditional statistical learning methods to recently\npopular deep learning method, e.g. Logistic Regression (LR), Random Forest\n(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the\nclassification task. Genetic Algorithm (GA) is also used to implement feature\nselection. The effectiveness of the stock selection strategy is validated in\nChinese stock market in both statistical and practical aspects, showing that:\n1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic\nAlgorithm picks a subset of 114 features and the prediction performances of all\nmodels remain almost unchanged after the selection procedure, which suggests\nsome features are indeed redundant; 3) LR and DNN are radical models; RF is\nrisk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios\nconstructed by our models outperform market average in back tests.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:23:10 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 09:54:20 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Fu", "XingYu", ""], ["Du", "JinHong", ""], ["Guo", "YiFeng", ""], ["Liu", "MingWen", ""], ["Dong", "Tao", ""], ["Duan", "XiuWen", ""]]}, {"id": "1806.01754", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki and Aapo Hyv\\\"arinen", "title": "Neural-Kernelized Conditional Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional density estimation is a general framework for solving various\nproblems in machine learning. Among existing methods, non-parametric and/or\nkernel-based methods are often difficult to use on large datasets, while\nmethods based on neural networks usually make restrictive parametric\nassumptions on the probability densities. Here, we propose a novel method for\nestimating the conditional density based on score matching. In contrast to\nexisting methods, we employ scalable neural networks, but do not make explicit\nparametric assumptions on densities. The key challenge in applying score\nmatching to neural networks is computation of the first- and second-order\nderivatives of a model for the log-density. We tackle this challenge by\ndeveloping a new neural-kernelized approach, which can be applied on large\ndatasets with stochastic gradient descent, while the reproducing kernels allow\nfor easy computation of the derivatives needed in score matching. We show that\nthe neural-kernelized function approximator has universal approximation\ncapability and that our method is consistent in conditional density estimation.\nWe numerically demonstrate that our method is useful in high-dimensional\nconditional density estimation, and compares favourably with existing methods.\nFinally, we prove that the proposed method has interesting connections to two\nprobabilistically principled frameworks of representation learning: Nonlinear\nsufficient dimension reduction and nonlinear independent component analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:43:13 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Hyv\u00e4rinen", "Aapo", ""]]}, {"id": "1806.01768", "submitter": "Murat Sensoy", "authors": "Murat Sensoy, Lance Kaplan, Melih Kandemir", "title": "Evidential Deep Learning to Quantify Classification Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic neural nets have been shown to learn effective predictors on a\nwide range of machine learning problems. However, as the standard approach is\nto train the network to minimize a prediction loss, the resultant model remains\nignorant to its prediction confidence. Orthogonally to Bayesian neural nets\nthat indirectly infer prediction uncertainty through weight uncertainties, we\npropose explicit modeling of the same using the theory of subjective logic. By\nplacing a Dirichlet distribution on the class probabilities, we treat\npredictions of a neural net as subjective opinions and learn the function that\ncollects the evidence leading to these opinions by a deterministic neural net\nfrom data. The resultant predictor for a multi-class classification problem is\nanother Dirichlet distribution whose parameters are set by the continuous\noutput of a neural net. We provide a preliminary analysis on how the\npeculiarities of our new loss function drive improved uncertainty estimation.\nWe observe that our method achieves unprecedented success on detection of\nout-of-distribution queries and endurance against adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:07:27 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 19:41:22 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 23:49:45 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Sensoy", "Murat", ""], ["Kaplan", "Lance", ""], ["Kandemir", "Melih", ""]]}, {"id": "1806.01771", "submitter": "Louis Tiao", "authors": "Louis C. Tiao, Edwin V. Bonilla, Fabio Ramos", "title": "Cycle-Consistent Adversarial Learning as Approximate Bayesian Inference", "comments": "Presented at the ICML 2018 Workshop on Theoretical Foundations and\n  Applications of Deep Generative Models. Stockholm, Sweden, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the problem of learning interdomain correspondences in the\nabsence of paired data as Bayesian inference in a latent variable model (LVM),\nwhere one seeks the underlying hidden representations of entities from one\ndomain as entities from the other domain. First, we introduce implicit latent\nvariable models, where the prior over hidden representations can be specified\nflexibly as an implicit distribution. Next, we develop a new variational\ninference (VI) algorithm for this model based on minimization of the symmetric\nKullback-Leibler (KL) divergence between a variational joint and the exact\njoint distribution. Lastly, we demonstrate that the state-of-the-art\ncycle-consistent adversarial learning (CYCLEGAN) models can be derived as a\nspecial case within our proposed VI framework, thus establishing its connection\nto approximate Bayesian inference methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:09:45 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 09:07:21 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 04:22:19 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Tiao", "Louis C.", ""], ["Bonilla", "Edwin V.", ""], ["Ramos", "Fabio", ""]]}, {"id": "1806.01780", "submitter": "Siddhant M. Jayakumar", "authors": "Wojciech Marian Czarnecki, Siddhant M. Jayakumar, Max Jaderberg,\n  Leonard Hasenclever, Yee Whye Teh, Simon Osindero, Nicolas Heess, Razvan\n  Pascanu", "title": "Mix&Match - Agent Curricula for Reinforcement Learning", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Mix&Match (M&M) - a training framework designed to facilitate\nrapid and effective learning in RL agents, especially those that would be too\nslow or too challenging to train otherwise. The key innovation is a procedure\nthat allows us to automatically form a curriculum over agents. Through such a\ncurriculum we can progressively train more complex agents by, effectively,\nbootstrapping from solutions found by simpler agents. In contradistinction to\ntypical curriculum learning approaches, we do not gradually modify the tasks or\nenvironments presented, but instead use a process to gradually alter how the\npolicy is represented internally. We show the broad applicability of our method\nby demonstrating significant performance gains in three different experimental\nsetups: (1) We train an agent able to control more than 700 actions in a\nchallenging 3D first-person task; using our method to progress through an\naction-space curriculum we achieve both faster training and better final\nperformance than one obtains using traditional methods. (2) We further show\nthat M&M can be used successfully to progress through a curriculum of\narchitectural variants defining an agents internal state. (3) Finally, we\nillustrate how a variant of our method can be used to improve agent performance\nin a multitask setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:21:25 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Jayakumar", "Siddhant M.", ""], ["Jaderberg", "Max", ""], ["Hasenclever", "Leonard", ""], ["Teh", "Yee Whye", ""], ["Osindero", "Simon", ""], ["Heess", "Nicolas", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1806.01793", "submitter": "Daniel Recoskie", "authors": "Daniel Recoskie, Richard Mann", "title": "Gradient-based Filter Design for the Dual-tree Wavelet Transform", "comments": "19 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet transform has seen success when incorporated into neural network\narchitectures, such as in wavelet scattering networks. More recently, it has\nbeen shown that the dual-tree complex wavelet transform can provide better\nrepresentations than the standard transform. With this in mind, we extend our\nprevious method for learning filters for the 1D and 2D wavelet transforms into\nthe dual-tree domain. We show that with few modifications to our original\nmodel, we can learn directional filters that leverage the properties of the\ndual-tree wavelet transform.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:29:23 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Recoskie", "Daniel", ""], ["Mann", "Richard", ""]]}, {"id": "1806.01794", "submitter": "Adam Kosiorek", "authors": "Adam R. Kosiorek, Hyunjik Kim, Ingmar Posner, Yee Whye Teh", "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects", "comments": "25 pages, 19 figures, NeurIPS 2018, code:\n  https://github.com/akosiorek/sqair, video: https://youtu.be/-IUNQgSLE0c", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep\ngenerative model for videos of moving objects. It can reliably discover and\ntrack objects throughout the sequence of frames, and can also generate future\nframes conditioning on the current frame, thereby simulating expected motion of\nobjects. This is achieved by explicitly encoding object presence, locations and\nappearances in the latent variables of the model. SQAIR retains all strengths\nof its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016),\nincluding learning in an unsupervised manner, and addresses its shortcomings.\nWe use a moving multi-MNIST dataset to show limitations of AIR in detecting\noverlapping or partially occluded objects, and show how SQAIR overcomes them by\nleveraging temporal consistency of objects. Finally, we also apply SQAIR to\nreal-world pedestrian CCTV data, where it learns to reliably detect, track and\ngenerate walking pedestrians with no supervision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:29:44 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:08:23 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Kim", "Hyunjik", ""], ["Posner", "Ingmar", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1806.01796", "submitter": "Mor Shpigel Nacson", "authors": "Mor Shpigel Nacson, Nathan Srebro, Daniel Soudry", "title": "Stochastic Gradient Descent on Separable Data: Exact Convergence with a\n  Fixed Learning Rate", "comments": "AISTATS Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a central tool in machine learning. We\nprove that SGD converges to zero loss, even with a fixed (non-vanishing)\nlearning rate - in the special case of homogeneous linear classifiers with\nsmooth monotone loss functions, optimized on linearly separable data. Previous\nworks assumed either a vanishing learning rate, iterate averaging, or loss\nassumptions that do not hold for monotone loss functions used for\nclassification, such as the logistic loss. We prove our result on a fixed\ndataset, both for sampling with or without replacement. Furthermore, for\nlogistic loss (and similar exponentially-tailed losses), we prove that with SGD\nthe weight vector converges in direction to the $L_2$ max margin vector as\n$O(1/\\log(t))$ for almost all separable datasets, and the loss converges as\n$O(1/t)$ - similarly to gradient descent. Lastly, we examine the case of a\nfixed learning rate proportional to the minibatch size. We prove that in this\ncase, the asymptotic convergence rate of SGD (with replacement) does not depend\non the minibatch size in terms of epochs, if the support vectors span the data.\nThese results may suggest an explanation to similar behaviors observed in deep\nnetworks, when trained with SGD.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:37:19 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 09:47:18 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Nacson", "Mor Shpigel", ""], ["Srebro", "Nathan", ""], ["Soudry", "Daniel", ""]]}, {"id": "1806.01811", "submitter": "Xiaoixa Wu", "authors": "Rachel Ward, Xiaoxia Wu, and Leon Bottou", "title": "AdaGrad stepsizes: Sharp convergence over nonconvex landscapes", "comments": null, "journal-ref": "journal = {Journal of Machine Learning Research}, year = {2020},\n  volume = {21}, number = {219}, pages = {1-30}, url =\n  {http://jmlr.org/papers/v21/18-352.html}", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods such as AdaGrad and its variants update the\nstepsize in stochastic gradient descent on the fly according to the gradients\nreceived along the way; such methods have gained widespread use in large-scale\noptimization for their ability to converge robustly, without the need to\nfine-tune the stepsize schedule. Yet, the theoretical guarantees to date for\nAdaGrad are for online and convex optimization. We bridge this gap by providing\ntheoretical guarantees for the convergence of AdaGrad for smooth, nonconvex\nfunctions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to\na stationary point at the $\\mathcal{O}(\\log(N)/\\sqrt{N})$ rate in the\nstochastic setting, and at the optimal $\\mathcal{O}(1/N)$ rate in the batch\n(non-stochastic) setting -- in this sense, our convergence guarantees are\n'sharp'. In particular, the convergence of AdaGrad-Norm is robust to the choice\nof all hyper-parameters of the algorithm, in contrast to stochastic gradient\ndescent whose convergence depends crucially on tuning the step-size to the\n(generally unknown) Lipschitz smoothness constant and level of stochastic noise\non the gradient. Extensive numerical experiments are provided to corroborate\nour theory; moreover, the experiments suggest that the robustness of\nAdaGrad-Norm extends to state-of-the-art models in deep learning, without\nsacrificing generalization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:59:08 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 02:47:02 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 04:54:41 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 04:51:29 GMT"}, {"version": "v5", "created": "Thu, 21 Jun 2018 16:20:44 GMT"}, {"version": "v6", "created": "Wed, 10 Apr 2019 15:50:16 GMT"}, {"version": "v7", "created": "Sun, 21 Feb 2021 17:25:21 GMT"}, {"version": "v8", "created": "Mon, 19 Apr 2021 02:15:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ward", "Rachel", ""], ["Wu", "Xiaoxia", ""], ["Bottou", "Leon", ""]]}, {"id": "1806.01818", "submitter": "Stefan Braun", "authors": "Stefan Braun", "title": "LSTM Benchmarks for Deep Learning Frameworks", "comments": "7 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study provides benchmarks for different implementations of LSTM units\nbetween the deep learning frameworks PyTorch, TensorFlow, Lasagne and Keras.\nThe comparison includes cuDNN LSTMs, fused LSTM variants and less optimized,\nbut more flexible LSTM implementations. The benchmarks reflect two typical\nscenarios for automatic speech recognition, notably continuous speech\nrecognition and isolated digit recognition. These scenarios cover input\nsequences of fixed and variable length as well as the loss functions CTC and\ncross entropy. Additionally, a comparison between four different PyTorch\nversions is included. The code is available online\nhttps://github.com/stefbraun/rnn_benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:15:41 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Braun", "Stefan", ""]]}, {"id": "1806.01822", "submitter": "Adam Santoro", "authors": "Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski,\n  Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy\n  Lillicrap", "title": "Relational recurrent neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-based neural networks model temporal data by leveraging an ability to\nremember information for long periods. It is unclear, however, whether they\nalso have an ability to perform complex relational reasoning with the\ninformation they remember. Here, we first confirm our intuitions that standard\nmemory architectures may struggle at tasks that heavily involve an\nunderstanding of the ways in which entities are connected -- i.e., tasks\ninvolving relational reasoning. We then improve upon these deficits by using a\nnew memory module -- a \\textit{Relational Memory Core} (RMC) -- which employs\nmulti-head dot product attention to allow memories to interact. Finally, we\ntest the RMC on a suite of tasks that may profit from more capable relational\nreasoning across sequential information, and show large gains in RL domains\n(e.g. Mini PacMan), program evaluation, and language modeling, achieving\nstate-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:24:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 15:12:50 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Santoro", "Adam", ""], ["Faulkner", "Ryan", ""], ["Raposo", "David", ""], ["Rae", "Jack", ""], ["Chrzanowski", "Mike", ""], ["Weber", "Theophane", ""], ["Wierstra", "Daan", ""], ["Vinyals", "Oriol", ""], ["Pascanu", "Razvan", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1806.01827", "submitter": "Gaurush Hiranandani", "authors": "Gaurush Hiranandani, Shant Boodaghians, Ruta Mehta, Oluwasanmi Koyejo", "title": "Performance Metric Elicitation from Pairwise Classifier Comparisons", "comments": "The paper to appear in AISTATS 2019. 35 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a binary prediction problem, which performance metric should the\nclassifier optimize? We address this question by formalizing the problem of\nMetric Elicitation. The goal of metric elicitation is to discover the\nperformance metric of a practitioner, which reflects her innate rewards (costs)\nfor correct (incorrect) classification. In particular, we focus on eliciting\nbinary classification performance metrics from pairwise feedback, where a\npractitioner is queried to provide relative preference between two classifiers.\nBy exploiting key geometric properties of the space of confusion matrices, we\nobtain provably query efficient algorithms for eliciting linear and\nlinear-fractional performance metrics. We further show that our method is\nrobust to feedback and finite sample noise.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:34:29 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 07:13:19 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Hiranandani", "Gaurush", ""], ["Boodaghians", "Shant", ""], ["Mehta", "Ruta", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1806.01830", "submitter": "Adam Santoro", "authors": "Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li,\n  Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward\n  Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew\n  Botvinick, Oriol Vinyals, Peter Battaglia", "title": "Relational Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for deep reinforcement learning (RL) that improves\nupon the efficiency, generalization capacity, and interpretability of\nconventional approaches through structured perception and relational reasoning.\nIt uses self-attention to iteratively reason about the relations between\nentities in a scene and to guide a model-free policy. Our results show that in\na novel navigation and planning task called Box-World, our agent finds\ninterpretable solutions that improve upon baselines in terms of sample\ncomplexity, ability to generalize to more complex scenes than experienced\nduring training, and overall performance. In the StarCraft II Learning\nEnvironment, our agent achieves state-of-the-art performance on six mini-games\n-- surpassing human grandmaster performance on four. By considering\narchitectural inductive biases, our work opens new directions for overcoming\nimportant, but stubborn, challenges in deep RL.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:39:12 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 14:59:32 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zambaldi", "Vinicius", ""], ["Raposo", "David", ""], ["Santoro", "Adam", ""], ["Bapst", "Victor", ""], ["Li", "Yujia", ""], ["Babuschkin", "Igor", ""], ["Tuyls", "Karl", ""], ["Reichert", "David", ""], ["Lillicrap", "Timothy", ""], ["Lockhart", "Edward", ""], ["Shanahan", "Murray", ""], ["Langston", "Victoria", ""], ["Pascanu", "Razvan", ""], ["Botvinick", "Matthew", ""], ["Vinyals", "Oriol", ""], ["Battaglia", "Peter", ""]]}, {"id": "1806.01844", "submitter": "Snehanshu Saha", "authors": "Snehanshu Saha, Archana Mathur, Kakoli Bora, Surbhi Agrawal, Suryoday\n  Basak", "title": "SBAF: A New Activation Function for Artificial Neural Net based\n  Habitability Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.08810", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the efficacy of using a novel activation function in Artificial\nNeural Networks (ANN) in characterizing exoplanets into different classes. We\ncall this Saha-Bora Activation Function (SBAF) as the motivation is derived\nfrom long standing understanding of using advanced calculus in modeling\nhabitability score of Exoplanets. The function is demonstrated to possess nice\nanalytical properties and doesn't seem to suffer from local oscillation\nproblems. The manuscript presents the analytical properties of the activation\nfunction and the architecture implemented on the function. Keywords:\nAstroinformatics, Machine Learning, Exoplanets, ANN, Activation Function.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:33:04 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Saha", "Snehanshu", ""], ["Mathur", "Archana", ""], ["Bora", "Kakoli", ""], ["Agrawal", "Surbhi", ""], ["Basak", "Suryoday", ""]]}, {"id": "1806.01845", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang and Junru Shao and Ruslan Salakhutdinov", "title": "Deep Neural Networks with Multi-Branch Architectures Are Less Non-Convex", "comments": "26 pages, 6 figures, 3 tables; v2 fixes some typos. arXiv admin note:\n  text overlap with arXiv:1712.08559 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recently proposed architectures of neural networks such as ResNeXt,\nInception, Xception, SqueezeNet and Wide ResNet are based on the designing idea\nof having multiple branches and have demonstrated improved performance in many\napplications. We show that one cause for such success is due to the fact that\nthe multi-branch architecture is less non-convex in terms of duality gap. The\nduality gap measures the degree of intrinsic non-convexity of an optimization\nproblem: smaller gap in relative value implies lower degree of intrinsic\nnon-convexity. The challenge is to quantitatively measure the duality gap of\nhighly non-convex problems such as deep neural networks. In this work, we\nprovide strong guarantees of this quantity for two classes of network\narchitectures. For the neural networks with arbitrary activation functions,\nmulti-branch architecture and a variant of hinge loss, we show that the duality\ngap of both population and empirical risks shrinks to zero as the number of\nbranches increases. This result sheds light on better understanding the power\nof over-parametrization where increasing the network width tends to make the\nloss surface less non-convex. For the neural networks with linear activation\nfunction and $\\ell_2$ loss, we show that the duality gap of empirical risk is\nzero. Our two results work for arbitrary depths and adversarial data, while the\nanalytical techniques might be of independent interest to non-convex\noptimization more broadly. Experiments on both synthetic and real-world\ndatasets validate our results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:16:36 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 15:51:46 GMT"}], "update_date": "2018-08-26", "authors_parsed": [["Zhang", "Hongyang", ""], ["Shao", "Junru", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1806.01851", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak, Fritz Obermeyer", "title": "Pathwise Derivatives Beyond the Reparameterization Trick", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that gradients computed via the reparameterization trick are in\ndirect correspondence with solutions of the transport equation in the formalism\nof optimal transport. We use this perspective to compute (approximate) pathwise\ngradients for probability distributions not directly amenable to the\nreparameterization trick: Gamma, Beta, and Dirichlet. We further observe that\nwhen the reparameterization trick is applied to the Cholesky-factorized\nmultivariate Normal distribution, the resulting gradients are suboptimal in the\nsense of optimal transport. We derive the optimal gradients and show that they\nhave reduced variance in a Gaussian Process regression task. We demonstrate\nwith a variety of synthetic experiments and stochastic variational inference\ntasks that our pathwise gradients are competitive with other methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:00:01 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 16:28:24 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Jankowiak", "Martin", ""], ["Obermeyer", "Fritz", ""]]}, {"id": "1806.01856", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak, Theofanis Karaletsos", "title": "Pathwise Derivatives for Multivariate Distributions", "comments": "To appear at AISTATS 2019; 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the link between the transport equation and derivatives of\nexpectations to construct efficient pathwise gradient estimators for\nmultivariate distributions. We focus on two main threads. First, we use null\nsolutions of the transport equation to construct adaptive control variates that\ncan be used to construct gradient estimators with reduced variance. Second, we\nconsider the case of multivariate mixture distributions. In particular we show\nhow to compute pathwise derivatives for mixtures of multivariate Normal\ndistributions with arbitrary means and diagonal covariances. We demonstrate in\na variety of experiments in the context of variational inference that our\ngradient estimators can outperform other methods, especially in high\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:00:04 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 20:20:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Jankowiak", "Martin", ""], ["Karaletsos", "Theofanis", ""]]}, {"id": "1806.01875", "submitter": "Kay Gregor Hartmann", "authors": "Kay Gregor Hartmann, Robin Tibor Schirrmeister, Tonio Ball", "title": "EEG-GAN: Generative adversarial networks for electroencephalograhic\n  (EEG) brain signals", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are recently highly successful in\ngenerative applications involving images and start being applied to time series\ndata. Here we describe EEG-GAN as a framework to generate\nelectroencephalographic (EEG) brain signals. We introduce a modification to the\nimproved training of Wasserstein GANs to stabilize training and investigate a\nrange of architectural choices critical for time series generation (most\nnotably up- and down-sampling). For evaluation we consider and compare\ndifferent metrics such as Inception score, Frechet inception distance and\nsliced Wasserstein distance, together showing that our EEG-GAN framework\ngenerated naturalistic EEG examples. It thus opens up a range of new generative\napplication scenarios in the neuroscientific and neurological context, such as\ndata augmentation in brain-computer interfacing tasks, EEG super-sampling, or\nrestoration of corrupted data segments. The possibility to generate signals of\na certain class and/or with specific properties may also open a new avenue for\nresearch into the underlying structure of brain signals.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:10:11 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hartmann", "Kay Gregor", ""], ["Schirrmeister", "Robin Tibor", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.01879", "submitter": "Jonathan Weed", "authors": "Jonathan Weed", "title": "An explicit analysis of the entropic penalty in linear programming", "comments": "To appear at Conference on Learning Theory (COLT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving linear programs by using entropic penalization has recently attracted\nnew interest in the optimization community, since this strategy forms the basis\nfor the fastest-known algorithms for the optimal transport problem, with many\napplications in modern large-scale machine learning. Crucial to these\napplications has been an analysis of how quickly solutions to the penalized\nprogram approach true optima to the original linear program. More than 20 years\nago, Cominetti and San Mart\\'in showed that this convergence is exponentially\nfast; however, their proof is asymptotic and does not give any indication of\nhow accurately the entropic program approximates the original program for any\nparticular choice of the penalization parameter. We close this long-standing\ngap in the literature regarding entropic penalization by giving a new proof of\nthe exponential convergence, valid for any linear program. Our proof is\nnon-asymptotic, yields explicit constants, and has the virtue of being\nextremely simple. We provide matching lower bounds and show that the entropic\napproach does not lead to a near-linear time approximation scheme for the\nlinear assignment problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:15:57 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Weed", "Jonathan", ""]]}, {"id": "1806.01896", "submitter": "Yuri G. Gordienko", "authors": "Vlad Taran, Nikita Gordienko, Yuriy Kochura, Yuri Gordienko, Alexandr\n  Rokovyi, Oleg Alienin, Sergii Stirenko", "title": "Performance Evaluation of Deep Learning Networks for Semantic\n  Segmentation of Traffic Stereo-Pair Images", "comments": "8 pages, 10 figures; accepted for presentation at 19-th International\n  Conference on Computer Systems and Technologies (CompSysTech'18) 13-14\n  September 2018, University of Ruse, Bulgaria", "journal-ref": "Proceedings of the 19th International Conference on Computer\n  Systems and Technologies (CompSysTech'18), Boris Rachev and Angel Smrikarov\n  (Eds.). ACM, New York, NY, USA, 73-80 (2018)", "doi": "10.1145/3274005.3274032", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is one the most demanding task, especially for\nanalysis of traffic conditions for self-driving cars. Here the results of\napplication of several deep learning architectures (PSPNet and ICNet) for\nsemantic image segmentation of traffic stereo-pair images are presented. The\nimages from Cityscapes dataset and custom urban images were analyzed as to the\nsegmentation accuracy and image inference time. For the models pre-trained on\nCityscapes dataset, the inference time was equal in the limits of standard\ndeviation, but the segmentation accuracy was different for various cities and\nstereo channels even. The distributions of accuracy (mean intersection over\nunion - mIoU) values for each city and channel are asymmetric, long-tailed, and\nhave many extreme outliers, especially for PSPNet network in comparison to\nICNet network. Some statistical properties of these distributions (skewness,\nkurtosis) allow us to distinguish these two networks and open the question\nabout relations between architecture of deep learning networks and statistical\ndistribution of the predicted results (mIoU here). The results obtained\ndemonstrated the different sensitivity of these networks to: (1) the local\nstreet view peculiarities in different cities that should be taken into account\nduring the targeted fine tuning the models before their practical applications,\n(2) the right and left data channels in stereo-pairs. For both networks, the\ndifference in the predicted results (mIoU here) for the right and left data\nchannels in stereo-pairs is out of the limits of statistical error in relation\nto mIoU values. It means that the traffic stereo pairs can be effectively used\nnot only for depth calculations (as it is usually used), but also as an\nadditional data channel that can provide much more information about scene\nobjects than simple duplication of the same street view images.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:00:35 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Taran", "Vlad", ""], ["Gordienko", "Nikita", ""], ["Kochura", "Yuriy", ""], ["Gordienko", "Yuri", ""], ["Rokovyi", "Alexandr", ""], ["Alienin", "Oleg", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1806.01899", "submitter": "Md Badsha", "authors": "Md. Bahadur Badsha, Evan A Martin, Audrey Qiuyan Fu", "title": "MRPC: An R package for accurate inference of causal graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present MRPC, an R package that learns causal graphs with improved\naccuracy over existing packages, such as pcalg and bnlearn. Our algorithm\nbuilds on the powerful PC algorithm, the canonical algorithm in computer\nscience for learning directed acyclic graphs. The improvement in accuracy\nresults from online control of the false discovery rate (FDR) that reduces\nfalse positive edges, a more accurate approach to identifying v-structures\n(i.e., $T_1 \\rightarrow T_2 \\leftarrow T_3$), and robust estimation of the\ncorrelation matrix among nodes. For genomic data that contain genotypes and\ngene expression for each sample, MRPC incorporates the principle of Mendelian\nrandomization to orient the edges. Our package can be applied to continuous and\ndiscrete data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:12:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Badsha", "Md. Bahadur", ""], ["Martin", "Evan A", ""], ["Fu", "Audrey Qiuyan", ""]]}, {"id": "1806.01910", "submitter": "Robert Peharz", "authors": "Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina,\n  Martin Trapp, Kristian Kersting, and Zoubin Ghahramani", "title": "Probabilistic Deep Learning using Random Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for consistent treatment of uncertainty has recently triggered\nincreased interest in probabilistic deep learning methods. However, most\ncurrent approaches have severe limitations when it comes to inference, since\nmany of these models do not even permit to evaluate exact data likelihoods.\nSum-product networks (SPNs), on the other hand, are an excellent architecture\nin that regard, as they allow to efficiently evaluate likelihoods, as well as\narbitrary marginalization and conditioning tasks. Nevertheless, SPNs have not\nbeen fully explored as serious deep learning models, likely due to their\nspecial structural requirements, which complicate learning. In this paper, we\nmake a drastic simplification and use random SPN structures which are trained\nin a \"classical deep learning manner\", i.e. employing automatic\ndifferentiation, SGD, and GPU support. The resulting models, called RAT-SPNs,\nyield prediction results comparable to deep neural networks, while still being\ninterpretable as generative model and maintaining well-calibrated\nuncertainties. This property makes them highly robust under missing input\nfeatures and enables them to naturally detect outliers and peculiar samples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:44:44 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 14:46:45 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Peharz", "Robert", ""], ["Vergari", "Antonio", ""], ["Stelzner", "Karl", ""], ["Molina", "Alejandro", ""], ["Trapp", "Martin", ""], ["Kersting", "Kristian", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1806.01911", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty, Mario Fritz, Bernt Schiele", "title": "Adversarial Scene Editing: Automatic Object Removal from Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While great progress has been made recently in automatic image manipulation,\nit has been limited to object centric images like faces or structured scene\ndatasets. In this work, we take a step towards general scene-level image\nediting by developing an automatic interaction-free object removal model. Our\nmodel learns to find and remove objects from general scene images using\nimage-level labels and unpaired data in a generative adversarial network (GAN)\nframework. We achieve this with two key contributions: a two-stage editor\narchitecture consisting of a mask generator and image in-painter that\nco-operate to remove objects, and a novel GAN based prior for the mask\ngenerator that allows us to flexibly incorporate knowledge about object shapes.\nWe experimentally show on two datasets that our method effectively removes a\nwide variety of objects using weak supervision only\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:45:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Shetty", "Rakshith", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1806.01918", "submitter": "Peter Hinz", "authors": "Peter Hinz and Sara van de Geer", "title": "A Framework for the construction of upper bounds on the number of affine\n  linear regions of ReLU feed-forward neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to derive upper bounds on the number of regions that\nfeed-forward neural networks with ReLU activation functions are affine linear\non. It is based on an inductive analysis that keeps track of the number of such\nregions per dimensionality of their images within the layers. More precisely,\nthe information about the number regions per dimensionality is pushed through\nthe layers starting with one region of the input dimension of the neural\nnetwork and using a recursion based on an analysis of how many regions per\noutput dimensionality a subsequent layer with a certain width can induce on an\ninput region with a given dimensionality. The final bound on the number of\nregions depends on the number and widths of the layers of the neural network\nand on some additional parameters that were used for the recursion. It is\nstated in terms of the $L1$-norm of the last column of a product of matrices\nand provides a unifying treatment of several previously known bounds: Depending\non the choice of the recursion parameters that determine these matrices, it is\npossible to obtain the bounds from Mont\\'{u}far (2014), (2017) and Serra et.\nal. (2017) as special cases. For the latter, which is the strongest of these\nbounds, the formulation in terms of matrices provides new insight. In\nparticular, by using explicit formulas for a Jordan-like decomposition of the\ninvolved matrices, we achieve new tighter results for the asymptotic setting,\nwhere the number of layers of the same fixed width tends to infinity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 20:05:51 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 11:05:06 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 11:19:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hinz", "Peter", ""], ["van de Geer", "Sara", ""]]}, {"id": "1806.01933", "submitter": "Joel Vaughan", "authors": "Joel Vaughan, Agus Sudjianto, Erind Brahimi, Jie Chen, Vijayan N. Nair", "title": "Explainable Neural Networks based on Additive Index Models", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning algorithms are increasingly being used in recent years due\nto their flexibility in model fitting and increased predictive performance.\nHowever, the complexity of the models makes them hard for the data analyst to\ninterpret the results and explain them without additional tools. This has led\nto much research in developing various approaches to understand the model\nbehavior. In this paper, we present the Explainable Neural Network (xNN), a\nstructured neural network designed especially to learn interpretable features.\nUnlike fully connected neural networks, the features engineered by the xNN can\nbe extracted from the network in a relatively straightforward manner and the\nresults displayed. With appropriate regularization, the xNN provides a\nparsimonious explanation of the relationship between the features and the\noutput. We illustrate this interpretable feature--engineering property on\nsimulated examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 20:40:56 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Vaughan", "Joel", ""], ["Sudjianto", "Agus", ""], ["Brahimi", "Erind", ""], ["Chen", "Jie", ""], ["Nair", "Vijayan N.", ""]]}, {"id": "1806.01947", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, Paul Fearnhead", "title": "A linear time method for the detection of point and collective anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of efficiently identifying anomalies in data sequences is an\nimportant statistical problem that now arises in many applications. Whilst\nthere has been substantial work aimed at making statistical analyses robust to\noutliers, or point anomalies, there has been much less work on detecting\nanomalous segments, or collective anomalies, particularly in those settings\nwhere point anomalies might also occur. In this article, we introduce\nCollective And Point Anomalies (CAPA), a computationally efficient approach\nthat is suitable when collective anomalies are characterised by either a change\nin mean, variance, or both, and distinguishes them from point anomalies.\nTheoretical results establish the consistency of CAPA at detecting collective\nanomalies and, as a by-product, the consistency of a popular penalised cost\nbased change in mean and variance detection method. Empirical results show that\nCAPA has close to linear computational cost as well as being more accurate at\ndetecting and locating collective anomalies than other approaches. We\ndemonstrate the utility of CAPA through its ability to detect exoplanets from\nlight curve data from the Kepler telescope.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:02:53 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 11:57:18 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1806.01949", "submitter": "Maruti Mudunuru", "authors": "A. Hunter, B. A. Moore, M. K. Mudunuru, V. T. Chau, R. L. Miller, R.\n  B. Tchoua, C. Nyshadham, S. Karra, D. O. Malley, E. Rougier, H. S.\n  Viswanathan, and G. Srinivasan", "title": "Reduced-Order Modeling through Machine Learning Approaches for Brittle\n  Fracture Applications", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, five different approaches for reduced-order modeling of\nbrittle fracture in geomaterials, specifically concrete, are presented and\ncompared. Four of the five methods rely on machine learning (ML) algorithms to\napproximate important aspects of the brittle fracture problem. In addition to\nthe ML algorithms, each method incorporates different physics-based assumptions\nin order to reduce the computational complexity while maintaining the physics\nas much as possible. This work specifically focuses on using the ML approaches\nto model a 2D concrete sample under low strain rate pure tensile loading\nconditions with 20 preexisting cracks present. A high-fidelity finite\nelement-discrete element model is used to both produce a training dataset of\n150 simulations and an additional 35 simulations for validation. Results from\nthe ML approaches are directly compared against the results from the\nhigh-fidelity model. Strengths and weaknesses of each approach are discussed\nand the most important conclusion is that a combination of physics-informed and\ndata-driven features are necessary for emulating the physics of crack\npropagation, interaction and coalescence. All of the models presented here have\nruntimes that are orders of magnitude faster than the original high-fidelity\nmodel and pave the path for developing accurate reduced order models that could\nbe used to inform larger length-scale models with important sub-scale physics\nthat often cannot be accounted for due to computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:08:09 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hunter", "A.", ""], ["Moore", "B. A.", ""], ["Mudunuru", "M. K.", ""], ["Chau", "V. T.", ""], ["Miller", "R. L.", ""], ["Tchoua", "R. B.", ""], ["Nyshadham", "C.", ""], ["Karra", "S.", ""], ["Malley", "D. O.", ""], ["Rougier", "E.", ""], ["Viswanathan", "H. S.", ""], ["Srinivasan", "G.", ""]]}, {"id": "1806.01969", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski and Manfred K. Warmuth", "title": "Reverse iterative volume sampling for linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following basic machine learning task: Given a fixed set of\n$d$-dimensional input points for a linear regression problem, we wish to\npredict a hidden response value for each of the points. We can only afford to\nattain the responses for a small subset of the points that are then used to\nconstruct linear predictions for all points in the dataset. The performance of\nthe predictions is evaluated by the total square loss on all responses (the\nattained as well as the hidden ones). We show that a good approximate solution\nto this least squares problem can be obtained from just dimension $d$ many\nresponses by using a joint sampling technique called volume sampling. Moreover,\nthe least squares solution obtained for the volume sampled subproblem is an\nunbiased estimator of optimal solution based on all n responses. This\nunbiasedness is a desirable property that is not shared by other common subset\nselection techniques.\n  Motivated by these basic properties, we develop a theoretical framework for\nstudying volume sampling, resulting in a number of new matrix expectation\nequalities and statistical guarantees which are of importance not only to least\nsquares regression but also to numerical linear algebra in general. Our methods\nalso lead to a regularized variant of volume sampling, and we propose the first\nefficient algorithms for volume sampling which make this technique a practical\ntool in the machine learning toolbox. Finally, we provide experimental evidence\nwhich confirms our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 00:24:15 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1806.01973", "submitter": "Rex Ying", "authors": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L.\n  Hamilton, Jure Leskovec", "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "comments": "KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219890", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. However,\nmaking these methods practical and scalable to web-scale recommendation tasks\nwith billions of items and hundreds of millions of users remains a challenge.\nHere we describe a large-scale deep recommendation engine that we developed and\ndeployed at Pinterest. We develop a data-efficient Graph Convolutional Network\n(GCN) algorithm PinSage, which combines efficient random walks and graph\nconvolutions to generate embeddings of nodes (i.e., items) that incorporate\nboth graph structure as well as node feature information. Compared to prior GCN\napproaches, we develop a novel method based on highly efficient random walks to\nstructure the convolutions and design a novel training strategy that relies on\nharder-and-harder training examples to improve robustness and convergence of\nthe model. We also develop an efficient MapReduce model inference algorithm to\ngenerate embeddings using a trained model. We deploy PinSage at Pinterest and\ntrain it on 7.5 billion examples on a graph with 3 billion nodes representing\npins and boards, and 18 billion edges. According to offline metrics, user\nstudies and A/B tests, PinSage generates higher-quality recommendations than\ncomparable deep learning and graph-based alternatives. To our knowledge, this\nis the largest application of deep graph embeddings to date and paves the way\nfor a new generation of web-scale recommender systems based on graph\nconvolutional architectures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 01:26:33 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ying", "Rex", ""], ["He", "Ruining", ""], ["Chen", "Kaifeng", ""], ["Eksombatchai", "Pong", ""], ["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.01984", "submitter": "Tristan Sylvain", "authors": "Margaux Luck, Tristan Sylvain, Joseph Paul Cohen, Heloise Cardinal,\n  Andrea Lodi, Yoshua Bengio", "title": "Learning to rank for censored survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a type of semi-supervised ranking task where the target\noutput (the survival time) is often right-censored. Utilizing this information\nis a challenge because it is not obvious how to correctly incorporate these\ncensored examples into a model. We study how three categories of loss\nfunctions, namely partial likelihood methods, rank methods, and our\nclassification method based on a Wasserstein metric (WM) and the non-parametric\nKaplan Meier estimate of the probability density to impute the labels of\ncensored examples, can take advantage of this information. The proposed method\nallows us to have a model that predict the probability distribution of an\nevent. If a clinician had access to the detailed probability of an event over\ntime this would help in treatment planning. For example, determining if the\nrisk of kidney graft rejection is constant or peaked after some time. Also, we\ndemonstrate that this approach directly optimizes the expected C-index which is\nthe most common evaluation metric for ranking survival models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:30:00 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 18:55:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Luck", "Margaux", ""], ["Sylvain", "Tristan", ""], ["Cohen", "Joseph Paul", ""], ["Cardinal", "Heloise", ""], ["Lodi", "Andrea", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.01993", "submitter": "Amit Moscovich", "authors": "Yaniv Tenzer, Amit Moscovich, Mary Frances Dorn, Boaz Nadler, Clifford\n  Spiegelman", "title": "Beyond Trees: Classification with Sparse Pairwise Dependencies", "comments": "32 pages, 12 figures, 3 tables. Major revision with new\n  feature-selection step and more extensive simulations", "journal-ref": "Journal of Machine Learning Research 21:189 (2020) 1-33", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several classification methods assume that the underlying distributions\nfollow tree-structured graphical models. Indeed, trees capture statistical\ndependencies between pairs of variables, which may be crucial to attain low\nclassification errors. The resulting classifier is linear in the\nlog-transformed univariate and bivariate densities that correspond to the tree\nedges. In practice, however, observed data may not be well approximated by\ntrees. Yet, motivated by the importance of pairwise dependencies for accurate\nclassification, here we propose to approximate the optimal decision boundary by\na sparse linear combination of the univariate and bivariate log-transformed\ndensities. Our proposed approach is semi-parametric in nature: we\nnon-parametrically estimate the univariate and bivariate densities, remove\npairs of variables that are nearly independent using the Hilbert-Schmidt\nindependence criteria, and finally construct a linear SVM on the retained\nlog-transformed densities. We demonstrate using both synthetic and real data\nthat our resulting classifier, denoted SLB (Sparse Log-Bivariate density), is\ncompetitive with popular classification methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 03:13:01 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 22:37:47 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tenzer", "Yaniv", ""], ["Moscovich", "Amit", ""], ["Dorn", "Mary Frances", ""], ["Nadler", "Boaz", ""], ["Spiegelman", "Clifford", ""]]}, {"id": "1806.02003", "submitter": "Abhejit Rajagopal", "authors": "Abhejit Rajagopal, Shivkumar Chandrasekaran, Hrushikesh N. Mhaskar", "title": "Deep Algorithms: designs for networks", "comments": "submitted to Thirty-second Annual Conference on Neural Information\n  Processing Systems (NIPS), May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new design methodology for neural networks that is guided by traditional\nalgorithm design is presented. To prove our point, we present two heuristics\nand demonstrate an algorithmic technique for incorporating additional weights\nin their signal-flow graphs. We show that with training the performance of\nthese networks can not only exceed the performance of the initial network, but\ncan match the performance of more-traditional neural network architectures. A\nkey feature of our approach is that these networks are initialized with\nparameters that provide a known performance threshold for the architecture on a\ngiven task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 04:39:37 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Rajagopal", "Abhejit", ""], ["Chandrasekaran", "Shivkumar", ""], ["Mhaskar", "Hrushikesh N.", ""]]}, {"id": "1806.02012", "submitter": "Uday Singh Saini", "authors": "Uday Singh Saini, Evangelos E. Papalexakis", "title": "A Peek Into the Hidden Layers of a Convolutional Neural Network Through\n  a Factorization Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their increasing popularity and success in a variety of supervised\nlearning problems, deep neural networks are extremely hard to interpret and\ndebug: Given and already trained Deep Neural Net, and a set of test inputs, how\ncan we gain insight into how those inputs interact with different layers of the\nneural network? Furthermore, can we characterize a given deep neural network\nbased on it's observed behavior on different inputs? In this paper we propose a\nnovel factorization based approach on understanding how different deep neural\nnetworks operate. In our preliminary results, we identify fascinating patterns\nthat link the factorization rank (typically used as a measure of\ninterestingness in unsupervised data analysis) with how well or poorly the deep\nnetwork has been trained. Finally, our proposed approach can help provide\nvisual insights on how high-level. interpretable patterns of the network's\ninput behave inside the hidden layers of the deep network.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 05:27:38 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Saini", "Uday Singh", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1806.02032", "submitter": "Kathrin Grosse", "authors": "Kathrin Grosse, Michael T. Smith, Michael Backes", "title": "Killing four birds with one Gaussian process: the relation between\n  different test-time attacks", "comments": "10 pages, 8 figures, long version of paper accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning (ML) security, attacks like evasion, model stealing or\nmembership inference are generally studied in individually. Previous work has\nalso shown a relationship between some attacks and decision function curvature\nof the targeted model. Consequently, we study an ML model allowing direct\ncontrol over the decision surface curvature: Gaussian Process classifiers\n(GPCs). For evasion, we find that changing GPC's curvature to be robust against\none attack algorithm boils down to enabling a different norm or attack\nalgorithm to succeed. This is backed up by our formal analysis showing that\nstatic security guarantees are opposed to learning. Concerning intellectual\nproperty, we show formally that lazy learning does not necessarily leak all\ninformation when applied. In practice, often a seemingly secure curvature can\nbe found. For example, we are able to secure GPC against empirical membership\ninference by proper configuration. In this configuration, however, the GPC's\nhyper-parameters are leaked, e.g. model reverse engineering succeeds. We\nconclude that attacks on classification should not be studied in isolation, but\nin relation to each other.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:55:36 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 13:47:09 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 09:40:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Grosse", "Kathrin", ""], ["Smith", "Michael T.", ""], ["Backes", "Michael", ""]]}, {"id": "1806.02034", "submitter": "David Hofmeyr", "authors": "David P. Hofmeyr", "title": "Degrees of Freedom and Model Selection for k-means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the model degrees of freedom in k-means clustering.\nAn extension of Stein's lemma provides an expression for the effective degrees\nof freedom in the k-means model. Approximating the degrees of freedom in\npractice requires simplifications of this expression, however empirical studies\nevince the appropriateness of our proposed approach. The practical relevance of\nthis new degrees of freedom formulation for k-means is demonstrated through\nmodel selection using the Bayesian Information Criterion. The reliability of\nthis method is validated through experiments on simulated data as well as on a\nlarge collection of publicly available benchmark data sets from diverse\napplication areas. Comparisons with popular existing techniques indicate that\nthis approach is extremely competitive for selecting high quality clustering\nsolutions. Code to implement the proposed approach is available in the form of\nan R package from https://github.com/DavidHofmeyr/edfkmeans.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 07:04:49 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 07:23:54 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 19:37:41 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 05:16:00 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Hofmeyr", "David P.", ""]]}, {"id": "1806.02046", "submitter": "Anastasios Kyrillidis", "authors": "Kelly Geyer, Anastasios Kyrillidis and Amir Kalev", "title": "Implicit regularization and solution uniqueness in over-parameterized\n  matrix sensing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider whether algorithmic choices in over-parameterized linear matrix\nfactorization introduce implicit regularization. We focus on noiseless matrix\nsensing over rank-$r$ positive semi-definite (PSD) matrices in $\\mathbb{R}^{n\n\\times n}$, with a sensing mechanism that satisfies restricted isometry\nproperties (RIP). The algorithm we study is \\emph{factored gradient descent},\nwhere we model the low-rankness and PSD constraints with the factorization\n$UU^\\top$, for $U \\in \\mathbb{R}^{n \\times r}$. Surprisingly, recent work\nargues that the choice of $r \\leq n$ is not pivotal: even setting $U \\in\n\\mathbb{R}^{n \\times n}$ is sufficient for factored gradient descent to find\nthe rank-$r$ solution, which suggests that operating over the factors leads to\nan implicit regularization. In this contribution, we provide a different\nperspective to the problem of implicit regularization. We show that under\ncertain conditions, the PSD constraint by itself is sufficient to lead to a\nunique rank-$r$ matrix recovery, without implicit or explicit low-rank\nregularization. \\emph{I.e.}, under assumptions, the set of PSD matrices, that\nare consistent with the observed data, is a singleton, regardless of the\nalgorithm used.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 07:55:52 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:42:23 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Geyer", "Kelly", ""], ["Kyrillidis", "Anastasios", ""], ["Kalev", "Amir", ""]]}, {"id": "1806.02071", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Vinicius C. Azevedo, Nils Thuerey, Theodore Kim, Markus\n  Gross, Barbara Solenthaler", "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations", "comments": "Computer Graphics Forum (Proceedings of EUROGRAPHICS 2019),\n  additional materials: http://www.byungsoo.me/project/deep-fluids/", "journal-ref": "Computer Graphics Forum (Proc. Eurographics), 38, 2 (2019), 59-70", "doi": "10.1111/cgf.13619", "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel generative model to synthesize fluid simulations\nfrom a set of reduced parameters. A convolutional neural network is trained on\na collection of discrete, parameterizable fluid simulation velocity fields. Due\nto the capability of deep learning architectures to learn representative\nfeatures of the data, our generative model is able to accurately approximate\nthe training data set, while providing plausible interpolated in-betweens. The\nproposed generative model is optimized for fluids by a novel loss function that\nguarantees divergence-free velocity fields at all times. In addition, we\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\nadvance simulations in time by integrating in the latent space with a second\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\napplications such as fast construction of simulations, interpolation of fluids\nwith different parameters, time re-sampling, latent space simulations, and\ncompression of fluid simulation data. Reconstructed velocity fields are\ngenerated up to 700x faster than re-simulating the data with the underlying CPU\nsolver, while achieving compression rates of up to 1300x.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:57:18 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 14:44:57 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Thuerey", "Nils", ""], ["Kim", "Theodore", ""], ["Gross", "Markus", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1806.02078", "submitter": "Kunjin Chen", "authors": "Kunjin Chen, Qin Wang, Ziyu He, Kunlong Chen, Jun Hu, Jinliang He", "title": "Convolutional Sequence to Sequence Non-intrusive Load Monitoring", "comments": "This paper is submitted to IET-The Journal of Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolutional sequence to sequence non-intrusive load monitoring model is\nproposed in this paper. Gated linear unit convolutional layers are used to\nextract information from the sequences of aggregate electricity consumption.\nResidual blocks are also introduced to refine the output of the neural network.\nThe partially overlapped output sequences of the network are averaged to\nproduce the final output of the model. We apply the proposed model to the REDD\ndataset and compare it with the convolutional sequence to point model in the\nliterature. Results show that the proposed model is able to give satisfactory\ndisaggregation performance for appliances with varied characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 09:19:08 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chen", "Kunjin", ""], ["Wang", "Qin", ""], ["He", "Ziyu", ""], ["Chen", "Kunlong", ""], ["Hu", "Jun", ""], ["He", "Jinliang", ""]]}, {"id": "1806.02121", "submitter": "Jonathan Laserson", "authors": "Jonathan Laserson, Christine Dan Lantsman, Michal Cohen-Sfady, Itamar\n  Tamir, Eli Goz, Chen Brestel, Shir Bar, Maya Atar, Eldad Elnekave", "title": "TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest\n  X-rays", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-ray (CXR) is by far the most commonly performed radiological\nexamination for screening and diagnosis of many cardiac and pulmonary diseases.\nThere is an immense world-wide shortage of physicians capable of providing\nrapid and accurate interpretation of this study. A radiologist-driven analysis\nof over two million CXR reports generated an ontology including the 40 most\nprevalent pathologies on CXR. By manually tagging a relatively small set of\nsentences, we were able to construct a training set of 959k studies. A deep\nlearning model was trained to predict the findings given the patient frontal\nand lateral scans. For 12 of the findings we compare the model performance\nagainst a team of radiologists and show that in most cases the radiologists\nagree on average more with the algorithm than with each other.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:17:59 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Laserson", "Jonathan", ""], ["Lantsman", "Christine Dan", ""], ["Cohen-Sfady", "Michal", ""], ["Tamir", "Itamar", ""], ["Goz", "Eli", ""], ["Brestel", "Chen", ""], ["Bar", "Shir", ""], ["Atar", "Maya", ""], ["Elnekave", "Eldad", ""]]}, {"id": "1806.02136", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Andrew Fitzgibbon, Dimitrios Vytiniotis, Simon Peyton\n  Jones, Christoph Koch", "title": "Efficient Differentiable Programming in a Functional Array-Processing\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.PL cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for the automatic differentiation of a higher-order\nfunctional array-processing language. The core functional language underlying\nthis system simultaneously supports both source-to-source automatic\ndifferentiation and global optimizations such as loop transformations. Thanks\nto this feature, we demonstrate how for some real-world machine learning and\ncomputer vision benchmarks, the system outperforms the state-of-the-art\nautomatic differentiation tools.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:54:34 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Shaikhha", "Amir", ""], ["Fitzgibbon", "Andrew", ""], ["Vytiniotis", "Dimitrios", ""], ["Jones", "Simon Peyton", ""], ["Koch", "Christoph", ""]]}, {"id": "1806.02146", "submitter": "Rahul Gupta", "authors": "Saurabh Sahu, Rahul Gupta, Ganesh Sivaraman, Wael AbdAlmageed, Carol\n  Espy-Wilson", "title": "Adversarial Auto-encoders for Speech Based Emotion Recognition", "comments": "5 pages, INTERSPEECH 2017 August 20-24, 2017, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative adversarial networks and adversarial autoencoders have\ngained a lot of attention in machine learning community due to their\nexceptional performance in tasks such as digit classification and face\nrecognition. They map the autoencoder's bottleneck layer output (termed as code\nvectors) to different noise Probability Distribution Functions (PDFs), that can\nbe further regularized to cluster based on class information. In addition, they\nalso allow a generation of synthetic samples by sampling the code vectors from\nthe mapped PDFs. Inspired by these properties, we investigate the application\nof adversarial autoencoders to the domain of emotion recognition. Specifically,\nwe conduct experiments on the following two aspects: (i) their ability to\nencode high dimensional feature vector representations for emotional utterances\ninto a compressed space (with a minimal loss of emotion class discriminability\nin the compressed space), and (ii) their ability to regenerate synthetic\nsamples in the original feature space, to be later used for purposes such as\ntraining emotion recognition classifiers. We demonstrate the promise of\nadversarial autoencoders with regards to these aspects on the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:40:37 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Sahu", "Saurabh", ""], ["Gupta", "Rahul", ""], ["Sivaraman", "Ganesh", ""], ["AbdAlmageed", "Wael", ""], ["Espy-Wilson", "Carol", ""]]}, {"id": "1806.02169", "submitter": "Hirokazu Kameoka", "authors": "Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, Nobukatsu Hojo", "title": "StarGAN-VC: Non-parallel many-to-many voice conversion with star\n  generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method that allows non-parallel many-to-many voice\nconversion (VC) by using a variant of a generative adversarial network (GAN)\ncalled StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it\n(1) requires no parallel utterances, transcriptions, or time alignment\nprocedures for speech generator training, (2) simultaneously learns\nmany-to-many mappings across different attribute domains using a single\ngenerator network, (3) is able to generate converted speech signals quickly\nenough to allow real-time implementations and (4) requires only several minutes\nof training examples to generate reasonably realistic-sounding speech.\nSubjective evaluation experiments on a non-parallel many-to-many speaker\nidentity conversion task revealed that the proposed method obtained higher\nsound quality and speaker similarity than a state-of-the-art method based on\nvariational autoencoding GANs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:24:23 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 06:19:34 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Kameoka", "Hirokazu", ""], ["Kaneko", "Takuhiro", ""], ["Tanaka", "Kou", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1806.02185", "submitter": "Gideon Dresdner", "authors": "Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, and\n  Gunnar R\\\"atsch", "title": "Boosting Black Box Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating a probability density in a tractable manner is a central task\nin Bayesian statistics. Variational Inference (VI) is a popular technique that\nachieves tractability by choosing a relatively simple variational family.\nBorrowing ideas from the classic boosting framework, recent approaches attempt\nto \\emph{boost} VI by replacing the selection of a single density with a\ngreedily constructed mixture of densities. In order to guarantee convergence,\nprevious works impose stringent assumptions that require significant effort for\npractitioners. Specifically, they require a custom implementation of the greedy\nstep (called the LMO) for every probabilistic model with respect to an\nunnatural variational family of truncated distributions. Our work fixes these\nissues with novel theoretical and algorithmic insights. On the theoretical\nside, we show that boosting VI satisfies a relaxed smoothness assumption which\nis sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm.\nFurthermore, we rephrase the LMO problem and propose to maximize the Residual\nELBO (RELBO) which replaces the standard ELBO optimization in VI. These\ntheoretical enhancements allow for black box implementation of the boosting\nsubroutine. Finally, we present a stopping criterion drawn from the duality gap\nin the classic FW analyses and exhaustive experiments to illustrate the\nusefulness of our theoretical and algorithmic contributions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:53:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 17:01:20 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 14:51:17 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 16:20:01 GMT"}, {"version": "v5", "created": "Wed, 28 Nov 2018 22:41:11 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Locatello", "Francesco", ""], ["Dresdner", "Gideon", ""], ["Khanna", "Rajiv", ""], ["Valera", "Isabel", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1806.02190", "submitter": "Vahid Behzadan", "authors": "Vahid Behzadan and Arslan Munir", "title": "Mitigation of Policy Manipulation Attacks on Deep Q-Networks with\n  Parameter-Space Noise", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.04143,\n  arXiv:1712.09344", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments have established the vulnerability of deep reinforcement\nlearning to policy manipulation attacks via intentionally perturbed inputs,\nknown as adversarial examples. In this work, we propose a technique for\nmitigation of such attacks based on addition of noise to the parameter space of\ndeep reinforcement learners during training. We experimentally verify the\neffect of parameter-space noise in reducing the transferability of adversarial\nexamples, and demonstrate the promising performance of this technique in\nmitigating the impact of whitebox and blackbox attacks at both test and\ntraining times.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 22:50:47 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Behzadan", "Vahid", ""], ["Munir", "Arslan", ""]]}, {"id": "1806.02193", "submitter": "Giannis Nikolentzos", "authors": "Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos\n  Giatsidis, Konstantinos Skianis, Michalis Vazirgiannis", "title": "GraKeL: A Graph Kernel Library in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of accurately measuring the similarity between graphs is at the\ncore of many applications in a variety of disciplines. Graph kernels have\nrecently emerged as a promising approach to this problem. There are now many\nkernels, each focusing on different structural aspects of graphs. Here, we\npresent GraKeL, a library that unifies several graph kernels into a common\nframework. The library is written in Python and adheres to the scikit-learn\ninterface. It is simple to use and can be naturally combined with\nscikit-learn's modules to build a complete machine learning pipeline for tasks\nsuch as graph classification and clustering. The code is BSD licensed and is\navailable at: https://github.com/ysig/GraKeL .\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:04:28 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 10:02:10 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Siglidis", "Giannis", ""], ["Nikolentzos", "Giannis", ""], ["Limnios", "Stratis", ""], ["Giatsidis", "Christos", ""], ["Skianis", "Konstantinos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1806.02199", "submitter": "Vincent Fortuin", "authors": "Vincent Fortuin, Matthias H\\\"user, Francesco Locatello, Heiko\n  Strathmann, Gunnar R\\\"atsch", "title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "comments": "Accepted for publication at the Seventh International Conference on\n  Learning Representations (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time series are common in many domains. Since human\ncognition is not optimized to work well in high-dimensional spaces, these areas\ncould benefit from interpretable low-dimensional representations. However, most\nrepresentation learning algorithms for time series data are difficult to\ninterpret. This is due to non-intuitive mappings from data features to salient\nproperties of the representation and non-smoothness over time. To address this\nproblem, we propose a new representation learning framework building on ideas\nfrom interpretable discrete dimensionality reduction and deep generative\nmodeling. This framework allows us to learn discrete representations of time\nseries, which give rise to smooth and interpretable embeddings with superior\nclustering performance. We introduce a new way to overcome the\nnon-differentiability in discrete representation learning and present a\ngradient-based version of the traditional self-organizing map algorithm that is\nmore performant than the original. Furthermore, to allow for a probabilistic\ninterpretation of our method, we integrate a Markov model in the representation\nspace. This model uncovers the temporal transition structure, improves\nclustering performance even further and provides additional explanatory\ninsights as well as a natural representation of uncertainty. We evaluate our\nmodel in terms of clustering performance and interpretability on static\n(Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST\nimages, a chaotic Lorenz attractor system with two macro states, as well as on\na challenging real world medical time series application on the eICU data set.\nOur learned representations compare favorably with competitor methods and\nfacilitate downstream tasks on the real world data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:11:30 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 09:26:19 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 09:12:06 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 14:53:38 GMT"}, {"version": "v5", "created": "Fri, 5 Oct 2018 12:34:56 GMT"}, {"version": "v6", "created": "Fri, 21 Dec 2018 10:23:45 GMT"}, {"version": "v7", "created": "Fri, 4 Jan 2019 14:33:08 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Fortuin", "Vincent", ""], ["H\u00fcser", "Matthias", ""], ["Locatello", "Francesco", ""], ["Strathmann", "Heiko", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1806.02215", "submitter": "David Pfau", "authors": "David Pfau, Stig Petersen, Ashish Agarwal, David G. T. Barrett,\n  Kimberly L. Stachenfeld", "title": "Spectral Inference Networks: Unifying Deep and Spectral Learning", "comments": "Fixed typo in math in section 4", "journal-ref": "Seventh International Conference on Learning Representations (ICLR\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Spectral Inference Networks, a framework for learning\neigenfunctions of linear operators by stochastic optimization. Spectral\nInference Networks generalize Slow Feature Analysis to generic symmetric\noperators, and are closely related to Variational Monte Carlo methods from\ncomputational physics. As such, they can be a powerful tool for unsupervised\nrepresentation learning from video or graph-structured data. We cast training\nSpectral Inference Networks as a bilevel optimization problem, which allows for\nonline learning of multiple eigenfunctions. We show results of training\nSpectral Inference Networks on problems in quantum mechanics and feature\nlearning for videos on synthetic datasets. Our results demonstrate that\nSpectral Inference Networks accurately recover eigenfunctions of linear\noperators and can discover interpretable representations from video in a fully\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:28:03 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:11:43 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 17:02:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Pfau", "David", ""], ["Petersen", "Stig", ""], ["Agarwal", "Ashish", ""], ["Barrett", "David G. T.", ""], ["Stachenfeld", "Kimberly L.", ""]]}, {"id": "1806.02246", "submitter": "Xueru Zhang", "authors": "Xueru Zhang, Mohammad Mahdi Khalili, Mingyan Liu", "title": "Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms", "comments": "accepted to 35th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating direction method of multiplier (ADMM) is a popular method used to\ndesign distributed versions of a machine learning algorithm, whereby local\ncomputations are performed on local data with the output exchanged among\nneighbors in an iterative fashion. During this iterative process the leakage of\ndata privacy arises. A differentially private ADMM was proposed in prior work\n(Zhang & Zhu, 2017) where only the privacy loss of a single node during one\niteration was bounded, a method that makes it difficult to balance the tradeoff\nbetween the utility attained through distributed computation and privacy\nguarantees when considering the total privacy loss of all nodes over the entire\niterative process. We propose a perturbation method for ADMM where the\nperturbed term is correlated with the penalty parameters; this is shown to\nimprove the utility and privacy simultaneously. The method is based on a\nmodified ADMM where each node independently determines its own penalty\nparameter in every iteration and decouples it from the dual updating step size.\nThe condition for convergence of the modified ADMM and the lower bound on the\nconvergence rate are also derived.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:31:48 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Zhang", "Xueru", ""], ["Khalili", "Mohammad Mahdi", ""], ["Liu", "Mingyan", ""]]}, {"id": "1806.02248", "submitter": "Shuai Li", "authors": "Tor Lattimore, Branislav Kveton, Shuai Li, Csaba Szepesvari", "title": "TopRank: A practical algorithm for online stochastic ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning to rank is a sequential decision-making problem where in each\nround the learning agent chooses a list of items and receives feedback in the\nform of clicks from the user. Many sample-efficient algorithms have been\nproposed for this problem that assume a specific click model connecting\nrankings and user behavior. We propose a generalized click model that\nencompasses many existing models, including the position-based and cascade\nmodels. Our generalization motivates a novel online learning algorithm based on\ntopological sort, which we call TopRank. TopRank is (a) more natural than\nexisting algorithms, (b) has stronger regret guarantees than existing\nalgorithms with comparable generality, (c) has a more insightful proof that\nleaves the door open to many generalizations, (d) outperforms existing\nalgorithms empirically.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:33:08 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 03:17:31 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Lattimore", "Tor", ""], ["Kveton", "Branislav", ""], ["Li", "Shuai", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1806.02252", "submitter": "Akihiro Yabe", "authors": "Akihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori\n  Kakimura, Takuro Fukunaga, and Ken-ichi Kawarabayashi", "title": "Causal Bandits with Propagating Inference", "comments": "To appear in International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit is a framework for designing sequential experiments. In each\nexperiment, a learner selects an arm $A \\in \\mathcal{A}$ and obtains an\nobservation corresponding to $A$. Theoretically, the tight regret lower-bound\nfor the general bandit is polynomial with respect to the number of arms\n$|\\mathcal{A}|$. This makes bandit incapable of handling an exponentially large\nnumber of arms, hence the bandit problem with side-information is often\nconsidered to overcome this lower bound. Recently, a bandit framework over a\ncausal graph was introduced, where the structure of the causal graph is\navailable as side-information. A causal graph is a fundamental model that is\nfrequently used with a variety of real problems. In this setting, the arms are\nidentified with interventions on a given causal graph, and the effect of an\nintervention propagates throughout all over the causal graph. The task is to\nfind the best intervention that maximizes the expected value on a target node.\nExisting algorithms for causal bandit overcame the\n$\\Omega(\\sqrt{|\\mathcal{A}|/T})$ simple-regret lower-bound; however, their\nalgorithms work only when the interventions $\\mathcal{A}$ are localized around\na single node (i.e., an intervention propagates only to its neighbors).\n  We propose a novel causal bandit algorithm for an arbitrary set of\ninterventions, which can propagate throughout the causal graph. We also show\nthat it achieves $O(\\sqrt{ \\gamma^*\\log(|\\mathcal{A}|T) / T})$ regret bound,\nwhere $\\gamma^*$ is determined by using a causal graph structure. In\nparticular, if the in-degree of the causal graph is bounded, then $\\gamma^* =\nO(N^2)$, where $N$ is the number $N$ of nodes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:39:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Yabe", "Akihiro", ""], ["Hatano", "Daisuke", ""], ["Sumita", "Hanna", ""], ["Ito", "Shinji", ""], ["Kakimura", "Naonori", ""], ["Fukunaga", "Takuro", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1806.02256", "submitter": "Liang Tong", "authors": "Liang Tong, Sixie Yu, Scott Alfeld, Yevgeniy Vorobeychik", "title": "Adversarial Regression with Multiple Learners", "comments": "Accepted by ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the considerable success enjoyed by machine learning techniques in\npractice, numerous studies demonstrated that many approaches are vulnerable to\nattacks. An important class of such attacks involves adversaries changing\nfeatures at test time to cause incorrect predictions. Previous investigations\nof this problem pit a single learner against an adversary. However, in many\nsituations an adversary's decision is aimed at a collection of learners, rather\nthan specifically targeted at each independently. We study the problem of\nadversarial linear regression with multiple learners. We approximate the\nresulting game by exhibiting an upper bound on learner loss functions, and show\nthat the resulting game has a unique symmetric equilibrium. We present an\nalgorithm for computing this equilibrium, and show through extensive\nexperiments that equilibrium models are significantly more robust than\nconventional regularized linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:44:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Tong", "Liang", ""], ["Yu", "Sixie", ""], ["Alfeld", "Scott", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1806.02261", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch and Jack Jewson and Theodoros Damoulas", "title": "Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with\n  $\\beta$-Divergences", "comments": "39 pages, 11 figures, published at Neural Information Processing\n  Systems (NeurIPS) 2018", "journal-ref": "Neural Information Processing Systems (NeurIPS) 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the very first robust Bayesian Online Changepoint Detection\nalgorithm through General Bayesian Inference (GBI) with $\\beta$-divergences.\nThe resulting inference procedure is doubly robust for both the parameter and\nthe changepoint (CP) posterior, with linear time and constant space complexity.\nWe provide a construction for exponential models and demonstrate it on the\nBayesian Linear Regression model. In so doing, we make two additional\ncontributions: Firstly, we make GBI scalable using Structural Variational\napproximations that are exact as $\\beta \\to 0$. Secondly, we give a principled\nway of choosing the divergence parameter $\\beta$ by minimizing expected\npredictive loss on-line. Reducing False Discovery Rates of CPs from more than\n90% to 0% on real world data, this offers the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:55:42 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:39:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Knoblauch", "Jeremias", ""], ["Jewson", "Jack", ""], ["Damoulas", "Theodoros", ""]]}, {"id": "1806.02282", "submitter": "Pierre Perrault", "authors": "Pierre Perrault and Vianney Perchet and Michal Valko", "title": "Finding the bandit in a graph: Sequential search-and-stop", "comments": "in International Conference on Artificial Intelligence and Statistics\n  (AISTATS 2019), April 2019, Naha, Okinawa, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem where an agent wants to find a hidden object that is\nrandomly located in some vertex of a directed acyclic graph (DAG) according to\na fixed but possibly unknown distribution. The agent can only examine vertices\nwhose in-neighbors have already been examined. In this paper, we address a\nlearning setting where we allow the agent to stop before having found the\nobject and restart searching on a new independent instance of the same problem.\nOur goal is to maximize the total number of hidden objects found given a time\nbudget. The agent can thus skip an instance after realizing that it would spend\ntoo much time on it. Our contributions are both to the search theory and\nmulti-armed bandits. If the distribution is known, we provide a quasi-optimal\nand efficient stationary strategy. If the distribution is unknown, we\nadditionally show how to sequentially approximate it and, at the same time, act\nnear-optimally in order to collect as many hidden objects as possible.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:22:33 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:09:41 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 10:43:54 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Perrault", "Pierre", ""], ["Perchet", "Vianney", ""], ["Valko", "Michal", ""]]}, {"id": "1806.02300", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Katherine Swett, Susan M. Resnick, Laurie E. Cutting,\n  Bennett A. Landman", "title": "Data-driven Probabilistic Atlases Capture Whole-brain Individual\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic atlases provide essential spatial contextual information for\nimage interpretation, Bayesian modeling, and algorithmic processing. Such\natlases are typically constructed by grouping subjects with similar demographic\ninformation. Importantly, use of the same scanner minimizes inter-group\nvariability. However, generalizability and spatial specificity of such\napproaches is more limited than one might like. Inspired by Commowick\n\"Frankenstein's creature paradigm\" which builds a personal specific anatomical\natlas, we propose a data-driven framework to build a personal specific\nprobabilistic atlas under the large-scale data scheme. The data-driven\nframework clusters regions with similar features using a point distribution\nmodel to learn different anatomical phenotypes. Regional structural atlases and\ncorresponding regional probabilistic atlases are used as indices and targets in\nthe dictionary. By indexing the dictionary, the whole brain probabilistic\natlases adapt to each new subject quickly and can be used as spatial priors for\nvisualization and processing. The novelties of this approach are (1) it\nprovides a new perspective of generating personal specific whole brain\nprobabilistic atlases (132 regions) under data-driven scheme across sites. (2)\nThe framework employs the large amount of heterogeneous data (2349 images). (3)\nThe proposed framework achieves low computational cost since only one affine\nregistration and Pearson correlation operation are required for a new subject.\nOur method matches individual regions better with higher Dice similarity value\nwhen testing the probabilistic atlases. Importantly, the advantage the\nlarge-scale scheme is demonstrated by the better performance of using\nlarge-scale training data (1888 images) than smaller training set (720 images).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:53:55 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Huo", "Yuankai", ""], ["Swett", "Katherine", ""], ["Resnick", "Susan M.", ""], ["Cutting", "Laurie E.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1806.02315", "submitter": "Ahmed Touati", "authors": "Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau and Pascal\n  Vincent", "title": "Randomized Value Functions via Multiplicative Normalizing Flows", "comments": null, "journal-ref": "UAI 2019: Conference on Uncertainty in Artificial Intelligence\n  2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized value functions offer a promising approach towards the challenge\nof efficient exploration in complex environments with high dimensional state\nand action spaces. Unlike traditional point estimate methods, randomized value\nfunctions maintain a posterior distribution over action-space values. This\nprevents the agent's behavior policy from prematurely exploiting early\nestimates and falling into local optima. In this work, we leverage recent\nadvances in variational Bayesian neural networks and combine these with\ntraditional Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG)\nto achieve randomized value functions for high-dimensional domains. In\nparticular, we augment DQN and DDPG with multiplicative normalizing flows in\norder to track a rich approximate posterior distribution over the parameters of\nthe value function. This allows the agent to perform approximate Thompson\nsampling in a computationally efficient manner via stochastic gradient methods.\nWe demonstrate the benefits of our approach through an empirical comparison in\nhigh dimensional environments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:32:24 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 21:50:02 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 19:08:17 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Touati", "Ahmed", ""], ["Satija", "Harsh", ""], ["Romoff", "Joshua", ""], ["Pineau", "Joelle", ""], ["Vincent", "Pascal", ""]]}, {"id": "1806.02321", "submitter": "Patrick Perry", "authors": "Ningshan Zhang, Kyle Schmaus, Patrick O. Perry", "title": "Fitting a deeply-nested hierarchical model to a large book review\n  dataset using a moment-based estimator", "comments": "32 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a particular instance of a common problem in recommender systems:\nusing a database of book reviews to inform user-targeted recommendations. In\nour dataset, books are categorized into genres and sub-genres. To exploit this\nnested taxonomy, we use a hierarchical model that enables information pooling\nacross across similar items at many levels within the genre hierarchy. The main\nchallenge in deploying this model is computational: the data sizes are large,\nand fitting the model at scale using off-the-shelf maximum likelihood\nprocedures is prohibitive. To get around this computational bottleneck, we\nextend a moment-based fitting procedure proposed for fitting single-level\nhierarchical models to the general case of arbitrarily deep hierarchies. This\nextension is an order of magnetite faster than standard maximum likelihood\nprocedures. The fitting method can be deployed beyond recommender systems to\ngeneral contexts with deeply-nested hierarchical generalized linear mixed\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:34:04 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Zhang", "Ningshan", ""], ["Schmaus", "Kyle", ""], ["Perry", "Patrick O.", ""]]}, {"id": "1806.02322", "submitter": "Hadi Ghauch", "authors": "Hadi Ghauch, Mikael Skoglund, Hossein Shokri-Ghadikolaei, Carlo\n  Fischione, Ali H. Sayed", "title": "Learning Kolmogorov Models for Binary Random Variables", "comments": "9 pages, accecpted to ICML 2018: Workshop on Nonconvex Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize our recent findings, where we proposed a framework for learning\na Kolmogorov model, for a collection of binary random variables. More\nspecifically, we derive conditions that link outcomes of specific random\nvariables, and extract valuable relations from the data. We also propose an\nalgorithm for computing the model and show its first-order optimality, despite\nthe combinatorial nature of the learning problem. We apply the proposed\nalgorithm to recommendation systems, although it is applicable to other\nscenarios. We believe that the work is a significant step toward interpretable\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:39:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ghauch", "Hadi", ""], ["Skoglund", "Mikael", ""], ["Shokri-Ghadikolaei", "Hossein", ""], ["Fischione", "Carlo", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1806.02326", "submitter": "Brendan Juba", "authors": "Diego Calderon, Brendan Juba, Sirui Li, Zongyi Li, Lisa Ruan", "title": "Conditional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in machine learning and statistics commonly focuses on building models\nthat capture the vast majority of data, possibly ignoring a segment of the\npopulation as outliers. However, there does not often exist a good model on the\nwhole dataset, so we seek to find a small subset where there exists a useful\nmodel. We are interested in finding a linear rule capable of achieving more\naccurate predictions for just a segment of the population. We give an efficient\nalgorithm with theoretical analysis for the conditional linear regression task,\nwhich is the joint task of identifying a significant segment of the population,\ndescribed by a k-DNF, along with its linear regression fit.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:46:30 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 21:25:30 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Calderon", "Diego", ""], ["Juba", "Brendan", ""], ["Li", "Sirui", ""], ["Li", "Zongyi", ""], ["Ruan", "Lisa", ""]]}, {"id": "1806.02329", "submitter": "Aaron Roth", "authors": "Seth Neel, Aaron Roth", "title": "Mitigating Bias in Adaptive Data Gathering via Differential Privacy", "comments": "Conference version appears in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data that is gathered adaptively --- via bandit algorithms, for example ---\nexhibits bias. This is true both when gathering simple numeric valued data ---\nthe empirical means kept track of by stochastic bandit algorithms are biased\ndownwards --- and when gathering more complicated data --- running hypothesis\ntests on complex data gathered via contextual bandit algorithms leads to false\ndiscovery. In this paper, we show that this problem is mitigated if the data\ncollection procedure is differentially private. This lets us both bound the\nbias of simple numeric valued quantities (like the empirical means of\nstochastic bandit algorithms), and correct the p-values of hypothesis tests run\non the adaptively gathered data. Moreover, there exist differentially private\nbandit algorithms with near optimal regret bounds: we apply existing theorems\nin the simple stochastic case, and give a new analysis for linear contextual\nbandits. We complement our theoretical results with experiments validating our\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:54:07 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1806.02336", "submitter": "Naoyuki Ichimura Dr.", "authors": "Naoyuki Ichimura", "title": "Spatial Frequency Loss for Learning Convolutional Autoencoders", "comments": "9 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning method for convolutional autoencoders (CAEs)\nfor extracting features from images. CAEs can be obtained by utilizing\nconvolutional neural networks to learn an approximation to the identity\nfunction in an unsupervised manner. The loss function based on the pixel loss\n(PL) that is the mean squared error between the pixel values of original and\nreconstructed images is the common choice for learning. However, using the loss\nfunction leads to blurred reconstructed images. A method for learning CAEs\nusing a loss function computed from features reflecting spatial frequencies is\nproposed to mitigate the problem. The blurs in reconstructed images show lack\nof high spatial frequency components mainly constituting edges and detailed\ntextures that are important features for tasks such as object detection and\nspatial matching. In order to evaluate the lack of components, a convolutional\nlayer with a Laplacian filter bank as weights is added to CAEs and the mean\nsquared error of features in a subband, called the spatial frequency loss\n(SFL), is computed from the outputs of each filter. The learning is performed\nusing a loss function based on the SFL. Empirical evaluation demonstrates that\nusing the SFL reduces the blurs in reconstructed images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:34:12 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ichimura", "Naoyuki", ""]]}, {"id": "1806.02338", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Georg N\\\"uhrenberg, Chung-Hao Huang, Harald Ruess,\n  Hirotoshi Yasuoka", "title": "Towards Dependability Metrics for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (NN) are instrumental in realizing\nhighly-automated driving functionality. An overarching challenge is to identify\nbest safety engineering practices for NN and other learning-enabled components.\nIn particular, there is an urgent need for an adequate set of metrics for\nmeasuring all-important NN dependability attributes. We address this challenge\nby proposing a number of NN-specific and efficiently computable metrics for\nmeasuring NN dependability attributes including robustness, interpretability,\ncompleteness, and correctness.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:27:37 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 16:46:40 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["N\u00fchrenberg", "Georg", ""], ["Huang", "Chung-Hao", ""], ["Ruess", "Harald", ""], ["Yasuoka", "Hirotoshi", ""]]}, {"id": "1806.02371", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song", "title": "Adversarial Attack on Graph Structured Data", "comments": "to appear in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on graph structures has shown exciting results in various\napplications. However, few attentions have been paid to the robustness of such\nmodels, in contrast to numerous research work for image or text adversarial\nattack and defense. In this paper, we focus on the adversarial attacks that\nfool the model by modifying the combinatorial structure of data. We first\npropose a reinforcement learning based attack method that learns the\ngeneralizable attack policy, while only requiring prediction labels from the\ntarget classifier. Also, variants of genetic algorithms and gradient methods\nare presented in the scenario where prediction confidence or gradients are\navailable. We use both synthetic and real-world data to show that, a family of\nGraph Neural Network models are vulnerable to these attacks, in both\ngraph-level and node-level classification tasks. We also show such attacks can\nbe used to diagnose the learned classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 18:23:33 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Dai", "Hanjun", ""], ["Li", "Hui", ""], ["Tian", "Tian", ""], ["Huang", "Xin", ""], ["Wang", "Lin", ""], ["Zhu", "Jun", ""], ["Song", "Le", ""]]}, {"id": "1806.02375", "submitter": "Johan Bjorck", "authors": "Johan Bjorck, Carla Gomes, Bart Selman, Kilian Q. Weinberger", "title": "Understanding Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) is a technique to normalize activations in\nintermediate layers of deep neural networks. Its tendency to improve accuracy\nand speed up training have established BN as a favorite technique in deep\nlearning. Yet, despite its enormous success, there remains little consensus on\nthe exact reason and mechanism behind these improvements. In this paper we take\na step towards a better understanding of BN, following an empirical approach.\nWe conduct several experiments, and show that BN primarily enables training\nwith larger learning rates, which is the cause for faster convergence and\nbetter generalization. For networks without BN we demonstrate how large\ngradient updates can result in diverging loss and activations growing\nuncontrollably with network depth, which limits possible learning rates. BN\navoids this problem by constantly correcting activations to be zero-mean and of\nunit standard deviation, which enables larger gradient steps, yields faster\nconvergence and may help bypass sharp local minima. We further show various\nways in which gradients and activations of deep unnormalized networks are\nill-behaved. We contrast our results against recent findings in random matrix\ntheory, shedding new light on classical initialization schemes and their\nconsequences.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:57:56 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 15:12:45 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 23:22:43 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 05:56:20 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Bjorck", "Johan", ""], ["Gomes", "Carla", ""], ["Selman", "Bart", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1806.02380", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Chris Russell, Joshua R. Loftus, Ricardo Silva", "title": "Causal Interventions for Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches in algorithmic fairness constrain machine learning methods so\nthe resulting predictions satisfy one of several intuitive notions of fairness.\nWhile this may help private companies comply with non-discrimination laws or\navoid negative publicity, we believe it is often too little, too late. By the\ntime the training data is collected, individuals in disadvantaged groups have\nalready suffered from discrimination and lost opportunities due to factors out\nof their control. In the present work we focus instead on interventions such as\na new public policy, and in particular, how to maximize their positive effects\nwhile improving the fairness of the overall system. We use causal methods to\nmodel the effects of interventions, allowing for potential interference--each\nindividual's outcome may depend on who else receives the intervention. We\ndemonstrate this with an example of allocating a budget of teaching resources\nusing a dataset of schools in New York City.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 18:46:11 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Kusner", "Matt J.", ""], ["Russell", "Chris", ""], ["Loftus", "Joshua R.", ""], ["Silva", "Ricardo", ""]]}, {"id": "1806.02382", "submitter": "Oleg Ivanov", "authors": "Oleg Ivanov, Michael Figurnov, Dmitry Vetrov", "title": "Variational Autoencoder with Arbitrary Conditioning", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 18:52:13 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 10:16:28 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 19:06:41 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Ivanov", "Oleg", ""], ["Figurnov", "Michael", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1806.02389", "submitter": "Parameswaran Kamalaruban Dr.", "authors": "Parameswaran Kamalaruban and Victor Perrier and Hassan Jameel Asghar\n  and Mohamed Ali Kaafar", "title": "Not All Attributes are Created Equal: $d_{\\mathcal{X}}$-Private\n  Mechanisms for Linear Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy provides strong privacy guarantees simultaneously\nenabling useful insights from sensitive datasets. However, it provides the same\nlevel of protection for all elements (individuals and attributes) in the data.\nThere are practical scenarios where some data attributes need more/less\nprotection than others. In this paper, we consider $d_{\\mathcal{X}}$-privacy,\nan instantiation of the privacy notion introduced in\n\\cite{chatzikokolakis2013broadening}, which allows this flexibility by\nspecifying a separate privacy budget for each pair of elements in the data\ndomain. We describe a systematic procedure to tailor any existing\ndifferentially private mechanism that assumes a query set and a sensitivity\nvector as input into its $d_{\\mathcal{X}}$-private variant, specifically\nfocusing on linear queries. Our proposed meta procedure has broad applications\nas linear queries form the basis of a range of data analysis and machine\nlearning algorithms, and the ability to define a more flexible privacy budget\nacross the data domain results in improved privacy/utility tradeoff in these\napplications. We propose several $d_{\\mathcal{X}}$-private mechanisms, and\nprovide theoretical guarantees on the trade-off between utility and privacy. We\nalso experimentally demonstrate the effectiveness of our procedure, by\nevaluating our proposed $d_{\\mathcal{X}}$-private Laplace mechanism on both\nsynthetic and real datasets using a set of randomly generated linear queries.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:11:39 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 20:09:30 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Kamalaruban", "Parameswaran", ""], ["Perrier", "Victor", ""], ["Asghar", "Hassan Jameel", ""], ["Kaafar", "Mohamed Ali", ""]]}, {"id": "1806.02390", "submitter": "Chao Ma", "authors": "Chao Ma, Yingzhen Li, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Variational Implicit Processes", "comments": "ICML 2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the implicit processes (IPs), a stochastic process that places\nimplicitly defined multivariate distributions over any finite collections of\nrandom variables. IPs are therefore highly flexible implicit priors over\nfunctions, with examples including data simulators, Bayesian neural networks\nand non-linear transformations of stochastic processes. A novel and efficient\napproximate inference algorithm for IPs, namely the variational implicit\nprocesses (VIPs), is derived using generalised wake-sleep updates. This method\nreturns simple update equations and allows scalable hyper-parameter learning\nwith stochastic optimization. Experiments show that VIPs return better\nuncertainty estimates and lower errors over existing inference methods for\nchallenging models such as Bayesian neural networks, and Gaussian processes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:13:53 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:22:39 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Ma", "Chao", ""], ["Li", "Yingzhen", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1806.02402", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Francis Bach, Alessandro Rudi", "title": "Localized Structured Prediction", "comments": "53 pages, 7 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to structured prediction is exploiting the problem structure to simplify\nthe learning process. A major challenge arises when data exhibit a local\nstructure (e.g., are made by \"parts\") that can be leveraged to better\napproximate the relation between (parts of) the input and (parts of) the\noutput. Recent literature on signal processing, and in particular computer\nvision, has shown that capturing these aspects is indeed essential to achieve\nstate-of-the-art performance. While such algorithms are typically derived on a\ncase-by-case basis, in this work we propose the first theoretical framework to\ndeal with part-based data from a general perspective. We derive a novel\napproach to deal with these problems and study its generalization properties\nwithin the setting of statistical learning theory. Our analysis is novel in\nthat it explicitly quantifies the benefits of leveraging the part-based\nstructure of the problem with respect to the learning rates of the proposed\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:46:04 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 09:49:24 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 18:13:49 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Bach", "Francis", ""], ["Rudi", "Alessandro", ""]]}, {"id": "1806.02421", "submitter": "Cheol Young Park", "authors": "Cheol Young Park, Kathryn Blackmond Laskey", "title": "Human-aided Multi-Entity Bayesian Networks Learning from Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Artificial Intelligence (AI) system is an autonomous system which emulates\nhuman mental and physical activities such as Observe, Orient, Decide, and Act,\ncalled the OODA process. An AI system performing the OODA process requires a\nsemantically rich representation to handle a complex real world situation and\nability to reason under uncertainty about the situation. Multi-Entity Bayesian\nNetworks (MEBNs) combines First-Order Logic with Bayesian Networks for\nrepresenting and reasoning about uncertainty in complex, knowledge-rich\ndomains. MEBN goes beyond standard Bayesian networks to enable reasoning about\nan unknown number of entities interacting with each other in various types of\nrelationships, a key requirement for the OODA process of an AI system. MEBN\nmodels have heretofore been constructed manually by a domain expert. However,\nmanual MEBN modeling is labor-intensive and insufficiently agile. To address\nthese problems, an efficient method is needed for MEBN modeling. One of the\nmethods is to use machine learning to learn a MEBN model in whole or in part\nfrom data. In the era of Big Data, data-rich environments, characterized by\nuncertainty and complexity, have become ubiquitous. The larger the data sample\nis, the more accurate the results of the machine learning approach can be.\nTherefore, machine learning has potential to improve the quality of MEBN models\nas well as the effectiveness for MEBN modeling. In this research, we study a\nMEBN learning framework to develop a MEBN model from a combination of domain\nexpert's knowledge and data. To evaluate the MEBN learning framework, we\nconduct an experiment to compare the MEBN learning framework and the existing\nmanual MEBN modeling in terms of development efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:51:06 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Park", "Cheol Young", ""], ["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1806.02426", "submitter": "Maximilian Igl", "authors": "Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon\n  Whiteson", "title": "Deep Variational Reinforcement Learning for POMDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world sequential decision making problems are partially observable\nby nature, and the environment model is typically unknown. Consequently, there\nis great need for reinforcement learning methods that can tackle such problems\ngiven only a stream of incomplete and noisy observations. In this paper, we\npropose deep variational reinforcement learning (DVRL), which introduces an\ninductive bias that allows an agent to learn a generative model of the\nenvironment and perform inference in that model to effectively aggregate the\navailable information. We develop an n-step approximation to the evidence lower\nbound (ELBO), allowing the model to be trained jointly with the policy. This\nensures that the latent state representation is suitable for the control task.\nIn experiments on Mountain Hike and flickering Atari we show that our method\noutperforms previous approaches relying on recurrent neural networks to encode\nthe past.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 21:09:39 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Igl", "Maximilian", ""], ["Zintgraf", "Luisa", ""], ["Le", "Tuan Anh", ""], ["Wood", "Frank", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1806.02448", "submitter": "Philip Bontrager", "authors": "Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin\n  Liu, Diego Perez-Liebana", "title": "Deep Reinforcement Learning for General Video Game AI", "comments": "8 pages, 4 figures, Accepted at the conference on Computational\n  Intelligence and Games 2018 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Video Game AI (GVGAI) competition and its associated software\nframework provides a way of benchmarking AI algorithms on a large number of\ngames written in a domain-specific description language. While the competition\nhas seen plenty of interest, it has so far focused on online planning,\nproviding a forward model that allows the use of algorithms such as Monte Carlo\nTree Search.\n  In this paper, we describe how we interface GVGAI to the OpenAI Gym\nenvironment, a widely used way of connecting agents to reinforcement learning\nproblems. Using this interface, we characterize how widely used implementations\nof several deep reinforcement learning algorithms fare on a number of GVGAI\ngames. We further analyze the results to provide a first indication of the\nrelative difficulty of these games relative to each other, and relative to\nthose in the Arcade Learning Environment under similar conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:39:26 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Torrado", "Ruben Rodriguez", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1806.02450", "submitter": "Daniel Russo", "authors": "Jalaj Bhandari, Daniel Russo, Raghav Singal", "title": "A Finite Time Analysis of Temporal Difference Learning With Linear\n  Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal difference learning (TD) is a simple iterative algorithm used to\nestimate the value function corresponding to a given policy in a Markov\ndecision process. Although TD is one of the most widely used algorithms in\nreinforcement learning, its theoretical analysis has proved challenging and few\nguarantees on its statistical efficiency are available. In this work, we\nprovide a simple and explicit finite time analysis of temporal difference\nlearning with linear function approximation. Except for a few key insights, our\nanalysis mirrors standard techniques for analyzing stochastic gradient descent\nalgorithms, and therefore inherits the simplicity and elegance of that\nliterature. Final sections of the paper show how all of our main results extend\nto the study of TD learning with eligibility traces, known as TD($\\lambda$),\nand to Q-learning applied in high-dimensional optimal stopping problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:57:08 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 07:34:09 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Bhandari", "Jalaj", ""], ["Russo", "Daniel", ""], ["Singal", "Raghav", ""]]}, {"id": "1806.02455", "submitter": "Cheol Young Park", "authors": "Cheol Young Park and Kathryn Blackmond Laskey", "title": "MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational\n  Model", "comments": null, "journal-ref": "Applied Sciences 2019,9", "doi": "10.3390/app9091743", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Entity Bayesian Network (MEBN) is a knowledge representation formalism\ncombining Bayesian Networks (BN) with First-Order Logic (FOL). MEBN has\nsufficient expressive power for general-purpose knowledge representation and\nreasoning. Developing a MEBN model to support a given application is a\nchallenge, requiring definition of entities, relationships, random variables,\nconditional dependence relationships, and probability distributions. When\navailable, data can be invaluable both to improve performance and to streamline\ndevelopment. By far the most common format for available data is the relational\ndatabase (RDB). Relational databases describe and organize data according to\nthe Relational Model (RM). Developing a MEBN model from data stored in an RDB\ntherefore requires mapping between the two formalisms. This paper presents\nMEBN-RM, a set of mapping rules between key elements of MEBN and RM. We\nidentify links between the two languages (RM and MEBN) and define four levels\nof mapping from elements of RM to elements of MEBN. These definitions are\nimplemented in the MEBN-RM algorithm, which converts a relational schema in RM\nto a partial MEBN model. Through this research, the software has been released\nas a MEBN-RM open-source software tool. The method is illustrated through two\nexample use cases using MEBN-RM to develop MEBN models: a Critical\nInfrastructure Defense System and a Smart Manufacturing System.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:12:02 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 00:36:05 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Park", "Cheol Young", ""], ["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1806.02460", "submitter": "Lech Szymanski", "authors": "Lech Szymanski, Brendan McCane, Michael Albert", "title": "The effect of the choice of neural network depth and breadth on the size\n  of its hypothesis space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the number of unique function mappings in a neural network\nhypothesis space is inversely proportional to $\\prod_lU_l!$, where $U_{l}$ is\nthe number of neurons in the hidden layer $l$.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:34:06 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Szymanski", "Lech", ""], ["McCane", "Brendan", ""], ["Albert", "Michael", ""]]}, {"id": "1806.02473", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec", "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph\n  Generation", "comments": "NeurIPS 2018, spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating novel graph structures that optimize given objectives while\nobeying some given underlying rules is fundamental for chemistry, biology and\nsocial science research. This is especially important in the task of molecular\ngraph generation, whose goal is to discover novel molecules with desired\nproperties such as drug-likeness and synthetic accessibility, while obeying\nphysical laws such as chemical valency. However, designing models to find\nmolecules that optimize desired properties while incorporating highly complex\nand non-differentiable rules remains to be a challenging task. Here we propose\nGraph Convolutional Policy Network (GCPN), a general graph convolutional\nnetwork based model for goal-directed graph generation through reinforcement\nlearning. The model is trained to optimize domain-specific rewards and\nadversarial loss through policy gradient, and acts in an environment that\nincorporates domain-specific rules. Experimental results show that GCPN can\nachieve 61% improvement on chemical property optimization over state-of-the-art\nbaselines while resembling known molecules, and achieve 184% improvement on the\nconstrained property optimization task.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 00:47:09 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 01:17:56 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 03:19:38 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["You", "Jiaxuan", ""], ["Liu", "Bowen", ""], ["Ying", "Rex", ""], ["Pande", "Vijay", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.02485", "submitter": "Mason A. Porter", "authors": "Zachary M. Boyd, Mason A. Porter, and Andrea L. Bertozzi", "title": "Stochastic Block Models are a Discrete Surface Tension", "comments": "to appear in Journal of Nonlinear Science", "journal-ref": null, "doi": "10.1007/s00332-019-09541-8", "report-no": null, "categories": "cs.SI cond-mat.stat-mech math.ST nlin.AO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks, which represent agents and interactions between them, arise in\nmyriad applications throughout the sciences, engineering, and even the\nhumanities. To understand large-scale structure in a network, a common task is\nto cluster a network's nodes into sets called \"communities\", such that there\nare dense connections within communities but sparse connections between them. A\npopular and statistically principled method to perform such clustering is to\nuse a family of generative models known as stochastic block models (SBMs). In\nthis paper, we show that maximum likelihood estimation in an SBM is a network\nanalog of a well-known continuum surface-tension problem that arises from an\napplication in metallurgy. To illustrate the utility of this relationship, we\nimplement network analogs of three surface-tension algorithms, with which we\nsuccessfully recover planted community structure in synthetic networks and\nwhich yield fascinating insights on empirical networks that we construct from\nhyperspectral videos.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 01:56:03 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 03:19:29 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Boyd", "Zachary M.", ""], ["Porter", "Mason A.", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1806.02499", "submitter": "Wen Yu", "authors": "Erick de la Rosa, Wen Yu", "title": "Conditional probability calculation using restricted Boltzmann machine\n  with application to system identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many advantages to use probability method for nonlinear system\nidentification, such as the noises and outliers in the data set do not affect\nthe probability models significantly; the input features can be extracted in\nprobability forms. The biggest obstacle of the probability model is the\nprobability distributions are not easy to be obtained. In this paper, we form\nthe nonlinear system identification into solving the conditional probability.\nThen we modify the restricted Boltzmann machine (RBM), such that the joint\nprobability, input distribution, and the conditional probability can be\ncalculated by the RBM training. Binary encoding and continue valued methods are\ndiscussed. The universal approximation analysis for the conditional probability\nbased modelling is proposed. We use two benchmark nonlinear systems to compare\nour probability modelling method with the other black-box modeling methods. The\nresults show that this novel method is much better when there are big noises\nand the system dynamics are complex.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 03:28:48 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["de la Rosa", "Erick", ""], ["Yu", "Wen", ""]]}, {"id": "1806.02507", "submitter": "Qizhi Zhang", "authors": "Qizhi Zhang, Kuang-Chih Lee, Hongying Bao, Yuan You, Wenjie Li,\n  Dongbai Guo", "title": "Large scale classification in deep neural network with Label Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural network is widely used in machine learning. The\nmulti-class classification problem is a class of important problem in machine\nlearning. However, in order to solve those types of multi-class classification\nproblems effectively, the required network size should have hyper-linear growth\nwith respect to the number of classes. Therefore, it is infeasible to solve the\nmulti-class classification problem using deep neural network when the number of\nclasses are huge. This paper presents a method, so called Label Mapping (LM),\nto solve this problem by decomposing the original classification problem to\nseveral smaller sub-problems which are solvable theoretically. Our method is an\nensemble method like error-correcting output codes (ECOC), but it allows base\nlearners to be multi-class classifiers with different number of class labels.\nWe propose two design principles for LM, one is to maximize the number of base\nclassifier which can separate two different classes, and the other is to keep\nall base learners to be independent as possible in order to reduce the\nredundant information. Based on these principles, two different LM algorithms\nare derived using number theory and information theory. Since each base learner\ncan be trained independently, it is easy to scale our method into a large scale\ntraining system. Experiments show that our proposed method outperforms the\nstandard one-hot encoding and ECOC significantly in terms of accuracy and model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:14:47 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhang", "Qizhi", ""], ["Lee", "Kuang-Chih", ""], ["Bao", "Hongying", ""], ["You", "Yuan", ""], ["Li", "Wenjie", ""], ["Guo", "Dongbai", ""]]}, {"id": "1806.02510", "submitter": "Alexandre Maurer", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, L\\^e Nguy\\^en Hoang, Alexandre\n  Maurer", "title": "Removing Algorithmic Discrimination (With Minimal Individual Error)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of correcting group discriminations within a score\nfunction, while minimizing the individual error. Each group is described by a\nprobability density function on the set of profiles. We first solve the problem\nanalytically in the case of two populations, with a uniform bonus-malus on the\nzones where each population is a majority. We then address the general case of\nn populations, where the entanglement of populations does not allow a similar\nanalytical solution. We show that an approximate solution with an arbitrarily\nhigh level of precision can be computed with linear programming. Finally, we\naddress the inverse problem where the error should not go beyond a certain\nvalue and we seek to minimize the discrimination.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:49:46 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Hoang", "L\u00ea Nguy\u00ean", ""], ["Maurer", "Alexandre", ""]]}, {"id": "1806.02511", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan", "title": "Exact Low Tubal Rank Tensor Recovery from Gaussian Measurements", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proposed Tensor Nuclear Norm (TNN) [Lu et al., 2016; 2018a] is an\ninteresting convex penalty induced by the tensor SVD [Kilmer and Martin, 2011].\nIt plays a similar role as the matrix nuclear norm which is the convex\nsurrogate of the matrix rank. Considering that the TNN based Tensor Robust PCA\n[Lu et al., 2018a] is an elegant extension of Robust PCA with a similar tight\nrecovery bound, it is natural to solve other low rank tensor recovery problems\nextended from the matrix cases. However, the extensions and proofs are\ngenerally tedious. The general atomic norm provides a unified view of\nlow-complexity structures induced norms, e.g., the $\\ell_1$-norm and nuclear\nnorm. The sharp estimates of the required number of generic measurements for\nexact recovery based on the atomic norm are known in the literature. In this\nwork, with a careful choice of the atomic set, we prove that TNN is a special\natomic norm. Then by computing the Gaussian width of certain cone which is\nnecessary for the sharp estimate, we achieve a simple bound for guaranteed low\ntubal rank tensor recovery from Gaussian measurements. Specifically, we show\nthat by solving a TNN minimization problem, the underlying tensor of size\n$n_1\\times n_2\\times n_3$ with tubal rank $r$ can be exactly recovered when the\ngiven number of Gaussian measurements is $O(r(n_1+n_2-r)n_3)$. It is order\noptimal when comparing with the degrees of freedom $r(n_1+n_2-r)n_3$. Beyond\nthe Gaussian mapping, we also give the recovery guarantee of tensor completion\nbased on the uniform random mapping by TNN minimization. Numerical experiments\nverify our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 05:03:59 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Lu", "Canyi", ""], ["Feng", "Jiashi", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1806.02512", "submitter": "Maurice Diesendruck", "authors": "Maurice Diesendruck, Ethan R. Elenberg, Rajat Sen, Guy W. Cole, Sanjay\n  Shakkottai, Sinead A. Williamson", "title": "Importance Weighted Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative networks can simulate from a complex target distribution, by\nminimizing a loss with respect to samples from that distribution. However,\noften we do not have direct access to our target distribution - our data may be\nsubject to sample selection bias, or may be from a different but related\ndistribution. We present methods based on importance weighting that can\nestimate the loss with respect to a target distribution, even if we cannot\naccess that distribution directly, in a variety of settings. These estimators,\nwhich differentially weight the contribution of data to the loss function,\noffer both theoretical guarantees and impressive empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 05:16:53 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 01:09:43 GMT"}, {"version": "v3", "created": "Sun, 6 Sep 2020 19:36:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Diesendruck", "Maurice", ""], ["Elenberg", "Ethan R.", ""], ["Sen", "Rajat", ""], ["Cole", "Guy W.", ""], ["Shakkottai", "Sanjay", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1806.02538", "submitter": "Rogelio Andrade Mancisidor", "authors": "Rogelio Andrade Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert\n  Jenssen", "title": "Segment-Based Credit Scoring Using Latent Clusters in the Variational\n  Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying customer segments in retail banking portfolios with different\nrisk profiles can improve the accuracy of credit scoring. The Variational\nAutoencoder (VAE) has shown promising results in different research domains,\nand it has been documented the powerful information embedded in the latent\nspace of the VAE. We use the VAE and show that transforming the input data into\na meaningful representation, it is possible to steer configurations in the\nlatent space of the VAE. Specifically, the Weight of Evidence (WoE)\ntransformation encapsulates the propensity to fall into financial distress and\nthe latent space in the VAE preserves this characteristic in a well-defined\nclustering structure. These clusters have considerably different risk profiles\nand therefore are suitable not only for credit scoring but also for marketing\nand customer purposes. This new clustering methodology offers solutions to some\nof the challenges in the existing clustering algorithms, e.g., suggests the\nnumber of clusters, assigns cluster labels to new customers, enables cluster\nvisualization, scales to large datasets, captures non-linear relationships\namong others. Finally, for portfolios with a large number of customers in each\ncluster, developing one classifier model per cluster can improve the credit\nscoring assessment.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 07:19:44 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Mancisidor", "Rogelio Andrade", ""], ["Kampffmeyer", "Michael", ""], ["Aas", "Kjersti", ""], ["Jenssen", "Robert", ""]]}, {"id": "1806.02543", "submitter": "Astrid Dahl", "authors": "Astrid Dahl and Edwin V. Bonilla", "title": "Grouped Gaussian Processes for Solar Power Prediction", "comments": "15 pages, 4 figures. Replacement extends last version with further\n  experimental results and additional figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-task regression models where the observations are assumed\nto be a linear combination of several latent node functions and weight\nfunctions, which are both drawn from Gaussian process priors. Driven by the\nproblem of developing scalable methods for forecasting distributed solar and\nother renewable power generation, we propose coupled priors over groups of\n(node or weight) processes to exploit spatial dependence between functions. We\nestimate forecast models for solar power at multiple distributed sites and\nground wind speed at multiple proximate weather stations. Our results show that\nour approach maintains or improves point-prediction accuracy relative to\ncompeting solar benchmarks and improves over wind forecast benchmark models on\nall measures. Our approach consistently dominates the equivalent model without\ncoupled priors, achieving faster gains in forecast accuracy. At the same time\nour approach provides better quantification of predictive uncertainties.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 07:27:45 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 03:28:20 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 03:16:43 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Dahl", "Astrid", ""], ["Bonilla", "Edwin V.", ""]]}, {"id": "1806.02612", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah M. Erfani,\n  Shu-Tao Xia, Sudanthi Wijewickrema, James Bailey", "title": "Dimensionality-Driven Learning with Noisy Labels", "comments": "In Proceedings of the International Conference on Machine Learning\n  (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with significant proportions of noisy (incorrect) class labels\npresent challenges for training accurate Deep Neural Networks (DNNs). We\npropose a new perspective for understanding DNN generalization for such\ndatasets, by investigating the dimensionality of the deep representation\nsubspace of training samples. We show that from a dimensionality perspective,\nDNNs exhibit quite distinctive learning styles when trained with clean labels\nversus when trained with a proportion of noisy labels. Based on this finding,\nwe develop a new dimensionality-driven learning strategy, which monitors the\ndimensionality of subspaces during training and adapts the loss function\naccordingly. We empirically demonstrate that our approach is highly tolerant to\nsignificant proportions of noisy labels, and can effectively learn\nlow-dimensional local subspaces that capture the data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:11:13 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 14:54:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ma", "Xingjun", ""], ["Wang", "Yisen", ""], ["Houle", "Michael E.", ""], ["Zhou", "Shuo", ""], ["Erfani", "Sarah M.", ""], ["Xia", "Shu-Tao", ""], ["Wijewickrema", "Sudanthi", ""], ["Bailey", "James", ""]]}, {"id": "1806.02617", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli, \\c{C}a\\u{g}atay Y{\\i}ld{\\i}z, Thanh Huy Nguyen,\n  Ga\\\"el Richard, A. Taylan Cemgil", "title": "Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization", "comments": "Published in the International Conference on Machine Learning (ICML\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have illustrated that stochastic gradient Markov Chain Monte\nCarlo techniques have a strong potential in non-convex optimization, where\nlocal and global convergence guarantees can be shown under certain conditions.\nBy building up on this recent theory, in this study, we develop an\nasynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization.\nThe proposed algorithm is suitable for both distributed and shared-memory\nsettings. We provide formal theoretical analysis and show that the proposed\nmethod achieves an ergodic convergence rate of ${\\cal O}(1/\\sqrt{N})$ ($N$\nbeing the total number of iterations) and it can achieve a linear speedup under\ncertain conditions. We perform several experiments on both synthetic and real\ndatasets. The results support our theory and show that the proposed algorithm\nprovides a significant speedup over the recently proposed synchronous\ndistributed L-BFGS algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:20:10 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""], ["Y\u0131ld\u0131z", "\u00c7a\u011fatay", ""], ["Nguyen", "Thanh Huy", ""], ["Richard", "Ga\u00ebl", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1806.02639", "submitter": "Han Cai", "authors": "Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, Yong Yu", "title": "Path-Level Network Transformation for Efficient Architecture Search", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new function-preserving transformation for efficient neural\narchitecture search. This network transformation allows reusing previously\ntrained networks and existing successful architectures that improves sample\nefficiency. We aim to address the limitation of current network transformation\noperations that can only perform layer-level architecture modifications, such\nas adding (pruning) filters or inserting (removing) a layer, which fails to\nchange the topology of connection paths. Our proposed path-level transformation\noperations enable the meta-controller to modify the path topology of the given\nnetwork while keeping the merits of reusing weights, and thus allow efficiently\ndesigning effective structures with complex path topologies like Inception\nmodels. We further propose a bidirectional tree-structured reinforcement\nlearning meta-controller to explore a simple yet highly expressive\ntree-structured architecture space that can be viewed as a generalization of\nmulti-branch architectures. We experimented on the image classification\ndatasets with limited computational resources (about 200 GPU-hours), where we\nobserved improved parameter efficiency and better test results (97.70% test\naccuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet\nin the mobile setting), demonstrating the effectiveness and transferability of\nour designed architectures.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 12:25:05 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cai", "Han", ""], ["Yang", "Jiacheng", ""], ["Zhang", "Weinan", ""], ["Han", "Song", ""], ["Yu", "Yong", ""]]}, {"id": "1806.02643", "submitter": "David Balduzzi", "authors": "David Balduzzi, Karl Tuyls, Julien Perolat, Thore Graepel", "title": "Re-evaluating Evaluation", "comments": "NIPS 2018, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in machine learning is measured by careful evaluation on problems of\noutstanding common interest. However, the proliferation of benchmark suites and\nenvironments, adversarial attacks, and other complications has diluted the\nbasic evaluation model by overwhelming researchers with choices. Deliberate or\naccidental cherry picking is increasingly likely, and designing well-balanced\nevaluation suites requires increasing effort. In this paper we take a step back\nand propose Nash averaging. The approach builds on a detailed analysis of the\nalgebraic structure of evaluation in two basic scenarios: agent-vs-agent and\nagent-vs-task. The key strength of Nash averaging is that it automatically\nadapts to redundancies in evaluation data, so that results are not biased by\nthe incorporation of easy tasks or weak agents. Nash averaging thus encourages\nmaximally inclusive evaluation -- since there is no harm (computational cost\naside) from including all available tasks and agents.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 12:38:19 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 18:23:30 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Balduzzi", "David", ""], ["Tuyls", "Karl", ""], ["Perolat", "Julien", ""], ["Graepel", "Thore", ""]]}, {"id": "1806.02654", "submitter": "Laura-Mar\\'ia Cornejo-Bueno", "authors": "L. Cornejo-Bueno", "title": "New Hybrid Neuro-Evolutionary Algorithms for Renewable Energy and\n  Facilities Management Problems", "comments": "arXiv admin note: text overlap with arXiv:1706.03673,\n  arXiv:1805.03463 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This Ph.D. thesis deals with the optimization of several renewable energy\nresources development as well as the improvement of facilities management in\noceanic engineering and airports, using computational hybrid methods belonging\nto AI to this end. Energy is essential to our society in order to ensure a good\nquality of life. This means that predictions over the characteristics on which\nrenewable energies depend are necessary, in order to know the amount of energy\nthat will be obtained at any time. The second topic tackled in this thesis is\nrelated to the basic parameters that influence in different marine activities\nand airports, whose knowledge is necessary to develop a proper facilities\nmanagement in these environments. Within this work, a study of the\nstate-of-the-art Machine Learning have been performed to solve the problems\nassociated with the topics above-mentioned, and several contributions have been\nproposed: One of the pillars of this work is focused on the estimation of the\nmost important parameters in the exploitation of renewable resources. The\nsecond contribution of this thesis is related to feature selection problems.\nThe proposed methodologies are applied to multiple problems: the prediction of\n$H_s$, relevant for marine energy applications and marine activities, the\nestimation of WPREs, undesirable variations in the electric power produced by a\nwind farm, the prediction of global solar radiation in areas from Spain and\nAustralia, really important in terms of solar energy, and the prediction of\nlow-visibility events at airports. All of these practical issues are developed\nwith the consequent previous data analysis, normally, in terms of\nmeteorological variables.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:53:01 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cornejo-Bueno", "L.", ""]]}, {"id": "1806.02659", "submitter": "Martin Wistuba", "authors": "Martin Wistuba and Ambrish Rawat", "title": "Scalable Multi-Class Bayesian Support Vector Machines for Structured and\n  Unstructured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new Bayesian multi-class support vector machine by formulating\na pseudo-likelihood for a multi-class hinge loss in the form of a\nlocation-scale mixture of Gaussians. We derive a variational-inference-based\ntraining objective for gradient-based learning. Additionally, we employ an\ninducing point approximation which scales inference to large data sets.\nFurthermore, we develop hybrid Bayesian neural networks that combine standard\ndeep learning components with the proposed model to enable learning for\nunstructured data. We provide empirical evidence that our model outperforms the\ncompetitor methods with respect to both training time and accuracy in\nclassification experiments on 68 structured and two unstructured data sets.\nFinally, we highlight the key capability of our model in yielding prediction\nuncertainty for classification by demonstrating its effectiveness in the tasks\nof large-scale active learning and detection of adversarial images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:22:30 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Wistuba", "Martin", ""], ["Rawat", "Ambrish", ""]]}, {"id": "1806.02679", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Daniel C. Castro, Loic Le Folgoc, Ian Walker,\n  Ryutaro Tanno, Daniel Rueckert, Ben Glocker, Antonio Criminisi, Aditya Nori", "title": "Semi-Supervised Learning via Compact Latent Space Clustering", "comments": "Presented as a long oral in ICML 2018. Post-conference camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel cost function for semi-supervised learning of neural\nnetworks that encourages compact clustering of the latent space to facilitate\nseparation. The key idea is to dynamically create a graph over embeddings of\nlabeled and unlabeled samples of a training batch to capture underlying\nstructure in feature space, and use label propagation to estimate its high and\nlow density regions. We then devise a cost function based on Markov chains on\nthe graph that regularizes the latent space to form a single compact cluster\nper class, while avoiding to disturb existing clusters during optimization. We\nevaluate our approach on three benchmarks and compare to state-of-the art with\npromising results. Our approach combines the benefits of graph-based\nregularization with efficient, inductive inference, does not require\nmodifications to a network architecture, and can thus be easily applied to\nexisting networks to enable an effective use of unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:41:56 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 19:20:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Castro", "Daniel C.", ""], ["Folgoc", "Loic Le", ""], ["Walker", "Ian", ""], ["Tanno", "Ryutaro", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1806.02682", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas and Elena Garces", "title": "Transfer Learning for Illustration Classification", "comments": "9 pages, 8 figures, 4 tables", "journal-ref": "2017 Spanish Computer Graphics Conference (CEIG)", "doi": "10.2312/ceig.20171213", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of image classification has shown an outstanding success thanks to\nthe development of deep learning techniques. Despite the great performance\nobtained, most of the work has focused on natural images ignoring other domains\nlike artistic depictions. In this paper, we use transfer learning techniques to\npropose a new classification network with better performance in illustration\nimages. Starting from the deep convolutional network VGG19, pre-trained with\nnatural images, we propose two novel models which learn object representations\nin the new domain. Our optimized network will learn new low-level features of\nthe images (colours, edges, textures) while keeping the knowledge of the\nobjects and shapes that it already learned from the ImageNet dataset. Thus,\nrequiring much less data for the training. We propose a novel dataset of\nillustration images labelled by content where our optimized architecture\nachieves $\\textbf{86.61\\%}$ of top-1 and $\\textbf{97.21\\%}$ of top-5 precision.\nWe additionally demonstrate that our model is still able to recognize objects\nin photographs.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:06:16 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Lagunas", "Manuel", ""], ["Garces", "Elena", ""]]}, {"id": "1806.02714", "submitter": "Yemeserach Mekonnen", "authors": "Syed Rahman, Haneen Aburub, Yemeserach Mekonnen, and Arif I.Sarwat", "title": "A Study of EV BMS Cyber Security Based on Neural Network SOC Prediction", "comments": "5 pages, 13 figures Accepted to 2018 IEEE PES Transmission and\n  Distribution Conference & Exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent changes to greenhouse gas emission policies are catalyzing the\nelectric vehicle (EV) market making it readily accessible to consumers. While\nthere are challenges that arise with dense deployment of EVs, one of the major\nfuture concerns is cyber security threat. In this paper, cyber security threats\nin the form of tampering with EV battery's State of Charge (SOC) was explored.\nA Back Propagation (BP) Neural Network (NN) was trained and tested based on\nexperimental data to estimate SOC of battery under normal operation and\ncyber-attack scenarios. NeuralWare software was used to run scenarios.\nDifferent statistic metrics of the predicted values were compared against the\nactual values of the specific battery tested to measure the stability and\naccuracy of the proposed BP network under different operating conditions. The\nresults showed that BP NN was able to capture and detect the false entries due\nto a cyber-attack on its network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:50:44 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Rahman", "Syed", ""], ["Aburub", "Haneen", ""], ["Mekonnen", "Yemeserach", ""], ["Sarwat", "Arif I.", ""]]}, {"id": "1806.02775", "submitter": "Jun Han Mr", "authors": "Jun Han, Qiang Liu", "title": "Stein Variational Gradient Descent Without Gradient", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient decent (SVGD) has been shown to be a powerful\napproximate inference algorithm for complex distributions. However, the\nstandard SVGD requires calculating the gradient of the target density and\ncannot be applied when the gradient is unavailable. In this work, we develop a\ngradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with\na surrogate gradient, and corrects the induced bias by re-weighting the\ngradients in a proper form. We show that our GF-SVGD can be viewed as the\nstandard SVGD with a special choice of kernel, and hence directly inherits the\ntheoretical properties of SVGD. We shed insights on the empirical choice of the\nsurrogate gradient and propose an annealed GF-SVGD that leverages the idea of\nsimulated annealing to improve the performance on high dimensional complex\ndistributions. Empirical studies show that our method consistently outperforms\na number of recent advanced gradient-free MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:45:32 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Han", "Jun", ""], ["Liu", "Qiang", ""]]}, {"id": "1806.02782", "submitter": "Lei Xie", "authors": "Sining Sun, Ching-Feng Yeh, Mari Ostendorf, Mei-Yuh Hwang, Lei Xie", "title": "Training Augmentation with Adversarial Examples for Robust Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of adversarial examples in training speech\nrecognition systems to increase robustness of deep neural network acoustic\nmodels. During training, the fast gradient sign method is used to generate\nadversarial examples augmenting the original training data. Different from\nconventional data augmentation based on data transformations, the examples are\ndynamically generated based on current acoustic model parameters. We assess the\nimpact of adversarial data augmentation in experiments on the Aurora-4 and\nCHiME-4 single-channel tasks, showing improved robustness against noise and\nchannel variation. Further improvement is obtained when combining adversarial\nexamples with teacher/student training, leading to a 23% relative word error\nrate reduction on Aurora-4.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:53:12 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 04:09:26 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sun", "Sining", ""], ["Yeh", "Ching-Feng", ""], ["Ostendorf", "Mari", ""], ["Hwang", "Mei-Yuh", ""], ["Xie", "Lei", ""]]}, {"id": "1806.02794", "submitter": "Joseph Pfeiffer", "authors": "Elon Portugaly, Joseph J. Pfeiffer III", "title": "Unbiased Estimation of the Value of an Optimized Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials, also known as A/B tests, are used to select between two\npolicies: a control and a treatment. Given a corresponding set of features, we\ncan ideally learn an optimized policy P that maps the A/B test data features to\naction space and optimizes reward. However, although A/B testing provides an\nunbiased estimator for the value of deploying B (i.e., switching from policy A\nto B), direct application of those samples to learn the the optimized policy P\ngenerally does not provide an unbiased estimator of the value of P as the\nsamples were observed when constructing P. In situations where the cost and\nrisks associated of deploying a policy are high, such an unbiased estimator is\nhighly desirable.\n  We present a procedure for learning optimized policies and getting unbiased\nestimates for the value of deploying them. We wrap any policy learning\nprocedure with a bagging process and obtain out-of-bag policy inclusion\ndecisions for each sample. We then prove that inverse-propensity-weighting\neffect estimator is unbiased when applied to the optimized subset. Likewise, we\napply the same idea to obtain out-of-bag unbiased per-sample value estimate of\nthe measurement that is independent of the randomized treatment, and use these\nestimates to build an unbiased doubly-robust effect estimator. Lastly, we\nempirically shown that even when the average treatment effect is negative we\ncan find a positive optimized policy.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:12:50 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Portugaly", "Elon", ""], ["Pfeiffer", "Joseph J.", "III"]]}, {"id": "1806.02813", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach,\n  Pieter Abbeel, Sergey Levine", "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement\n  Learning with Trajectory Embeddings", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we take a representation learning perspective on hierarchical\nreinforcement learning, where the problem of learning lower layers in a\nhierarchy is transformed into the problem of learning trajectory-level\ngenerative models. We show that we can learn continuous latent representations\nof trajectories, which are effective in solving temporally extended and\nmulti-stage problems. Our proposed model, SeCTAR, draws inspiration from\nvariational autoencoders, and learns latent representations of trajectories. A\nkey component of this method is to learn both a latent-conditioned policy and a\nlatent-conditioned model which are consistent with each other. Given the same\nlatent, the policy generates a trajectory which should match the trajectory\npredicted by the model. This model provides a built-in prediction mechanism, by\npredicting the outcome of closed loop policy behavior. We propose a novel\nalgorithm for performing hierarchical RL with this model, combining model-based\nplanning in the learned latent space with an unsupervised exploration\nobjective. We show that our model is effective at reasoning over long horizons\nwith sparse rewards for several simulated tasks, outperforming standard\nreinforcement learning methods and prior methods for hierarchical reasoning,\nmodel-based planning, and exploration.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:49:08 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Liu", "YuXuan", ""], ["Gupta", "Abhishek", ""], ["Eysenbach", "Benjamin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.02815", "submitter": "Ehsan Kazemi", "authors": "Marko Mitrovic and Ehsan Kazemi and Morteza Zadimoghaddam and Amin\n  Karbasi", "title": "Data Summarization at Scale: A Two-Stage Submodular Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer scale of modern datasets has resulted in a dire need for\nsummarization techniques that identify representative elements in a dataset.\nFortunately, the vast majority of data summarization tasks satisfy an intuitive\ndiminishing returns condition known as submodularity, which allows us to find\nnearly-optimal solutions in linear time. We focus on a two-stage submodular\nframework where the goal is to use some given training functions to reduce the\nground set so that optimizing new functions (drawn from the same distribution)\nover the reduced set provides almost as much value as optimizing them over the\nentire ground set. In this paper, we develop the first streaming and\ndistributed solutions to this problem. In addition to providing strong\ntheoretical guarantees, we demonstrate both the utility and efficiency of our\nalgorithms on real-world tasks including image summarization and ride-share\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:50:24 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Mitrovic", "Marko", ""], ["Kazemi", "Ehsan", ""], ["Zadimoghaddam", "Morteza", ""], ["Karbasi", "Amin", ""]]}, {"id": "1806.02817", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Kelvin Xu, Sergey Levine", "title": "Probabilistic Model-Agnostic Meta-Learning", "comments": "NeurIPS 2018. First two authors contributed equally. Supplementary\n  results available at https://sites.google.com/view/probabilistic-maml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning for few-shot learning entails acquiring a prior over previous\ntasks and experiences, such that new tasks be learned from small amounts of\ndata. However, a critical challenge in few-shot learning is task ambiguity:\neven when a powerful prior can be meta-learned from a large number of prior\ntasks, a small dataset for a new task can simply be too ambiguous to acquire a\nsingle model (e.g., a classifier) for that task that is accurate. In this\npaper, we propose a probabilistic meta-learning algorithm that can sample\nmodels for a new task from a model distribution. Our approach extends\nmodel-agnostic meta-learning, which adapts to new tasks via gradient descent,\nto incorporate a parameter distribution that is trained via a variational lower\nbound. At meta-test time, our algorithm adapts via a simple procedure that\ninjects noise into gradient descent, and at meta-training time, the model is\ntrained such that this stochastic adaptation procedure produces samples from\nthe approximate model posterior. Our experimental results show that our method\ncan sample plausible classifiers and regressors in ambiguous few-shot learning\nproblems. We also show how reasoning about ambiguity can also be used for\ndownstream active learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:53:20 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 00:40:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Finn", "Chelsea", ""], ["Xu", "Kelvin", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.02825", "submitter": "Biplav Srivastava", "authors": "Ramashish Gaurav, Biplav Srivastava", "title": "Estimating Train Delays in a Large Rail Network Using a Zero Shot Markov\n  Model", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India runs the fourth largest railway transport network size carrying over 8\nbillion passengers per year. However, the travel experience of passengers is\nfrequently marked by delays, i.e., late arrival of trains at stations, causing\ninconvenience. In a first, we study the systemic delays in train arrivals using\nn-order Markov frameworks and experiment with two regression based models.\nUsing train running-status data collected for two years, we report on an\nefficient algorithm for estimating delays at railway stations with near\naccurate results. This work can help railways to manage their resources, while\nalso helping passengers and businesses served by them to efficiently plan their\nactivities.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 02:08:40 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gaurav", "Ramashish", ""], ["Srivastava", "Biplav", ""]]}, {"id": "1806.02855", "submitter": "Henri Palacci", "authors": "Henri Palacci, Henry Hess", "title": "Scalable Natural Gradient Langevin Dynamics in Practice", "comments": "ICML 2018 Workshop on Non-Convex Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for\nBayesian modeling adapted to large datasets and models. SGLD relies on the\ninjection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)\nupdate. In this scheme, every component in the noise vector is independent and\nhas the same scale, whereas the parameters we seek to estimate exhibit strong\nvariations in scale and significant correlation structures, leading to poor\nconvergence and mixing times. We compare different preconditioning approaches\nto the normalization of the noise vector and benchmark these approaches on the\nfollowing criteria: 1) mixing times of the multivariate parameter vector, 2)\nregularizing effect on small dataset where it is easy to overfit, 3) covariate\nshift detection and 4) resistance to adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:38:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Palacci", "Henri", ""], ["Hess", "Henry", ""]]}, {"id": "1806.02863", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Saurabh Sahu, Carol Espy-Wilson, Shrikanth Narayanan", "title": "Semi-supervised and Transfer learning approaches for low resource\n  sentiment classification", "comments": "5 pages, Accepted to International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification involves quantifying the affective reaction of a\nhuman to a document, media item or an event. Although researchers have\ninvestigated several methods to reliably infer sentiment from lexical, speech\nand body language cues, training a model with a small set of labeled datasets\nis still a challenge. For instance, in expanding sentiment analysis to new\nlanguages and cultures, it may not always be possible to obtain comprehensive\nlabeled datasets. In this paper, we investigate the application of\nsemi-supervised and transfer learning methods to improve performances on low\nresource sentiment classification tasks. We experiment with extracting dense\nfeature representations, pre-training and manifold regularization in enhancing\nthe performance of sentiment classification systems. Our goal is a coherent\nimplementation of these methods and we evaluate the gains achieved by these\nmethods in matched setting involving training and testing on a single corpus\nsetting as well as two cross corpora settings. In both the cases, our\nexperiments demonstrate that the proposed methods can significantly enhance the\nmodel performance against a purely supervised approach, particularly in cases\ninvolving a handful of training data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:59:26 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gupta", "Rahul", ""], ["Sahu", "Saurabh", ""], ["Espy-Wilson", "Carol", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1806.02865", "submitter": "Tiantian Liu", "authors": "Tiantian Liu and Yair Goldberg", "title": "Kernel Machines With Missing Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing responses is a missing data format in which outcomes are not always\nobserved. In this work we develop kernel machines that can handle missing\nresponses. First, we propose a kernel machine family that uses mainly the\ncomplete cases. For the quadratic loss, we then propose a family of\ndoubly-robust kernel machines. The proposed kernel-machine estimators can be\napplied to both regression and classification problems. We prove oracle\ninequalities for the finite-sample differences between the kernel machine risk\nand Bayes risk. We use these oracle inequalities to prove consistency and to\ncalculate convergence rates. We demonstrate the performance of the two proposed\nkernel machine families using both a simulation study and a real-world data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 19:02:18 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Liu", "Tiantian", ""], ["Goldberg", "Yair", ""]]}, {"id": "1806.02867", "submitter": "Guy Lorberbom", "authors": "Guy Lorberbom (Technion), Andreea Gane (MIT), Tommi Jaakkola (MIT),\n  Tamir Hazan (Technion)", "title": "Direct Optimization through $\\arg \\max$ for Discrete Variational\n  Auto-Encoder", "comments": "Accepted by Neural Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reparameterization of variational auto-encoders with continuous random\nvariables is an effective method for reducing the variance of their gradient\nestimates. In the discrete case, one can perform reparametrization using the\nGumbel-Max trick, but the resulting objective relies on an $\\arg \\max$\noperation and is non-differentiable. In contrast to previous works which resort\nto softmax-based relaxations, we propose to optimize it directly by applying\nthe direct loss minimization approach. Our proposal extends naturally to\nstructured discrete latent variable models when evaluating the $\\arg \\max$\noperation is tractable. We demonstrate empirically the effectiveness of the\ndirect loss minimization technique in variational autoencoders with both\nunstructured and structured discrete latent variables.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 19:09:21 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 17:07:53 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2019 19:34:43 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 13:49:37 GMT"}, {"version": "v5", "created": "Sun, 8 Dec 2019 08:59:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lorberbom", "Guy", "", "Technion"], ["Gane", "Andreea", "", "MIT"], ["Jaakkola", "Tommi", "", "MIT"], ["Hazan", "Tamir", "", "Technion"]]}, {"id": "1806.02878", "submitter": "Harini Suresh", "authors": "Harini Suresh, Jen J. Gong and John Guttag", "title": "Learning Tasks for Multitask Learning: Heterogenous Patient Populations\n  in the ICU", "comments": "KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219930", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches have been effective in predicting adverse\noutcomes in different clinical settings. These models are often developed and\nevaluated on datasets with heterogeneous patient populations. However, good\npredictive performance on the aggregate population does not imply good\nperformance for specific groups.\n  In this work, we present a two-step framework to 1) learn relevant patient\nsubgroups, and 2) predict an outcome for separate patient populations in a\nmulti-task framework, where each population is a separate task. We demonstrate\nhow to discover relevant groups in an unsupervised way with a\nsequence-to-sequence autoencoder. We show that using these groups in a\nmulti-task framework leads to better predictive performance of in-hospital\nmortality both across groups and overall. We also highlight the need for more\ngranular evaluation of performance when dealing with heterogeneous populations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 19:36:19 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Suresh", "Harini", ""], ["Gong", "Jen J.", ""], ["Guttag", "John", ""]]}, {"id": "1806.02887", "submitter": "Nathan Kallus", "authors": "Nathan Kallus and Angela Zhou", "title": "Residual Unfairness in Fair Machine Learning from Prejudiced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in fairness in machine learning has proposed adjusting for\nfairness by equalizing accuracy metrics across groups and has also studied how\ndatasets affected by historical prejudices may lead to unfair decision\npolicies. We connect these lines of work and study the residual unfairness that\narises when a fairness-adjusted predictor is not actually fair on the target\npopulation due to systematic censoring of training data by existing biased\npolicies. This scenario is particularly common in the same applications where\nfairness is a concern. We characterize theoretically the impact of such\ncensoring on standard fairness metrics for binary classifiers and provide\ncriteria for when residual unfairness may or may not appear. We prove that,\nunder certain conditions, fairness-adjusted classifiers will in fact induce\nresidual unfairness that perpetuates the same injustices, against the same\ngroups, that biased the data to begin with, thus showing that even\nstate-of-the-art fair machine learning can have a \"bias in, bias out\" property.\nWhen certain benchmark data is available, we show how sample reweighting can\nestimate and adjust fairness metrics while accounting for censoring. We use\nthis to study the case of Stop, Question, and Frisk (SQF) and demonstrate that\nattempting to adjust for fairness perpetuates the same injustices that the\npolicy is infamous for.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:16:42 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Kallus", "Nathan", ""], ["Zhou", "Angela", ""]]}, {"id": "1806.02892", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Mubarak Shah", "title": "Training Faster by Separating Modes of Variation in Batch-normalized\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is essential to effectively train state-of-the-art\ndeep Convolutional Neural Networks (CNN). It normalizes inputs to the layers\nduring training using the statistics of each mini-batch. In this work, we study\nBN from the viewpoint of Fisher kernels. We show that assuming samples within a\nmini-batch are from the same probability density function, then BN is identical\nto the Fisher vector of a Gaussian distribution. That means BN can be explained\nin terms of kernels that naturally emerge from the probability density function\nof the underlying data distribution. However, given the rectifying\nnon-linearities employed in CNN architectures, distribution of inputs to the\nlayers show heavy tail and asymmetric characteristics. Therefore, we propose\napproximating underlying data distribution not with one, but a mixture of\nGaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),\nreveals that BN can be improved by independently normalizing with respect to\nthe statistics of disentangled sub-populations. We refer to our proposed soft\npiecewise version of BN as Mixture Normalization (MN). Through extensive set of\nexperiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively\naccelerates training image classification and Generative Adversarial networks,\nbut also reaches higher quality models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:41:09 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 16:09:03 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Shah", "Mubarak", ""]]}, {"id": "1806.02901", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Andrew Gordon Wilson, Anima Anandkumar", "title": "Probabilistic FastText for Multi-Sense Word Embeddings", "comments": "Published at ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Probabilistic FastText, a new model for word embeddings that can\ncapture multiple word senses, sub-word structure, and uncertainty information.\nIn particular, we represent each word with a Gaussian mixture density, where\nthe mean of a mixture component is given by the sum of n-grams. This\nrepresentation allows the model to share statistical strength across sub-word\nstructures (e.g. Latin roots), producing accurate representations of rare,\nmisspelt, or even unseen words. Moreover, each component of the mixture can\ncapture a different word sense. Probabilistic FastText outperforms both\nFastText, which has no probabilistic model, and dictionary-level probabilistic\nembeddings, which do not incorporate subword structures, on several\nword-similarity benchmarks, including English RareWord and foreign language\ndatasets. We also achieve state-of-art performance on benchmarks that measure\nability to discern different meanings. Thus, the proposed model is the first to\nachieve multi-sense representations while having enriched semantics on rare\nwords.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:57:22 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Wilson", "Andrew Gordon", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1806.02920", "submitter": "Jinsung Yoon", "authors": "Jinsung Yoon, James Jordon, Mihaela van der Schaar", "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets", "comments": "10 pages, 3 figures, 2018 International Conference of Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for imputing missing data by adapting the\nwell-known Generative Adversarial Nets (GAN) framework. Accordingly, we call\nour method Generative Adversarial Imputation Nets (GAIN). The generator (G)\nobserves some components of a real data vector, imputes the missing components\nconditioned on what is actually observed, and outputs a completed vector. The\ndiscriminator (D) then takes a completed vector and attempts to determine which\ncomponents were actually observed and which were imputed. To ensure that D\nforces G to learn the desired distribution, we provide D with some additional\ninformation in the form of a hint vector. The hint reveals to D partial\ninformation about the missingness of the original sample, which is used by D to\nfocus its attention on the imputation quality of particular components. This\nhint ensures that G does in fact learn to generate according to the true data\ndistribution. We tested our method on various datasets and found that GAIN\nsignificantly outperforms state-of-the-art imputation methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 22:57:16 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Yoon", "Jinsung", ""], ["Jordon", "James", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1806.02922", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e L. Torrecilla and Alberto Su\\'arez", "title": "Feature selection in functional data classification with recursive\n  maxima hunting", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (pp. 4835-4843)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is one of the key issues in the design of effective\nmachine learning methods for automatic induction. In this work, we introduce\nrecursive maxima hunting (RMH) for variable selection in classification\nproblems with functional data. In this context, variable selection techniques\nare especially attractive because they reduce the dimensionality, facilitate\nthe interpretation and can improve the accuracy of the predictive models. The\nmethod, which is a recursive extension of maxima hunting (MH), performs\nvariable selection by identifying the maxima of a relevance function, which\nmeasures the strength of the correlation of the predictor functional variable\nwith the class label. At each stage, the information associated with the\nselected variable is removed by subtracting the conditional expectation of the\nprocess. The results of an extensive empirical evaluation are used to\nillustrate that, in the problems investigated, RMH has comparable or higher\npredictive accuracy than the standard dimensionality reduction techniques, such\nas PCA and PLS, and state-of-the-art feature selection methods for functional\ndata, such as maxima hunting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 23:21:29 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Torrecilla", "Jos\u00e9 L.", ""], ["Su\u00e1rez", "Alberto", ""]]}, {"id": "1806.02924", "submitter": "Arun Sai Suggala", "authors": "Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, Pradeep\n  Ravikumar", "title": "Revisiting Adversarial Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on adversarial perturbations show that there is an inherent\ntrade-off between standard test accuracy and adversarial accuracy.\nSpecifically, they show that no classifier can simultaneously be robust to\nadversarial perturbations and achieve high standard test accuracy. However,\nthis is contrary to the standard notion that on tasks such as image\nclassification, humans are robust classifiers with low error rate. In this\nwork, we show that the main reason behind this confusion is the inexact\ndefinition of adversarial perturbation that is used in the literature. To fix\nthis issue, we propose a slight, yet important modification to the existing\ndefinition of adversarial perturbation. Based on the modified definition, we\nshow that there is no trade-off between adversarial and standard accuracies;\nthere exist classifiers that are robust and achieve high standard accuracy. We\nfurther study several properties of this new definition of adversarial risk and\nits relation to the existing definition.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 23:27:16 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 21:39:19 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 20:38:47 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 01:47:52 GMT"}, {"version": "v5", "created": "Sat, 23 Mar 2019 03:06:32 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Suggala", "Arun Sai", ""], ["Prasad", "Adarsh", ""], ["Nagarajan", "Vaishnavh", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1806.02925", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "A Spectral Approach to Gradient Estimation for Implicit Distributions", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been increasing interests in learning and inference with\nimplicit distributions (i.e., distributions without tractable densities). To\nthis end, we develop a gradient estimator for implicit distributions based on\nStein's identity and a spectral decomposition of kernel operators, where the\neigenfunctions are approximated by the Nystr\\\"om method. Unlike the previous\nworks that only provide estimates at the sample points, our approach directly\nestimates the gradient function, thus allows for a simple and principled\nout-of-sample extension. We provide theoretical results on the error bound of\nthe estimator and discuss the bias-variance tradeoff in practice. The\neffectiveness of our method is demonstrated by applications to gradient-free\nHamiltonian Monte Carlo and variational inference with implicit distributions.\nFinally, we discuss the intuition behind the estimator by drawing connections\nbetween the Nystr\\\"om method and kernel PCA, which indicates that the estimator\ncan automatically adapt to the geometry of the underlying distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 23:30:41 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1806.02927", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and James T. Kwok", "title": "Lightweight Stochastic Optimization for Minimizing Finite Sums with\n  Infinite Data", "comments": "To appear in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction has been commonly used in stochastic optimization. It\nrelies crucially on the assumption that the data set is finite. However, when\nthe data are imputed with random noise as in data augmentation, the perturbed\ndata set be- comes essentially infinite. Recently, the stochastic MISO (S-MISO)\nalgorithm is introduced to address this expected risk minimization problem.\nThough it converges faster than SGD, a significant amount of memory is\nrequired. In this pa- per, we propose two SGD-like algorithms for expected risk\nminimization with random perturbation, namely, stochastic sample average\ngradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not\ndepend on the sample size, while that of S-SAGA is the same as those of\nvariance reduction methods on un- perturbed data. Theoretical analysis and\nexperimental results on logistic regression and AUC maximization show that SSAG\nhas faster convergence rate than SGD with comparable space requirement, while\nS-SAGA outperforms S-MISO in terms of both iteration complexity and storage.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 00:06:41 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zheng", "Shuai", ""], ["Kwok", "James T.", ""]]}, {"id": "1806.02934", "submitter": "Ashwin Vijayakumar", "authors": "Ashwin Kalyan, Stefan Lee, Anitha Kannan, Dhruv Batra", "title": "Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse\n  Annotations", "comments": "To be presented at ICML 2018; 10 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many structured prediction problems (particularly in vision and language\ndomains) are ambiguous, with multiple outputs being correct for an input - e.g.\nthere are many ways of describing an image, multiple ways of translating a\nsentence; however, exhaustively annotating the applicability of all possible\noutputs is intractable due to exponentially large output spaces (e.g. all\nEnglish sentences). In practice, these problems are cast as multi-class\nprediction, with the likelihood of only a sparse set of annotations being\nmaximized - unfortunately penalizing for placing beliefs on plausible but\nunannotated outputs. We make and test the following hypothesis - for a given\ninput, the annotations of its neighbors may serve as an additional supervisory\nsignal. Specifically, we propose an objective that transfers supervision from\nneighboring examples. We first study the properties of our developed method in\na controlled toy setup before reporting results on multi-label classification\nand two image-grounded sequence modeling tasks - captioning and question\ngeneration. We evaluate using standard task-specific metrics and measures of\noutput diversity, finding consistent improvements over standard maximum\nlikelihood training and other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:18:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Kalyan", "Ashwin", ""], ["Lee", "Stefan", ""], ["Kannan", "Anitha", ""], ["Batra", "Dhruv", ""]]}, {"id": "1806.02935", "submitter": "Kwangho Kim", "authors": "Kwangho Kim, Jisu Kim, Edward H. Kennedy", "title": "Causal effects based on distributional distances", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a framework for characterizing causal effects via\ndistributional distances. In particular we define a causal effect in terms of\nthe $L_1$ distance between different counterfactual outcome distributions,\nrather than the typical mean difference in outcome values. Comparing entire\ncounterfactual outcome distributions can provide more nuanced and valuable\nmeasures for exploring causal effects beyond the average treatment effect.\nFirst, we propose a novel way to estimate counterfactual outcome densities,\nwhich is of independent interest. Then we develop an efficient estimator of our\ntarget causal effect. We go on to provide error bounds and asymptotic\nproperties of the proposed estimator, along with bootstrap-based confidence\nintervals. Finally, we illustrate the methods via simulations and real data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:26:46 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 21:07:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kim", "Kwangho", ""], ["Kim", "Jisu", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1806.02942", "submitter": "Yu Li", "authors": "Yu Li, Zhongxiao Li, Lizhong Ding, Yijie Pan, Chao Huang, Yuhui Hu,\n  Wei Chen, Xin Gao", "title": "SupportNet: solving catastrophic forgetting in class incremental\n  learning with support data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plain well-trained deep learning model often does not have the ability to\nlearn new knowledge without forgetting the previously learned knowledge, which\nis known as catastrophic forgetting. Here we propose a novel method,\nSupportNet, to efficiently and effectively solve the catastrophic forgetting\nproblem in the class incremental learning scenario. SupportNet combines the\nstrength of deep learning and support vector machine (SVM), where SVM is used\nto identify the support data from the old data, which are fed to the deep\nlearning model together with the new data for further training so that the\nmodel can review the essential information of the old data when learning the\nnew information. Two powerful consolidation regularizers are applied to\nstabilize the learned representation and ensure the robustness of the learned\nmodel. We validate our method with comprehensive experiments on various tasks,\nwhich show that SupportNet drastically outperforms the state-of-the-art\nincremental learning methods and even reaches similar performance as the deep\nlearning model trained from scratch on both old and new data. Our program is\naccessible at: https://github.com/lykaust15/SupportNet\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:58:51 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 12:37:58 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 08:51:17 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Yu", ""], ["Li", "Zhongxiao", ""], ["Ding", "Lizhong", ""], ["Pan", "Yijie", ""], ["Huang", "Chao", ""], ["Hu", "Yuhui", ""], ["Chen", "Wei", ""], ["Gao", "Xin", ""]]}, {"id": "1806.02954", "submitter": "Jielong Yang", "authors": "Jielong Yang, Junshan Wang, and Wee Peng Tay", "title": "Using Social Network Information in Bayesian Truth Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of truth discovery based on opinions from multiple\nagents who may be unreliable or biased. We consider the case where agents'\nreliabilities or biases are correlated if they belong to the same community,\nwhich defines a group of agents with similar opinions regarding a particular\nevent. An agent can belong to different communities for different events, and\nthese communities are unknown a priori. We incorporate knowledge of the agents'\nsocial network in our truth discovery framework and develop Laplace variational\ninference methods to estimate agents' reliabilities, communities, and the event\nstates. We also develop a stochastic variational inference method to scale our\nmodel to large social networks. Simulations and experiments on real data\nsuggest that when observations are sparse, our proposed methods perform better\nthan several other inference methods, including majority voting, TruthFinder,\nAccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier\nCombination (BCC) method, and the Community BCC method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 02:58:37 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 07:38:49 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 13:55:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Jielong", ""], ["Wang", "Junshan", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1806.02957", "submitter": "Mohammad Amin Nabian", "authors": "Mohammad Amin Nabian, Hadi Meidani", "title": "A Deep Neural Network Surrogate for High-Dimensional Random Partial\n  Differential Equations", "comments": null, "journal-ref": "Probabilistic Engineering Mechanics, 57, pp.14-25 (2019)", "doi": "10.1016/j.probengmech.2019.05.001", "report-no": null, "categories": "cs.LG cs.NA cs.NE physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient numerical algorithms for the solution of high\ndimensional random Partial Differential Equations (PDEs) has been a challenging\ntask due to the well-known curse of dimensionality. We present a new solution\nframework for these problems based on a deep learning approach. Specifically,\nthe random PDE is approximated by a feed-forward fully-connected deep residual\nnetwork, with either strong or weak enforcement of initial and boundary\nconstraints. The framework is mesh-free, and can handle irregular computational\ndomains. Parameters of the approximating deep neural network are determined\niteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.\nThe satisfactory accuracy of the proposed frameworks is numerically\ndemonstrated on diffusion and heat conduction problems, in comparison with the\nconverged Monte Carlo-based finite element results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 03:24:50 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 23:59:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Nabian", "Mohammad Amin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1806.02958", "submitter": "Cyril Zhang", "authors": "Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh,\n  Cyril Zhang, Yi Zhang", "title": "Efficient Full-Matrix Adaptive Regularization", "comments": "Updated to ICML 2019 camera-ready version. Title of preprint was \"The\n  Case for Full-Matrix Adaptive Regularization\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive regularization methods pre-multiply a descent direction by a\npreconditioning matrix. Due to the large number of parameters of machine\nlearning problems, full-matrix preconditioning methods are prohibitively\nexpensive. We show how to modify full-matrix adaptive regularization in order\nto make it practical and effective. We also provide a novel theoretical\nanalysis for adaptive regularization in non-convex optimization settings. The\ncore of our algorithm, termed GGT, consists of the efficient computation of the\ninverse square root of a low-rank matrix. Our preliminary experiments show\nimproved iteration-wise convergence rates across synthetic tasks and standard\ndeep learning benchmarks, and that the more carefully-preconditioned steps\nsometimes lead to a better solution.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 03:31:05 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 20:47:18 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Agarwal", "Naman", ""], ["Bullins", "Brian", ""], ["Chen", "Xinyi", ""], ["Hazan", "Elad", ""], ["Singh", "Karan", ""], ["Zhang", "Cyril", ""], ["Zhang", "Yi", ""]]}, {"id": "1806.02970", "submitter": "Wenbo Ren", "authors": "Wenbo Ren, Jia Liu, Ness B. Shroff", "title": "PAC Ranking from Pairwise and Listwise Queries: Lower Bounds and Upper\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the adaptive (active) PAC (probably approximately\ncorrect) top-$k$ ranking (i.e., top-$k$ item selection) and total ranking\nproblems from $l$-wise ($l\\geq 2$) comparisons under the multinomial logit\n(MNL) model. By adaptively choosing sets to query and observing the noisy\noutput of the most favored item of each query, we want to design ranking\nalgorithms that recover the top-$k$ or total ranking using as few queries as\npossible. For the PAC top-$k$ ranking problem, we derive a lower bound on the\nsample complexity (aka number of queries), and propose an algorithm that is\nsample-complexity-optimal up to an $O(\\log(k+l)/\\log{k})$ factor. When $l=2$\n(i.e., pairwise comparisons) or $l=O(poly(k))$, this algorithm matches the\nlower bound. For the PAC total ranking problem, we derive a tight lower bound,\nand propose an algorithm that matches the lower bound. When $l=2$, the MNL\nmodel reduces to the popular Plackett-Luce (PL) model. In this setting, our\nresults still outperform the state-of-the-art both theoretically and\nnumerically. We also compare our algorithms with the state-of-the-art using\nsynthetic data as well as real-world data to verify the efficiency of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 05:04:34 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 01:08:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ren", "Wenbo", ""], ["Liu", "Jia", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1806.02977", "submitter": "Richard Nock", "authors": "Zac Cranko, Aditya Krishna Menon, Richard Nock, Cheng Soon Ong, Zhan\n  Shi, Christian Walder", "title": "Monge blunts Bayes: Hardness Results for Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have seen a staggering number of empirical studies of the\nrobustness of neural networks in a model of adversarial perturbations of their\ninputs. Most rely on an adversary which carries out local modifications within\nprescribed balls. None however has so far questioned the broader picture: how\nto frame a resource-bounded adversary so that it can be severely detrimental to\nlearning, a non-trivial problem which entails at a minimum the choice of loss\nand classifiers.\n  We suggest a formal answer for losses that satisfy the minimal statistical\nrequirement of being proper. We pin down a simple sufficient property for any\ngiven class of adversaries to be detrimental to learning, involving a central\nmeasure of \"harmfulness\" which generalizes the well-known class of integral\nprobability metrics. A key feature of our result is that it holds for all\nproper losses, and for a popular subset of these, the optimisation of this\ncentral measure appears to be independent of the loss. When classifiers are\nLipschitz -- a now popular approach in adversarial training --, this\noptimisation resorts to optimal transport to make a low-budget compression of\nclass marginals. Toy experiments reveal a finding recently separately observed:\ntraining against a sufficiently budgeted adversary of this kind improves\ngeneralization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:00:08 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 22:16:39 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 21:49:08 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 23:59:29 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Cranko", "Zac", ""], ["Menon", "Aditya Krishna", ""], ["Nock", "Richard", ""], ["Ong", "Cheng Soon", ""], ["Shi", "Zhan", ""], ["Walder", "Christian", ""]]}, {"id": "1806.02978", "submitter": "Zhe Gan", "authors": "Yunchen Pu, Shuyang Dai, Zhe Gan, Weiyao Wang, Guoyin Wang, Yizhe\n  Zhang, Ricardo Henao, Lawrence Carin", "title": "JointGAN: Multi-Domain Joint Distribution Learning with Generative\n  Adversarial Nets", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generative adversarial network is developed for joint distribution\nmatching. Distinct from most existing approaches, that only learn conditional\ndistributions, the proposed model aims to learn a joint distribution of\nmultiple random variables (domains). This is achieved by learning to sample\nfrom conditional distributions between the domains, while simultaneously\nlearning to sample from the marginals of each individual domain. The proposed\nframework consists of multiple generators and a single softmax-based critic,\nall jointly trained via adversarial learning. From a simple noise source, the\nproposed framework allows synthesis of draws from the marginals, conditional\ndraws given observations from a subset of random variables, or complete draws\nfrom the full joint distribution. Most examples considered are for joint\nanalysis of two domains, with examples for three domains also presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:01:34 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Pu", "Yunchen", ""], ["Dai", "Shuyang", ""], ["Gan", "Zhe", ""], ["Wang", "Weiyao", ""], ["Wang", "Guoyin", ""], ["Zhang", "Yizhe", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1806.02988", "submitter": "Zhuohan Li", "authors": "Zhuohan Li, Di He, Fei Tian, Wei Chen, Tao Qin, Liwei Wang, Tie-Yan\n  Liu", "title": "Towards Binary-Valued Gates for Robust LSTM Training", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is one of the most widely used recurrent\nstructures in sequence modeling. It aims to use gates to control information\nflow (e.g., whether to skip some information or not) in the recurrent\ncomputations, although its practical implementation based on soft gates only\npartially achieves this goal. In this paper, we propose a new way for LSTM\ntraining, which pushes the output values of the gates towards 0 or 1. By doing\nso, we can better control the information flow: the gates are mostly open or\nclosed, instead of in a middle state, which makes the results more\ninterpretable. Empirical studies show that (1) Although it seems that we\nrestrict the model capacity, there is no performance drop: we achieve better or\ncomparable performances due to its better generalization ability; (2) The\noutputs of gates are not sensitive to their inputs: we can easily compress the\nLSTM unit in multiple ways, e.g., low-rank approximation and low-precision\napproximation. The compressed models are even better than the baseline models\nwithout compression.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:57:16 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Li", "Zhuohan", ""], ["He", "Di", ""], ["Tian", "Fei", ""], ["Chen", "Wei", ""], ["Qin", "Tao", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1806.02997", "submitter": "Aleksei Vasilev", "authors": "Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora\n  Sgarlata, Valentina Tomassini, Derek K. Jones, Daniel Cremers", "title": "q-Space Novelty Detection with Variational Autoencoders", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:28:36 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 17:34:51 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Vasilev", "Aleksei", ""], ["Golkov", "Vladimir", ""], ["Meissner", "Marc", ""], ["Lipp", "Ilona", ""], ["Sgarlata", "Eleonora", ""], ["Tomassini", "Valentina", ""], ["Jones", "Derek K.", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.03000", "submitter": "Taegyun Jeon", "authors": "Junghoon Seo, Jeongyeol Choe, Jamyoung Koo, Seunghyeon Jeon, Beomsu\n  Kim, Taegyun Jeon", "title": "Noise-adding Methods of Saliency Map as Series of Higher Order Partial\n  Derivative", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SmoothGrad and VarGrad are techniques that enhance the empirical quality of\nstandard saliency maps by adding noise to input. However, there were few works\nthat provide a rigorous theoretical interpretation of those methods. We\nanalytically formalize the result of these noise-adding methods. As a result,\nwe observe two interesting results from the existing noise-adding methods.\nFirst, SmoothGrad does not make the gradient of the score function smooth.\nSecond, VarGrad is independent of the gradient of the score function. We\nbelieve that our findings provide a clue to reveal the relationship between\nlocal explanation methods of deep neural networks and higher-order partial\nderivatives of the score function.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:41:41 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Seo", "Junghoon", ""], ["Choe", "Jeongyeol", ""], ["Koo", "Jamyoung", ""], ["Jeon", "Seunghyeon", ""], ["Kim", "Beomsu", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1806.03044", "submitter": "Andriy Temko Dr", "authors": "Alison O'Shea, Gordon Lightbody, Geraldine Boylan and Andriy Temko", "title": "Investigating the Impact of CNN Depth on Neonatal Seizure Detection\n  Performance", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel, deep, fully convolutional architecture which is\noptimized for the task of EEG-based neonatal seizure detection. Architectures\nof different depths were designed and tested; varying network depth impacts\nconvolutional receptive fields and the corresponding learned feature\ncomplexity. Two deep convolutional networks are compared with a shallow\nSVM-based neonatal seizure detector, which relies on the extraction of\nhand-crafted features. On a large clinical dataset, of over 800 hours of\nmultichannel unedited EEG, containing 1389 seizure events, the deep 11-layer\narchitecture significantly outperforms the shallower architectures, improving\nthe AUC90 from 82.6% to 86.8%. Combining the end-to-end deep architecture with\nthe feature-based shallow SVM further improves the AUC90 to 87.6%. The fusion\nof classifiers of different depths gives greatly improved performance and\nreduced variability, making the combined classifier more clinically reliable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:34:22 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["O'Shea", "Alison", ""], ["Lightbody", "Gordon", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.03085", "submitter": "Gianluca Detommaso", "authors": "Gianluca Detommaso, Tiangang Cui, Alessio Spantini, Youssef Marzouk\n  and Robert Scheichl", "title": "A Stein variational Newton method", "comments": "18 pages, 7 figures", "journal-ref": "NIPS 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient descent (SVGD) was recently proposed as a general\npurpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]:\nit minimizes the Kullback-Leibler divergence between the target distribution\nand its approximation by implementing a form of functional gradient descent on\na reproducing kernel Hilbert space. In this paper, we accelerate and generalize\nthe SVGD algorithm by including second-order information, thereby approximating\na Newton-like iteration in function space. We also show how second-order\ninformation can lead to more effective choices of kernel. We observe\nsignificant computational gains over the original SVGD algorithm in multiple\ntest cases.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 11:05:29 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 22:11:26 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Detommaso", "Gianluca", ""], ["Cui", "Tiangang", ""], ["Spantini", "Alessio", ""], ["Marzouk", "Youssef", ""], ["Scheichl", "Robert", ""]]}, {"id": "1806.03107", "submitter": "Karol Gregor", "authors": "Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing,\n  Theophane Weber", "title": "Temporal Difference Variational Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To act and plan in complex environments, we posit that agents should have a\nmental simulator of the world with three characteristics: (a) it should build\nan abstract state representing the condition of the world; (b) it should form a\nbelief which represents uncertainty on the world; (c) it should go beyond\nsimple step-by-step simulation, and exhibit temporal abstraction. Motivated by\nthe absence of a model satisfying all these requirements, we propose TD-VAE, a\ngenerative sequence model that learns representations containing explicit\nbeliefs about states several steps into the future, and that can be rolled out\ndirectly without single-step transitions. TD-VAE is trained on pairs of\ntemporally separated time points, using an analogue of temporal difference\nlearning used in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:10:58 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 19:38:12 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 16:53:13 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gregor", "Karol", ""], ["Papamakarios", "George", ""], ["Besse", "Frederic", ""], ["Buesing", "Lars", ""], ["Weber", "Theophane", ""]]}, {"id": "1806.03121", "submitter": "Kieran Bull", "authors": "Kieran Bull, Yang-Hui He, Vishnu Jejjala, Challenger Mishra", "title": "Machine Learning CICY Threefolds", "comments": "22 pages, 10 figures. V2: Added new results of using permutations of\n  the CICY matrix to reduce the class imbalance when predicting discrete\n  symmetries. V3: Corrected typing errors in table 5", "journal-ref": null, "doi": "10.1016/j.physletb.2018.08.008", "report-no": null, "categories": "hep-th hep-ph math.AG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest techniques from Neural Networks and Support Vector Machines (SVM)\nare used to investigate geometric properties of Complete Intersection\nCalabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model\nbuilding. An advanced neural network classifier and SVM are employed to (1)\nlearn Hodge numbers and report a remarkable improvement over previous efforts,\n(2) query for favourability, and (3) predict discrete symmetries, a highly\nimbalanced problem to which both Synthetic Minority Oversampling Technique\n(SMOTE) and permutations of the CICY matrix are used to decrease the class\nimbalance and improve performance. In each case study, we employ a genetic\nalgorithm to optimise the hyperparameters of the neural network. We demonstrate\nthat our approach provides quick diagnostic tools capable of shortlisting\nquasi-realistic string models based on compactification over smooth CICYs and\nfurther supports the paradigm that classes of problems in algebraic geometry\ncan be machine learned.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:40:04 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 14:00:12 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 11:58:24 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Bull", "Kieran", ""], ["He", "Yang-Hui", ""], ["Jejjala", "Vishnu", ""], ["Mishra", "Challenger", ""]]}, {"id": "1806.03125", "submitter": "Erica Kido Shimomoto", "authors": "Erica K. Shimomoto, Lincon S. Souza, Bernardo B. Gatto, Kazuhiro Fukui", "title": "Text Classification based on Word Subspace with Term-Frequency", "comments": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification has become indispensable due to the rapid increase of\ntext in digital form. Over the past three decades, efforts have been made to\napproach this task using various learning algorithms and statistical models\nbased on bag-of-words (BOW) features. Despite its simple implementation, BOW\nfeatures lack semantic meaning representation. To solve this problem, neural\nnetworks started to be employed to learn word vectors, such as the word2vec.\nWord2vec embeds word semantic structure into vectors, where the angle between\nvectors indicates the meaningful similarity between words. To measure the\nsimilarity between texts, we propose the novel concept of word subspace, which\ncan represent the intrinsic variability of features in a set of word vectors.\nThrough this concept, it is possible to model text from word vectors while\nholding semantic information. To incorporate the word frequency directly in the\nsubspace model, we further extend the word subspace to the term-frequency (TF)\nweighted word subspace. Based on these new concepts, text classification can be\nperformed under the mutual subspace method (MSM) framework. The validity of our\nmodeling is shown through experiments on the Reuters text database, comparing\nthe results to various state-of-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:55:37 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shimomoto", "Erica K.", ""], ["Souza", "Lincon S.", ""], ["Gatto", "Bernardo B.", ""], ["Fukui", "Kazuhiro", ""]]}, {"id": "1806.03143", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Yixin Wang, David M. Blei, Raul Rabadan", "title": "Black Box FDR", "comments": "To appear at ICML'18; code available at\n  https://github.com/tansey/bb-fdr", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large-scale, multi-experiment studies requires scientists to test\neach experimental outcome for statistical significance and then assess the\nresults as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes\nmethod for analyzing multi-experiment studies when many covariates are gathered\nper experiment. BB-FDR learns a series of black box predictive models to boost\npower and control the false discovery rate (FDR) at two stages of study\nanalysis. In Stage 1, it uses a deep neural network prior to report which\nexperiments yielded significant outcomes. In Stage 2, a separate black box\nmodel of each covariate is used to select features that have significant\npredictive power across all experiments. In benchmarks, BB-FDR outperforms\ncompeting state-of-the-art methods in both stages of analysis. We apply BB-FDR\nto two real studies on cancer drug efficacy. For both studies, BB-FDR increases\nthe proportion of significant outcomes discovered and selects variables that\nreveal key genomic drivers of drug sensitivity and resistance in cancer.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:29:08 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Tansey", "Wesley", ""], ["Wang", "Yixin", ""], ["Blei", "David M.", ""], ["Rabadan", "Raul", ""]]}, {"id": "1806.03145", "submitter": "Daoyi Dong", "authors": "Chunlin Chen, Daoyi Dong, Han-Xiong Li, Jian Chu, Tzyh-Jong Tarn", "title": "Fidelity-based Probabilistic Q-learning for Control of Quantum Systems", "comments": "13 pages, 16 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, VOL.\n  25, NO. 5, pp.920-933, MAY 2014", "doi": "10.1109/TNNLS.2013.2283574", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance between exploration and exploitation is a key problem for\nreinforcement learning methods, especially for Q-learning. In this paper, a\nfidelity-based probabilistic Q-learning (FPQL) approach is presented to\nnaturally solve this problem and applied for learning control of quantum\nsystems. In this approach, fidelity is adopted to help direct the learning\nprocess and the probability of each action to be selected at a certain state is\nupdated iteratively along with the learning process, which leads to a natural\nexploration strategy instead of a pointed one with configured parameters. A\nprobabilistic Q-learning (PQL) algorithm is first presented to demonstrate the\nbasic idea of probabilistic action selection. Then the FPQL algorithm is\npresented for learning control of quantum systems. Two examples (a spin- 1/2\nsystem and a lamda-type atomic system) are demonstrated to test the performance\nof the FPQL algorithm. The results show that FPQL algorithms attain a better\nbalance between exploration and exploitation, and can also avoid local optimal\npolicies and accelerate the learning process.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:30:48 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chen", "Chunlin", ""], ["Dong", "Daoyi", ""], ["Li", "Han-Xiong", ""], ["Chu", "Jian", ""], ["Tarn", "Tzyh-Jong", ""]]}, {"id": "1806.03146", "submitter": "Peter Bj{\\o}rn J{\\o}rgensen", "authors": "Peter Bj{\\o}rn J{\\o}rgensen, Karsten Wedel Jacobsen and Mikkel N.\n  Schmidt", "title": "Neural Message Passing with Edge Updates for Predicting Properties of\n  Molecules and Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural message passing on molecular graphs is one of the most promising\nmethods for predicting formation energy and other properties of molecules and\nmaterials. In this work we extend the neural message passing model with an edge\nupdate network which allows the information exchanged between atoms to depend\non the hidden state of the receiving atom. We benchmark the proposed model on\nthree publicly available datasets (QM9, The Materials Project and OQMD) and\nshow that the proposed model yields superior prediction of formation energies\nand other properties on all three datasets in comparison with the best\npublished results. Furthermore we investigate different methods for\nconstructing the graph used to represent crystalline structures and we find\nthat using a graph based on K-nearest neighbors achieves better prediction\naccuracy than using maximum distance cutoff or the Voronoi tessellation graph.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:35:37 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["J\u00f8rgensen", "Peter Bj\u00f8rn", ""], ["Jacobsen", "Karsten Wedel", ""], ["Schmidt", "Mikkel N.", ""]]}, {"id": "1806.03168", "submitter": "Lei Huang", "authors": "Lei Huang, Guangjie Ren, Shun Jiang, Raphael Arar, Eric Young Liu", "title": "Data-driven Analytics for Business Architectures: Proposed Use of Graph\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Architecture (BA) plays a significant role in helping organizations\nunderstand enterprise structures and processes, and align them with strategic\nobjectives. However, traditional BAs are represented in fixed structure with\nstatic model elements and fail to dynamically capture business insights based\non internal and external data. To solve this problem, this paper introduces the\ngraph theory into BAs with aim of building extensible data-driven analytics and\nautomatically generating business insights. We use IBM's Component Business\nModel (CBM) as an example to illustrate various ways in which graph theory can\nbe leveraged for data-driven analytics, including what and how business\ninsights can be obtained. Future directions for applying graph theory to\nbusiness architecture analytics are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 06:25:25 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Huang", "Lei", ""], ["Ren", "Guangjie", ""], ["Jiang", "Shun", ""], ["Arar", "Raphael", ""], ["Liu", "Eric Young", ""]]}, {"id": "1806.03182", "submitter": "Yujie Zhang", "authors": "Yujie Zhang and Wenjing Ye", "title": "Deep learning based inverse method for layout design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout design with complex constraints is a challenging problem to solve due\nto the non-uniqueness of the solution and the difficulties in incorporating the\nconstraints into the conventional optimization-based methods. In this paper, we\npropose a design method based on the recently developed machine learning\ntechnique, Variational Autoencoder (VAE). We utilize the learning capability of\nthe VAE to learn the constraints and the generative capability of the VAE to\ngenerate design candidates that automatically satisfy all the constraints. As\nsuch, no constraints need to be imposed during the design stage. In addition,\nwe show that the VAE network is also capable of learning the underlying physics\nof the design problem, leading to an efficient design tool that does not need\nany physical simulation once the network is constructed. We demonstrated the\nperformance of the method on two cases: inverse design of surface diffusion\ninduced morphology change and mask design for optical microlithography.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 01:55:06 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhang", "Yujie", ""], ["Ye", "Wenjing", ""]]}, {"id": "1806.03185", "submitter": "Daniel Stoller", "authors": "Daniel Stoller, Sebastian Ewert, Simon Dixon", "title": "Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source\n  Separation", "comments": "7 pages (1 for references), 4 figures, 3 tables. Appearing in the\n  proceedings of the 19th International Society for Music Information Retrieval\n  Conference (ISMIR 2018) (camera-ready version). Implementation available at\n  https://github.com/f90/Wave-U-Net", "journal-ref": "19th International Society for Music Information Retrieval\n  Conference (ISMIR 2018)", "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for audio source separation usually operate on the magnitude spectrum,\nwhich ignores phase information and makes separation performance dependant on\nhyper-parameters for the spectral front-end. Therefore, we investigate\nend-to-end source separation in the time-domain, which allows modelling phase\ninformation and avoids fixed spectral transformations. Due to high sampling\nrates for audio, employing a long temporal input context on the sample level is\ndifficult, but required for high quality separation results because of\nlong-range temporal correlations. In this context, we propose the Wave-U-Net,\nan adaptation of the U-Net to the one-dimensional time domain, which repeatedly\nresamples feature maps to compute and combine features at different time\nscales. We introduce further architectural improvements, including an output\nlayer that enforces source additivity, an upsampling technique and a\ncontext-aware prediction framework to reduce output artifacts. Experiments for\nsinging voice separation indicate that our architecture yields a performance\ncomparable to a state-of-the-art spectrogram-based U-Net architecture, given\nthe same data. Finally, we reveal a problem with outliers in the currently used\nSDR evaluation metrics and suggest reporting rank-based statistics to alleviate\nthis problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:29:08 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Stoller", "Daniel", ""], ["Ewert", "Sebastian", ""], ["Dixon", "Simon", ""]]}, {"id": "1806.03190", "submitter": "Yuanzhi Li", "authors": "Yuanzhi Li, Yoram Singer", "title": "The Well Tempered Lasso", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the entire regularization path for least squares\nregression with 1-norm penalty, known as the Lasso. Every regression parameter\nin the Lasso changes linearly as a function of the regularization value. The\nnumber of changes is regarded as the Lasso's complexity. Experimental results\nusing exact path following exhibit polynomial complexity of the Lasso in the\nproblem size. Alas, the path complexity of the Lasso on artificially designed\nregression problems is exponential.\n  We use smoothed analysis as a mechanism for bridging the gap between worst\ncase settings and the de facto low complexity. Our analysis assumes that the\nobserved data has a tiny amount of intrinsic noise. We then prove that the\nLasso's complexity is polynomial in the problem size. While building upon the\nseminal work of Spielman and Teng on smoothed complexity, our analysis is\nmorally different as it is divorced from specific path following algorithms. We\nverify the validity of our analysis in experiments with both worst case\nsettings and real datasets. The empirical results we obtain closely match our\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:34:26 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Li", "Yuanzhi", ""], ["Singer", "Yoram", ""]]}, {"id": "1806.03198", "submitter": "Alexandre Sablayrolles", "authors": "Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\\'e\n  J\\'egou", "title": "Spreading vectors for similarity search", "comments": "Published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discretizing multi-dimensional data distributions is a fundamental step of\nmodern indexing methods. State-of-the-art techniques learn parameters of\nquantizers on training data for optimal performance, thus adapting quantizers\nto the data. In this work, we propose to reverse this paradigm and adapt the\ndata to the quantizer: we train a neural net which last layer forms a fixed\nparameter-free quantizer, such as pre-defined points of a hyper-sphere. As a\nproxy objective, we design and train a neural network that favors uniformity in\nthe spherical latent space, while preserving the neighborhood structure after\nthe mapping. We propose a new regularizer derived from the Kozachenko--Leonenko\ndifferential entropy estimator to enforce uniformity and combine it with a\nlocality-aware triplet loss. Experiments show that our end-to-end approach\noutperforms most learned quantization methods, and is competitive with the\nstate of the art on widely adopted benchmarks. Furthermore, we show that\ntraining without the quantization step results in almost no difference in\naccuracy, but yields a generic catalyzer that can be applied with any\nsubsequent quantizer.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:46:22 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 16:21:19 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 12:54:38 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Sablayrolles", "Alexandre", ""], ["Douze", "Matthijs", ""], ["Schmid", "Cordelia", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1806.03207", "submitter": "Daniel Sheldon", "authors": "Daniel Sheldon and Kevin Winner and Debora Sujono", "title": "Learning in Integer Latent Variable Models with Nested Automatic\n  Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop nested automatic differentiation (AD) algorithms for exact\ninference and learning in integer latent variable models. Recently, Winner,\nSujono, and Sheldon showed how to reduce marginalization in a class of integer\nlatent variable models to evaluating a probability generating function which\ncontains many levels of nested high-order derivatives. We contribute faster and\nmore stable AD algorithms for this challenging problem and a novel algorithm to\ncompute exact gradients for learning. These contributions lead to significantly\nfaster and more accurate learning algorithms, and are the first AD algorithms\nwhose running time is polynomial in the number of levels of nesting.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:01:11 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sheldon", "Daniel", ""], ["Winner", "Kevin", ""], ["Sujono", "Debora", ""]]}, {"id": "1806.03218", "submitter": "Alexey Zaytsev", "authors": "Nikita Klyuchnikov, Alexey Zaytsev, Arseniy Gruzdev, Georgiy\n  Ovchinnikov, Ksenia Antipova, Leyla Ismailova, Ekaterina Muravleva, Evgeny\n  Burnaev, Artyom Semenikhin, Alexey Cherepanov, Vitaliy Koryabkin, Igor Simon,\n  Alexey Tsurgan, Fedor Krasnov, Dmitry Koroteev", "title": "Data-driven model for the identification of the rock type at a drilling\n  bit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional oil well drilling requires high precision of the wellbore\npositioning inside the productive area. However, due to specifics of\nengineering design, sensors that explicitly determine the type of the drilled\nrock are located farther than 15m from the drilling bit. As a result, the\ntarget area runaways can be detected only after this distance, which in turn,\nleads to a loss in well productivity and the risk of the need for an expensive\nre-boring operation.\n  We present a novel approach for identifying rock type at the drilling bit\nbased on machine learning classification methods and data mining on sensors\nreadings. We compare various machine-learning algorithms, examine extra\nfeatures coming from mathematical modeling of drilling mechanics, and show that\nthe real-time rock type classification error can be reduced from 13.5 % to 9 %.\nThe approach is applicable for precise directional drilling in relatively thin\ntarget intervals of complex shapes and generalizes appropriately to new wells\nthat are different from the ones used for training the machine learning model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:28:54 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 15:26:39 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 11:54:38 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Klyuchnikov", "Nikita", ""], ["Zaytsev", "Alexey", ""], ["Gruzdev", "Arseniy", ""], ["Ovchinnikov", "Georgiy", ""], ["Antipova", "Ksenia", ""], ["Ismailova", "Leyla", ""], ["Muravleva", "Ekaterina", ""], ["Burnaev", "Evgeny", ""], ["Semenikhin", "Artyom", ""], ["Cherepanov", "Alexey", ""], ["Koryabkin", "Vitaliy", ""], ["Simon", "Igor", ""], ["Tsurgan", "Alexey", ""], ["Krasnov", "Fedor", ""], ["Koroteev", "Dmitry", ""]]}, {"id": "1806.03232", "submitter": "Marco Saerens Marco", "authors": "Guillaume Guex, Ilkka Kivim\\\"aki and Marco Saerens", "title": "Randomized Optimal Transport on a Graph: framework and new distance\n  measures", "comments": "Preprint paper to appear in Network Science journal, Cambridge\n  University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently developed bag-of-paths (BoP) framework consists in setting a\nGibbs-Boltzmann distribution on all feasible paths of a graph. This probability\ndistribution favors short paths over long ones, with a free parameter (the\ntemperature $T$) controlling the entropic level of the distribution. This\nformalism enables the computation of new distances or dissimilarities,\ninterpolating between the shortest-path and the resistance distance, which have\nbeen shown to perform well in clustering and classification tasks. In this\nwork, the bag-of-paths formalism is extended by adding two independent equality\nconstraints fixing starting and ending nodes distributions of paths (margins).\nWhen the temperature is low, this formalism is shown to be equivalent to a\nrelaxation of the optimal transport problem on a network where paths carry a\nflow between two discrete distributions on nodes. The randomization is achieved\nby considering free energy minimization instead of traditional cost\nminimization. Algorithms computing the optimal free energy solution are\ndeveloped for two types of paths: hitting (or absorbing) paths and non-hitting,\nregular, paths, and require the inversion of an $n \\times n$ matrix with $n$\nbeing the number of nodes. Interestingly, for regular paths on an undirected\ngraph, the resulting optimal policy interpolates between the deterministic\noptimal transport policy ($T \\rightarrow 0^{+}$) and the solution to the\ncorresponding electrical circuit ($T \\rightarrow \\infty$). Two distance\nmeasures between nodes and a dissimilarity between groups of nodes, both\nintegrating weights on nodes, are derived from this framework.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:42:09 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:05:04 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Guex", "Guillaume", ""], ["Kivim\u00e4ki", "Ilkka", ""], ["Saerens", "Marco", ""]]}, {"id": "1806.03240", "submitter": "Radek Pel\\'anek", "authors": "Radek Pel\\'anek and Tom\\'a\\v{s} Effenberger and Mat\\v{e}j Van\\v{e}k\n  and Vojt\\v{e}ch Sassmann and Dominik Gmiterko", "title": "Measuring Item Similarity in Introductory Programming: Python and Robot\n  Programming Case Studies", "comments": "Full version of the L@S'18 paper \"Measuring Item Similarity in\n  Introductory Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A personalized learning system needs a large pool of items for learners to\nsolve. When working with a large pool of items, it is useful to measure the\nsimilarity of items. We outline a general approach to measuring the similarity\nof items and discuss specific measures for items used in introductory\nprogramming. Evaluation of quality of similarity measures is difficult. To this\nend, we propose an evaluation approach utilizing three levels of abstraction.\nWe illustrate our approach to measuring similarity and provide evaluation using\nitems from three diverse programming environments.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 11:12:25 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Pel\u00e1nek", "Radek", ""], ["Effenberger", "Tom\u00e1\u0161", ""], ["Van\u011bk", "Mat\u011bj", ""], ["Sassmann", "Vojt\u011bch", ""], ["Gmiterko", "Dominik", ""]]}, {"id": "1806.03281", "submitter": "Niki Kilbertus", "authors": "Niki Kilbertus, Adri\\`a Gasc\\'on, Matt J. Kusner, Michael Veale,\n  Krishna P. Gummadi, Adrian Weller", "title": "Blind Justice: Fairness with Encrypted Sensitive Attributes", "comments": "published at ICML 2018", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:2630-2639, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has explored how to train machine learning models which do not\ndiscriminate against any subgroup of the population as determined by sensitive\nattributes such as gender or race. To avoid disparate treatment, sensitive\nattributes should not be considered. On the other hand, in order to avoid\ndisparate impact, sensitive attributes must be examined, e.g., in order to\nlearn a fair model, or to check if a given model is fair. We introduce methods\nfrom secure multi-party computation which allow us to avoid both. By encrypting\nsensitive attributes, we show how an outcome-based fair model may be learned,\nchecked, or have its outputs verified and held to account, without users\nrevealing their sensitive attributes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:19:38 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Kilbertus", "Niki", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Kusner", "Matt J.", ""], ["Veale", "Michael", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""]]}, {"id": "1806.03285", "submitter": "Brian Quistorff", "authors": "Matt Goldman, Brian Quistorff", "title": "Pricing Engine: Estimating Causal Impacts in Real World Business\n  Settings", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Pricing Engine package to enable the use of Double ML\nestimation techniques in general panel data settings. Customization allows the\nuser to specify first-stage models, first-stage featurization, second stage\ntreatment selection and second stage causal-modeling. We also introduce a\nDynamicDML class that allows the user to generate dynamic treatment-aware\nforecasts at a range of leads and to understand how the forecasts will vary as\na function of causally estimated treatment parameters. The Pricing Engine is\nbuilt on Python 3.5 and can be run on an Azure ML Workbench environment with\nthe addition of only a few Python packages. This note provides high-level\ndiscussion of the Double ML method, describes the packages intended use and\nincludes an example Jupyter notebook demonstrating application to some publicly\navailable data. Installation of the package and additional technical\ndocumentation is available at\n$\\href{https://github.com/bquistorff/pricingengine}{github.com/bquistorff/pricingengine}$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:31:25 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 19:34:54 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Goldman", "Matt", ""], ["Quistorff", "Brian", ""]]}, {"id": "1806.03286", "submitter": "Yichong Xu", "authors": "Yichong Xu, Sivaraman Balakrishnan, Aarti Singh and Artur Dubrawski", "title": "Regression with Comparisons: Escaping the Curse of Dimensionality with\n  Ordinal Information", "comments": "52 pages, 11 figures; Preliminary version in International Conference\n  on Machine Learning 2018", "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-54", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning, we typically leverage a fully labeled dataset to\ndesign methods for function estimation or prediction. In many practical\nsituations, we are able to obtain alternative feedback, possibly at a low cost.\nA broad goal is to understand the usefulness of, and to design algorithms to\nexploit, this alternative feedback. In this paper, we consider a\nsemi-supervised regression setting, where we obtain additional ordinal (or\ncomparison) information for the unlabeled samples. We consider ordinal feedback\nof varying qualities where we have either a perfect ordering of the samples, a\nnoisy ordering of the samples or noisy pairwise comparisons between the\nsamples. We provide a precise quantification of the usefulness of these types\nof ordinal feedback in both nonparametric and linear regression, showing that\nin many cases it is possible to accurately estimate an underlying function with\na very small labeled set, effectively \\emph{escaping the curse of\ndimensionality}. We also present lower bounds, that establish fundamental\nlimits for the task and show that our algorithms are optimal in a variety of\nsettings. Finally, we present extensive experiments on new datasets that\ndemonstrate the efficacy and practicality of our algorithms and investigate\ntheir robustness to various sources of noise and model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:33:43 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 01:44:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Yichong", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1806.03287", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er and Dan Boneh", "title": "Slalom: Fast, Verifiable and Private Execution of Neural Networks in\n  Trusted Hardware", "comments": "Accepted as an oral presentation at ICLR 2019. OpenReview available\n  at https://openreview.net/forum?id=rJVorjCcKQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Machine Learning (ML) gets applied to security-critical or sensitive\ndomains, there is a growing need for integrity and privacy for outsourced ML\ncomputations. A pragmatic solution comes from Trusted Execution Environments\n(TEEs), which use hardware and software protections to isolate sensitive\ncomputations from the untrusted software stack. However, these isolation\nguarantees come at a price in performance, compared to untrusted alternatives.\nThis paper initiates the study of high performance execution of Deep Neural\nNetworks (DNNs) in TEEs by efficiently partitioning DNN computations between\ntrusted and untrusted devices. Building upon an efficient outsourcing scheme\nfor matrix multiplication, we propose Slalom, a framework that securely\ndelegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX\nor Sanctum) to a faster, yet untrusted, co-located processor. We evaluate\nSlalom by running DNNs in an Intel SGX enclave, which selectively delegates\nwork to an untrusted GPU. For canonical DNNs (VGG16, MobileNet and ResNet\nvariants) we obtain 6x to 20x increases in throughput for verifiable inference,\nand 4x to 11x for verifiable and private inference.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:34:55 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 17:17:10 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Boneh", "Dan", ""]]}, {"id": "1806.03316", "submitter": "Zhiyuan Xu", "authors": "Chengxiang Yin, Jian Tang, Zhiyuan Xu, Yanzhi Wang", "title": "Adversarial Meta-Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning enables a model to learn from very limited data to undertake a\nnew task. In this paper, we study the general meta-learning with adversarial\nsamples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner),\nwhich leverages clean and adversarial samples to optimize the initialization of\na learning model in an adversarial manner. ADML leads to the following\ndesirable properties: 1) it turns out to be very effective even in the cases\nwith only clean samples; 2) it is robust to adversarial samples, i.e., unlike\nother meta-learning algorithms, it only leads to a minor performance\ndegradation when there are adversarial samples; 3) it sheds light on tackling\nthe cases with limited and even contaminated samples. It has been shown by\nextensive experimental results that ADML consistently outperforms three\nrepresentative meta-learning algorithms in the cases involving adversarial\nsamples, on two widely-used image datasets, MiniImageNet and CIFAR100, in terms\nof both accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 18:42:14 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 01:56:30 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 15:05:13 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yin", "Chengxiang", ""], ["Tang", "Jian", ""], ["Xu", "Zhiyuan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1806.03335", "submitter": "Ian Osband", "authors": "Ian Osband, John Aslanides, Albin Cassirer", "title": "Randomized Prior Functions for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with uncertainty is essential for efficient reinforcement learning.\nThere is a growing literature on uncertainty estimation for deep learning from\nfixed datasets, but many of the most popular approaches are poorly-suited to\nsequential decision problems. Other methods, such as bootstrap sampling, have\nno mechanism for uncertainty that does not come from the observed data. We\nhighlight why this can be a crucial shortcoming and propose a simple remedy\nthrough addition of a randomized untrainable `prior' network to each ensemble\nmember. We prove that this approach is efficient with linear representations,\nprovide simple illustrations of its efficacy with nonlinear representations and\nshow that this approach scales to large-scale problems far better than previous\nattempts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 19:47:54 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 17:53:47 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Osband", "Ian", ""], ["Aslanides", "John", ""], ["Cassirer", "Albin", ""]]}, {"id": "1806.03342", "submitter": "Palash Goyal", "authors": "Palash Goyal, KSM Tozammel Hossain, Ashok Deb, Nazgol Tavabi, Nathan\n  Bartley, Andr'es Abeliuk, Emilio Ferrara, Kristina Lerman", "title": "Discovering Signals from Web Sources to Predict Cyber Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber attacks are growing in frequency and severity. Over the past year alone\nwe have witnessed massive data breaches that stole personal information of\nmillions of people and wide-scale ransomware attacks that paralyzed critical\ninfrastructure of several countries. Combating the rising cyber threat calls\nfor a multi-pronged strategy, which includes predicting when these attacks will\noccur. The intuition driving our approach is this: during the planning and\npreparation stages, hackers leave digital traces of their activities on both\nthe surface web and dark web in the form of discussions on platforms like\nhacker forums, social media, blogs and the like. These data provide predictive\nsignals that allow anticipating cyber attacks. In this paper, we describe\nmachine learning techniques based on deep neural networks and autoregressive\ntime series models that leverage external signals from publicly available Web\nsources to forecast cyber attacks. Performance of our framework across ground\ntruth data over real-world forecasting tasks shows that our methods yield a\nsignificant lift or increase of F1 for the top signals on predicted cyber\nattacks. Our results suggest that, when deployed, our system will be able to\nprovide an effective line of defense against various types of targeted cyber\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 20:11:05 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Goyal", "Palash", ""], ["Hossain", "KSM Tozammel", ""], ["Deb", "Ashok", ""], ["Tavabi", "Nazgol", ""], ["Bartley", "Nathan", ""], ["Abeliuk", "Andr'es", ""], ["Ferrara", "Emilio", ""], ["Lerman", "Kristina", ""]]}, {"id": "1806.03349", "submitter": "Joshua Wang", "authors": "Tim Roughgarden and Joshua R. Wang", "title": "An Optimal Algorithm for Online Unconstrained Submodular Maximization", "comments": "COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a basic problem at the interface of two fundamental fields:\nsubmodular optimization and online learning. In the online unconstrained\nsubmodular maximization (online USM) problem, there is a universe\n$[n]=\\{1,2,...,n\\}$ and a sequence of $T$ nonnegative (not necessarily\nmonotone) submodular functions arrive over time. The goal is to design a\ncomputationally efficient online algorithm, which chooses a subset of $[n]$ at\neach time step as a function only of the past, such that the accumulated value\nof the chosen subsets is as close as possible to the maximum total value of a\nfixed subset in hindsight. Our main result is a polynomial-time no-$1/2$-regret\nalgorithm for this problem, meaning that for every sequence of nonnegative\nsubmodular functions, the algorithm's expected total value is at least $1/2$\ntimes that of the best subset in hindsight, up to an error term sublinear in\n$T$. The factor of $1/2$ cannot be improved upon by any polynomial-time online\nalgorithm when the submodular functions are presented as value oracles.\nPrevious work on the offline problem implies that picking a subset uniformly at\nrandom in each time step achieves zero $1/4$-regret.\n  A byproduct of our techniques is an explicit subroutine for the two-experts\nproblem that has an unusually strong regret guarantee: the total value of its\nchoices is comparable to twice the total value of either expert on rounds it\ndid not pick that expert. This subroutine may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 20:42:52 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Roughgarden", "Tim", ""], ["Wang", "Joshua R.", ""]]}, {"id": "1806.03404", "submitter": "Kar-Ann Toh", "authors": "Kar-Ann Toh, Lei Sun and Zhiping Lin", "title": "Deterministic Stretchy Regression", "comments": "Submitted for journal (JMLR) review since 28-Sept-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension of the regularized least-squares in which the estimation\nparameters are stretchable is introduced and studied in this paper. The\nsolution of this ridge regression with stretchable parameters is given in\nprimal and dual spaces and in closed-form. Essentially, the proposed solution\nstretches the covariance computation by a power term, thereby compressing or\namplifying the estimation parameters. To maintain the computation of power root\nterms within the real space, an input transformation is proposed. The results\nof an empirical evaluation in both synthetic and real-world data illustrate\nthat the proposed method is effective for compressive learning with\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 03:22:53 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Toh", "Kar-Ann", ""], ["Sun", "Lei", ""], ["Lin", "Zhiping", ""]]}, {"id": "1806.03417", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Douwe Kiela", "title": "Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic\n  Geometry", "comments": "Accepted at ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the discovery of hierarchical relationships from\nlarge-scale unstructured similarity scores. For this purpose, we study\ndifferent models of hyperbolic space and find that learning embeddings in the\nLorentz model is substantially more efficient than in the Poincar\\'e-ball\nmodel. We show that the proposed approach allows us to learn high-quality\nembeddings of large taxonomies which yield improvements over Poincar\\'e\nembeddings, especially in low dimensions. Lastly, we apply our model to\ndiscover hierarchies in two real-world datasets: we show that an embedding in\nhyperbolic space can reveal important aspects of a company's organizational\nstructure as well as reveal historical relationships between language families.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 05:56:50 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 13:06:31 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1806.03430", "submitter": "Shiqian Ma", "authors": "Shiqian Ma, Necdet Serhat Aybat", "title": "Efficient Optimization Algorithms for Robust Principal Component\n  Analysis and Its Variants", "comments": "to appear in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA has drawn significant attention in the last decade due to its\nsuccess in numerous application domains, ranging from bio-informatics,\nstatistics, and machine learning to image and video processing in computer\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\nformulated as optimization problems with exploitable special structures. Many\nspecialized efficient optimization methods have been proposed to solve robust\nPCA and related problems. In this paper we review existing optimization methods\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\ntheir advantages and disadvantages, and elaborate on their convergence\nbehaviors. We also provide some insights for possible future research\ndirections including new algorithmic frameworks that might be suitable for\nimplementing on multi-processor setting to handle large-scale problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 07:27:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ma", "Shiqian", ""], ["Aybat", "Necdet Serhat", ""]]}, {"id": "1806.03432", "submitter": "Xiaofei Ma", "authors": "Xiaofei Ma and Satya Dhavala", "title": "Hierarchical Clustering with Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a class of algorithms that seeks to build a\nhierarchy of clusters. It has been the dominant approach to constructing\nembedded classification schemes since it outputs dendrograms, which capture the\nhierarchical relationship among members at all levels of granularity,\nsimultaneously. Being greedy in the algorithmic sense, a hierarchical\nclustering partitions data at every step solely based on a similarity /\ndissimilarity measure. The clustering results oftentimes depend on not only the\ndistribution of the underlying data, but also the choice of dissimilarity\nmeasure and the clustering algorithm. In this paper, we propose a method to\nincorporate prior domain knowledge about entity relationship into the\nhierarchical clustering. Specifically, we use a distance function in\nultrametric space to encode the external ontological information. We show that\npopular linkage-based algorithms can faithfully recover the encoded structure.\nSimilar to some regularized machine learning techniques, we add this distance\nas a penalty term to the original pairwise distance to regulate the final\nstructure of the dendrogram. As a case study, we applied this method on real\ndata in the building of a customer behavior based product taxonomy for an\nAmazon service, leveraging the information from a larger Amazon-wide browse\nstructure. The method is useful when one wants to leverage the relational\ninformation from external sources, or the data used to generate the distance\nmatrix is noisy and sparse. Our work falls in the category of semi-supervised\nor constrained clustering.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 07:42:01 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 11:41:00 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2018 19:32:18 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Ma", "Xiaofei", ""], ["Dhavala", "Satya", ""]]}, {"id": "1806.03461", "submitter": "Amartya Sanyal", "authors": "Amartya Sanyal and Matt J. Kusner and Adri\\`a Gasc\\'on and Varun\n  Kanade", "title": "TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service", "comments": "Accepted at International Conference in Machine Learning (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods are widely used for a variety of prediction\nproblems. \\emph{Prediction as a service} is a paradigm in which service\nproviders with technological expertise and computational resources may perform\npredictions for clients. However, data privacy severely restricts the\napplicability of such services, unless measures to keep client data private\n(even from the service provider) are designed. Equally important is to minimize\nthe amount of computation and communication required between client and server.\nFully homomorphic encryption offers a possible way out, whereby clients may\nencrypt their data, and on which the server may perform arithmetic\ncomputations. The main drawback of using fully homomorphic encryption is the\namount of time required to evaluate large machine learning models on encrypted\ndata. We combine ideas from the machine learning literature, particularly work\non binarization and sparsification of neural networks, together with\nalgorithmic tools to speed-up and parallelize computation using encrypted data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 11:40:22 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sanyal", "Amartya", ""], ["Kusner", "Matt J.", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Kanade", "Varun", ""]]}, {"id": "1806.03467", "submitter": "Zhiwei Steven Wu", "authors": "Miruna Oprescu, Vasilis Syrgkanis, Zhiwei Steven Wu", "title": "Orthogonal Random Forest for Causal Inference", "comments": "This paper appeared in the Proceedings of the 36th International\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the orthogonal random forest, an algorithm that combines\nNeyman-orthogonality to reduce sensitivity with respect to estimation error of\nnuisance parameters with generalized random forests (Athey et al., 2017)--a\nflexible non-parametric method for statistical estimation of conditional moment\nmodels using random forests. We provide a consistency rate and establish\nasymptotic normality for our estimator. We show that under mild assumptions on\nthe consistency rate of the nuisance estimator, we can achieve the same error\nrate as an oracle with a priori knowledge of these nuisance parameters. We show\nthat when the nuisance functions have a locally sparse parametrization, then a\nlocal $\\ell_1$-penalized regression achieves the required rate. We apply our\nmethod to estimate heterogeneous treatment effects from observational data with\ndiscrete treatments or continuous treatments, and we show that, unlike prior\nwork, our method provably allows to control for a high-dimensional set of\nvariables under standard sparsity conditions. We also provide a comprehensive\nempirical evaluation of our algorithm on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 12:14:25 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 13:43:43 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 05:06:04 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2019 20:03:10 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Oprescu", "Miruna", ""], ["Syrgkanis", "Vasilis", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1806.03482", "submitter": "Won-Yong Shin", "authors": "Cong Tran, Won-Yong Shin, Sang-Il Choi", "title": "DIR-ST$^2$: Delineation of Imprecise Regions Using\n  Spatio--Temporal--Textual Information", "comments": "11 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An imprecise region is referred to as a geographical area without a\nclearly-defined boundary in the literature. Previous clustering-based\napproaches exploit spatial information to find such regions. However, the prior\nstudies suffer from the following two problems: the subjectivity in selecting\nclustering parameters and the inclusion of a large portion of the undesirable\nregion (i.e., a large number of noise points). To overcome these problems, we\npresent DIR-ST$^2$, a novel framework for delineating an imprecise region by\niteratively performing density-based clustering, namely DBSCAN, along with not\nonly spatio--textual information but also temporal information on social media.\nSpecifically, we aim at finding a proper radius of a circle used in the\niterative DBSCAN process by gradually reducing the radius for each iteration in\nwhich the temporal information acquired from all resulting clusters are\nleveraged. Then, we propose an efficient and automated algorithm delineating\nthe imprecise region via hierarchical clustering. Experiment results show that\nby virtue of the significant noise reduction in the region, our DIR-ST$^2$\nmethod outperforms the state-of-the-art approach employing one-class support\nvector machine in terms of the $\\mathcal{F}_1$ score from comparison with\nprecisely-defined regions regarded as a ground truth, and returns apparently\nbetter delineation of imprecise regions. The computational complexity of\nDIR-ST$^2$ is also analytically and numerically shown.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 14:49:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Tran", "Cong", ""], ["Shin", "Won-Yong", ""], ["Choi", "Sang-Il", ""]]}, {"id": "1806.03492", "submitter": "Joshua Bertram", "authors": "Josh Bertram and Peng Wei", "title": "Explainable Deterministic MDPs", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for a certain class of Markov Decision Processes (MDPs)\nthat can relate the optimal policy back to one or more reward sources in the\nenvironment. For a given initial state, without fully computing the value\nfunction, q-value function, or the optimal policy the algorithm can determine\nwhich rewards will and will not be collected, whether a given reward will be\ncollected only once or continuously, and which local maximum within the value\nfunction the initial state will ultimately lead to. We demonstrate that the\nmethod can be used to map the state space to identify regions that are\ndominated by one reward source and can fully analyze the state space to explain\nall actions. We provide a mathematical framework to show how all of this is\npossible without first computing the optimal policy or value function.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 15:44:54 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bertram", "Josh", ""], ["Wei", "Peng", ""]]}, {"id": "1806.03497", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Baoxiong Jia, Song-Chun Zhu", "title": "Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data\n  for Future Prediction", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future predictions on sequence data (e.g., videos or audios) require the\nalgorithms to capture non-Markovian and compositional properties of high-level\nsemantics. Context-free grammars are natural choices to capture such\nproperties, but traditional grammar parsers (e.g., Earley parser) only take\nsymbolic sentences as inputs. In this paper, we generalize the Earley parser to\nparse sequence data which is neither segmented nor labeled. This generalized\nEarley parser integrates a grammar parser with a classifier to find the optimal\nsegmentation and labels, and makes top-down future predictions. Experiments\nshow that our method significantly outperforms other approaches for future\nhuman activity prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 16:07:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Qi", "Siyuan", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1806.03514", "submitter": "Junwei Pan", "authors": "Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan,\n  Yu Sun, Quan Lu", "title": "Field-weighted Factorization Machines for Click-Through Rate Prediction\n  in Display Advertising", "comments": null, "journal-ref": null, "doi": "10.1145/3178876.3186040", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a critical task in online display\nadvertising. The data involved in CTR prediction are typically multi-field\ncategorical data, i.e., every feature is categorical and belongs to one and\nonly one field. One of the interesting characteristics of such data is that\nfeatures from one field often interact differently with features from different\nother fields. Recently, Field-aware Factorization Machines (FFMs) have been\namong the best performing models for CTR prediction by explicitly modeling such\ndifference. However, the number of parameters in FFMs is in the order of\nfeature number times field number, which is unacceptable in the real-world\nproduction systems. In this paper, we propose Field-weighted Factorization\nMachines (FwFMs) to model the different feature interactions between different\nfields in a much more memory-efficient way. Our experimental evaluations show\nthat FwFMs can achieve competitive prediction performance with only as few as\n4% parameters of FFMs. When using the same number of parameters, FwFMs can\nbring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 17:42:17 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 23:38:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Pan", "Junwei", ""], ["Xu", "Jian", ""], ["Ruiz", "Alfonso Lobos", ""], ["Zhao", "Wenliang", ""], ["Pan", "Shengjun", ""], ["Sun", "Yu", ""], ["Lu", "Quan", ""]]}, {"id": "1806.03536", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi\n  Kawarabayashi, Stefanie Jegelka", "title": "Representation Learning on Graphs with Jumping Knowledge Networks", "comments": "ICML 2018, accepted as a long oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of \"neighboring\" nodes that a node's representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models'\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 19:49:57 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 19:52:28 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Xu", "Keyulu", ""], ["Li", "Chengtao", ""], ["Tian", "Yonglong", ""], ["Sonobe", "Tomohiro", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1806.03547", "submitter": "Christoph Studer", "authors": "Ramina Ghods, Andrew S. Lan, Tom Goldstein, Christoph Studer", "title": "Linear Spectral Estimators and an Application to Phase Retrieval", "comments": "To appear at ICML 2018, extended version with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval refers to the problem of recovering real- or complex-valued\nvectors from magnitude measurements. The best-known algorithms for this problem\nare iterative in nature and rely on so-called spectral initializers that\nprovide accurate initialization vectors. We propose a novel class of estimators\nsuitable for general nonlinear measurement systems, called linear spectral\nestimators (LSPEs), which can be used to compute accurate initialization\nvectors for phase retrieval problems. The proposed LSPEs not only provide\naccurate initialization vectors for noisy phase retrieval systems with\nstructured or random measurement matrices, but also enable the derivation of\nsharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy\nof LSPEs on synthetic and real-world phase retrieval problems, and show that\nour estimators significantly outperform existing methods for structured\nmeasurement systems that arise in practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 21:41:03 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ghods", "Ramina", ""], ["Lan", "Andrew S.", ""], ["Goldstein", "Tom", ""], ["Studer", "Christoph", ""]]}, {"id": "1806.03551", "submitter": "Christoph Studer", "authors": "Andrew S. Lan, Mung Chiang, Christoph Studer", "title": "An Estimation and Analysis Framework for the Rasch Model", "comments": "To be presented at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rasch model is widely used for item response analysis in applications\nranging from recommender systems to psychology, education, and finance. While a\nnumber of estimators have been proposed for the Rasch model over the last\ndecades, the available analytical performance guarantees are mostly asymptotic.\nThis paper provides a framework that relies on a novel linear minimum\nmean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic,\nand closed-form analysis of the parameter estimation error under the Rasch\nmodel. The proposed framework provides guidelines on the number of items and\nresponses required to attain low estimation errors in tests or surveys. We\nfurthermore demonstrate its efficacy on a number of real-world collaborative\nfiltering datasets, which reveals that the proposed L-MMSE estimator performs\non par with state-of-the-art nonlinear estimators in terms of predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 21:56:41 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Lan", "Andrew S.", ""], ["Chiang", "Mung", ""], ["Studer", "Christoph", ""]]}, {"id": "1806.03555", "submitter": "Aman Agarwal", "authors": "Aman Agarwal, Ivan Zaitsev, Thorsten Joachims", "title": "Consistent Position Bias Estimation without Online Interventions for\n  Learning-to-Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presentation bias is one of the key challenges when learning from implicit\nfeedback in search engines, as it confounds the relevance signal with\nuninformative signals due to position in the ranking, saliency, and other\npresentation factors. While it was recently shown how counterfactual\nlearning-to-rank (LTR) approaches \\cite{Joachims/etal/17a} can provably\novercome presentation bias if observation propensities are known, it remains to\nshow how to accurately estimate these propensities. In this paper, we propose\nthe first method for producing consistent propensity estimates without manual\nrelevance judgments, disruptive interventions, or restrictive relevance\nmodeling assumptions. We merely require that we have implicit feedback data\nfrom multiple different ranking functions. Furthermore, we argue that our\nestimation technique applies to an extended class of Contextual Position-Based\nPropensity Models, where propensities not only depend on position but also on\nobservable features of the query and document. Initial simulation studies\nconfirm that the approach is scalable, accurate, and robust.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 23:04:56 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Agarwal", "Aman", ""], ["Zaitsev", "Ivan", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1806.03563", "submitter": "Hao Zhou", "authors": "Hao Henry Zhou, Yunyang Xiong, Vikas Singh", "title": "Building Bayesian Neural Networks with Blocks: On Structure,\n  Interpretability and Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple schemes to build Bayesian Neural Networks (BNNs), block by\nblock, inspired by a recent idea of computation skeletons. We show how by\nadjusting the types of blocks that are used within the computation skeleton, we\ncan identify interesting relationships with Deep Gaussian Processes (DGPs),\ndeep kernel learning (DKL), random features type approximation and other\ntopics. We give strategies to approximate the posterior via doubly stochastic\nvariational inference for such models which yield uncertainty estimates. We\ngive a detailed theoretical analysis and point out extensions that may be of\nindependent interest. As a special case, we instantiate our procedure to define\na Bayesian {\\em additive} Neural network -- a promising strategy to identify\nstatistical interactions and has direct benefits for obtaining interpretable\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 00:58:58 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhou", "Hao Henry", ""], ["Xiong", "Yunyang", ""], ["Singh", "Vikas", ""]]}, {"id": "1806.03571", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Vahid Tarokh", "title": "Stationary Geometric Graphical Model Selection", "comments": "arXiv admin note: text overlap with arXiv:1802.03848", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model selection in Gaussian Markov fields in the\nsample deficient scenario. In many practically important cases, the underlying\nnetworks are embedded into Euclidean spaces. Using the natural geometric\nstructure, we introduce the notion of spatially stationary distributions over\ngeometric graphs. This directly generalizes the notion of stationary time\nseries to the multidimensional setting lacking time axis. We show that the idea\nof spatial stationarity leads to a dramatic decrease in the sample complexity\nof the model selection compared to abstract graphs with the same level of\nsparsity. For geometric graphs on randomly spread vertices and edges of bounded\nlength, we develop tight information-theoretic bounds on sample complexity and\nshow that a finite number of independent samples is sufficient for a consistent\nrecovery. Finally, we develop an efficient technique capable of reliably and\nconsistently reconstructing graphs with a bounded number of measurements.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 01:58:46 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 20:47:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1806.03583", "submitter": "Mehdi Faraji", "authors": "Ji Yang, Lin Tong, Mehdi Faraji, Anup Basu", "title": "IVUS-Net: An Intravascular Ultrasound Segmentation Network", "comments": "7 pages, 3 figures, accepted to be published in International\n  Conference of Smart Multimedia. The final authenticated publication is\n  available online at https://doi.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IntraVascular UltraSound (IVUS) is one of the most effective imaging\nmodalities that provides assistance to experts in order to diagnose and treat\ncardiovascular diseases. We address a central problem in IVUS image analysis\nwith Fully Convolutional Network (FCN): automatically delineate the lumen and\nmedia-adventitia borders in IVUS images, which is crucial to shorten the\ndiagnosis process or benefits a faster and more accurate 3D reconstruction of\nthe artery. Particularly, we propose an FCN architecture, called IVUS-Net,\nfollowed by a post-processing contour extraction step, in order to\nautomatically segments the interior (lumen) and exterior (media-adventitia)\nregions of the human arteries. We evaluated our IVUS-Net on the test set of a\nstandard publicly available dataset containing 326 IVUS B-mode images with two\nmeasurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The\nevaluation result shows that IVUS-Net outperforms the state-of-the-art lumen\nand media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net\nperforms well on images in the test set that contain a significant amount of\nmajor artifacts such as bifurcations, shadows, and side branches that are not\ncommon in the training set. Furthermore, using a modern GPU, IVUS-Net segments\neach IVUS frame only in 0.15 seconds. The proposed work, to the best of our\nknowledge, is the first deep learning based method for segmentation of both the\nlumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the\nbest results without any manual intervention. Code is available at\nhttps://github.com/Kulbear/ivus-segmentation-icsm2018\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 04:28:53 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 17:53:51 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yang", "Ji", ""], ["Tong", "Lin", ""], ["Faraji", "Mehdi", ""], ["Basu", "Anup", ""]]}, {"id": "1806.03664", "submitter": "Ciwan Ceylan", "authors": "Ciwan Ceylan and Michael U. Gutmann", "title": "Conditional Noise-Contrastive Estimation of Unnormalised Models", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many parametric statistical models are not properly normalised and only\nspecified up to an intractable partition function, which renders parameter\nestimation difficult. Examples of unnormalised models are Gibbs distributions,\nMarkov random fields, and neural network models in unsupervised deep learning.\nIn previous work, the estimation principle called noise-contrastive estimation\n(NCE) was introduced where unnormalised models are estimated by learning to\ndistinguish between data and auxiliary noise. An open question is how to best\nchoose the auxiliary noise distribution. We here propose a new method that\naddresses this issue. The proposed method shares with NCE the idea of\nformulating density estimation as a supervised learning problem but in contrast\nto NCE, the proposed method leverages the observed data when generating noise\nsamples. The noise can thus be generated in a semi-automated manner. We first\npresent the underlying theory of the new method, show that score matching\nemerges as a limiting case, validate the method on continuous and discrete\nvalued synthetic data, and show that we can expect an improved performance\ncompared to NCE when the data lie in a lower-dimensional manifold. Then we\ndemonstrate its applicability in unsupervised deep learning by estimating a\nfour-layer neural image model.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 14:16:27 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ceylan", "Ciwan", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1806.03677", "submitter": "Laurent Lessard", "authors": "Bin Hu, Stephen Wright, Laurent Lessard", "title": "Dissipativity Theory for Accelerating Stochastic Variance Reduction: A\n  Unified Analysis of SVRG and Katyusha Using Semidefinite Programs", "comments": "to appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques for reducing the variance of gradient estimates used in stochastic\nprogramming algorithms for convex finite-sum problems have received a great\ndeal of attention in recent years. By leveraging dissipativity theory from\ncontrol, we provide a new perspective on two important variance-reduction\nalgorithms: SVRG and its direct accelerated variant Katyusha. Our perspective\nprovides a physically intuitive understanding of the behavior of SVRG-like\nmethods via a principle of energy conservation. The tools discussed here allow\nus to automate the convergence analysis of SVRG-like methods by capturing their\nessential properties in small semidefinite programs amenable to standard\nanalysis and computational techniques. Our approach recovers existing\nconvergence results for SVRG and Katyusha and generalizes the theory to\nalternative parameter choices. We also discuss how our approach complements the\nlinear coupling technique. Our combination of perspectives leads to a better\nunderstanding of accelerated variance-reduced stochastic methods for finite-sum\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 15:39:43 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Hu", "Bin", ""], ["Wright", "Stephen", ""], ["Lessard", "Laurent", ""]]}, {"id": "1806.03688", "submitter": "Michael Bommarito II", "authors": "Michael J Bommarito II and Daniel Martin Katz and Eric M Detterman", "title": "LexNLP: Natural language processing and information extraction for legal\n  and regulatory texts", "comments": "9 pages, 0 figures; see also\n  https://github.com/LexPredict/lexpredict-lexnlp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LexNLP is an open source Python package focused on natural language\nprocessing and machine learning for legal and regulatory text. The package\nincludes functionality to (i) segment documents, (ii) identify key text such as\ntitles and section headings, (iii) extract over eighteen types of structured\ninformation like distances and dates, (iv) extract named entities such as\ncompanies and geopolitical entities, (v) transform text into features for model\ntraining, and (vi) build unsupervised and supervised models such as word\nembedding or tagging models. LexNLP includes pre-trained models based on\nthousands of unit tests drawn from real documents available from the SEC EDGAR\ndatabase as well as various judicial and regulatory proceedings. LexNLP is\ndesigned for use in both academic research and industrial applications, and is\ndistributed at https://github.com/LexPredict/lexpredict-lexnlp.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 16:55:40 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bommarito", "Michael J", "II"], ["Katz", "Daniel Martin", ""], ["Detterman", "Eric M", ""]]}, {"id": "1806.03720", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic seismic waveform inversion using generative adversarial\n  networks as a geological prior", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of deep generative models in the context of\npartial-differential equation (PDE) constrained inverse problems. We combine a\ngenerative adversarial network (GAN) representing an a priori model that\ncreates subsurface geological structures and their petrophysical properties,\nwith the numerical solution of the PDE governing the propagation of acoustic\nwaves within the earth's interior. We perform Bayesian inversion using an\napproximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the\nposterior given seismic observations. Gradients with respect to the model\nparameters governing the forward problem are obtained by solving the adjoint of\nthe acoustic wave equation. Gradients of the mismatch with respect to the\nlatent variables are obtained by leveraging the differentiable nature of the\ndeep neural network used to represent the generative model. We show that\napproximate MALA sampling allows efficient Bayesian inversion of model\nparameters obtained from a prior represented by a deep generative model,\nobtaining a diverse set of realizations that reflect the observed seismic\nresponse.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 20:38:29 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1806.03723", "submitter": "Guillaume Leclerc", "authors": "Guillaume Leclerc, Manasi Vartak, Raul Castro Fernandez, Tim Kraska,\n  Samuel Madden", "title": "Smallify: Learning Network Size while Training", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks become widely deployed in different applications and on\ndifferent hardware, it has become increasingly important to optimize inference\ntime and model size along with model accuracy. Most current techniques optimize\nmodel size, model accuracy and inference time in different stages, resulting in\nsuboptimal results and computational inefficiency. In this work, we propose a\nnew technique called Smallify that optimizes all three of these metrics at the\nsame time. Specifically we present a new method to simultaneously optimize\nnetwork size and model performance by neuron-level pruning during training.\nNeuron-level pruning not only produces much smaller networks but also produces\ndense weight matrices that are amenable to efficient inference. By applying our\ntechnique to convolutional as well as fully connected models, we show that\nSmallify can reduce network size by 35X with a 6X improvement in inference time\nwith similar accuracy as models found by traditional training techniques.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 20:58:59 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Leclerc", "Guillaume", ""], ["Vartak", "Manasi", ""], ["Fernandez", "Raul Castro", ""], ["Kraska", "Tim", ""], ["Madden", "Samuel", ""]]}, {"id": "1806.03763", "submitter": "Thomas Pumir", "authors": "Thomas Pumir, Samy Jelassi, Nicolas Boumal", "title": "Smoothed analysis of the low-rank approach for smooth semidefinite\n  programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider semidefinite programs (SDPs) of size n with equality constraints.\nIn order to overcome scalability issues, Burer and Monteiro proposed a\nfactorized approach based on optimizing over a matrix Y of size $n$ by $k$ such\nthat $X = YY^*$ is the SDP variable. The advantages of such formulation are\ntwofold: the dimension of the optimization variable is reduced and positive\nsemidefiniteness is naturally enforced. However, the problem in Y is\nnon-convex. In prior work, it has been shown that, when the constraints on the\nfactorized variable regularly define a smooth manifold, provided k is large\nenough, for almost all cost matrices, all second-order stationary points\n(SOSPs) are optimal. Importantly, in practice, one can only compute points\nwhich approximately satisfy necessary optimality conditions, leading to the\nquestion: are such points also approximately optimal? To this end, and under\nsimilar assumptions, we use smoothed analysis to show that approximate SOSPs\nfor a randomly perturbed objective function are approximate global optima, with\nk scaling like the square root of the number of constraints (up to log\nfactors). Moreover, we bound the optimality gap at the approximate solution of\nthe perturbed problem with respect to the original problem. We particularize\nour results to an SDP relaxation of phase retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 01:44:00 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 22:22:10 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Pumir", "Thomas", ""], ["Jelassi", "Samy", ""], ["Boumal", "Nicolas", ""]]}, {"id": "1806.03791", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen and Hongyi Wang and Jinman Zhao and Dimitris\n  Papailiopoulos and Paraschos Koutris", "title": "The Effect of Network Width on the Performance of Large-batch Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed implementations of mini-batch stochastic gradient descent (SGD)\nsuffer from communication overheads, attributed to the high frequency of\ngradient updates inherent in small-batch training. Training with large batches\ncan reduce these overheads; however, large batches can affect the convergence\nproperties and generalization performance of SGD. In this work, we take a first\nstep towards analyzing how the structure (width and depth) of a neural network\naffects the performance of large-batch training. We present new theoretical\nresults which suggest that--for a fixed number of parameters--wider networks\nare more amenable to fast large-batch training compared to deeper ones. We\nprovide extensive experiments on residual and fully-connected neural networks\nwhich suggest that wider networks can be trained using larger batches without\nincurring a convergence slow-down, unlike their deeper variants.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:29:17 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chen", "Lingjiao", ""], ["Wang", "Hongyi", ""], ["Zhao", "Jinman", ""], ["Papailiopoulos", "Dimitris", ""], ["Koutris", "Paraschos", ""]]}, {"id": "1806.03796", "submitter": "Yash Upadhyay", "authors": "Yash Upadhyay, Paul Schrater", "title": "Generative Adversarial Network Architectures For Image Synthesis Using\n  Capsule Networks", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Generative Adversarial Network (GAN) architectures\nthat use Capsule Networks for image-synthesis. Based on the principal of\npositional-equivariance of features, Capsule Network's ability to encode\nspatial relationships between the features of the image helps it become a more\npowerful critic in comparison to Convolutional Neural Networks (CNNs) used in\ncurrent architectures for image synthesis. Our proposed GAN architectures learn\nthe data manifold much faster and therefore, synthesize visually accurate\nimages in significantly lesser number of training samples and training epochs\nin comparison to GANs and its variants that use CNNs. Apart from analyzing the\nquantitative results corresponding the images generated by different\narchitectures, we also explore the reasons for the lower coverage and diversity\nexplored by the GAN architectures that use CNN critics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:54:24 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:55:20 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 15:45:28 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 18:33:11 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Upadhyay", "Yash", ""], ["Schrater", "Paul", ""]]}, {"id": "1806.03803", "submitter": "Amir Asadi", "authors": "Amir R. Asadi, Emmanuel Abbe, Sergio Verd\\'u", "title": "Chaining Mutual Information and Tightening Generalization Bounds", "comments": "20 pages, 1 figure; published at the NeurIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounding the generalization error of learning algorithms has a long history,\nwhich yet falls short in explaining various generalization successes including\nthose of deep learning. Two important difficulties are (i) exploiting the\ndependencies between the hypotheses, (ii) exploiting the dependence between the\nalgorithm's input and output. Progress on the first point was made with the\nchaining method, originating from the work of Kolmogorov, and used in the\nVC-dimension bound. More recently, progress on the second point was made with\nthe mutual information method by Russo and Zou '15. Yet, these two methods are\ncurrently disjoint. In this paper, we introduce a technique to combine the\nchaining and mutual information methods, to obtain a generalization bound that\nis both algorithm-dependent and that exploits the dependencies between the\nhypotheses. We provide an example in which our bound significantly outperforms\nboth the chaining and the mutual information bounds. As a corollary, we tighten\nDudley's inequality when the learning algorithm chooses its output from a small\nsubset of hypotheses with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:31:12 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 04:20:50 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Asadi", "Amir R.", ""], ["Abbe", "Emmanuel", ""], ["Verd\u00fa", "Sergio", ""]]}, {"id": "1806.03816", "submitter": "Kiarash Shaloudegi", "authors": "Kiarash Shaloudegi and Andr\\'as Gy\\\"orgy", "title": "Adaptive MCMC via Combining Local Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are widely used in machine learning.\nOne of the major problems with MCMC is the question of how to design chains\nthat mix fast over the whole state space; in particular, how to select the\nparameters of an MCMC algorithm. Here we take a different approach and,\nsimilarly to parallel MCMC methods, instead of trying to find a single chain\nthat samples from the whole distribution, we combine samples from several\nchains run in parallel, each exploring only parts of the state space (e.g., a\nfew modes only). The chains are prioritized based on kernel Stein discrepancy,\nwhich provides a good measure of performance locally. The samples from the\nindependent chains are combined using a novel technique for estimating the\nprobability of different regions of the sample space. Experimental results\ndemonstrate that the proposed algorithm may provide significant speedups in\ndifferent sampling problems. Most importantly, when combined with the\nstate-of-the-art NUTS algorithm as the base MCMC sampler, our method remained\ncompetitive with NUTS on sampling from unimodal distributions, while\nsignificantly outperforming state-of-the-art competitors on synthetic\nmultimodal problems as well as on a challenging sensor localization task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 05:35:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 01:44:04 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 21:12:10 GMT"}, {"version": "v4", "created": "Wed, 8 May 2019 16:48:49 GMT"}, {"version": "v5", "created": "Thu, 9 May 2019 10:15:34 GMT"}, {"version": "v6", "created": "Sat, 13 Jul 2019 02:42:35 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Shaloudegi", "Kiarash", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""]]}, {"id": "1806.03836", "submitter": "Jaesik Yoon", "authors": "Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio and\n  Sungjin Ahn", "title": "Bayesian Model-Agnostic Meta-Learning", "comments": "First two authors contributed equally. 15 pages with appendix\n  including experimental details. Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to infer Bayesian posterior from a few-shot dataset is an important\nstep towards robust meta-learning due to the model uncertainty inherent in the\nproblem. In this paper, we propose a novel Bayesian model-agnostic\nmeta-learning method. The proposed method combines scalable gradient-based\nmeta-learning with nonparametric variational inference in a principled\nprobabilistic framework. During fast adaptation, the method is capable of\nlearning complex uncertainty structure beyond a point estimate or a simple\nGaussian approximation. In addition, a robust Bayesian meta-update mechanism\nwith a new meta-loss prevents overfitting during meta-update. Remaining an\nefficient gradient-based meta-learner, the method is also model-agnostic and\nsimple to implement. Experiment results show the accuracy and robustness of the\nproposed method in various tasks: sinusoidal regression, image classification,\nactive learning, and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 07:11:28 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 02:03:08 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 11:04:41 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2018 01:53:11 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kim", "Taesup", ""], ["Yoon", "Jaesik", ""], ["Dia", "Ousmane", ""], ["Kim", "Sungwoong", ""], ["Bengio", "Yoshua", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1806.03857", "submitter": "Rein van 't Veer", "authors": "Rein van 't Veer and Peter Bloem and Erwin Folmer", "title": "Deep Learning for Classification Tasks on Geospatial Vector Polygons", "comments": "Major rewrite in article structure, reasoning flow and included\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate the accuracy of deep learning approaches on\ngeospatial vector geometry classification tasks. The purpose of this evaluation\nis to investigate the ability of deep learning models to learn from geometry\ncoordinates directly. Previous machine learning research applied to geospatial\npolygon data did not use geometries directly, but derived properties thereof.\nThese are produced by way of extracting geometry properties such as Fourier\ndescriptors. Instead, our introduced deep neural net architectures are able to\nlearn on sequences of coordinates mapped directly from polygons. In three\nclassification tasks we show that the deep learning architectures are\ncompetitive with common learning algorithms that require extracted features.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 08:33:04 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 13:06:06 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Veer", "Rein van 't", ""], ["Bloem", "Peter", ""], ["Folmer", "Erwin", ""]]}, {"id": "1806.03884", "submitter": "Thomas George", "authors": "Thomas George, C\\'esar Laurent, Xavier Bouthillier, Nicolas Ballas,\n  Pascal Vincent", "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored\n  Eigenbasis", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 09:44:23 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 16:44:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["George", "Thomas", ""], ["Laurent", "C\u00e9sar", ""], ["Bouthillier", "Xavier", ""], ["Ballas", "Nicolas", ""], ["Vincent", "Pascal", ""]]}, {"id": "1806.03903", "submitter": "Mehdi Katranji", "authors": "Mehdi Katranji, Sami Kraiem, Laurent Moalic, Guilhem Sanmarty,\n  Alexandre Caminada, Fouad Hadj Selem", "title": "Multi-task learning of daily work and study round-trips from survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a machine learning approach to infer the worker and\nstudent mobility flows on daily basis from static censuses. The rapid\nurbanization has made the estimation of the human mobility flows a critical\ntask for transportation and urban planners. The primary objective of this paper\nis to complete individuals' census data with working and studying trips,\nallowing its merging with other mobility data to better estimate the complete\norigin-destination matrices. Worker and student mobility flows are among the\nmost weekly regular displacements and consequently generate road congestion\nproblems. Estimating their round-trips eases the decision-making processes for\nlocal authorities. Worker and student censuses often contain home location,\nwork places and educational institutions. We thus propose a neural network\nmodel that learns the temporal distribution of displacements from other\nmobility sources and tries to predict them on new censuses data. The inclusion\nof multi-task learning in our neural network results in a significant error\nrate control in comparison to single task learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:02:15 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Katranji", "Mehdi", ""], ["Kraiem", "Sami", ""], ["Moalic", "Laurent", ""], ["Sanmarty", "Guilhem", ""], ["Caminada", "Alexandre", ""], ["Selem", "Fouad Hadj", ""]]}, {"id": "1806.03925", "submitter": "Hao Dong", "authors": "Hao Dong and Shuai Li and Dongchang Xu and Yi Ren and Di Zhang", "title": "Gear Training: A new way to implement high-performance model-parallel\n  training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of Deep Neural Networks usually needs tremendous computing\nresources. Therefore many deep models are trained in large cluster instead of\nsingle machine or GPU. Though major researchs at present try to run whole model\non all machines by using asynchronous asynchronous stochastic gradient descent\n(ASGD), we present a new approach to train deep model parallely -- split the\nmodel and then seperately train different parts of it in different speed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:44:33 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dong", "Hao", ""], ["Li", "Shuai", ""], ["Xu", "Dongchang", ""], ["Ren", "Yi", ""], ["Zhang", "Di", ""]]}, {"id": "1806.03945", "submitter": "Yutaro Shigeto", "authors": "Yutaro Shigeto, Masashi Shimbo, Yuji Matsumoto", "title": "A Fast and Easy Regression Technique for k-NN Classification Without\n  Using Negative Pairs", "comments": "Earlier version of this paper appeared in PAKDD 2017. This version\n  corrects an error in Eq. (6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an inexpensive way to learn an effective dissimilarity\nfunction to be used for $k$-nearest neighbor ($k$-NN) classification. Unlike\nMahalanobis metric learning methods that map both query (unlabeled) objects and\nlabeled objects to new coordinates by a single transformation, our method\nlearns a transformation of labeled objects to new points in the feature space\nwhereas query objects are kept in their original coordinates. This method has\nseveral advantages over existing distance metric learning methods: (i) In\nexperiments with large document and image datasets, it achieves $k$-NN\nclassification accuracy better than or at least comparable to the\nstate-of-the-art metric learning methods. (ii) The transformation can be\nlearned efficiently by solving a standard ridge regression problem. For\ndocument and image datasets, training is often more than two orders of\nmagnitude faster than the fastest metric learning methods tested. This speed-up\nis also due to the fact that the proposed method eliminates the optimization\nover \"negative\" object pairs, i.e., objects whose class labels are different.\n(iii) The formulation has a theoretical justification in terms of reducing\nhubness in data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:03:50 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 15:56:54 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Shigeto", "Yutaro", ""], ["Shimbo", "Masashi", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1806.03962", "submitter": "Bastiaan Veeling", "authors": "Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen and Max\n  Welling", "title": "Rotation Equivariant CNNs for Digital Pathology", "comments": "To be presented at MICCAI 2018. Implementations of equivariant layers\n  available at https://github.com/basveeling/keras_gcnn . PCam details and data\n  at https://github.com/basveeling/pcam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for digital pathology segmentation, based on the\nobservation that histopathology images are inherently symmetric under rotation\nand reflection. Utilizing recent findings on rotation equivariant CNNs, the\nproposed model leverages these symmetries in a principled manner. We present a\nvisual analysis showing improved stability on predictions, and demonstrate that\nexploiting rotation equivariance significantly improves tumor detection\nperformance on a challenging lymph node metastases dataset. We further present\na novel derived dataset to enable principled comparison of machine learning\nmodels, in combination with an initial benchmark. Through this dataset, the\ntask of histopathology diagnosis becomes accessible as a challenging benchmark\nfor fundamental machine learning research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:13:37 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Veeling", "Bastiaan S.", ""], ["Linmans", "Jasper", ""], ["Winkens", "Jim", ""], ["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "1806.03972", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Rodolphe Vadaine, Guillaume Hajduch, Ren\\'e Garello and\n  Ronan Fablet", "title": "A Multi-task Deep Learning Architecture for Maritime Surveillance using\n  AIS Data Streams", "comments": "Accepted to IEEE DSAA 2018", "journal-ref": null, "doi": "10.1109/DSAA.2018.00044", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world of global trading, maritime safety, security and efficiency are\ncrucial issues. We propose a multi-task deep learning framework for vessel\nmonitoring using Automatic Identification System (AIS) data streams. We combine\nrecurrent neural networks with latent variable modeling and an embedding of AIS\nmessages to a new representation space to jointly address key issues to be\ndealt with when considering AIS data streams: massive amount of streaming data,\nnoisy data and irregular timesampling. We demonstrate the relevance of the\nproposed deep learning framework on real AIS datasets for a three-task setting,\nnamely trajectory reconstruction, anomaly detection and vessel type\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:21:09 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 08:25:33 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 21:12:20 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Nguyen", "Duong", ""], ["Vadaine", "Rodolphe", ""], ["Hajduch", "Guillaume", ""], ["Garello", "Ren\u00e9", ""], ["Fablet", "Ronan", ""]]}, {"id": "1806.04000", "submitter": "Niharika Gauraha", "authors": "Ola Spjuth, Lars Carlsson, Niharika Gauraha", "title": "Aggregating Predictions on Multiple Non-disclosed Datasets using\n  Conformal Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal Prediction is a machine learning methodology that produces valid\nprediction regions under mild conditions. In this paper, we explore the\napplication of making predictions over multiple data sources of different sizes\nwithout disclosing data between the sources. We propose that each data source\napplies a transductive conformal predictor independently using the local data,\nand that the individual predictions are then aggregated to form a combined\nprediction region. We demonstrate the method on several data sets, and show\nthat the proposed method produces conservatively valid predictions and reduces\nthe variance in the aggregated predictions. We also study the effect that the\nnumber of data sources and size of each source has on aggregated predictions,\nas compared with equally sized sources and pooled data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:07:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 07:38:44 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Spjuth", "Ola", ""], ["Carlsson", "Lars", ""], ["Gauraha", "Niharika", ""]]}, {"id": "1806.04016", "submitter": "Gaelle Loosli", "authors": "Isma\\\"ila Seck (LIMOS, LITIS), Khouloud Dahmane (Cerema), Pierre\n  Duthon (Cerema), Ga\\\"elle Loosli (LIMOS)", "title": "Baselines and a datasheet for the Cerema AWP dataset", "comments": "Conf\\'erence d'Apprentissage CAp, Jun 2018, Rouen, France. 2018,\n  Conf\\'erence d'Apprentissage Francophone 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the recently published Cerema AWP (Adverse Weather\nPedestrian) dataset for various machine learning tasks and its exports in\nmachine learning friendly format. We explain why this dataset can be\ninteresting (mainly because it is a greatly controlled and fully annotated\nimage dataset) and present baseline results for various tasks. Moreover, we\ndecided to follow the very recent suggestions of datasheets for dataset, trying\nto standardize all the available information of the dataset, with a\ntransparency objective.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:22:48 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Seck", "Isma\u00efla", "", "LIMOS, LITIS"], ["Dahmane", "Khouloud", "", "Cerema"], ["Duthon", "Pierre", "", "Cerema"], ["Loosli", "Ga\u00eblle", "", "LIMOS"]]}, {"id": "1806.04028", "submitter": "Dmitrii Ostrovskii", "authors": "Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski, Dmitrii\n  Ostrovskii", "title": "Adaptive Denoising of Signals with Local Shift-Invariant Structure", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of adaptive discrete-time signal denoising in the\nsituation where the signal to be recovered admits a \"linear oracle\" -- an\nunknown linear estimate that takes the form of convolution of observations with\na time-invariant filter. It was shown by Juditsky and Nemirovski (2009) that\nwhen the $\\ell_2$-norm of the oracle filter is small enough, such oracle can be\n\"mimicked\" by an efficiently computable adaptive estimate of the same structure\nwith an observation-driven filter. The filter in question was obtained as a\nsolution to the optimization problem in which the $\\ell_\\infty$-norm of the\nDiscrete Fourier Transform (DFT) of the estimation residual is minimized under\nconstraint on the $\\ell_1$-norm of the filter DFT. In this paper, we discuss a\nnew family of adaptive estimates which rely upon minimizing the $\\ell_2$-norm\nof the estimation residual. We show that such estimators possess better\nstatistical properties than those based on $\\ell_\\infty$-fit; in particular, we\nprove oracle inequalities for their $\\ell_2$-loss and improved bounds for\n$\\ell_2$- and pointwise losses. The oracle inequalities rely on the\n\"approximate shift-invariance\" assumption stating that the signal to be\nrecovered is close to an (unknown) shift-invariant subspace. We also study the\nrelationship of the approximate shift-invariance assumption with the \"signal\nsimplicity\" assumption introduced in Juditsky and Nemirovski (2009) and discuss\nthe application of the proposed approach to harmonic oscillations denoising.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:51:11 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 02:12:26 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Harchaoui", "Zaid", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""], ["Ostrovskii", "Dmitrii", ""]]}, {"id": "1806.04037", "submitter": "Andriy Temko Dr", "authors": "Mark O'Sullivan, Sergi Gomez, Alison O'Shea, Eduard Salgado, Kevin\n  Huillca, Sean Mathieson, Geraldine Boylan, Emanuel Popovici, Andriy Temko", "title": "Neonatal EEG Interpretation and Decision Support Framework for Mobile\n  Platforms", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and implements an intuitive and pervasive solution for\nneonatal EEG monitoring assisted by sonification and deep learning AI that\nprovides information about neonatal brain health to all neonatal healthcare\nprofessionals, particularly those without EEG interpretation expertise. The\nsystem aims to increase the demographic of clinicians capable of diagnosing\nabnormalities in neonatal EEG. The proposed system uses a low-cost and\nlow-power EEG acquisition system. An Android app provides single-channel EEG\nvisualization, traffic-light indication of the presence of neonatal seizures\nprovided by a trained, deep convolutional neural network and an algorithm for\nEEG sonification, designed to facilitate the perception of changes in EEG\nmorphology specific to neonatal seizures. The multifaceted EEG interpretation\nframework is presented and the implemented mobile platform architecture is\nanalyzed with respect to its power consumption and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:35:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["O'Sullivan", "Mark", ""], ["Gomez", "Sergi", ""], ["O'Shea", "Alison", ""], ["Salgado", "Eduard", ""], ["Huillca", "Kevin", ""], ["Mathieson", "Sean", ""], ["Boylan", "Geraldine", ""], ["Popovici", "Emanuel", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.04047", "submitter": "Amir Asiaee T.", "authors": "Amir Asiaee, Samet Oymak, Kevin R. Coombes, Arindam Banerjee", "title": "High Dimensional Data Enrichment: Interpretable, Fast, and\n  Data-Efficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional structured data enriched model describes groups of\nobservations by shared and per-group individual parameters, each with its own\nstructure such as sparsity or group sparsity. In this paper, we consider the\ngeneral form of data enrichment where data comes in a fixed but arbitrary\nnumber of groups G. Any convex function, e.g., norms, can characterize the\nstructure of both shared and individual parameters. We propose an estimator for\nhigh dimensional data enriched model and provide conditions under which it\nconsistently estimates both shared and individual parameters. We also delineate\nsample complexity of the estimator and present high probability non-asymptotic\nbound on estimation error of all parameters. Interestingly the sample\ncomplexity of our estimator translates to conditions on both per-group sample\nsizes and the total number of samples. We propose an iterative estimation\nalgorithm with linear convergence rate and supplement our theoretical analysis\nwith synthetic and real experimental results. Particularly, we show the\npredictive power of data-enriched model along with its interpretable results in\nanticancer drug sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:15:44 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 21:22:50 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 21:51:38 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Asiaee", "Amir", ""], ["Oymak", "Samet", ""], ["Coombes", "Kevin R.", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1806.04090", "submitter": "Zachary Charles", "authors": "Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen\n  Wright, Dimitris Papailiopoulos", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed model training suffers from communication overheads due to\nfrequent gradient updates transmitted between compute nodes. To mitigate these\noverheads, several studies propose the use of sparsified stochastic gradients.\nWe argue that these are facets of a general sparsification method that can\noperate on any possible atomic decomposition. Notable examples include\nelement-wise, singular value, and Fourier decompositions. We present ATOMO, a\ngeneral framework for atomic sparsification of stochastic gradients. Given a\ngradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random\nunbiased sparsification of the atoms minimizing variance. We show that recent\nmethods such as QSGD and TernGrad are special cases of ATOMO and that\nsparsifiying the singular value decomposition of neural networks gradients,\nrather than their coordinates, can lead to significantly faster distributed\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 16:23:14 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 03:49:12 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 20:04:34 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Wang", "Hongyi", ""], ["Sievert", "Scott", ""], ["Charles", "Zachary", ""], ["Liu", "Shengchao", ""], ["Wright", "Stephen", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1806.04166", "submitter": "Jun-Ting Hsieh", "authors": "Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li Fei-Fei, Juan Carlos\n  Niebles", "title": "Learning to Decompose and Disentangle Representations for Video\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to predict future video frames given a sequence of input frames.\nDespite large amounts of video data, this remains a challenging task because of\nthe high-dimensionality of video frames. We address this challenge by proposing\nthe Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework\nthat combines structured probabilistic models and deep networks to\nautomatically (i) decompose the high-dimensional video that we aim to predict\ninto components, and (ii) disentangle each component to have low-dimensional\ntemporal dynamics that are easier to predict. Crucially, with an appropriately\nspecified generative model of video frames, our DDPAE is able to learn both the\nlatent decomposition and disentanglement without explicit supervision. For the\nMoving MNIST dataset, we show that DDPAE is able to recover the underlying\ncomponents (individual digits) and disentanglement (appearance and location) as\nwe would intuitively do. We further demonstrate that DDPAE can be applied to\nthe Bouncing Balls dataset involving complex interactions between multiple\nobjects to predict the video frame directly from the pixels and recover\nphysical states without explicit supervision.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:12:59 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 18:44:19 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Hsieh", "Jun-Ting", ""], ["Liu", "Bingbin", ""], ["Huang", "De-An", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1806.04169", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow", "title": "Defense Against the Dark Arts: An overview of adversarial example\n  security research and future research directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a summary of a keynote lecture at the Deep Learning\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\nthe state of the art in defenses against adversarial examples and provides\nrecommendations for future research directions on this topic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:22:45 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Goodfellow", "Ian", ""]]}, {"id": "1806.04205", "submitter": "Ankur Taly", "authors": "Mukund Sundararajan and Ankur Taly", "title": "A Note about: Local Explanation Methods for Deep Neural Networks lack\n  Sensitivity to Parameter Values", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local explanation methods, also known as attribution methods, attribute a\ndeep network's prediction to its input (cf. Baehrens et al. (2010)). We respond\nto the claim from Adebayo et al. (2018) that local explanation methods lack\nsensitivity, i.e., DNNs with randomly-initialized weights produce explanations\nthat are both visually and quantitatively similar to those produced by DNNs\nwith learned weights.\n  Further investigation reveals that their findings are due to two choices in\ntheir analysis: (a) ignoring the signs of the attributions; and (b) for\nintegrated gradients (IG), including pixels in their analysis that have zero\nattributions by choice of the baseline (an auxiliary input relative to which\nthe attributions are computed). When both factors are accounted for, IG\nattributions for a random network and the actual network are uncorrelated. Our\ninvestigation also sheds light on how these issues affect visualizations,\nalthough we note that more work is needed to understand how viewers interpret\nthe difference between the random and the actual attributions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:20:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sundararajan", "Mukund", ""], ["Taly", "Ankur", ""]]}, {"id": "1806.04207", "submitter": "Shi Pu", "authors": "Shi Pu and Alfredo Garcia", "title": "Swarming for Faster Convergence in Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed framework for stochastic optimization which is\ninspired by models of collective motion found in nature (e.g., swarming) with\nmild communication requirements. Specifically, we analyze a scheme in which\neach one of $N > 1$ independent threads, implements in a distributed and\nunsynchronized fashion, a stochastic gradient-descent algorithm which is\nperturbed by a swarming potential. Assuming the overhead caused by\nsynchronization is not negligible, we show the swarming-based approach exhibits\nbetter performance than a centralized algorithm (based upon the average of $N$\nobservations) in terms of (real-time) convergence speed. We also derive an\nerror bound that is monotone decreasing in network size and connectivity. We\ncharacterize the scheme's finite-time performances for both convex and\nnon-convex objective functions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:24:59 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 20:02:49 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Pu", "Shi", ""], ["Garcia", "Alfredo", ""]]}, {"id": "1806.04209", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Amy Kuceyeski and Mert Sabuncu", "title": "3D Convolutional Neural Networks for Classification of Functional\n  Connectomes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a\ndiagnostic or prognostic tool for a wide variety of conditions, such as autism,\nAlzheimer's disease, and stroke. While a growing number of studies have\ndemonstrated the promise of machine learning algorithms for rs-fMRI based\nclinical or behavioral prediction, most prior models have been limited in their\ncapacity to exploit the richness of the data. For example, classification\ntechniques applied to rs-fMRI often rely on region-based summary statistics\nand/or linear models. In this work, we propose a novel volumetric Convolutional\nNeural Network (CNN) framework that takes advantage of the full-resolution 3D\nspatial structure of rs-fMRI data and fits non-linear predictive models. We\nshowcase our approach on a challenging large-scale dataset (ABIDE, with N >\n2,000) and report state-of-the-art accuracy results on rs-fMRI-based\ndiscrimination of autism patients and healthy controls.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:30:20 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 17:15:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert", ""]]}, {"id": "1806.04214", "submitter": "Jean-Gabriel Young", "authors": "Jean-Gabriel Young, Guillaume St-Onge, Patrick Desrosiers, Louis J.\n  Dub\\'e", "title": "Universality of the stochastic block model", "comments": "13 pages, 4 figures", "journal-ref": "Phys. Rev. E 98, 032309 (2018)", "doi": "10.1103/PhysRevE.98.032309", "report-no": null, "categories": "physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesoscopic pattern extraction (MPE) is the problem of finding a partition of\nthe nodes of a complex network that maximizes some objective function. Many\nwell-known network inference problems fall in this category, including, for\ninstance, community detection, core-periphery identification, and imperfect\ngraph coloring. In this paper, we show that the most popular algorithms\ndesigned to solve MPE problems can in fact be understood as special cases of\nthe maximum likelihood formulation of the stochastic block model (SBM), or one\nof its direct generalizations. These equivalence relations show that the SBM is\nnearly universal with respect to MPE problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:43:59 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 18:59:08 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Young", "Jean-Gabriel", ""], ["St-Onge", "Guillaume", ""], ["Desrosiers", "Patrick", ""], ["Dub\u00e9", "Louis J.", ""]]}, {"id": "1806.04242", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "The Potential of the Return Distribution for Exploration in RL", "comments": "Published at the Exploration in Reinforcement Learning Workshop at\n  the 35th International Conference on Machine Learning, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the potential of the return distribution for exploration\nin deterministic reinforcement learning (RL) environments. We study network\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\nmixture distributions. Combined with exploration policies that leverage this\nreturn distribution, we solve, for example, a randomized Chain task of length\n100, which has not been reported before when learning with neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 21:02:50 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 18:49:35 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1806.04245", "submitter": "Xingyuan Pan", "authors": "Xingyuan Pan and Vivek Srikumar", "title": "Learning to Speed Up Structured Output Prediction", "comments": "International Conference on Machine Learning, Stockholm, Sweden, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting structured outputs can be computationally onerous due to the\ncombinatorially large output spaces. In this paper, we focus on reducing the\nprediction time of a trained black-box structured classifier without losing\naccuracy. To do so, we train a speedup classifier that learns to mimic a\nblack-box classifier under the learning-to-search approach. As the structured\nclassifier predicts more examples, the speedup classifier will operate as a\nlearned heuristic to guide search to favorable regions of the output space. We\npresent a mistake bound for the speedup classifier and identify inference\nsituations where it can independently make correct judgments without input\nfeatures. We evaluate our method on the task of entity and relation extraction\nand show that the speedup classifier outperforms even greedy search in terms of\nspeed without loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 21:06:51 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Pan", "Xingyuan", ""], ["Srikumar", "Vivek", ""]]}, {"id": "1806.04308", "submitter": "Chapman Siu", "authors": "Chapman Siu and Richard Yi Da Xu", "title": "Diverse Online Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online feature selection has been an active research area in recent years. We\npropose a novel diverse online feature selection method based on Determinantal\nPoint Processes (DPP). Our model aims to provide diverse features which can be\ncomposed in either a supervised or unsupervised framework. The framework aims\nto promote diversity based on the kernel produced on a feature level, through\nat most three stages: feature sampling, local criteria and global criteria for\nfeature selection. In the feature sampling, we sample incoming stream of\nfeatures using conditional DPP. The local criteria is used to assess and select\nstreamed features (i.e. only when they arrive), we use unsupervised scale\ninvariant methods to remove redundant features and optionally supervised\nmethods to introduce label information to assess relevant features. Lastly, the\nglobal criteria uses regularization methods to select a global optimal subset\nof features. This three stage procedure continues until there are no more\nfeatures arriving or some predefined stopping condition is met. We demonstrate\nbased on experiments conducted on that this approach yields better compactness,\nis comparable and in some instances outperforms other state-of-the-art online\nfeature selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 02:55:05 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 03:09:30 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 03:41:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Siu", "Chapman", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1806.04310", "submitter": "Amirali Aghazadeh", "authors": "Amirali Aghazadeh, Ryan Spring, Daniel LeJeune, Gautam Dasarathy,\n  Anshumali Shrivastava, Richard G. Baraniuk", "title": "MISSION: Ultra Large-Scale Feature Selection using Count-Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 03:03:13 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Aghazadeh", "Amirali", ""], ["Spring", "Ryan", ""], ["LeJeune", "Daniel", ""], ["Dasarathy", "Gautam", ""], ["Shrivastava", "Anshumali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1806.04321", "submitter": "Haichuan Yang", "authors": "Haichuan Yang, Yuhao Zhu, Ji Liu", "title": "Energy-Constrained Compression for Deep Neural Networks via Weighted\n  Sparse Projection and Layer Input Masking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are increasingly deployed in highly\nenergy-constrained environments such as autonomous drones and wearable devices\nwhile at the same time must operate in real-time. Therefore, reducing the\nenergy consumption has become a major design consideration in DNN training.\nThis paper proposes the first end-to-end DNN training framework that provides\nquantitative energy consumption guarantees via weighted sparse projection and\ninput masking. The key idea is to formulate the DNN training as an optimization\nproblem in which the energy budget imposes a previously unconsidered\noptimization constraint. We integrate the quantitative DNN energy estimation\ninto the DNN training process to assist the constrained optimization. We prove\nthat an approximate algorithm can be used to efficiently solve the optimization\nproblem. Compared to the best prior energy-saving methods, our framework trains\nDNNs that provide higher accuracies under same or lower energy budgets. Code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:10:06 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 04:53:58 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 23:05:40 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yang", "Haichuan", ""], ["Zhu", "Yuhao", ""], ["Liu", "Ji", ""]]}, {"id": "1806.04326", "submitter": "Shengyang Sun", "authors": "Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li,\n  Roger Grosse", "title": "Differentiable Compositional Kernel Learning for Gaussian Processes", "comments": "ICML 2018; update proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization properties of Gaussian processes depend heavily on the\nchoice of kernel, and this choice remains a dark art. We present the Neural\nKernel Network (NKN), a flexible family of kernels represented by a neural\nnetwork. The NKN architecture is based on the composition rules for kernels, so\nthat each unit of the network corresponds to a valid kernel. It can compactly\napproximate compositional kernel structures such as those used by the Automatic\nStatistician (Lloyd et al., 2014), but because the architecture is\ndifferentiable, it is end-to-end trainable with gradient-based optimization. We\nshow that the NKN is universal for the class of stationary kernels. Empirically\nwe demonstrate pattern discovery and extrapolation abilities of NKN on several\ntasks that depend crucially on identifying the underlying structure, including\ntime series and texture extrapolation, as well as Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:21:53 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 03:43:45 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 07:53:09 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Sun", "Shengyang", ""], ["Zhang", "Guodong", ""], ["Wang", "Chaoqi", ""], ["Zeng", "Wenyuan", ""], ["Li", "Jiaman", ""], ["Grosse", "Roger", ""]]}, {"id": "1806.04339", "submitter": "Tengyu Xu", "authors": "Tengyu Xu, Yi Zhou, Kaiyi Ji, Yingbin Liang", "title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU\n  Models?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the implicit bias of gradient descent methods in solving a binary\nclassification problem over a linearly separable dataset. The classifier is\ndescribed by a nonlinear ReLU model and the objective function adopts the\nexponential loss function. We first characterize the landscape of the loss\nfunction and show that there can exist spurious asymptotic local minima besides\nasymptotic global minima. We then show that gradient descent (GD) can converge\nto either a global or a local max-margin direction, or may diverge from the\ndesired max-margin direction in a general context. For stochastic gradient\ndescent (SGD), we show that it converges in expectation to either the global or\nthe local max-margin direction if SGD converges. We further explore the\nimplicit bias of these algorithms in learning a multi-neuron network under\ncertain stationary conditions, and show that the learned classifier maximizes\nthe margins of each sample pattern partition under the ReLU activation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:46:09 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 21:34:16 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Xu", "Tengyu", ""], ["Zhou", "Yi", ""], ["Ji", "Kaiyi", ""], ["Liang", "Yingbin", ""]]}, {"id": "1806.04342", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam\n  Trischler, Yoshua Bengio, Joelle Pineau, Laurent Charlin, Chris Pal", "title": "Focused Hierarchical RNNs for Conditional Sequence Processing", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) with attention mechanisms have obtained\nstate-of-the-art results for many sequence processing tasks. Most of these\nmodels use a simple form of encoder with attention that looks over the entire\nsequence and assigns a weight to each token independently. We present a\nmechanism for focusing RNN encoders for sequence modelling tasks which allows\nthem to attend to key parts of the input as needed. We formulate this using a\nmulti-layer conditional sequence encoder that reads in one token at a time and\nmakes a discrete decision on whether the token is relevant to the context or\nquestion being asked. The discrete gating mechanism takes in the context\nembedding and the current hidden state as inputs and controls information flow\ninto the layer above. We train it using policy gradient methods. We evaluate\nthis method on several types of tasks with different attributes. First, we\nevaluate the method on synthetic tasks which allow us to evaluate the model for\nits generalization ability and probe the behavior of the gates in more\ncontrolled settings. We then evaluate this approach on large scale Question\nAnswering tasks including the challenging MS MARCO and SearchQA tasks. Our\nmodels shows consistent improvements for both tasks over prior work and our\nbaselines. It has also shown to generalize significantly better on synthetic\ntasks as compared to the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:54:37 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Zolna", "Konrad", ""], ["Sordoni", "Alessandro", ""], ["Lin", "Zhouhan", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""], ["Pineau", "Joelle", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""]]}, {"id": "1806.04344", "submitter": "Johannes Otterbach", "authors": "Johannes S. Otterbach", "title": "Optimizing Variational Quantum Circuits using Evolution Strategies", "comments": "This version withdrawn by arXiv administrators because the submitter\n  did not have the right to agree to our license at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version withdrawn by arXiv administrators because the submitter did not\nhave the right to agree to our license at the time of submission.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:57:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Otterbach", "Johannes S.", ""]]}, {"id": "1806.04398", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Andreea Deac, Petar Veli\\v{c}kovi\\'c, Pietro Sormanni", "title": "Attentive cross-modal paratope prediction", "comments": "To appear at the 2018 ICML/IJCAI Workshop on Computational Biology. 5\n  pages, 6 figures", "journal-ref": null, "doi": "10.1089/cmb.2018.0175", "report-no": null, "categories": "stat.ML cs.LG q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antibodies are a critical part of the immune system, having the function of\ndirectly neutralising or tagging undesirable objects (the antigens) for future\ndestruction. Being able to predict which amino acids belong to the paratope,\nthe region on the antibody which binds to the antigen, can facilitate antibody\ndesign and contribute to the development of personalised medicine. The\nsuitability of deep neural networks has recently been confirmed for this task,\nwith Parapred outperforming all prior physical models. Our contribution is\ntwofold: first, we significantly outperform the computational efficiency of\nParapred by leveraging \\`a trous convolutions and self-attention. Secondly, we\nimplement cross-modal attention by allowing the antibody residues to attend\nover antigen residues. This leads to new state-of-the-art results on this task,\nalong with insightful interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 08:58:19 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Deac", "Andreea", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Sormanni", "Pietro", ""]]}, {"id": "1806.04418", "submitter": "Titouan Parcollet", "authors": "Titouan Parcollet, Mirco Ravanelli, Mohamed Morchid, Georges\n  Linar\\`es, Chiheb Trabelsi, Renato De Mori and Yoshua Bengio", "title": "Quaternion Recurrent Neural Networks", "comments": "ICLR Update - Full rework", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are powerful architectures to model\nsequential data, due to their capability to learn short and long-term\ndependencies between the basic elements of a sequence. Nonetheless, popular\ntasks such as speech or images recognition, involve multi-dimensional input\nfeatures that are characterized by strong internal dependencies between the\ndimensions of the input vector. We propose a novel quaternion recurrent neural\nnetwork (QRNN), alongside with a quaternion long-short term memory neural\nnetwork (QLSTM), that take into account both the external relations and these\ninternal structural dependencies with the quaternion algebra. Similarly to\ncapsules, quaternions allow the QRNN to code internal dependencies by composing\nand processing multidimensional features as single entities, while the\nrecurrent operation reveals correlations between the elements composing the\nsequence. We show that both QRNN and QLSTM achieve better performances than RNN\nand LSTM in a realistic application of automatic speech recognition. Finally,\nwe show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of\nfree parameters needed, compared to real-valued RNNs and LSTMs to reach better\nresults, leading to a more compact representation of the relevant information.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:49:40 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 09:58:00 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 10:24:11 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Parcollet", "Titouan", ""], ["Ravanelli", "Mirco", ""], ["Morchid", "Mohamed", ""], ["Linar\u00e8s", "Georges", ""], ["Trabelsi", "Chiheb", ""], ["De Mori", "Renato", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.04449", "submitter": "Eric Tramel", "authors": "Mikhail Zaslavskiy, Simon J\\'egou, Eric W. Tramel, Gilles Wainrib", "title": "ToxicBlend: Virtual Screening of Toxic Compounds with Ensemble\n  Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely assessment of compound toxicity is one of the biggest challenges\nfacing the pharmaceutical industry today. A significant proportion of compounds\nidentified as potential leads are ultimately discarded due to the toxicity they\ninduce. In this paper, we propose a novel machine learning approach for the\nprediction of molecular activity on ToxCast targets. We combine extreme\ngradient boosting with fully-connected and graph-convolutional neural network\narchitectures trained on QSAR physical molecular property descriptors, PubChem\nmolecular fingerprints, and SMILES sequences. Our ensemble predictor leverages\nthe strengths of each individual technique, significantly outperforming\nexisting state-of-the art models on the ToxCast and Tox21 toxicity-prediction\ndatasets. We provide free access to molecule toxicity prediction using our\nmodel at http://www.owkin.com/toxicblend.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 11:46:35 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Zaslavskiy", "Mikhail", ""], ["J\u00e9gou", "Simon", ""], ["Tramel", "Eric W.", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1806.04458", "submitter": "Mayumi Ohta", "authors": "Artem Sokolov, Julian Hitschler, Mayumi Ohta, Stefan Riezler", "title": "Sparse Stochastic Zeroth-Order Optimization with an Application to\n  Bandit Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic zeroth-order (SZO), or gradient-free, optimization allows to\noptimize arbitrary functions by relying only on function evaluations under\nparameter perturbations, however, the iteration complexity of SZO methods\nsuffers a factor proportional to the dimensionality of the perturbed function.\nWe show that in scenarios with natural sparsity patterns as in structured\nprediction applications, this factor can be reduced to the expected number of\nactive features over input-output pairs. We give a general proof that applies\nsparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic\nobjectives, and present an experimental evaluation on linear bandit structured\nprediction tasks with sparse word-based feature representations that confirm\nour theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:16:54 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 10:31:14 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 11:11:28 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sokolov", "Artem", ""], ["Hitschler", "Julian", ""], ["Ohta", "Mayumi", ""], ["Riezler", "Stefan", ""]]}, {"id": "1806.04465", "submitter": "Ilya Feige", "authors": "Benoit Gaujac, Ilya Feige, David Barber", "title": "Gaussian mixture models with Wasserstein distance", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models with both discrete and continuous latent variables are\nhighly motivated by the structure of many real-world data sets. They present,\nhowever, subtleties in training often manifesting in the discrete latent being\nunder leveraged. In this paper, we show that such models are more amenable to\ntraining when using the Optimal Transport framework of Wasserstein\nAutoencoders. We find our discrete latent variable to be fully leveraged by the\nmodel when trained, without any modifications to the objective function or\nsignificant fine tuning. Our model generates comparable samples to other\napproaches while using relatively simple neural networks, since the discrete\nlatent variable carries much of the descriptive burden. Furthermore, the\ndiscrete latent provides significant control over generation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:35:10 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gaujac", "Benoit", ""], ["Feige", "Ilya", ""], ["Barber", "David", ""]]}, {"id": "1806.04472", "submitter": "Sebastian Jaimungal", "authors": "Philippe Casgrain, Sebastian Jaimungal", "title": "Trading algorithms with learning in latent alpha models", "comments": "42 pages, 5 figures", "journal-ref": "Mathematical Finance, Forthcoming, 2018", "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.ST q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alpha signals for statistical arbitrage strategies are often driven by latent\nfactors. This paper analyses how to optimally trade with latent factors that\ncause prices to jump and diffuse. Moreover, we account for the effect of the\ntrader's actions on quoted prices and the prices they receive from trading.\nUnder fairly general assumptions, we demonstrate how the trader can learn the\nposterior distribution over the latent states, and explicitly solve the latent\noptimal trading problem. We provide a verification theorem, and a methodology\nfor calibrating the model by deriving a variation of the\nexpectation-maximization algorithm. To illustrate the efficacy of the optimal\nstrategy, we demonstrate its performance through simulations and compare it to\nstrategies which ignore learning in the latent factors. We also provide\ncalibration results for a particular model using Intel Corporation stock as an\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:46:30 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Casgrain", "Philippe", ""], ["Jaimungal", "Sebastian", ""]]}, {"id": "1806.04480", "submitter": "Ilya Feige", "authors": "Alex Mansbridge, Roberto Fierimonte, Ilya Feige, David Barber", "title": "Improving latent variable descriptiveness with AutoGen", "comments": "8 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful generative models, particularly in Natural Language Modelling, are\ncommonly trained by maximizing a variational lower bound on the data log\nlikelihood. These models often suffer from poor use of their latent variable,\nwith ad-hoc annealing factors used to encourage retention of information in the\nlatent variable. We discuss an alternative and general approach to latent\nvariable modelling, based on an objective that combines the data log likelihood\nas well as the likelihood of a perfect reconstruction through an autoencoder.\nTying these together ensures by design that the latent variable captures\ninformation about the observations, whilst retaining the ability to generate\nwell. Interestingly, though this approach is a priori unrelated to VAEs, the\nlower bound attained is identical to the standard VAE bound but with the\naddition of a simple pre-factor; thus, providing a formal interpretation of the\ncommonly used, ad-hoc pre-factors in training VAEs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:02:19 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Mansbridge", "Alex", ""], ["Fierimonte", "Roberto", ""], ["Feige", "Ilya", ""], ["Barber", "David", ""]]}, {"id": "1806.04498", "submitter": "Yasin Yaz{\\i}c{\\i}", "authors": "Yasin Yaz{\\i}c{\\i}, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap,\n  Georgios Piliouras, Vijay Chandrasekhar", "title": "The Unusual Effectiveness of Averaging in GAN Training", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine two different techniques for parameter averaging in GAN training.\nMoving Average (MA) computes the time-average of parameters, whereas\nExponential Moving Average (EMA) computes an exponentially discounted sum.\nWhilst MA is known to lead to convergence in bilinear settings, we provide the\n-- to our knowledge -- first theoretical arguments in support of EMA. We show\nthat EMA converges to limit cycles around the equilibrium with vanishing\namplitude as the discount parameter approaches one for simple bilinear games\nand also enhances the stability of general GAN training. We establish\nexperimentally that both techniques are strikingly effective in the\nnon-convex-concave GAN setting as well. Both improve inception and FID scores\non different architectures and for different GAN objectives. We provide\ncomprehensive experimental results across a range of datasets -- mixture of\nGaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its\neffectiveness. We achieve state-of-the-art results on CIFAR-10 and produce\nclean CelebA face images.\\footnote{~The code is available at\n\\url{https://github.com/yasinyazici/EMA_GAN}}\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:27:23 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 12:17:11 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Yaz\u0131c\u0131", "Yasin", ""], ["Foo", "Chuan-Sheng", ""], ["Winkler", "Stefan", ""], ["Yap", "Kim-Hui", ""], ["Piliouras", "Georgios", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1806.04509", "submitter": "Amaia Abanda", "authors": "Amaia Abanda, Usue Mori, Jose A. Lozano", "title": "A review on distance based time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification is an increasing research topic due to the vast\namount of time series data that are being created over a wide variety of\nfields. The particularity of the data makes it a challenging task and different\napproaches have been taken, including the distance based approach. 1-NN has\nbeen a widely used method within distance based time series classification due\nto it simplicity but still good performance. However, its supremacy may be\nattributed to being able to use specific distances for time series within the\nclassification process and not to the classifier itself. With the aim of\nexploiting these distances within more complex classifiers, new approaches have\narisen in the past few years that are competitive or which outperform the 1-NN\nbased approaches. In some cases, these new methods use the distance measure to\ntransform the series into feature vectors, bridging the gap between time series\nand traditional classifiers. In other cases, the distances are employed to\nobtain a time series kernel and enable the use of kernel methods for time\nseries classification. One of the main challenges is that a kernel function\nmust be positive semi-definite, a matter that is also addressed within this\nreview. The presented review includes a taxonomy of all those methods that aim\nto classify time series using a distance based approach, as well as a\ndiscussion of the strengths and weaknesses of each method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:40:30 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Abanda", "Amaia", ""], ["Mori", "Usue", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1806.04517", "submitter": "Akash Malhotra", "authors": "Akash Malhotra", "title": "A hybrid econometric-machine learning approach for relative importance\n  analysis: Prioritizing food policy", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.08789", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A measure of relative importance of variables is often desired by researchers\nwhen the explanatory aspects of econometric methods are of interest. To this\nend, the author briefly reviews the limitations of conventional econometrics in\nconstructing a reliable measure of variable importance. The author highlights\nthe relative stature of explanatory and predictive analysis in economics and\nthe emergence of fruitful collaborations between econometrics and computer\nscience. Learning lessons from both, the author proposes a hybrid approach\nbased on conventional econometrics and advanced machine learning (ML)\nalgorithms, which are otherwise, used in predictive analytics. The purpose of\nthis article is two-fold, to propose a hybrid approach to assess relative\nimportance and demonstrate its applicability in addressing policy priority\nissues with an example of food inflation in India, followed by a broader aim to\nintroduce the possibility of conflation of ML and conventional econometrics to\nan audience of researchers in economics and social sciences, in general.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 10:17:58 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 17:41:05 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 16:45:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Malhotra", "Akash", ""]]}, {"id": "1806.04522", "submitter": "Wenbo Gong", "authors": "Wenbo Gong, Yingzhen Li, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Meta-Learning for Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become\nincreasingly popular for simulating posterior samples in large-scale Bayesian\nmodeling. However, existing SG-MCMC schemes are not tailored to any specific\nprobabilistic model, even a simple modification of the underlying dynamical\nsystem requires significant physical intuition. This paper presents the first\nmeta-learning algorithm that allows automated design for the underlying\ncontinuous dynamics of an SG-MCMC sampler. The learned sampler generalizes\nHamiltonian dynamics with state-dependent drift and diffusion, enabling fast\ntraversal and efficient exploration of neural network energy landscapes.\nExperiments validate the proposed approach on both Bayesian fully connected\nneural network and Bayesian recurrent neural network tasks, showing that the\nlearned sampler out-performs generic, hand-designed SG-MCMC algorithms, and\ngeneralizes to different datasets and larger architectures.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:50:59 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gong", "Wenbo", ""], ["Li", "Yingzhen", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1806.04542", "submitter": "Charlie Frogner", "authors": "Charlie Frogner and Tomaso Poggio", "title": "Approximate inference with Wasserstein gradient flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approximate inference method for diffusion processes,\nbased on the Wasserstein gradient flow formulation of the diffusion. In this\nformulation, the time-dependent density of the diffusion is derived as the\nlimit of implicit Euler steps that follow the gradients of a particular free\nenergy functional. Existing methods for computing Wasserstein gradient flows\nrely on discretization of the domain of the diffusion, prohibiting their\napplication to domains in more than several dimensions. We propose instead a\ndiscretization-free inference method that computes the Wasserstein gradient\nflow directly in a space of continuous functions. We characterize approximation\nproperties of the proposed method and evaluate it on a nonlinear filtering\ntask, finding performance comparable to the state-of-the-art for filtering\ndiffusions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:08:16 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Frogner", "Charlie", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1806.04549", "submitter": "Maria H\\\"ugle", "authors": "Maria H\\\"ugle, Simon Heller, Manuel Watter, Manuel Blum, Farrokh\n  Manzouri, Matthias D\\\"umpelmann, Andreas Schulze-Bonhage, Peter Woias,\n  Joschka Boedecker", "title": "Early Seizure Detection with an Energy-Efficient Convolutional Neural\n  Network on an Implantable Microcontroller", "comments": "Accepted at IJCNN 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489493", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implantable, closed-loop devices for automated early detection and\nstimulation of epileptic seizures are promising treatment options for patients\nwith severe epilepsy that cannot be treated with traditional means. Most\napproaches for early seizure detection in the literature are, however, not\noptimized for implementation on ultra-low power microcontrollers required for\nlong-term implantation. In this paper we present a convolutional neural network\nfor the early detection of seizures from intracranial EEG signals, designed\nspecifically for this purpose. In addition, we investigate approximations to\ncomply with hardware limits while preserving accuracy. We compare our approach\nto three previously proposed convolutional neural networks and a feature-based\nSVM classifier with respect to detection accuracy, latency and computational\nneeds. Evaluation is based on a comprehensive database with long-term EEG\nrecordings. The proposed method outperforms the other detectors with a median\nsensitivity of 0.96, false detection rate of 10.1 per hour and median detection\ndelay of 3.7 seconds, while being the only approach suited to be realized on a\nlow power microcontroller due to its parsimonious use of computational and\nmemory resources.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:15:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["H\u00fcgle", "Maria", ""], ["Heller", "Simon", ""], ["Watter", "Manuel", ""], ["Blum", "Manuel", ""], ["Manzouri", "Farrokh", ""], ["D\u00fcmpelmann", "Matthias", ""], ["Schulze-Bonhage", "Andreas", ""], ["Woias", "Peter", ""], ["Boedecker", "Joschka", ""]]}, {"id": "1806.04550", "submitter": "Florian Schmidt", "authors": "Florian Schmidt and Thomas Hofmann", "title": "Deep State Space Models for Unconditional Word Generation", "comments": "NIPS camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive feedback is considered a necessity for successful\nunconditional text generation using stochastic sequence models. However, such\nfeedback is known to introduce systematic biases into the training process and\nit obscures a principle of generation: committing to global information and\nforgetting local nuances. We show that a non-autoregressive deep state space\nmodel with a clear separation of global and local uncertainty can be built from\nonly two ingredients: An independent noise source and a deterministic\ntransition function. Recent advances on flow-based variational inference can be\nused to train an evidence lower-bound without resorting to annealing, auxiliary\nlosses or similar measures. The result is a highly interpretable generative\nmodel on par with comparable auto-regressive models on the task of word\ngeneration.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:19:48 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 19:07:26 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Schmidt", "Florian", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1806.04552", "submitter": "Sreecharan Sankaranarayanan", "authors": "Sreecharan Sankaranarayanan, Raghuram Mandyam Annasamy, Katia Sycara,\n  Carolyn Penstein Ros\\'e", "title": "Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\n  Exploration", "comments": "Submitted to the Thirty-Second Annual Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-Ensembles are a model-free approach where input images are fed into\ndifferent Q-networks and exploration is driven by the assumption that\nuncertainty is proportional to the variance of the output Q-values obtained.\nThey have been shown to perform relatively well compared to other exploration\nstrategies. Further, model-based approaches, such as encoder-decoder models\nhave been used successfully for next frame prediction given previous frames.\nThis paper proposes to integrate the model-free Q-ensembles and model-based\napproaches with the hope of compounding the benefits of both and achieving\nsuperior exploration as a result. Results show that a model-based trajectory\nmemory approach when combined with Q-ensembles produces superior performance\nwhen compared to only using Q-ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:24:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sankaranarayanan", "Sreecharan", ""], ["Annasamy", "Raghuram Mandyam", ""], ["Sycara", "Katia", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "1806.04555", "submitter": "Bob Vanderheyden", "authors": "Bob Vanderheyden and Jennifer Priestley", "title": "Logistic Ensemble Models", "comments": "Presented at 30Th Annual Conference Of The International Academy Of\n  Business Disciplines", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models that are developed in a regulated industry or a regulated\napplication, like determination of credit worthiness, must be interpretable and\nrational (e.g., meaningful improvements in basic credit behavior must result in\nimproved credit worthiness scores). Machine Learning technologies provide very\ngood performance with minimal analyst intervention, making them well suited to\na high volume analytic environment, but the majority are black box tools that\nprovide very limited insight or interpretability into key drivers of model\nperformance or predicted model output values. This paper presents a methodology\nthat blends one of the most popular predictive statistical modeling methods for\nbinary classification with a core model enhancement strategy found in machine\nlearning. The resulting prediction methodology provides solid performance, from\nminimal analyst effort, while providing the interpretability and rationality\nrequired in regulated industries, as well as in other environments where\ninterpretation of model parameters is required (e.g. businesses that require\ninterpretation of models, to take action on them).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:27:21 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Vanderheyden", "Bob", ""], ["Priestley", "Jennifer", ""]]}, {"id": "1806.04561", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida", "title": "An Extension of Averaged-Operator-Based Algorithms", "comments": "26th Eur. Signal Process. Conf. (EUSIPCO 2018), accepted. 5 pages, 1\n  figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the algorithms used to solve minimization problems with\nsparsity-inducing regularizers are generic in the sense that they do not take\ninto account the sparsity of the solution in any particular way. However,\nalgorithms known as semismooth Newton are able to take advantage of this\nsparsity to accelerate their convergence. We show how to extend these\nalgorithms in different directions, and study the convergence of the resulting\nalgorithms by showing that they are a particular case of an extension of the\nwell-known Krasnosel'ski\\u{\\i}--Mann scheme.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:35:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""]]}, {"id": "1806.04562", "submitter": "Thanh Thi Nguyen", "authors": "Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi", "title": "Multi-Agent Deep Reinforcement Learning with Human Strategies", "comments": "2019 IEEE International Conference on Industrial Technology (ICIT),\n  Melbourne, Australia", "journal-ref": "2019 IEEE International Conference on Industrial Technology (ICIT)", "doi": "10.1109/ICIT.2019.8755032", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled traditional reinforcement learning methods to deal\nwith high-dimensional problems. However, one of the disadvantages of deep\nreinforcement learning methods is the limited exploration capacity of learning\nagents. In this paper, we introduce an approach that integrates human\nstrategies to increase the exploration capacity of multiple deep reinforcement\nlearning agents. We also report the development of our own multi-agent\nenvironment called Multiple Tank Defence to simulate the proposed approach. The\nresults show the significant performance improvement of multiple agents that\nhave learned cooperatively with human strategies. This implies that there is a\ncritical need for human intellect teamed with machines to solve complex\nproblems. In addition, the success of this simulation indicates that our\nmulti-agent environment can be used as a testbed platform to develop and\nvalidate other multi-agent control algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:40:24 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 05:58:28 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Nguyen", "Thanh", ""], ["Nguyen", "Ngoc Duy", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1806.04577", "submitter": "Abhishek Bansal", "authors": "Abhishek Bansal, Abhinav Anand, Chiranjib Bhattacharyya", "title": "Using Inherent Structures to design Lean 2-layer RBMs", "comments": "ICML 2018 (Full Paper + Long Talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the representational power of Restricted Boltzmann Machines\n(RBMs) with multiple layers is an ill-understood problem and is an area of\nactive research. Motivated from the approach of \\emph{Inherent Structure\nformalism} (Stillinger & Weber, 1982), extensively used in analysing Spin\nGlasses, we propose a novel measure called \\emph{Inherent Structure Capacity}\n(ISC), which characterizes the representation capacity of a fixed architecture\nRBM by the expected number of modes of distributions emanating from the RBM\nwith parameters drawn from a prior distribution. Though ISC is intractable, we\nshow that for a single layer RBM architecture ISC approaches a finite constant\nas number of hidden units are increased and to further improve the ISC, one\nneeds to add a second layer. Furthermore, we introduce \\emph{Lean} RBMs, which\nare multi-layer RBMs where each layer can have at-most $O(n)$ units with the\nnumber of visible units being n. We show that for every single layer RBM with\n$\\Omega(n^{2+r}), r \\ge 0$, hidden units there exists a two-layered \\emph{lean}\nRBM with $\\Theta(n^2)$ parameters with the same ISC, establishing that 2 layer\nRBMs can achieve the same representational power as single-layer RBMs but using\nfar fewer number of parameters. To the best of our knowledge, this is the first\nresult which quantitatively establishes the need for layering.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:55:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Bansal", "Abhishek", ""], ["Anand", "Abhinav", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "1806.04594", "submitter": "Sudeep Raja Putta", "authors": "Sudeep Raja Putta and Abhishek Shetty", "title": "Exponential Weights on the Hypercube in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general online linear optimization problem(OLO). At each round, a\nsubset of objects from a fixed universe of $n$ objects is chosen, and a linear\ncost associated with the chosen subset is incurred. To measure the performance\nof our algorithms, we use the notion of regret which is the difference between\nthe total cost incurred over all iterations and the cost of the best fixed\nsubset in hindsight. We consider Full Information and Bandit feedback for this\nproblem. This problem is equivalent to OLO on the $\\{0,1\\}^n$ hypercube. The\nExp2 algorithm and its bandit variant are commonly used strategies for this\nproblem. It was previously unknown if it is possible to run Exp2 on the\nhypercube in polynomial time.\n  In this paper, we present a polynomial time algorithm called PolyExp for OLO\non the hypercube. We show that our algorithm is equivalent Exp2 on $\\{0,1\\}^n$,\nOnline Mirror Descent(OMD), Follow The Regularized Leader(FTRL) and Follow The\nPerturbed Leader(FTPL) algorithms. We show PolyExp achieves expected regret\nbound that is a factor of $\\sqrt{n}$ better than Exp2 in the full information\nsetting under $L_\\infty$ adversarial losses. Because of the equivalence of\nthese algorithms, this implies an improvement on Exp2's regret bound in full\ninformation. We also show matching regret lower bounds. Finally, we show how to\nuse PolyExp on the $\\{-1,+1\\}^n$ hypercube, solving an open problem in Bubeck\net al (COLT 2012).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:12:48 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 09:32:43 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 12:07:19 GMT"}, {"version": "v4", "created": "Mon, 3 Dec 2018 18:53:47 GMT"}, {"version": "v5", "created": "Thu, 22 Aug 2019 22:00:59 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Putta", "Sudeep Raja", ""], ["Shetty", "Abhishek", ""]]}, {"id": "1806.04609", "submitter": "Laura Balzano", "authors": "Laura Balzano, Yuejie Chi, Yue M. Lu", "title": "Streaming PCA and Subspace Tracking: The Missing Data Case", "comments": "27 pages, 7 figures, submitted to the Proceedings of IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many modern applications in science and engineering, data are collected\nin a streaming fashion carrying time-varying information, and practitioners\nneed to process them with a limited amount of memory and computational\nresources in a timely manner for decision making. This often is coupled with\nthe missing data problem, such that only a small fraction of data attributes\nare observed. These complications impose significant, and unconventional,\nconstraints on the problem of streaming Principal Component Analysis (PCA) and\nsubspace tracking, which is an essential building block for many inference\ntasks in signal processing and machine learning. This survey article reviews a\nvariety of classical and recent algorithms for solving this problem with low\ncomputational and memory complexities, particularly those applicable in the big\ndata regime with missing data. We illustrate that streaming PCA and subspace\ntracking algorithms can be understood through algebraic and geometric\nperspectives, and they need to be adjusted carefully to handle missing data.\nBoth asymptotic and non-asymptotic convergence guarantees are reviewed.\nFinally, we benchmark the performance of several competitive algorithms in the\npresence of missing data for both well-conditioned and ill-conditioned systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:32:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Balzano", "Laura", ""], ["Chi", "Yuejie", ""], ["Lu", "Yue M.", ""]]}, {"id": "1806.04610", "submitter": "Ruifei Cui", "authors": "Ruifei Cui, Ioan Gabriel Bucur, Perry Groot, Tom Heskes", "title": "A Novel Bayesian Approach for Latent Variable Modeling from Mixed Data\n  with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning parameters of latent variable models from\nmixed (continuous and ordinal) data with missing values. We propose a novel\nBayesian Gaussian copula factor (BGCF) approach that is consistent under\ncertain conditions and that is quite robust to the violations of these\nconditions. In simulations, BGCF substantially outperforms two state-of-the-art\nalternative approaches. An illustration on the `Holzinger & Swineford 1939'\ndataset indicates that BGCF is favorable over the so-called robust maximum\nlikelihood (MLR) even if the data match the assumptions of MLR.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:38:10 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Cui", "Ruifei", ""], ["Bucur", "Ioan Gabriel", ""], ["Groot", "Perry", ""], ["Heskes", "Tom", ""]]}, {"id": "1806.04613", "submitter": "Ehsan Imani", "authors": "Ehsan Imani, Martha White", "title": "Improving Regression Performance with Distributional Losses", "comments": "12 pages, 4 figures. To appear in Proceedings of the 35th\n  International Conference on Machine Learning, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence that converting targets to soft targets in\nsupervised learning can provide considerable gains in performance. Much of this\nwork has considered classification, converting hard zero-one values to soft\nlabels---such as by adding label noise, incorporating label ambiguity or using\ndistillation. In parallel, there is some evidence from a regression setting in\nreinforcement learning that learning distributions can improve performance. In\nthis work, we investigate the reasons for this improvement, in a regression\nsetting. We introduce a novel distributional regression loss, and similarly\nfind it significantly improves prediction accuracy. We investigate several\ncommon hypotheses, around reducing overfitting and improved representations. We\ninstead find evidence for an alternative hypothesis: this loss is easier to\noptimize, with better behaved gradients, resulting in improved generalization.\nWe provide theoretical support for this alternative hypothesis, by\ncharacterizing the norm of the gradients of this loss.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:42:25 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Imani", "Ehsan", ""], ["White", "Martha", ""]]}, {"id": "1806.04640", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine", "title": "Unsupervised Meta-Learning for Reinforcement Learning", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning algorithms use past experience to learn to quickly solve new\ntasks. In the context of reinforcement learning, meta-learning algorithms\nacquire reinforcement learning procedures to solve new problems more\nefficiently by utilizing experience from prior tasks. The performance of\nmeta-learning algorithms depends on the tasks available for meta-training: in\nthe same way that supervised learning generalizes best to test points drawn\nfrom the same distribution as the training points, meta-learning methods\ngeneralize best to tasks from the same distribution as the meta-training tasks.\nIn effect, meta-reinforcement learning offloads the design burden from\nalgorithm design to task design. If we can automate the process of task design\nas well, we can devise a meta-learning algorithm that is truly automated. In\nthis work, we take a step in this direction, proposing a family of unsupervised\nmeta-learning algorithms for reinforcement learning. We motivate and describe a\ngeneral recipe for unsupervised meta-reinforcement learning, and present an\ninstantiation of this approach. Our conceptual and theoretical contributions\nconsist of formulating the unsupervised meta-reinforcement learning problem and\ndescribing how task proposals based on mutual information can be used to train\noptimal meta-learners. Our experimental results indicate that unsupervised\nmeta-reinforcement learning effectively acquires accelerated reinforcement\nlearning procedures without the need for manual task design and these\nprocedures exceed the performance of learning from scratch.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:48:52 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:01:41 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 16:55:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gupta", "Abhishek", ""], ["Eysenbach", "Benjamin", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.04642", "submitter": "Ching-An Cheng", "authors": "Ching-An Cheng and Xinyan Yan and Evangelos A. Theodorou and Byron\n  Boots", "title": "Accelerating Imitation Learning with Predictive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample efficiency is critical in solving real-world reinforcement learning\nproblems, where agent-environment interactions can be costly. Imitation\nlearning from expert advice has proved to be an effective strategy for reducing\nthe number of interactions required to train a policy. Online imitation\nlearning, which interleaves policy evaluation and policy optimization, is a\nparticularly effective technique with provable performance guarantees. In this\nwork, we seek to further accelerate the convergence rate of online imitation\nlearning, thereby making it more sample efficient. We propose two model-based\nalgorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based\non solving variational inequalities and MoBIL-Prox based on stochastic\nfirst-order updates. These two methods leverage a model to predict future\ngradients to speed up policy learning. When the model oracle is learned online,\nthese algorithms can provably accelerate the best known convergence rate up to\nan order. Our algorithms can be viewed as a generalization of stochastic\nMirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style\nanalysis of performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:55:06 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 06:34:01 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 00:36:20 GMT"}, {"version": "v4", "created": "Sat, 13 Oct 2018 16:07:18 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Cheng", "Ching-An", ""], ["Yan", "Xinyan", ""], ["Theodorou", "Evangelos A.", ""], ["Boots", "Byron", ""]]}, {"id": "1806.04655", "submitter": "Rahul Ramesh", "authors": "Revanth Reddy, Rahul Ramesh, Ameet Deshpande and Mitesh M. Khapra", "title": "FigureNet: A Deep Learning model for Question-Answering on Scientific\n  Plots", "comments": "To appear in the proceedings of the 2019 International Joint\n  Conference on Neural Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has managed to push boundaries in a wide variety of tasks. One\narea of interest is to tackle problems in reasoning and understanding, with an\naim to emulate human intelligence. In this work, we describe a deep learning\nmodel that addresses the reasoning task of question-answering on categorical\nplots. We introduce a novel architecture FigureNet, that learns to identify\nvarious plot elements, quantify the represented values and determine a relative\nordering of these statistical values. We test our model on the FigureQA dataset\nwhich provides images and accompanying questions for scientific plots like bar\ngraphs and pie charts, augmented with rich annotations. Our approach\noutperforms the state-of-the-art Relation Networks baseline by approximately\n$7\\%$ on this dataset, with a training time that is over an order of magnitude\nlesser.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 17:31:23 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:51:01 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Reddy", "Revanth", ""], ["Ramesh", "Rahul", ""], ["Deshpande", "Ameet", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1806.04702", "submitter": "Dimitri Block", "authors": "Philip Soeffker, Dimitri Block, Nico Wiebusch, Uwe Meier", "title": "Resource Allocation for a Wireless Coexistence Management System Based\n  on Reinforcement Learning", "comments": "Submitted to the 23rd IEEE International Conference on Emerging\n  Technologies and Factory Automation (ETFA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In industrial environments, an increasing amount of wireless devices are\nused, which utilize license-free bands. As a consequence of these mutual\ninterferences of wireless systems might decrease the state of coexistence.\nTherefore, a central coexistence management system is needed, which allocates\nconflict-free resources to wireless systems. To ensure a conflict-free resource\nutilization, it is useful to predict the prospective medium utilization before\nresources are allocated. This paper presents a self-learning concept, which is\nbased on reinforcement learning. A simulative evaluation of reinforcement\nlearning agents based on neural networks, called deep Q-networks and double\ndeep Q-networks, was realized for exemplary and practically relevant\ncoexistence scenarios. The evaluation of the double deep Q-network showed that\na prediction accuracy of at least 98 % can be reached in all investigated\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 12:05:30 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Soeffker", "Philip", ""], ["Block", "Dimitri", ""], ["Wiebusch", "Nico", ""], ["Meier", "Uwe", ""]]}, {"id": "1806.04731", "submitter": "Stephan Rasp", "authors": "Stephan Rasp, Michael S. Pritchard, Pierre Gentine", "title": "Deep learning to represent sub-grid processes in climate models", "comments": "View official PNAS version at https://doi.org/10.1073/pnas.1810286115", "journal-ref": "Proceedings of the National Academy of Sciences Sep 2018,\n  201810286; DOI: 10.1073/pnas.1810286115", "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The representation of nonlinear sub-grid processes, especially clouds, has\nbeen a major source of uncertainty in climate models for decades.\nCloud-resolving models better represent many of these processes and can now be\nrun globally but only for short-term simulations of at most a few years because\nof computational limitations. Here we demonstrate that deep learning can be\nused to capture many advantages of cloud-resolving modeling at a fraction of\nthe computational cost. We train a deep neural network to represent all\natmospheric sub-grid processes in a climate model by learning from a\nmulti-scale model in which convection is treated explicitly. The trained neural\nnetwork then replaces the traditional sub-grid parameterizations in a global\ngeneral circulation model in which it freely interacts with the resolved\ndynamics and the surface-flux scheme. The prognostic multi-year simulations are\nstable and closely reproduce not only the mean climate of the cloud-resolving\nsimulation but also key aspects of variability, including precipitation\nextremes and the equatorial wave spectrum. Furthermore, the neural network\napproximately conserves energy despite not being explicitly instructed to.\nFinally, we show that the neural network parameterization generalizes to new\nsurface forcing patterns but struggles to cope with temperatures far outside\nits training manifold. Our results show the feasibility of using deep learning\nfor climate model parameterization. In a broader context, we anticipate that\ndata-driven Earth System Model development could play a key role in reducing\nclimate prediction uncertainty in the coming decade.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:29:25 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 17:10:58 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 08:28:05 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Rasp", "Stephan", ""], ["Pritchard", "Michael S.", ""], ["Gentine", "Pierre", ""]]}, {"id": "1806.04743", "submitter": "Pablo de Castro", "authors": "Pablo de Castro and Tommaso Dorigo", "title": "INFERNO: Inference-Aware Neural Optimisation", "comments": "Code available at https://github.com/pablodecm/paper-inferno .\n  Version updates: - v2: fixed typos, improve text, link to code and a better\n  synthetic experiment", "journal-ref": null, "doi": "10.1016/j.cpc.2019.06.007", "report-no": null, "categories": "stat.ML cs.LG hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex computer simulations are commonly required for accurate data\nmodelling in many scientific disciplines, making statistical inference\nchallenging due to the intractability of the likelihood evaluation for the\nobserved data. Furthermore, sometimes one is interested on inference drawn over\na subset of the generative model parameters while taking into account model\nuncertainty or misspecification on the remaining nuisance parameters. In this\nwork, we show how non-linear summary statistics can be constructed by\nminimising inference-motivated losses via stochastic gradient descent such they\nprovided the smallest uncertainty for the parameters of interest. As a use\ncase, the problem of confidence interval estimation for the mixture coefficient\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\nis considered, where the proposed technique clearly outperforms summary\nstatistics based on probabilistic classification, which are a commonly used\nalternative but do not account for the presence of nuisance parameters.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:08:53 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 12:41:56 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["de Castro", "Pablo", ""], ["Dorigo", "Tommaso", ""]]}, {"id": "1806.04773", "submitter": "Edward Raff", "authors": "William Fleshman, Edward Raff, Richard Zak, Mark McLean, Charles\n  Nicholas", "title": "Static Malware Detection & Subterfuge: Quantifying the Robustness of\n  Machine Learning and Current Anti-Virus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine-learning (ML) based systems for malware detection become more\nprevalent, it becomes necessary to quantify the benefits compared to the more\ntraditional anti-virus (AV) systems widely used today. It is not practical to\nbuild an agreed upon test set to benchmark malware detection systems on pure\nclassification performance. Instead we tackle the problem by creating a new\ntesting methodology, where we evaluate the change in performance on a set of\nknown benign & malicious files as adversarial modifications are performed. The\nchange in performance combined with the evasion techniques then quantifies a\nsystem's robustness against that approach. Through these experiments we are\nable to show in a quantifiable way how purely ML based systems can be more\nrobust than AV products at detecting malware that attempts evasion through\nmodification, but may be slower to adapt in the face of significantly novel\nattacks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 21:35:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Fleshman", "William", ""], ["Raff", "Edward", ""], ["Zak", "Richard", ""], ["McLean", "Mark", ""], ["Nicholas", "Charles", ""]]}, {"id": "1806.04795", "submitter": "David Hallac", "authors": "David Hallac, Suvrat Bhooshan, Michael Chen, Kacem Abida, Rok Sosic,\n  Jure Leskovec", "title": "Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With automobiles becoming increasingly reliant on sensors to perform various\ndriving tasks, it is important to encode the relevant CAN bus sensor data in a\nway that captures the general state of the vehicle in a compact form. In this\npaper, we develop a deep learning-based method, called Drive2Vec, for embedding\nsuch sensor data in a low-dimensional yet actionable form. Our method is based\non stacked gated recurrent units (GRUs). It accepts a short interval of\nautomobile sensor data as input and computes a low-dimensional representation\nof that data, which can then be used to accurately solve a range of tasks. With\nthis representation, we (1) predict the exact values of the sensors in the\nshort term (up to three seconds in the future), (2) forecast the long-term\naverage values of these same sensors, (3) infer additional contextual\ninformation that is not encoded in the data, including the identity of the\ndriver behind the wheel, and (4) build a knowledge base that can be used to\nauto-label data and identify risky states. We evaluate our approach on a\ndataset collected by Audi, which equipped a fleet of test vehicles with data\nloggers to store all sensor readings on 2,098 hours of driving on real roads.\nWe show in several experiments that our method outperforms other baselines by\nup to 90%, and we further demonstrate how these embeddings of sensor data can\nbe used to solve a variety of real-world automotive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 23:19:54 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Hallac", "David", ""], ["Bhooshan", "Suvrat", ""], ["Chen", "Michael", ""], ["Abida", "Kacem", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.04798", "submitter": "Kunkun Pang", "authors": "Kunkun Pang, Mingzhi Dong, Yang Wu, Timothy Hospedales", "title": "Meta-Learning Transferable Active Learning Policies by Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) aims to enable training high performance classifiers\nwith low annotation cost by predicting which subset of unlabelled instances\nwould be most beneficial to label. The importance of AL has motivated extensive\nresearch, proposing a wide variety of manually designed AL algorithms with\ndiverse theoretical and intuitive motivations. In contrast to this body of\nresearch, we propose to treat active learning algorithm design as a\nmeta-learning problem and learn the best criterion from data. We model an\nactive learning algorithm as a deep neural network that inputs the base learner\nstate and the unlabelled point set and predicts the best point to annotate\nnext. Training this active query policy network with reinforcement learning,\nproduces the best non-myopic policy for a given dataset. The key challenge in\nachieving a general solution to AL then becomes that of learner generalisation,\nparticularly across heterogeneous datasets. We propose a multi-task\ndataset-embedding approach that allows dataset-agnostic active learners to be\ntrained. Our evaluation shows that AL algorithms trained in this way can\ndirectly generalise across diverse problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 23:52:08 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Pang", "Kunkun", ""], ["Dong", "Mingzhi", ""], ["Wu", "Yang", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1806.04808", "submitter": "Guansong Pang", "authors": "Guansong Pang, Longbing Cao, Ling Chen, Huan Liu", "title": "Learning Representations of Ultrahigh-dimensional Data for Random\n  Distance-based Outlier Detection", "comments": "10 pages, 4 figures, 3 tables. To appear in the proceedings of KDD18,\n  Long presentation (oral)", "journal-ref": null, "doi": "10.1145/3219819.3220042", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning expressive low-dimensional representations of ultrahigh-dimensional\ndata, e.g., data with thousands/millions of features, has been a major way to\nenable learning methods to address the curse of dimensionality. However,\nexisting unsupervised representation learning methods mainly focus on\npreserving the data regularity information and learning the representations\nindependently of subsequent outlier detection methods, which can result in\nsuboptimal and unstable performance of detecting irregularities (i.e.,\noutliers).\n  This paper introduces a ranking model-based framework, called RAMODO, to\naddress this issue. RAMODO unifies representation learning and outlier\ndetection to learn low-dimensional representations that are tailored for a\nstate-of-the-art outlier detection approach - the random distance-based\napproach. This customized learning yields more optimal and stable\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\nleverage little labeled data as prior knowledge to learn more expressive and\napplication-relevant representations. We instantiate RAMODO to an efficient\nmethod called REPEN to demonstrate the performance of RAMODO.\n  Extensive empirical results on eight real-world ultrahigh dimensional data\nsets show that REPEN (i) enables a random distance-based detector to obtain\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\nperforms substantially better and more stably than four state-of-the-art\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\nto achieve up to 32% AUC improvement.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 00:53:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Pang", "Guansong", ""], ["Cao", "Longbing", ""], ["Chen", "Ling", ""], ["Liu", "Huan", ""]]}, {"id": "1806.04819", "submitter": "Richard Nock", "authors": "Hisham Husain, Zac Cranko, Richard Nock", "title": "Integral Privacy for Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a leading protection setting, focused by design on\nindividual privacy. Many applications, in medical / pharmaceutical domains or\nsocial networks, rather posit privacy at a group level, a setting we call\nintegral privacy. We aim for the strongest form of privacy: the group size is\nin particular not known in advance. We study a problem with related\napplications in domains cited above that have recently met with substantial\nrecent press: sampling.\n  Keeping correct utility levels in such a strong model of statistical\nindistinguishability looks difficult to be achieved with the usual differential\nprivacy toolbox because it would typically scale in the worst case the\nsensitivity by the sample size and so the noise variance by up to its square.\nWe introduce a trick specific to sampling that bypasses the sensitivity\nanalysis. Privacy enforces an information theoretic barrier on approximation,\nand we show how to reach this barrier with guarantees on the approximation of\nthe target non private density. We do so using a recent approach to non private\ndensity estimation relying on the original boosting theory, learning the\nsufficient statistics of an exponential family with classifiers. Approximation\nguarantees cover the mode capture problem. In the context of learning, the\nsampling problem is particularly important: because integral privacy enjoys the\nsame closure under post-processing as differential privacy does, any algorithm\nusing integrally privacy sampled data would result in an output equally\nintegrally private. We also show that this brings fairness guarantees on\npost-processing that would eventually elude classical differential privacy: any\ndecision process has bounded data-dependent bias when the data is integrally\nprivately sampled. Experimental results against private kernel density\nestimation and private GANs displays the quality of our results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 02:01:22 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 22:51:03 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 02:33:17 GMT"}, {"version": "v4", "created": "Wed, 6 Feb 2019 02:41:26 GMT"}, {"version": "v5", "created": "Tue, 2 Jul 2019 22:41:35 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Husain", "Hisham", ""], ["Cranko", "Zac", ""], ["Nock", "Richard", ""]]}, {"id": "1806.04823", "submitter": "Vira Semenova", "authors": "Denis Nekipelov, Vira Semenova, Vasilis Syrgkanis", "title": "Regularized Orthogonal Machine Learning for Nonlinear Semiparametric\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Lasso-type estimator for a high-dimensional sparse\nparameter identified by a single index conditional moment restriction (CMR). In\naddition to this parameter, the moment function can also depend on a nuisance\nfunction, such as the propensity score or the conditional choice probability,\nwhich we estimate by modern machine learning tools. We first adjust the moment\nfunction so that the gradient of the future loss function is insensitive\n(formally, Neyman-orthogonal) with respect to the first-stage regularization\nbias, preserving the single index property. We then take the loss function to\nbe an indefinite integral of the adjusted moment function with respect to the\nsingle index. The proposed Lasso estimator converges at the oracle rate, where\nthe oracle knows the nuisance function and solves only the parametric problem.\nWe demonstrate our method by estimating the short-term heterogeneous impact of\nConnecticut's Jobs First welfare reform experiment on women's welfare\nparticipation decision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 02:22:51 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 15:34:02 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 21:20:53 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 16:29:49 GMT"}, {"version": "v5", "created": "Wed, 30 Sep 2020 02:51:52 GMT"}, {"version": "v6", "created": "Sat, 17 Oct 2020 13:14:04 GMT"}, {"version": "v7", "created": "Wed, 16 Jun 2021 02:38:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nekipelov", "Denis", ""], ["Semenova", "Vira", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1806.04838", "submitter": "Naonori Ueda", "authors": "Naonori Ueda and Akinori Fujino", "title": "Partial AUC Maximization via Nonlinear Scoring Functions", "comments": "9pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for maximizing a partial area under a receiver operating\ncharacteristic (ROC) curve (pAUC) for binary classification tasks. In binary\nclassification tasks, accuracy is the most commonly used as a measure of\nclassifier performance. In some applications such as anomaly detection and\ndiagnostic testing, accuracy is not an appropriate measure since prior\nprobabilties are often greatly biased. Although in such cases the pAUC has been\nutilized as a performance measure, few methods have been proposed for directly\nmaximizing the pAUC. This optimization is achieved by using a scoring function.\nThe conventional approach utilizes a linear function as the scoring function.\nIn contrast we newly introduce nonlinear scoring functions for this purpose.\nSpecifically, we present two types of nonlinear scoring functions based on\ngenerative models and deep neural networks. We show experimentally that\nnonlinear scoring fucntions improve the conventional methods through the\napplication of a binary classification of real and bogus objects obtained with\nthe Hyper Suprime-Cam on the Subaru telescope.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 03:26:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Ueda", "Naonori", ""], ["Fujino", "Akinori", ""]]}, {"id": "1806.04854", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin\n  Gal, Akash Srivastava", "title": "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam", "comments": "Camera ready version", "journal-ref": "Thirty-fifth International Conference on Machine Learning, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty computation in deep learning is essential to design robust and\nreliable systems. Variational inference (VI) is a promising approach for such\ncomputation, but requires more effort to implement and execute compared to\nmaximum-likelihood methods. In this paper, we propose new natural-gradient\nalgorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms\ncan be implemented within the Adam optimizer by perturbing the network weights\nduring gradient evaluations, and uncertainty estimates can be cheaply obtained\nby using the vector that adapts the learning rate. This requires lower memory,\ncomputation, and implementation effort than existing VI methods, while\nobtaining uncertainty estimates of comparable quality. Our empirical results\nconfirm this and further suggest that the weight-perturbation in our algorithm\ncould be useful for exploration in reinforcement learning and stochastic\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 05:45:22 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 12:19:00 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 08:21:25 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Nielsen", "Didrik", ""], ["Tangkaratt", "Voot", ""], ["Lin", "Wu", ""], ["Gal", "Yarin", ""], ["Srivastava", "Akash", ""]]}, {"id": "1806.04863", "submitter": "Behrooz Azarkhalili", "authors": "Farzad Abdolhosseini, Behrooz Azarkhalili, Abbas Maazallahi, Aryan\n  Kamal, Seyed Abolfazl Motahari, Ali Sharifi-Zarchi, and Hamidreza Chitsaz", "title": "Cell Identity Codes: Understanding Cell Identity from Gene Expression\n  Profiles using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding cell identity is an important task in many biomedical areas.\nExpression patterns of specific marker genes have been used to characterize\nsome limited cell types, but exclusive markers are not available for many cell\ntypes. A second approach is to use machine learning to discriminate cell types\nbased on the whole gene expression profiles (GEPs). The accuracies of simple\nclassification algorithms such as linear discriminators or support vector\nmachines are limited due to the complexity of biological systems. We used deep\nneural networks to analyze 1040 GEPs from 16 different human tissues and cell\ntypes. After comparing different architectures, we identified a specific\nstructure of deep autoencoders that can encode a GEP into a vector of 30\nnumeric values, which we call the cell identity code (CIC). The original GEP\ncan be reproduced from the CIC with an accuracy comparable to technical\nreplicates of the same experiment. Although we use an unsupervised approach to\ntrain the autoencoder, we show different values of the CIC are connected to\ndifferent biological aspects of the cell, such as different pathways or\nbiological processes. This network can use CIC to reproduce the GEP of the cell\ntypes it has never seen during the training. It also can resist some noise in\nthe measurement of the GEP. Furthermore, we introduce classifier autoencoder,\nan architecture that can accurately identify cell type based on the GEP or the\nCIC.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 06:42:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Abdolhosseini", "Farzad", ""], ["Azarkhalili", "Behrooz", ""], ["Maazallahi", "Abbas", ""], ["Kamal", "Aryan", ""], ["Motahari", "Seyed Abolfazl", ""], ["Sharifi-Zarchi", "Ali", ""], ["Chitsaz", "Hamidreza", ""]]}, {"id": "1806.04884", "submitter": "Tohru Nitta", "authors": "Tohru Nitta", "title": "No Bad Local Minima in Wide and Deep Nonlinear Neural Networks", "comments": "11 pages, 8 figures. We proved in the first version that the same\n  results as those of this version only in an initial state of training. In\n  contrast, we prove in this version that the same results hold true during\n  training. We changed the paper title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that no bad local minimum exists in the deep\nnonlinear neural networks with sufficiently large widths of the hidden layers\nif the parameters are initialized by He initialization method. Specifically, in\nthe deep ReLU neural network model with sufficiently large widths of the hidden\nlayers, the following four statements hold true: 1) the loss function is\nnon-convex and non-concave; 2) every local minimum is a global minimum; 3)\nevery critical point that is not a global minimum is a saddle point; and 4) bad\nsaddle points exist.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:18:00 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 21:26:29 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Nitta", "Tohru", ""]]}, {"id": "1806.04899", "submitter": "Yijun Bian", "authors": "Yijun Bian and Yijun Wang and Yaqiang Yao and Huanhuan Chen", "title": "Ensemble Pruning based on Objection Maximization with a General\n  Distributed Framework", "comments": "Accepted by TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2945116", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble pruning, selecting a subset of individual learners from an original\nensemble, alleviates the deficiencies of ensemble learning on the cost of time\nand space. Accuracy and diversity serve as two crucial factors while they\nusually conflict with each other. To balance both of them, we formalize the\nensemble pruning problem as an objection maximization problem based on\ninformation entropy. Then we propose an ensemble pruning method including a\ncentralized version and a distributed version, in which the latter is to speed\nup the former. At last, we extract a general distributed framework for ensemble\npruning, which can be widely suitable for most of the existing ensemble pruning\nmethods and achieve less time consuming without much accuracy degradation.\nExperimental results validate the efficiency of our framework and methods,\nparticularly concerning a remarkable improvement of the execution speed,\naccompanied by gratifying accuracy performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:58:49 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 03:40:07 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 12:22:24 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bian", "Yijun", ""], ["Wang", "Yijun", ""], ["Yao", "Yaqiang", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1806.04900", "submitter": "Anna Guitart", "authors": "Paul Bertens, Anna Guitart, Pei Pei Chen and \\'Africa Peri\\'a\\~nez", "title": "A Machine-Learning Item Recommendation System for Video Games", "comments": null, "journal-ref": "IEEE Computational Intelligence and Games (CIG), 1--4, 2018", "doi": "10.1109/CIG.2018.8490456", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-game players generate huge amounts of data, as everything they do\nwithin a game is recorded. In particular, among all the stored actions and\nbehaviors, there is information on the in-game purchases of virtual products.\nSuch information is of critical importance in modern free-to-play titles, where\ngamers can select or buy a profusion of items during the game in order to\nprogress and fully enjoy their experience.\n  To try to maximize these kind of purchases, one can use a recommendation\nsystem so as to present players with items that might be interesting for them.\nSuch systems can better achieve their goal by employing machine learning\nalgorithms that are able to predict the rating of an item or product by a\nparticular user. In this paper we evaluate and compare two of these algorithms,\nan ensemble-based model (extremely randomized trees) and a deep neural network,\nboth of which are promising candidates for operational video-game recommender\nengines.\n  Item recommenders can help developers improve the game. But, more\nimportantly, it should be possible to integrate them into the game, so that\nusers automatically get personalized recommendations while playing. The\npresented models are not only able to meet this challenge, providing accurate\npredictions of the items that a particular player will find attractive, but\nalso sufficiently fast and robust to be used in operational settings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 09:00:36 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:02:12 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Bertens", "Paul", ""], ["Guitart", "Anna", ""], ["Chen", "Pei Pei", ""], ["Peri\u00e1\u00f1ez", "\u00c1frica", ""]]}, {"id": "1806.04910", "submitter": "Luca Franceschi", "authors": "Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi,\n  Massimilano Pontil", "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning", "comments": "ICML 2018; code for replicating experiments at\n  https://github.com/prolearner/hyper-representation, main package (Far-HO) at\n  https://github.com/lucfra/FAR-HO", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework based on bilevel programming that unifies\ngradient-based hyperparameter optimization and meta-learning. We show that an\napproximate version of the bilevel problem can be solved by taking into\nexplicit account the optimization dynamics for the inner objective. Depending\non the specific setting, the outer variables take either the meaning of\nhyperparameters in a supervised learning problem or parameters of a\nmeta-learner. We provide sufficient conditions under which solutions of the\napproximate problem converge to those of the exact problem. We instantiate our\napproach for meta-learning in the case of deep learning where representation\nlayers are treated as hyperparameters shared across a set of training episodes.\nIn experiments, we confirm our theoretical findings, present encouraging\nresults for few-shot learning and contrast the bilevel approach against\nclassical approaches for learning-to-learn.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 09:21:42 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 14:53:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Franceschi", "Luca", ""], ["Frasconi", "Paolo", ""], ["Salzo", "Saverio", ""], ["Grazzi", "Riccardo", ""], ["Pontil", "Massimilano", ""]]}, {"id": "1806.04931", "submitter": "Marleen Balvert", "authors": "Bojian Yin, Marleen Balvert, Davide Zambrano, Alexander Sch\\\"onhuth,\n  Sander Bohte", "title": "An image representation based convolutional network for DNA\n  classification", "comments": "Published at ICLR 2018, https://openreview.net/pdf?id=HJvvRoe0W", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The folding structure of the DNA molecule combined with helper molecules,\nalso referred to as the chromatin, is highly relevant for the functional\nproperties of DNA. The chromatin structure is largely determined by the\nunderlying primary DNA sequence, though the interaction is not yet fully\nunderstood. In this paper we develop a convolutional neural network that takes\nan image-representation of primary DNA sequence as its input, and predicts key\ndeterminants of chromatin structure. The method is developed such that it is\ncapable of detecting interactions between distal elements in the DNA sequence,\nwhich are known to be highly relevant. Our experiments show that the method\noutperforms several existing methods both in terms of prediction accuracy and\ntraining time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:27:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Yin", "Bojian", ""], ["Balvert", "Marleen", ""], ["Zambrano", "Davide", ""], ["Sch\u00f6nhuth", "Alexander", ""], ["Bohte", "Sander", ""]]}, {"id": "1806.04941", "submitter": "Luca Franceschi", "authors": "Luca Franceschi, Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo,\n  Paolo Frasconi", "title": "Far-HO: A Bilevel Programming Package for Hyperparameter Optimization\n  and Meta-Learning", "comments": "This submission is a reduced version of (Franceschi et al.,\n  arXiv:1806.04910) which has been accepted at the main ICML 2018 conference.\n  In this paper we illustrate the software framework, material that could not\n  be included in the conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (Franceschi et al., 2018) we proposed a unified mathematical framework,\ngrounded on bilevel programming, that encompasses gradient-based hyperparameter\noptimization and meta-learning. We formulated an approximate version of the\nproblem where the inner objective is solved iteratively, and gave sufficient\nconditions ensuring convergence to the exact problem. In this work we show how\nto optimize learning rates, automatically weight the loss of single examples\nand learn hyper-representations with Far-HO, a software package based on the\npopular deep learning framework TensorFlow that allows to seamlessly tackle\nboth HO and ML problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:46:32 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Franceschi", "Luca", ""], ["Grazzi", "Riccardo", ""], ["Pontil", "Massimiliano", ""], ["Salzo", "Saverio", ""], ["Frasconi", "Paolo", ""]]}, {"id": "1806.04965", "submitter": "Volker Fischer", "authors": "Volker Fischer, Jan K\\\"ohler, Thomas Pfeil", "title": "The streaming rollout of deep networks - towards fully model-parallel\n  execution", "comments": "To appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks, and in particular recurrent networks, are promising\ncandidates to control autonomous agents that interact in real-time with the\nphysical world. However, this requires a seamless integration of temporal\nfeatures into the network's architecture. For the training of and inference\nwith recurrent neural networks, they are usually rolled out over time, and\ndifferent rollouts exist. Conventionally during inference, the layers of a\nnetwork are computed in a sequential manner resulting in sparse temporal\nintegration of information and long response times. In this study, we present a\ntheoretical framework to describe rollouts, the level of model-parallelization\nthey induce, and demonstrate differences in solving specific tasks. We prove\nthat certain rollouts, also for networks with only skip and no recurrent\nconnections, enable earlier and more frequent responses, and show empirically\nthat these early responses have better performance. The streaming rollout\nmaximizes these properties and enables a fully parallel execution of the\nnetwork reducing runtime on massively parallel devices. Finally, we provide an\nopen-source toolbox to design, train, evaluate, and interact with streaming\nrollouts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:53:23 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 11:22:56 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Fischer", "Volker", ""], ["K\u00f6hler", "Jan", ""], ["Pfeil", "Thomas", ""]]}, {"id": "1806.04994", "submitter": "S{\\o}ren Hauberg", "authors": "S{\\o}ren Hauberg", "title": "Only Bayes should learn a manifold (on the estimation of differential\n  geometric structure from data)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate learning of the differential geometric structure of a data\nmanifold embedded in a high-dimensional Euclidean space. We first analyze\nkernel-based algorithms and show that under the usual regularizations,\nnon-probabilistic methods cannot recover the differential geometric structure,\nbut instead find mostly linear manifolds or spaces equipped with teleports. To\nproperly learn the differential geometric structure, non-probabilistic methods\nmust apply regularizations that enforce large gradients, which go against\ncommon wisdom. We repeat the analysis for probabilistic methods and find that\nunder reasonable priors, the geometric structure can be recovered. Fully\nexploiting the recovered structure, however, requires the development of\nstochastic extensions to classic Riemannian geometry. We take early steps in\nthat regard. Finally, we partly extend the analysis to modern models based on\nneural networks, thereby highlighting geometric and probabilistic shortcomings\nof current deep generative models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:50:33 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 09:37:18 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 12:44:37 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hauberg", "S\u00f8ren", ""]]}, {"id": "1806.05009", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en, Claudio Gallicchio, Alessio Micheli, Barbara\n  Hammer", "title": "Tree Edit Distance Learning via Adaptive Symbol Embeddings", "comments": "Paper at the International Conference of Machine Learning (2018),\n  2018-07-10 to 2018-07-15 in Stockholm, Sweden", "journal-ref": "Proceedings of Machine Learning Research 80 (2018) 3973-3982", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metric learning has the aim to improve classification accuracy by learning a\ndistance measure which brings data points from the same class closer together\nand pushes data points from different classes further apart. Recent research\nhas demonstrated that metric learning approaches can also be applied to trees,\nsuch as molecular structures, abstract syntax trees of computer programs, or\nsyntax trees of natural language, by learning the cost function of an edit\ndistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.\nHowever, learning such costs directly may yield an edit distance which violates\nmetric axioms, is challenging to interpret, and may not generalize well. In\nthis contribution, we propose a novel metric learning approach for trees which\nwe call embedding edit distance learning (BEDL) and which learns an edit\ndistance indirectly by embedding the tree nodes as vectors, such that the\nEuclidean distance between those vectors supports class discrimination. We\nlearn such embeddings by reducing the distance to prototypical trees from the\nsame class and increasing the distance to prototypical trees from different\nclasses. In our experiments, we show that BEDL improves upon the\nstate-of-the-art in metric learning for trees on six benchmark data sets,\nranging from computer science over biomedical data to a natural-language\nprocessing data set containing over 300,000 nodes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:08:16 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 13:54:45 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 09:34:53 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""], ["Hammer", "Barbara", ""]]}, {"id": "1806.05017", "submitter": "Jordi Sol\\'e-Casals", "authors": "Jordi Sole-Casals, Cesar F. Caiafa, Qibin Zhao and Adrzej Cichocki", "title": "Brain-Computer Interface with Corrupted EEG Data: A Tensor Completion\n  Approach", "comments": "21 pages, 3 tables, 4 figures", "journal-ref": "Sole-Casals, J., Caiafa, C.F., Zhao, Q. et al. Cogn Comput (2018).\n  https://doi.org/10.1007/s12559-018-9574-9", "doi": "10.1007/s12559-018-9574-9", "report-no": null, "categories": "q-bio.QM eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the current issues in Brain-Computer Interface is how to deal with\nnoisy Electroencephalography measurements organized as multidimensional\ndatasets. On the other hand, recently, significant advances have been made in\nmultidimensional signal completion algorithms that exploit tensor decomposition\nmodels to capture the intricate relationship among entries in a\nmultidimensional signal. We propose to use tensor completion applied to EEG\ndata for improving the classification performance in a motor imagery BCI system\nwith corrupted measurements. Noisy measurements are considered as unknowns that\nare inferred from a tensor decomposition model. We evaluate the performance of\nfour recently proposed tensor completion algorithms plus a simple interpolation\nstrategy, first with random missing entries and then with missing samples\nconstrained to have a specific structure (random missing channels), which is a\nmore realistic assumption in BCI Applications. We measured the ability of these\nalgorithms to reconstruct the tensor from observed data. Then, we tested the\nclassification accuracy of imagined movement in a BCI experiment with missing\nsamples. We show that for random missing entries, all tensor completion\nalgorithms can recover missing samples increasing the classification\nperformance compared to a simple interpolation approach. For the random missing\nchannels case, we show that tensor completion algorithms help to reconstruct\nmissing channels, significantly improving the accuracy in the classification of\nmotor imagery, however, not at the same level as clean data. Tensor completion\nalgorithms are useful in real BCI applications. The proposed strategy could\nallow using motor imagery BCI systems even when EEG data is highly affected by\nmissing channels and/or samples, avoiding the need of new acquisitions in the\ncalibration stage.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:16:28 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 15:53:29 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Sole-Casals", "Jordi", ""], ["Caiafa", "Cesar F.", ""], ["Zhao", "Qibin", ""], ["Cichocki", "Adrzej", ""]]}, {"id": "1806.05034", "submitter": "Bernardino Romera-Paredes", "authors": "Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De\n  Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez\n  Rezende, Olaf Ronneberger", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "comments": "Last update: added further details about the LIDC experiment. 11\n  pages for the main paper, 28 pages including appendix. 5 figures in the main\n  paper, 18 figures in total, Advances in Neural Information Processing Systems\n  (NeurIPS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world vision problems suffer from inherent ambiguities. In clinical\napplications for example, it might not be clear from a CT scan alone which\nparticular region is cancer tissue. Therefore a group of graders typically\nproduces a set of diverse but plausible segmentations. We consider the task of\nlearning a distribution over segmentations given an input. To this end we\npropose a generative segmentation model based on a combination of a U-Net with\na conditional variational autoencoder that is capable of efficiently producing\nan unlimited number of plausible hypotheses. We show on a lung abnormalities\nsegmentation task and on a Cityscapes segmentation task that our model\nreproduces the possible segmentation variants as well as the frequencies with\nwhich they occur, doing so significantly better than published approaches.\nThese models could have a high impact in real-world applications, such as being\nused as clinical decision-making algorithms accounting for multiple plausible\nsemantic segmentation hypotheses to provide possible diagnoses and recommend\nfurther actions to resolve the present ambiguities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:47:04 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:34:57 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 09:50:55 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 18:26:47 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kohl", "Simon A. A.", ""], ["Romera-Paredes", "Bernardino", ""], ["Meyer", "Clemens", ""], ["De Fauw", "Jeffrey", ""], ["Ledsam", "Joseph R.", ""], ["Maier-Hein", "Klaus H.", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo Jimenez", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1806.05049", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Vladimir Kolmogorov", "title": "MAP inference via Block-Coordinate Frank-Wolfe Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new proximal bundle method for Maximum-A-Posteriori (MAP)\ninference in structured energy minimization problems. The method optimizes a\nLagrangean relaxation of the original energy minimization problem using a multi\nplane block-coordinate Frank-Wolfe method that takes advantage of the specific\nstructure of the Lagrangean decomposition. We show empirically that our method\noutperforms state-of-the-art Lagrangean decomposition based algorithms on some\nchallenging Markov Random Field, multi-label discrete tomography and graph\nmatching problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:03:38 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:27:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Swoboda", "Paul", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1806.05054", "submitter": "Melvin Robinson", "authors": "Pablo Guillen-Rondon, Melvin D. Robinson, Carlos Torres, Eduardo\n  Pereya", "title": "Support Vector Machine Application for Multiphase Flow Pattern\n  Prediction", "comments": "arXiv admin note: text overlap with arXiv:1705.07117", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a data analytical approach featuring support vector machines\n(SVM) is employed to train a predictive model over an experimentaldataset,\nwhich consists of the most relevant studies for two-phase flow pattern\nprediction. The database for this study consists of flow patterns or flow\nregimes in gas-liquid two-phase flow. The term flow pattern refers to the\ngeometrical configuration of the gas and liquid phases in the pipe. When gas\nand liquid flow simultaneously in a pipe, the two phases can distribute\nthemselves in a variety of flow configurations. Gas-liquid two-phase flow\noccurs ubiquitously in various major industrial fields: petroleum, chemical,\nnuclear, and geothermal industries. The flow configurations differ from each\nother in the spatial distribution of the interface, resulting in different flow\ncharacteristics. Experimental results obtained by applying the presented\nmethodology to different combinations of flow patterns demonstrate that the\nproposed approach is state-of-the-art alternatives by achieving 97% correct\nclassification. The results suggest machine learning could be used as an\neffective tool for automatic detection and classification of gas-liquid flow\npatterns.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 03:30:58 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Guillen-Rondon", "Pablo", ""], ["Robinson", "Melvin D.", ""], ["Torres", "Carlos", ""], ["Pereya", "Eduardo", ""]]}, {"id": "1806.05085", "submitter": "Jingyan Wang", "authors": "Jingyan Wang and Nihar B. Shah", "title": "Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in\n  Ratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinal scores (numeric ratings) collected from people are well known to\nsuffer from miscalibrations. A popular approach to address this issue is to\nassume simplistic models of miscalibration (such as linear biases) to de-bias\nthe scores. This approach, however, often fares poorly because people's\nmiscalibrations are typically far more complex and not well understood. In the\nabsence of simplifying assumptions on the miscalibration, it is widely believed\nby the crowdsourcing community that the only useful information in the cardinal\nscores is the induced ranking. In this paper, inspired by the framework of\nStein's shrinkage, empirical Bayes, and the classic two-envelope problem, we\ncontest this widespread belief. Specifically, we consider cardinal scores with\narbitrary (or even adversarially chosen) miscalibrations which are only\nrequired to be consistent with the induced ranking. We design estimators which\ndespite making no assumptions on the miscalibration, strictly and uniformly\noutperform all possible estimators that rely on only the ranking. Our\nestimators are flexible in that they can be used as a plug-in for a variety of\napplications, and we provide a proof-of-concept for A/B testing and ranking.\nOur results thus provide novel insights in the eternal debate between cardinal\nand ordinal data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:28:41 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 03:03:58 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Jingyan", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1806.05096", "submitter": "Purushottam Dixit", "authors": "Purushottam D. Dixit", "title": "Introducing user-prescribed constraints in Markov chains for nonlinear\n  dimensionality reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic kernel based dimensionality reduction approaches have become\npopular in the last decade. The central component of many of these methods is a\nsymmetric kernel that quantifies the vicinity between pairs of data points and\na kernel-induced Markov chain on the data. Typically, the Markov chain is fully\nspecified by the kernel through row normalization. However, in many cases, it\nis desirable to impose user-specified stationary-state and dynamical\nconstraints on the Markov chain. Unfortunately, no systematic framework exists\nto impose such user-defined constraints. Here, we introduce a path entropy\nmaximization based approach to derive the transition probabilities of Markov\nchains using a kernel and additional user-specified constraints. We illustrate\nthe usefulness of these Markov chains with examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:58:49 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 19:36:59 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Dixit", "Purushottam D.", ""]]}, {"id": "1806.05112", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama and Hajime Shimao", "title": "Comparing Fairness Criteria Based on Social Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in algorithmic decision-making processes is attracting increasing\nconcern. When an algorithm is applied to human-related decision-making an\nestimator solely optimizing its predictive power can learn biases on the\nexisting data, which motivates us the notion of fairness in machine learning.\nwhile several different notions are studied in the literature, little studies\nare done on how these notions affect the individuals. We demonstrate such a\ncomparison between several policies induced by well-known fairness criteria,\nincluding the color-blind (CB), the demographic parity (DP), and the equalized\nodds (EO). We show that the EO is the only criterion among them that removes\ngroup-level disparity. Empirical studies on the social welfare and disparity of\nthese policies are conducted.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:34:13 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Komiyama", "Junpei", ""], ["Shimao", "Hajime", ""]]}, {"id": "1806.05134", "submitter": "Carson Eisenach", "authors": "Carson Eisenach, Haichuan Yang, Ji Liu, Han Liu", "title": "Marginal Policy Gradients: A Unified Family of Estimators for Bounded\n  Action Spaces with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex domains, such as robotics control and real-time strategy (RTS)\ngames, require an agent to learn a continuous control. In the former, an agent\nlearns a policy over $\\mathbb{R}^d$ and in the latter, over a discrete set of\nactions each of which is parametrized by a continuous parameter. Such problems\nare naturally solved using policy based reinforcement learning (RL) methods,\nbut unfortunately these often suffer from high variance leading to instability\nand slow convergence. Unnecessary variance is introduced whenever policies over\nbounded action spaces are modeled using distributions with unbounded support by\napplying a transformation $T$ to the sampled action before execution in the\nenvironment. Recently, the variance reduced clipped action policy gradient\n(CAPG) was introduced for actions in bounded intervals, but to date no variance\nreduced methods exist when the action is a direction, something often seen in\nRTS games. To this end we introduce the angular policy gradient (APG), a\nstochastic policy gradient method for directional control. With the marginal\npolicy gradients family of estimators we present a unified analysis of the\nvariance reduction properties of APG and CAPG; our results provide a stronger\nguarantee than existing analyses for CAPG. Experimental results on a popular\nRTS game and a navigation task show that the APG estimator offers a substantial\nimprovement over the standard policy gradient.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:32:27 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 18:44:34 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 22:18:54 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Eisenach", "Carson", ""], ["Yang", "Haichuan", ""], ["Liu", "Ji", ""], ["Liu", "Han", ""]]}, {"id": "1806.05138", "submitter": "Harshil Shah", "authors": "Harshil Shah, David Barber", "title": "Generative Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Generative Neural Machine Translation (GNMT), a latent variable\narchitecture which is designed to model the semantics of the source and target\nsentences. We modify an encoder-decoder translation model by adding a latent\nvariable as a language agnostic representation which is encouraged to learn the\nmeaning of the sentence. GNMT achieves competitive BLEU scores on pure\ntranslation tasks, and is superior when there are missing words in the source\nsentence. We augment the model to facilitate multilingual translation and\nsemi-supervised learning without adding parameters. This framework\nsignificantly reduces overfitting when there is limited paired data available,\nand is effective for translating between pairs of languages not seen during\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:35:32 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Shah", "Harshil", ""], ["Barber", "David", ""]]}, {"id": "1806.05139", "submitter": "Carson Eisenach", "authors": "Carson Eisenach, Florentina Bunea, Yang Ning, Claudiu Dinicu", "title": "High-Dimensional Inference for Cluster-Based Graphical Models", "comments": null, "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-55", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by modern applications in which one constructs graphical models\nbased on a very large number of features, this paper introduces a new class of\ncluster-based graphical models, in which variable clustering is applied as an\ninitial step for reducing the dimension of the feature space. We employ model\nassisted clustering, in which the clusters contain features that are similar to\nthe same unobserved latent variable. Two different cluster-based Gaussian\ngraphical models are considered: the latent variable graph, corresponding to\nthe graphical model associated with the unobserved latent variables, and the\ncluster-average graph, corresponding to the vector of features averaged over\nclusters. Our study reveals that likelihood based inference for the latent\ngraph, not analyzed previously, is analytically intractable. Our main\ncontribution is the development and analysis of alternative estimation and\ninference strategies, for the precision matrix of an unobservable latent vector\n$Z$. We replace the likelihood of the data by an appropriate class of empirical\nrisk functions, that can be specialized to the latent graphical model and to\nthe simpler, but under-analyzed, cluster-average graphical model. The\nestimators thus derived can be used for inference on the graph structure, for\ninstance on edge strength or pattern recovery. Inference is based on the\nasymptotic limits of the entry-wise estimates of the precision matrices\nassociated with the conditional independence graphs under consideration. While\ntaking the uncertainty induced by the clustering step into account, we\nestablish Berry-Esseen central limit theorems for the proposed estimators. It\nis noteworthy that, although the clusters are estimated adaptively from the\ndata, the central limit theorems regarding the entries of the estimated graphs\nare proved under the same conditions one would use if the clusters were\nknown....\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:37:34 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:57:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Eisenach", "Carson", ""], ["Bunea", "Florentina", ""], ["Ning", "Yang", ""], ["Dinicu", "Claudiu", ""]]}, {"id": "1806.05151", "submitter": "Zhehui Chen", "authors": "Zhehui Chen, Xingguo Li, Lin F. Yang, Jarvis Haupt, and Tuo Zhao", "title": "On Landscape of Lagrangian Functions and Stochastic Search for\n  Constrained Nonconvex Optimization", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study constrained nonconvex optimization problems in machine learning,\nsignal processing, and stochastic control. It is well-known that these problems\ncan be rewritten to a minimax problem in a Lagrangian form. However, due to the\nlack of convexity, their landscape is not well understood and how to find the\nstable equilibria of the Lagrangian function is still unknown. To bridge the\ngap, we study the landscape of the Lagrangian function. Further, we define a\nspecial class of Lagrangian functions. They enjoy two properties: 1.Equilibria\nare either stable or unstable (Formal definition in Section 2); 2.Stable\nequilibria correspond to the global optima of the original problem. We show\nthat a generalized eigenvalue (GEV) problem, including canonical correlation\nanalysis and other problems, belongs to the class. Specifically, we\ncharacterize its stable and unstable equilibria by leveraging an invariant\ngroup and symmetric property (more details in Section 3). Motivated by these\nneat geometric structures, we propose a simple, efficient, and stochastic\nprimal-dual algorithm solving the online GEV problem. Theoretically, we provide\nsufficient conditions, based on which we establish an asymptotic convergence\nrate and obtain the first sample complexity result for the online GEV problem\nby diffusion approximations, which are widely used in applied probability and\nstochastic control. Numerical results are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:19:22 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 17:06:37 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 20:08:59 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Zhehui", ""], ["Li", "Xingguo", ""], ["Yang", "Lin F.", ""], ["Haupt", "Jarvis", ""], ["Zhao", "Tuo", ""]]}, {"id": "1806.05159", "submitter": "Xingguo Li", "authors": "Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, Tuo Zhao", "title": "On Tighter Generalization Bound for Deep Neural Networks: CNNs, ResNets,\n  and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a margin based data dependent generalization error bound for a\ngeneral family of deep neural networks in terms of the depth and width, as well\nas the Jacobian of the networks. Through introducing a new characterization of\nthe Lipschitz properties of neural network family, we achieve significantly\ntighter generalization bounds than existing results. Moreover, we show that the\ngeneralization bound can be further improved for bounded losses. Aside from the\ngeneral feedforward deep neural networks, our results can be applied to derive\nnew bounds for popular architectures, including convolutional neural networks\n(CNNs) and residual networks (ResNets). When achieving same generalization\nerrors with previous arts, our bounds allow for the choice of larger parameter\nspaces of weight matrices, inducing potentially stronger expressive ability for\nneural networks. Numerical evaluation is also provided to support our theory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:35:55 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:32:54 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 19:14:37 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2019 18:24:09 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Li", "Xingguo", ""], ["Lu", "Junwei", ""], ["Wang", "Zhaoran", ""], ["Haupt", "Jarvis", ""], ["Zhao", "Tuo", ""]]}, {"id": "1806.05161", "submitter": "Daniel Hsu", "authors": "Mikhail Belkin, Daniel Hsu, Partha Mitra", "title": "Overfitting or perfect fitting? Risk bounds for classification and\n  regression rules that interpolate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern machine learning models are trained to achieve zero or near-zero\ntraining error in order to obtain near-optimal (but non-zero) test error. This\nphenomenon of strong generalization performance for \"overfitted\" / interpolated\nclassifiers appears to be ubiquitous in high-dimensional data, having been\nobserved in deep networks, kernel machines, boosting and random forests. Their\nperformance is consistently robust even when the data contain large amounts of\nlabel noise.\n  Very little theory is available to explain these observations. The vast\nmajority of theoretical analyses of generalization allows for interpolation\nonly when there is little or no label noise. This paper takes a step toward a\ntheoretical foundation for interpolated classifiers by analyzing local\ninterpolating schemes, including geometric simplicial interpolation algorithm\nand singularly weighted $k$-nearest neighbor schemes. Consistency or\nnear-consistency is proved for these schemes in classification and regression\nproblems. Moreover, the nearest neighbor schemes exhibit optimal rates under\nsome standard statistical assumptions.\n  Finally, this paper suggests a way to explain the phenomenon of adversarial\nexamples, which are seemingly ubiquitous in modern machine learning, and also\ndiscusses some connections to kernel machines and random forests in the\ninterpolated regime.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:37:55 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 02:34:23 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 04:26:22 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Belkin", "Mikhail", ""], ["Hsu", "Daniel", ""], ["Mitra", "Partha", ""]]}, {"id": "1806.05178", "submitter": "Harshil Shah", "authors": "Harshil Shah, Bowen Zheng, David Barber", "title": "Generating Sentences Using a Dynamic Canvas", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word\nlevel generative model for natural language. It uses a recurrent neural network\nwith a dynamic attention and canvas memory mechanism to iteratively construct\nsentences. By viewing the state of the memory at intermediate stages and where\nthe model is placing its attention, we gain insight into how it constructs\nsentences. We demonstrate that AUTR learns a meaningful latent representation\nfor each sentence, and achieves competitive log-likelihood lower bounds whilst\nbeing computationally efficient. It is effective at generating and\nreconstructing sentences, as well as imputing missing words.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:57:19 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Shah", "Harshil", ""], ["Zheng", "Bowen", ""], ["Barber", "David", ""]]}, {"id": "1806.05236", "submitter": "Vikas Verma", "authors": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis\n  Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio", "title": "Manifold Mixup: Better Representations by Interpolating Hidden States", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel at learning the training data, but often provide\nincorrect and confident predictions when evaluated on slightly different test\nexamples. This includes distribution shifts, outliers, and adversarial\nexamples. To address these issues, we propose Manifold Mixup, a simple\nregularizer that encourages neural networks to predict less confidently on\ninterpolations of hidden representations. Manifold Mixup leverages semantic\ninterpolations as additional training signal, obtaining neural networks with\nsmoother decision boundaries at multiple levels of representation. As a result,\nneural networks trained with Manifold Mixup learn class-representations with\nfewer directions of variance. We prove theory on why this flattening happens\nunder ideal conditions, validate it on practical situations, and connect it to\nprevious works on information theory and generalization. In spite of incurring\nno significant computation and being implemented in a few lines of code,\nManifold Mixup improves strong baselines in supervised learning, robustness to\nsingle-step adversarial attacks, and test log-likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:32:59 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 18:10:16 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 20:49:11 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 18:18:18 GMT"}, {"version": "v5", "created": "Sat, 9 Mar 2019 18:27:06 GMT"}, {"version": "v6", "created": "Tue, 12 Mar 2019 02:14:07 GMT"}, {"version": "v7", "created": "Sat, 11 May 2019 16:50:55 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Verma", "Vikas", ""], ["Lamb", "Alex", ""], ["Beckham", "Christopher", ""], ["Najafi", "Amir", ""], ["Mitliagkas", "Ioannis", ""], ["Courville", "Aaron", ""], ["Lopez-Paz", "David", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.05250", "submitter": "Edward Raff", "authors": "Jared Sylvester, Edward Raff", "title": "What About Applied Fairness?", "comments": "Accepted at Machine Learning: The Debates (ML-D), at ICML Stockholm,\n  Sweden, 2018. 5 pages", "journal-ref": "Machine Learning: The Debates (ML-D), at ICML Stockholm, Sweden,\n  2018", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning practitioners are often ambivalent about the ethical aspects\nof their products. We believe anything that gets us from that current state to\none in which our systems are achieving some degree of fairness is an\nimprovement that should be welcomed. This is true even when that progress does\nnot get us 100% of the way to the goal of \"complete\" fairness or perfectly\nalign with our personal belief on which measure of fairness is used. Some\nmeasure of fairness being built would still put us in a better position than\nthe status quo. Impediments to getting fairness and ethical concerns applied in\nreal applications, whether they are abstruse philosophical debates or technical\noverhead such as the introduction of ever more hyper-parameters, should be\navoided. In this paper we further elaborate on our argument for this viewpoint\nand its importance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 20:15:28 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sylvester", "Jared", ""], ["Raff", "Edward", ""]]}, {"id": "1806.05272", "submitter": "Mireille Boutin", "authors": "Tarun Yellamraju and Jonas Hepp and Mireille Boutin", "title": "Benchmarks for Image Classification and Other High-dimensional Pattern\n  Recognition Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good classification method should yield more accurate results than simple\nheuristics. But there are classification problems, especially high-dimensional\nones like the ones based on image/video data, for which simple heuristics can\nwork quite accurately; the structure of the data in such problems is easy to\nuncover without any sophisticated or computationally expensive method. On the\nother hand, some problems have a structure that can only be found with\nsophisticated pattern recognition methods. We are interested in quantifying the\ndifficulty of a given high-dimensional pattern recognition problem. We consider\nthe case where the patterns come from two pre-determined classes and where the\nobjects are represented by points in a high-dimensional vector space. However,\nthe framework we propose is extendable to an arbitrarily large number of\nclasses. We propose classification benchmarks based on simple random projection\nheuristics. Our benchmarks are 2D curves parameterized by the classification\nerror and computational cost of these simple heuristics. Each curve divides the\nplane into a \"positive- gain\" and a \"negative-gain\" region. The latter contains\nmethods that are ill-suited for the given classification problem. The former is\ndivided into two by the curve asymptote; methods that lie in the small region\nunder the curve but right of the asymptote merely provide a computational gain\nbut no structural advantage over the random heuristics. We prove that the curve\nasymptotes are optimal (i.e. at Bayes error) in some cases, and thus no\nsophisticated method can provide a structural advantage over the random\nheuristics. Such classification problems, an example of which we present in our\nnumerical experiments, provide poor ground for testing new pattern\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:22:30 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yellamraju", "Tarun", ""], ["Hepp", "Jonas", ""], ["Boutin", "Mireille", ""]]}, {"id": "1806.05297", "submitter": "Mireille Boutin", "authors": "Tarun Yellamraju and Mireille Boutin", "title": "Pattern Dependence Detection using n-TARP Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an experiment involving a potentially small number of subjects. Some\nrandom variables are observed on each subject: a high-dimensional one called\nthe \"observed\" random variable, and a one-dimensional one called the \"outcome\"\nrandom variable. We are interested in the dependencies between the observed\nrandom variable and the outcome random variable. We propose a method to\nquantify and validate the dependencies of the outcome random variable on the\nvarious patterns contained in the observed random variable. Different degrees\nof relationship are explored (linear, quadratic, cubic, ...). This work is\nmotivated by the need to analyze educational data, which often involves\nhigh-dimensional data representing a small number of students. Thus our\nimplementation is designed for a small number of subjects; however, it can be\neasily modified to handle a very large dataset. As an illustration, the\nproposed method is used to study the influence of certain skills on the course\ngrade of students in a signal processing class. A valid dependency of the grade\non the different skill patterns is observed in the data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:14:45 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yellamraju", "Tarun", ""], ["Boutin", "Mireille", ""]]}, {"id": "1806.05310", "submitter": "Vadim Sokolov", "authors": "Laura Schultz and Vadim Sokolov", "title": "Deep Reinforcement Learning for Dynamic Urban Transportation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of deep learning and deep reinforcement learning for\noptimization problems in transportation. Many transportation system analysis\ntasks are formulated as an optimization problem - such as optimal control\nproblems in intelligent transportation systems and long term urban planning.\nOften transportation models used to represent dynamics of a transportation\nsystem involve large data sets with complex input-output interactions and are\ndifficult to use in the context of optimization. Use of deep learning\nmetamodels can produce a lower dimensional representation of those relations\nand allow to implement optimization and reinforcement learning algorithms in an\nefficient manner. In particular, we develop deep learning models for\ncalibrating transportation simulators and for reinforcement learning to solve\nthe problem of optimal scheduling of travelers on the network.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 00:24:49 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Schultz", "Laura", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1806.05337", "submitter": "Chandan Singh", "authors": "Chandan Singh, W. James Murdoch, Bin Yu", "title": "Hierarchical interpretations for neural network predictions", "comments": "Published in ICLR 2019", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 02:41:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 07:15:40 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Singh", "Chandan", ""], ["Murdoch", "W. James", ""], ["Yu", "Bin", ""]]}, {"id": "1806.05355", "submitter": "Yibo Yang", "authors": "Yibo Yang, Nicholas Ruozzi, Vibhav Gogate", "title": "Scalable Neural Network Compression and Pruning Using Hard Clustering\n  and L1 Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a simple and easy to implement neural network compression\nalgorithm that achieves results competitive with more complicated\nstate-of-the-art methods. The key idea is to modify the original optimization\nproblem by adding K independent Gaussian priors (corresponding to the k-means\nobjective) over the network parameters to achieve parameter quantization, as\nwell as an L1 penalty to achieve pruning. Unlike many existing\nquantization-based methods, our method uses hard clustering assignments of\nnetwork parameters, which adds minimal change or overhead to standard network\ntraining. We also demonstrate experimentally that tying neural network\nparameters provides less gain in generalization performance than changing\nnetwork architecture and connectivity patterns entirely.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 03:59:37 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yang", "Yibo", ""], ["Ruozzi", "Nicholas", ""], ["Gogate", "Vibhav", ""]]}, {"id": "1806.05356", "submitter": "Yael Yankelevsky", "authors": "Yael Yankelevsky and Michael Elad", "title": "Finding GEMS: Multi-Scale Dictionaries for High-Dimensional Graph\n  Signals", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2899822", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data introduces new challenges to classic signal processing\napproaches, leading to a growing interest in the field of graph signal\nprocessing. A powerful and well established model for real world signals in\nvarious domains is sparse representation over a dictionary, combined with the\nability to train the dictionary from signal examples. This model has been\nsuccessfully applied to graph signals as well by integrating the underlying\ngraph topology into the learned dictionary. Nonetheless, dictionary learning\nmethods for graph signals are typically restricted to small dimensions due to\nthe computational constraints that the dictionary learning problem entails, and\ndue to the direct use of the graph Laplacian matrix. In this paper, we propose\na dictionary learning algorithm that applies to a broader class of graph\nsignals, and is capable of handling much higher dimensional data. We\nincorporate the underlying graph topology both implicitly, by forcing the\nlearned dictionary atoms to be sparse combinations of graph-wavelet functions,\nand explicitly, by adding direct graph constraints to promote smoothness in\nboth the feature and manifold domains. The resulting atoms are thus adapted to\nthe data of interest while adhering to the underlying graph structure and\npossessing a desired multi-scale property. Experimental results on several\ndatasets, representing both synthetic and real network data of different\nnature, demonstrate the effectiveness of the proposed algorithm for graph\nsignal processing even in high dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:04:24 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Yankelevsky", "Yael", ""], ["Elad", "Michael", ""]]}, {"id": "1806.05357", "submitter": "Ian Fox", "authors": "Ian Fox, Lynn Ang, Mamta Jaiswal, Rodica Pop-Busui, Jenna Wiens", "title": "Deep Multi-Output Forecasting: Learning to Accurately Predict Blood\n  Glucose Trajectories", "comments": "KDD 2018", "journal-ref": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining, 2018", "doi": "10.1145/3219819.3220102", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many forecasting applications, it is valuable to predict not only the\nvalue of a signal at a certain time point in the future, but also the values\nleading up to that point. This is especially true in clinical applications,\nwhere the future state of the patient can be less important than the patient's\noverall trajectory. This requires multi-step forecasting, a forecasting variant\nwhere one aims to predict multiple values in the future simultaneously.\nStandard methods to accomplish this can propagate error from prediction to\nprediction, reducing quality over the long term. In light of these challenges,\nwe propose multi-output deep architectures for multi-step forecasting in which\nwe explicitly model the distribution of future values of the signal over a\nprediction horizon. We apply these techniques to the challenging and clinically\nrelevant task of blood glucose forecasting. Through a series of experiments on\na real-world dataset consisting of 550K blood glucose measurements, we\ndemonstrate the effectiveness of our proposed approaches in capturing the\nunderlying signal dynamics. Compared to existing shallow and deep methods, we\nfind that our proposed approaches improve performance individually and capture\ncomplementary information, leading to a large improvement over the baseline\nwhen combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the\nresults suggest the efficacy of our proposed approach in predicting blood\nglucose level and multi-step forecasting more generally.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:06:00 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Fox", "Ian", ""], ["Ang", "Lynn", ""], ["Jaiswal", "Mamta", ""], ["Pop-Busui", "Rodica", ""], ["Wiens", "Jenna", ""]]}, {"id": "1806.05358", "submitter": "Dong Yin", "authors": "Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett", "title": "Defending Against Saddle Point Attack in Byzantine-Robust Distributed\n  Learning", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study robust distributed learning that involves minimizing a non-convex\nloss function with saddle points. We consider the Byzantine setting where some\nworker machines have abnormal or even arbitrary and adversarial behavior. In\nthis setting, the Byzantine machines may create fake local minima near a saddle\npoint that is far away from any true local minimum, even when robust gradient\nestimators are used. We develop ByzantinePGD, a robust first-order algorithm\nthat can provably escape saddle points and fake local minima, and converge to\nan approximate true local minimizer with low iteration complexity. As a\nby-product, we give a simpler algorithm and analysis for escaping saddle points\nin the usual non-Byzantine setting. We further discuss three robust gradient\nestimators that can be used in ByzantinePGD, including median, trimmed mean,\nand iterative filtering. We characterize their performance in concrete\nstatistical settings, and argue for their near-optimality in low and high\ndimensional regimes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:15:59 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 19:12:59 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 20:02:41 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 04:52:23 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yin", "Dong", ""], ["Chen", "Yudong", ""], ["Ramchandran", "Kannan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1806.05382", "submitter": "Kohei Yamamoto", "authors": "Kohei Yamamoto, Kurato Maeno", "title": "PCAS: Pruning Channels with Attention Statistics for Deep Network\n  Compression", "comments": "Accepted at BMVC 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques for deep neural networks are important for\nimplementing them on small embedded devices. In particular, channel-pruning is\na useful technique for realizing compact networks. However, many conventional\nmethods require manual setting of compression ratios in each layer. It is\ndifficult to analyze the relationships between all layers, especially for\ndeeper models. To address these issues, we propose a simple channel-pruning\ntechnique based on attention statistics that enables to evaluate the importance\nof channels. We improved the method by means of a criterion for automatic\nchannel selection, using a single compression ratio for the entire model in\nplace of per-layer model analysis. The proposed approach achieved superior\nperformance over conventional methods with respect to accuracy and the\ncomputational costs for various models and datasets. We provide analysis\nresults for behavior of the proposed criterion on different datasets to\ndemonstrate its favorable properties for channel pruning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:28:59 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 03:22:02 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 06:58:29 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Yamamoto", "Kohei", ""], ["Maeno", "Kurato", ""]]}, {"id": "1806.05387", "submitter": "Erik Schlogl", "authors": "Karol Gellert and Erik Schl\\\"ogl", "title": "Parameter Learning and Change Detection Using a Particle Filter With\n  Accelerated Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": "QFRC working paper 392", "categories": "stat.ML cs.CE cs.LG cs.NE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the construction of a particle filter, which incorporates\nelements inspired by genetic algorithms, in order to achieve accelerated\nadaptation of the estimated posterior distribution to changes in model\nparameters. Specifically, the filter is designed for the situation where the\nsubsequent data in online sequential filtering does not match the model\nposterior filtered based on data up to a current point in time. The examples\nconsidered encompass parameter regime shifts and stochastic volatility. The\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\nfor distinguishing between regime shifts and stochastic volatility, even though\nthe model dynamics assumed by the filter exhibit neither of those features.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:41:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Gellert", "Karol", ""], ["Schl\u00f6gl", "Erik", ""]]}, {"id": "1806.05393", "submitter": "Samuel Schoenholz", "authors": "Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S.\n  Schoenholz, and Jeffrey Pennington", "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train\n  10,000-Layer Vanilla Convolutional Neural Networks", "comments": "ICML 2018 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 07:04:15 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 16:09:39 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Xiao", "Lechao", ""], ["Bahri", "Yasaman", ""], ["Sohl-Dickstein", "Jascha", ""], ["Schoenholz", "Samuel S.", ""], ["Pennington", "Jeffrey", ""]]}, {"id": "1806.05394", "submitter": "Samuel Schoenholz", "authors": "Minmin Chen, Jeffrey Pennington, and Samuel S. Schoenholz", "title": "Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables\n  Signal Propagation in Recurrent Neural Networks", "comments": "ICML 2018 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have gained widespread use in modeling sequence\ndata across various domains. While many successful recurrent architectures\nemploy a notion of gating, the exact mechanism that enables such remarkable\nperformance is not well understood. We develop a theory for signal propagation\nin recurrent networks after random initialization using a combination of mean\nfield theory and random matrix theory. To simplify our discussion, we introduce\na new RNN cell with a simple gating mechanism that we call the minimalRNN and\ncompare it with vanilla RNNs. Our theory allows us to define a maximum\ntimescale over which RNNs can remember an input. We show that this theory\npredicts trainability for both recurrent architectures. We show that gated\nrecurrent networks feature a much broader, more robust, trainable region than\nvanilla RNNs, which corroborates recent experimental findings. Finally, we\ndevelop a closed-form critical initialization scheme that achieves dynamical\nisometry in both vanilla RNNs and minimalRNNs. We show that this results in\nsignificantly improvement in training dynamics. Finally, we demonstrate that\nthe minimalRNN achieves comparable performance to its more complex\ncounterparts, such as LSTMs or GRUs, on a language modeling task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 07:04:31 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 17:19:40 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Chen", "Minmin", ""], ["Pennington", "Jeffrey", ""], ["Schoenholz", "Samuel S.", ""]]}, {"id": "1806.05403", "submitter": "Ido Nachum", "authors": "Shay Moran, Ido Nachum, Itai Panasoff, Amir Yehudayoff", "title": "On the Perceptron's Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and provide exposition to several phenomena that are related to the\nperceptron's compression. One theme concerns modifications of the perceptron\nalgorithm that yield better guarantees on the margin of the hyperplane it\noutputs. These modifications can be useful in training neural networks as well,\nand we demonstrate them with some experimental data. In a second theme, we\ndeduce conclusions from the perceptron's compression in various contexts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 08:00:45 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Moran", "Shay", ""], ["Nachum", "Ido", ""], ["Panasoff", "Itai", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1806.05413", "submitter": "Arnu Pretorius", "authors": "Arnu Pretorius, Steve Kroon, Herman Kamper", "title": "Learning Dynamics of Linear Denoising Autoencoders", "comments": "14 pages, 7 figures, accepted at the 35th International Conference on\n  Machine Learning (ICML) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising autoencoders (DAEs) have proven useful for unsupervised\nrepresentation learning, but a thorough theoretical understanding is still\nlacking of how the input noise influences learning. Here we develop theory for\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\nto derive analytic expressions that exactly describe their learning dynamics.\nWe verify our theoretical predictions with simulations as well as experiments\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\nallows DAEs to ignore low variance directions in the inputs while learning to\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\nto standard regularised autoencoders, we show that noise has a similar\nregularisation effect to weight decay, but with faster training dynamics. We\nalso show that our theoretical predictions approximate learning dynamics on\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 08:46:36 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 12:27:31 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Pretorius", "Arnu", ""], ["Kroon", "Steve", ""], ["Kamper", "Herman", ""]]}, {"id": "1806.05419", "submitter": "Raja Giryes", "authors": "Tal Levy and Alireza Vahid and Raja Giryes", "title": "Ranking Recovery from Limited Comparisons using Low-Rank Matrix\n  Completion", "comments": "10 Pages, 9 figures. A prediction table for 2018 FIFA soccer world\n  cup is included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for solving the well-known rank aggregation\nproblem from pairwise comparisons using the method of low-rank matrix\ncompletion. The partial and noisy data of pairwise comparisons is transformed\ninto a matrix form. We then use tools from matrix completion, which has served\nas a major component in the low-rank completion solution of the Netflix\nchallenge, to construct the preference of the different objects. In our\napproach, the data of multiple comparisons is used to create an estimate of the\nprobability of object i to win (or be chosen) over object j, where only a\npartial set of comparisons between N objects is known. The data is then\ntransformed into a matrix form for which the noiseless solution has a known\nrank of one. An alternating minimization algorithm, in which the target matrix\ntakes a bilinear form, is then used in combination with maximum likelihood\nestimation for both factors. The reconstructed matrix is used to obtain the\ntrue underlying preference intensity. This work demonstrates the improvement of\nour proposed algorithm over the current state-of-the-art in both simulated\nscenarios and real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:01:46 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Levy", "Tal", ""], ["Vahid", "Alireza", ""], ["Giryes", "Raja", ""]]}, {"id": "1806.05421", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars", "title": "Selfless Sequential Learning", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential learning, also called lifelong learning, studies the problem of\nlearning tasks in a sequence with access restricted to only the data of the\ncurrent task. In this paper we look at a scenario with fixed model capacity,\nand postulate that the learning process should not be selfish, i.e. it should\naccount for future tasks to be added and thus leave enough capacity for them.\nTo achieve Selfless Sequential Learning we study different regularization\nstrategies and activation functions. We find that imposing sparsity at the\nlevel of the representation (i.e.~neuron activations) is more beneficial for\nsequential learning than encouraging parameter sparsity. In particular, we\npropose a novel regularizer, that encourages representation sparsity by means\nof neural inhibition. It results in few active neurons which in turn leaves\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\nan entire layer can be too drastic, especially for complex tasks requiring\nstrong representations, our regularizer only inhibits other neurons in a local\nneighbourhood, inspired by lateral inhibition processes in the brain. We\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\nthat penalize changes to important previously learned parts of the network. We\nshow that our new regularizer leads to increased sparsity which translates in\nconsistent performance improvement %over alternative regularizers we studied on\ndiverse datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:06:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 07:23:59 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 13:58:26 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2018 02:13:23 GMT"}, {"version": "v5", "created": "Fri, 12 Apr 2019 14:47:02 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1806.05437", "submitter": "Yilong Yang", "authors": "Yilong Yang, Nafees Qamar, Peng Liu, Katarina Grolinger, Weiru Wang,\n  Zhi Li, Zhifang Liao", "title": "ServeNet: A Deep Neural Network for Web Services Classification", "comments": "Accepted by ICWS'20", "journal-ref": null, "doi": "10.1109/ICWS49710.2020.00029", "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated service classification plays a crucial role in service discovery,\nselection, and composition. Machine learning has been widely used for service\nclassification in recent years. However, the performance of conventional\nmachine learning methods highly depends on the quality of manual feature\nengineering. In this paper, we present a novel deep neural network to\nautomatically abstract low-level representation of both service name and\nservice description to high-level merged features without feature engineering\nand the length limitation, and then predict service classification on 50\nservice categories. To demonstrate the effectiveness of our approach, we\nconduct a comprehensive experimental study by comparing 10 machine learning\nmethods on 10,000 real-world web services. The result shows that the proposed\ndeep neural network can achieve higher accuracy in classification and more\nrobust than other machine learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:53:56 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 16:58:10 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 00:07:05 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Yang", "Yilong", ""], ["Qamar", "Nafees", ""], ["Liu", "Peng", ""], ["Grolinger", "Katarina", ""], ["Wang", "Weiru", ""], ["Li", "Zhi", ""], ["Liao", "Zhifang", ""]]}, {"id": "1806.05438", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda, Taiji Suzuki", "title": "Stochastic Gradient Descent with Exponential Convergence Rates of\n  Expected Classification Errors", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic gradient descent and its averaging variant for binary\nclassification problems in a reproducing kernel Hilbert space. In the\ntraditional analysis using a consistency property of loss functions, it is\nknown that the expected classification error converges more slowly than the\nexpected risk even when assuming a low-noise condition on the conditional label\nprobabilities. Consequently, the resulting rate is sublinear. Therefore, it is\nimportant to consider whether much faster convergence of the expected\nclassification error can be achieved. In recent research, an exponential\nconvergence rate for stochastic gradient descent was shown under a strong\nlow-noise condition but provided theoretical analysis was limited to the\nsquared loss function, which is somewhat inadequate for binary classification\ntasks. In this paper, we show an exponential convergence of the expected\nclassification error in the final phase of the stochastic gradient descent for\na wide class of differentiable convex loss functions under similar assumptions.\nAs for the averaged stochastic gradient descent, we show that the same\nconvergence rate holds from the early phase of training. In experiments, we\nverify our analyses on the $L_2$-regularized logistic regression.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:55:42 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 05:14:29 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nitanda", "Atsushi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1806.05451", "submitter": "Benjamin Aubin", "authors": "Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala,\n  Nicolas Macris and Lenka Zdeborov\\'a", "title": "The committee machine: Computational to statistical gaps in learning a\n  two-layers neural network", "comments": "16 pages + supplementary material, 2 figures", "journal-ref": "J. Stat. Mech. (2019) 124023. & NeurIPS 2018", "doi": "10.1088/1742-5468/ab43d2", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic tools from statistical physics have been used in the past to locate\nthe phase transitions and compute the optimal learning and generalization\nerrors in the teacher-student scenario in multi-layer neural networks. In this\ncontribution, we provide a rigorous justification of these approaches for a\ntwo-layers neural network model called the committee machine. We also introduce\na version of the approximate message passing (AMP) algorithm for the committee\nmachine that allows to perform optimal learning in polynomial time for a large\nset of parameters. We find that there are regimes in which a low generalization\nerror is information-theoretically achievable while the AMP algorithm fails to\ndeliver it, strongly suggesting that no efficient algorithm exists for those\ncases, and unveiling a large computational gap.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:22:04 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 15:34:07 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Aubin", "Benjamin", ""], ["Maillard", "Antoine", ""], ["Barbier", "Jean", ""], ["Krzakala", "Florent", ""], ["Macris", "Nicolas", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1806.05454", "submitter": "Bamdev Mishra", "authors": "Mukul Bhutani, Pratik Jawanpuria, Hiroyuki Kasai, and Bamdev Mishra", "title": "Low-rank geometric mean metric learning", "comments": "Accepted to the geometry in machine learning (GiMLi) workshop at ICML\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a low-rank approach to learning a Mahalanobis metric from data.\nInspired by the recent geometric mean metric learning (GMML) algorithm, we\npropose a low-rank variant of the algorithm. This allows to jointly learn a\nlow-dimensional subspace where the data reside and the Mahalanobis metric that\nappropriately fits the data. Our results show that we compete effectively with\nGMML at lower ranks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:35:21 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Bhutani", "Mukul", ""], ["Jawanpuria", "Pratik", ""], ["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1806.05476", "submitter": "Rodrigo Berriel", "authors": "Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue,\n  Alberto F. de Souza, Thiago Oliveira-Santos", "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random\n  Non-Labeled Data", "comments": "8 pages, 3 figures, accepted by IJCNN 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489592", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, Convolutional Neural Networks (CNNs) have been\nachieving state-of-the-art performance on a variety of problems. Many companies\nemploy resources and money to generate these models and provide them as an API,\ntherefore it is in their best interest to protect them, i.e., to avoid that\nsomeone else copies them. Recent studies revealed that state-of-the-art CNNs\nare vulnerable to adversarial examples attacks, and this weakness indicates\nthat CNNs do not need to operate in the problem domain (PD). Therefore, we\nhypothesize that they also do not need to be trained with examples of the PD in\norder to operate in it.\n  Given these facts, in this paper, we investigate if a target black-box CNN\ncan be copied by persuading it to confess its knowledge through random\nnon-labeled data. The copy is two-fold: i) the target network is queried with\nrandom data and its predictions are used to create a fake dataset with the\nknowledge of the network; and ii) a copycat network is trained with the fake\ndataset and should be able to achieve similar performance as the target\nnetwork.\n  This hypothesis was evaluated locally in three problems (facial expression,\nobject, and crosswalk classification) and against a cloud-based API. In the\ncopy attacks, images from both non-problem domain and PD were used. All copycat\nnetworks achieved at least 93.7% of the performance of the original models with\nnon-problem domain data, and at least 98.6% using additional data from the PD.\nAdditionally, the copycat CNN successfully copied at least 97.3% of the\nperformance of the Microsoft Azure Emotion API. Our results show that it is\npossible to create a copycat CNN by simply querying a target network as\nblack-box with random non-labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:32:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Correia-Silva", "Jacson Rodrigues", ""], ["Berriel", "Rodrigo F.", ""], ["Badue", "Claudine", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1806.05490", "submitter": "Marton Havasi", "authors": "Marton Havasi, Jos\\'e Miguel Hern\\'andez-Lobato, Juan Jos\\'e\n  Murillo-Fuentes", "title": "Inference in Deep Gaussian Processes using Stochastic Gradient\n  Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\nProcesses that combine well calibrated uncertainty estimates with the high\nflexibility of multilayer models. One of the biggest challenges with these\nmodels is that exact inference is intractable. The current state-of-the-art\ninference method, Variational Inference (VI), employs a Gaussian approximation\nto the posterior distribution. This can be a potentially poor unimodal\napproximation of the generally multimodal posterior. In this work, we provide\nevidence for the non-Gaussian nature of the posterior and we apply the\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\nalgorithm. This results in significantly better predictions at a lower\ncomputational cost than its VI counterpart. Thus our method establishes a new\nstate-of-the-art for inference in DGPs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:00:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 02:15:34 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 11:45:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Havasi", "Marton", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""]]}, {"id": "1806.05502", "submitter": "Fabian B. Fuchs Mr", "authors": "Fabian B. Fuchs, Oliver Groth, Adam R. Kosiorek, Alex Bewley, Markus\n  Wulfmeier, Andrea Vedaldi, Ingmar Posner", "title": "Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually predicting the stability of block towers is a popular task in the\ndomain of intuitive physics. While previous work focusses on prediction\naccuracy, a one-dimensional performance measure, we provide a broader analysis\nof the learned physical understanding of the final model and how the learning\nprocess can be guided. To this end, we introduce neural stethoscopes as a\ngeneral purpose framework for quantifying the degree of importance of specific\nfactors of influence in deep neural networks as well as for actively promoting\nand suppressing information as appropriate. In doing so, we unify concepts from\nmultitask learning as well as training with auxiliary and adversarial losses.\nWe apply neural stethoscopes to analyse the state-of-the-art neural network for\nstability prediction. We show that the baseline model is susceptible to being\nmisled by incorrect visual cues. This leads to a performance breakdown to the\nlevel of random guessing when training on scenarios where visual cues are\ninversely correlated with stability. Using stethoscopes to promote meaningful\nfeature extraction increases performance from 51% to 90% prediction accuracy.\nConversely, training on an easy dataset where visual cues are positively\ncorrelated with stability, the baseline model learns a bias leading to poor\nperformance on a harder dataset. Using an adversarial stethoscope, the network\nis successfully de-biased, leading to a performance increase from 66% to 88%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:35:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 09:51:18 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 19:31:50 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 16:32:21 GMT"}, {"version": "v5", "created": "Fri, 6 Sep 2019 13:49:37 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Fuchs", "Fabian B.", ""], ["Groth", "Oliver", ""], ["Kosiorek", "Adam R.", ""], ["Bewley", "Alex", ""], ["Wulfmeier", "Markus", ""], ["Vedaldi", "Andrea", ""], ["Posner", "Ingmar", ""]]}, {"id": "1806.05512", "submitter": "Alexander Wong", "authors": "Alexander Wong", "title": "NetScore: Towards Universal Metrics for Large-scale Performance Analysis\n  of Deep Neural Networks for Practical On-Device Edge Usage", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in the design of deep neural networks has been on improving\naccuracy, leading to more powerful yet highly complex network architectures\nthat are difficult to deploy in practical scenarios, particularly on edge\ndevices such as mobile and other consumer devices given their high\ncomputational and memory requirements. As a result, there has been a recent\ninterest in the design of quantitative metrics for evaluating deep neural\nnetworks that accounts for more than just model accuracy as the sole indicator\nof network performance. In this study, we continue the conversation towards\nuniversal metrics for evaluating the performance of deep neural networks for\npractical on-device edge usage. In particular, we propose a new balanced metric\ncalled NetScore, which is designed specifically to provide a quantitative\nassessment of the balance between accuracy, computational complexity, and\nnetwork architecture complexity of a deep neural network, which is important\nfor on-device edge operation. In what is one of the largest comparative\nanalysis between deep neural networks in literature, the NetScore metric, the\ntop-1 accuracy metric, and the popular information density metric were compared\nacross a diverse set of 60 different deep convolutional neural networks for\nimage classification on the ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC 2012) dataset. The evaluation results across these three metrics for\nthis diverse set of networks are presented in this study to act as a reference\nguide for practitioners in the field. The proposed NetScore metric, along with\nthe other tested metrics, are by no means perfect, but the hope is to push the\nconversation towards better universal metrics for evaluating deep neural\nnetworks for use in practical on-device edge scenarios to help guide\npractitioners in model design for such scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:55:35 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 01:33:02 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wong", "Alexander", ""]]}, {"id": "1806.05514", "submitter": "Cencheng Shen", "authors": "Cencheng Shen and Joshua T. Vogelstein", "title": "The Exact Equivalence of Distance and Kernel Methods for Hypothesis\n  Testing", "comments": "24 pages main + 7 pages appendix, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance-based tests, also called \"energy statistics\", are leading methods\nfor two-sample and independence tests from the statistics community.\nKernel-based tests, developed from \"kernel mean embeddings\", are leading\nmethods for two-sample and independence tests from the machine learning\ncommunity. A fixed-point transformation was previously proposed to connect the\ndistance methods and kernel methods for the population statistics. In this\npaper, we propose a new bijective transformation between metrics and kernels.\nIt simplifies the fixed-point transformation, inherits similar theoretical\nproperties, allows distance methods to be exactly the same as kernel methods\nfor sample statistics and p-value, and better preserves the data structure upon\ntransformation. Our results further advance the understanding in distance and\nkernel-based tests, streamline the code base for implementing these tests, and\nenable a rich literature of distance-based and kernel-based methodologies to\ndirectly communicate with each other.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:57:57 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 12:32:43 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 19:51:46 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 22:33:14 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2020 00:33:43 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Shen", "Cencheng", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1806.05559", "submitter": "Francis Gr\\'egoire", "authors": "Francis Gr\\'egoire, Philippe Langlais", "title": "Extracting Parallel Sentences with Bidirectional Recurrent Neural\n  Networks to Improve Machine Translation", "comments": "12 pages, 7 figures, COLING 2018. arXiv admin note: text overlap with\n  arXiv:1709.09783", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentence extraction is a task addressing the data sparsity problem\nfound in multilingual natural language processing applications. We propose a\nbidirectional recurrent neural network based approach to extract parallel\nsentences from collections of multilingual texts. Our experiments with noisy\nparallel corpora show that we can achieve promising results against a\ncompetitive baseline by removing the need of specific feature engineering or\nadditional external resources. To justify the utility of our approach, we\nextract sentence pairs from Wikipedia articles to train machine translation\nsystems and show significant improvements in translation performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:57:13 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 18:16:03 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gr\u00e9goire", "Francis", ""], ["Langlais", "Philippe", ""]]}, {"id": "1806.05575", "submitter": "Will Dabney", "authors": "Georg Ostrovski, Will Dabney, R\\'emi Munos", "title": "Autoregressive Quantile Networks for Generative Modeling", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce autoregressive implicit quantile networks (AIQN), a\nfundamentally different approach to generative modeling than those commonly\nused, that implicitly captures the distribution using quantile regression. AIQN\nis able to achieve superior perceptual quality and improvements in evaluation\nmetrics, without incurring a loss of sample diversity. The method can be\napplied to many existing models and architectures. In this work we extend the\nPixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using\nInception score, FID, non-cherry-picked samples, and inpainting results. We\nconsistently observe that AIQN yields a highly stable algorithm that improves\nperceptual quality while maintaining a highly diverse distribution.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:29:18 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Ostrovski", "Georg", ""], ["Dabney", "Will", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1806.05594", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson", "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should\n  Average", "comments": "Appears at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently the most successful approaches to semi-supervised learning are\nbased on consistency regularization, whereby a model is trained to be robust to\nsmall perturbations of its inputs and parameters. To understand consistency\nregularization, we conceptually explore how loss geometry interacts with\ntraining procedures. The consistency loss dramatically improves generalization\nperformance over supervised-only training; however, we show that SGD struggles\nto converge on the consistency loss and continues to make large steps that lead\nto changes in predictions on the test data. Motivated by these observations, we\npropose to train consistency-based methods with Stochastic Weight Averaging\n(SWA), a recent approach which averages weights along the trajectory of SGD\nwith a modified learning rate schedule. We also propose fast-SWA, which further\naccelerates convergence by averaging multiple points within each cycle of a\ncyclical learning rate schedule. With weight averaging, we achieve the best\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\nquantities of labeled training data. For example, we achieve 5.0% error on\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\nliterature of 6.3%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:58:36 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 16:21:21 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 15:26:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Finzi", "Marc", ""], ["Izmailov", "Pavel", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1806.05618", "submitter": "Matteo Papini", "authors": "Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta,\n  Marcello Restelli", "title": "Stochastic Variance-Reduced Policy Gradient", "comments": null, "journal-ref": "Proceedings of the 35 th International Conference on Machine\n  Learning, Stockholm, Sweden, PMLR 80, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel reinforcement- learning algorithm\nconsisting in a stochastic variance-reduced version of policy gradient for\nsolving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient\n(SVRG) methods have proven to be very successful in supervised learning.\nHowever, their adaptation to policy gradient is not straightforward and needs\nto account for I) a non-concave objective func- tion; II) approximations in the\nfull gradient com- putation; and III) a non-stationary sampling pro- cess. The\nresult is SVRPG, a stochastic variance- reduced policy gradient algorithm that\nleverages on importance weights to preserve the unbiased- ness of the gradient\nestimate. Under standard as- sumptions on the MDP, we provide convergence\nguarantees for SVRPG with a convergence rate that is linear under increasing\nbatch sizes. Finally, we suggest practical variants of SVRPG, and we\nempirically evaluate them on continuous MDPs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 15:49:13 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Papini", "Matteo", ""], ["Binaghi", "Damiano", ""], ["Canonaco", "Giuseppe", ""], ["Pirotta", "Matteo", ""], ["Restelli", "Marcello", ""]]}, {"id": "1806.05635", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee", "title": "Self-Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Self-Imitation Learning (SIL), a simple off-policy\nactor-critic algorithm that learns to reproduce the agent's past good\ndecisions. This algorithm is designed to verify our hypothesis that exploiting\npast good experiences can indirectly drive deep exploration. Our empirical\nresults show that SIL significantly improves advantage actor-critic (A2C) on\nseveral hard exploration Atari games and is competitive to the state-of-the-art\ncount-based exploration methods. We also show that SIL improves proximal policy\noptimization (PPO) on MuJoCo tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 16:25:55 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Oh", "Junhyuk", ""], ["Guo", "Yijie", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""]]}, {"id": "1806.05662", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen,\n  Ruslan Salakhutdinov, Yann LeCun", "title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep transfer learning approaches have mainly focused on learning\ngeneric feature vectors from one task that are transferable to other tasks,\nsuch as word embeddings in language and pretrained convolutional features in\nvision. However, these approaches usually transfer unary features and largely\nignore more structured graphical representations. This work explores the\npossibility of learning generic latent relational graphs that capture\ndependencies between pairs of data units (e.g., words or pixels) from\nlarge-scale unlabeled data and transferring the graphs to downstream tasks. Our\nproposed transfer learning framework improves performance on various tasks\nincluding question answering, natural language inference, sentiment analysis,\nand image classification. We also show that the learned graphs are generic\nenough to be transferred to different embeddings on which the graphs have not\nbeen trained (including GloVe embeddings, ELMo embeddings, and task-specific\nRNN hidden unit), or embedding-free units such as image pixels.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:41:19 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 04:36:08 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 20:24:33 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yang", "Zhilin", ""], ["Zhao", "Jake", ""], ["Dhingra", "Bhuwan", ""], ["He", "Kaiming", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""], ["LeCun", "Yann", ""]]}, {"id": "1806.05694", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Anastasios Noulas, Cecilia Mascoloo, and Zhongxiang Zhao", "title": "Discovering Latent Patterns of Urban Cultural Interactions in WeChat for\n  Modern City Planning", "comments": "10 pages, 10 figures, KDD'18: The 24th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining", "journal-ref": null, "doi": "10.1145/3219819.3219929", "report-no": null, "categories": "cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural activity is an inherent aspect of urban life and the success of a\nmodern city is largely determined by its capacity to offer generous cultural\nentertainment to its citizens. To this end, the optimal allocation of cultural\nestablishments and related resources across urban regions becomes of vital\nimportance, as it can reduce financial costs in terms of planning and improve\nquality of life in the city, more generally. In this paper, we make use of a\nlarge longitudinal dataset of user location check-ins from the online social\nnetwork WeChat to develop a data-driven framework for cultural planning in the\ncity of Beijing. We exploit rich spatio-temporal representations on user\nactivity at cultural venues and use a novel extended version of the traditional\nlatent Dirichlet allocation model that incorporates temporal information to\nidentify latent patterns of urban cultural interactions. Using the\ncharacteristic typologies of mobile user cultural activities emitted by the\nmodel, we determine the levels of demand for different types of cultural\nresources across urban areas. We then compare those with the corresponding\nlevels of supply as driven by the presence and spatial reach of cultural venues\nin local areas to obtain high resolution maps that indicate urban regions with\nlack of cultural resources, and thus give suggestions for further urban\ncultural planning and investment optimisation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:07:15 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Zhou", "Xiao", ""], ["Noulas", "Anastasios", ""], ["Mascoloo", "Cecilia", ""], ["Zhao", "Zhongxiang", ""]]}, {"id": "1806.05703", "submitter": "C.B. Scott", "authors": "C.B. Scott and Eric Mjolsness", "title": "Multilevel Artificial Neural Network Training for Spatially Correlated\n  Learning", "comments": "Manuscript (24 pages) and Supplementary Material (4 pages). Updated\n  January 2019 to reflect new formulation of MsANN structure and new training\n  procedure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multigrid modeling algorithms are a technique used to accelerate relaxation\nmodels running on a hierarchy of similar graphlike structures. We introduce and\ndemonstrate a new method for training neural networks which uses multilevel\nmethods. Using an objective function derived from a graph-distance metric, we\nperform orthogonally-constrained optimization to find optimal prolongation and\nrestriction maps between graphs. We compare and contrast several methods for\nperforming this numerical optimization, and additionally present some new\ntheoretical results on upper bounds of this type of objective function. Once\ncalculated, these optimal maps between graphs form the core of Multiscale\nArtificial Neural Network (MsANN) training, a new procedure we present which\nsimultaneously trains a hierarchy of neural network models of varying spatial\nresolution. Parameter information is passed between members of this hierarchy\naccording to standard coarsening and refinement schedules from the multiscale\nmodelling literature. In our machine learning experiments, these models are\nable to learn faster than default training, achieving a comparable level of\nerror in an order of magnitude fewer training examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:50:47 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 03:15:58 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 19:17:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Scott", "C. B.", ""], ["Mjolsness", "Eric", ""]]}, {"id": "1806.05722", "submitter": "Samet Oymak", "authors": "Samet Oymak and Necmiye Ozay", "title": "Non-asymptotic Identification of LTI Systems from a Single Trajectory", "comments": "Version 2 has two improvements: First, paper now uses spectral radius\n  rather than largest singular value hence applies to a larger class of\n  systems. Secondly, new sample complexity bounds are provided for\n  approximating the system's Hankel operator via estimated Markov parameters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a realization for a linear time-invariant\n(LTI) dynamical system from input/output data. Given a single input/output\ntrajectory, we provide finite time analysis for learning the system's Markov\nparameters, from which a balanced realization is obtained using the classical\nHo-Kalman algorithm. By proving a stability result for the Ho-Kalman algorithm\nand combining it with the sample complexity results for Markov parameters, we\nshow how much data is needed to learn a balanced realization of the system up\nto a desired accuracy with high probability.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:05:25 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 23:44:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Oymak", "Samet", ""], ["Ozay", "Necmiye", ""]]}, {"id": "1806.05730", "submitter": "Ming Yu", "authors": "Ming Yu, Varun Gupta, Mladen Kolar", "title": "Learning Influence-Receptivity Network Structure with Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional works on community detection from observations of information\ncascade assume that a single adjacency matrix parametrizes all the observed\ncascades. However, in reality the connection structure usually does not stay\nthe same across cascades. For example, different people have different topics\nof interest, therefore the connection structure depends on the\ninformation/topic content of the cascade. In this paper we consider the case\nwhere we observe a sequence of noisy adjacency matrices triggered by\ninformation/event with different topic distributions. We propose a novel latent\nmodel using the intuition that a connection is more likely to exist between two\nnodes if they are interested in similar topics, which are common with the\ninformation/event. Specifically, we endow each node with two node-topic\nvectors: an influence vector that measures how influential/authoritative they\nare on each topic; and a receptivity vector that measures how\nreceptive/susceptible they are to each topic. We show how these two node-topic\nstructures can be estimated from observed adjacency matrices with theoretical\nguarantee on estimation error, in cases where the topic distributions of the\ninformation/event are known, as well as when they are unknown. Experiments on\nsynthetic and real data demonstrate the effectiveness of our model and superior\nperformance compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:24:16 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 16:27:34 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Yu", "Ming", ""], ["Gupta", "Varun", ""], ["Kolar", "Mladen", ""]]}, {"id": "1806.05738", "submitter": "Jingyu He", "authors": "P. Richard Hahn, Jingyu He, Hedibert Lopes", "title": "Efficient sampling for Gaussian linear regression with arbitrary priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a slice sampler for Bayesian linear regression models\nwith arbitrary priors. The new sampler has two advantages over current\napproaches. One, it is faster than many custom implementations that rely on\nauxiliary latent variables, if the number of regressors is large. Two, it can\nbe used with any prior with a density function that can be evaluated up to a\nnormalizing constant, making it ideal for investigating the properties of new\nshrinkage priors without having to develop custom sampling algorithms. The new\nsampler takes advantage of the special structure of the linear regression\nlikelihood, allowing it to produce better effective sample size per second than\ncommon alternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:39:06 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Hahn", "P. Richard", ""], ["He", "Jingyu", ""], ["Lopes", "Hedibert", ""]]}, {"id": "1806.05759", "submitter": "Ari Morcos", "authors": "Ari S. Morcos, Maithra Raghu, and Samy Bengio", "title": "Insights on representational similarity in neural networks with\n  canonical correlation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 22:34:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 23:09:23 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 18:59:02 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Morcos", "Ari S.", ""], ["Raghu", "Maithra", ""], ["Bengio", "Samy", ""]]}, {"id": "1806.05767", "submitter": "Ahmed Qureshi", "authors": "Ahmed H. Qureshi, Anthony Simeonov, Mayur J. Bency and Michael C. Yip", "title": "Motion Planning Networks", "comments": "Paper published in ICRA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and efficient motion planning algorithms are crucial for many\nstate-of-the-art robotics applications such as self-driving cars. Existing\nmotion planning methods become ineffective as their computational complexity\nincreases exponentially with the dimensionality of the motion planning problem.\nTo address this issue, we present Motion Planning Networks (MPNet), a neural\nnetwork-based novel planning algorithm. The proposed method encodes the given\nworkspaces directly from a point cloud measurement and generates the end-to-end\ncollision-free paths for the given start and goal configurations. We evaluate\nMPNet on various 2D and 3D environments including the planning of a 7 DOF\nBaxter robot manipulator. The results show that MPNet is not only consistently\ncomputationally efficient in all environments but also generalizes to\ncompletely unseen environments. The results also show that the computation time\nof MPNet consistently remains less than 1 second in all presented experiments,\nwhich is significantly lower than existing state-of-the-art motion planning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 23:48:08 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 07:05:44 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Qureshi", "Ahmed H.", ""], ["Simeonov", "Anthony", ""], ["Bency", "Mayur J.", ""], ["Yip", "Michael C.", ""]]}, {"id": "1806.05768", "submitter": "Joseph Clements", "authors": "Joseph Clements, Yingjie Lao", "title": "Hardware Trojan Attacks on Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising popularity of machine learning and the ever increasing demand\nfor computational power, there is a growing need for hardware optimized\nimplementations of neural networks and other machine learning models. As the\ntechnology evolves, it is also plausible that machine learning or artificial\nintelligence will soon become consumer electronic products and military\nequipment, in the form of well-trained models. Unfortunately, the modern\nfabless business model of manufacturing hardware, while economic, leads to\ndeficiencies in security through the supply chain. In this paper, we illuminate\nthese security issues by introducing hardware Trojan attacks on neural\nnetworks, expanding the current taxonomy of neural network security to\nincorporate attacks of this nature. To aid in this, we develop a novel\nframework for inserting malicious hardware Trojans in the implementation of a\nneural network classifier. We evaluate the capabilities of the adversary in\nthis setting by implementing the attack algorithm on convolutional neural\nnetworks while controlling a variety of parameters available to the adversary.\nOur experimental results show that the proposed algorithm could effectively\nclassify a selected input trigger as a specified class on the MNIST dataset by\ninjecting hardware Trojans into $0.03\\%$, on average, of neurons in the 5th\nhidden layer of arbitrary 7-layer convolutional neural networks, while\nundetectable under the test data. Finally, we discuss the potential defenses to\nprotect neural networks against hardware Trojan attacks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 23:49:55 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Clements", "Joseph", ""], ["Lao", "Yingjie", ""]]}, {"id": "1806.05779", "submitter": "Michele Pratusevich", "authors": "Michele Pratusevich", "title": "Deep Learning Approximation: Zero-Shot Neural Network Speedup", "comments": "Submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks offer high-accuracy solutions to a range of problems, but are\ncostly to run in production systems because of computational and memory\nrequirements during a forward pass. Given a trained network, we propose a\ntechique called Deep Learning Approximation to build a faster network in a tiny\nfraction of the time required for training by only manipulating the network\nstructure and coefficients without requiring re-training or access to the\ntraining data. Speedup is achieved by by applying a sequential series of\nindependent optimizations that reduce the floating-point operations (FLOPs)\nrequired to perform a forward pass. First, lossless optimizations are applied,\nfollowed by lossy approximations using singular value decomposition (SVD) and\nlow-rank matrix decomposition. The optimal approximation is chosen by weighing\nthe relative accuracy loss and FLOP reduction according to a single parameter\nspecified by the user. On PASCAL VOC 2007 with the YOLO network, we show an\nend-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can\nbe re-gained by finetuning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:25:47 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Pratusevich", "Michele", ""]]}, {"id": "1806.05780", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Zachary C Lipton,\n  Animashree Anandkumar", "title": "Surprising Negative Results for Generative Adversarial Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many recent advances in deep reinforcement learning (RL) rely on\nmodel-free methods, model-based approaches remain an alluring prospect for\ntheir potential to exploit unsupervised data to learn environment model. In\nthis work, we provide an extensive study on the design of deep generative\nmodels for RL environments and propose a sample efficient and robust method to\nlearn the model of Atari environments. We deploy this model and propose\ngenerative adversarial tree search (GATS) a deep RL algorithm that learns the\nenvironment model and implements Monte Carlo tree search (MCTS) on the learned\nmodel for planning. While MCTS on the learned model is computationally\nexpensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs\ndeep Q network (DQN) and learns a Q-function to assign values to the leaves of\nthe tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance\ntrade-off and show GATS is able to mitigate the worst-case error in the\nQ-estimate. While we were expecting GATS to enjoy a better sample complexity\nand faster converges to better policies, surprisingly, GATS fails to outperform\nDQN. We provide a study on which we show why depth limited MCTS fails to\nperform desirably.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:35:03 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 07:09:45 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 04:38:55 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 02:31:13 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Yang", "Brandon", ""], ["Liu", "Weitang", ""], ["Lipton", "Zachary C", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1806.05789", "submitter": "Usman Roshan", "authors": "Yunzhe Xue and Usman Roshan", "title": "Image classification and retrieval with random depthwise signed\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a random convolutional neural network to generate a feature space\nin which we study image classification and retrieval performance. Put briefly\nwe apply random convolutional blocks followed by global average pooling to\ngenerate a new feature, and we repeat this k times to produce a k-dimensional\nfeature space. This can be interpreted as partitioning the space of image\npatches with random hyperplanes which we formalize as a random depthwise\nconvolutional neural network. In the network's final layer we perform image\nclassification and retrieval with the linear support vector machine and\nk-nearest neighbor classifiers and study other empirical properties. We show\nthat the ratio of image pixel distribution similarity across classes to within\nclasses is higher in our network's final layer compared to the input space.\nWhen we apply the linear support vector machine for image classification we see\nthat the accuracy is higher than if we were to train just the final layer of\nVGG16, ResNet18, and DenseNet40 with random weights. In the same setting we\ncompare it to an unsupervised feature learning method and find our accuracy to\nbe comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the\naccuracy is not far behind that of trained networks, particularly in the top-k\nsetting. For example the top-2 accuracy of our network is near 90% on both\nCIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest\nneighbor gives a comparable precision on the Corel Princeton Image Similarity\nBenchmark than if we were to use the final layer of trained networks. As with\nother networks we find that our network fails to a black box attack even though\nwe lack a gradient and use the sign activation. We highlight sensitivity of our\nnetwork to background as a potential pitfall and an advantage. Overall our work\npushes the boundary of what can be achieved with random weights.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 02:26:11 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 22:12:31 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 21:20:48 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Xue", "Yunzhe", ""], ["Roshan", "Usman", ""]]}, {"id": "1806.05791", "submitter": "Hiroaki Nakajima", "authors": "Hiroaki Nakajima, Yu Takahashi, Kazunobu Kondo and Yuji Hisaminato", "title": "Monaural source enhancement maximizing source-to-distortion ratio via\n  automatic differentiation", "comments": "This paper is submitted to 16th International Workshop on Acoustic\n  Signal Enhancement (IWAENC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural network (DNN) has made a breakthrough in monaural\nsource enhancement. Through a training step by using a large amount of data,\nDNN estimates a mapping between mixed signals and clean signals. At this time,\nwe use an objective function that numerically expresses the quality of a\nmapping by DNN. In the conventional methods, L1 norm, L2 norm, and\nItakura-Saito divergence are often used as objective functions. Recently, an\nobjective function based on short-time objective intelligibility (STOI) has\nalso been proposed. However, these functions only indicate similarity between\nthe clean signal and the estimated signal by DNN. In other words, they do not\nshow the quality of noise reduction or source enhancement. Motivated by the\nfact, this paper adopts signal-to-distortion ratio (SDR) as the objective\nfunction. Since SDR virtually shows signal-to-noise ratio (SNR), maximizing SDR\nsolves the above problem. The experimental results revealed that the proposed\nmethod achieved better performance than the conventional methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 02:38:40 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Nakajima", "Hiroaki", ""], ["Takahashi", "Yu", ""], ["Kondo", "Kazunobu", ""], ["Hisaminato", "Yuji", ""]]}, {"id": "1806.05805", "submitter": "Jaechang Lim", "authors": "Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim", "title": "Molecular generative model based on conditional variational autoencoder\n  for de novo molecular design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a molecular generative model based on the conditional variational\nautoencoder for de novo molecular design. It is specialized to control multiple\nmolecular properties simultaneously by imposing them on a latent space. As a\nproof of concept, we demonstrate that it can be used to generate drug-like\nmolecules with five target properties. We were also able to adjust a single\nproperty without changing the others and to manipulate it beyond the range of\nthe dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 05:32:00 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Lim", "Jaechang", ""], ["Ryu", "Seongok", ""], ["Kim", "Jin Woo", ""], ["Kim", "Woo Youn", ""]]}, {"id": "1806.05810", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Samuel Rota Bul\\`o, Barbara Caputo, Elisa Ricci", "title": "Best sources forward: domain generalization through source-specific nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long standing problem in visual object categorization is the ability of\nalgorithms to generalize across different testing conditions. The problem has\nbeen formalized as a covariate shift among the probability distributions\ngenerating the training data (source) and the test data (target) and several\ndomain adaptation methods have been proposed to address this issue. While these\napproaches have considered the single source-single target scenario, it is\nplausible to have multiple sources and require adaptation to any possible\ntarget domain. This last scenario, named Domain Generalization (DG), is the\nfocus of our work. Differently from previous DG methods which learn domain\ninvariant representations from source data, we design a deep network with\nmultiple domain-specific classifiers, each associated to a source domain. At\ntest time we estimate the probabilities that a target sample belongs to each\nsource domain and exploit them to optimally fuse the classifiers predictions.\nTo further improve the generalization ability of our model, we also introduced\na domain agnostic component supporting the final classifier. Experiments on two\npublic benchmarks demonstrate the power of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 05:42:20 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "1806.05817", "submitter": "Shaogang Ren", "authors": "Shaogang Ren, Jianhua Z. Huang, Shuai Huang, Xiaoning Qian", "title": "Safe Active Feature Selection for Sparse Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present safe active incremental feature selection~(SAIF) to scale up the\ncomputation of LASSO solutions. SAIF does not require a solution from a heavier\npenalty parameter as in sequential screening or updating the full model for\neach iteration as in dynamic screening. Different from these existing screening\nmethods, SAIF starts from a small number of features and incrementally recruits\nactive features and updates the significantly reduced model. Hence, it is much\nmore computationally efficient and scalable with the number of features. More\ncritically, SAIF has the safe guarantee as it has the convergence guarantee to\nthe optimal solution to the original full LASSO problem. Such an incremental\nprocedure and theoretical convergence guarantee can be extended to fused LASSO\nproblems. Compared with state-of-the-art screening methods as well as working\nset and homotopy methods, which may not always guarantee the optimal solution,\nSAIF can achieve superior or comparable efficiency and high scalability with\nthe safe guarantee when facing extremely high dimensional data sets.\nExperiments with both synthetic and real-world data sets show that SAIF can be\nup to 50 times faster than dynamic screening, and hundreds of times faster than\ncomputing LASSO or fused LASSO solutions without screening.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:05:48 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 17:30:32 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Ren", "Shaogang", ""], ["Huang", "Jianhua Z.", ""], ["Huang", "Shuai", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1806.05819", "submitter": "Chang Li", "authors": "Chang Li, Branislav Kveton, Tor Lattimore, Ilya Markov, Maarten de\n  Rijke, Csaba Szepesvari, and Masrour Zoghi", "title": "BubbleRank: Safe Online Learning to Re-Rank via Implicit Click Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of safe online learning to re-rank, where\nuser feedback is used to improve the quality of displayed lists. Learning to\nrank has traditionally been studied in two settings. In the offline setting,\nrankers are typically learned from relevance labels created by judges. This\napproach has generally become standard in industrial applications of ranking,\nsuch as search. However, this approach lacks exploration and thus is limited by\nthe information content of the offline training data. In the online setting, an\nalgorithm can experiment with lists and learn from feedback on them in a\nsequential fashion. Bandit algorithms are well-suited for this setting but they\ntend to learn user preferences from scratch, which results in a high initial\ncost of exploration. This poses an additional challenge of safe exploration in\nranked lists. We propose BubbleRank, a bandit algorithm for safe re-ranking\nthat combines the strengths of both the offline and online settings. The\nalgorithm starts with an initial base list and improves it online by gradually\nexchanging higher-ranked less attractive items for lower-ranked more attractive\nitems. We prove an upper bound on the n-step regret of BubbleRank that degrades\ngracefully with the quality of the initial base list. Our theoretical findings\nare supported by extensive experiments on a large-scale real-world click\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:17:55 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 14:23:22 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Li", "Chang", ""], ["Kveton", "Branislav", ""], ["Lattimore", "Tor", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""], ["Szepesvari", "Csaba", ""], ["Zoghi", "Masrour", ""]]}, {"id": "1806.05823", "submitter": "Christoph Brauer", "authors": "Christoph Brauer and Dirk Lorenz", "title": "Primal-dual residual networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep neural network architecture motivated by\nprimal-dual splitting methods from convex optimization. We show theoretically\nthat there exists a close relation between the derived architecture and\nresidual networks, and further investigate this connection in numerical\nexperiments. Moreover, we demonstrate how our approach can be used to unroll\noptimization algorithms for certain problems with hard constraints. Using the\nexample of speech dequantization, we show that our method can outperform\nclassical splitting methods when both are applied to the same task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:34:34 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Brauer", "Christoph", ""], ["Lorenz", "Dirk", ""]]}, {"id": "1806.05824", "submitter": "Alexandre Benoit", "authors": "Amina Ben Hamida (LISTIC), A Benoit (LISTIC), Patrick Lambert\n  (LISTIC), Chokri Ben Amar (REGIM)", "title": "Three dimensional Deep Learning approach for remote sensing image\n  classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Institute of\n  Electrical and Electronics Engineers, 2018, pp.1 - 15", "doi": "10.1109/TGRS.2018.2818945", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a variety of approaches has been enriching the field of Remote\nSensing (RS) image processing and analysis. Unfortunately, existing methods\nremain limited faced to the rich spatio-spectral content of today's large\ndatasets. It would seem intriguing to resort to Deep Learning (DL) based\napproaches at this stage with regards to their ability to offer accurate\nsemantic interpretation of the data. However, the specificity introduced by the\ncoexistence of spectral and spatial content in the RS datasets widens the scope\nof the challenges presented to adapt DL methods to these contexts. Therefore,\nthe aim of this paper is firstly to explore the performance of DL architectures\nfor the RS hyperspectral dataset classification and secondly to introduce a new\nthree-dimensional DL approach that enables a joint spectral and spatial\ninformation process. A set of three-dimensional schemes is proposed and\nevaluated. Experimental results based on well knownhyperspectral datasets\ndemonstrate that the proposed method is able to achieve a better classification\nrate than state of the art methods with lower computational costs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:35:47 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Hamida", "Amina Ben", "", "LISTIC"], ["Benoit", "A", "", "LISTIC"], ["Lambert", "Patrick", "", "LISTIC"], ["Amar", "Chokri Ben", "", "REGIM"]]}, {"id": "1806.05833", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "On the exact minimization of saturated loss functions for robust\n  regression and subspace estimation", "comments": "Pattern Recognition Letters, Elsevier, 2018", "journal-ref": null, "doi": "10.1016/j.patrec.2018.08.004", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust regression and subspace estimation and more\nprecisely with the problem of minimizing a saturated loss function. In\nparticular, we focus on computational complexity issues and show that an exact\nalgorithm with polynomial time-complexity with respect to the number of data\ncan be devised for robust regression and subspace estimation. This result is\nobtained by adopting a classification point of view and relating the problems\nto the search for a linear model that can approximate the maximal number of\npoints with a given error. Approximate variants of the algorithms based on\nramdom sampling are also discussed and experiments show that it offers an\naccuracy gain over the traditional RANSAC for a similar algorithmic simplicity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 07:24:36 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 07:12:13 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1806.05865", "submitter": "Adam Gaier", "authors": "Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret", "title": "Data-Efficient Design Exploration through Surrogate-Assisted\n  Illumination", "comments": "ArXiv preprint version, final version published in Evolutionary\n  Computation, doi: 10.1162/evco_a_00231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design optimization techniques are often used at the beginning of the design\nprocess to explore the space of possible designs. In these domains illumination\nalgorithms, such as MAP-Elites, are promising alternatives to classic\noptimization algorithms because they produce diverse, high-quality solutions in\na single run, instead of only a single near-optimal solution. Unfortunately,\nthese algorithms currently require a large number of function evaluations,\nlimiting their applicability. In this article we introduce a new illumination\nalgorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate\nmodeling techniques to create a map of the design space according to\nuser-defined features while minimizing the number of fitness evaluations. On a\n2-dimensional airfoil optimization problem SAIL produces hundreds of diverse\nbut high-performing designs with several orders of magnitude fewer evaluations\nthan MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of\nproducing maps of high-performing designs in realistic 3-dimensional\naerodynamic tasks with an accurate flow simulation. Data-efficient design\nexploration with SAIL can help designers understand what is possible, beyond\nwhat is optimal, by considering more than pure objective-based optimization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:00:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Gaier", "Adam", ""], ["Asteroth", "Alexander", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.05876", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Financial Risk and Returns Prediction with Modular Networked Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:49:39 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1806.05892", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Shabnam Ghaffarzadegan, Zhe Feng and Taufiq\n  Hasan", "title": "Learning Front-end Filter-bank Parameters using Convolutional Neural\n  Networks for Abnormal Heart Sound Detection", "comments": "4 pages, 6 figures, IEEE International Engineering in Medicine and\n  Biology Conference (EMBC)", "journal-ref": null, "doi": "10.1109/EMBC.2018.8512578", "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic heart sound abnormality detection can play a vital role in the\nearly diagnosis of heart diseases, particularly in low-resource settings. The\nstate-of-the-art algorithms for this task utilize a set of Finite Impulse\nResponse (FIR) band-pass filters as a front-end followed by a Convolutional\nNeural Network (CNN) model. In this work, we propound a novel CNN architecture\nthat integrates the front-end bandpass filters within the network using\ntime-convolution (tConv) layers, which enables the FIR filter-bank parameters\nto become learnable. Different initialization strategies for the learnable\nfilters, including random parameters and a set of predefined FIR filter-bank\ncoefficients, are examined. Using the proposed tConv layers, we add constraints\nto the learnable FIR filters to ensure linear and zero phase responses.\nExperimental evaluations are performed on a balanced 4-fold cross-validation\ntask prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that\nthe proposed models yield superior performance compared to the state-of-the-art\nsystem, while the linear phase FIR filterbank method provides an absolute\nimprovement of 9.54% over the baseline in terms of an overall accuracy metric.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:33:31 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Feng", "Zhe", ""], ["Hasan", "Taufiq", ""]]}, {"id": "1806.05897", "submitter": "Eyke H\\\"ullermeier", "authors": "Sascha Henzgen and Eyke H\\\"ullermeier", "title": "Mining Rank Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of frequent pattern mining has been studied quite extensively for\nvarious types of data, including sets, sequences, and graphs. Somewhat\nsurprisingly, another important type of data, namely rank data, has received\nvery little attention in data mining so far. In this paper, we therefore\naddresses the problem of mining rank data, that is, data in the form of\nrankings (total orders) of an underlying set of items. More specifically, two\ntypes of patterns are considered, namely frequent rankings and dependencies\nbetween such rankings in the form of association rules. Algorithms for mining\nfrequent rankings and frequent closed rankings are proposed and tested\nexperimentally, using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:36:40 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Henzgen", "Sascha", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1806.05924", "submitter": "Daniel Andrade", "authors": "Daniel Andrade, Akiko Takeda, Kenji Fukumizu", "title": "Robust Bayesian Model Selection for Variable Clustering with the\n  Gaussian Graphical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable clustering is important for explanatory analysis. However, only few\ndedicated methods for variable clustering with the Gaussian graphical model\nhave been proposed. Even more severe, small insignificant partial correlations\ndue to noise can dramatically change the clustering result when evaluating for\nexample with the Bayesian Information Criteria (BIC). In this work, we try to\naddress this issue by proposing a Bayesian model that accounts for negligible\nsmall, but not necessarily zero, partial correlations. Based on our model, we\npropose to evaluate a variable clustering result using the marginal likelihood.\nTo address the intractable calculation of the marginal likelihood, we propose\ntwo solutions: one based on a variational approximation, and another based on\nMCMC. Experiments on simulated data shows that the proposed method is similarly\naccurate as BIC in the no noise setting, but considerably more accurate when\nthere are noisy partial correlations. Furthermore, on real data the proposed\nmethod provides clustering results that are intuitively sensible, which is not\nalways the case when using BIC or its extensions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 12:06:03 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Andrade", "Daniel", ""], ["Takeda", "Akiko", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1806.05938", "submitter": "Eli Chien", "authors": "I Chien, Chao Pan and Olgica Milenkovic", "title": "Query K-means Clustering and the Double Dixie Cup Problem", "comments": "To be appeared in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate $K$-means clustering with outliers and\nside information provided by same-cluster queries and possibly noisy answers.\nOur solution shows that, under some mild assumptions on the smallest cluster\nsize, one can obtain an $(1+\\epsilon)$-approximation for the optimal potential\nwith probability at least $1-\\delta$, where $\\epsilon>0$ and $\\delta\\in(0,1)$,\nusing an expected number of $O(\\frac{K^3}{\\epsilon \\delta})$ noiseless\nsame-cluster queries and comparison-based clustering of complexity $O(ndK +\n\\frac{K^3}{\\epsilon \\delta})$, here, $n$ denotes the number of points and $d$\nthe dimension of space. Compared to a handful of other known approaches that\nperform importance sampling to account for small cluster sizes, the proposed\nquery technique reduces the number of queries by a factor of roughly\n$O(\\frac{K^6}{\\epsilon^3})$, at the cost of possibly missing very small\nclusters. We extend this settings to the case where some queries to the oracle\nproduce erroneous information, and where certain points, termed outliers, do\nnot belong to any clusters. Our proof techniques differ from previous methods\nused for $K$-means clustering analysis, as they rely on estimating the sizes of\nthe clusters and the number of points needed for accurate centroid estimation\nand subsequent nontrivial generalizations of the double Dixie cup problem. We\nillustrate the performance of the proposed algorithm both on synthetic and real\ndatasets, including MNIST and CIFAR $10$.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:04:13 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 17:25:37 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chien", "I", ""], ["Pan", "Chao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1806.05953", "submitter": "Jin Xu", "authors": "Jin Xu, Yee Whye Teh", "title": "Controllable Semantic Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for user-controllable semantic image inpainting: Given an\narbitrary set of observed pixels, the unobserved pixels can be imputed in a\nuser-controllable range of possibilities, each of which is semantically\ncoherent and locally consistent with the observed pixels. We achieve this using\na deep generative model bringing together: an encoder which can encode an\narbitrary set of observed pixels, latent variables which are trained to\nrepresent disentangled factors of variations, and a bidirectional PixelCNN\nmodel. We experimentally demonstrate that our method can generate plausible\ninpainting results matching the user-specified semantics, but is still coherent\nwith observed pixels. We justify our choices of architecture and training\nregime through more experiments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:35:19 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Xu", "Jin", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1806.05964", "submitter": "Ivan Glasser", "authors": "Ivan Glasser, Nicola Pancotti, J. Ignacio Cirac", "title": "From probabilistic graphical models to generalized tensor networks for\n  supervised learning", "comments": "15 pages, 18 figures, improved version with additional explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.str-el cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks have found a wide use in a variety of applications in physics\nand computer science, recently leading to both theoretical insights as well as\npractical algorithms in machine learning. In this work we explore the\nconnection between tensor networks and probabilistic graphical models, and show\nthat it motivates the definition of generalized tensor networks where\ninformation from a tensor can be copied and reused in other parts of the\nnetwork. We discuss the relationship between generalized tensor network\narchitectures used in quantum physics, such as string-bond states, and\narchitectures commonly used in machine learning. We provide an algorithm to\ntrain these networks in a supervised-learning context and show that they\novercome the limitations of regular tensor networks in higher dimensions, while\nkeeping the computation efficient. A method to combine neural networks and\ntensor networks as part of a common deep learning architecture is also\nintroduced. We benchmark our algorithm for several generalized tensor network\narchitectures on the task of classifying images and sounds, and show that they\noutperform previously introduced tensor-network algorithms. The models we\nconsider also have a natural implementation on a quantum computer and may guide\nthe development of near-term quantum machine learning architectures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:47:50 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 11:28:46 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Glasser", "Ivan", ""], ["Pancotti", "Nicola", ""], ["Cirac", "J. Ignacio", ""]]}, {"id": "1806.05975", "submitter": "Soumya Ghosh", "authors": "Soumya Ghosh, Jiayu Yao, Finale Doshi-Velez", "title": "Structured Variational Learning of Bayesian Neural Networks with\n  Horseshoe Priors", "comments": "ICML 2018. v2 -- Minor edits and fixes typos. arXiv admin note: text\n  overlap with arXiv:1705.10388", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Neural Networks (BNNs) have recently received increasing attention\nfor their ability to provide well-calibrated posterior uncertainties. However,\nmodel selection---even choosing the number of nodes---remains an open question.\nRecent work has proposed the use of a horseshoe prior over node pre-activations\nof a Bayesian neural network, which effectively turns off nodes that do not\nhelp explain the data. In this work, we propose several modeling and inference\nadvances that consistently improve the compactness of the model learned while\nmaintaining predictive performance, especially in smaller-sample settings\nincluding reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:20:43 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 13:48:56 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ghosh", "Soumya", ""], ["Yao", "Jiayu", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1806.05978", "submitter": "Felix Laumann", "authors": "Kumar Shridhar, Felix Laumann, Marcus Liwicki", "title": "Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:55:18 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 11:35:35 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:37:03 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 08:16:32 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 13:48:37 GMT"}, {"version": "v6", "created": "Tue, 14 May 2019 09:04:11 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Shridhar", "Kumar", ""], ["Laumann", "Felix", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1806.06003", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier", "title": "On Machine Learning and Structure for Mobile Robots", "comments": "Informal Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 14:49:35 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Wulfmeier", "Markus", ""]]}, {"id": "1806.06047", "submitter": "Richard Combes", "authors": "Richard Combes and Mikael Touati", "title": "Computationally Efficient Estimation of the Spectral Gap of a Markov\n  Chain", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating from sample paths the absolute spectral\ngap $\\gamma_*$ of a reversible, irreducible and aperiodic Markov chain\n$(X_t)_{t \\in \\mathbb{N}}$ over a finite state space $\\Omega$. We propose the\n${\\tt UCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a\nlow-complexity algorithm which estimates the spectral gap in time ${\\cal O}(n)$\nand memory space ${\\cal O}((\\ln n)^2)$ given $n$ samples. This is in stark\ncontrast with most known methods which require at least memory space ${\\cal\nO}(|\\Omega|)$, so that they cannot be applied to large state spaces.\nFurthermore, ${\\tt UCPI}$ is amenable to parallel implementation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:14:53 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 17:14:25 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Combes", "Richard", ""], ["Touati", "Mikael", ""]]}, {"id": "1806.06055", "submitter": "Huang Lingxiao", "authors": "L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi", "title": "Classification with Fairness Constraints: A Meta-Algorithm with Provable\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing classification algorithms that are fair with respect to sensitive\nattributes of the data has become an important problem due to the growing\ndeployment of classification algorithms in various social contexts. Several\nrecent works have focused on fairness with respect to a specific metric,\nmodeled the corresponding fair classification problem as a constrained\noptimization problem, and developed tailored algorithms to solve them. Despite\nthis, there still remain important metrics for which we do not have fair\nclassifiers and many of the aforementioned algorithms do not come with\ntheoretical guarantees; perhaps because the resulting optimization problem is\nnon-convex. The main contribution of this paper is a new meta-algorithm for\nclassification that takes as input a large class of fairness constraints, with\nrespect to multiple non-disjoint sensitive attributes, and which comes with\nprovable guarantees. This is achieved by first developing a meta-algorithm for\na large family of classification problems with convex constraints, and then\nshowing that classification problems with general types of fairness constraints\ncan be reduced to those in this family. We present empirical results that show\nthat our algorithm can achieve near-perfect fairness with respect to various\nfairness metrics, and that the loss in accuracy due to the imposed fairness\nconstraints is often small. Overall, this work unifies several prior works on\nfair classification, presents a practical algorithm with theoretical\nguarantees, and can handle fairness metrics that were previously not possible.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:45:58 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:53:43 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 06:08:21 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Keswani", "Vijay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1806.06063", "submitter": "Maximilian Sieb", "authors": "Maximilian Sieb, Matthias Schultheis, Sebastian Szelag, Rudolf\n  Lioutikov, Jan Peters", "title": "Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet\n  Process Switching Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using movement primitive libraries is an effective means to enable robots to\nsolve more complex tasks. In order to build these movement libraries, current\nalgorithms require a prior segmentation of the demonstration trajectories. A\npromising approach is to model the trajectory as being generated by a set of\nSwitching Linear Dynamical Systems and inferring a meaningful segmentation by\ninspecting the transition points characterized by the switching dynamics. With\nrespect to the learning, a nonparametric Bayesian approach is employed\nutilizing a Gibbs sampler.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 19:37:37 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 17:30:23 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 16:42:38 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sieb", "Maximilian", ""], ["Schultheis", "Matthias", ""], ["Szelag", "Sebastian", ""], ["Lioutikov", "Rudolf", ""], ["Peters", "Jan", ""]]}, {"id": "1806.06068", "submitter": "Utku Evci", "authors": "Utku Evci", "title": "Detecting Dead Weights and Units in Neural Networks", "comments": "M.Sc. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are highly over-parameterized and the size of the neural\nnetworks can be reduced significantly after training without any decrease in\nperformance. One can clearly see this phenomenon in a wide range of\narchitectures trained for various problems. Weight/channel pruning,\ndistillation, quantization, matrix factorization are some of the main methods\none can use to remove the redundancy to come up with smaller and faster models.\n  This work starts with a short informative chapter, where we motivate the\npruning idea and provide the necessary notation. In the second chapter, we\ncompare various saliency scores in the context of parameter pruning. Using the\ninsights obtained from this comparison and stating the problems it brings we\nmotivate why pruning units instead of the individual parameters might be a\nbetter idea. We propose some set of definitions to quantify and analyze units\nthat don't learn and create any useful information. We propose an efficient way\nfor detecting dead units and use it to select which units to prune. We get 5x\nmodel size reduction through unit-wise pruning on MNIST.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:58:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Evci", "Utku", ""]]}, {"id": "1806.06086", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Vincent Chen, Wing Wong", "title": "Minibatch Gibbs Sampling on Large Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is the de facto Markov chain Monte Carlo method used for\ninference and learning on large scale graphical models. For complicated factor\ngraphs with lots of factors, the performance of Gibbs sampling can be limited\nby the computational cost of executing a single update step of the Markov\nchain. This cost is proportional to the degree of the graph, the number of\nfactors adjacent to each variable. In this paper, we show how this cost can be\nreduced by using minibatching: subsampling the factors to form an estimate of\ntheir sum. We introduce several minibatched variants of Gibbs, show that they\ncan be made unbiased, prove bounds on their convergence rates, and show that\nunder some conditions they can result in asymptotic single-update-run-time\nspeedups over plain Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 18:26:11 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["De Sa", "Christopher", ""], ["Chen", "Vincent", ""], ["Wong", "Wing", ""]]}, {"id": "1806.06095", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu and Yao Xie", "title": "Crime Event Embedding with Unsupervised Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel event embedding algorithm for crime data that can jointly\ncapture time, location, and the complex free-text component of each event. The\nembedding is achieved by regularized Restricted Boltzmann Machines (RBMs), and\nwe introduce a new way to regularize by imposing a $\\ell_1$ penalty on the\nconditional distributions of the observed variables of RBMs. This choice of\nregularization performs feature selection and it also leads to efficient\ncomputation since the gradient can be computed in a closed form. The feature\nselection forces embedding to be based on the most important keywords, which\ncaptures the common modus operandi (M. O.) in crime series. Using numerical\nexperiments on a large-scale crime dataset, we show that our regularized RBMs\ncan achieve better event embedding and the selected features are highly\ninterpretable from human understanding.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 19:11:35 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 20:18:38 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:24:27 GMT"}, {"version": "v4", "created": "Sun, 4 Nov 2018 04:40:53 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhu", "Shixiang", ""], ["Xie", "Yao", ""]]}, {"id": "1806.06100", "submitter": "Jonathan Ullman", "authors": "Kobbi Nissim and Adam Smith and Thomas Steinke and Uri Stemmer and\n  Jonathan Ullman", "title": "The Limits of Post-Selection Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistics and machine learning offers numerous methods for ensuring\ngeneralization, these methods often fail in the presence of adaptivity---the\ncommon practice in which the choice of analysis depends on previous\ninteractions with the same dataset. A recent line of work has introduced\npowerful, general purpose algorithms that ensure post hoc generalization (also\ncalled robust or post-selection generalization), which says that, given the\noutput of the algorithm, it is hard to find any statistic for which the data\ndiffers significantly from the population it came from.\n  In this work we show several limitations on the power of algorithms\nsatisfying post hoc generalization. First, we show a tight lower bound on the\nerror of any algorithm that satisfies post hoc generalization and answers\nadaptively chosen statistical queries, showing a strong barrier to progress in\npost selection data analysis. Second, we show that post hoc generalization is\nnot closed under composition, despite many examples of such algorithms\nexhibiting strong composition properties.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 19:36:35 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Nissim", "Kobbi", ""], ["Smith", "Adam", ""], ["Steinke", "Thomas", ""], ["Stemmer", "Uri", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1806.06108", "submitter": "Edward Raff", "authors": "William Fleshman, Edward Raff, Jared Sylvester, Steven Forsyth, Mark\n  McLean", "title": "Non-Negative Networks Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": "AICS/2019/08", "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks against neural networks are a problem of considerable\nimportance, for which effective defenses are not yet readily available. We make\nprogress toward this problem by showing that non-negative weight constraints\ncan be used to improve resistance in specific scenarios. In particular, we show\nthat they can provide an effective defense for binary classification problems\nwith asymmetric cost, such as malware or spam detection. We also show the\npotential for non-negativity to be helpful to non-binary problems by applying\nit to image classification.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:02:57 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:23:59 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Fleshman", "William", ""], ["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Forsyth", "Steven", ""], ["McLean", "Mark", ""]]}, {"id": "1806.06116", "submitter": "Guokun Lai", "authors": "Guokun Lai, Bohan Li, Guoqing Zheng, Yiming Yang", "title": "Stochastic WaveNet: A Generative Latent Variable Model for Sequential\n  Data", "comments": "ICML 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model distribution of sequential data, including but not limited to\nspeech and human motions, is an important ongoing research problem. It has been\ndemonstrated that model capacity can be significantly enhanced by introducing\nstochastic latent variables in the hidden states of recurrent neural networks.\nSimultaneously, WaveNet, equipped with dilated convolutions, achieves\nastonishing empirical performance in natural speech generation task. In this\npaper, we combine the ideas from both stochastic latent variables and dilated\nconvolutions, and propose a new architecture to model sequential data, termed\nas Stochastic WaveNet, where stochastic latent variables are injected into the\nWaveNet structure. We argue that Stochastic WaveNet enjoys powerful\ndistribution modeling capacity and the advantage of parallel training from\ndilated convolutions. In order to efficiently infer the posterior distribution\nof the latent variables, a novel inference network structure is designed based\non the characteristics of WaveNet architecture. State-of-the-art performances\non benchmark datasets are obtained by Stochastic WaveNet on natural speech\nmodeling and high quality human handwriting samples can be generated as well.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:23:44 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lai", "Guokun", ""], ["Li", "Bohan", ""], ["Zheng", "Guoqing", ""], ["Yang", "Yiming", ""]]}, {"id": "1806.06121", "submitter": "Go\\\"ery Genty", "authors": "Mikko N\\\"arhi, Lauri Salmela, Juha Toivonen, Cyril Billet, John M.\n  Dudley, and Go\\\"ery Genty", "title": "Machine learning for prediction of extreme statistics in modulation\n  instability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central area of research in nonlinear science is the study of instabilities\nthat drive the emergence of extreme events. Unfortunately, experimental\ntechniques for measuring such phenomena often provide only partial\ncharacterization. For example, real-time studies of instabilities in nonlinear\nfibre optics frequently use only spectral data, precluding detailed predictions\nabout the associated temporal properties. Here, we show how Machine Learning\ncan overcome this limitation by predicting statistics for the maximum intensity\nof temporal peaks in modulation instability based only on spectral\nmeasurements. Specifically, we train a neural network based Machine Learning\nmodel to correlate spectral and temporal properties of optical fibre modulation\ninstability using data from numerical simulations, and we then use this model\nto predict the temporal probability distribution based on high-dynamic range\nspectral data from experiments. These results open novel perspectives in all\nsystems exhibiting chaos and instability where direct time-domain observations\nare difficult.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:36:33 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["N\u00e4rhi", "Mikko", ""], ["Salmela", "Lauri", ""], ["Toivonen", "Juha", ""], ["Billet", "Cyril", ""], ["Dudley", "John M.", ""], ["Genty", "Go\u00ebry", ""]]}, {"id": "1806.06122", "submitter": "Christina Ilvento", "authors": "Cynthia Dwork, Christina Ilvento", "title": "Fairness Under Composition", "comments": "Fixed two word omissions", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2019.33", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic fairness, and in particular the fairness of scoring and\nclassification algorithms, has become a topic of increasing social concern and\nhas recently witnessed an explosion of research in theoretical computer\nscience, machine learning, statistics, the social sciences, and law. Much of\nthe literature considers the case of a single classifier (or scoring function)\nused once, in isolation. In this work, we initiate the study of the fairness\nproperties of systems composed of algorithms that are fair in isolation; that\nis, we study fairness under composition. We identify pitfalls of naive\ncomposition and give general constructions for fair composition, demonstrating\nboth that classifiers that are fair in isolation do not necessarily compose\ninto fair systems and also that seemingly unfair components may be carefully\ncombined to construct fair systems. We focus primarily on the individual\nfairness setting proposed in [Dwork, Hardt, Pitassi, Reingold, Zemel, 2011],\nbut also extend our results to a large class of group fairness definitions\npopular in the recent literature, exhibiting several cases in which group\nfairness definitions give misleading signals under composition.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:47:33 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 00:48:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Dwork", "Cynthia", ""], ["Ilvento", "Christina", ""]]}, {"id": "1806.06123", "submitter": "Stephen Mussmann", "authors": "Stephen Mussmann, Percy Liang", "title": "On the Relationship between Data Efficiency and Error for Uncertainty\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While active learning offers potential cost savings, the actual data\nefficiency---the reduction in amount of labeled data needed to obtain the same\nerror rate---observed in practice is mixed. This paper poses a basic question:\nwhen is active learning actually helpful? We provide an answer for logistic\nregression with the popular active learning algorithm, uncertainty sampling.\nEmpirically, on 21 datasets from OpenML, we find a strong inverse correlation\nbetween data efficiency and the error rate of the final classifier.\nTheoretically, we show that for a variant of uncertainty sampling, the\nasymptotic data efficiency is within a constant factor of the inverse error\nrate of the limiting classifier.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:47:50 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mussmann", "Stephen", ""], ["Liang", "Percy", ""]]}, {"id": "1806.06124", "submitter": "Pooya Ashtari", "authors": "Pooya Ashtari, Fateme Nateghi Haredasht, Hamid Beigy", "title": "Supervised Fuzzy Partitioning", "comments": "30 pages, 6 figures, 6 tables, 1 algorithm", "journal-ref": null, "doi": "10.1016/j.patcog.2019.107013", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centroid-based methods including k-means and fuzzy c-means are known as\neffective and easy-to-implement approaches to clustering purposes in many\napplications. However, these algorithms cannot be directly applied to\nsupervised tasks. This paper thus presents a generative model extending the\ncentroid-based clustering approach to be applicable to classification and\nregression tasks. Given an arbitrary loss function, the proposed approach,\ntermed Supervised Fuzzy Partitioning (SFP), incorporates labels information\ninto its objective function through a surrogate term penalizing the empirical\nrisk. Entropy-based regularization is also employed to fuzzify the partition\nand to weight features, enabling the method to capture more complex patterns,\nidentify significant features, and yield better performance facing\nhigh-dimensional data. An iterative algorithm based on block coordinate descent\nscheme is formulated to efficiently find a local optimum. Extensive\nclassification experiments on synthetic, real-world, and high-dimensional\ndatasets demonstrate that the predictive performance of SFP is competitive with\nstate-of-the-art algorithms such as SVM and random forest. SFP has a major\nadvantage over such methods, in that it not only leads to a flexible, nonlinear\nmodel but also can exploit any convex loss function in the training phase\nwithout compromising computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:49:08 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 09:50:39 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 11:37:30 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 12:18:35 GMT"}, {"version": "v5", "created": "Mon, 19 Aug 2019 11:52:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ashtari", "Pooya", ""], ["Haredasht", "Fateme Nateghi", ""], ["Beigy", "Hamid", ""]]}, {"id": "1806.06142", "submitter": "Ruben J Sanchez-Garcia", "authors": "Fabio Strazzeri, Rub\\'en J. S\\'anchez-Garc\\'ia", "title": "Possibility results for graph clustering: A novel consistency axiom", "comments": "Minor changes, 'Related work' and 'Overview of results' sections\n  added to the introduction, small rewrite of the conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kleinberg introduced three natural clustering properties, or axioms, and\nshowed they cannot be simultaneously satisfied by any clustering algorithm. We\npresent a new clustering property, Monotonic Consistency, which avoids the\nwell-known problematic behaviour of Kleinberg's Consistency axiom, and the\nimpossibility result. Namely, we describe a clustering algorithm, Morse\nClustering, inspired by Morse Theory in Differential Topology, which satisfies\nKleinberg's original axioms with Consistency replaced by Monotonic Consistency.\nMorse clustering uncovers the underlying flow structure on a set or graph and\nreturns a partition into trees representing basins of attraction of critical\nvertices. We also generalise Kleinberg's axiomatic approach to sparse graphs,\nshowing an impossibility result for Consistency, and a possibility result for\nMonotonic Consistency and Morse clustering.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 22:07:10 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 15:34:19 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 12:47:10 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 09:09:09 GMT"}, {"version": "v5", "created": "Fri, 11 Jun 2021 10:27:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Strazzeri", "Fabio", ""], ["S\u00e1nchez-Garc\u00eda", "Rub\u00e9n J.", ""]]}, {"id": "1806.06144", "submitter": "Siyuan Ma", "authors": "Siyuan Ma, Mikhail Belkin", "title": "Kernel machines that adapt to GPUs for effective large batch training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning models are typically trained using Stochastic\nGradient Descent (SGD) on massively parallel computing resources such as GPUs.\nIncreasing mini-batch size is a simple and direct way to utilize the parallel\ncomputing capacity. For small batch an increase in batch size results in the\nproportional reduction in the training time, a phenomenon known as linear\nscaling. However, increasing batch size beyond a certain value leads to no\nfurther improvement in training time. In this paper we develop the first\nanalytical framework that extends linear scaling to match the parallel\ncomputing capacity of a resource. The framework is designed for a class of\nclassical kernel machines. It automatically modifies a standard kernel machine\nto output a mathematically equivalent prediction function, yet allowing for\nextended linear scaling, i.e., higher effective parallelization and faster\ntraining time on given hardware.\n  The resulting algorithms are accurate, principled and very fast. For example,\nusing a single Titan Xp GPU, training on ImageNet with $1.3\\times 10^6$ data\npoints and $1000$ labels takes under an hour, while smaller datasets, such as\nMNIST, take seconds. As the parameters are chosen analytically, based on the\ntheoretical bounds, little tuning beyond selecting the kernel and the kernel\nparameter is needed, further facilitating the practical use of these methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 22:12:44 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 19:50:14 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 16:48:09 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Ma", "Siyuan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1806.06173", "submitter": "Georgina Hall", "authors": "Amir Ali Ahmadi, Georgina Hall", "title": "On the Complexity of Detecting Convexity over a Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 03:22:50 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 11:45:06 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Hall", "Georgina", ""]]}, {"id": "1806.06176", "submitter": "Paul Pu Liang", "authors": "Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and\n  Louis-Philippe Morency and Ruslan Salakhutdinov", "title": "Learning Factorized Multimodal Representations", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 03:48:50 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:17:03 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 14:16:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1806.06207", "submitter": "Wlodzislaw Duch", "authors": "W{\\l}odzis{\\l}aw Duch and Karol Grudzi\\'nsk", "title": "Meta-learning: searching in the model space", "comments": "6 pages. To our best knowledge this is the first paper on\n  meta-learning as search in the model spaces; for later developments search\n  \"Duch meta-learning\"", "journal-ref": "Proceedings of the International Conference on Neural Information\n  Processing, Shanghai, 2001, Vol. I, pp. 235-240", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no free lunch, no single learning algorithm that will outperform\nother algorithms on all data. In practice different approaches are tried and\nthe best algorithm selected. An alternative solution is to build new algorithms\non demand by creating a framework that accommodates many algorithms. The best\ncombination of parameters and procedures is searched here in the space of all\npossible models belonging to the framework of Similarity-Based Methods (SBMs).\nSuch meta-learning approach gives a chance to find the best method in all\ncases. Issues related to the meta-learning and first tests of this approach are\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:24:35 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Duch", "W\u0142odzis\u0142aw", ""], ["Grudzi\u0144sk", "Karol", ""]]}, {"id": "1806.06209", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi, Ali Shojaie", "title": "The Reduced PC-Algorithm: Improved Causal Structure Learning in Large\n  Random Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research, 20(164), 1-31 (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating a high-dimensional directed acyclic graph,\ngiven observations from a linear structural equation model with arbitrary noise\ndistribution. By exploiting properties of common random graphs, we develop a\nnew algorithm that requires conditioning only on small sets of variables. The\nproposed algorithm, which is essentially a modified version of the\nPC-Algorithm, offers significant gains in both computational complexity and\nestimation accuracy. In particular, it results in more efficient and accurate\nestimation in large networks containing hub nodes, which are common in\nbiological systems. We prove the consistency of the proposed algorithm, and\nshow that it also requires a less stringent faithfulness assumption than the\nPC-Algorithm. Simulations in low and high-dimensional settings are used to\nillustrate these findings. An application to gene expression data suggests that\nthe proposed algorithm can identify a greater number of clinically relevant\ngenes than current methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:40:14 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 21:00:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sondhi", "Arjun", ""], ["Shojaie", "Ali", ""]]}, {"id": "1806.06232", "submitter": "Alexandre Quemy", "authors": "Alexandre Quemy", "title": "Binary Classification in Unstructured Space With Hypergraph Case-Based\n  Reasoning", "comments": "Accepted for publication by Information Systems. Arxiv version\n  contains the additional material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binary classification is one of the most common problem in machine learning.\nIt consists in predicting whether a given element belongs to a particular\nclass. In this paper, a new algorithm for binary classification is proposed\nusing a hypergraph representation. The method is agnostic to data\nrepresentation, can work with multiple data sources or in non-metric spaces,\nand accommodates with missing values. As a result, it drastically reduces the\nneed for data preprocessing or feature engineering. Each element to be\nclassified is partitioned according to its interactions with the training set.\nFor each class, a seminorm over the training set partition is learnt to\nrepresent the distribution of evidence supporting this class.\n  Empirical validation demonstrates its high potential on a wide range of\nwell-known datasets and the results are compared to the state-of-the-art. The\ntime complexity is given and empirically validated. Its robustness with regard\nto hyperparameter sensitivity is studied and compared to standard\nclassification methods. Finally, the limitation of the model space is\ndiscussed, and some potential solutions proposed.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:27:14 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:00:54 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:32:09 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Quemy", "Alexandre", ""]]}, {"id": "1806.06237", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah and Aarti Singh", "title": "PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automated assignment of papers to reviewers in\nconference peer review, with a focus on fairness and statistical accuracy. Our\nfairness objective is to maximize the review quality of the most disadvantaged\npaper, in contrast to the commonly used objective of maximizing the total\nquality over all papers. We design an assignment algorithm based on an\nincremental max-flow procedure that we prove is near-optimally fair. Our\nstatistical accuracy objective is to ensure correct recovery of the papers that\nshould be accepted. We provide a sharp minimax analysis of the accuracy of the\npeer-review process for a popular objective-score model as well as for a novel\nsubjective-score model that we propose in the paper. Our analysis proves that\nour proposed assignment algorithm also leads to a near-optimal statistical\naccuracy. Finally, we design a novel experiment that allows for an objective\ncomparison of various assignment algorithms, and overcomes the inherent\ndifficulty posed by the absence of a ground truth in experiments on\npeer-review. The results of this experiment as well as of other experiments on\nsynthetic and real data corroborate the theoretical guarantees of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:42:04 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 02:15:04 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""]]}, {"id": "1806.06253", "submitter": "Jung Lee", "authors": "Jung H. Lee", "title": "DynMat, a network that can learn after learning", "comments": "40 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To survive in the dynamically-evolving world, we accumulate knowledge and\nimprove our skills based on experience. In the process, gaining new knowledge\ndoes not disrupt our vigilance to external stimuli. In other words, our\nlearning process is 'accumulative' and 'online' without interruption. However,\ndespite the recent success, artificial neural networks (ANNs) must be trained\noffline, and they suffer catastrophic interference between old and new\nlearning, indicating that ANNs' conventional learning algorithms may not be\nsuitable for building intelligent agents comparable to our brain. In this\nstudy, we propose a novel neural network architecture (DynMat) consisting of\ndual learning systems, inspired by the complementary learning system (CLS)\ntheory suggesting that the brain relies on short- and long-term learning\nsystems to learn continuously. Our experiments show that 1) DynMat can learn a\nnew class without catastrophic interference and 2) it does not strictly require\noffline training.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 15:32:32 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 19:57:26 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Lee", "Jung H.", ""]]}, {"id": "1806.06266", "submitter": "Han Zhao", "authors": "Yichong Xu, Han Zhao, Xiaofei Shi, Jeremy Zhang, and Nihar B. Shah", "title": "On Strategyproof Conference Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider peer review in a conference setting where there is typically an\noverlap between the set of reviewers and the set of authors. This overlap can\nincentivize strategic reviews to influence the final ranking of one's own\npapers. In this work, we address this problem through the lens of social\nchoice, and present a theoretical framework for strategyproof and efficient\npeer review. We first present and analyze an algorithm for reviewer-assignment\nand aggregation that guarantees strategyproofness and a natural efficiency\nproperty called unanimity, when the authorship graph satisfies a simple\nproperty. Our algorithm is based on the so-called partitioning method, and can\nbe thought as a generalization of this method to conference peer review\nsettings. We then empirically show that the requisite property on the\nauthorship graph is indeed satisfied in the submission data from the ICLR\nconference, and further demonstrate a simple trick to make the partitioning\nmethod more practically appealing for conference peer review. Finally, we\ncomplement our positive results with negative theoretical results where we\nprove that under various ways of strengthening the requirements, it is\nimpossible for any algorithm to be strategyproof and efficient.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 16:40:39 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:52:02 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 03:01:04 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Xu", "Yichong", ""], ["Zhao", "Han", ""], ["Shi", "Xiaofei", ""], ["Zhang", "Jeremy", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1806.06270", "submitter": "Kun Kuang", "authors": "Kun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, Bo Li", "title": "Stable Prediction across Unknown Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many important machine learning applications, the training distribution\nused to learn a probabilistic classifier differs from the testing distribution\non which the classifier will be used to make predictions. Traditional methods\ncorrect the distribution shift by reweighting the training data with the ratio\nof the density between test and training data. In many applications training\ntakes place without prior knowledge of the testing distribution on which the\nalgorithm will be applied in the future. Recently, methods have been proposed\nto address the shift by learning causal structure, but those methods rely on\nthe diversity of multiple training data to a good performance, and have\ncomplexity limitations in high dimensions. In this paper, we propose a novel\nDeep Global Balancing Regression (DGBR) algorithm to jointly optimize a deep\nauto-encoder model for feature selection and a global balancing model for\nstable prediction across unknown environments. The global balancing model\nconstructs balancing weights that facilitate estimating of partial effects of\nfeatures (holding fixed all other features), a problem that is challenging in\nhigh dimensions, and thus helps to identify stable, causal relationships\nbetween features and outcomes. The deep auto-encoder model is designed to\nreduce the dimensionality of the feature space, thus making global balancing\neasier. We show, both theoretically and with empirical experiments, that our\nalgorithm can make stable predictions across unknown environments. Our\nexperiments on both synthetic and real world datasets demonstrate that our DGBR\nalgorithm outperforms the state-of-the-art methods for stable prediction across\nunknown environments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 17:08:15 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 20:54:07 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kuang", "Kun", ""], ["Xiong", "Ruoxuan", ""], ["Cui", "Peng", ""], ["Athey", "Susan", ""], ["Li", "Bo", ""]]}, {"id": "1806.06296", "submitter": "Thomas Lansdall-Welfare", "authors": "Sen Jia, Thomas Lansdall-Welfare, Nello Cristianini", "title": "Right for the Right Reason: Training Agnostic Networks", "comments": "Author's original version", "journal-ref": null, "doi": "10.1007/978-3-030-01768-2_14", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of a neural network being requested to classify\nimages (or other inputs) without making implicit use of a \"protected concept\",\nthat is a concept that should not play any role in the decision of the network.\nTypically these concepts include information such as gender or race, or other\ncontextual information such as image backgrounds that might be implicitly\nreflected in unknown correlations with other variables, making it insufficient\nto simply remove them from the input features. In other words, making accurate\npredictions is not good enough if those predictions rely on information that\nshould not be used: predictive performance is not the only important metric for\nlearning systems. We apply a method developed in the context of domain\nadaptation to address this problem of \"being right for the right reason\", where\nwe request a classifier to make a decision in a way that is entirely 'agnostic'\nto a given protected concept (e.g. gender, race, background etc.), even if this\ncould be implicitly reflected in other attributes via unknown correlations.\nAfter defining the concept of an 'agnostic model', we demonstrate how the\nDomain-Adversarial Neural Network can remove unwanted information from a model\nusing a gradient reversal layer.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:09:40 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jia", "Sen", ""], ["Lansdall-Welfare", "Thomas", ""], ["Cristianini", "Nello", ""]]}, {"id": "1806.06298", "submitter": "Xianglei Xing", "authors": "Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, Ying Nian Wu", "title": "Deformable Generator Network: Unsupervised Disentanglement of Appearance\n  and Geometry", "comments": "version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deformable generator model to disentangle the appearance and\ngeometric information for both image and video data in a purely unsupervised\nmanner. The appearance generator network models the information related to\nappearance, including color, illumination, identity or category, while the\ngeometric generator performs geometric warping, such as rotation and\nstretching, through generating deformation field which is used to warp the\ngenerated appearance to obtain the final image or video sequences. Two\ngenerators take independent latent vectors as input to disentangle the\nappearance and geometric information from image or video sequences. For video\ndata, a nonlinear transition model is introduced to both the appearance and\ngeometric generators to capture the dynamics over time. The proposed scheme is\ngeneral and can be easily integrated into different generative models. An\nextensive set of qualitative and quantitative experiments shows that the\nappearance and geometric information can be well disentangled, and the learned\ngeometric generator can be conveniently transferred to other image datasets to\nfacilitate knowledge transfer tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:17:02 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 05:20:31 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 01:26:23 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Xing", "Xianglei", ""], ["Gao", "Ruiqi", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1806.06301", "submitter": "Thomas Lansdall-Welfare", "authors": "Adam Sutton, Thomas Lansdall-Welfare, Nello Cristianini", "title": "Biased Embeddings from Wild Data: Measuring, Understanding and Removing", "comments": "Author's original version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern Artificial Intelligence (AI) systems make use of data embeddings,\nparticularly in the domain of Natural Language Processing (NLP). These\nembeddings are learnt from data that has been gathered \"from the wild\" and have\nbeen found to contain unwanted biases. In this paper we make three\ncontributions towards measuring, understanding and removing this problem. We\npresent a rigorous way to measure some of these biases, based on the use of\nword lists created for social psychology applications; we observe how gender\nbias in occupations reflects actual gender bias in the same occupations in the\nreal world; and finally we demonstrate how a simple projection can\nsignificantly reduce the effects of embedding bias. All this is part of an\nongoing effort to understand how trust can be built into AI systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:46:59 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sutton", "Adam", ""], ["Lansdall-Welfare", "Thomas", ""], ["Cristianini", "Nello", ""]]}, {"id": "1806.06317", "submitter": "Bao Wang", "authors": "Stanley Osher, Bao Wang, Penghang Yin, Xiyang Luo, Farzin Barekat,\n  Minh Pham, Alex Lin", "title": "Laplacian Smoothing Gradient Descent", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of very simple modifications of gradient descent and\nstochastic gradient descent. We show that when applied to a large variety of\nmachine learning problems, ranging from logistic regression to deep neural\nnets, the proposed surrogates can dramatically reduce the variance, allow to\ntake a larger step size, and improve the generalization accuracy. The methods\nonly involve multiplying the usual (stochastic) gradient by the inverse of a\npositive definitive matrix (which can be computed efficiently by FFT) with a\nlow condition number coming from a one-dimensional discrete Laplacian or its\nhigh order generalizations. It also preserves the mean and increases the\nsmallest component and decreases the largest component. The theory of\nHamilton-Jacobi partial differential equations demonstrates that the implicit\nversion of the new algorithm is almost the same as doing gradient descent on a\nnew function which (i) has the same global minima as the original function and\n(ii) is ``more convex\". Moreover, we show that optimization algorithms with\nthese surrogates converge uniformly in the discrete Sobolev $H_\\sigma^p$ sense\nand reduce the optimality gap for convex optimization problems. The code is\navailable at:\n\\url{https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent}\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 00:42:09 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 18:21:15 GMT"}, {"version": "v3", "created": "Sun, 23 Sep 2018 03:41:23 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2018 23:15:29 GMT"}, {"version": "v5", "created": "Sat, 27 Apr 2019 23:05:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Osher", "Stanley", ""], ["Wang", "Bao", ""], ["Yin", "Penghang", ""], ["Luo", "Xiyang", ""], ["Barekat", "Farzin", ""], ["Pham", "Minh", ""], ["Lin", "Alex", ""]]}, {"id": "1806.06362", "submitter": "Rebekka Burkholz", "authors": "Rebekka Burkholz, Alina Dubatovka", "title": "Initialization of ReLUs for Dynamical Isometry", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning relies on good initialization schemes and hyperparameter\nchoices prior to training a neural network. Random weight initializations\ninduce random network ensembles, which give rise to the trainability, training\nspeed, and sometimes also generalization ability of an instance. In addition,\nsuch ensembles provide theoretical insights into the space of candidate models\nof which one is selected during training. The results obtained so far rely on\nmean field approximations that assume infinite layer width and that study\naverage squared signals. We derive the joint signal output distribution\nexactly, without mean field assumptions, for fully-connected networks with\nGaussian weights and biases, and analyze deviations from the mean field\nresults. For rectified linear units, we further discuss limitations of the\nstandard initialization scheme, such as its lack of dynamical isometry, and\npropose a simple alternative that overcomes these by initial parameter sharing.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 10:56:45 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 15:10:01 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 17:55:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Burkholz", "Rebekka", ""], ["Dubatovka", "Alina", ""]]}, {"id": "1806.06365", "submitter": "Thiago Serra", "authors": "Thiago Serra and Christian Tjandraatmadja and Srikumar Ramalingam", "title": "How Could Polyhedral Theory Harness Deep Learning?", "comments": null, "journal-ref": "Scientific Machine Learning Workshop, U.S. Department of Energy\n  Office of Advanced Scientific Computing Research, January 30 -- February 1,\n  2018", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The holy grail of deep learning is to come up with an automatic method to\ndesign optimal architectures for different applications. In other words, how\ncan we effectively dimension and organize neurons along the network layers\nbased on the computational resources, input size, and amount of training data?\nWe outline promising research directions based on polyhedral theory and\nmixed-integer representability that may offer an analytical approach to this\nquestion, in contrast to the empirical techniques often employed.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 11:18:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Serra", "Thiago", ""], ["Tjandraatmadja", "Christian", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1806.06384", "submitter": "Tian Guo", "authors": "Tian Guo, Tao Lin", "title": "Multi-variable LSTM neural network for autoregressive exogenous model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 13:42:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Guo", "Tian", ""], ["Lin", "Tao", ""]]}, {"id": "1806.06392", "submitter": "Junchi Liang", "authors": "Junchi Liang, Abdeslam Boularias", "title": "Task-Relevant Object Discovery and Categorization for Playing\n  First-person Shooter Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to play first-person shooter (FPS) video\ngames using raw screen images as observations and keyboard inputs as actions.\nThe high-dimensionality of the observations in this type of applications leads\nto prohibitive needs of training data for model-free methods, such as the deep\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\nlearning low-dimensional representations that may reduce the need for data.\nThis paper presents a new and efficient method for learning such\nrepresentations. Salient segments of consecutive frames are detected from their\noptical flow, and clustered based on their feature descriptors. The clusters\ntypically correspond to different discovered categories of objects. Segments\ndetected in new frames are then classified based on their nearest clusters.\nBecause only a few categories are relevant to a given task, the importance of a\ncategory is defined as the correlation between its occurrence and the agent's\nperformance. The result is encoded as a vector indicating objects that are in\nthe frame and their locations, and used as a side input to DRQN. Experiments on\nthe game Doom provide a good evidence for the benefit of this approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 15:02:08 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Liang", "Junchi", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1806.06408", "submitter": "Lisa Lee", "authors": "Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, Ruslan\n  Salakhutdinov", "title": "Gated Path Planning Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value Iteration Networks (VINs) are effective differentiable path planning\nmodules that can be used by agents to perform navigation while still\nmaintaining end-to-end differentiability of the entire architecture. Despite\ntheir effectiveness, they suffer from several disadvantages including training\ninstability, random seed sensitivity, and other optimization problems. In this\nwork, we reframe VINs as recurrent-convolutional networks which demonstrates\nthat VINs couple recurrent convolutions with an unconventional max-pooling\nactivation. From this perspective, we argue that standard gated recurrent\nupdate equations could potentially alleviate the optimization issues plaguing\nVIN. The resulting architecture, which we call the Gated Path Planning Network,\nis shown to empirically outperform VIN on a variety of metrics such as learning\nspeed, hyperparameter sensitivity, iteration count, and even generalization.\nFurthermore, we show that this performance gap is consistent across different\nmaze transition types, maze sizes and even show success on a challenging 3D\nenvironment, where the planner is only provided with first-person RGB images.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:32:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lee", "Lisa", ""], ["Parisotto", "Emilio", ""], ["Chaplot", "Devendra Singh", ""], ["Xing", "Eric", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1806.06415", "submitter": "Shan Shi", "authors": "Shan Shi, Farouk Nathoo", "title": "Feature Learning and Classification in Neuroimaging: Predicting\n  Cognitive Impairment from Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid innovation of technology and the desire to find and employ\nbiomarkers for neurodegenerative disease, high-dimensional data classification\nproblems are routinely encountered in neuroimaging studies. To avoid\nover-fitting and to explore relationships between disease and potential\nbiomarkers, feature learning and selection plays an important role in\nclassifier construction and is an important area in machine learning. In this\narticle, we review several important feature learning and selection techniques\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\nauto-encoders. We compare these approaches using a numerical study involving\nthe prediction of Alzheimer's disease from Magnetic Resonance Imaging.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 17:14:17 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Shi", "Shan", ""], ["Nathoo", "Farouk", ""]]}, {"id": "1806.06438", "submitter": "Dave Van Veen", "authors": "Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, Eric Price, Sriram\n  Vishwanath, Alexandros G. Dimakis", "title": "Compressed Sensing with Deep Image Prior and Learned Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel method for compressed sensing recovery using untrained\ndeep generative models. Our method is based on the recently proposed Deep Image\nPrior (DIP), wherein the convolutional weights of the network are optimized to\nmatch the observed measurements. We show that this approach can be applied to\nsolve any differentiable linear inverse problem, outperforming previous\nunlearned methods. Unlike various learned approaches based on generative\nmodels, our method does not require pre-training over large datasets. We\nfurther introduce a novel learned regularization technique, which incorporates\nprior information on the network weights. This reduces reconstruction error,\nespecially for noisy measurements. Finally, we prove that, using the DIP\noptimization approach, moderately overparameterized single-layer networks can\nperfectly fit any signal despite the non-convex nature of the fitting problem.\nThis theoretical result provides justification for early stopping.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 20:11:03 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 23:45:32 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 21:48:04 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 19:55:19 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Van Veen", "Dave", ""], ["Jalal", "Ajil", ""], ["Soltanolkotabi", "Mahdi", ""], ["Price", "Eric", ""], ["Vishwanath", "Sriram", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1806.06439", "submitter": "James Robinson", "authors": "Mark Herbster and James Robinson", "title": "Online Prediction of Switching Graph Labelings with Cluster Specialists", "comments": "20 pages (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting the labeling of a graph in an online\nsetting when the labeling is changing over time. We present an algorithm based\non a specialist approach; we develop the machinery of cluster specialists which\nprobabilistically exploits the cluster structure in the graph. Our algorithm\nhas two variants, one of which surprisingly only requires $\\mathcal{O}(\\log n)$\ntime on any trial $t$ on an $n$-vertex graph, an exponential speed up over\nexisting methods. We prove switching mistake-bound guarantees for both variants\nof our algorithm. Furthermore these mistake bounds smoothly vary with the\nmagnitude of the change between successive labelings. We perform experiments on\nChicago Divvy Bicycle Sharing data and show that our algorithms significantly\noutperform an existing algorithm (a kernelized Perceptron) as well as several\nnatural benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 20:17:33 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 18:21:05 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 14:34:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Herbster", "Mark", ""], ["Robinson", "James", ""]]}, {"id": "1806.06457", "submitter": "Alireza Aghasi", "authors": "Alireza Aghasi, Afshin Abdi, Justin Romberg", "title": "Fast Convex Pruning of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fast, tractable technique called Net-Trim for simplifying a\ntrained neural network. The method is a convex post-processing module, which\nprunes (sparsifies) a trained network layer by layer, while preserving the\ninternal responses. We present a comprehensive analysis of Net-Trim from both\nthe algorithmic and sample complexity standpoints, centered on a fast, scalable\nconvex optimization program. Our analysis includes consistency results between\nthe initial and retrained models before and after Net-Trim application and\nguarantees on the number of training samples needed to discover a network that\ncan be expressed using a certain number of nonzero terms. Specifically, if\nthere is a set of weights that uses at most $s$ terms that can re-create the\nlayer outputs from the layer inputs, we can find these weights from\n$\\mathcal{O}(s\\log N/s)$ samples, where $N$ is the input size. These\ntheoretical results are similar to those for sparse regression using the Lasso,\nand our analysis uses some of the same recently-developed tools (namely recent\nresults on the concentration of measure and convex analysis). Finally, we\npropose an algorithmic framework based on the alternating direction method of\nmultipliers (ADMM), which allows a fast and simple implementation of Net-Trim\nfor network pruning and compression.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 22:16:18 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 01:20:40 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Aghasi", "Alireza", ""], ["Abdi", "Afshin", ""], ["Romberg", "Justin", ""]]}, {"id": "1806.06464", "submitter": "Aditya Grover", "authors": "Aditya Grover, Maruan Al-Shedivat, Jayesh K. Gupta, Yura Burda,\n  Harrison Edwards", "title": "Learning Policy Representations in Multiagent Systems", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling agent behavior is central to understanding the emergence of complex\nphenomena in multiagent systems. Prior work in agent modeling has largely been\ntask-specific and driven by hand-engineering domain-specific prior knowledge.\nWe propose a general learning framework for modeling agent behavior in any\nmultiagent system using only a handful of interaction data. Our framework casts\nagent modeling as a representation learning problem. Consequently, we construct\na novel objective inspired by imitation learning and agent identification and\ndesign an algorithm for unsupervised learning of representations of agent\npolicies. We demonstrate empirically the utility of the proposed framework in\n(i) a challenging high-dimensional competitive environment for continuous\ncontrol and (ii) a cooperative environment for communication, on supervised\npredictive tasks, unsupervised clustering, and policy optimization using deep\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 23:29:19 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 21:36:26 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Grover", "Aditya", ""], ["Al-Shedivat", "Maruan", ""], ["Gupta", "Jayesh K.", ""], ["Burda", "Yura", ""], ["Edwards", "Harrison", ""]]}, {"id": "1806.06513", "submitter": "Chao Zhang", "authors": "Chao Zhang, Philip Woodland", "title": "Semi-tied Units for Efficient Gating in LSTM and Highway Networks", "comments": "To appear in Proc. INTERSPEECH 2018, September 2-6, 2018, Hyderabad,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gating is a key technique used for integrating information from multiple\nsources by long short-term memory (LSTM) models and has recently also been\napplied to other models such as the highway network. Although gating is\npowerful, it is rather expensive in terms of both computation and storage as\neach gating unit uses a separate full weight matrix. This issue can be severe\nsince several gates can be used together in e.g. an LSTM cell. This paper\nproposes a semi-tied unit (STU) approach to solve this efficiency issue, which\nuses one shared weight matrix to replace those in all the units in the same\nlayer. The approach is termed \"semi-tied\" since extra parameters are used to\nseparately scale each of the shared output values. These extra scaling factors\nare associated with the network activation functions and result in the use of\nparameterised sigmoid, hyperbolic tangent, and rectified linear unit functions.\nSpeech recognition experiments using British English multi-genre broadcast data\nshowed that using STUs can reduce the calculation and storage cost by a factor\nof three for highway networks and four for LSTMs, while giving similar word\nerror rates to the original models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 06:49:00 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhang", "Chao", ""], ["Woodland", "Philip", ""]]}, {"id": "1806.06514", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "The Information Autoencoding Family: A Lagrangian Perspective on Latent\n  Variable Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of objectives have been proposed to train latent variable\ngenerative models. We show that many of them are Lagrangian dual functions of\nthe same primal optimization problem. The primal problem optimizes the mutual\ninformation between latent and visible variables, subject to the constraints of\naccurately modeling the data distribution and performing correct amortized\ninference. By choosing to maximize or minimize mutual information, and choosing\ndifferent Lagrange multipliers, we obtain different objectives including\nInfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB,\nAS-VAE and InfoVAE. Based on this observation, we provide an exhaustive\ncharacterization of the statistical and computational trade-offs made by all\nthe training objectives in this class of Lagrangian duals. Next, we propose a\ndual optimization method where we optimize model parameters as well as the\nLagrange multipliers. This method achieves Pareto optimal solutions in terms of\noptimizing information and satisfying the constraints.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 06:51:28 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 00:21:02 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1806.06553", "submitter": "Chang Li Mr.", "authors": "Chang Li, Maarten de Rijke", "title": "Incremental Sparse Bayesian Ordinal Regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.07.015", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal Regression (OR) aims to model the ordering information between\ndifferent data categories, which is a crucial topic in multi-label learning. An\nimportant class of approaches to OR models the problem as a linear combination\nof basis functions that map features to a high dimensional non-linear space.\nHowever, most of the basis function-based algorithms are time consuming. We\npropose an incremental sparse Bayesian approach to OR tasks and introduce an\nalgorithm to sequentially learn the relevant basis functions in the ordinal\nscenario. Our method, called Incremental Sparse Bayesian Ordinal Regression\n(ISBOR), automatically optimizes the hyper-parameters via the type-II maximum\nlikelihood method. By exploiting fast marginal likelihood optimization, ISBOR\ncan avoid big matrix inverses, which is the main bottleneck in applying basis\nfunction-based algorithms to OR tasks on large-scale datasets. We show that\nISBOR can make accurate predictions with parsimonious basis functions while\noffering automatic estimates of the prediction uncertainty. Extensive\nexperiments on synthetic and real word datasets demonstrate the efficiency and\neffectiveness of ISBOR compared to other basis function-based OR approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:55:20 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Li", "Chang", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1806.06573", "submitter": "Sarit Khirirat", "authors": "Sarit Khirirat, Hamid Reza Feyzmahdavian and Mikael Johansson", "title": "Distributed learning with compressed gradients", "comments": "33 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous computation and gradient compression have emerged as two key\ntechniques for achieving scalability in distributed optimization for\nlarge-scale machine learning. This paper presents a unified analysis framework\nfor distributed gradient methods operating with staled and compressed\ngradients. Non-asymptotic bounds on convergence rates and information exchange\nare derived for several optimization algorithms. These bounds give explicit\nexpressions for step-sizes and characterize how the amount of asynchrony and\nthe compression accuracy affect iteration and communication complexity\nguarantees. Numerical results highlight convergence properties of different\ngradient compression algorithms and confirm that fast convergence under limited\ninformation exchange is indeed possible.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 09:37:39 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 14:57:17 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Khirirat", "Sarit", ""], ["Feyzmahdavian", "Hamid Reza", ""], ["Johansson", "Mikael", ""]]}, {"id": "1806.06610", "submitter": "Alejandro Cervantes", "authors": "Alejandro Cervantes, Christian Gagn\\'e, Pedro Isasi and Marc Parizeau", "title": "Evaluating and Characterizing Incremental Learning from Non-Stationary\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Incremental learning from non-stationary data poses special challenges to the\nfield of machine learning. Although new algorithms have been developed for\nthis, assessment of results and comparison of behaviors are still open\nproblems, mainly because evaluation metrics, adapted from more traditional\ntasks, can be ineffective in this context. Overall, there is a lack of common\ntesting practices. This paper thus presents a testbed for incremental\nnon-stationary learning algorithms, based on specially designed synthetic\ndatasets. Also, test results are reported for some well-known algorithms to\nshow that the proposed methodology is effective at characterizing their\nstrengths and weaknesses. It is expected that this methodology will provide a\ncommon basis for evaluating future contributions in the field.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:00:31 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Cervantes", "Alejandro", ""], ["Gagn\u00e9", "Christian", ""], ["Isasi", "Pedro", ""], ["Parizeau", "Marc", ""]]}, {"id": "1806.06616", "submitter": "Damien Garreau", "authors": "Siavash Haghiri, Damien Garreau, Ulrike von Luxburg", "title": "Comparison-Based Random Forests", "comments": "Accepted at ICML 2018, camera-ready version (32 pages, 14 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume we are given a set of items from a general metric space, but we\nneither have access to the representation of the data nor to the distances\nbetween data points. Instead, suppose that we can actively choose a triplet of\nitems (A,B,C) and ask an oracle whether item A is closer to item B or to item\nC. In this paper, we propose a novel random forest algorithm for regression and\nclassification that relies only on such triplet comparisons. In the theory part\nof this paper, we establish sufficient conditions for the consistency of such a\nforest. In a set of comprehensive experiments, we then demonstrate that the\nproposed random forest is efficient both for classification and regression. In\nparticular, it is even competitive with other methods that have direct access\nto the metric representation of the data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:06:33 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Haghiri", "Siavash", ""], ["Garreau", "Damien", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1806.06720", "submitter": "Ajin George Joseph", "authors": "Ajin George Joseph, Shalabh Bhatnagar", "title": "An Online Prediction Algorithm for Reinforcement Learning with Linear\n  Function Approximation using Cross Entropy Method", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.09449", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide two new stable online algorithms for the problem of\nprediction in reinforcement learning, \\emph{i.e.}, estimating the value\nfunction of a model-free Markov reward process using the linear function\napproximation architecture and with memory and computation costs scaling\nquadratically in the size of the feature set. The algorithms employ the\nmulti-timescale stochastic approximation variant of the very popular cross\nentropy (CE) optimization method which is a model based search method to find\nthe global optimum of a real-valued function. A proof of convergence of the\nalgorithms using the ODE method is provided. We supplement our theoretical\nresults with experimental comparisons. The algorithms achieve good performance\nfairly consistently on many RL benchmark problems with regards to computational\nefficiency, accuracy and stability.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:37:37 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Joseph", "Ajin George", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1806.06763", "submitter": "Quanquan Gu", "authors": "Jinghui Chen and Dongruo Zhou and Yiqi Tang and Ziyan Yang and Yuan\n  Cao and Quanquan Gu", "title": "Closing the Generalization Gap of Adaptive Gradient Methods in Training\n  Deep Neural Networks", "comments": "17 pages, 4 figures, 4 tables. In IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods, which adopt historical gradient information to\nautomatically adjust the learning rate, despite the nice property of fast\nconvergence, have been observed to generalize worse than stochastic gradient\ndescent (SGD) with momentum in training deep neural networks. This leaves how\nto close the generalization gap of adaptive gradient methods an open problem.\nIn this work, we show that adaptive gradient methods such as Adam, Amsgrad, are\nsometimes \"over adapted\". We design a new algorithm, called Partially adaptive\nmomentum estimation method, which unifies the Adam/Amsgrad with SGD by\nintroducing a partial adaptive parameter $p$, to achieve the best from both\nworlds. We also prove the convergence rate of our proposed algorithm to a\nstationary point in the stochastic nonconvex optimization setting. Experiments\non standard benchmarks show that our proposed algorithm can maintain a fast\nconvergence rate as Adam/Amsgrad while generalizing as well as SGD in training\ndeep neural networks. These results would suggest practitioners pick up\nadaptive gradient methods once again for faster training of deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:17:01 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 02:28:00 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 06:47:00 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chen", "Jinghui", ""], ["Zhou", "Dongruo", ""], ["Tang", "Yiqi", ""], ["Yang", "Ziyan", ""], ["Cao", "Yuan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1806.06765", "submitter": "Jason Jo", "authors": "Jason Jo, Vikas Verma, Yoshua Bengio", "title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks", "comments": "Modified abstract to fit arXiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:19:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jo", "Jason", ""], ["Verma", "Vikas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.06775", "submitter": "Armin Askari", "authors": "Armin Askari, Forest Yang, Laurent El Ghaoui", "title": "Kernel-based Outlier Detection using the Inverse Christoffel Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection methods have become increasingly relevant in recent years\ndue to increased security concerns and because of its vast application to\ndifferent fields. Recently, Pauwels and Lasserre (2016) noticed that the\nsublevel sets of the inverse Christoffel function accurately depict the shape\nof a cloud of data using a sum-of-squares polynomial and can be used to perform\noutlier detection. In this work, we propose a kernelized variant of the inverse\nChristoffel function that makes it computationally tractable for data sets with\na large number of features. We compare our approach to current methods on 15\ndifferent data sets and achieve the best average area under the precision\nrecall curve (AUPRC) score, the best average rank and the lowest root mean\nsquare deviation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:34:23 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Askari", "Armin", ""], ["Yang", "Forest", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "1806.06777", "submitter": "Li Ma", "authors": "Shai Gorsky and Li Ma", "title": "Multiscale Fisher's Independence Test for Multivariate Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying dependency in multivariate data is a common inference task that\narises in numerous applications. However, existing nonparametric independence\ntests typically require computation that scales at least quadratically with the\nsample size, making it difficult to apply them to massive data. Moreover,\nresampling is usually necessary to evaluate the statistical significance of the\nresulting test statistics at finite sample sizes, further worsening the\ncomputational burden. We introduce a scalable, resampling-free approach to\ntesting the independence between two random vectors by breaking down the task\ninto simple univariate tests of independence on a collection of 2x2 contingency\ntables constructed through sequential coarse-to-fine discretization of the\nsample space, transforming the inference task into a multiple testing problem\nthat can be completed with almost linear complexity with respect to the sample\nsize. To address increasing dimensionality, we introduce a coarse-to-fine\nsequential adaptive procedure that exploits the spatial features of dependency\nstructures to more effectively examine the sample space. We derive a\nfinite-sample theory that guarantees the inferential validity of our adaptive\nprocedure at any given sample size. In particular, we show that our approach\ncan achieve strong control of the family-wise error rate without resampling or\nlarge-sample approximation. We demonstrate the substantial computational\nadvantage of the procedure in comparison to existing approaches as well as its\ndecent statistical power under various dependency scenarios through an\nextensive simulation study, and illustrate how the divide-and-conquer nature of\nthe procedure can be exploited to not just test independence but to learn the\nnature of the underlying dependency. Finally, we demonstrate the use of our\nmethod through analyzing a large data set from a flow cytometry experiment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:38:54 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 19:02:19 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 13:48:05 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:13:48 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 17:21:40 GMT"}, {"version": "v6", "created": "Tue, 14 Jan 2020 19:09:36 GMT"}, {"version": "v7", "created": "Wed, 7 Jul 2021 11:37:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Gorsky", "Shai", ""], ["Ma", "Li", ""]]}, {"id": "1806.06784", "submitter": "Cheng Ju", "authors": "Cheng Ju and David Benkeser and Mark J. van der Laan", "title": "Robust inference on the average treatment effect using the outcome\n  highly adaptive lasso", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many estimators of the average effect of a treatment on an outcome require\nestimation of the propensity score, the outcome regression, or both. It is\noften beneficial to utilize flexible techniques such as semiparametric\nregression or machine learning to estimate these quantities. However, optimal\nestimation of these regressions does not necessarily lead to optimal estimation\nof the average treatment effect, particularly in settings with strong\ninstrumental variables. A recent proposal addressed these issues via the\noutcome-adaptive lasso, a penalized regression technique for estimating the\npropensity score that seeks to minimize the impact of instrumental variables on\ntreatment effect estimators. However, a notable limitation of this approach is\nthat its application is restricted to parametric models. We propose a more\nflexible alternative that we call the outcome highly adaptive lasso. We discuss\nlarge sample theory for this estimator and propose closed form confidence\nintervals based on the proposed estimator. We show via simulation that our\nmethod offers benefits over several popular approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:47:37 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 18:01:26 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 00:04:41 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ju", "Cheng", ""], ["Benkeser", "David", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1806.06790", "submitter": "Roel Dobbe", "authors": "Roel Dobbe, Oscar Sondermeijer, David Fridovich-Keil, Daniel Arnold,\n  Duncan Callaway and Claire Tomlin", "title": "Towards Distributed Energy Services: Decentralizing Optimal Power Flow\n  with Machine Learning", "comments": "Accepted for publication. To appear in the IEEE Transactions on Smart\n  Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of optimal power flow (OPF) methods to perform voltage and\npower flow regulation in electric networks is generally believed to require\nextensive communication. We consider distribution systems with multiple\ncontrollable Distributed Energy Resources (DERs) and present a data-driven\napproach to learn control policies for each DER to reconstruct and mimic the\nsolution to a centralized OPF problem from solely locally available\ninformation. Collectively, all local controllers closely match the centralized\nOPF solution, providing near optimal performance and satisfaction of system\nconstraints. A rate distortion framework enables the analysis of how well the\nresulting fully decentralized control policies are able to reconstruct the OPF\nsolution. The methodology provides a natural extension to decide what nodes a\nDER should communicate with to improve the reconstruction of its individual\npolicy. The method is applied on both single- and three-phase test feeder\nnetworks using data from real loads and distributed generators, focusing on\nDERs that do not exhibit inter-temporal dependencies. It provides a framework\nfor Distribution System Operators to efficiently plan and operate the\ncontributions of DERs to achieve Distributed Energy Services in distribution\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 19:46:37 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 21:30:07 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 23:30:09 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Dobbe", "Roel", ""], ["Sondermeijer", "Oscar", ""], ["Fridovich-Keil", "David", ""], ["Arnold", "Daniel", ""], ["Callaway", "Duncan", ""], ["Tomlin", "Claire", ""]]}, {"id": "1806.06798", "submitter": "Yunhao Tang", "authors": "Yunhao Tang and Shipra Agrawal", "title": "Implicit Policy for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Implicit Policy, a general class of expressive policies that can\nflexibly represent complex action distributions in reinforcement learning, with\nefficient algorithms to compute entropy regularized policy gradients. We\nempirically show that, despite its simplicity in implementation, entropy\nregularization combined with a rich policy class can attain desirable\nproperties displayed under maximum entropy reinforcement learning framework,\nsuch as robustness and multi-modality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 08:24:36 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 16:26:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tang", "Yunhao", ""], ["Agrawal", "Shipra", ""]]}, {"id": "1806.06802", "submitter": "Yameng Liu", "authors": "Yameng Liu, Aw Dieng, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky", "title": "Interpretable Almost Matching Exactly for Causal Inference", "comments": "AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to create the highest possible quality of treatment-control matches\nfor categorical data in the potential outcomes framework. Matching methods are\nheavily used in the social sciences due to their interpretability, but most\nmatching methods do not pass basic sanity checks: they fail when irrelevant\nvariables are introduced, and tend to be either computationally slow or produce\nlow-quality matches. The method proposed in this work aims to match units on a\nweighted Hamming distance, taking into account the relative importance of the\ncovariates; the algorithm aims to match units on as many relevant variables as\npossible. To do this, the algorithm creates a hierarchy of covariate\ncombinations on which to match (similar to downward closure), in the process\nsolving an optimization problem for each unit in order to construct the optimal\nmatches. The algorithm uses a single dynamic program to solve all of the\noptimization problems simultaneously. Notable advantages of our method over\nexisting matching procedures are its high-quality matches, versatility in\nhandling different data distributions that may have irrelevant variables, and\nability to handle missing data by matching on as many available covariates as\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:11:56 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 03:29:31 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 22:07:54 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 04:50:17 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 04:12:27 GMT"}, {"version": "v6", "created": "Sat, 8 Jun 2019 04:48:20 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Yameng", ""], ["Dieng", "Aw", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1806.06827", "submitter": "Omar Rivasplata", "authors": "Omar Rivasplata, Emilio Parrado-Hernandez, John Shawe-Taylor, Shiliang\n  Sun, Csaba Szepesvari", "title": "PAC-Bayes bounds for stable algorithms with instance-dependent priors", "comments": "16 pages, discussion of theory and experiments in the main body,\n  detailed proofs and experimental details in the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAC-Bayes bounds have been proposed to get risk estimates based on a training\nsample. In this paper the PAC-Bayes approach is combined with stability of the\nhypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting\nis used with a Gaussian prior centered at the expected output. Thus a novelty\nof our paper is using priors defined in terms of the data-generating\ndistribution. Our main result estimates the risk of the randomized algorithm in\nterms of the hypothesis stability coefficients. We also provide a new bound for\nthe SVM classifier, which is compared to other known bounds experimentally.\nOurs appears to be the first stability-based bound that evaluates to\nnon-trivial values.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 17:04:04 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 16:44:24 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Rivasplata", "Omar", ""], ["Parrado-Hernandez", "Emilio", ""], ["Shawe-Taylor", "John", ""], ["Sun", "Shiliang", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1806.06850", "submitter": "Norm Matloff PhD", "authors": "Xi Cheng and Bohdan Khomtchouk and Norman Matloff and Pete Mohanty", "title": "Polynomial Regression As an Alternative to Neural Nets", "comments": "23 pages, 1 figure, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of neural networks (NNs), there is still a concern among\nmany over their \"black box\" nature. Why do they work? Here we present a simple\nanalytic argument that NNs are in fact essentially polynomial regression\nmodels. This view will have various implications for NNs, e.g. providing an\nexplanation for why convergence problems arise in NNs, and it gives rough\nguidance on avoiding overfitting. In addition, we use this phenomenon to\npredict and confirm a multicollinearity property of NNs not previously reported\nin the literature. Most importantly, given this loose correspondence, one may\nchoose to routinely use polynomial models instead of NNs, thus avoiding some\nmajor problems of the latter, such as having to set many tuning parameters and\ndealing with convergence issues. We present a number of empirical results; in\neach case, the accuracy of the polynomial approach matches or exceeds that of\nNN approaches. A many-featured, open-source software package, polyreg, is\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 05:06:43 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 06:24:21 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 06:14:17 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cheng", "Xi", ""], ["Khomtchouk", "Bohdan", ""], ["Matloff", "Norman", ""], ["Mohanty", "Pete", ""]]}, {"id": "1806.06875", "submitter": "Amjad Almahairi", "authors": "Amjad Almahairi, Kyle Kastner, Kyunghyun Cho, Aaron Courville", "title": "Learning Distributed Representations from Reviews for Collaborative\n  Filtering", "comments": "Published in RecSys 2015 conference", "journal-ref": null, "doi": "10.1145/2792838.2800192", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that collaborative filter-based recommender systems can\nbe improved by incorporating side information, such as natural language\nreviews, as a way of regularizing the derived product representations.\nMotivated by the success of this approach, we introduce two different models of\nreviews and study their effect on collaborative filtering performance. While\nthe previous state-of-the-art approach is based on a latent Dirichlet\nallocation (LDA) model of reviews, the models we explore are neural network\nbased: a bag-of-words product-of-experts model and a recurrent neural network.\nWe demonstrate that the increased flexibility offered by the product-of-experts\nmodel allowed it to achieve state-of-the-art performance on the Amazon review\ndataset, outperforming the LDA-based approach. However, interestingly, the\ngreater modeling power offered by the recurrent neural network appears to\nundermine the model's ability to act as a regularizer of the product\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:18:05 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Almahairi", "Amjad", ""], ["Kastner", "Kyle", ""], ["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""]]}, {"id": "1806.06877", "submitter": "Prashant Doshi", "authors": "Saurabh Arora and Prashant Doshi", "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and\n  Progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning (IRL) is the problem of inferring the reward\nfunction of an agent, given its policy or observed behavior. Analogous to RL,\nIRL is perceived both as a problem and as a class of methods. By categorically\nsurveying the current literature in IRL, this article serves as a reference for\nresearchers and practitioners of machine learning and beyond to understand the\nchallenges of IRL and select the approaches best suited for the problem on\nhand. The survey formally introduces the IRL problem along with its central\nchallenges such as the difficulty in performing accurate inference and its\ngeneralizability, its sensitivity to prior knowledge, and the disproportionate\ngrowth in solution complexity with problem size. The article elaborates how the\ncurrent methods mitigate these challenges. We further discuss the extensions to\ntraditional IRL methods for handling: inaccurate and incomplete perception, an\nincomplete model, multiple reward functions, and nonlinear reward functions.\nThis survey concludes the discussion with some broad advances in the research\narea and currently open research questions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:26:29 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 15:07:39 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 18:45:24 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Arora", "Saurabh", ""], ["Doshi", "Prashant", ""]]}, {"id": "1806.06908", "submitter": "Nikhil Garg", "authors": "Nikhil Garg and Ramesh Johari", "title": "Designing Optimal Binary Rating Systems", "comments": "38 pages; To appear in proceedings of AISTATS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern online platforms rely on effective rating systems to learn about\nitems. We consider the optimal design of rating systems that collect binary\nfeedback after transactions. We make three contributions. First, we formalize\nthe performance of a rating system as the speed with which it recovers the true\nunderlying ranking on items (in a large deviations sense), accounting for both\nitems' underlying match rates and the platform's preferences. Second, we\nprovide an efficient algorithm to compute the binary feedback system that\nyields the highest such performance. Finally, we show how this theoretical\nperspective can be used to empirically design an implementable, approximately\noptimal rating system, and validate our approach using real-world experimental\ndata collected on Amazon Mechanical Turk.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 20:06:54 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 03:17:32 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 00:33:56 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Garg", "Nikhil", ""], ["Johari", "Ramesh", ""]]}, {"id": "1806.06913", "submitter": "Vitaly Shalumov", "authors": "Vitaly Shalumov, Itzik Klein", "title": "Deep Learning based Estimation of Weaving Target Maneuvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In target tracking, the estimation of an unknown weaving target frequency is\ncrucial for improving the miss distance. The estimation process is commonly\ncarried out in a Kalman framework. The objective of this paper is to examine\nthe potential of using neural networks in target tracking applications. To that\nend, we propose estimating the weaving frequency using deep neural networks,\ninstead of classical Kalman framework based estimation. Particularly, we focus\non the case where a set of possible constant target frequencies is known.\nSeveral neural network architectures, requiring low computational resources\nwere designed to estimate the unknown frequency out of the known set of\nfrequencies. The proposed approach performance is compared with the multiple\nmodel adaptive estimation algorithm. Simulation results show that in the\nexamined scenarios, deep neural network outperforms multiple model adaptive\nestimation in terms of accuracy and the amount of required measurements to\nconvergence.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 06:16:14 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Shalumov", "Vitaly", ""], ["Klein", "Itzik", ""]]}, {"id": "1806.06914", "submitter": "Shangda Li", "authors": "Shangda Li, Selina Bing, Steven Yang", "title": "Distributional Advantage Actor-Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional reinforcement learning, an agent maximizes the reward\ncollected during its interaction with the environment by approximating the\noptimal policy through the estimation of value functions. Typically, given a\nstate s and action a, the corresponding value is the expected discounted sum of\nrewards. The optimal action is then chosen to be the action a with the largest\nvalue estimated by value function. However, recent developments have shown both\ntheoretical and experimental evidence of superior performance when value\nfunction is replaced with value distribution in context of deep Q learning [1].\nIn this paper, we develop a new algorithm that combines advantage actor-critic\nwith value distribution estimated by quantile regression. We evaluated this new\nalgorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a\nvariety of tasks, and observed it to achieve at least as good as baseline\nalgorithms, and outperforming baseline in some tasks with smaller variance and\nincreased stability.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 06:17:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Shangda", ""], ["Bing", "Selina", ""], ["Yang", "Steven", ""]]}, {"id": "1806.06915", "submitter": "Frank Glavin", "authors": "Frank G. Glavin", "title": "A One-Sided Classification Toolkit with Applications in the Analysis of\n  Spectroscopy Data", "comments": "Research Master's Dissertation. National University of Ireland,\n  Galway. (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation investigates the use of one-sided classification algorithms\nin the application of separating hazardous chlorinated solvents from other\nmaterials, based on their Raman spectra. The experimentation is carried out\nusing a new one-sided classification toolkit that was designed and developed\nfrom the ground up. In the one-sided classification paradigm, the objective is\nto separate elements of the target class from all outliers. These one-sided\nclassifiers are generally chosen, in practice, when there is a deficiency of\nsome sort in the training examples. Sometimes outlier examples can be rare,\nexpensive to label, or even entirely absent. However, this author would like to\nnote that they can be equally applicable when outlier examples are plentiful\nbut nonetheless not statistically representative of the complete outlier\nconcept. It is this scenario that is explicitly dealt with in this research\nwork. In these circumstances, one-sided classifiers have been found to be more\nrobust that conventional multi-class classifiers. The term \"unexpected\"\noutliers is introduced to represent outlier examples, encountered in the test\nset, that have been taken from a different distribution to the training set\nexamples. These are examples that are a result of an inadequate representation\nof all possible outliers in the training set. It can often be impossible to\nfully characterise outlier examples given the fact that they can represent the\nimmeasurable quantity of \"everything else\" that is not a target. The findings\nfrom this research have shown the potential drawbacks of using conventional\nmulti-class classification algorithms when the test data come from a completely\ndifferent distribution to that of the training samples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:43:21 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Glavin", "Frank G.", ""]]}, {"id": "1806.06920", "submitter": "Abbas Abdolmaleki", "authors": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos,\n  Nicolas Heess, Martin Riedmiller", "title": "Maximum a Posteriori Policy Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.RO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm for reinforcement learning called Maximum\naposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative\nentropy objective. We show that several existing methods can directly be\nrelated to our derivation. We develop two off-policy algorithms and demonstrate\nthat they are competitive with the state-of-the-art in deep reinforcement\nlearning. In particular, for continuous control, our method outperforms\nexisting methods with respect to sample efficiency, premature convergence and\nrobustness to hyperparameter settings while achieving similar or better final\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:46:23 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Abdolmaleki", "Abbas", ""], ["Springenberg", "Jost Tobias", ""], ["Tassa", "Yuval", ""], ["Munos", "Remi", ""], ["Heess", "Nicolas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1806.06923", "submitter": "Will Dabney", "authors": "Will Dabney, Georg Ostrovski, David Silver, R\\'emi Munos", "title": "Implicit Quantile Networks for Distributional Reinforcement Learning", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we build on recent advances in distributional reinforcement\nlearning to give a generally applicable, flexible, and state-of-the-art\ndistributional variant of DQN. We achieve this by using quantile regression to\napproximate the full quantile function for the state-action return\ndistribution. By reparameterizing a distribution over the sample space, this\nyields an implicitly defined return distribution and gives rise to a large\nclass of risk-sensitive policies. We demonstrate improved performance on the 57\nAtari 2600 games in the ALE, and use our algorithm's implicitly defined\ndistributions to study the effects of risk-sensitive policies in Atari games.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:28:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dabney", "Will", ""], ["Ostrovski", "Georg", ""], ["Silver", "David", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1806.06924", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carriere and Ulrich Bauer", "title": "On the Metric Distortion of Embedding Persistence Diagrams into\n  separable Hilbert spaces", "comments": "Small change in proof of Lemma 3.4", "journal-ref": "35th International Symposium on Computational Geometry (SoCG\n  2019), 21:1-15", "doi": "10.4230/LIPIcs.SoCG.2019.21", "report-no": null, "categories": "cs.LG cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams are important descriptors in Topological Data Analysis.\nDue to the nonlinearity of the space of persistence diagrams equipped with\ntheir {\\em diagram distances}, most of the recent attempts at using persistence\ndiagrams in machine learning have been done through kernel methods, i.e.,\nembeddings of persistence diagrams into Reproducing Kernel Hilbert Spaces, in\nwhich all computations can be performed easily. Since persistence diagrams\nenjoy theoretical stability guarantees for the diagram distances, the {\\em\nmetric properties} of the feature map, i.e., the relationship between the\nHilbert distance and the diagram distances, are of central interest for\nunderstanding if the persistence diagram guarantees carry over to the\nembedding. In this article, we study the possibility of embedding persistence\ndiagrams into separable Hilbert spaces, with bi-Lipschitz maps. In particular,\nwe show that for several stable embeddings into infinite-dimensional Hilbert\nspaces defined in the literature, any lower bound must depend on the\ncardinalities of the persistence diagrams, and that when the Hilbert space is\nfinite dimensional, finding a bi-Lipschitz embedding is impossible, even when\nrestricting the persistence diagrams to have bounded cardinalities.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:49:08 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 11:55:41 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 22:01:55 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Carriere", "Mathieu", ""], ["Bauer", "Ulrich", ""]]}, {"id": "1806.06926", "submitter": "Wojciech Samek", "authors": "Christopher Anders, Gr\\'egoire Montavon, Wojciech Samek, Klaus-Robert\n  M\\\"uller", "title": "Understanding Patch-Based Learning by Explaining Predictions", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are able to learn highly predictive models of video data. Due\nto video length, a common strategy is to train them on small video snippets. We\napply the deep Taylor / LRP technique to understand the deep network's\nclassification decisions, and identify a \"border effect\": a tendency of the\nclassifier to look mainly at the bordering frames of the input. This effect\nrelates to the step size used to build the video snippet, which we can then\ntune in order to improve the classifier's accuracy without retraining the\nmodel. To our knowledge, this is the the first work to apply the deep Taylor /\nLRP technique on any video analyzing neural network.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:44:45 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Anders", "Christopher", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1806.06927", "submitter": "Jaehong Kim", "authors": "Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee,\n  Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim", "title": "Auto-Meta: Automated Gradient Based Meta Learner Search", "comments": "Presented at NIPS 2018 Workshop on Meta-Learning (MetaLearn 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:28:02 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:02:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kim", "Jaehong", ""], ["Lee", "Sangyeul", ""], ["Kim", "Sungwan", ""], ["Cha", "Moonsu", ""], ["Lee", "Jung Kwon", ""], ["Choi", "Youngduck", ""], ["Choi", "Yongseok", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.06928", "submitter": "Risto Vuorio", "authors": "Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon Kim", "title": "Meta Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using neural networks in practical settings would benefit from the ability of\nthe networks to learn new tasks throughout their lifetimes without forgetting\nthe previous tasks. This ability is limited in the current deep neural networks\nby a problem called catastrophic forgetting, where training on new tasks tends\nto severely degrade performance on previous tasks. One way to lessen the impact\nof the forgetting problem is to constrain parameters that are important to\nprevious tasks to stay close to the optimal parameters. Recently, multiple\ncompetitive approaches for computing the importance of the parameters with\nrespect to the previous tasks have been presented. In this paper, we propose a\nlearning to optimize algorithm for mitigating catastrophic forgetting. Instead\nof trying to formulate a new constraint function ourselves, we propose to train\nanother neural network to predict parameter update steps that respect the\nimportance of parameters to the previous tasks. In the proposed meta-training\nscheme, the update predictor is trained to minimize loss on a combination of\ncurrent and past tasks. We show experimentally that the proposed approach works\nin the continual learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:49:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Vuorio", "Risto", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Daejoong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.06931", "submitter": "Yangchen Pan", "authors": "Yangchen Pan, Amir-massoud Farahmand, Martha White, Saleh Nabi, Piyush\n  Grover, Daniel Nikovski", "title": "Reinforcement Learning with Function-Valued Action Spaces for Partial\n  Differential Equation Control", "comments": "ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that reinforcement learning (RL) is a promising\napproach to control dynamical systems described by partial differential\nequations (PDE). This paper shows how to use RL to tackle more general PDE\ncontrol problems that have continuous high-dimensional action spaces with\nspatial relationship among action dimensions. In particular, we propose the\nconcept of action descriptors, which encode regularities among\nspatially-extended action dimensions and enable the agent to control\nhigh-dimensional action PDEs. We provide theoretical evidence suggesting that\nthis approach can be more sample efficient compared to a conventional approach\nthat treats each action dimension separately and does not explicitly exploit\nthe spatial regularity of the action space. The action descriptor approach is\nthen used within the deep deterministic policy gradient algorithm. Experiments\non two PDE control problems, with up to 256-dimensional continuous actions,\nshow the advantage of the proposed approach over the conventional one.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 03:47:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pan", "Yangchen", ""], ["Farahmand", "Amir-massoud", ""], ["White", "Martha", ""], ["Nabi", "Saleh", ""], ["Grover", "Piyush", ""], ["Nikovski", "Daniel", ""]]}, {"id": "1806.06940", "submitter": "ChengAn Bai", "authors": "Cheng'an Bai, Chao Zhou", "title": "Pressure Predictions of Turbine Blades with Deep Learning", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been used in many areas, such as feature detections in\nimages and the game of go. This paper presents a study that attempts to use the\ndeep learning method to predict turbomachinery performance. Three different\ndeep neural networks are built and trained to predict the pressure\ndistributions of turbine airfoils. The performance of a library of turbine\nairfoils were firstly predicted using methods based on Euler equations, which\nwere then used to train and validate the deep learning neural networks. The\nresults show that network with four layers of convolutional neural network and\ntwo layers of fully connected neural network provides the best predictions. For\nthe best neural network architecture, the pressure prediction on more than 99%\nlocations are better than 3% and 90% locations are better than 1%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 03:17:08 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Bai", "Cheng'an", ""], ["Zhou", "Chao", ""]]}, {"id": "1806.06945", "submitter": "Xueyu Mao", "authors": "Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti", "title": "Overlapping Clustering Models, and One (class) SVM to Bind Them All", "comments": "In NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People belong to multiple communities, words belong to multiple topics, and\nbooks cover multiple genres; overlapping clusters are commonplace. Many\nexisting overlapping clustering methods model each person (or word, or book) as\na non-negative weighted combination of \"exemplars\" who belong solely to one\ncommunity, with some small noise. Geometrically, each person is a point on a\ncone whose corners are these exemplars. This basic form encompasses the widely\nused Mixed Membership Stochastic Blockmodel of networks (Airoldi et al., 2008)\nand its degree-corrected variants (Jin et al., 2017), as well as topic models\nsuch as LDA (Blei et al., 2003). We show that a simple one-class SVM yields\nprovably consistent parameter inference for all such models, and scales to\nlarge datasets. Experimental results on several simulated and real datasets\nshow our algorithm (called SVM-cone) is both accurate and scalable.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 21:00:00 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 18:00:01 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Mao", "Xueyu", ""], ["Sarkar", "Purnamrita", ""], ["Chakrabarti", "Deepayan", ""]]}, {"id": "1806.06949", "submitter": "Mieszko Lis", "authors": "Maximilian Golub, Guy Lemieux, Mieszko Lis", "title": "Full deep neural network training on a pruned weight budget", "comments": null, "journal-ref": "Proceedings of the 2nd SysML Conference, Palo Alto, CA, USA, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a DNN training technique that learns only a fraction of the full\nparameter set without incurring an accuracy penalty. To do this, our algorithm\nconstrains the total number of weights updated during backpropagation to those\nwith the highest total gradients. The remaining weights are not tracked, and\ntheir initial value is regenerated at every access to avoid storing them in\nmemory. This can dramatically reduce the number of off-chip memory accesses\nduring both training and inference, a key component of the energy needs of DNN\naccelerators. By ensuring that the total weight diffusion remains close to that\nof baseline unpruned SGD, networks pruned using our technique are able to\nretain state-of-the-art accuracy across network architectures -- including\nnetworks previously identified as difficult to compress, such as Densenet and\nWRN. With ResNet18 on ImageNet, we observe an 11.7$\\times$ weight reduction\nwith no accuracy loss, and up to 24.4$\\times$ with a small accuracy impact.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:40:33 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 05:51:53 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Golub", "Maximilian", ""], ["Lemieux", "Guy", ""], ["Lis", "Mieszko", ""]]}, {"id": "1806.06950", "submitter": "Patrick Chen", "authors": "Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, Cho-jui Hsieh", "title": "GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model\n  Shrinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is essential for serving large deep neural nets on devices\nwith limited resources or applications that require real-time responses. As a\ncase study, a state-of-the-art neural language model usually consists of one or\nmore recurrent layers sandwiched between an embedding layer used for\nrepresenting input tokens and a softmax layer for generating output tokens. For\nproblems with a very large vocabulary size, the embedding and the softmax\nmatrices can account for more than half of the model size. For instance, the\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\nmethod for neural language models, based on vocabulary-partition (block) based\nlow-rank matrix approximation and the inherent frequency distribution of tokens\n(the power-law distribution of words). The experimental results show our method\ncan significantly outperform traditional compression methods such as low-rank\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\ncompression rate for the embedding and softmax matrices, and when combined with\nquantization, our method can achieve 26 times compression rate, which\ntranslates to a factor of 12.8 times compression for the entire model with very\nlittle degradation in perplexity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:08:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chen", "Patrick H.", ""], ["Si", "Si", ""], ["Li", "Yang", ""], ["Chelba", "Ciprian", ""], ["Hsieh", "Cho-jui", ""]]}, {"id": "1806.06953", "submitter": "Wenjia Meng", "authors": "Wenjia Meng, Qian Zheng, Long Yang, Pengfei Li, Gang Pan", "title": "Qualitative Measurements of Policy Discrepancy for Return-Based Deep\n  Q-Network", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2948892", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Q-network (DQN) and return-based reinforcement learning are two\npromising algorithms proposed in recent years. DQN brings advances to complex\nsequential decision problems, while return-based algorithms have advantages in\nmaking use of sample trajectories. In this paper, we propose a general\nframework to combine DQN and most of the return-based reinforcement learning\nalgorithms, named R-DQN. We show the performance of traditional DQN can be\nimproved effectively by introducing return-based reinforcement learning. In\norder to further improve the R-DQN, we design a strategy with two measurements\nwhich can qualitatively measure the policy discrepancy. Moreover, we give the\ntwo measurements' bounds in the proposed R-DQN framework. We show that\nalgorithms with our strategy can accurately express the trace coefficient and\nachieve a better approximation to return. The experiments, conducted on several\nrepresentative tasks from the OpenAI Gym library, validate the effectiveness of\nthe proposed measurements. The results also show that the algorithms with our\nstrategy outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:12:18 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 10:26:32 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 13:31:08 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meng", "Wenjia", ""], ["Zheng", "Qian", ""], ["Yang", "Long", ""], ["Li", "Pengfei", ""], ["Pan", "Gang", ""]]}, {"id": "1806.06975", "submitter": "Joseph Paul Cohen", "authors": "Francis Dutil, Joseph Paul Cohen, Martin Weiss, Georgy Derevyanko,\n  Yoshua Bengio", "title": "Towards Gene Expression Convolutions using Gene Interaction Graphs", "comments": "4 pages +1 page references, To appear in the International Conference\n  on Machine Learning Workshop on Computational Biology, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the challenges of applying deep learning to gene expression data. We\nfind experimentally that there exists non-linear signal in the data, however is\nit not discovered automatically given the noise and low numbers of samples used\nin most research. We discuss how gene interaction graphs (same pathway,\nprotein-protein, co-expression, or research paper text association) can be used\nto impose a bias on a deep model similar to the spatial bias imposed by\nconvolutions on an image. We explore the usage of Graph Convolutional Neural\nNetworks coupled with dropout and gene embeddings to utilize the graph\ninformation. We find this approach provides an advantage for particular tasks\nin a low data regime but is very dependent on the quality of the graph used. We\nconclude that more work should be done in this direction. We design experiments\nthat show why existing methods fail to capture signal that is present in the\ndata when features are added which clearly isolates the problem that needs to\nbe addressed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:40:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dutil", "Francis", ""], ["Cohen", "Joseph Paul", ""], ["Weiss", "Martin", ""], ["Derevyanko", "Georgy", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.06977", "submitter": "Akhilesh Gotmare", "authors": "Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, Richard Socher", "title": "Using Mode Connectivity for Loss Landscape Analysis", "comments": "Accepted as a workshop paper at ICML's Workshop on Modern Trends in\n  Nonconvex Optimization for Machine Learning, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode connectivity is a recently introduced frame- work that empirically\nestablishes the connected- ness of minima by finding a high accuracy curve\nbetween two independently trained models. To investigate the limits of this\nsetup, we examine the efficacy of this technique in extreme cases where the\ninput models are trained or initialized differently. We find that the procedure\nis resilient to such changes. Given this finding, we propose using the\nframework for analyzing loss surfaces and training trajectories more generally,\nand in this direction, study SGD with cosine annealing and restarts (SGDR). We\nreport that while SGDR moves over barriers in its trajectory, propositions\nclaiming that it converges to and escapes from multiple local minima are not\nsubstantiated by our empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:00:26 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Gotmare", "Akhilesh", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1806.06988", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Irene Garcia Morillo and Timothy M. Hospedales", "title": "Deep Neural Decision Trees", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been proven powerful at processing perceptual data,\nsuch as images and audio. However for tabular data, tree-based models are more\npopular. A nice property of tree-based models is their natural\ninterpretability. In this work, we present Deep Neural Decision Trees (DNDT) --\ntree models realised by neural networks. A DNDT is intrinsically interpretable,\nas it is a tree. Yet as it is also a neural network (NN), it can be easily\nimplemented in NN toolkits, and trained with gradient descent rather than\ngreedy splitting. We evaluate DNDT on several tabular datasets, verify its\nefficacy, and investigate similarities and differences between DNDT and vanilla\ndecision trees. Interestingly, DNDT self-prunes at both split and\nfeature-level.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 00:05:20 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Yang", "Yongxin", ""], ["Morillo", "Irene Garcia", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1806.07001", "submitter": "Xudong Pan", "authors": "Xudong Pan, Mi Zhang, Daizong Ding", "title": "Theoretical Analysis of Image-to-Image Translation with Adversarial\n  Learning", "comments": "will appear in ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, a unified model for image-to-image translation tasks within\nadversarial learning framework has aroused widespread research interests in\ncomputer vision practitioners. Their reported empirical success however lacks\nsolid theoretical interpretations for its inherent mechanism. In this paper, we\nreformulate their model from a brand-new geometrical perspective and have\neventually reached a full interpretation on some interesting but unclear\nempirical phenomenons from their experiments. Furthermore, by extending the\ndefinition of generalization for generative adversarial nets to a broader\nsense, we have derived a condition to control the generalization capability of\ntheir model. According to our derived condition, several practical suggestions\nhave also been proposed on model design and dataset construction as a guidance\nfor further empirical researches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 01:31:28 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pan", "Xudong", ""], ["Zhang", "Mi", ""], ["Ding", "Daizong", ""]]}, {"id": "1806.07004", "submitter": "Satoshi Hara", "authors": "Satoshi Hara, Kouichi Ikeno, Tasuku Soma, Takanori Maehara", "title": "Maximally Invariant Data Perturbation as Explanation", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several feature scoring methods are proposed to explain the output of\ncomplex machine learning models, most of them lack formal mathematical\ndefinitions. In this study, we propose a novel definition of the feature score\nusing the maximally invariant data perturbation, which is inspired from the\nidea of adversarial example. In adversarial example, one seeks the smallest\ndata perturbation that changes the model's output. In our proposed approach, we\nconsider the opposite: we seek the maximally invariant data perturbation that\ndoes not change the model's output. In this way, we can identify important\ninput features as the ones with small allowable data perturbations. To find the\nmaximally invariant data perturbation, we formulate the problem as linear\nprogramming. The experiment on the image classification with VGG16 shows that\nthe proposed method could identify relevant parts of the images effectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 01:48:59 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 14:28:24 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Hara", "Satoshi", ""], ["Ikeno", "Kouichi", ""], ["Soma", "Tasuku", ""], ["Maehara", "Takanori", ""]]}, {"id": "1806.07057", "submitter": "Md Mohaimenuzzaman", "authors": "Md Mohaimenuzzaman, Zahraa Said Abdallah, Joarder Kamruzzaman, Bala\n  Srinivasan", "title": "Effect of Hyper-Parameter Optimization on the Deep Learning Model\n  Proposed for Distributed Attack Detection in Internet of Things Environment", "comments": "6 pages, 2 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the effect of various hyper-parameters and their selection\nfor the best performance of the deep learning model proposed in [1] for\ndistributed attack detection in the Internet of Things (IoT). The findings show\nthat there are three hyper-parameters that have more influence on the best\nperformance achieved by the model. As a consequence, this study shows that the\nmodel's accuracy as reported in the paper is not achievable, based on the best\nselections of parameters, which is also supported by another recent publication\n[2].\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:11:32 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Mohaimenuzzaman", "Md", ""], ["Abdallah", "Zahraa Said", ""], ["Kamruzzaman", "Joarder", ""], ["Srinivasan", "Bala", ""]]}, {"id": "1806.07066", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Restricted Boltzmann Machines: Introduction and Review", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted Boltzmann machine is a network of stochastic units with\nundirected interactions between pairs of visible and hidden units. This model\nwas popularized as a building block of deep learning architectures and has\ncontinued to play an important role in applied and theoretical machine\nlearning. Restricted Boltzmann machines carry a rich structure, with\nconnections to geometry, applied algebra, probability, statistics, machine\nlearning, and other areas. The analysis of these models is attractive in its\nown right and also as a platform to combine and generalize mathematical tools\nfor graphical models with hidden variables. This article gives an introduction\nto the mathematical analysis of restricted Boltzmann machines, reviews recent\nresults on the geometry of the sets of probability distributions representable\nby these models, and suggests a few directions for further investigation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:53:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1806.07082", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Simplifying Probabilistic Expressions in Causal Inference", "comments": "This is the version published in JMLR", "journal-ref": "Journal of Machine Learning Research (JMLR), 18(36):1-30, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining a non-parametric expression for an interventional distribution is\none of the most fundamental tasks in causal inference. Such an expression can\nbe obtained for an identifiable causal effect by an algorithm or by manual\napplication of do-calculus. Often we are left with a complicated expression\nwhich can lead to biased or inefficient estimates when missing data or\nmeasurement errors are involved. We present an automatic simplification\nalgorithm that seeks to eliminate symbolically unnecessary variables from these\nexpressions by taking advantage of the structure of the underlying graphical\nmodel. Our method is applicable to all causal effect formulas and is readily\navailable in the R package causaleffect.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:45:14 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07085", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Enhancing Identification of Causal Effects by Pruning", "comments": "This is the version published in JMLR", "journal-ref": "Journal of Machine Learning Research (JMLR), 18(194):1-23, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal models communicate our assumptions about causes and effects in\nreal-world phe- nomena. Often the interest lies in the identification of the\neffect of an action which means deriving an expression from the observed\nprobability distribution for the interventional distribution resulting from the\naction. In many cases an identifiability algorithm may return a complicated\nexpression that contains variables that are in fact unnecessary. In practice\nthis can lead to additional computational burden and increased bias or\ninefficiency of estimates when dealing with measurement error or missing data.\nWe present graphical criteria to detect variables which are redundant in\nidentifying causal effects. We also provide an improved version of a well-known\nidentifiability algorithm that implements these criteria.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:52:17 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07104", "submitter": "Alon Cohen", "authors": "Alon Cohen, Avinatan Hassidim, Tomer Koren, Nevena Lazic, Yishay\n  Mansour, Kunal Talwar", "title": "Online Linear Quadratic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of controlling linear time-invariant systems with known\nnoisy dynamics and adversarially chosen quadratic losses. We present the first\nefficient online learning algorithms in this setting that guarantee\n$O(\\sqrt{T})$ regret under mild assumptions, where $T$ is the time horizon. Our\nalgorithms rely on a novel SDP relaxation for the steady-state distribution of\nthe system. Crucially, and in contrast to previously proposed relaxations, the\nfeasible solutions of our SDP all correspond to \"strongly stable\" policies that\nmix exponentially fast to a steady state.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:42:45 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Cohen", "Alon", ""], ["Hassidim", "Avinatan", ""], ["Koren", "Tomer", ""], ["Lazic", "Nevena", ""], ["Mansour", "Yishay", ""], ["Talwar", "Kunal", ""]]}, {"id": "1806.07108", "submitter": "Qiqi Zhang", "authors": "Qiqi Zhang and Ying Liu", "title": "Improving brain computer interface performance by data augmentation with\n  conditional Deep Convolutional Generative Adversarial Networks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the big restrictions in brain computer interface field is the very\nlimited training samples, it is difficult to build a reliable and usable system\nwith such limited data. Inspired by generative adversarial networks, we propose\na conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks\nmethod to generate more artificial EEG signal automatically for data\naugmentation to improve the performance of convolutional neural networks in\nbrain computer interface field and overcome the small training dataset\nproblems. We evaluate the proposed cDCGAN method on BCI competition dataset of\nmotor imagery. The results show that the generated artificial EEG data from\nGaussian noise can learn the features from raw EEG data and has no less than\nthe classification accuracy of raw EEG data in the testing dataset. Also by\nusing generated artificial data can effectively improve classification accuracy\nat the same model with limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:49:50 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 08:32:32 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhang", "Qiqi", ""], ["Liu", "Ying", ""]]}, {"id": "1806.07129", "submitter": "Dennis Collaris", "authors": "Dennis Collaris, Leo M. Vink, Jarke J. van Wijk", "title": "Instance-Level Explanations for Fraud Detection: A Case Study", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud detection is a difficult problem that can benefit from predictive\nmodeling. However, the verification of a prediction is challenging; for a\nsingle insurance policy, the model only provides a prediction score. We present\na case study where we reflect on different instance-level model explanation\ntechniques to aid a fraud detection team in their work. To this end, we\ndesigned two novel dashboards combining various state-of-the-art explanation\ntechniques. These enable the domain expert to analyze and understand\npredictions, dramatically speeding up the process of filtering potential fraud\ncases. Finally, we discuss the lessons learned and outline open research\nissues.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:42:36 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Collaris", "Dennis", ""], ["Vink", "Leo M.", ""], ["van Wijk", "Jarke J.", ""]]}, {"id": "1806.07137", "submitter": "Jack Baker", "authors": "Jack Baker, Paul Fearnhead, Emily B Fox, Christopher Nemeth", "title": "Large-Scale Stochastic Sampling from the Probability Simplex", "comments": "Accepted to Advances in Neural Information Processing Systems (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular\nmethod for scalable Bayesian inference. These methods are based on sampling a\ndiscrete-time approximation to a continuous time process, such as the Langevin\ndiffusion. When applied to distributions defined on a constrained space the\ntime-discretization error can dominate when we are near the boundary of the\nspace. We demonstrate that because of this, current SGMCMC methods for the\nsimplex struggle with sparse simplex spaces; when many of the components are\nclose to zero. Unfortunately, many popular large-scale Bayesian models, such as\nnetwork or topic models, require inference on sparse simplex spaces. To avoid\nthe biases caused by this discretization error, we propose the stochastic\nCox-Ingersoll-Ross process (SCIR), which removes all discretization error and\nwe prove that samples from the SCIR process are asymptotically unbiased. We\ndiscuss how this idea can be extended to target other constrained spaces. Use\nof the SCIR process within a SGMCMC algorithm is shown to give substantially\nbetter performance for a topic model and a Dirichlet process mixture model than\nexisting SGMCMC approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 10:08:37 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 16:06:24 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Baker", "Jack", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B", ""], ["Nemeth", "Christopher", ""]]}, {"id": "1806.07139", "submitter": "Henry Moss", "authors": "Henry B.Moss, David S.Leslie and Paul Rayson", "title": "Using J-K fold Cross Validation to Reduce Variance When Tuning NLP\n  Models", "comments": "COLING 2018. Code available at:\n  https://github.com/henrymoss/COLING2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  K-fold cross validation (CV) is a popular method for estimating the true\nperformance of machine learning models, allowing model selection and parameter\ntuning. However, the very process of CV requires random partitioning of the\ndata and so our performance estimates are in fact stochastic, with variability\nthat can be substantial for natural language processing tasks. We demonstrate\nthat these unstable estimates cannot be relied upon for effective parameter\ntuning. The resulting tuned parameters are highly sensitive to how our data is\npartitioned, meaning that we often select sub-optimal parameter choices and\nhave serious reproducibility issues.\n  Instead, we propose to use the less variable J-K-fold CV, in which J\nindependent K-fold cross validations are used to assess performance. Our main\ncontributions are extending J-K-fold CV from performance estimation to\nparameter tuning and investigating how to choose J and K. We argue that\nvariability is more important than bias for effective tuning and so advocate\nlower choices of K than are typically seen in the NLP literature, instead use\nthe saved computation to increase J. To demonstrate the generality of our\nrecommendations we investigate a wide range of case-studies: sentiment\nclassification (both general and target-specific), part-of-speech tagging and\ndocument classification.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 10:12:25 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Moss", "Henry B.", ""], ["Leslie", "David S.", ""], ["Rayson", "Paul", ""]]}, {"id": "1806.07174", "submitter": "Farshid Rayhan", "authors": "Farshid Rayhan, Sajid Ahmed, Zaynab Mousavian, Dewan Md Farid,\n  Swakkhar Shatabda", "title": "FRnet-DTI: Deep Convolutional Neural Networks with Evolutionary and\n  Structural Features for Drug-Target Interaction", "comments": null, "journal-ref": "2019", "doi": "10.1016/j.heliyon.2020.e03444", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The task of drug-target interaction prediction holds significant importance\nin pharmacology and therapeutic drug design. In this paper, we present\nFRnet-DTI, an auto encoder and a convolutional classifier for feature\nmanipulation and drug target interaction prediction. Two convolutional neural\nneworks are proposed where one model is used for feature manipulation and the\nother one for classification. Using the first method FRnet-1, we generate 4096\nfeatures for each of the instances in each of the datasets and use the second\nmethod, FRnet-2, to identify interaction probability employing those features.\nWe have tested our method on four gold standard datasets exhaustively used by\nother researchers. Experimental results shows that our method significantly\nimproves over the state-of-the-art method on three of the four drug-target\ninteraction gold standard datasets on both area under curve for Receiver\nOperating Characteristic(auROC) and area under Precision Recall curve(auPR)\nmetric. We also introduce twenty new potential drug-target pairs for\ninteraction based on high prediction scores. Codes Available: https: // github.\ncom/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.\npythonanywhere. com/ FRnet-DTI/\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:01:57 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 09:14:56 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 08:25:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Rayhan", "Farshid", ""], ["Ahmed", "Sajid", ""], ["Mousavian", "Zaynab", ""], ["Farid", "Dewan Md", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "1806.07185", "submitter": "Thomas Lucas", "authors": "Thomas Lucas, Corentin Tallec, Jakob Verbeek, Yann Ollivier", "title": "Mixed batches and symmetric discriminators for GAN training", "comments": "Accepted at ICML 2018 (long oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are pow- erful generative models based\non providing feed- back to a generative network via a discriminator network.\nHowever, the discriminator usually as- sesses individual samples. This prevents\nthe dis- criminator from accessing global distributional statistics of\ngenerated samples, and often leads to mode dropping: the generator models only\npart of the target distribution. We propose to feed the discriminator with\nmixed batches of true and fake samples, and train it to predict the ratio of\ntrue samples in the batch. The latter score does not depend on the order of\nsamples in a batch. Rather than learning this invariance, we introduce a\ngeneric permutation-invariant discriminator ar- chitecture. This architecture\nis provably a uni- versal approximator of all symmetric functions.\nExperimentally, our approach reduces mode col- lapse in GANs on two synthetic\ndatasets, and obtains good results on the CIFAR10 and CelebA datasets, both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:39:11 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lucas", "Thomas", ""], ["Tallec", "Corentin", ""], ["Verbeek", "Jakob", ""], ["Ollivier", "Yann", ""]]}, {"id": "1806.07190", "submitter": "Thomas Beckers", "authors": "Thomas Beckers, Dana Kuli\\'c, Sandra Hirche", "title": "Stable Gaussian Process based Tracking Control of Euler-Lagrange Systems", "comments": "Accepted manuscript for publication in Elsevier Automatica", "journal-ref": null, "doi": "10.1016/j.automatica.2019.01.023", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfect tracking control for real-world Euler-Lagrange systems is challenging\ndue to uncertainties in the system model and external disturbances. The\nmagnitude of the tracking error can be reduced either by increasing the\nfeedback gains or improving the model of the system. The latter is clearly\npreferable as it allows to maintain good tracking performance at low feedback\ngains. However, accurate models are often difficult to obtain. In this article,\nwe address the problem of stable high-performance tracking control for unknown\nEuler-Lagrange systems. In particular, we employ Gaussian Process regression to\nobtain a data-driven model that is used for the feed-forward compensation of\nunknown dynamics of the system. The model fidelity is used to adapt the\nfeedback gains allowing low feedback gains in state space regions of high model\nconfidence. The proposed control law guarantees a globally bounded tracking\nerror with a specific probability. Simulation studies demonstrate the\nsuperiority over state of the art tracking control approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:48:35 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 16:15:41 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Beckers", "Thomas", ""], ["Kuli\u0107", "Dana", ""], ["Hirche", "Sandra", ""]]}, {"id": "1806.07200", "submitter": "Sebastian Curi", "authors": "Sebastian Curi, Kfir Y. Levy, Andreas Krause", "title": "Adaptive Input Estimation in Linear Dynamical Systems with Applications\n  to Learning-from-Observations", "comments": "CDC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the inputs of a dynamical system from\nmeasurements of the system's outputs. To this end, we introduce a novel\nestimation algorithm that explicitly trades off bias and variance to optimally\nreduce the overall estimation error. This optimal trade-off is done efficiently\nand adaptively in every time step. Experimentally, we show that our method\noften produces estimates with substantially lower error compared to the\nstate-of-the-art. Finally, we consider the more complex\n\\emph{Learning-from-Observations} framework, where an agent should learn a\ncontroller from the outputs of an expert's demonstration. We incorporate our\nestimation algorithm as a building block inside this framework and show that it\nenables learning controllers successfully.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:06:22 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 12:32:48 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Curi", "Sebastian", ""], ["Levy", "Kfir Y.", ""], ["Krause", "Andreas", ""]]}, {"id": "1806.07223", "submitter": "Christian H\\\"ager", "authors": "Christoffer Fougstedt, Christian H\\\"ager, Lars Svensson, Henry D.\n  Pfister, Per Larsson-Edefors", "title": "ASIC Implementation of Time-Domain Digital Backpropagation with\n  Deep-Learned Chromatic Dispersion Filters", "comments": "3 pages, 3 figures, updated reference list, added one sentence in the\n  result section for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider time-domain digital backpropagation with chromatic dispersion\nfilters jointly optimized and quantized using machine-learning techniques.\nCompared to the baseline implementations, we show improved BER performance and\n>40% power dissipation reductions in 28-nm CMOS.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:42:33 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 00:35:46 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Fougstedt", "Christoffer", ""], ["H\u00e4ger", "Christian", ""], ["Svensson", "Lars", ""], ["Pfister", "Henry D.", ""], ["Larsson-Edefors", "Per", ""]]}, {"id": "1806.07247", "submitter": "Canyi Lu", "authors": "Canyi Lu", "title": "Tensor-Tensor Product Toolbox", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.03728.\n  Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]\nis a natural generalization of matrix multiplication. Based on t-product, many\noperations on matrix can be extended to tensor cases, including tensor SVD,\ntensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many\nothers. The linear algebraic structure of tensors are similar to the matrix\ncases. We develop a Matlab toolbox to implement several basic operations on\ntensors based on t-product. The toolbox is available at\nhttps://github.com/canyilu/tproduct.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 08:14:42 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 03:23:18 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lu", "Canyi", ""]]}, {"id": "1806.07259", "submitter": "Georg Martius", "authors": "Subham S. Sahoo and Christoph H. Lampert and Georg Martius", "title": "Learning Equations for Extrapolation and Control", "comments": "9 pages, 9 figures, ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to identify concise equations from data using a\nshallow neural network approach. In contrast to ordinary black-box regression,\nthis approach allows understanding functional relations and generalizing them\nfrom observed data to unseen parts of the parameter space. We show how to\nextend the class of learnable equations for a recently proposed equation\nlearning network to include divisions, and we improve the learning and model\nselection strategy to be useful for challenging real-world data. For systems\ngoverned by analytical expressions, our method can in many cases identify the\ntrue underlying equation and extrapolate to unseen domains. We demonstrate its\neffectiveness by experiments on a cart-pendulum system, where only 2 random\nrollouts are required to learn the forward dynamics and successfully achieve\nthe swing-up task.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:06:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sahoo", "Subham S.", ""], ["Lampert", "Christoph H.", ""], ["Martius", "Georg", ""]]}, {"id": "1806.07268", "submitter": "Rahul Savani", "authors": "Frans A. Oliehoek, Rahul Savani, Jose Gallego, Elise van der Pol,\n  Roderich Gro{\\ss}", "title": "Beyond Local Nash Equilibria for Adversarial Networks", "comments": "Supersedes arXiv:1712.00679; v2 includes Fictitious GAN in the\n  related work and refers to Danskin (1981)", "journal-ref": "Published in Benelearn/BANIC 2018", "doi": "10.1007/978-3-030-31978-6_7", "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Save for some special cases, current training methods for Generative\nAdversarial Networks (GANs) are at best guaranteed to converge to a `local Nash\nequilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual\nNash equilibrium (NE), which implies that there are no guarantees on the\nquality of the found generator or classifier. This paper proposes to model GANs\nexplicitly as finite games in mixed strategies, thereby ensuring that every LNE\nis an NE. With this formulation, we propose a solution method that is proven to\nmonotonically converge to a resource-bounded Nash equilibrium (RB-NE): by\nincreasing computational resources we can find better solutions. We empirically\ndemonstrate that our method is less prone to typical GAN problems such as mode\ncollapse, and produces solutions that are less exploitable than those produced\nby GANs and MGANs, and closely resemble theoretical predictions about NEs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:23:50 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 15:15:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Oliehoek", "Frans A.", ""], ["Savani", "Rahul", ""], ["Gallego", "Jose", ""], ["van der Pol", "Elise", ""], ["Gro\u00df", "Roderich", ""]]}, {"id": "1806.07297", "submitter": "TImothee Lacroix", "authors": "Timoth\\'ee Lacroix, Nicolas Usunier, Guillaume Obozinski", "title": "Canonical Tensor Decomposition for Knowledge Base Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Knowledge Base Completion can be framed as a 3rd-order binary\ntensor completion problem. In this light, the Canonical Tensor Decomposition\n(CP) (Hitchcock, 1927) seems like a natural solution; however, current\nimplementations of CP on standard Knowledge Base Completion benchmarks are\nlagging behind their competitors. In this work, we attempt to understand the\nlimits of CP for knowledge base completion. First, we motivate and test a novel\nregularizer, based on tensor nuclear $p$-norms. Then, we present a\nreformulation of the problem that makes it invariant to arbitrary choices in\nthe inclusion of predicates or their reciprocals in the dataset. These two\nmethods combined allow us to beat the current state of the art on several\ndatasets with a CP decomposition, and obtain even better results using the more\nadvanced ComplEx model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:57:18 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lacroix", "Timoth\u00e9e", ""], ["Usunier", "Nicolas", ""], ["Obozinski", "Guillaume", ""]]}, {"id": "1806.07307", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani", "title": "Estimation from Non-Linear Observations via Convex Programming with\n  Application to Bilinear Regression", "comments": "Some elaboration on the algorithm and theoretical results are added.\n  Minor errors and typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient estimator, formulated as a convex\nprogram, for a broad class of non-linear regression problems that involve\ndifference of convex (DC) non-linearities. The proposed method can be viewed as\na significant extension of the \"anchored regression\" method formulated and\nanalyzed in [10] for regression with convex non-linearities. Our main\nassumption, in addition to other mild statistical and computational\nassumptions, is availability of a certain approximation oracle for the average\nof the gradients of the observation functions at a ground truth. Under this\nassumption and using a PAC-Bayesian analysis we show that the proposed\nestimator produces an accurate estimate with high probability. As a concrete\nexample, we study the proposed framework in the bilinear regression problem\nwith Gaussian factors and quantify a sufficient sample complexity for exact\nrecovery. Furthermore, we describe a computationally tractable scheme that\nprovably produces the required approximation oracle in the considered bilinear\nregression problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:28:46 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 18:43:46 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Bahmani", "Sohail", ""]]}, {"id": "1806.07336", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler", "title": "Neural Code Comprehension: A Learnable Representation of Code Semantics", "comments": "Published at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of embeddings in natural language processing,\nresearch has been conducted into applying similar methods to code analysis.\nMost works attempt to process the code directly or use a syntactic tree\nrepresentation, treating it like sentences written in a natural language.\nHowever, none of the existing methods are sufficient to comprehend program\nsemantics robustly, due to structural features such as function calls,\nbranching, and interchangeable order of statements. In this paper, we propose a\nnovel processing technique to learn code semantics, and apply it to a variety\nof program analysis tasks. In particular, we stipulate that a robust\ndistributional hypothesis of code applies to both human- and machine-generated\nprograms. Following this hypothesis, we define an embedding space, inst2vec,\nbased on an Intermediate Representation (IR) of the code that is independent of\nthe source programming language. We provide a novel definition of contextual\nflow for this IR, leveraging both the underlying data- and control-flow of the\nprogram. We then analyze the embeddings qualitatively using analogies and\nclustering, and evaluate the learned representation on three different\nhigh-level tasks. We show that even without fine-tuning, a single RNN\narchitecture and fixed inst2vec embeddings outperform specialized approaches\nfor performance prediction (compute device mapping, optimal thread coarsening);\nand algorithm classification from raw code (104 classes), where we set a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:43:44 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 16:10:52 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 08:15:00 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Jakobovits", "Alice Shoshana", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1806.07348", "submitter": "Jan-Christian H\\\"utter", "authors": "Aden Forrow, Jan-Christian H\\\"utter, Mor Nitzan, Philippe Rigollet,\n  Geoffrey Schiebinger, Jonathan Weed", "title": "Statistical Optimal Transport via Factored Couplings", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to estimate Wasserstein distances and optimal\ntransport plans between two probability distributions from samples in high\ndimension. Unlike plug-in rules that simply replace the true distributions by\ntheir empirical counterparts, our method promotes couplings with low transport\nrank, a new structural assumption that is similar to the nonnegative rank of a\nmatrix. Regularizing based on this assumption leads to drastic improvements on\nhigh-dimensional data for various tasks, including domain adaptation in\nsingle-cell RNA sequencing data. These findings are supported by a theoretical\nanalysis that indicates that the transport rank is key in overcoming the curse\nof dimensionality inherent to data-driven optimal transport.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:58:54 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 16:01:34 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 03:40:28 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Forrow", "Aden", ""], ["H\u00fctter", "Jan-Christian", ""], ["Nitzan", "Mor", ""], ["Rigollet", "Philippe", ""], ["Schiebinger", "Geoffrey", ""], ["Weed", "Jonathan", ""]]}, {"id": "1806.07353", "submitter": "Matteo Fischetti", "authors": "Matteo Fischetti, Iacopo Mandatelli, Domenico Salvagnin", "title": "Faster SGD training by minibatch persistency", "comments": "Submitted to an international conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that, for most datasets, the use of large-size minibatches\nfor Stochastic Gradient Descent (SGD) typically leads to slow convergence and\npoor generalization. On the other hand, large minibatches are of great\npractical interest as they allow for a better exploitation of modern GPUs.\nPrevious literature on the subject concentrated on how to adjust the main SGD\nparameters (in particular, the learning rate) when using large minibatches. In\nthis work we introduce an additional feature, that we call minibatch\npersistency, that consists in reusing the same minibatch for K consecutive SGD\niterations. The computational conjecture here is that a large minibatch\ncontains a significant sample of the training set, so one can afford to\nslightly overfitting it without worsening generalization too much. The approach\nis intended to speedup SGD convergence, and also has the advantage of reducing\nthe overhead related to data loading on the internal GPU memory. We present\ncomputational results on CIFAR-10 with an AlexNet architecture, showing that\neven small persistency values (K=2 or 5) already lead to a significantly faster\nconvergence and to a comparable (or even better) generalization than the\nstandard \"disposable minibatch\" approach (K=1), in particular when large\nminibatches are used. The lesson learned is that minibatch persistency can be a\nsimple yet effective way to deal with large minibatches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 17:27:11 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Fischetti", "Matteo", ""], ["Mandatelli", "Iacopo", ""], ["Salvagnin", "Domenico", ""]]}, {"id": "1806.07366", "submitter": "David Duvenaud", "authors": "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud", "title": "Neural Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of deep neural network models. Instead of\nspecifying a discrete sequence of hidden layers, we parameterize the derivative\nof the hidden state using a neural network. The output of the network is\ncomputed using a black-box differential equation solver. These continuous-depth\nmodels have constant memory cost, adapt their evaluation strategy to each\ninput, and can explicitly trade numerical precision for speed. We demonstrate\nthese properties in continuous-depth residual networks and continuous-time\nlatent variable models. We also construct continuous normalizing flows, a\ngenerative model that can train by maximum likelihood, without partitioning or\nordering the data dimensions. For training, we show how to scalably\nbackpropagate through any ODE solver, without access to its internal\noperations. This allows end-to-end training of ODEs within larger models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 17:50:12 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 00:13:07 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 22:06:50 GMT"}, {"version": "v4", "created": "Tue, 15 Jan 2019 01:56:48 GMT"}, {"version": "v5", "created": "Sat, 14 Dec 2019 02:01:18 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chen", "Ricky T. Q.", ""], ["Rubanova", "Yulia", ""], ["Bettencourt", "Jesse", ""], ["Duvenaud", "David", ""]]}, {"id": "1806.07373", "submitter": "Evan Shelhamer", "authors": "Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A. Efros, Sergey\n  Levine", "title": "Few-Shot Segmentation Propagation with Guided Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based methods for visual segmentation have made progress on\nparticular types of segmentation tasks, but are limited by the necessary\nsupervision, the narrow definitions of fixed tasks, and the lack of control\nduring inference for correcting errors. To remedy the rigidity and annotation\nburden of standard approaches, we address the problem of few-shot segmentation:\ngiven few image and few pixel supervision, segment any images accordingly. We\npropose guided networks, which extract a latent task representation from any\namount of supervision, and optimize our architecture end-to-end for fast,\naccurate few-shot segmentation. Our method can switch tasks without further\noptimization and quickly update when given more guidance. We report the first\nresults for segmentation from one pixel per concept and show real-time\ninteractive video segmentation. Our unified approach propagates pixel\nannotations across space for interactive segmentation, across time for video\nsegmentation, and across scenes for semantic segmentation. Our guided segmentor\nis state-of-the-art in accuracy for the amount of annotation and time. See\nhttp://github.com/shelhamer/revolver for code, models, and more details.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:02:03 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Rakelly", "Kate", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.07385", "submitter": "Nils Strodthoff", "authors": "Nils Strodthoff and Claas Strodthoff", "title": "Detecting and interpreting myocardial infarction using fully\n  convolutional neural networks", "comments": "11 pages, 4 figures", "journal-ref": "Physiological Measurement, vol. 40, no. 1, p. 015001, 2019", "doi": "10.1088/1361-6579/aaf34d", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We aim to provide an algorithm for the detection of myocardial\ninfarction that operates directly on ECG data without any preprocessing and to\ninvestigate its decision criteria. Approach: We train an ensemble of fully\nconvolutional neural networks on the PTB ECG dataset and apply state-of-the-art\nattribution methods. Main results: Our classifier reaches 93.3% sensitivity and\n89.7% specificity evaluated using 10-fold cross-validation with sampling based\non patients. The presented method outperforms state-of-the-art approaches and\nreaches the performance level of human cardiologists for detection of\nmyocardial infarction. We are able to discriminate channel-specific regions\nthat contribute most significantly to the neural network's decision.\nInterestingly, the network's decision is influenced by signs also recognized by\nhuman cardiologists as indicative of myocardial infarction. Significance: Our\nresults demonstrate the high prospects of algorithmic ECG analysis for future\nclinical applications considering both its quantitative performance as well as\nthe possibility of assessing decision criteria on a per-example basis, which\nenhances the comprehensibility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 19:00:44 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 13:28:55 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Strodthoff", "Nils", ""], ["Strodthoff", "Claas", ""]]}, {"id": "1806.07406", "submitter": "Georgios Detorakis", "authors": "Georgios Detorakis, Travis Bartley, Emre Neftci", "title": "Contrastive Hebbian Learning with Random Feedback Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks are commonly trained to make predictions through learning\nalgorithms. Contrastive Hebbian learning, which is a powerful rule inspired by\ngradient backpropagation, is based on Hebb's rule and the contrastive\ndivergence algorithm. It operates in two phases, the forward (or free) phase,\nwhere the data are fed to the network, and a backward (or clamped) phase, where\nthe target signals are clamped to the output layer of the network and the\nfeedback signals are transformed through the transpose synaptic weight\nmatrices. This implies symmetries at the synaptic level, for which there is no\nevidence in the brain. In this work, we propose a new variant of the algorithm,\ncalled random contrastive Hebbian learning, which does not rely on any synaptic\nweights symmetries. Instead, it uses random matrices to transform the feedback\nsignals during the clamped phase, and the neural dynamics are described by\nfirst order non-linear differential equations. The algorithm is experimentally\nverified by solving a Boolean logic task, classification tasks (handwritten\ndigits and letters), and an autoencoding task. This article also shows how the\nparameters affect learning, especially the random matrices. We use the\npseudospectra analysis to investigate further how random matrices impact the\nlearning process. Finally, we discuss the biological plausibility of the\nproposed algorithm, and how it can give rise to better computational models for\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:02:34 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Detorakis", "Georgios", ""], ["Bartley", "Travis", ""], ["Neftci", "Emre", ""]]}, {"id": "1806.07409", "submitter": "Thomas Tanay", "authors": "Thomas Tanay, Jerone T. A. Andrews and Lewis D. Griffin", "title": "Built-in Vulnerabilities to Imperceptible Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing models that are robust to small adversarial perturbations of their\ninputs has proven remarkably difficult. In this work we show that the reverse\nproblem---making models more vulnerable---is surprisingly easy. After\npresenting some proofs of concept on MNIST, we introduce a generic tilting\nattack that injects vulnerabilities into the linear layers of pre-trained\nnetworks by increasing their sensitivity to components of low variance in the\ntraining data without affecting their performance on test data. We illustrate\nthis attack on a multilayer perceptron trained on SVHN and use it to design a\nstand-alone adversarial module which we call a steganogram decoder. Finally, we\nshow on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1%\ncan induce vulnerabilities to chosen imperceptible backdoor signals in\nstate-of-the-art networks. Beyond their practical implications, these different\nresults shed new light on the nature of the adversarial example phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:12:36 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 22:20:58 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Tanay", "Thomas", ""], ["Andrews", "Jerone T. A.", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1806.07441", "submitter": "Zhiyu Sun", "authors": "Zhiyu Sun, Jia Lu, Stephen Baek", "title": "Wall Stress Estimation of Cerebral Aneurysm based on Zernike\n  Convolutional Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (ConvNets) have demonstrated an exceptional\ncapacity to discern visual patterns from digital images and signals.\nUnfortunately, such powerful ConvNets do not generalize well to\narbitrary-shaped manifolds, where data representation does not fit into a\ntensor-like grid. Hence, many fields of science and engineering, where data\npoints possess some manifold structure, cannot enjoy the full benefits of the\nrecent advances in ConvNets. The aneurysm wall stress estimation problem\nintroduced in this paper is one of many such problems. The problem is\nwell-known to be of a paramount clinical importance, but yet, traditional\nConvNets cannot be applied due to the manifold structure of the data, neither\ndoes the state-of-the-art geometric ConvNets perform well. Motivated by this,\nwe propose a new geometric ConvNet method named ZerNet, which builds upon our\nnovel mathematical generalization of convolution and pooling operations on\nmanifolds. Our study shows that the ZerNet outperforms the other\nstate-of-the-art geometric ConvNets in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:51:25 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sun", "Zhiyu", ""], ["Lu", "Jia", ""], ["Baek", "Stephen", ""]]}, {"id": "1806.07464", "submitter": "Stephen Bonner", "authors": "Stephen Bonner, Ibad Kureshi, John Brennan, Georgios Theodoropoulos,\n  Andrew Stephen McGough, Boguslaw Obara", "title": "Exploring the Semantic Content of Unsupervised Graph Embeddings: An\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings have become a key and widely used technique within the field\nof graph mining, proving to be successful across a broad range of domains\nincluding social, citation, transportation and biological. Graph embedding\ntechniques aim to automatically create a low-dimensional representation of a\ngiven graph, which captures key structural elements in the resulting embedding\nspace. However, to date, there has been little work exploring exactly which\ntopological structures are being learned in the embeddings process. In this\npaper, we investigate if graph embeddings are approximating something analogous\nwith traditional vertex level graph features. If such a relationship can be\nfound, it could be used to provide a theoretical insight into how graph\nembedding approaches function. We perform this investigation by predicting\nknown topological features, using supervised and unsupervised methods, directly\nfrom the embedding space. If a mapping between the embeddings and topological\nfeatures can be found, then we argue that the structural information\nencapsulated by the features is represented in the embedding space. To explore\nthis, we present extensive experimental evaluation from five state-of-the-art\nunsupervised graph embedding techniques, across a range of empirical graph\ndatasets, measuring a selection of topological features. We demonstrate that\nseveral topological features are indeed being approximated by the embedding\nspace, allowing key insight into how graph embeddings create good\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:06:32 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Bonner", "Stephen", ""], ["Kureshi", "Ibad", ""], ["Brennan", "John", ""], ["Theodoropoulos", "Georgios", ""], ["McGough", "Andrew Stephen", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1806.07470", "submitter": "Marcel Robeer", "authors": "Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu\n  Brinkhuis, Mark Neerincx", "title": "Contrastive Explanations with Local Foil Trees", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in interpretable Machine Learning (iML) and eXplainable AI\n(XAI) construct explanations based on the importance of features in\nclassification tasks. However, in a high-dimensional feature space this\napproach may become unfeasible without restraining the set of important\nfeatures. We propose to utilize the human tendency to ask questions like \"Why\nthis output (the fact) instead of that output (the foil)?\" to reduce the number\nof features to those that play a main role in the asked contrast. Our proposed\nmethod utilizes locally trained one-versus-all decision trees to identify the\ndisjoint set of rules that causes the tree to classify data points as the foil\nand not as the fact. In this study we illustrate this approach on three\nbenchmark classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:12:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["van der Waa", "Jasper", ""], ["Robeer", "Marcel", ""], ["van Diggelen", "Jurriaan", ""], ["Brinkhuis", "Matthieu", ""], ["Neerincx", "Mark", ""]]}, {"id": "1806.07492", "submitter": "Gustavo Botelho de Souza", "authors": "Gustavo Botelho de Souza, Jo\\~ao Paulo Papa and Aparecido Nilceu\n  Marana", "title": "On the Learning of Deep Local Features for Robust Face Spoofing\n  Detection", "comments": null, "journal-ref": "Proceedings of 31st Conference on Graphics, Patterns and Images\n  (SIBGRAPI) 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:56:17 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:00:52 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["de Souza", "Gustavo Botelho", ""], ["Papa", "Jo\u00e3o Paulo", ""], ["Marana", "Aparecido Nilceu", ""]]}, {"id": "1806.07498", "submitter": "Thibault Laugel", "authors": "Thibault Laugel, Xavier Renard, Marie-Jeanne Lesot, Christophe\n  Marsala, Marcin Detyniecki", "title": "Defining Locality for Surrogates in Post-hoc Interpretablity", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local surrogate models, to approximate the local decision boundary of a\nblack-box classifier, constitute one approach to generate explanations for the\nrationale behind an individual prediction made by the back-box. This paper\nhighlights the importance of defining the right locality, the neighborhood on\nwhich a local surrogate is trained, in order to approximate accurately the\nlocal black-box decision boundary. Unfortunately, as shown in this paper, this\nissue is not only a parameter or sampling distribution challenge and has a\nmajor impact on the relevance and quality of the approximation of the local\nblack-box decision boundary and thus on the meaning and accuracy of the\ngenerated explanation. To overcome the identified problems, quantified with an\nadapted measure and procedure, we propose to generate surrogate-based\nexplanations for individual predictions based on a sampling centered on\nparticular place of the decision boundary, relevant for the prediction to be\nexplained, rather than on the prediction itself as it is classically done. We\nevaluate the novel approach compared to state-of-the-art methods and a\nstraightforward improvement thereof on four UCI datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:18:48 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Laugel", "Thibault", ""], ["Renard", "Xavier", ""], ["Lesot", "Marie-Jeanne", ""], ["Marsala", "Christophe", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "1806.07504", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Siyu Tao, Wei Chen, and Daniel W. Apley", "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative\n  and Quantitative Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations often involve both qualitative and numerical inputs.\nExisting Gaussian process (GP) methods for handling this mainly assume a\ndifferent response surface for each combination of levels of the qualitative\nfactors and relate them via a multiresponse cross-covariance matrix. We\nintroduce a substantially different approach that maps each qualitative factor\nto an underlying numerical latent variable (LV), with the mapped value for each\nlevel estimated similarly to the correlation parameters. This provides a\nparsimonious GP parameterization that treats qualitative factors the same as\nnumerical variables and views them as effecting the response via similar\nphysical mechanisms. This has strong physical justification, as the effects of\na qualitative factor in any physics-based simulation model must always be due\nto some underlying numerical variables. Even when the underlying variables are\nmany, sufficient dimension reduction arguments imply that their effects can be\nrepresented by a low-dimensional LV. This conjecture is supported by the\nsuperior predictive performance observed across a variety of examples.\nMoreover, the mapped LVs provide substantial insight into the nature and\neffects of the qualitative factors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:38:03 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 17:16:39 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Yichi", ""], ["Tao", "Siyu", ""], ["Chen", "Wei", ""], ["Apley", "Daniel W.", ""]]}, {"id": "1806.07506", "submitter": "Eduardo Fonseca", "authors": "Eduardo Fonseca, Rong Gong, Xavier Serra", "title": "A Simple Fusion of Deep and Shallow Learning for Acoustic Scene\n  Classification", "comments": "accepted to SMC 2018; updated Figure 7, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past, Acoustic Scene Classification systems have been based on hand\ncrafting audio features that are input to a classifier. Nowadays, the common\ntrend is to adopt data driven techniques, e.g., deep learning, where audio\nrepresentations are learned from data. In this paper, we propose a system that\nconsists of a simple fusion of two methods of the aforementioned types: a deep\nlearning approach where log-scaled mel-spectrograms are input to a\nconvolutional neural network, and a feature engineering approach, where a\ncollection of hand-crafted features is input to a gradient boosting machine. We\nfirst show that both methods provide complementary information to some extent.\nThen, we use a simple late fusion strategy to combine both methods. We report\nclassification accuracy of each method individually and the combined system on\nthe TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms\neach of the individual methods and attains a classification accuracy of 72.8%\non the evaluation set, improving the baseline system by 11.8%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:42:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 19:29:00 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Fonseca", "Eduardo", ""], ["Gong", "Rong", ""], ["Serra", "Xavier", ""]]}, {"id": "1806.07528", "submitter": "Negar Rostamzadeh", "authors": "Alexandre Lacoste, Boris Oreshkin, Wonchang Chung, Thomas Boquet,\n  Negar Rostamzadeh, David Krueger", "title": "Uncertainty in Multitask Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using variational Bayes neural networks, we develop an algorithm capable of\naccumulating knowledge into a prior from multiple different tasks. The result\nis a rich and meaningful prior capable of few-shot learning on new tasks. The\nposterior can go beyond the mean field approximation and yields good\nuncertainty on the performed experiments. Analysis on toy tasks shows that it\ncan learn from significantly different tasks while finding similarities among\nthem. Experiments of Mini-Imagenet yields the new state of the art with 74.5%\naccuracy on 5 shot learning. Finally, we provide experiments showing that other\nexisting methods can fail to perform well in different benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 02:52:37 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 02:32:10 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 07:18:25 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Lacoste", "Alexandre", ""], ["Oreshkin", "Boris", ""], ["Chung", "Wonchang", ""], ["Boquet", "Thomas", ""], ["Rostamzadeh", "Negar", ""], ["Krueger", "David", ""]]}, {"id": "1806.07533", "submitter": "Sanvesh Srivastava", "authors": "Sanvesh Srivastava, Glen DePalma, and Chuanhai Liu", "title": "An Asynchronous Distributed Expectation Maximization Algorithm For\n  Massive Data: The DEM Algorithm", "comments": "34 pages, 5 figures, 5 tables. Accepted for publication in JCGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of Expectation-Maximization (EM) algorithms provides a general\napproach to fitting flexible models for large and complex data. The expectation\n(E) step of EM-type algorithms is time-consuming in massive data applications\nbecause it requires multiple passes through the full data. We address this\nproblem by proposing an asynchronous and distributed generalization of the EM\ncalled the Distributed EM (DEM). Using DEM, existing EM-type algorithms are\neasily extended to massive data settings by exploiting the divide-and-conquer\ntechnique and widely available computing power, such as grid computing. The DEM\nalgorithm reserves two groups of computing processes called \\emph{workers} and\n\\emph{managers} for performing the E step and the maximization step (M step),\nrespectively. The samples are randomly partitioned into a large number of\ndisjoint subsets and are stored on the worker processes. The E step of DEM\nalgorithm is performed in parallel on all the workers, and every worker\ncommunicates its results to the managers at the end of local E step. The\nmanagers perform the M step after they have received results from a\n$\\gamma$-fraction of the workers, where $\\gamma$ is a fixed constant in $(0,\n1]$. The sequence of parameter estimates generated by the DEM algorithm retains\nthe attractive properties of EM: convergence of the sequence of parameter\nestimates to a local mode and linear global rate of convergence. Across diverse\nsimulations focused on linear mixed-effects models, the DEM algorithm is\nsignificantly faster than competing EM-type algorithms while having a similar\naccuracy. The DEM algorithm maintains its superior empirical performance on a\nmovie ratings database consisting of 10 million ratings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 03:20:04 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Srivastava", "Sanvesh", ""], ["DePalma", "Glen", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1806.07537", "submitter": "Yang Shen", "authors": "Mostafa Karimi, Di Wu, Zhangyang Wang, Yang Shen", "title": "DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity\n  through Unified Recurrent and Convolutional Neural Networks", "comments": "https://github.com/Shen-Lab/DeepAffinity", "journal-ref": "Bioinformatics 35, no. 18 (2019): 3329-3338", "doi": "10.1093/bioinformatics/btz111", "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: Drug discovery demands rapid quantification of compound-protein\ninteraction (CPI). However, there is a lack of methods that can predict\ncompound-protein affinity from sequences alone with high applicability,\naccuracy, and interpretability.\n  Results: We present a seamless integration of domain knowledges and\nlearning-based approaches. Under novel representations of\nstructurally-annotated protein sequences, a semi-supervised deep learning model\nthat unifies recurrent and convolutional neural networks has been proposed to\nexploit both unlabeled and labeled data, for jointly encoding molecular\nrepresentations and predicting affinities. Our representations and models\noutperform conventional options in achieving relative error in IC$_{50}$ within\n5-fold for test cases and 20-fold for protein classes not included for\ntraining. Performances for new protein classes with few labeled data are\nfurther improved by transfer learning. Furthermore, separate and joint\nattention mechanisms are developed and embedded to our model to add to its\ninterpretability, as illustrated in case studies for predicting and explaining\nselective drug-target interactions. Lastly, alternative representations using\nprotein sequences or compound graphs and a unified RNN/GCNN-CNN model using\ngraph CNN (GCNN) are also explored to reveal algorithmic challenges ahead.\n  Availability: Data and source codes are available at\nhttps://github.com/Shen-Lab/DeepAffinity\n  Supplementary Information: Supplementary data are available at\nhttp://shen-lab.github.io/deep-affinity-bioinf18-supp-rev.pdf\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 03:39:33 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 05:49:49 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Karimi", "Mostafa", ""], ["Wu", "Di", ""], ["Wang", "Zhangyang", ""], ["Shen", "Yang", ""]]}, {"id": "1806.07538", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis and Tommi S. Jaakkola", "title": "Towards Robust Interpretability with Self-Explaining Neural Networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent work on interpretability of complex machine learning models has\nfocused on estimating $\\textit{a posteriori}$ explanations for previously\ntrained models around specific predictions. $\\textit{Self-explaining}$ models\nwhere interpretability plays a key role already during learning have received\nmuch less attention. We propose three desiderata for explanations in general --\nexplicitness, faithfulness, and stability -- and show that existing methods do\nnot satisfy them. In response, we design self-explaining models in stages,\nprogressively generalizing linear classifiers to complex yet architecturally\nexplicit models. Faithfulness and stability are enforced via regularization\nspecifically tailored to such models. Experimental results across various\nbenchmark datasets show that our framework offers a promising direction for\nreconciling model complexity and interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 03:47:03 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 22:15:26 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1806.07550", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Xin Dong, Hao Su", "title": "Binary Ensemble Neural Network: More Bits per Network or More Networks\n  per Bit?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNN) have been studied extensively since they run\ndramatically faster at lower memory and power consumption than floating-point\nnetworks, thanks to the efficiency of bit operations. However, contemporary\nBNNs whose weights and activations are both single bits suffer from severe\naccuracy degradation. To understand why, we investigate the representation\nability, speed and bias/variance of BNNs through extensive experiments. We\nconclude that the error of BNNs is predominantly caused by the intrinsic\ninstability (training time) and non-robustness (train & test time). Inspired by\nthis investigation, we propose the Binary Ensemble Neural Network (BENN) which\nleverages ensemble methods to improve the performance of BNNs with limited\nefficiency cost. While ensemble techniques have been broadly believed to be\nonly marginally helpful for strong classifiers such as deep neural networks,\nour analyses and experiments show that they are naturally a perfect fit to\nboost BNNs. We find that our BENN, which is faster and much more robust than\nstate-of-the-art binary networks, can even surpass the accuracy of the\nfull-precision floating number network with the same architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 04:48:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:08:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhu", "Shilin", ""], ["Dong", "Xin", ""], ["Su", "Hao", ""]]}, {"id": "1806.07555", "submitter": "Vincent Zhuang", "authors": "Yanan Sui, Vincent Zhuang, Joel W. Burdick, and Yisong Yue", "title": "Stagewise Safe Bayesian Optimization with Gaussian Processes", "comments": "International Conference on Machine Learning (ICML) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enforcing safety is a key aspect of many problems pertaining to sequential\ndecision making under uncertainty, which require the decisions made at every\nstep to be both informative of the optimal decision and also safe. For example,\nwe value both efficacy and comfort in medical therapy, and efficiency and\nsafety in robotic control. We consider this problem of optimizing an unknown\nutility function with absolute feedback or preference feedback subject to\nunknown safety constraints. We develop an efficient safe Bayesian optimization\nalgorithm, StageOpt, that separates safe region expansion and utility function\nmaximization into two distinct stages. Compared to existing approaches which\ninterleave between expansion and optimization, we show that StageOpt is more\nefficient and naturally applicable to a broader class of problems. We provide\ntheoretical guarantees for both the satisfaction of safety constraints as well\nas convergence to the optimal utility value. We evaluate StageOpt on both a\nvariety of synthetic experiments, as well as in clinical practice. We\ndemonstrate that StageOpt is more effective than existing safe optimization\napproaches, and is able to safely and effectively optimize spinal cord\nstimulation therapy in our clinical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 05:06:15 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 09:21:39 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sui", "Yanan", ""], ["Zhuang", "Vincent", ""], ["Burdick", "Joel W.", ""], ["Yue", "Yisong", ""]]}, {"id": "1806.07562", "submitter": "Clara Stegehuis", "authors": "Clara Stegehuis and Laurent Massouli\\'e", "title": "Efficient inference in stochastic block models with vertex labels", "comments": null, "journal-ref": null, "doi": "10.1109/TNSE.2019.2913949", "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic block model with two communities where vertices\ncontain side information in the form of a vertex label. These vertex labels may\nhave arbitrary label distributions, depending on the community memberships. We\nanalyze a linearized version of the popular belief propagation algorithm. We\nshow that this algorithm achieves the highest accuracy possible whenever a\ncertain function of the network parameters has a unique fixed point. Whenever\nthis function has multiple fixed points, the belief propagation algorithm may\nnot perform optimally. We show that increasing the information in the vertex\nlabels may reduce the number of fixed points and hence lead to optimality of\nbelief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 05:53:32 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 11:40:54 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Stegehuis", "Clara", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1806.07568", "submitter": "Jaehong Kim", "authors": "Jaehong Kim, Sungeun Hong, Yongseok Choi, Jiwon Kim", "title": "Doubly Nested Network for Resource-Efficient Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose doubly nested network(DNNet) where all neurons represent their own\nsub-models that solve the same task. Every sub-model is nested both layer-wise\nand channel-wise. While nesting sub-models layer-wise is straight-forward with\ndeep-supervision as proposed in \\cite{xie2015holistically}, channel-wise\nnesting has not been explored in the literature to our best knowledge.\nChannel-wise nesting is non-trivial as neurons between consecutive layers are\nall connected to each other. In this work, we introduce a technique to solve\nthis problem by sorting channels topologically and connecting neurons\naccordingly. For the purpose, channel-causal convolutions are used. Slicing\ndoubly nested network gives a working sub-network. The most notable application\nof our proposed network structure with slicing operation is resource-efficient\ninference. At test time, computing resources such as time and memory available\nfor running the prediction algorithm can significantly vary across devices and\napplications. Given a budget constraint, we can slice the network accordingly\nand use a sub-model for inference within budget, requiring no additional\ncomputation such as training or fine-tuning after deployment. We demonstrate\nthe effectiveness of our approach in several practical scenarios of utilizing\navailable resource efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:11:35 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kim", "Jaehong", ""], ["Hong", "Sungeun", ""], ["Choi", "Yongseok", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.07569", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Aurelien Lucchi, Matilde Gargiani, An Bian, Thomas\n  Hofmann, Martin Jaggi", "title": "A Distributed Second-Order Algorithm You Can Trust", "comments": "appearing at ICML 2018 - Proceedings of the 35th International\n  Conference on Machine Learning, Stockholm, Schweden, PMLR 80, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid growth of data and computational resources, distributed\noptimization has become an active research area in recent years. While\nfirst-order methods seem to dominate the field, second-order methods are\nnevertheless attractive as they potentially require fewer communication rounds\nto converge. However, there are significant drawbacks that impede their wide\nadoption, such as the computation and the communication of a large Hessian\nmatrix. In this paper we present a new algorithm for distributed training of\ngeneralized linear models that only requires the computation of diagonal blocks\nof the Hessian matrix on the individual workers. To deal with this approximate\ninformation we propose an adaptive approach that - akin to trust-region methods\n- dynamically adapts the auxiliary model to compensate for modeling errors. We\nprovide theoretical rates of convergence for a wide class of problems including\nL1-regularized objectives. We also demonstrate that our approach achieves\nstate-of-the-art results on multiple large benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:18:00 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Lucchi", "Aurelien", ""], ["Gargiani", "Matilde", ""], ["Bian", "An", ""], ["Hofmann", "Thomas", ""], ["Jaggi", "Martin", ""]]}, {"id": "1806.07572", "submitter": "Arthur Jacot", "authors": "Arthur Jacot, Franck Gabriel, Cl\\'ement Hongler", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "comments": null, "journal-ref": "In Advances in neural information processing systems (pp.\n  8571-8580) 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At initialization, artificial neural networks (ANNs) are equivalent to\nGaussian processes in the infinite-width limit, thus connecting them to kernel\nmethods. We prove that the evolution of an ANN during training can also be\ndescribed by a kernel: during gradient descent on the parameters of an ANN, the\nnetwork function $f_\\theta$ (which maps input vectors to output vectors)\nfollows the kernel gradient of the functional cost (which is convex, in\ncontrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel\n(NTK). This kernel is central to describe the generalization features of ANNs.\nWhile the NTK is random at initialization and varies during training, in the\ninfinite-width limit it converges to an explicit limiting kernel and it stays\nconstant during training. This makes it possible to study the training of ANNs\nin function space instead of parameter space. Convergence of the training can\nthen be related to the positive-definiteness of the limiting NTK. We prove the\npositive-definiteness of the limiting NTK when the data is supported on the\nsphere and the non-linearity is non-polynomial. We then focus on the setting of\nleast-squares regression and show that in the infinite-width limit, the network\nfunction $f_\\theta$ follows a linear differential equation during training. The\nconvergence is fastest along the largest kernel principal components of the\ninput data with respect to the NTK, hence suggesting a theoretical motivation\nfor early stopping. Finally we study the NTK numerically, observe its behavior\nfor wide networks, and compare it to the infinite-width limit.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:35:46 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 10:31:42 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 15:42:05 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 08:39:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jacot", "Arthur", ""], ["Gabriel", "Franck", ""], ["Hongler", "Cl\u00e9ment", ""]]}, {"id": "1806.07644", "submitter": "Gustavo Botelho de Souza", "authors": "Johnatan S. Oliveira, Gustavo B. Souza, Anderson R. Rocha, Fl\\'avio E.\n  Deus and Aparecido N. Marana", "title": "Cross-Domain Deep Face Matching for Real Banking Security Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the security of transactions is currently one of the major\nchallenges that banking systems deal with. The usage of face for biometric\nauthentication of users is attracting large investments from banks worldwide\ndue to its convenience and acceptability by people, especially in cross-domain\nscenarios, in which facial images from ID documents are compared with digital\nself-portraits (selfies) for the automated opening of new checking accounts,\ne.g, or financial transactions authorization. Actually, the comparison of\nselfies and IDs has also been applied in another wide variety of tasks\nnowadays, such as automated immigration control. The major difficulty in such\nprocess consists in attenuating the differences between the facial images\ncompared given their different domains. In this work, in addition to collecting\na large cross-domain face dataset, with 27,002 real facial images of selfies\nand ID documents (13,501 subjects) captured from the databases of the major\npublic Brazilian bank, we propose a novel architecture for such cross-domain\nmatching problem based on deep features extracted by two well-referenced\nConvolutional Neural Networks (CNN). Results obtained on the dataset collected,\ncalled FaceBank, with accuracy rates higher than 93%, demonstrate the\nrobustness of the proposed approach to the cross-domain face matching problem\nand its feasible application in real banking security systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 10:00:52 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 22:15:39 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 12:56:58 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oliveira", "Johnatan S.", ""], ["Souza", "Gustavo B.", ""], ["Rocha", "Anderson R.", ""], ["Deus", "Fl\u00e1vio E.", ""], ["Marana", "Aparecido N.", ""]]}, {"id": "1806.07690", "submitter": "Hao Song", "authors": "Hao Song, Meelis Kull, Peter Flach", "title": "Non-Parametric Calibration of Probabilistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of calibration is to retrospectively adjust the outputs from a\nmachine learning model to provide better probability estimates on the target\nvariable. While calibration has been investigated thoroughly in classification,\nit has not yet been well-established for regression tasks. This paper considers\nthe problem of calibrating a probabilistic regression model to improve the\nestimated probability densities over the real-valued targets. We propose to\ncalibrate a regression model through the cumulative probability density, which\ncan be derived from calibrating a multi-class classifier. We provide three\nnon-parametric approaches to solve the problem, two of which provide empirical\nestimates and the third providing smooth density estimates. The proposed\napproaches are experimentally evaluated to show their ability to improve the\nperformance of regression models on the predictive likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:20:46 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Song", "Hao", ""], ["Kull", "Meelis", ""], ["Flach", "Peter", ""]]}, {"id": "1806.07692", "submitter": "Marek Grzes", "authors": "Jack Shannon and Marek Grzes", "title": "Reinforcement Learning using Augmented Neural Networks", "comments": "7 pages; two columns; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks allow Q-learning reinforcement learning agents such as deep\nQ-networks (DQN) to approximate complex mappings from state spaces to value\nfunctions. However, this also brings drawbacks when compared to other function\napproximators such as tile coding or their generalisations, radial basis\nfunctions (RBF) because they introduce instability due to the side effect of\nglobalised updates present in neural networks. This instability does not even\nvanish in neural networks that do not have any hidden layers. In this paper, we\nshow that simple modifications to the structure of the neural network can\nimprove stability of DQN learning when a multi-layer perceptron is used for\nfunction approximation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:29:14 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Shannon", "Jack", ""], ["Grzes", "Marek", ""]]}, {"id": "1806.07697", "submitter": "Zhao Kang", "authors": "Zhao Kang, Xiao Lu, Jinfeng Yi, Zenglin Xu", "title": "Self-weighted Multiple Kernel Learning for Graph-based Clustering and\n  Semi-supervised Classification", "comments": "Accepted by IJCAI 2018, Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple kernel learning (MKL) method is generally believed to perform better\nthan single kernel method. However, some empirical studies show that this is\nnot always true: the combination of multiple kernels may even yield an even\nworse performance than using a single kernel. There are two possible reasons\nfor the failure: (i) most existing MKL methods assume that the optimal kernel\nis a linear combination of base kernels, which may not hold true; and (ii) some\nkernel weights are inappropriately assigned due to noises and carelessly\ndesigned algorithms. In this paper, we propose a novel MKL framework by\nfollowing two intuitive assumptions: (i) each kernel is a perturbation of the\nconsensus kernel; and (ii) the kernel that is close to the consensus kernel\nshould be assigned a large weight. Impressively, the proposed method can\nautomatically assign an appropriate weight to each kernel without introducing\nadditional parameters, as existing methods do. The proposed framework is\nintegrated into a unified framework for graph-based clustering and\nsemi-supervised classification. We have conducted experiments on multiple\nbenchmark datasets and our empirical results verify the superiority of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:46:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Yi", "Jinfeng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1806.07703", "submitter": "Ye Liu", "authors": "Ye Liu, Lifang He, Bokai Cao, Philip S. Yu, Ann B. Ragin, Alex D. Leow", "title": "Multi-View Multi-Graph Embedding for Brain Network Clustering Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analysis of human brain connectivity is critically important for\nunderstanding brain function and disease states. Embedding a brain network as a\nwhole graph instance into a meaningful low-dimensional representation can be\nused to investigate disease mechanisms and inform therapeutic interventions.\nMoreover, by exploiting information from multiple neuroimaging modalities or\nviews, we are able to obtain an embedding that is more useful than the\nembedding learned from an individual view. Therefore, multi-view multi-graph\nembedding becomes a crucial task. Currently, only a few studies have been\ndevoted to this topic, and most of them focus on the vector-based strategy\nwhich will cause structural information contained in the original graphs lost.\nAs a novel attempt to tackle this problem, we propose Multi-view Multi-graph\nEmbedding (M2E) by stacking multi-graphs into multiple partially-symmetric\ntensors and using tensor techniques to simultaneously leverage the dependencies\nand correlations among multi-view and multi-graph brain networks. Extensive\nexperiments on real HIV and bipolar disorder brain network datasets demonstrate\nthe superior performance of M2E on clustering brain networks by leveraging the\nmulti-view multi-graph interactions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:02:31 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Liu", "Ye", ""], ["He", "Lifang", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""], ["Leow", "Alex D.", ""]]}, {"id": "1806.07741", "submitter": "Felix Alexander Heilmeyer", "authors": "Felix A. Heilmeyer, Robin T. Schirrmeister, Lukas D. J. Fiederer,\n  Martin V\\\"olker, Joos Behncke, Tonio Ball", "title": "A large-scale evaluation framework for EEG deep learning architectures", "comments": "7 pages, 3 figures, final version accepted for presentation at IEEE\n  SMC 2018 conference", "journal-ref": null, "doi": "10.1109/SMC.2018.00185", "report-no": null, "categories": "eess.SP cs.LG cs.NE q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EEG is the most common signal source for noninvasive BCI applications. For\nsuch applications, the EEG signal needs to be decoded and translated into\nappropriate actions. A recently emerging EEG decoding approach is deep learning\nwith Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many\ndifferent architectures already published. Here we present a novel framework\nfor the large-scale evaluation of different deep-learning architectures on\ndifferent EEG datasets. This framework comprises (i) a collection of EEG\ndatasets currently including 100 examples (recording sessions) from six\ndifferent classification problems, (ii) a collection of different EEG decoding\nalgorithms, and (iii) a wrapper linking the decoders to the data as well as\nhandling structured documentation of all settings and (hyper-) parameters and\nstatistics, designed to ensure transparency and reproducibility. As an\napplications example we used our framework by comparing three publicly\navailable CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow\nConvNet, and two versions of EEGNet. We also show how our framework can be used\nto study similarities and differences in the performance of different decoding\nmethods across tasks. We argue that the deep learning EEG framework as\ndescribed here could help to tap the full potential of deep learning for BCI\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:49:23 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:25:46 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Heilmeyer", "Felix A.", ""], ["Schirrmeister", "Robin T.", ""], ["Fiederer", "Lukas D. J.", ""], ["V\u00f6lker", "Martin", ""], ["Behncke", "Joos", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.07751", "submitter": "Shabab Bazrafkan", "authors": "Shabab Bazrafkan, Peter Corcoran", "title": "Versatile Auxiliary Classifier with Generative Adversarial Network\n  (VAC+GAN), Multi Class Scenarios", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.00316", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generators learn the data distribution for each class in a\nmulti-class scenario and generate samples for a specific class given the right\ninput from the latent space. In this work, a method known as \"Versatile\nAuxiliary Classifier with Generative Adversarial Network\" for multi-class\nscenarios is presented. In this technique, the Generative Adversarial Networks\n(GAN)'s generator is turned into a conditional generator by placing a\nmulti-class classifier in parallel with the discriminator network and\nbackpropagate the classification error through the generator. This technique is\nversatile enough to be applied to any GAN implementation. The results on two\ndatabases and comparisons with other method are provided as well.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 10:24:38 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Bazrafkan", "Shabab", ""], ["Corcoran", "Peter", ""]]}, {"id": "1806.07755", "submitter": "Gao Huang", "authors": "Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, Kilian\n  Weinberger", "title": "An empirical study on evaluation metrics of generative adversarial\n  networks", "comments": "arXiv admin note: text overlap with arXiv:1802.03446 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating generative adversarial networks (GANs) is inherently challenging.\nIn this paper, we revisit several representative sample-based evaluation\nmetrics for GANs, and address the problem of how to evaluate the evaluation\nmetrics. We start with a few necessary conditions for metrics to produce\nmeaningful scores, such as distinguishing real from generated samples,\nidentifying mode dropping and mode collapsing, and detecting overfitting. With\na series of carefully designed experiments, we comprehensively investigate\nexisting sample-based metrics and identify their strengths and limitations in\npractical settings. Based on these results, we observe that kernel Maximum Mean\nDiscrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to\nsatisfy most of the desirable properties, provided that the distances between\nsamples are computed in a suitable feature space. Our experiments also unveil\ninteresting properties about the behavior of several popular GAN models, such\nas whether they are memorizing training samples, and how far they are from\nlearning the target distribution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:01:27 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 00:20:11 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Xu", "Qiantong", ""], ["Huang", "Gao", ""], ["Yuan", "Yang", ""], ["Guo", "Chuan", ""], ["Sun", "Yu", ""], ["Wu", "Felix", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1806.07772", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Bernt Schiele, Mario Fritz", "title": "Accurate and Diverse Sampling of Sequences based on a \"Best of Many\"\n  Sample Objective", "comments": "Added additional references and baselines. (Appeared in CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem has been formalized as a sequence extrapolation problem, where a number\nof observations are used to predict the sequence into the future. Real-world\nscenarios demand a model of uncertainty of such predictions, as predictions\nbecome increasingly uncertain -- in particular on long time horizons. While\nimpressive results have been shown on point estimates, scenarios that induce\nmulti-modal distributions over future sequences remain challenging. Our work\naddresses these challenges in a Gaussian Latent Variable model for sequence\nprediction. Our core contribution is a \"Best of Many\" sample objective that\nleads to more accurate and more diverse predictions that better capture the\ntrue variations in real-world sequence data. Beyond our analysis of improved\nmodel fit, our models also empirically outperform prior work on three diverse\ntasks ranging from traffic scenes to weather data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:49:45 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 12:56:36 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1806.07788", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Lester Mackey", "title": "Random Feature Stein Discrepancies", "comments": "In Proceedings of the 32nd Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2018). Code available at:\n  https://bitbucket.org/jhhuggins/random-feature-stein-discrepancies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computable Stein discrepancies have been deployed for a variety of\napplications, ranging from sampler selection in posterior inference to\napproximate Bayesian inference to goodness-of-fit testing. Existing\nconvergence-determining Stein discrepancies admit strong theoretical guarantees\nbut suffer from a computational cost that grows quadratically in the sample\nsize. While linear-time Stein discrepancies have been proposed for\ngoodness-of-fit testing, they exhibit avoidable degradations in testing\npower---even when power is explicitly optimized. To address these shortcomings,\nwe introduce feature Stein discrepancies ($\\Phi$SDs), a new family of quality\nmeasures that can be cheaply approximated using importance sampling. We show\nhow to construct $\\Phi$SDs that provably determine the convergence of a sample\nto its target and develop high-accuracy approximations---random $\\Phi$SDs\n(R$\\Phi$SDs)---which are computable in near-linear time. In our experiments\nwith sampler selection for approximate posterior inference and goodness-of-fit\ntesting, R$\\Phi$SDs perform as well or better than quadratic-time KSDs while\nbeing orders of magnitude faster to compute.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 15:14:15 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 03:28:04 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 01:00:51 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 16:32:09 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Mackey", "Lester", ""]]}, {"id": "1806.07789", "submitter": "Titouan Parcollet", "authors": "Titouan Parcollet, Ying Zhang, Mohamed Morchid, Chiheb Trabelsi,\n  Georges Linar\\`es, Renato De Mori and Yoshua Bengio", "title": "Quaternion Convolutional Neural Networks for End-to-End Automatic Speech\n  Recognition", "comments": "Accepted at INTERSPEECH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the connectionist temporal classification (CTC) model coupled with\nrecurrent (RNN) or convolutional neural networks (CNN), made it easier to train\nspeech recognition systems in an end-to-end fashion. However in real-valued\nmodels, time frame components such as mel-filter-bank energies and the cepstral\ncoefficients obtained from them, together with their first and second order\nderivatives, are processed as individual elements, while a natural alternative\nis to process such components as composed entities. We propose to group such\nelements in the form of quaternions and to process these quaternions using the\nestablished quaternion algebra. Quaternion numbers and quaternion neural\nnetworks have shown their efficiency to process multidimensional inputs as\nentities, to encode internal dependencies, and to solve many tasks with less\nlearning parameters than real-valued models. This paper proposes to integrate\nmultiple feature views in quaternion-valued convolutional neural network\n(QCNN), to be used for sequence-to-sequence mapping with the CTC model.\nPromising results are reported using simple QCNNs in phoneme recognition\nexperiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme\nerror rate (PER) with less learning parameters than a competing model based on\nreal-valued CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 15:16:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Parcollet", "Titouan", ""], ["Zhang", "Ying", ""], ["Morchid", "Mohamed", ""], ["Trabelsi", "Chiheb", ""], ["Linar\u00e8s", "Georges", ""], ["De Mori", "Renato", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.07808", "submitter": "Quanquan Gu", "authors": "Xiao Zhang and Yaodong Yu and Lingxiao Wang and Quanquan Gu", "title": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning one-hidden-layer neural networks with\nRectified Linear Unit (ReLU) activation function, where the inputs are sampled\nfrom standard Gaussian distribution and the outputs are generated from a noisy\nteacher network. We analyze the performance of gradient descent for training\nsuch kind of neural networks based on empirical risk minimization, and provide\nalgorithm-dependent guarantees. In particular, we prove that tensor\ninitialization followed by gradient descent can converge to the ground-truth\nparameters at a linear rate up to some statistical error. To the best of our\nknowledge, this is the first work characterizing the recovery guarantee for\npractical learning of one-hidden-layer ReLU networks with multiple neurons.\nNumerical experiments verify our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 15:52:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Zhang", "Xiao", ""], ["Yu", "Yaodong", ""], ["Wang", "Lingxiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1806.07811", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou and Pan Xu and Quanquan Gu", "title": "Stochastic Nested Variance Reduction for Nonconvex Optimization", "comments": "26 pages, 4 figures, 4 tables. In NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study finite-sum nonconvex optimization problems, where the objective\nfunction is an average of $n$ nonconvex functions. We propose a new stochastic\ngradient descent algorithm based on nested variance reduction. Compared with\nconventional stochastic variance reduced gradient (SVRG) algorithm that uses\ntwo reference points to construct a semi-stochastic gradient with diminishing\nvariance in each iteration, our algorithm uses $K+1$ nested reference points to\nbuild a semi-stochastic gradient to further reduce its variance in each\niteration. For smooth nonconvex functions, the proposed algorithm converges to\nan $\\epsilon$-approximate first-order stationary point (i.e., $\\|\\nabla\nF(\\mathbf{x})\\|_2\\leq \\epsilon$) within $\\tilde O(n\\land\n\\epsilon^{-2}+\\epsilon^{-3}\\land n^{1/2}\\epsilon^{-2})$ number of stochastic\ngradient evaluations. This improves the best known gradient complexity of SVRG\n$O(n+n^{2/3}\\epsilon^{-2})$ and that of SCSG $O(n\\land\n\\epsilon^{-2}+\\epsilon^{-10/3}\\land n^{2/3}\\epsilon^{-2})$. For gradient\ndominated functions, our algorithm also achieves better gradient complexity\nthan the state-of-the-art algorithms. Thorough experimental results on\ndifferent nonconvex optimization problems back up our theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:01:09 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 06:09:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhou", "Dongruo", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1806.07819", "submitter": "G\\\"okhan Yildirim", "authors": "G\\\"okhan Yildirim, Calvin Seward, Urs Bergmann", "title": "Disentangling Multiple Conditional Inputs in GANs", "comments": "5 pages, 9 figures, Paper is accepted to the workshop \"AI for\n  Fashion\" in KDD Conference, 2018, London, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method that disentangles the effects of multiple\ninput conditions in Generative Adversarial Networks (GANs). In particular, we\ndemonstrate our method in controlling color, texture, and shape of a generated\ngarment image for computer-aided fashion design. To disentangle the effect of\ninput attributes, we customize conditional GANs with consistency loss\nfunctions. In our experiments, we tune one input at a time and show that we can\nguide our network to generate novel and realistic images of clothing articles.\nIn addition, we present a fashion design process that estimates the input\nattributes of an existing garment and modifies them using our generator.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:15:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Yildirim", "G\u00f6khan", ""], ["Seward", "Calvin", ""], ["Bergmann", "Urs", ""]]}, {"id": "1806.07822", "submitter": "Tanmay Shankar", "authors": "Tanmay Shankar, Nicholas Rhinehart, Katharina Muelling, Kris M. Kitani", "title": "Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning", "comments": "Accepted to Conference on Robot Learning, CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:15:54 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 14:58:04 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Shankar", "Tanmay", ""], ["Rhinehart", "Nicholas", ""], ["Muelling", "Katharina", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1806.07846", "submitter": "Liangzhen Lai", "authors": "Liangzhen Lai, Naveen Suda", "title": "Rethinking Machine Learning Development and Deployment for Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML), especially deep learning is made possible by the\navailability of big data, enormous compute power and, often overlooked,\ndevelopment tools or frameworks. As the algorithms become mature and efficient,\nmore and more ML inference is moving out of datacenters/cloud and deployed on\nedge devices. This model deployment process can be challenging as the\ndeployment environment and requirements can be substantially different from\nthose during model development. In this paper, we propose a new ML development\nand deployment approach that is specially designed and optimized for\ninference-only deployment on edge devices. We build a prototype and demonstrate\nthat this approach can address all the deployment challenges and result in more\nefficient and high-quality solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:11:54 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lai", "Liangzhen", ""], ["Suda", "Naveen", ""]]}, {"id": "1806.07857", "submitter": "Jose A. Arjona-Medina", "authors": "Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas\n  Unterthiner, Johannes Brandstetter, Sepp Hochreiter", "title": "RUDDER: Return Decomposition for Delayed Rewards", "comments": "9 Pages plus appendix. For videos https://goo.gl/EQerZV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RUDDER, a novel reinforcement learning approach for delayed\nrewards in finite Markov decision processes (MDPs). In MDPs the Q-values are\nequal to the expected immediate reward plus the expected future rewards. The\nlatter are related to bias problems in temporal difference (TD) learning and to\nhigh variance problems in Monte Carlo (MC) learning. Both problems are even\nmore severe when rewards are delayed. RUDDER aims at making the expected future\nrewards zero, which simplifies Q-value estimation to computing the mean of the\nimmediate reward. We propose the following two new concepts to push the\nexpected future rewards toward zero. (i) Reward redistribution that leads to\nreturn-equivalent decision processes with the same optimal policies and, when\noptimal, zero expected future rewards. (ii) Return decomposition via\ncontribution analysis which transforms the reinforcement learning task into a\nregression task at which deep learning excels. On artificial tasks with delayed\nrewards, RUDDER is significantly faster than MC and exponentially faster than\nMonte Carlo Tree Search (MCTS), TD({\\lambda}), and reward shaping approaches.\nAt Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline\nimproves the scores, which is most prominent at games with delayed rewards.\nSource code is available at \\url{https://github.com/ml-jku/rudder} and\ndemonstration videos at \\url{https://goo.gl/EQerZV}.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:34:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:45:22 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 16:27:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Arjona-Medina", "Jose A.", ""], ["Gillhofer", "Michael", ""], ["Widrich", "Michael", ""], ["Unterthiner", "Thomas", ""], ["Brandstetter", "Johannes", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1806.07863", "submitter": "Gauri Jagatap", "authors": "Gauri Jagatap and Chinmay Hegde", "title": "Learning ReLU Networks via Alternating Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new family of algorithms for training neural\nnetworks with ReLU activations. Our algorithms are based on the technique of\nalternating minimization: estimating the activation patterns of each ReLU for\nall given samples, interleaved with weight updates via a least-squares step.\nThe main focus of our paper are 1-hidden layer networks with $k$ hidden neurons\nand ReLU activation. We show that under standard distributional assumptions on\nthe $d-$dimensional input data, our algorithm provably recovers the true\n`ground truth' parameters in a linearly convergent fashion. This holds as long\nas the weights are sufficiently well initialized; furthermore, our method\nrequires only $n=\\widetilde{O}(dk^2)$ samples. We also analyze the special case\nof 1-hidden layer networks with skipped connections, commonly used in\nResNet-type architectures, and propose a novel initialization strategy for the\nsame. For ReLU based ResNet type networks, we provide the first linear\nconvergence guarantee with an end-to-end algorithm. We also extend this\nframework to deeper networks and empirically demonstrate its convergence to a\nglobal minimum.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:43:38 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 19:26:03 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Jagatap", "Gauri", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1806.07870", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, George Michailidis, Yves Atchade", "title": "Sequential change-point detection in high-dimensional Gaussian graphical\n  models", "comments": "47 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional piecewise stationary graphical models represent a versatile\nclass for modelling time varying networks arising in diverse application areas,\nincluding biology, economics, and social sciences. There has been recent work\nin offline detection and estimation of regime changes in the topology of sparse\ngraphical models. However, the online setting remains largely unexplored,\ndespite its high relevance to applications in sensor networks and other\nengineering monitoring systems, as well as financial markets. To that end, this\nwork introduces a novel scalable online algorithm for detecting an unknown\nnumber of abrupt changes in the inverse covariance matrix of sparse Gaussian\ngraphical models with small delay. The proposed algorithm is based upon\nmonitoring the conditional log-likelihood of all nodes in the network and can\nbe extended to a large class of continuous and discrete graphical models. We\nalso investigate asymptotic properties of our procedure under certain mild\nregularity conditions on the graph size, sparsity level, number of samples, and\npre- and post-changes in the topology of the network. Numerical works on both\nsynthetic and real data illustrate the good performance of the proposed\nmethodology both in terms of computational and statistical efficiency across\nnumerous experimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:54:49 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Michailidis", "George", ""], ["Atchade", "Yves", ""]]}, {"id": "1806.07908", "submitter": "Moacir Antonelli Ponti", "authors": "Moacir Antonelli Ponti and Gabriel B. Paranhos da Costa", "title": "Como funciona o Deep Learning", "comments": "Book chapter, in Portuguese, 31 pages", "journal-ref": "In: T\\'opicos em Gerenciamento de Dados e Informa\\c{c}\\~oes, SBC,\n  Cap.3, ISBN 978-85-7669-400-7, pp.63-93, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning methods are currently the state-of-the-art in many problems\nwhich can be tackled via machine learning, in particular classification\nproblems. However there is still lack of understanding on how those methods\nwork, why they work and what are the limitations involved in using them. In\nthis chapter we will describe in detail the transition from shallow to deep\nnetworks, include examples of code on how to implement them, as well as the\nmain issues one faces when training a deep network. Afterwards, we introduce\nsome theoretical background behind the use of deep models, and discuss their\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:04:09 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ponti", "Moacir Antonelli", ""], ["da Costa", "Gabriel B. Paranhos", ""]]}, {"id": "1806.07937", "submitter": "Amy Zhang", "authors": "Amy Zhang, Nicolas Ballas, Joelle Pineau", "title": "A Dissection of Overfitting and Generalization in Continuous\n  Reinforcement Learning", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risks and perils of overfitting in machine learning are well known.\nHowever most of the treatment of this, including diagnostic tools and remedies,\nwas developed for the supervised learning case. In this work, we aim to offer\nnew perspectives on the characterization and prevention of overfitting in deep\nReinforcement Learning (RL) methods, with a particular focus on continuous\ndomains. We examine several aspects, such as how to define and diagnose\noverfitting in MDPs, and how to reduce risks by injecting sufficient training\ndiversity. This work complements recent findings on the brittleness of deep RL\nmethods and offers practical observations for RL researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 19:27:59 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:09:04 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Amy", ""], ["Ballas", "Nicolas", ""], ["Pineau", "Joelle", ""]]}, {"id": "1806.07944", "submitter": "Avik Ray", "authors": "Avik Ray, Sujay Sanghavi, Sanjay Shakkottai", "title": "Searching for a Single Community in a Graph", "comments": "ACM Journal on Modeling and Performance Evaluation of Computing\n  Systems (TOMPECS) [to appear]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard graph clustering/community detection, one is interested in\npartitioning the graph into more densely connected subsets of nodes. In\ncontrast, the \"search\" problem of this paper aims to only find the nodes in a\n\"single\" such community, the target, out of the many communities that may\nexist. To do so , we are given suitable side information about the target; for\nexample, a very small number of nodes from the target are labeled as such.\n  We consider a general yet simple notion of side information: all nodes are\nassumed to have random weights, with nodes in the target having higher weights\non average. Given these weights and the graph, we develop a variant of the\nmethod of moments that identifies nodes in the target more reliably, and with\nlower computation, than generic community detection methods that do not use\nside information and partition the entire graph. Our empirical results show\nsignificant gains in runtime, and also gains in accuracy over other graph\nclustering algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:55:14 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ray", "Avik", ""], ["Sanghavi", "Sujay", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1806.07956", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Reconstructing networks with unknown and heterogeneous errors", "comments": "27 pages, 17 figures", "journal-ref": "Phys. Rev. X 8, 041011 (2018)", "doi": "10.1103/PhysRevX.8.041011", "report-no": null, "categories": "cs.SI cs.LG physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The vast majority of network datasets contains errors and omissions, although\nthis is rarely incorporated in traditional network analysis. Recently, an\nincreasing effort has been made to fill this methodological gap by developing\nnetwork reconstruction approaches based on Bayesian inference. These\napproaches, however, rely on assumptions of uniform error rates and on direct\nestimations of the existence of each edge via repeated measurements, something\nthat is currently unavailable for the majority of network data. Here we develop\na Bayesian reconstruction approach that lifts these limitations by not only\nallowing for heterogeneous errors, but also for single edge measurements\nwithout direct error estimates. Our approach works by coupling the inference\napproach with structured generative network models, which enable the\ncorrelations between edges to be used as reliable uncertainty estimates.\nAlthough our approach is general, we focus on the stochastic block model as the\nbasic generative process, from which efficient nonparametric inference can be\nperformed, and yields a principled method to infer hierarchical community\nstructure from noisy data. We demonstrate the efficacy of our approach with a\nvariety of empirical and artificial networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 14:09:25 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 09:53:46 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 07:23:02 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1806.07963", "submitter": "Hafiz Tiomoko Ali", "authors": "Hafiz Tiomoko Ali, Sijia Liu, Yasin Yilmaz, Romain Couillet, Indika\n  Rajapakse, Alfred Hero", "title": "Latent heterogeneous multilayer community detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for simultaneously detecting shared and unshared\ncommunities in heterogeneous multilayer weighted and undirected networks. The\nmultilayer network is assumed to follow a generative probabilistic model that\ntakes into account the similarities and dissimilarities between the\ncommunities. We make use of a variational Bayes approach for jointly inferring\nthe shared and unshared hidden communities from multilayer network\nobservations. We show that our approach outperforms state-of-the-art algorithms\nin detecting disparate (shared and private) communities on synthetic data as\nwell as on real genome-wide fibroblast proliferation dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 18:02:11 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 16:38:28 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ali", "Hafiz Tiomoko", ""], ["Liu", "Sijia", ""], ["Yilmaz", "Yasin", ""], ["Couillet", "Romain", ""], ["Rajapakse", "Indika", ""], ["Hero", "Alfred", ""]]}, {"id": "1806.07978", "submitter": "Tobias Eichinger", "authors": "Tobias Eichinger", "title": "The Corpus Replication Task", "comments": "the references might not render appropriately. contact the author for\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the field of Natural Language Processing (NLP), we revisit the well-known\nword embedding algorithm word2vec. Word embeddings identify words by vectors\nsuch that the words' distributional similarity is captured. Unexpectedly,\nbesides semantic similarity even relational similarity has been shown to be\ncaptured in word embeddings generated by word2vec, whence two questions arise.\nFirstly, which kind of relations are representable in continuous space and\nsecondly, how are relations built. In order to tackle these questions we\npropose a bottom-up point of view. We call generating input text for which\nword2vec outputs target relations solving the Corpus Replication Task. Deeming\ngeneralizations of this approach to any set of relations possible, we expect\nsolving of the Corpus Replication Task to provide partial answers to the\nquestions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 20:37:28 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Eichinger", "Tobias", ""]]}, {"id": "1806.08010", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto and Megha Srivastava and Hongseok Namkoong and\n  Percy Liang", "title": "Fairness Without Demographics in Repeated Loss Minimization", "comments": "Final version for ICML2018, corrects typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models (e.g., speech recognizers) are usually trained to\nminimize average loss, which results in representation disparity---minority\ngroups (e.g., non-native speakers) contribute less to the training objective\nand thus tend to suffer higher loss. Worse, as model accuracy affects user\nretention, a minority group can shrink over time. In this paper, we first show\nthat the status quo of empirical risk minimization (ERM) amplifies\nrepresentation disparity over time, which can even make initially fair models\nunfair. To mitigate this, we develop an approach based on distributionally\nrobust optimization (DRO), which minimizes the worst case risk over all\ndistributions close to the empirical distribution. We prove that this approach\ncontrols the risk of the minority group at each time step, in the spirit of\nRawlsian distributive justice, while remaining oblivious to the identity of the\ngroups. We demonstrate that DRO prevents disparity amplification on examples\nwhere ERM fails, and show improvements in minority group user satisfaction in a\nreal-world text autocomplete task.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 22:17:08 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 19:48:40 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Srivastava", "Megha", ""], ["Namkoong", "Hongseok", ""], ["Liang", "Percy", ""]]}, {"id": "1806.08028", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, Zhao Chen, Vijay Badrinarayanan and Andrew Rabinovich", "title": "Gradient Adversarial Training of Neural Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose gradient adversarial training, an auxiliary deep learning\nframework applicable to different machine learning problems. In gradient\nadversarial training, we leverage a prior belief that in many contexts,\nsimultaneous gradient updates should be statistically indistinguishable from\neach other. We enforce this consistency using an auxiliary network that\nclassifies the origin of the gradient tensor, and the main network serves as an\nadversary to the auxiliary network in addition to performing standard\ntask-based training. We demonstrate gradient adversarial training for three\ndifferent scenarios: (1) as a defense to adversarial examples we classify\ngradient tensors and tune them to be agnostic to the class of their\ncorresponding example, (2) for knowledge distillation, we do binary\nclassification of gradient tensors derived from the student or teacher network\nand tune the student gradient tensor to mimic the teacher's gradient tensor;\nand (3) for multi-task learning we classify the gradient tensors derived from\ndifferent task loss functions and tune them to be statistically\nindistinguishable. For each of the three scenarios we show the potential of\ngradient adversarial training procedure. Specifically, gradient adversarial\ntraining increases the robustness of a network to adversarial attacks, is able\nto better distill the knowledge from a teacher network to a student network\ncompared to soft targets, and boosts multi-task learning by aligning the\ngradient tensors derived from the task specific loss functions. Overall, our\nexperiments demonstrate that gradient tensors contain latent information about\nwhatever tasks are being trained, and can support diverse machine learning\nproblems when intelligently guided through adversarialization using a auxiliary\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 00:54:07 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Sinha", "Ayan", ""], ["Chen", "Zhao", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1806.08049", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis and Tommi S. Jaakkola", "title": "On the Robustness of Interpretability Methods", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that robustness of explanations---i.e., that similar inputs should\ngive rise to similar explanations---is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 02:33:44 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1806.08065", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Christopher MacLellan, Ruslan Salakhutdinov,\n  Kenneth Koedinger", "title": "Learning Cognitive Models using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cognitive model of human learning provides information about skills a\nlearner must acquire to perform accurately in a task domain. Cognitive models\nof learning are not only of scientific interest, but are also valuable in\nadaptive online tutoring systems. A more accurate model yields more effective\ntutoring through better instructional decisions. Prior methods of automated\ncognitive model discovery have typically focused on well-structured domains,\nrelied on student performance data or involved substantial human knowledge\nengineering. In this paper, we propose Cognitive Representation Learner\n(CogRL), a novel framework to learn accurate cognitive models in ill-structured\ndomains with no data and little to no human knowledge engineering. Our\ncontribution is two-fold: firstly, we show that representations learnt using\nCogRL can be used for accurate automatic cognitive model discovery without\nusing any student performance data in several ill-structured domains: Rumble\nBlocks, Chinese Character, and Article Selection. This is especially effective\nand useful in domains where an accurate human-authored cognitive model is\nunavailable or authoring a cognitive model is difficult. Secondly, for domains\nwhere a cognitive model is available, we show that representations learned\nthrough CogRL can be used to get accurate estimates of skill difficulty and\nlearning rate parameters without using any student performance data. These\nestimates are shown to highly correlate with estimates using student\nperformance data on an Article Selection dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 04:43:35 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["MacLellan", "Christopher", ""], ["Salakhutdinov", "Ruslan", ""], ["Koedinger", "Kenneth", ""]]}, {"id": "1806.08079", "submitter": "Manqing Dong", "authors": "Manqing Dong, Lina Yao, Xianzhi Wang, Boualem Benatallah and Shuai\n  Zhang", "title": "GrCAN: Gradient Boost Convolutional Autoencoder with Neural Decision\n  Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest and deep neural network are two schools of effective\nclassification methods in machine learning. While the random forest is robust\nirrespective of the data domain, the deep neural network has advantages in\nhandling high dimensional data. In view that a differentiable neural decision\nforest can be added to the neural network to fully exploit the benefits of both\nmodels, in our work, we further combine convolutional autoencoder with neural\ndecision forest, where autoencoder has its advantages in finding the hidden\nrepresentations of the input data. We develop a gradient boost module and embed\nit into the proposed convolutional autoencoder with neural decision forest to\nimprove the performance. The idea of gradient boost is to learn and use the\nresidual in the prediction. In addition, we design a structure to learn the\nparameters of the neural decision forest and gradient boost module at\ncontiguous steps. The extensive experiments on several public datasets\ndemonstrate that our proposed model achieves good efficiency and prediction\nperformance compared with a series of baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 06:35:00 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 12:30:42 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Dong", "Manqing", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Benatallah", "Boualem", ""], ["Zhang", "Shuai", ""]]}, {"id": "1806.08117", "submitter": "Constantin Grigo", "authors": "Constantin Grigo, Phaedon-Stelios Koutsourelakis", "title": "A data-driven model order reduction approach for Stokes flow through\n  random porous media", "comments": "2 pages, 2 figures", "journal-ref": "PAMM Proc. Appl. Math. Mech.2018;18:e201800314", "doi": "10.1002/pamm.201800314", "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct numerical simulation of Stokes flow through an impermeable, rigid body\nmatrix by finite elements requires meshes fine enough to resolve the pore-size\nscale and is thus a computationally expensive task. The cost is significantly\namplified when randomness in the pore microstructure is present and therefore\nmultiple simulations need to be carried out. It is well known that in the limit\nof scale-separation, Stokes flow can be accurately approximated by Darcy's law\nwith an effective diffusivity field depending on viscosity and the pore-matrix\ntopology. We propose a fully probabilistic, Darcy-type, reduced-order model\nwhich, based on only a few tens of full-order Stokes model runs, is capable of\nlearning a map from the fine-scale topology to the effective diffusivity and is\nmaximally predictive of the fine-scale response. The reduced-order model\nlearned can significantly accelerate uncertainty quantification tasks as well\nas provide quantitative confidence metrics of the predictive estimates\nproduced.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:53:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Grigo", "Constantin", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1806.08141", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Antoine Liutkus, Umut \\c{S}im\\c{s}ekli, Szymon Majewski, Alain Durmus,\n  Fabian-Robert St\\\"oter", "title": "Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal\n  Transport and Diffusions", "comments": "Published at the International Conference on Machine Learning (ICML)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By building upon the recent theory that established the connection between\nimplicit generative modeling (IGM) and optimal transport, in this study, we\npropose a novel parameter-free algorithm for learning the underlying\ndistributions of complicated datasets and sampling from them. The proposed\nalgorithm is based on a functional optimization problem, which aims at finding\na measure that is close to the data distribution as much as possible and also\nexpressive enough for generative modeling purposes. We formulate the problem as\na gradient flow in the space of probability measures. The connections between\ngradient flows and stochastic differential equations let us develop a\ncomputationally efficient algorithm for solving the optimization problem. We\nprovide formal theoretical analysis where we prove finite-time error guarantees\nfor the proposed algorithm. To the best of our knowledge, the proposed\nalgorithm is the first nonparametric IGM algorithm with explicit theoretical\nguarantees. Our experimental results support our theory and show that our\nalgorithm is able to successfully capture the structure of different types of\ndata distributions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:44:20 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 16:39:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liutkus", "Antoine", ""], ["\u015eim\u015fekli", "Umut", ""], ["Majewski", "Szymon", ""], ["Durmus", "Alain", ""], ["St\u00f6ter", "Fabian-Robert", ""]]}, {"id": "1806.08151", "submitter": "Xin Dang", "authors": "Zhi Xiao, Zhe Luo, Bo Zhong, and Xin Dang", "title": "Robust and Efficient Boosting Method using the Conditional Risk", "comments": "14 Pages, 2 figures and 5 tables", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2711028", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-known for its simplicity and effectiveness in classification, AdaBoost,\nhowever, suffers from overfitting when class-conditional distributions have\nsignificant overlap. Moreover, it is very sensitive to noise that appears in\nthe labels. This article tackles the above limitations simultaneously via\noptimizing a modified loss function (i.e., the conditional risk). The proposed\napproach has the following two advantages. (1) It is able to directly take into\naccount label uncertainty with an associated label confidence. (2) It\nintroduces a \"trustworthiness\" measure on training samples via the Bayesian\nrisk rule, and hence the resulting classifier tends to have finite sample\nperformance that is superior to that of the original AdaBoost when there is a\nlarge overlap between class conditional distributions. Theoretical properties\nof the proposed method are investigated. Extensive experimental results using\nsynthetic data and real-world data sets from UCI machine learning repository\nare provided. The empirical study shows the high competitiveness of the\nproposed method in predication accuracy and robustness when compared with the\noriginal AdaBoost and several existing robust AdaBoost algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 10:07:12 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Xiao", "Zhi", ""], ["Luo", "Zhe", ""], ["Zhong", "Bo", ""], ["Dang", "Xin", ""]]}, {"id": "1806.08156", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Identifiability of Gaussian Structural Equation Models with Dependent\n  Errors Having Equal Variances", "comments": "7th Causal Inference Workshop at UAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that some Gaussian structural equation models with\ndependent errors having equal variances are identifiable from their\ncorresponding Gaussian distributions. Specifically, we prove identifiability\nfor the Gaussian structural equation models that can be represented as\nAndersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain\ngraphs were originally developed to represent independence models. However,\nthey are also suitable for representing causal models with additive noise\n(Pe\\~na, 2016. Our result implies then that these causal models can be\nidentified from observational data alone. Our result generalizes the result by\nPeters and B\\\"uhlmann (2014), who considered independent errors having equal\nvariances. The suitability of the equal error variances assumption should be\nassessed on a per domain basis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 10:23:23 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 07:43:09 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 07:35:23 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 20:33:50 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1806.08195", "submitter": "Philip J{\\o}rgensen", "authors": "Philip J. H. J{\\o}rgensen, S{\\o}ren F. V. Nielsen, Jesper L. Hinrich,\n  Mikkel N. Schmidt, Kristoffer H. Madsen, Morten M{\\o}rup", "title": "Probabilistic PARAFAC2", "comments": "16 pages (incl. 4 pages of supplemental material), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PARAFAC2 is a multimodal factor analysis model suitable for analyzing\nmulti-way data when one of the modes has incomparable observation units, for\nexample because of differences in signal sampling or batch sizes. A fully\nprobabilistic treatment of the PARAFAC2 is desirable in order to improve\nrobustness to noise and provide a well founded principle for determining the\nnumber of factors, but challenging because the factor loadings are constrained\nto be orthogonal. We develop two probabilistic formulations of the PARAFAC2\nalong with variational procedures for inference: In the one approach, the mean\nvalues of the factor loadings are orthogonal leading to closed form variational\nupdates, and in the other, the factor loadings themselves are orthogonal using\na matrix Von Mises-Fisher distribution. We contrast our probabilistic\nformulation to the conventional direct fitting algorithm based on maximum\nlikelihood. On simulated data and real fluorescence spectroscopy and gas\nchromatography-mass spectrometry data, we compare our approach to the\nconventional PARAFAC2 model estimation and find that the probabilistic\nformulation is more robust to noise and model order misspecification. The\nprobabilistic PARAFAC2 thus forms a promising framework for modeling multi-way\ndata accounting for uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:22:07 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["J\u00f8rgensen", "Philip J. H.", ""], ["Nielsen", "S\u00f8ren F. V.", ""], ["Hinrich", "Jesper L.", ""], ["Schmidt", "Mikkel N.", ""], ["Madsen", "Kristoffer H.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1806.08212", "submitter": "George Panagopoulos", "authors": "George Panagopoulos", "title": "A Review of Network Inference Techniques for Neural Activation Time\n  Series", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying neural connectivity is considered one of the most promising and\nchallenging areas of modern neuroscience. The underpinnings of cognition are\nhidden in the way neurons interact with each other. However, our experimental\nmethods of studying real neural connections at a microscopic level are still\narduous and costly. An efficient alternative is to infer connectivity based on\nthe neuronal activations using computational methods. A reliable method for\nnetwork inference, would not only facilitate research of neural circuits\nwithout the need of laborious experiments but also reveal insights on the\nunderlying mechanisms of the brain. In this work, we perform a review of\nmethods for neural circuit inference given the activation time series of the\nneural population. Approaching it from machine learning perspective, we divide\nthe methodologies into unsupervised and supervised learning. The methods are\nbased on correlation metrics, probabilistic point processes, and neural\nnetworks. Furthermore, we add a data mining methodology inspired by influence\nestimation in social networks as a new supervised learning approach. For\ncomparison, we use the small version of the Chalearn Connectomics competition,\nthat is accompanied with ground truth connections between neurons. The\nexperiments indicate that unsupervised learning methods perform better,\nhowever, supervised methods could surpass them given enough data and resources.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:26:52 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Panagopoulos", "George", ""]]}, {"id": "1806.08235", "submitter": "Omid Kavehei", "authors": "Nhan Duy Truong and Levin Kuhlmann and Mohammad Reza Bonyadi and Omid\n  Kavehei", "title": "Semi-supervised Seizure Prediction with Generative Adversarial Networks", "comments": "6 pages, 5 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1707.01976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:47:57 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Truong", "Nhan Duy", ""], ["Kuhlmann", "Levin", ""], ["Bonyadi", "Mohammad Reza", ""], ["Kavehei", "Omid", ""]]}, {"id": "1806.08240", "submitter": "Edouard Pineau", "authors": "Edouard Pineau, Marc Lelarge", "title": "InfoCatVAE: Representation Learning with Categorical Variational\n  Autoencoders", "comments": "9 pages, 3 appendix, 5 figures. arXiv admin note: text overlap with\n  arXiv:1606.03657 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes InfoCatVAE, an extension of the variational autoencoder\nthat enables unsupervised disentangled representation learning. InfoCatVAE uses\nmultimodal distributions for the prior and the inference network and then\nmaximizes the evidence lower bound objective (ELBO). We connect the new ELBO\nderived for our model with a natural soft clustering objective which explains\nthe robustness of our approach. We then adapt the InfoGANs method to our\nsetting in order to maximize the mutual information between the categorical\ncode and the generated inputs and obtain an improved model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:56:08 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 14:58:24 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Pineau", "Edouard", ""], ["Lelarge", "Marc", ""]]}, {"id": "1806.08267", "submitter": "Moritz Wolter", "authors": "Moritz Wolter and Angela Yao", "title": "Complex Gated Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex numbers have long been favoured for digital signal processing, yet\ncomplex representations rarely appear in deep learning architectures. RNNs,\nwidely used to process time series and sequence information, could greatly\nbenefit from complex representations. We present a novel complex gated\nrecurrent cell, which is a hybrid cell combining complex-valued and\nnorm-preserving state transitions with a gating mechanism. The resulting RNN\nexhibits excellent stability and convergence properties and performs\ncompetitively on the synthetic memory and adding task, as well as on the\nreal-world tasks of human motion prediction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 14:32:35 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 18:48:54 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Wolter", "Moritz", ""], ["Yao", "Angela", ""]]}, {"id": "1806.08295", "submitter": "Olivier Sigaud", "authors": "C\\'edric Colas and Olivier Sigaud and Pierre-Yves Oudeyer", "title": "How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement\n  Learning Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistently checking the statistical significance of experimental results is\none of the mandatory methodological steps to address the so-called\n\"reproducibility crisis\" in deep reinforcement learning. In this tutorial\npaper, we explain how the number of random seeds relates to the probabilities\nof statistical errors. For both the t-test and the bootstrap confidence\ninterval test, we recall theoretical guidelines to determine the number of\nrandom seeds one should use to provide a statistically significant comparison\nof the performance of two algorithms. Finally, we discuss the influence of\ndeviations from the assumptions usually made by statistical tests. We show that\nthey can lead to inaccurate evaluations of statistical errors and provide\nguidelines to counter these negative effects. We make our code available to\nperform the tests.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:39:19 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 06:50:33 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Colas", "C\u00e9dric", ""], ["Sigaud", "Olivier", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1806.08297", "submitter": "Philip Amortila", "authors": "Philip Amortila and Guillaume Rabusseau", "title": "Learning Graph Weighted Models on Pictures", "comments": "International Conference on Grammatical Inference 2018 (v2:\n  camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Weighted Models (GWMs) have recently been proposed as a natural\ngeneralization of weighted automata over strings and trees to arbitrary\nfamilies of labeled graphs (and hypergraphs). A GWM generically associates a\nlabeled graph with a tensor network and computes a value by successive\ncontractions directed by its edges. In this paper, we consider the problem of\nlearning GWMs defined over the graph family of pictures (or 2-dimensional\nwords). As a proof of concept, we consider regression and classification tasks\nover the simple Bars & Stripes and Shifting Bits picture languages and provide\nan experimental study investigating whether these languages can be learned in\nthe form of a GWM from positive and negative examples using gradient-based\nmethods. Our results suggest that this is indeed possible and that\ninvestigating the use of gradient-based methods to learn picture series and\nfunctions computed by GWMs over other families of graphs could be a fruitful\ndirection.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:44:51 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 17:38:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Amortila", "Philip", ""], ["Rabusseau", "Guillaume", ""]]}, {"id": "1806.08301", "submitter": "Adrian Rivera Cardoso", "authors": "Adrian Rivera, He Wang, Huan Xu", "title": "The Online Saddle Point Problem and Online Convex Optimization with\n  Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online saddle point problem, an online learning problem where at\neach iteration a pair of actions need to be chosen without knowledge of the\ncurrent and future (convex-concave) payoff functions. The objective is to\nminimize the gap between the cumulative payoffs and the saddle point value of\nthe aggregate payoff function, which we measure using a metric called\n\"SP-Regret\". The problem generalizes the online convex optimization framework\nbut here we must ensure both players incur cumulative payoffs close to that of\nthe Nash equilibrium of the sum of the games. We propose an algorithm that\nachieves SP-Regret proportional to $\\sqrt{\\ln(T)T}$ in the general case, and\n$\\log(T)$ SP-Regret for the strongly convex-concave case. We also consider the\nspecial case where the payoff functions are bilinear and the decision sets are\nthe probability simplex. In this setting we are able to design algorithms that\nreduce the bounds on SP-Regret from a linear dependence in the dimension of the\nproblem to a \\textit{logarithmic} one. We also study the problem under bandit\nfeedback and provide an algorithm that achieves sublinear SP-Regret. We then\nconsider an online convex optimization with knapsacks problem motivated by a\nwide variety of applications such as: dynamic pricing, auctions, and\ncrowdsourcing. We relate this problem to the online saddle point problem and\nestablish $O(\\sqrt{T})$ regret using a primal-dual algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:57:17 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 19:44:37 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 20:11:12 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rivera", "Adrian", ""], ["Wang", "He", ""], ["Xu", "Huan", ""]]}, {"id": "1806.08317", "submitter": "Negar Rostamzadeh", "authors": "Negar Rostamzadeh, Seyedarian Hosseini, Thomas Boquet, Wojciech\n  Stokowiec, Ying Zhang, Christian Jauvin, Chris Pal", "title": "Fashion-Gen: The Generative Fashion Dataset and Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels)\nfashion images paired with item descriptions provided by professional stylists.\nEach item is photographed from a variety of angles. We provide baseline results\non 1) high-resolution image generation, and 2) image generation conditioned on\nthe given text descriptions. We invite the community to improve upon these\nbaselines. In this paper, we also outline the details of a challenge that we\nare launching based upon this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 16:53:02 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 16:12:32 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Rostamzadeh", "Negar", ""], ["Hosseini", "Seyedarian", ""], ["Boquet", "Thomas", ""], ["Stokowiec", "Wojciech", ""], ["Zhang", "Ying", ""], ["Jauvin", "Christian", ""], ["Pal", "Chris", ""]]}, {"id": "1806.08324", "submitter": "Anand Avati", "authors": "Anand Avati, Tony Duan, Sharon Zhou, Kenneth Jung, Nigam H. Shah and\n  Andrew Ng", "title": "Countdown Regression: Sharp and Calibrated Survival Predictions", "comments": "UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic survival predictions from models trained with Maximum\nLikelihood Estimation (MLE) can have high, and sometimes unacceptably high\nvariance. The field of meteorology, where the paradigm of maximizing sharpness\nsubject to calibration is popular, has addressed this problem by using scoring\nrules beyond MLE, such as the Continuous Ranked Probability Score (CRPS). In\nthis paper we present the \\emph{Survival-CRPS}, a generalization of the CRPS to\nthe survival prediction setting, with right-censored and interval-censored\nvariants. We evaluate our ideas on the mortality prediction task using two\ndifferent Electronic Health Record (EHR) data sets (STARR and MIMIC-III)\ncovering millions of patients, with suitable deep neural network architectures:\na Recurrent Neural Network (RNN) for STARR and a Fully Connected Network (FCN)\nfor MIMIC-III. We compare results between the two scoring rules while keeping\nthe network architecture and data fixed, and show that models trained with\nSurvival-CRPS result in sharper predictive distributions compared to those\ntrained by MLE, while still maintaining calibration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:12:10 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 23:12:23 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Avati", "Anand", ""], ["Duan", "Tony", ""], ["Zhou", "Sharon", ""], ["Jung", "Kenneth", ""], ["Shah", "Nigam H.", ""], ["Ng", "Andrew", ""]]}, {"id": "1806.08340", "submitter": "Kiri Wagstaff", "authors": "Kiri L. Wagstaff and Jake Lee", "title": "Interpretable Discovery in Large Image Data Sets", "comments": "Presented at the 2018 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of new, interesting, unusual, or anomalous images within\nlarge data sets has great value for applications from surveillance (e.g.,\nairport security) to science (observations that don't fit a given theory can\nlead to new discoveries). Many image data analysis systems are turning to\nconvolutional neural networks (CNNs) to represent image content due to their\nsuccess in achieving high classification accuracy rates. However, CNN\nrepresentations are notoriously difficult for humans to interpret. We describe\na new strategy that combines novelty detection with CNN image features to\nachieve rapid discovery with interpretable explanations of novel image content.\nWe applied this technique to familiar images from ImageNet as well as to a\nscientific image collection from planetary science.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:30:26 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wagstaff", "Kiri L.", ""], ["Lee", "Jake", ""]]}, {"id": "1806.08342", "submitter": "Raghuraman Krishnamoorthi", "authors": "Raghuraman Krishnamoorthi", "title": "Quantizing deep convolutional networks for efficient inference: A\n  whitepaper", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an overview of techniques for quantizing convolutional neural\nnetworks for inference with integer weights and activations. Per-channel\nquantization of weights and per-layer quantization of activations to 8-bits of\nprecision post-training produces classification accuracies within 2% of\nfloating point networks for a wide variety of CNN architectures. Model sizes\ncan be reduced by a factor of 4 by quantizing weights to 8-bits, even when\n8-bit arithmetic is not supported. This can be achieved with simple, post\ntraining quantization of weights.We benchmark latencies of quantized networks\non CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations\ncompared to floating point on CPUs. Speedups of up to 10x are observed on\nspecialized processors with fixed point SIMD capabilities, like the Qualcomm\nQDSPs with HVX.\n  Quantization-aware training can provide further improvements, reducing the\ngap to floating point to 1% at 8-bit precision. Quantization-aware training\nalso allows for reducing the precision of weights to four bits with accuracy\nlosses ranging from 2% to 10%, with higher accuracy drop for smaller\nnetworks.We introduce tools in TensorFlow and TensorFlowLite for quantizing\nconvolutional networks and review best practices for quantization-aware\ntraining to obtain high accuracy with quantized weights and activations. We\nrecommend that per-channel quantization of weights and per-layer quantization\nof activations be the preferred quantization scheme for hardware acceleration\nand kernel optimization. We also propose that future processors and hardware\naccelerators for optimized inference support precisions of 4, 8 and 16 bits.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:32:46 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Krishnamoorthi", "Raghuraman", ""]]}, {"id": "1806.08354", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Yide Shentu, Dian Chen, Pulkit Agrawal, Trevor Darrell,\n  Sergey Levine, Jitendra Malik", "title": "Learning Instance Segmentation by Interaction", "comments": "Website at https://pathak22.github.io/seg-by-interaction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for building an active agent that learns to segment\nits visual observations into individual objects by interacting with its\nenvironment in a completely self-supervised manner. The agent uses its current\nsegmentation model to infer pixels that constitute objects and refines the\nsegmentation model by interacting with these pixels. The model learned from\nover 50K interactions generalizes to novel objects and backgrounds. To deal\nwith noisy training signal for segmenting objects obtained by self-supervised\ninteractions, we propose robust set loss. A dataset of robot's interactions\nalong-with a few human labeled examples is provided as a benchmark for future\nresearch. We test the utility of the learned segmentation model by providing\nresults on a downstream vision-based control task of rearranging multiple\nobjects into target configurations from visual inputs alone. Videos, code, and\nrobotic interaction dataset are available at\nhttps://pathak22.github.io/seg-by-interaction/\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:59:09 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Shentu", "Yide", ""], ["Chen", "Dian", ""], ["Agrawal", "Pulkit", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1806.08449", "submitter": "Hsiang Hsu", "authors": "Hsiang Hsu, Salman Salamatian, Flavio P. Calmon", "title": "Generalizing Correspondence Analysis for Applications in Machine\n  Learning", "comments": "30 pages, 7 figures, 6 tables. arXiv admin note: text overlap with\n  arXiv:1902.07828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence analysis (CA) is a multivariate statistical tool used to\nvisualize and interpret data dependencies by finding maximally correlated\nembeddings of pairs of random variables. CA has found applications in fields\nranging from epidemiology to social sciences; however, current methods do not\nscale to large, high-dimensional datasets. In this paper, we provide a novel\ninterpretation of CA in terms of an information-theoretic quantity called the\nprincipal inertia components. We show that estimating the principal inertia\ncomponents, which consists in solving a functional optimization problem over\nthe space of finite variance functions of two random variable, is equivalent to\nperforming CA. We then leverage this insight to design novel algorithms to\nperform CA at an unprecedented scale. Particularly, we demonstrate how the\nprincipal inertia components can be reliably approximated from data using deep\nneural networks. Finally, we show how these maximally correlated embeddings of\npairs of random variables in CA further play a central role in several learning\nproblems including visualization of classification boundary and training\nprocess, and underlying recent multi-view and multi-modal learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 23:12:45 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 21:33:50 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 20:38:38 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hsu", "Hsiang", ""], ["Salamatian", "Salman", ""], ["Calmon", "Flavio P.", ""]]}, {"id": "1806.08456", "submitter": "Xuekui Zhang", "authors": "Yan Xu, Li Xing, Jessica Su, Xuekui Zhang, Weiliang Qiu", "title": "Model-based clustering for identifying disease-associated SNPs in\n  case-control genome-wide association studies", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-019-50229-6", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWASs) aim to detect genetic risk factors\nfor complex human diseases by identifying disease-associated single-nucleotide\npolymorphisms (SNPs). The traditional SNP-wise approach along with multiple\ntesting adjustment is over-conservative and lack of power in many GWASs. In\nthis article, we proposed a model-based clustering method that transforms the\nchallenging high-dimension-small-sample-size problem to\nlow-dimension-large-sample-size problem and borrows information across SNPs by\ngrouping SNPs into three clusters. We pre-specify the patterns of clusters by\nminor allele frequencies of SNPs between cases and controls, and enforce the\npatterns with prior distributions. In the simulation studies our proposed novel\nmodel outperform traditional SNP-wise approach by showing better controls of\nfalse discovery rate (FDR) and higher sensitivity. We re-analyzed two real\nstudies to identifying SNPs associated with severe bortezomib-induced\nperipheral neuropathy (BiPN) in patients with multiple myeloma (MM). The\noriginal analysis in the literature failed to identify SNPs after FDR\nadjustment. Our proposed method not only detected the reported SNPs after FDR\nadjustment but also discovered a novel BiPN-associated SNP rs4351714 that has\nbeen reported to be related to MM in another study.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 23:43:03 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 08:06:28 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "Yan", ""], ["Xing", "Li", ""], ["Su", "Jessica", ""], ["Zhang", "Xuekui", ""], ["Qiu", "Weiliang", ""]]}, {"id": "1806.08462", "submitter": "Lili Mou", "authors": "Hareesh Bahuleyan, Lili Mou, Hao Zhou, Olga Vechtomova", "title": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation", "comments": "Accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) imposes a probabilistic distribution\n(typically Gaussian) on the latent space and penalizes the Kullback--Leibler\n(KL) divergence between the posterior and prior. In NLP, VAEs are extremely\ndifficult to train due to the problem of KL collapsing to zero. One has to\nimplement various heuristics such as KL weight annealing and word dropout in a\ncarefully engineered manner to successfully train a VAE for text. In this\npaper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic\nsentence generation, where the encoder could be either stochastic or\ndeterministic. We show theoretically and empirically that, in the original WAE,\nthe stochastically encoded Gaussian distribution tends to become a Dirac-delta\nfunction, and we propose a variant of WAE that encourages the stochasticity of\nthe encoder. Experimental results show that the latent space learned by WAE\nexhibits properties of continuity and smoothness as in VAEs, while\nsimultaneously achieving much higher BLEU scores for sentence reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 01:11:40 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 17:43:25 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Bahuleyan", "Hareesh", ""], ["Mou", "Lili", ""], ["Zhou", "Hao", ""], ["Vechtomova", "Olga", ""]]}, {"id": "1806.08541", "submitter": "Lin Guo", "authors": "Lin Guo, Hui Ye, Wenbo Su, Henhuan Liu, Kai Sun, Hang Xiang", "title": "Visualizing and Understanding Deep Neural Networks in CTR Prediction", "comments": "Accept by 2018 SIGIR Workshop on eCommerce", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning techniques have been successfully applied to many\ntasks, interpreting deep neural network models is still a big challenge to us.\nRecently, many works have been done on visualizing and analyzing the mechanism\nof deep neural networks in the areas of image processing and natural language\nprocessing. In this paper, we present our approaches to visualize and\nunderstand deep neural networks for a very important commercial task--CTR\n(Click-through rate) prediction. We conduct experiments on the productive data\nfrom our online advertising system with daily varying distribution. To\nunderstand the mechanism and the performance of the model, we inspect the\nmodel's inner status at neuron level. Also, a probe approach is implemented to\nmeasure the layer-wise performance of the model. Moreover, to measure the\ninfluence from the input features, we calculate saliency scores based on the\nback-propagated gradients. Practical applications are also discussed, for\nexample, in understanding, monitoring, diagnosing and refining models and\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 08:03:35 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Guo", "Lin", ""], ["Ye", "Hui", ""], ["Su", "Wenbo", ""], ["Liu", "Henhuan", ""], ["Sun", "Kai", ""], ["Xiang", "Hang", ""]]}, {"id": "1806.08568", "submitter": "Vincenzo Lomonaco", "authors": "Davide Maltoni and Vincenzo Lomonaco", "title": "Continuous Learning in Single-Incremental-Task Scenarios", "comments": "26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),\n  several typos and minor mistakes corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that architectural, regularization and rehearsal\nstrategies can be used to train deep models sequentially on a number of\ndisjoint tasks without forgetting previously acquired knowledge. However, these\nstrategies are still unsatisfactory if the tasks are not disjoint but\nconstitute a single incremental task (e.g., class-incremental learning). In\nthis paper we point out the differences between multi-task and\nsingle-incremental-task scenarios and show that well-known approaches such as\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\ndenoted as AR1, combining architectural and regularization strategies is then\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\nsmall thus making it suitable for online learning. When tested on CORe50 and\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:22:42 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 11:13:40 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 21:49:25 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Maltoni", "Davide", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "1806.08593", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison", "title": "Tensor Monte Carlo: particle methods for the GPU era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sample, importance-weighted variational autoencoders (IWAE) give\ntighter bounds and more accurate uncertainty estimates than variational\nautoencoders (VAE) trained with a standard single-sample objective. However,\nIWAEs scale poorly: as the latent dimensionality grows, they require\nexponentially many samples to retain the benefits of importance weighting.\nWhile sequential Monte-Carlo (SMC) can address this problem, it is\nprohibitively slow because the resampling step imposes sequential structure\nwhich cannot be parallelised, and moreover, resampling is non-differentiable\nwhich is problematic when learning approximate posteriors. To address these\nissues, we developed tensor Monte-Carlo (TMC) which gives exponentially many\nimportance samples by separately drawing $K$ samples for each of the $n$ latent\nvariables, then averaging over all $K^n$ possible combinations. While the sum\nover exponentially many terms might seem to be intractable, in many cases it\ncan be computed efficiently as a series of tensor inner-products. We show that\nTMC is superior to IWAE on a generative model with multiple stochastic layers\ntrained on the MNIST handwritten digit database, and we show that TMC can be\ncombined with standard variance reduction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 10:40:39 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 15:59:30 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 18:47:14 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Aitchison", "Laurence", ""]]}, {"id": "1806.08647", "submitter": "Somsubhra Barik", "authors": "Somsubhra Barik and Haris Vikalo", "title": "Matrix Completion and Performance Guarantees for Single Individual\n  Haplotyping", "comments": "28 pages, 3 pages, 3 Tables", "journal-ref": "IEEE Transactions on Signal Processing 2019", "doi": "10.1109/TSP.2019.2931207", "report-no": null, "categories": "cs.LG cs.IT math.IT q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single individual haplotyping is an NP-hard problem that emerges when\nattempting to reconstruct an organism's inherited genetic variations using data\ntypically generated by high-throughput DNA sequencing platforms. Genomes of\ndiploid organisms, including humans, are organized into homologous pairs of\nchromosomes that differ from each other in a relatively small number of variant\npositions. Haplotypes are ordered sequences of the nucleotides in the variant\npositions of the chromosomes in a homologous pair; for diploids, haplotypes\nassociated with a pair of chromosomes may be conveniently represented by means\nof complementary binary sequences. In this paper, we consider a binary matrix\nfactorization formulation of the single individual haplotyping problem and\nefficiently solve it by means of alternating minimization. We analyze the\nconvergence properties of the alternating minimization algorithm and establish\ntheoretical bounds for the achievable haplotype reconstruction error. The\nproposed technique is shown to outperform existing methods when applied to\nsynthetic as well as real-world Fosmid-based HapMap NA12878 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:50:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 06:59:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Barik", "Somsubhra", ""], ["Vikalo", "Haris", ""]]}, {"id": "1806.08648", "submitter": "Markus Pfeiffer", "authors": "Manuel Machado Martins and Markus Pfeiffer", "title": "Francy - An Interactive Discrete Mathematics Framework for GAP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization and interaction with large data sets is known to be\nessential and critical in many businesses today, and the same applies to\nresearch and teaching, in this case, when exploring large and complex\nmathematical objects. GAP is a computer algebra system for computational\ndiscrete algebra with an emphasis on computational group theory. The existing\nXGAP package for GAP works exclusively on the X Window System. It lacks\nabstraction between its mathematical and graphical cores, making it difficult\nto extend, maintain, or port. In this paper, we present Francy, a graphical\nsemantics package for GAP. Francy is responsible for creating a\nrepresentational structure that can be rendered using many GUI frameworks\nindependent from any particular programming language or operating system.\nBuilding on this, we use state of the art web technologies that take advantage\nof an improved REPL environment, which is currently under development for GAP.\nThe integration of this project with Jupyter provides a rich graphical\nenvironment full of features enhancing the usability and accessibility of GAP.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:58:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Martins", "Manuel Machado", ""], ["Pfeiffer", "Markus", ""]]}, {"id": "1806.08672", "submitter": "Rita Kuznetsova", "authors": "Rita Kuznetsova, Oleg Bakhteev, Alexandr Ogaltsov", "title": "Variational learning across domains with triplet information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work investigates deep generative models, which allow us to use training\ndata from one domain to build a model for another domain. We propose the\nVariational Bi-domain Triplet Autoencoder (VBTA) that learns a joint\ndistribution of objects from different domains. We extend the VBTAs objective\nfunction by the relative constraints or triplets that sampled from the shared\nlatent space across domains. In other words, we combine the deep generative\nmodels with a metric learning ideas in order to improve the final objective\nwith the triplets information. The performance of the VBTA model is\ndemonstrated on different tasks: image-to-image translation, bi-directional\nimage generation and cross-lingual document classification.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 13:58:42 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 19:47:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kuznetsova", "Rita", ""], ["Bakhteev", "Oleg", ""], ["Ogaltsov", "Alexandr", ""]]}, {"id": "1806.08675", "submitter": "Justus Tilmann Caspar Schwabedal", "authors": "Justus T. C. Schwabedal, John C. Snyder, Ayse Cakmak, Shamim Nemati,\n  Gari D. Clifford", "title": "Addressing Class Imbalance in Classification Problems of Noisy Signals\n  by using Fourier Transform Surrogates", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP nlin.CD q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomizing the Fourier-transform (FT) phases of temporal-spatial data\ngenerates surrogates that approximate examples from the data-generating\ndistribution. We propose such FT surrogates as a novel tool to augment and\nanalyze training of neural networks and explore the approach in the example of\nsleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG\nsignals of under-represented sleep stages, we balanced the CAPSLPDB sleep\ndatabase. We then trained and tested a convolutional neural network for sleep\nstage classification, and found that our surrogate-based augmentation improved\nthe mean F1-score by 7%. As another application of FT surrogates, we formulated\nan approach to compute saliency maps for individual sleep epochs. The\nvisualization is based on the response of inferred class probabilities under\nreplacement of short data segments by partial surrogates. To quantify how well\nthe distributions of the surrogates and the original data match, we evaluated a\ntrained classifier on surrogates of correctly classified examples, and\nsummarized these conditional predictions in a confusion matrix. We show how\nsuch conditional confusion matrices can qualitatively explain the performance\nof surrogates in class balancing. The FT-surrogate augmentation approach may\nimprove classification on noisy signals if carefully adapted to the data\ndistribution under analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:36:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 10:57:19 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Schwabedal", "Justus T. C.", ""], ["Snyder", "John C.", ""], ["Cakmak", "Ayse", ""], ["Nemati", "Shamim", ""], ["Clifford", "Gari D.", ""]]}, {"id": "1806.08716", "submitter": "Andrew Ross", "authors": "Andrew Slavin Ross, Weiwei Pan, Finale Doshi-Velez", "title": "Learning Qualitatively Diverse and Interpretable Rules for\n  Classification", "comments": "Presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden (revision fixes minor issues)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in developing accurate models that can also\nbe explained to humans. Unfortunately, if there exist multiple distinct but\naccurate models for some dataset, current machine learning methods are unlikely\nto find them: standard techniques will likely recover a complex model that\ncombines them. In this work, we introduce a way to identify a maximal set of\ndistinct but accurate models for a dataset. We demonstrate empirically that, in\nsituations where the data supports multiple accurate classifiers, we tend to\nrecover simpler, more interpretable classifiers rather than more complex ones.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 15:10:41 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 19:38:01 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Ross", "Andrew Slavin", ""], ["Pan", "Weiwei", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1806.08727", "submitter": "Pasquale Minervini", "authors": "Dirk Weissenborn, Pasquale Minervini, Tim Dettmers, Isabelle\n  Augenstein, Johannes Welbl, Tim Rockt\\\"aschel, Matko Bo\\v{s}njak, Jeff\n  Mitchell, Thomas Demeester, Pontus Stenetorp, Sebastian Riedel", "title": "Jack the Reader - A Machine Reading Framework", "comments": "Proceedings of the Annual Meeting of the Association for\n  Computational Linguistics (ACL 2018), System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Machine Reading and Natural Language Understanding tasks require reading\nsupporting text in order to answer questions. For example, in Question\nAnswering, the supporting text can be newswire or Wikipedia articles; in\nNatural Language Inference, premises can be seen as the supporting text and\nhypotheses as questions. Providing a set of useful primitives operating in a\nsingle framework of related tasks would allow for expressive modelling, and\neasier model comparison and replication. To that end, we present Jack the\nReader (Jack), a framework for Machine Reading that allows for quick model\nprototyping by component reuse, evaluation of new models on existing datasets\nas well as integrating new datasets and applying them on a growing set of\nimplemented baseline models. Jack is currently supporting (but not limited to)\nthree tasks: Question Answering, Natural Language Inference, and Link\nPrediction. It is developed with the aim of increasing research efficiency and\ncode reuse.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 00:30:29 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Weissenborn", "Dirk", ""], ["Minervini", "Pasquale", ""], ["Dettmers", "Tim", ""], ["Augenstein", "Isabelle", ""], ["Welbl", "Johannes", ""], ["Rockt\u00e4schel", "Tim", ""], ["Bo\u0161njak", "Matko", ""], ["Mitchell", "Jeff", ""], ["Demeester", "Thomas", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1806.08730", "submitter": "Nitish Shirish Keskar", "authors": "Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard\n  Socher", "title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN's\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:39:26 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["McCann", "Bryan", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1806.08734", "submitter": "Aristide Baratin", "authors": "Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min\n  Lin, Fred A. Hamprecht, Yoshua Bengio, Aaron Courville", "title": "On the Spectral Bias of Neural Networks", "comments": "23 pages", "journal-ref": "ICML 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be a class of highly expressive functions able\nto fit even random input-output mappings with $100\\%$ accuracy. In this work,\nwe present properties of neural networks that complement this aspect of\nexpressivity. By using tools from Fourier analysis, we show that deep ReLU\nnetworks are biased towards low frequency functions, meaning that they cannot\nhave local fluctuations without affecting their global behavior. Intuitively,\nthis property is in line with the observation that over-parameterized networks\nfind simple patterns that generalize across data samples. We also investigate\nhow the shape of the data manifold affects expressivity by showing evidence\nthat learning high frequencies gets \\emph{easier} with increasing manifold\ncomplexity, and present a theoretical understanding of this behavior. Finally,\nwe study the robustness of the frequency components with respect to parameter\nperturbation, to develop the intuition that the parameters must be finely tuned\nto express high frequency functions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 15:39:05 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 18:59:38 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 13:45:08 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Rahaman", "Nasim", ""], ["Baratin", "Aristide", ""], ["Arpit", "Devansh", ""], ["Draxler", "Felix", ""], ["Lin", "Min", ""], ["Hamprecht", "Fred A.", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1806.08748", "submitter": "Heeyoul Choi", "authors": "Heeyoul Choi", "title": "Persistent Hidden States and Nonlinear Transformation for Long\n  Short-Term Memory", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been drawing much attention with great\nsuccess in many applications like speech recognition and neural machine\ntranslation. Long short-term memory (LSTM) is one of the most popular RNN units\nin deep learning applications. LSTM transforms the input and the previous\nhidden states to the next states with the affine transformation, multiplication\noperations and a nonlinear activation function, which makes a good data\nrepresentation for a given task. The affine transformation includes rotation\nand reflection, which change the semantic or syntactic information of\ndimensions in the hidden states. However, considering that a model interprets\nthe output sequence of LSTM over the whole input sequence, the dimensions of\nthe states need to keep the same type of semantic or syntactic information\nregardless of the location in the sequence. In this paper, we propose a simple\nvariant of the LSTM unit, persistent recurrent unit (PRU), where each dimension\nof hidden states keeps persistent information across time, so that the space\nkeeps the same meaning over the whole sequence. In addition, to improve the\nnonlinear transformation power, we add a feedforward layer in the PRU\nstructure. In the experiment, we evaluate our proposed methods with three\ndifferent tasks, and the results confirm that our methods have better\nperformance than the conventional LSTM.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:19:46 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 07:09:14 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Choi", "Heeyoul", ""]]}, {"id": "1806.08764", "submitter": "Saif Jabari", "authors": "Saif Eddin Jabari and Deepthi Mary Dilip and DianChao Lin and Bilal\n  Thonnam Thodi", "title": "Learning Traffic Flow Dynamics using Random Fields", "comments": null, "journal-ref": "IEEE Access, 7, 2019, pp. 130566 - 130577", "doi": "10.1109/ACCESS.2019.2941088", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a mesoscopic traffic flow model that explicitly describes\nthe spatio-temporal evolution of the probability distributions of vehicle\ntrajectories. The dynamics are represented by a sequence of factor graphs,\nwhich enable learning of traffic dynamics from limited Lagrangian measurements\nusing an efficient message passing technique. The approach ensures that\nestimated speeds and traffic densities are non-negative with probability one.\nThe estimation technique is tested using vehicle trajectory datasets generated\nusing an independent microscopic traffic simulator and is shown to efficiently\nreproduce traffic conditions with probe vehicle penetration levels as little as\n10\\%. The proposed algorithm is also compared with state-of-the-art traffic\nstate estimation techniques developed for the same purpose and it is shown that\nthe proposed approach can outperform the state-of-the-art techniques in terms\nreconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:54:50 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 08:58:51 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Jabari", "Saif Eddin", ""], ["Dilip", "Deepthi Mary", ""], ["Lin", "DianChao", ""], ["Thodi", "Bilal Thonnam", ""]]}, {"id": "1806.08782", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou and Pan Xu and Quanquan Gu", "title": "Finding Local Minima via Stochastic Nested Variance Reduction", "comments": "37 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two algorithms that can find local minima faster than the\nstate-of-the-art algorithms in both finite-sum and general stochastic nonconvex\noptimization. At the core of the proposed algorithms is\n$\\text{One-epoch-SNVRG}^+$ using stochastic nested variance reduction (Zhou et\nal., 2018a), which outperforms the state-of-the-art variance reduction\nalgorithms such as SCSG (Lei et al., 2017). In particular, for finite-sum\noptimization problems, the proposed\n$\\text{SNVRG}^{+}+\\text{Neon2}^{\\text{finite}}$ algorithm achieves\n$\\tilde{O}(n^{1/2}\\epsilon^{-2}+n\\epsilon_H^{-3}+n^{3/4}\\epsilon_H^{-7/2})$\ngradient complexity to converge to an $(\\epsilon, \\epsilon_H)$-second-order\nstationary point, which outperforms $\\text{SVRG}+\\text{Neon2}^{\\text{finite}}$\n(Allen-Zhu and Li, 2017) , the best existing algorithm, in a wide regime. For\ngeneral stochastic optimization problems, the proposed\n$\\text{SNVRG}^{+}+\\text{Neon2}^{\\text{online}}$ achieves\n$\\tilde{O}(\\epsilon^{-3}+\\epsilon_H^{-5}+\\epsilon^{-2}\\epsilon_H^{-3})$\ngradient complexity, which is better than both\n$\\text{SVRG}+\\text{Neon2}^{\\text{online}}$ (Allen-Zhu and Li, 2017) and\nNatasha2 (Allen-Zhu, 2017) in certain regimes. Furthermore, we explore the\nacceleration brought by third-order smoothness of the objective function.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 17:59:14 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Zhou", "Dongruo", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1806.08804", "submitter": "Rex Ying", "authors": "Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L.\n  Hamilton, Jure Leskovec", "title": "Hierarchical Graph Representation Learning with Differentiable Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks (GNNs) have revolutionized the field of graph\nrepresentation learning through effectively learned node embeddings, and\nachieved state-of-the-art results in tasks such as node classification and link\nprediction. However, current GNN methods are inherently flat and do not learn\nhierarchical representations of graphs---a limitation that is especially\nproblematic for the task of graph classification, where the goal is to predict\nthe label associated with an entire graph. Here we propose DiffPool, a\ndifferentiable graph pooling module that can generate hierarchical\nrepresentations of graphs and can be combined with various graph neural network\narchitectures in an end-to-end fashion. DiffPool learns a differentiable soft\ncluster assignment for nodes at each layer of a deep GNN, mapping nodes to a\nset of clusters, which then form the coarsened input for the next GNN layer.\nOur experimental results show that combining existing GNN methods with DiffPool\nyields an average improvement of 5-10% accuracy on graph classification\nbenchmarks, compared to all existing pooling approaches, achieving a new\nstate-of-the-art on four out of five benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:04:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 21:34:40 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 00:20:42 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 08:54:36 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ying", "Rex", ""], ["You", "Jiaxuan", ""], ["Morris", "Christopher", ""], ["Ren", "Xiang", ""], ["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.08805", "submitter": "Joe Antognini", "authors": "Joseph M. Antognini and Jascha Sohl-Dickstein", "title": "PCA of high dimensional random walks with comparison to neural network\n  training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One technique to visualize the training of neural networks is to perform PCA\non the parameters over the course of training and to project to the subspace\nspanned by the first few PCA components. In this paper we compare this\ntechnique to the PCA of a high dimensional random walk. We compute the\neigenvalues and eigenvectors of the covariance of the trajectory and prove that\nin the long trajectory and high dimensional limit most of the variance is in\nthe first few PCA components, and that the projection of the trajectory onto\nany subspace spanned by PCA components is a Lissajous curve. We generalize\nthese results to a random walk with momentum and to an Ornstein-Uhlenbeck\nprocesses (i.e., a random walk in a quadratic potential) and show that in high\ndimensions the walk is not mean reverting, but will instead be trapped at a\nfixed distance from the minimum. We finally compare the distribution of PCA\nvariances and the PCA projected training trajectories of a linear model trained\non CIFAR-10 and ResNet-50-v2 trained on Imagenet and find that the distribution\nof PCA variances resembles a random walk with drift.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:08:20 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Antognini", "Joseph M.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.08819", "submitter": "Benjamin Huynh", "authors": "Benjamin Q. Huynh and Sanjay Basu", "title": "Forecasting Internally Displaced Population Migration Patterns in Syria\n  and Yemen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Armed conflict has led to an unprecedented number of internally displaced\npersons (IDPs) - individuals who are forced out of their homes but remain\nwithin their country. IDPs often urgently require shelter, food, and\nhealthcare, yet prediction of when large fluxes of IDPs will cross into an area\nremains a major challenge for aid delivery organizations. Accurate forecasting\nof IDP migration would empower humanitarian aid groups to more effectively\nallocate resources during conflicts. We show that monthly flow of IDPs from\nprovince to province in both Syria and Yemen can be accurately forecasted one\nmonth in advance, using publicly available data. We model monthly IDP flow\nusing data on food price, fuel price, wage, geospatial, and news data. We find\nthat machine learning approaches can more accurately forecast migration trends\nthan baseline persistence models. Our findings thus potentially enable\nproactive aid allocation for IDPs in anticipation of forecasted arrivals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:53:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Huynh", "Benjamin Q.", ""], ["Basu", "Sanjay", ""]]}, {"id": "1806.08829", "submitter": "Fernando Gama", "authors": "Fernando Gama and Alejandro Ribeiro and Joan Bruna", "title": "Diffusion Scattering Transforms on Graphs", "comments": "Submitted to the International Conference on Learning Representations\n  (ICLR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is a key aspect of data analysis. In many applications, the natural\nnotion of stability is geometric, as illustrated for example in computer\nvision. Scattering transforms construct deep convolutional representations\nwhich are certified stable to input deformations. This stability to\ndeformations can be interpreted as stability with respect to changes in the\nmetric structure of the domain. In this work, we show that scattering\ntransforms can be generalized to non-Euclidean domains using diffusion\nwavelets, while preserving a notion of stability with respect to metric changes\nin the domain, measured with diffusion maps. The resulting representation is\nstable to metric perturbations of the domain while being able to capture\n\"high-frequency\" information, akin to the Euclidean Scattering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 19:32:25 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:20:46 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Gama", "Fernando", ""], ["Ribeiro", "Alejandro", ""], ["Bruna", "Joan", ""]]}, {"id": "1806.08834", "submitter": "Siddharth Bhela", "authors": "Siddharth Bhela, Vassilis Kekatos, Sriharsha Veeramachaneni", "title": "Smart Inverter Grid Probing for Learning Loads: Part I - Identifiability\n  Analysis", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution grids currently lack comprehensive real-time metering.\nNevertheless, grid operators require precise knowledge of loads and renewable\ngeneration to accomplish any feeder optimization task. At the same time, new\ngrid technologies, such as solar photovoltaics and energy storage units are\ninterfaced via inverters with advanced sensing and actuation capabilities. In\nthis context, this two-part work puts forth the idea of engaging power\nelectronics to probe an electric grid and record its voltage response at\nactuated and metered buses, to infer non-metered loads. Probing can be\naccomplished by commanding inverters to momentarily perturb their power\ninjections. Multiple probing actions can be induced within a few tens of\nseconds. In Part I, load inference via grid probing is formulated as an\nimplicit nonlinear system identification task, which is shown to be\ntopologically observable under certain conditions. The conditions can be\nreadily checked upon solving a max-flow problem on a bipartite graph derived\nfrom the feeder topology and the placement of actuated and non-metered buses.\nThe analysis holds for single- and multi-phase grids, radial or meshed, and\napplies to phasor or magnitude-only voltage data. The topological observability\nof distribution systems using smart meter or phasor data is cast and analyzed a\nspecial case.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 19:53:17 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 04:28:40 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Bhela", "Siddharth", ""], ["Kekatos", "Vassilis", ""], ["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1806.08835", "submitter": "Nabeel Abdur Rehman", "authors": "Nabeel Abdur Rehman and Maxwell Matthaios Aliapoulios and Disha\n  Umarwani and Rumi Chunara", "title": "Domain Adaptation for Infection Prediction from Symptoms Based on Data\n  from Different Study Designs and Contexts", "comments": "Main: 18 pages, 2 figures, 2 tables. Appendix: 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute respiratory infections have epidemic and pandemic potential and thus\nare being studied worldwide, albeit in many different contexts and study\nformats. Predicting infection from symptom data is critical, though using\nsymptom data from varied studies in aggregate is challenging because the data\nis collected in different ways. Accordingly, different symptom profiles could\nbe more predictive in certain studies, or even symptoms of the same name could\nhave different meanings in different contexts. We assess state-of-the-art\ntransfer learning methods for improving prediction of infection from symptom\ndata in multiple types of health care data ranging from clinical, to home-visit\nas well as crowdsourced studies. We show interesting characteristics regarding\nsix different study types and their feature domains. Further, we demonstrate\nthat it is possible to use data collected from one study to predict infection\nin another, at close to or better than using a single dataset for prediction on\nitself. We also investigate in which conditions specific transfer learning and\ndomain adaptation methods may perform better on symptom data. This work has the\npotential for broad applicability as we show how it is possible to transfer\nlearning from one public health study design to another, and data collected\nfrom one study may be used for prediction of labels for another, even collected\nthrough different study designs, populations and contexts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 19:55:14 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rehman", "Nabeel Abdur", ""], ["Aliapoulios", "Maxwell Matthaios", ""], ["Umarwani", "Disha", ""], ["Chunara", "Rumi", ""]]}, {"id": "1806.08836", "submitter": "Siddharth Bhela", "authors": "Siddharth Bhela, Vassilis Kekatos, Sriharsha Veeramachaneni", "title": "Smart Inverter Grid Probing for Learning Loads: Part II - Probing\n  Injection Design", "comments": "9 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This two-part work puts forth the idea of engaging power electronics to probe\nan electric grid to infer non-metered loads. Probing can be accomplished by\ncommanding inverters to perturb their power injections and record the induced\nvoltage response. Once a probing setup is deemed topologically observable by\nthe tests of Part I, Part II provides a methodology for designing probing\ninjections abiding by inverter and network constraints to improve load\nestimates. The task is challenging since system estimates depend on both\nprobing injections and unknown loads in an implicit nonlinear fashion. The\nmethodology first constructs a library of candidate probing vectors by sampling\nover the feasible set of inverter injections. Leveraging a linearized grid\nmodel and a robust approach, the candidate probing vectors violating voltage\nconstraints for any anticipated load value are subsequently rejected. Among the\nqualified candidates, the design finally identifies the probing vectors\nyielding the most diverse system states. The probing task under noisy phasor\nand non-phasor data is tackled using a semidefinite-program (SDP) relaxation.\nNumerical tests using synthetic and real-world data on a benchmark feeder\nvalidate the conditions of Part I; the SDP-based solver; the importance of\nprobing design; and the effects of probing duration and noise.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 20:01:39 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 04:23:58 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Bhela", "Siddharth", ""], ["Kekatos", "Vassilis", ""], ["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1806.08838", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Matthias Poloczek", "title": "Bayesian Optimization of Combinatorial Structures", "comments": "Published at International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of expensive-to-evaluate black-box functions over\ncombinatorial structures is an ubiquitous task in machine learning, engineering\nand the natural sciences. The combinatorial explosion of the search space and\ncostly evaluations pose challenges for current techniques in discrete\noptimization and machine learning, and critically require new algorithmic\nideas. This article proposes, to the best of our knowledge, the first algorithm\nto overcome these challenges, based on an adaptive, scalable model that\nidentifies useful combinatorial structure even when data is scarce. Our\nacquisition function pioneers the use of semidefinite programming to achieve\nefficiency and scalability. Experimental evaluations demonstrate that this\nalgorithm consistently outperforms other methods from combinatorial and\nBayesian optimization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 20:08:53 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 15:17:25 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Baptista", "Ricardo", ""], ["Poloczek", "Matthias", ""]]}, {"id": "1806.08867", "submitter": "Shalmali Joshi", "authors": "Shalmali Joshi and Oluwasanmi Koyejo and Been Kim and Joydeep Ghosh", "title": "xGEMs: Generating Examplars to Explain Black-Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes xGEMs or manifold guided exemplars, a framework to\nunderstand black-box classifier behavior by exploring the landscape of the\nunderlying data manifold as data points cross decision boundaries. To do so, we\ntrain an unsupervised implicit generative model -- treated as a proxy to the\ndata manifold. We summarize black-box model behavior quantitatively by\nperturbing data samples along the manifold. We demonstrate xGEMs' ability to\ndetect and quantify bias in model learning and also for understanding the\nchanges in model behavior as training progresses.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:58:14 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Joshi", "Shalmali", ""], ["Koyejo", "Oluwasanmi", ""], ["Kim", "Been", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1806.08887", "submitter": "Yubei Chen", "authors": "Yubei Chen, Dylan M. Paiton, Bruno A. Olshausen", "title": "The Sparse Manifold Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a signal representation framework called the sparse manifold\ntransform that combines key ideas from sparse coding, manifold learning, and\nslow feature analysis. It turns non-linear transformations in the primary\nsensory signal space into linear interpolations in a representational embedding\nspace while maintaining approximate invertibility. The sparse manifold\ntransform is an unsupervised and generative framework that explicitly and\nsimultaneously models the sparse discreteness and low-dimensional manifold\nstructure found in natural scenes. When stacked, it also models hierarchical\ncomposition. We provide a theoretical description of the transform and\ndemonstrate properties of the learned representation on both synthetic data and\nnatural videos.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 01:44:50 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 01:22:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chen", "Yubei", ""], ["Paiton", "Dylan M.", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1806.08894", "submitter": "Sajad Mousavi", "authors": "Seyed Sajad Mousavi, Michael Schukat, Enda Howley", "title": "Deep Reinforcement Learning: An Overview", "comments": "Proceedings of SAI Intelligent Systems Conference (IntelliSys) 2016", "journal-ref": null, "doi": "10.1007/978-3-319-56991-8_32", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a specific machine learning method called deep learning has\ngained huge attraction, as it has obtained astonishing results in broad\napplications such as pattern recognition, speech recognition, computer vision,\nand natural language processing. Recent research has also been shown that deep\nlearning techniques can be combined with reinforcement learning methods to\nlearn useful representations for the problems with high dimensional raw data\ninput. This chapter reviews the recent advances in deep reinforcement learning\nwith a focus on the most used deep architectures such as autoencoders,\nconvolutional neural networks and recurrent neural networks which have\nsuccessfully been come together with the reinforcement learning framework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 02:18:26 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Mousavi", "Seyed Sajad", ""], ["Schukat", "Michael", ""], ["Howley", "Enda", ""]]}, {"id": "1806.08911", "submitter": "Qiang Wu", "authors": "Ning Zhang, Zhou Yu and Qiang Wu", "title": "Overlapping Sliced Inverse Regression for Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced inverse regression (SIR) is a pioneer tool for supervised dimension\nreduction. It identifies the effective dimension reduction space, the subspace\nof significant factors with intrinsic lower dimensionality. In this paper, we\npropose to refine the SIR algorithm through an overlapping slicing scheme. The\nnew algorithm, called overlapping sliced inverse regression (OSIR), is able to\nestimate the effective dimension reduction space and determine the number of\neffective factors more accurately. We show that such overlapping procedure has\nthe potential to identify the information contained in the derivatives of the\ninverse regression curve, which helps to explain the superiority of OSIR. We\nalso prove that OSIR algorithm is $\\sqrt n $-consistent and verify its\neffectiveness by simulations and real applications.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 05:39:11 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Ning", ""], ["Yu", "Zhou", ""], ["Wu", "Qiang", ""]]}, {"id": "1806.08915", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek", "title": "DALEX: explainers for complex predictive models", "comments": "12 pages", "journal-ref": "Journal of Machine Learning Research 19 (2018) 1-5", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is invaded by elastic, yet complex methods such as neural\nnetworks or ensembles (model stacking, boosting or bagging). Such methods are\nusually described by a large number of parameters or hyper parameters - a price\nthat one needs to pay for elasticity. The very number of parameters makes\nmodels hard to understand. This paper describes a consistent collection of\nexplainers for predictive models, a.k.a. black boxes. Each explainer is a\ntechnique for exploration of a black box model. Presented approaches are\nmodel-agnostic, what means that they extract useful information from any\npredictive method despite its internal structure. Each explainer is linked with\na specific aspect of a model. Some are useful in decomposing predictions, some\nserve better in understanding performance, while others are useful in\nunderstanding importance and conditional responses of a particular variable.\nEvery explainer presented in this paper works for a single model or for a\ncollection of models. In the latter case, models can be compared against each\nother. Such comparison helps to find strengths and weaknesses of different\napproaches and gives additional possibilities for model validation. Presented\nexplainers are implemented in the DALEX package for R. They are based on a\nuniform standardized grammar of model exploration which may be easily extended.\nThe current implementation supports the most popular frameworks for\nclassification and regression.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 06:28:38 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:15:54 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Biecek", "Przemyslaw", ""]]}, {"id": "1806.08941", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "A Recursive PLS (Partial Least Squares) based Approach for Enterprise\n  Threat Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing solutions to enterprise threat management are preventive\napproaches prescribing means to prevent policy violations with varying degrees\nof success. In this paper we consider the complementary scenario where a number\nof security violations have already occurred, or security threats, or\nvulnerabilities have been reported and a security administrator needs to\ngenerate optimal response to these security events. We present a principled\napproach to study and model the human expertise in responding to the emergent\nthreats owing to these security events. A recursive Partial Least Squares based\nadaptive learning model is defined using a factorial analysis of the security\nevents together with a method for estimating the effect of global context\ndependent semantic information used by the security administrators. Presented\nmodel is theoretically optimal and operationally recursive in nature to deal\nwith the set of security events being generated continuously. We discuss the\nunderlying challenges and ways in which the model could be operationalized in\ncentralized versus decentralized, and real-time versus batch processing modes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 10:12:38 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Misra", "Janardan", ""]]}, {"id": "1806.08946", "submitter": "Ze Wang", "authors": "Jingyuan Wang, Ze Wang, Jianfeng Li, Junjie Wu", "title": "Multilevel Wavelet Decomposition Network for Interpretable Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the unprecedented rising of time series from\nalmost all kindes of academic and industrial fields. Various types of deep\nneural network models have been introduced to time series analysis, but the\nimportant frequency information is yet lack of effective modeling. In light of\nthis, in this paper we propose a wavelet-based neural network structure called\nmultilevel Wavelet Decomposition Network (mWDN) for building frequency-aware\ndeep learning models for time series analysis. mWDN preserves the advantage of\nmultilevel discrete wavelet decomposition in frequency learning while enables\nthe fine-tuning of all parameters under a deep neural network framework. Based\non mWDN, we further propose two deep learning models called Residual\nClassification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for\ntime series classification and forecasting, respectively. The two models take\nall or partial mWDN decomposed sub-series in different frequencies as input,\nand resort to the back propagation algorithm to learn all the parameters\nglobally, which enables seamless embedding of wavelet-based frequency analysis\ninto deep learning frameworks. Extensive experiments on 40 UCR datasets and a\nreal-world user volume dataset demonstrate the excellent performance of our\ntime series models based on mWDN. In particular, we propose an importance\nanalysis method to mWDN based models, which successfully identifies those\ntime-series elements and mWDN layers that are crucially important to time\nseries analysis. This indeed indicates the interpretability advantage of mWDN,\nand can be viewed as an indepth exploration to interpretable deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 11:12:12 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Wang", "Jingyuan", ""], ["Wang", "Ze", ""], ["Li", "Jianfeng", ""], ["Wu", "Junjie", ""]]}, {"id": "1806.08990", "submitter": "Zhewei Huang", "authors": "Zhewei Huang, Wen Heng, Yuanzheng Tao, Shuchang Zhou", "title": "Stroke-based Character Reconstruction", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:47:34 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 06:38:00 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 13:44:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Huang", "Zhewei", ""], ["Heng", "Wen", ""], ["Tao", "Yuanzheng", ""], ["Zhou", "Shuchang", ""]]}, {"id": "1806.09018", "submitter": "Yao Zhou", "authors": "Yao Zhou, Jingrui He", "title": "Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching", "comments": "3 pages, SBP-BRiMS 18 Doctoral Consortium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented demand for large amount of data has catalyzed the trend of\ncombining human insights with machine learning techniques, which facilitate the\nuse of crowdsourcing to enlist label information both effectively and\nefficiently. The classic work on crowdsourcing mainly focuses on the label\ninference problem under the categorization setting. However, inferring the true\nlabel requires sophisticated aggregation models that usually can only perform\nwell under certain assumptions. Meanwhile, no matter how complicated the\naggregation model is, the true model that generated the crowd labels remains\nunknown. Therefore, the label inference problem can never infer the ground\ntruth perfectly. Based on the fact that the crowdsourcing labels are abundant\nand utilizing aggregation will lose such kind of rich annotation information\n(e.g., which worker provided which labels), we believe that it is critical to\ntake the diverse labeling abilities of the crowdsourcing workers as well as\ntheir correlations into consideration. To address the above challenge, we\npropose to tackle three research problems, namely inference, learning, and\nteaching.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 18:29:06 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhou", "Yao", ""], ["He", "Jingrui", ""]]}, {"id": "1806.09035", "submitter": "Alex Kouzemtchenko", "authors": "Alex Kouzemtchenko", "title": "Defending Malware Classification Networks Against Adversarial\n  Perturbations with Non-Negative Weight Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing body of literature showing that deep neural networks are\nvulnerable to adversarial input modification. Recently this work has been\nextended from image classification to malware classification over boolean\nfeatures. In this paper we present several new methods for training restricted\nnetworks in this specific domain that are highly effective at preventing\nadversarial perturbations. We start with a fully adversarially resistant neural\nnetwork that has hard non-negative weight restrictions and is equivalent to\nlearning a monotonic boolean function and then attempt to relax the constraints\nto improve classifier accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 20:46:06 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kouzemtchenko", "Alex", ""]]}, {"id": "1806.09038", "submitter": "Marek Rychlik", "authors": "Marek Rychlik", "title": "Deductron -- A Recurrent Neural Network", "comments": "34 pages, contains Python code, Python code requires data file\n  data.py (should be included in the archive)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper is a study in Recurrent Neural Networks (RNN), motivated by\nthe lack of examples simple enough so that they can be thoroughly understood\ntheoretically, but complex enough to be realistic. We constructed an example of\nstructured data, motivated by problems from image-to-text conversion (OCR),\nwhich requires long-term memory to decode. Our data is a simple writing system,\nencoding characters 'X' and 'O' as their upper halves, which is possible due to\nsymmetry of the two characters. The characters can be connected, as in some\nlanguages using cursive, such as Arabic (abjad). The string 'XOOXXO' may be\nencoded as\n'${\\vee}{\\wedge}\\kern-1.5pt{\\wedge}{\\vee}\\kern-1.5pt{\\vee}{\\wedge}$'. It\nfollows that we may need to know arbitrarily long past to decode a current\ncharacter, thus requiring long-term memory. Subsequently we constructed an RNN\ncapable of decoding sequences encoded in this manner. Rather than by training,\nwe constructed our RNN \"by inspection\", i.e. we guessed its weights. This\ninvolved a sequence of steps. We wrote a conventional program which decodes the\nsequences as the example above. Subsequently, we interpreted the program as a\nneural network (the only example of this kind known to us). Finally, we\ngeneralized this neural network to discover a new RNN architecture whose\ninstance is our handcrafted RNN. It turns out to be a 3 layer network, where\nthe middle layer is capable of performing simple logical inferences; thus the\nname \"deductron\". It is demonstrated that it is possible to train our network\nby simulated annealing. Also, known variants of stochastic gradient descent\n(SGD) methods are shown to work.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 21:15:26 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 13:21:40 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 20:06:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Rychlik", "Marek", ""]]}, {"id": "1806.09039", "submitter": "Mathieu Desbrun", "authors": "Max Budninskiy, Glorian Yin, Leman Feng, Yiying Tong, Mathieu Desbrun", "title": "Parallel Transport Unfolding: A Connection-based Manifold Learning\n  Approach", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning offers nonlinear dimensionality reduction of\nhigh-dimensional datasets. In this paper, we bring geometry processing to bear\non manifold learning by introducing a new approach based on metric connection\nfor generating a quasi-isometric, low-dimensional mapping from a sparse and\nirregular sampling of an arbitrary manifold embedded in a high-dimensional\nspace. Geodesic distances of discrete paths over the input pointset are\nevaluated through \"parallel transport unfolding\" (PTU) to offer robustness to\npoor sampling and arbitrary topology. Our new geometric procedure exhibits the\nsame strong resilience to noise as one of the staples of manifold learning, the\nIsomap algorithm, as it also exploits all pairwise geodesic distances to\ncompute a low-dimensional embedding. While Isomap is limited to\ngeodesically-convex sampled domains, parallel transport unfolding does not\nsuffer from this crippling limitation, resulting in an improved robustness to\nirregularity and voids in the sampling. Moreover, it involves only simple\nlinear algebra, significantly improves the accuracy of all pairwise geodesic\ndistance approximations, and has the same computational complexity as Isomap.\nFinally, we show that our connection-based distance estimation can be used for\nfaster variants of Isomap such as L-Isomap.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 21:29:50 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 16:20:25 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Budninskiy", "Max", ""], ["Yin", "Glorian", ""], ["Feng", "Leman", ""], ["Tong", "Yiying", ""], ["Desbrun", "Mathieu", ""]]}, {"id": "1806.09048", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "A classification point-of-view about conditional Kendall's tau", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the problem of estimating conditional Kendall's tau can be\nrewritten as a classification task. Conditional Kendall's tau is a conditional\ndependence parameter that is a characteristic of a given pair of random\nvariables. The goal is to predict whether the pair is concordant (value of $1$)\nor discordant (value of $-1$) conditionally on some covariates. We prove the\nconsistency and the asymptotic normality of a family of penalized approximate\nmaximum likelihood estimators, including the equivalent of the logit and probit\nregressions in our framework. Then, we detail specific algorithms adapting\nusual machine learning techniques, including nearest neighbors, decision trees,\nrandom forests and neural networks, to the setting of the estimation of\nconditional Kendall's tau. Finite sample properties of these estimators and\ntheir sensitivities to each component of the data-generating process are\nassessed in a simulation study. Finally, we apply all these estimators to a\ndataset of European stock indices.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 22:03:10 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 15:10:28 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 10:35:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "1806.09055", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Karen Simonyan, Yiming Yang", "title": "DARTS: Differentiable Architecture Search", "comments": "Published at ICLR 2019; Code and pretrained models available at\n  https://github.com/quark0/darts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 00:06:13 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 06:29:32 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Liu", "Hanxiao", ""], ["Simonyan", "Karen", ""], ["Yang", "Yiming", ""]]}, {"id": "1806.09060", "submitter": "Samuel Ainsworth", "authors": "Samuel K. Ainsworth, Nicholas J. Foti, Emily B. Fox", "title": "Disentangled VAE Representations for Multi-Aspect and Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and related application areas are\nfundamentally variants of conditional modeling and sampling across multi-aspect\ndata, either multi-view, multi-modal, or simply multi-group. For example,\nsampling from the distribution of English sentences conditioned on a given\nFrench sentence or sampling audio waveforms conditioned on a given piece of\ntext. Central to many of these problems is the issue of missing data: we can\nobserve many English, French, or German sentences individually but only\noccasionally do we have data for a sentence pair. Motivated by these\napplications and inspired by recent progress in variational autoencoders for\ngrouped data, we develop factVAE, a deep generative model capable of handling\nmulti-aspect data, robust to missing observations, and with a prior that\nencourages disentanglement between the groups and the latent dimensions. The\neffectiveness of factVAE is demonstrated on a variety of rich real-world\ndatasets, including motion capture poses and pictures of faces captured from\nvarying poses and perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 01:06:06 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ainsworth", "Samuel K.", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily B.", ""]]}, {"id": "1806.09070", "submitter": "Gokul Swamy", "authors": "Patrick Chao, Alexander Li, Gokul Swamy", "title": "Generative Models for Pose Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate nearest neighbor and generative models for transferring pose\nbetween persons. We take in a video of one person performing a sequence of\nactions and attempt to generate a video of another person performing the same\nactions. Our generative model (pix2pix) outperforms k-NN at both generating\ncorresponding frames and generalizing outside the demonstrated action set. Our\nmost salient contribution is determining a pipeline (pose detection, face\ndetection, k-NN based pairing) that is effective at perform-ing the desired\ntask. We also detail several iterative improvements and failure modes.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 02:33:00 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chao", "Patrick", ""], ["Li", "Alexander", ""], ["Swamy", "Gokul", ""]]}, {"id": "1806.09077", "submitter": "Ronny Luss", "authors": "Anna Choromanska, Benjamin Cowen, Sadhana Kumaravel, Ronny Luss,\n  Mattia Rigotti, Irina Rish, Brian Kingsbury, Paolo DiAchille, Viatcheslav\n  Gurev, Ravi Tejwani, Djallel Bouneffouf", "title": "Beyond Backprop: Online Alternating Minimization with Auxiliary\n  Variables", "comments": "First six authors contributed equally to this work: A.C. - theory,\n  manuscript, B.C. - code, experiments, S.K. - code, experiments, R.L. -\n  algorithm, experiments, M.R. - code, experiments, I.R. - algorithm,\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent advances in deep neural networks, training them\nremains a challenge due to the highly non-convex nature of the objective\nfunction. State-of-the-art methods rely on error backpropagation, which suffers\nfrom several well-known issues, such as vanishing and exploding gradients,\ninability to handle non-differentiable nonlinearities and to parallelize\nweight-updates across layers, and biological implausibility. These limitations\ncontinue to motivate exploration of alternative training algorithms, including\nseveral recently proposed auxiliary-variable methods which break the complex\nnested objective function into local subproblems. However, those techniques are\nmainly offline (batch), which limits their applicability to extremely large\ndatasets, as well as to online, continual or reinforcement learning. The main\ncontribution of our work is a novel online (stochastic/mini-batch) alternating\nminimization (AM) approach for training deep neural networks, together with the\nfirst theoretical convergence guarantees for AM in stochastic settings and\npromising empirical results on a variety of architectures and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 03:55:28 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 16:53:35 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 21:32:59 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 16:55:28 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Choromanska", "Anna", ""], ["Cowen", "Benjamin", ""], ["Kumaravel", "Sadhana", ""], ["Luss", "Ronny", ""], ["Rigotti", "Mattia", ""], ["Rish", "Irina", ""], ["Kingsbury", "Brian", ""], ["DiAchille", "Paolo", ""], ["Gurev", "Viatcheslav", ""], ["Tejwani", "Ravi", ""], ["Bouneffouf", "Djallel", ""]]}, {"id": "1806.09141", "submitter": "Raanan Rohekar", "authors": "Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik", "title": "Constructing Deep Neural Networks by Bayesian Network Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a principled approach for unsupervised structure learning of\ndeep neural networks. We propose a new interpretation for depth and inter-layer\nconnectivity where conditional independencies in the input distribution are\nencoded hierarchically in the network structure. Thus, the depth of the network\nis determined inherently. The proposed method casts the problem of neural\nnetwork structure learning as a problem of Bayesian network structure learning.\nThen, instead of directly learning the discriminative structure, it learns a\ngenerative graph, constructs its stochastic inverse, and then constructs a\ndiscriminative graph. We prove that conditional-dependency relations among the\nlatent variables in the generative graph are preserved in the class-conditional\ndiscriminative graph. We demonstrate on image classification benchmarks that\nthe deepest layers (convolutional and dense) of common networks can be replaced\nby significantly smaller learned structures, while maintaining classification\naccuracy---state-of-the-art on tested benchmarks. Our structure learning\nalgorithm requires a small computational cost and runs efficiently on a\nstandard desktop CPU.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 13:05:06 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:50:14 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 12:11:20 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Rohekar", "Raanan Y.", ""], ["Nisimov", "Shami", ""], ["Gurwicz", "Yaniv", ""], ["Koren", "Guy", ""], ["Novik", "Gal", ""]]}, {"id": "1806.09178", "submitter": "Zhu Li", "authors": "Zhu Li, Jean-Francois Ton, Dino Oglic, Dino Sejdinovic", "title": "Towards A Unified Analysis of Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Fourier features is a widely used, simple, and effective technique for\nscaling up kernel methods. The existing theoretical analysis of the approach,\nhowever, remains focused on specific learning tasks and typically gives\npessimistic bounds which are at odds with the empirical results. We tackle\nthese problems and provide the first unified risk analysis of learning with\nrandom Fourier features using the squared error and Lipschitz continuous loss\nfunctions. In our bounds, the trade-off between the computational cost and the\nexpected risk convergence rate is problem specific and expressed in terms of\nthe regularization parameter and the \\emph{number of effective degrees of\nfreedom}. We study both the standard random Fourier features method for which\nwe improve the existing bounds on the number of features required to guarantee\nthe corresponding minimax risk convergence rate of kernel ridge regression, as\nwell as a data-dependent modification which samples features proportional to\n\\emph{ridge leverage scores} and further reduces the required number of\nfeatures. As ridge leverage scores are expensive to compute, we devise a simple\napproximation scheme which provably reduces the computational cost without loss\nof statistical efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 16:52:13 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 17:33:20 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 03:41:26 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 15:49:52 GMT"}, {"version": "v5", "created": "Thu, 4 Feb 2021 20:07:17 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Li", "Zhu", ""], ["Ton", "Jean-Francois", ""], ["Oglic", "Dino", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1806.09186", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu,\n  Hongyue Zha and Nenghai Yu", "title": "Detection based Defense against Adversarial Examples from the\n  Steganalysis Point of View", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 04:57:20 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 01:37:46 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 08:25:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Liu", "Jiayang", ""], ["Zhang", "Weiming", ""], ["Zhang", "Yiwei", ""], ["Hou", "Dongdong", ""], ["Liu", "Yujia", ""], ["Zha", "Hongyue", ""], ["Yu", "Nenghai", ""]]}, {"id": "1806.09206", "submitter": "Yingyu Liang", "authors": "Shengchao Liu, Mehmet Furkan Demirel, Yingyu Liang", "title": "N-Gram Graph: Simple Unsupervised Representation for Graphs, with\n  Applications to Molecules", "comments": "28 pages. Accepted in NeurIPS 2019 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have recently been adopted in various\napplications in medicine, biology, chemistry, and material engineering. An\nimportant task is to predict the properties of molecules, which serves as the\nmain subroutine in many downstream applications such as virtual screening and\ndrug design. Despite the increasing interest, the key challenge is to construct\nproper representations of molecules for learning algorithms. This paper\nintroduces the N-gram graph, a simple unsupervised representation for\nmolecules. The method first embeds the vertices in the molecule graph. It then\nconstructs a compact representation for the graph by assembling the vertex\nembeddings in short walks in the graph, which we show is equivalent to a simple\ngraph neural network that needs no training. The representations can thus be\nefficiently computed and then used with supervised learning methods for\nprediction. Experiments on 60 tasks from 10 benchmark datasets demonstrate its\nadvantages over both popular graph neural networks and traditional\nrepresentation methods. This is complemented by theoretical analysis showing\nits strong representation and prediction power.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 20:28:49 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 18:39:10 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Liu", "Shengchao", ""], ["Demirel", "Mehmet Furkan", ""], ["Liang", "Yingyu", ""]]}, {"id": "1806.09211", "submitter": "Govind Ramnarayan", "authors": "Govind Ramnarayan", "title": "Equalizing Financial Impact in Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notions of \"fair classification\" that have arisen in computer science\ngenerally revolve around equalizing certain statistics across protected groups.\nThis approach has been criticized as ignoring societal issues, including how\nerrors can hurt certain groups disproportionately. We pose a modification of\none of the fairness criteria from Hardt, Price, and Srebro [NIPS, 2016] that\nmakes a small step towards addressing this issue in the case of financial\ndecisions like giving loans. We call this new notion \"equalized financial\nimpact.\"\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 21:08:16 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ramnarayan", "Govind", ""]]}, {"id": "1806.09228", "submitter": "Junru Wu", "authors": "Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan,\n  Yingyan Lin", "title": "Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster\n  Assignments for Compressing Deep Convolutions", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend of pushing CNNs deeper with convolutions has created a\npressing demand to achieve higher compression gains on CNNs where convolutions\ndominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide\nResNet). Further, the high energy consumption of convolutions limits its\ndeployment on mobile devices. To this end, we proposed a simple yet effective\nscheme for compressing convolutions though applying k-means clustering on the\nweights, compression is achieved through weight-sharing, by only recording $K$\ncluster centers and weight assignment indexes. We then introduced a novel\nspectrally relaxed $k$-means regularization, which tends to make hard\nassignments of convolutional layer weights to $K$ learned cluster centers\nduring re-training. We additionally propose an improved set of metrics to\nestimate energy consumption of CNN hardware implementations, whose estimation\nresults are verified to be consistent with previously proposed energy\nestimation tool extrapolated from actual hardware measurements. We finally\nevaluated Deep $k$-Means across several CNN models in terms of both compression\nratio and energy consumption reduction, observing promising results without\nincurring accuracy loss. The code is available at\nhttps://github.com/Sandbox3aster/Deep-K-Means\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 22:49:24 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Wu", "Junru", ""], ["Wang", "Yue", ""], ["Wu", "Zhenyu", ""], ["Wang", "Zhangyang", ""], ["Veeraraghavan", "Ashok", ""], ["Lin", "Yingyan", ""]]}, {"id": "1806.09231", "submitter": "Shubhendu Trivedi", "authors": "Risi Kondor, Zhen Lin, Shubhendu Trivedi", "title": "Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional\n  Neural Network", "comments": "Camera ready version for the proceedings of the thirty-second\n  conference on Neural Information Processing Systems (NIPS), Montreal, Canada,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Cohen \\emph{et al.} has achieved state-of-the-art results for\nlearning spherical images in a rotation invariant way by using ideas from group\nrepresentation theory and noncommutative harmonic analysis. In this paper we\npropose a generalization of this work that generally exhibits improved\nperformace, but from an implementation point of view is actually simpler. An\nunusual feature of the proposed architecture is that it uses the\nClebsch--Gordan transform as its only source of nonlinearity, thus avoiding\nrepeated forward and backward Fourier transforms. The underlying ideas of the\npaper generalize to constructing neural networks that are invariant to the\naction of other compact groups.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 23:17:05 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 18:14:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kondor", "Risi", ""], ["Lin", "Zhen", ""], ["Trivedi", "Shubhendu", ""]]}, {"id": "1806.09235", "submitter": "Weili Nie", "authors": "Weili Nie and Ankit Patel", "title": "Towards a Better Understanding and Regularization of GAN Training\n  Dynamics", "comments": "UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are notoriously difficult to train and\nthe reasons underlying their (non-)convergence behaviors are still not\ncompletely understood. By first considering a simple yet representative GAN\nexample, we mathematically analyze its local convergence behavior in a\nnon-asymptotic way. Furthermore, the analysis is extended to general GANs under\ncertain assumptions. We find that in order to ensure a good convergence rate,\ntwo factors of the Jacobian in the GAN training dynamics should be\nsimultaneously avoided, which are (i) the Phase Factor, i.e., the Jacobian has\ncomplex eigenvalues with a large imaginary-to-real ratio, and (ii) the\nConditioning Factor, i.e., the Jacobian is ill-conditioned. Previous methods of\nregularizing the Jacobian can only alleviate one of these two factors, while\nmaking the other more severe. Thus we propose a new JAcobian REgularization\n(JARE) for GANs, which simultaneously addresses both factors by construction.\nFinally, we conduct experiments that confirm our theoretical analysis and\ndemonstrate the advantages of JARE over previous methods in stabilizing GANs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 23:19:23 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 17:24:20 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nie", "Weili", ""], ["Patel", "Ankit", ""]]}, {"id": "1806.09266", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li\n  Fei-Fei, Silvio Savarese", "title": "Learning Task-Oriented Grasping for Tool Manipulation from Simulated\n  Self-Supervision", "comments": "RSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tool manipulation is vital for facilitating robots to complete challenging\ntask goals. It requires reasoning about the desired effect of the task and thus\nproperly grasping and manipulating the tool to achieve the task. Task-agnostic\ngrasping optimizes for grasp robustness while ignoring crucial task-specific\nconstraints. In this paper, we propose the Task-Oriented Grasping Network\n(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the\nmanipulation policy for that tool. The training process of the model is based\non large-scale simulated self-supervision with procedurally generated tool\nobjects. We perform both simulated and real-world experiments on two tool-based\nmanipulation tasks: sweeping and hammering. Our model achieves overall 71.1%\ntask success rate for sweeping and 80.0% task success rate for hammering.\nSupplementary material is available at: bit.ly/task-oriented-grasp\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 03:08:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Fang", "Kuan", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Kurenkov", "Andrey", ""], ["Mehta", "Viraj", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1806.09277", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis, Stefanie Jegelka, Tommi S. Jaakkola", "title": "Towards Optimal Transport with Global Invariances", "comments": "AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 03:57:49 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 21:45:59 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Jegelka", "Stefanie", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1806.09300", "submitter": "Esben Jannik Bjerrum", "authors": "Esben Jannik Bjerrum and Boris Sattarov", "title": "Improving Chemical Autoencoder Latent Space and Molecular De novo\n  Generation Diversity with Heteroencoders", "comments": null, "journal-ref": "Biomolecules 2018, 8(4), 131", "doi": "10.3390/biom8040131", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical autoencoders are attractive models as they combine chemical space\nnavigation with possibilities for de-novo molecule generation in areas of\ninterest. This enables them to produce focused chemical libraries around a\nsingle lead compound for employment early in a drug discovery project. Here it\nis shown that the choice of chemical representation, such as SMILES strings,\nhas a large influence on the properties of the latent space. It is further\nexplored to what extent translating between different chemical representations\ninfluences the latent space similarity to the SMILES strings or circular\nfingerprints. By employing SMILES enumeration for either the encoder or\ndecoder, it is found that the decoder has the largest influence on the\nproperties of the latent space. Training a sequence to sequence heteroencoder\nbased on recurrent neural networks(RNNs) with long short-term memory cells\n(LSTM) to predict different enumerated SMILES strings from the same canonical\nSMILES string gives the largest similarity between latent space distance and\nmolecular similarity measured as circular fingerprints similarity. Using the\noutput from the bottleneck in QSAR modelling of five molecular datasets shows\nthat heteroencoder derived vectors markedly outperforms autoencoder derived\nvectors as well as models built using ECFP4 fingerprints, underlining the\nincreased chemical relevance of the latent space. However, the use of\nenumeration during training of the decoder leads to a markedly increase in the\nrate of decoding to a different molecules than encoded, a tendency that can be\ncounteracted with more complex network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 06:46:35 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 20:35:42 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Bjerrum", "Esben Jannik", ""], ["Sattarov", "Boris", ""]]}, {"id": "1806.09351", "submitter": "Rituraj Kaushik", "authors": "Rituraj Kaushik, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret", "title": "Multi-objective Model-based Policy Search for Data-efficient Learning\n  with Sparse Rewards", "comments": "Conference on Robot Learning (CoRL)- 2018; Code at\n  https://github.com/resibots/kaushik_2018_multi-dex ; Video at\n  https://youtu.be/9ZLwUxAAq6M", "journal-ref": "Proceedings of the Conference on Robot Learning, PMLR 87:839-855,\n  2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most data-efficient algorithms for reinforcement learning in robotics are\nmodel-based policy search algorithms, which alternate between learning a\ndynamical model of the robot and optimizing a policy to maximize the expected\nreturn given the model and its uncertainties. However, the current algorithms\nlack an effective exploration strategy to deal with sparse or misleading reward\nscenarios: if they do not experience any state with a positive reward during\nthe initial random exploration, it is very unlikely to solve the problem. Here,\nwe propose a novel model-based policy search algorithm, Multi-DEX, that\nleverages a learned dynamical model to efficiently explore the task space and\nsolve tasks with sparse rewards in a few episodes. To achieve this, we frame\nthe policy search problem as a multi-objective, model-based policy optimization\nproblem with three objectives: (1) generate maximally novel state trajectories,\n(2) maximize the expected return and (3) keep the system in state-space regions\nfor which the model is as accurate as possible. We then optimize these\nobjectives using a Pareto-based multi-objective optimization algorithm. The\nexperiments show that Multi-DEX is able to solve sparse reward scenarios (with\na simulated robotic arm) in much lower interaction time than VIME, TRPO,\nGEP-PG, CMA-ES and Black-DROPS.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:46:47 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 10:20:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 22:57:46 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kaushik", "Rituraj", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.09385", "submitter": "Daniel Nissani", "authors": "Daniel N. Nissani (Nissensohn)", "title": "An Unsupervised Learning Classifier with Competitive Error Performance", "comments": "To be published at LOD 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised learning classification model is described. It achieves\nclassification error probability competitive with that of popular supervised\nlearning classifiers such as SVM or kNN. The model is based on the incremental\nexecution of small step shift and rotation operations upon selected\ndiscriminative hyperplanes at the arrival of input samples. When applied, in\nconjunction with a selected feature extractor, to a subset of the ImageNet\ndataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by\nmerely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both\nusing same feature extractor. This result may also be contrasted with popular\nunsupervised learning schemes such as k-Means which is shown to be practically\nuseless on same dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 11:12:03 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 11:47:03 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Nissani", "Daniel N.", "", "Nissensohn"]]}, {"id": "1806.09390", "submitter": "Pierre Ablin", "authors": "Pierre Ablin (PARIETAL), Jean-Fran\\c{c}ois Cardoso (IAP, CNRS),\n  Alexandre Gramfort (PARIETAL)", "title": "Accelerating likelihood optimization for ICA on real signals", "comments": null, "journal-ref": "LVA-ICA 2018, Jul 2018, Guildford, United Kingdom", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimization methods for solving the maximum likelihood formulation\nof independent component analysis (ICA). We consider both the the problem\nconstrained to white signals and the unconstrained problem. The Hessian of the\nobjective function is costly to compute, which renders Newton's method\nimpractical for large data sets. Many algorithms proposed in the literature can\nbe rewritten as quasi-Newton methods, for which the Hessian approximation is\ncheap to compute. These algorithms are very fast on simulated data where the\nlinear mixture assumption really holds. However, on real signals, we observe\nthat their rate of convergence can be severely impaired. In this paper, we\ninvestigate the origins of this behavior, and show that the recently proposed\nPreconditioned ICA for Real Data (Picard) algorithm overcomes this issue on\nboth constrained and unconstrained problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 11:31:08 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ablin", "Pierre", "", "PARIETAL"], ["Cardoso", "Jean-Fran\u00e7ois", "", "IAP, CNRS"], ["Gramfort", "Alexandre", "", "PARIETAL"]]}, {"id": "1806.09429", "submitter": "Franck Iutzeler", "authors": "Konstantin Mishchenko, Franck Iutzeler, and J\\'er\\^ome Malick", "title": "A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm", "comments": "to appear in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze an asynchronous algorithm for distributed convex\noptimization when the objective writes a sum of smooth functions, local to each\nworker, and a non-smooth function. Unlike many existing methods, our\ndistributed algorithm is adjustable to various levels of communication cost,\ndelays, machines computational power, and functions smoothness. A unique\nfeature is that the stepsizes do not depend on communication delays nor number\nof machines, which is highly desirable for scalability. We prove that the\nalgorithm converges linearly in the strongly convex case, and provide\nguarantees of convergence for the non-strongly convex case. The obtained rates\nare the same as the vanilla proximal gradient algorithm over some introduced\nepoch sequence that subsumes the delays of the system. We provide numerical\nresults on large-scale machine learning problems to demonstrate the merits of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:03:04 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 12:02:02 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 09:47:08 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Mishchenko", "Konstantin", ""], ["Iutzeler", "Franck", ""], ["Malick", "J\u00e9r\u00f4me", ""]]}, {"id": "1806.09431", "submitter": "Manan Gandhi", "authors": "Manan Gandhi, Keuntaek Lee, Yunpeng Pan, Evangelos Theodorou", "title": "Propagating Uncertainty through the tanh Function with Application to\n  Reservoir Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural networks use the tanh activation function, however when given a\nprobability distribution as input, the problem of computing the output\ndistribution in neural networks with tanh activation has not yet been\naddressed. One important example is the initialization of the echo state\nnetwork in reservoir computing, where random initialization of the reservoir\nrequires time to wash out the initial conditions, thereby wasting precious data\nand computational resources. Motivated by this problem, we propose a novel\nsolution utilizing a moment based approach to propagate uncertainty through an\nEcho State Network to reduce the washout time. In this work, we contribute two\nnew methods to propagate uncertainty through the tanh activation function and\npropose the Probabilistic Echo State Network (PESN), a method that is shown to\nhave better average performance than deterministic Echo State Networks given\nthe random initialization of reservoir states. Additionally we test single and\nmulti-step uncertainty propagation of our method on two regression tasks and\nshow that we are able to recover similar means and variances as computed by\nMonte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:06:59 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gandhi", "Manan", ""], ["Lee", "Keuntaek", ""], ["Pan", "Yunpeng", ""], ["Theodorou", "Evangelos", ""]]}, {"id": "1806.09444", "submitter": "Nikita Jaipuria", "authors": "Nikita Jaipuria, Golnaz Habibi, Jonathan P. How", "title": "A Transferable Pedestrian Motion Prediction Model for Intersections with\n  Different Geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for accurate pedestrian intent\nprediction at intersections. Given some prior knowledge of the curbside\ngeometry, the presented framework can accurately predict pedestrian\ntrajectories, even in new intersections that it has not been trained on. This\nis achieved by making use of the contravariant components of trajectories in\nthe curbside coordinate system, which ensures that the transformation of\ntrajectories across intersections is affine, regardless of the curbside\ngeometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding\n(ASNSC) formulation and we use that as a baseline to show improvement in\nprediction performance on real pedestrian datasets collected at two\nintersections in Cambridge, with distinctly different curbside and crosswalk\ngeometries. We demonstrate a 7.2% improvement in prediction accuracy in the\ncase of same train and test intersections. Furthermore, we show a comparable\nprediction performance of TASNSC when trained and tested in different\nintersections with the baseline, trained and tested on the same intersection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:19:45 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Jaipuria", "Nikita", ""], ["Habibi", "Golnaz", ""], ["How", "Jonathan P.", ""]]}, {"id": "1806.09453", "submitter": "Nikita Jaipuria", "authors": "Golnaz Habibi, Nikita Jaipuria, Jonathan P. How", "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel context-based approach for pedestrian motion\nprediction in crowded, urban intersections, with the additional flexibility of\nprediction in similar, but new, environments. Previously, Chen et. al. combined\nMarkovian-based and clustering-based approaches to learn motion primitives in a\ngrid-based world and subsequently predict pedestrian trajectories by modeling\nthe transition between learned primitives as a Gaussian Process (GP). This work\nextends that prior approach by incorporating semantic features from the\nenvironment (relative distance to curbside and status of pedestrian traffic\nlights) in the GP formulation for more accurate predictions of pedestrian\ntrajectories over the same timescale. We evaluate the new approach on\nreal-world data collected using one of the vehicles in the MIT Mobility On\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\nto quantify the span of predicted set of trajectories, such that a lower AUC\ncorresponds to a higher level of confidence in the future direction of\npedestrian motion.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:45:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Habibi", "Golnaz", ""], ["Jaipuria", "Nikita", ""], ["How", "Jonathan P.", ""]]}, {"id": "1806.09460", "submitter": "Benjamin Recht", "authors": "Benjamin Recht", "title": "A Tour of Reinforcement Learning: The View from Continuous Control", "comments": "minor revision with a few clarifying passages and corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript surveys reinforcement learning from the perspective of\noptimization and control with a focus on continuous control applications. It\nsurveys the general formulation, terminology, and typical experimental\nimplementations of reinforcement learning and reviews competing solution\nparadigms. In order to compare the relative merits of various techniques, this\nsurvey presents a case study of the Linear Quadratic Regulator (LQR) with\nunknown dynamics, perhaps the simplest and best-studied problem in optimal\ncontrol. The manuscript describes how merging techniques from learning theory\nand control can provide non-asymptotic characterizations of LQR performance and\nshows that these characterizations tend to match experimental behavior. In\nturn, when revisiting more complex applications, many of the observed phenomena\nin LQR persist. In particular, theory and experiment demonstrate the role and\nimportance of models and the cost of generality in reinforcement learning\nalgorithms. This survey concludes with a discussion of some of the challenges\nin designing learning systems that safely and reliably interact with complex\nand uncertain environments and how tools from reinforcement learning and\ncontrol might be combined to approach these challenges.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:58:20 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 15:15:27 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Recht", "Benjamin", ""]]}, {"id": "1806.09463", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw and Marco Loog", "title": "Target Robust Discriminant Analysis", "comments": "10 pages, no figures, 2 tables, 2 lemma's, 1 theorem. arXiv admin\n  note: substantial text overlap with arXiv:1706.08082 Accepted to the IAPR\n  Joint International Workshops on Statistical + Structural and Syntactic\n  Pattern Recognition (S+SSPR 2020). The final authenticated publication will\n  soon be available online", "journal-ref": null, "doi": "10.1007/978-3-030-73973-7_1", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, the data distribution at test time often differs, to a smaller\nor larger extent, from that of the original training data. Consequentially, the\nso-called source classifier, trained on the available labelled data,\ndeteriorates on the test, or target, data. Domain adaptive classifiers aim to\ncombat this problem, but typically assume some particular form of domain shift.\nMost are not robust to violations of domain shift assumptions and may even\nperform worse than their non-adaptive counterparts. We construct robust\nparameter estimators for discriminant analysis that guarantee performance\nimprovements of the adaptive classifier over the non-adaptive source\nclassifier.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 18:27:15 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 09:27:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""]]}, {"id": "1806.09464", "submitter": "Ting Chen", "authors": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Codes for Compact Embedding\n  Representations", "comments": "ICML 2018. arXiv admin note: text overlap with arXiv:1711.03067", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional embedding methods directly associate each symbol with a\ncontinuous embedding vector, which is equivalent to applying a linear\ntransformation based on a \"one-hot\" encoding of the discrete symbols. Despite\nits simplicity, such approach yields the number of parameters that grows\nlinearly with the vocabulary size and can lead to overfitting. In this work, we\npropose a much more compact K-way D-dimensional discrete encoding scheme to\nreplace the \"one-hot\" encoding. In the proposed \"KD encoding\", each symbol is\nrepresented by a $D$-dimensional code with a cardinality of $K$, and the final\nsymbol embedding vector is generated by composing the code embedding vectors.\nTo end-to-end learn semantically meaningful codes, we derive a relaxed discrete\noptimization approach based on stochastic gradient descent, which can be\ngenerally applied to any differentiable computational graph with an embedding\nlayer. In our experiments with various applications from natural language\nprocessing to graph convolutional networks, the total size of the embedding\nlayer can be reduced up to 98\\% while achieving similar or better performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 18:59:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chen", "Ting", ""], ["Min", "Martin Renqiang", ""], ["Sun", "Yizhou", ""]]}, {"id": "1806.09471", "submitter": "Alexander Rakhlin", "authors": "Mikhail Belkin and Alexander Rakhlin and Alexandre B. Tsybakov", "title": "Does data interpolation contradict statistical optimality?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that learning methods interpolating the training data can achieve\noptimal rates for the problems of nonparametric regression and prediction with\nsquare loss.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:04:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Belkin", "Mikhail", ""], ["Rakhlin", "Alexander", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1806.09504", "submitter": "Arthur Colombini Gusm\\~ao", "authors": "Arthur Colombini Gusm\\~ao, Alvaro Henrique Chaim Correia, Glauber De\n  Bona, and Fabio Gagliardi Cozman", "title": "Interpreting Embedding Models of Knowledge Bases: A Pedagogical Approach", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are employed in a variety of applications from natural\nlanguage processing to semantic web search; alas, in practice their usefulness\nis hurt by their incompleteness. Embedding models attain state-of-the-art\naccuracy in knowledge base completion, but their predictions are notoriously\nhard to interpret. In this paper, we adapt \"pedagogical approaches\" (from the\nliterature on neural networks) so as to interpret embedding models by\nextracting weighted Horn rules from them. We show how pedagogical approaches\nhave to be adapted to take upon the large-scale relational aspects of knowledge\nbases and show experimentally their strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 23:01:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gusm\u00e3o", "Arthur Colombini", ""], ["Correia", "Alvaro Henrique Chaim", ""], ["De Bona", "Glauber", ""], ["Cozman", "Fabio Gagliardi", ""]]}, {"id": "1806.09533", "submitter": "Fabrice Daniel", "authors": "Marc Velay and Fabrice Daniel", "title": "Using NLP on news headlines to predict index trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to provide a state of the art in trend prediction using\nnews headlines. We present the research done on predicting DJIA trends using\nNatural Language Processing. We will explain the different algorithms we have\nused as well as the various embedding techniques attempted. We rely on\nstatistical and deep learning models in order to extract information from the\ncorpuses.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 15:37:35 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Velay", "Marc", ""], ["Daniel", "Fabrice", ""]]}, {"id": "1806.09542", "submitter": "Wei-Hung Weng", "authors": "Wei-Hung Weng and Peter Szolovits", "title": "Mapping Unparalleled Clinical Professional and Consumer Languages with\n  Embedding Alignment", "comments": "Accepted by 2018 KDD Workshop on Machine Learning for Medicine and\n  Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping and translating professional but arcane clinical jargons to consumer\nlanguage is essential to improve the patient-clinician communication.\nResearchers have used the existing biomedical ontologies and consumer health\nvocabulary dictionary to translate between the languages. However, such\napproaches are limited by expert efforts to manually build the dictionary,\nwhich is hard to be generalized and scalable. In this work, we utilized the\nembeddings alignment method for the word mapping between unparalleled clinical\nprofessional and consumer language embeddings. To map semantically similar\nwords in two different word embeddings, we first independently trained word\nembeddings on both the corpus with abundant clinical professional terms and the\nother with mainly healthcare consumer terms. Then, we aligned the embeddings by\nthe Procrustes algorithm. We also investigated the approach with the\nadversarial training with refinement. We evaluated the quality of the alignment\nthrough the similar words retrieval both by computing the model precision and\nas well as judging qualitatively by human. We show that the Procrustes\nalgorithm can be performant for the professional consumer language embeddings\nalignment, whereas adversarial training with refinement may find some relations\nbetween two languages.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:54:45 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Weng", "Wei-Hung", ""], ["Szolovits", "Peter", ""]]}, {"id": "1806.09544", "submitter": "Cheng Mao", "authors": "Cheng Mao, Ashwin Pananjady, Martin J. Wainwright", "title": "Towards Optimal Estimation of Bivariate Isotonic Matrices with Unknown\n  Permutations", "comments": "60 pages, 1 figure. This paper is a longer version of the paper\n  arXiv:1802.09963 v3, which appeared in part as a 4-page extended abstract at\n  Conference on Learning Theory (COLT) 2018. This paper studies the problem in\n  more general settings and in another error metric. This version corrects a\n  statement in Theorem 2 of v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications, including rank aggregation, crowd-labeling, and graphon\nestimation, can be modeled in terms of a bivariate isotonic matrix with unknown\npermutations acting on its rows and/or columns. We consider the problem of\nestimating an unknown matrix in this class, based on noisy observations of\n(possibly, a subset of) its entries. We design and analyze polynomial-time\nalgorithms that improve upon the state of the art in two distinct metrics,\nshowing, in particular, that minimax optimal, computationally efficient\nestimation is achievable in certain settings. Along the way, we prove matching\nupper and lower bounds on the minimax radii of certain cone testing problems,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:55:10 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 03:55:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mao", "Cheng", ""], ["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1806.09548", "submitter": "Andreas Lindholm", "authors": "Andreas Lindholm and Fredrik Lindsten", "title": "Learning dynamical systems with particle stochastic approximation EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the particle stochastic approximation EM (PSAEM) algorithm for\nlearning of dynamical systems. The method builds on the EM algorithm, an\niterative procedure for maximum likelihood inference in latent variable models.\nBy combining stochastic approximation EM and particle Gibbs with ancestor\nsampling (PGAS), PSAEM obtains superior computational performance and\nconvergence properties compared to plain particle-smoothing-based\napproximations of the EM algorithm. PSAEM can be used for plain maximum\nlikelihood inference as well as for empirical Bayes learning of\nhyperparameters. Specifically, the latter point means that existing PGAS\nimplementations easily can be extended with PSAEM to estimate hyperparameters\nat almost no extra computational cost. We discuss the convergence properties of\nthe algorithm, and demonstrate it on several signal processing applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:12:35 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 10:38:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lindholm", "Andreas", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1806.09550", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Yuan Zhou, Xiaoyu Lu, Yee Whye Teh, Frank Wood,\n  Hongseok Yang, Jan-Willem van de Meent", "title": "Inference Trees: Adaptive Inference with Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce inference trees (ITs), a new class of inference methods that\nbuild on ideas from Monte Carlo tree search to perform adaptive sampling in a\nmanner that balances exploration with exploitation, ensures consistency, and\nalleviates pathologies in existing adaptive methods. ITs adaptively sample from\nhierarchical partitions of the parameter space, while simultaneously learning\nthese partitions in an online manner. This enables ITs to not only identify\nregions of high posterior mass, but also maintain uncertainty estimates to\ntrack regions where significant posterior mass may have been missed. ITs can be\nbased on any inference method that provides a consistent estimate of the\nmarginal likelihood. They are particularly effective when combined with\nsequential Monte Carlo, where they capture long-range dependencies and yield\nimprovements beyond proposal adaptation alone.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:13:23 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rainforth", "Tom", ""], ["Zhou", "Yuan", ""], ["Lu", "Xiaoyu", ""], ["Teh", "Yee Whye", ""], ["Wood", "Frank", ""], ["Yang", "Hongseok", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "1806.09571", "submitter": "Vladislav Z. B. Tadi\\'c", "authors": "Vladislav Z.B. Tadic and Arnaud Doucet", "title": "Asymptotic Properties of Recursive Maximum Likelihood Estimation in\n  Non-Linear State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using stochastic gradient search and the optimal filter derivative, it is\npossible to perform recursive (i.e., online) maximum likelihood estimation in a\nnon-linear state-space model. As the optimal filter and its derivative are\nanalytically intractable for such a model, they need to be approximated\nnumerically. In [Poyiadjis, Doucet and Singh, Biometrika 2018], a recursive\nmaximum likelihood algorithm based on a particle approximation to the optimal\nfilter derivative has been proposed and studied through numerical simulations.\nHere, this algorithm and its asymptotic behavior are analyzed theoretically. We\nshow that the algorithm accurately estimates maxima to the underlying (average)\nlog-likelihood when the number of particles is sufficiently large. We also\nderive (relatively) tight bounds on the estimation error. The obtained results\nhold under (relatively) mild conditions and cover several classes of non-linear\nstate-space models met in practice.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:12:53 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 15:28:05 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 23:04:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tadic", "Vladislav Z. B.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1806.09588", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Florent Krzakala, Michael I. Jordan", "title": "Fundamental limits of detection in the spiked Wigner model", "comments": "Substantial text overlap with arXiv:1710.02903. This manuscript\n  focuses on the LR fluctuations and the detection problem. The result is\n  strengthened and the proof (execution of the interpolation and cavity\n  methods) substantially simplified. Reflects more accurately the version to be\n  published", "journal-ref": "Ann. Statist., Volume 48, Number 2 (2020), 863-885", "doi": "10.1214/19-AOS1826", "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental limits of detecting the presence of an additive\nrank-one perturbation, or spike, to a Wigner matrix. When the spike comes from\na prior that is i.i.d. across coordinates, we prove that the log-likelihood\nratio of the spiked model against the non-spiked one is asymptotically normal\nbelow a certain reconstruction threshold which is not necessarily of a\n\"spectral\" nature, and that it is degenerate above. This establishes the\nmaximal region of contiguity between the planted and null models. It is known\nthat this threshold also marks a phase transition for estimating the spike: the\nlatter task is possible above the threshold and impossible below. Therefore,\nboth estimation and detection undergo the same transition in this random matrix\nmodel. We also provide further information about the performance of the optimal\ntest. Our proofs are based on Gaussian interpolation methods and a rigorous\nincarnation of the cavity method, as devised by Guerra and Talagrand in their\nstudy of the Sherrington--Kirkpatrick spin-glass model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:35:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Krzakala", "Florent", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1806.09597", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Daniel Duckworth, Semon Rezchikov, Quoc V. Le and\n  Jascha Sohl-Dickstein", "title": "Stochastic natural gradient descent draws posterior samples in function\n  space", "comments": "Workshop on Bayesian Deep Learning (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has argued that stochastic gradient descent can approximate the\nBayesian uncertainty in model parameters near local minima. In this work we\ndevelop a similar correspondence for minibatch natural gradient descent (NGD).\nWe prove that for sufficiently small learning rates, if the model predictions\non the training set approach the true conditional distribution of labels given\ninputs, the stationary distribution of minibatch NGD approaches a Bayesian\nposterior near local minima. The temperature $T = \\epsilon N / (2B)$ is\ncontrolled by the learning rate $\\epsilon$, training set size $N$ and batch\nsize $B$. However minibatch NGD is not parameterisation invariant and it does\nnot sample a valid posterior away from local minima. We therefore propose a\nnovel optimiser, \"stochastic NGD\", which introduces the additional correction\nterms required to preserve both properties.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:47:42 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 01:10:14 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 15:04:58 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 18:07:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Smith", "Samuel L.", ""], ["Duckworth", "Daniel", ""], ["Rezchikov", "Semon", ""], ["Le", "Quoc V.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.09602", "submitter": "Thomas K\\\"ustner", "authors": "Thomas K\\\"ustner, Sergios Gatidis, Annika Liebgott, Martin Schwartz,\n  Lukas Mauch, Petros Martirosian, Holger Schmidt, Nina F. Schwenzer,\n  Konstantin Nikolaou, Fabian Bamberg, Bin Yang, Fritz Schick", "title": "A Machine-learning framework for automatic reference-free quality\n  assessment in MRI", "comments": null, "journal-ref": null, "doi": "10.1016/j.mri.2018.07.003", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.\nA large amount of data is created per examination which needs to be checked for\nsufficient quality in order to derive a meaningful diagnosis. This is a manual\nprocess and therefore time- and cost-intensive. Any imaging artifacts\noriginating from scanner hardware, signal processing or induced by the patient\nmay reduce the image quality and complicate the diagnosis or any image\npost-processing. Therefore, the assessment or the ensurance of sufficient image\nquality in an automated manner is of high interest. Usually no reference image\nis available or difficult to define. Therefore, classical reference-based\napproaches are not applicable. Model observers mimicking the human observers\n(HO) can assist in this task. Thus, we propose a new machine-learning-based\nreference-free MR image quality assessment framework which is trained on\nHO-derived labels to assess MR image quality immediately after each\nacquisition. We include the concept of active learning and present an efficient\nblinded reading platform to reduce the effort in the HO labeling procedure.\nDerived image features and the applied classifiers (support-vector-machine,\ndeep neural network) are investigated for a cohort of 250 patients. The MR\nimage quality assessment framework can achieve a high test accuracy of 93.7$\\%$\nfor estimating quality classes on a 5-point Likert-scale. The proposed MR image\nquality assessment framework is able to provide an accurate and efficient\nquality estimation which can be used as a prospective quality assurance\nincluding automatic acquisition adaptation or guided MR scanner operation,\nand/or as a retrospective quality assessment including support of diagnostic\ndecisions or quality control in cohort studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:56:32 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 09:53:40 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["K\u00fcstner", "Thomas", ""], ["Gatidis", "Sergios", ""], ["Liebgott", "Annika", ""], ["Schwartz", "Martin", ""], ["Mauch", "Lukas", ""], ["Martirosian", "Petros", ""], ["Schmidt", "Holger", ""], ["Schwenzer", "Nina F.", ""], ["Nikolaou", "Konstantin", ""], ["Bamberg", "Fabian", ""], ["Yang", "Bin", ""], ["Schick", "Fritz", ""]]}, {"id": "1806.09605", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Junhyuk Oh, Satinder Singh", "title": "Many-Goals Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-goals updating exploits the off-policy nature of Q-learning to update all\npossible goals an agent could have from each transition in the world, and was\nintroduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work\nthis was mostly explored in small-state RL problems that allowed tabular\nrepresentations and where all possible goals could be explicitly enumerated and\nlearned separately. In this paper we empirically explore 3 different extensions\nof the idea of updating many (instead of all) goals in the context of RL with\ndeep neural networks (or DeepRL for short). First, in a direct adaptation of\nKaelbling's approach we explore if many-goals updating can be used to achieve\nmastery in non-tabular visual-observation domains. Second, we explore whether\nmany-goals updating can be used to pre-train a network to subsequently learn\nfaster and better on a single main task of interest. Third, we explore whether\nmany-goals updating can be used to provide auxiliary task updates in training a\nnetwork to learn faster and better on a single main task of interest. We\nprovide comparisons to baselines for each of the 3 extensions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:31:24 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Veeriah", "Vivek", ""], ["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""]]}, {"id": "1806.09614", "submitter": "Olivier Sigaud", "authors": "Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves\n  Oudeyer", "title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new form of automated curriculum learning\nbased on adaptive selection of accuracy requirements, called accuracy-based\ncurriculum learning. Using a reinforcement learning agent based on the Deep\nDeterministic Policy Gradient algorithm and addressing the Reacher environment,\nwe first show that an agent trained with various accuracy requirements sampled\nrandomly learns more efficiently than when asked to be very accurate at all\ntimes. Then we show that adaptive selection of accuracy requirements, based on\na local measure of competence progress, automatically generates a curriculum\nwhere difficulty progressively increases, resulting in a better learning\nefficiency than sampling randomly.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 12:06:28 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 11:20:05 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Fournier", "Pierre", ""], ["Sigaud", "Olivier", ""], ["Chetouani", "Mohamed", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1806.09655", "submitter": "Oleh Rybkin", "authors": "Oleh Rybkin, Karl Pertsch, Konstantinos G. Derpanis, Kostas\n  Daniilidis, and Andrew Jaegle", "title": "Learning what you can do before doing anything", "comments": "Published at ICLR 2019. 10 pages + 15 pages of references and\n  appendices", "journal-ref": "International Conference on Learning Representations, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents can learn to represent the action spaces of other agents\nsimply by observing them act. Such representations help agents quickly learn to\npredict the effects of their own actions on the environment and to plan complex\naction sequences. In this work, we address the problem of learning an agent's\naction space purely from visual observation. We use stochastic video prediction\nto learn a latent variable that captures the scene's dynamics while being\nminimally sensitive to the scene's static content. We introduce a loss term\nthat encourages the network to capture the composability of visual sequences\nand show that it leads to representations that disentangle the structure of\nactions. We call the full model with composable action representations\nComposable Learned Action Space Predictor (CLASP). We show the applicability of\nour method to synthetic settings and its potential to capture action spaces in\ncomplex, realistic visual settings. When used in a semi-supervised setting, our\nlearned representations perform comparably to existing fully supervised methods\non tasks such as action-conditioned video prediction and planning in the\nlearned action space, while requiring orders of magnitude fewer action labels.\nProject website: https://daniilidis-group.github.io/learned_action_spaces\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 18:33:34 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 18:53:33 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Rybkin", "Oleh", ""], ["Pertsch", "Karl", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""], ["Jaegle", "Andrew", ""]]}, {"id": "1806.09679", "submitter": "Behzad Salami", "authors": "Behzad Salami, Osman Unsal, Adrian Cristal", "title": "On the Resilience of RTL NN Accelerators: Fault Characterization and\n  Mitigation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is making a strong resurgence in tune with the massive\ngeneration of unstructured data which in turn requires massive computational\nresources. Due to the inherently compute- and power-intensive structure of\nNeural Networks (NNs), hardware accelerators emerge as a promising solution.\nHowever, with technology node scaling below 10nm, hardware accelerators become\nmore susceptible to faults, which in turn can impact the NN accuracy. In this\npaper, we study the resilience aspects of Register-Transfer Level (RTL) model\nof NN accelerators, in particular, fault characterization and mitigation. By\nfollowing a High-Level Synthesis (HLS) approach, first, we characterize the\nvulnerability of various components of RTL NN. We observed that the severity of\nfaults depends on both i) application-level specifications, i.e., NN data\n(inputs, weights, or intermediate), NN layers, and NN activation functions, and\nii) architectural-level specifications, i.e., data representation model and the\nparallelism degree of the underlying accelerator. Second, motivated by\ncharacterization results, we present a low-overhead fault mitigation technique\nthat can efficiently correct bit flips, by 47.3% better than state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 08:52:18 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Salami", "Behzad", ""], ["Unsal", "Osman", ""], ["Cristal", "Adrian", ""]]}, {"id": "1806.09692", "submitter": "Benjamin Goldstein", "authors": "Benjamin A. Goldstein, Matthew Phelan, Neha J. Pagidipati, Rury R.\n  Holman, Michael J. Pencina Elizabeth A Stuart", "title": "An Outcome Model Approach to Translating a Randomized Controlled Trial\n  Results to a Target Population", "comments": "2 Tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participants enrolled into randomized controlled trials (RCTs) often do not\nreflect real-world populations. Previous research in how best to translate RCT\nresults to target populations has focused on weighting RCT data to look like\nthe target data. Simulation work, however, has suggested that an outcome model\napproach may be preferable. Here we describe such an approach using source data\nfrom the 2x2 factorial NAVIGATOR trial which evaluated the impact of valsartan\nand nateglinide on cardiovascular outcomes and new-onset diabetes in a\npre-diabetic population. Our target data consisted of people with pre-diabetes\nserviced at our institution. We used Random Survival Forests to develop\nseparate outcome models for each of the 4 treatments, estimating the 5-year\nrisk difference for progression to diabetes and estimated the treatment effect\nin our local patient populations, as well as sub-populations, and the results\ncompared to the traditional weighting approach. Our models suggested that the\ntreatment effect for valsartan in our patient population was the same as in the\ntrial, whereas for nateglinide treatment effect was stronger than observed in\nthe original trial. Our effect estimates were more efficient than the weighting\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 20:33:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Goldstein", "Benjamin A.", ""], ["Phelan", "Matthew", ""], ["Pagidipati", "Neha J.", ""], ["Holman", "Rury R.", ""], ["Stuart", "Michael J. Pencina Elizabeth A", ""]]}, {"id": "1806.09708", "submitter": "Arman Rahimzamani", "authors": "Rajat Sen, Karthikeyan Shanmugam, Himanshu Asnani, Arman Rahimzamani,\n  Sreeram Kannan", "title": "Mimic and Classify : A meta-algorithm for Conditional Independence\n  Testing", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given independent samples generated from the joint distribution\n$p(\\mathbf{x},\\mathbf{y},\\mathbf{z})$, we study the problem of Conditional\nIndependence (CI-Testing), i.e., whether the joint equals the CI distribution\n$p^{CI}(\\mathbf{x},\\mathbf{y},\\mathbf{z})= p(\\mathbf{z})\np(\\mathbf{y}|\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})$ or not. We cast this problem\nunder the purview of the proposed, provable meta-algorithm, \"Mimic and\nClassify\", which is realized in two-steps: (a) Mimic the CI distribution close\nenough to recover the support, and (b) Classify to distinguish the joint and\nthe CI distribution. Thus, as long as we have a good generative model and a\ngood classifier, we potentially have a sound CI Tester. With this modular\nparadigm, CI Testing becomes amiable to be handled by state-of-the-art, both\ngenerative and classification methods from the modern advances in Deep\nLearning, which in general can handle issues related to curse of dimensionality\nand operation in small sample regime. We show intensive numerical experiments\non synthetic and real datasets where new mimic methods such conditional GANs,\nRegression with Neural Nets, outperform the current best CI Testing performance\nin the literature. Our theoretical results provide analysis on the estimation\nof null distribution as well as allow for general measures, i.e., when either\nsome of the random variables are discrete and some are continuous or when one\nor more of them are discrete-continuous mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 21:24:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Asnani", "Himanshu", ""], ["Rahimzamani", "Arman", ""], ["Kannan", "Sreeram", ""]]}, {"id": "1806.09710", "submitter": "Kush Varshney", "authors": "Kush R. Varshney, Prashant Khanduri, Pranay Sharma, Shan Zhang, Pramod\n  K. Varshney", "title": "Why Interpretability in Machine Learning? An Answer Using Distributed\n  Detection and Data Fusion Theory", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence is increasingly affecting all parts of society and\nlife, there is growing recognition that human interpretability of machine\nlearning models is important. It is often argued that accuracy or other similar\ngeneralization performance metrics must be sacrificed in order to gain\ninterpretability. Such arguments, however, fail to acknowledge that the overall\ndecision-making system is composed of two entities: the learned model and a\nhuman who fuses together model outputs with his or her own information. As\nsuch, the relevant performance criteria should be for the entire system, not\njust for the machine learning component. In this work, we characterize the\nperformance of such two-node tandem data fusion systems using the theory of\ndistributed detection. In doing so, we work in the population setting and model\ninterpretable learned models as multi-level quantizers. We prove that under our\nabstraction, the overall system of a human with an interpretable classifier\noutperforms one with a black box classifier.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 21:37:21 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Varshney", "Kush R.", ""], ["Khanduri", "Prashant", ""], ["Sharma", "Pranay", ""], ["Zhang", "Shan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1806.09712", "submitter": "Stefano Favaro", "authors": "Fadhel Ayed, Marco Battiston, Federico Camerlenghi, Stefano Favaro", "title": "On consistent estimation of the missing mass", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ samples from a population of individuals belonging to different\ntypes with unknown proportions, how do we estimate the probability of\ndiscovering a new type at the $(n+1)$-th draw? This is a classical problem in\nstatistics, commonly referred to as the missing mass estimation problem. Recent\nresults by Ohannessian and Dahleh \\citet{Oha12} and Mossel and Ohannessian\n\\citet{Mos15} showed: i) the impossibility of estimating (learning) the missing\nmass without imposing further structural assumptions on the type proportions;\nii) the consistency of the Good-Turing estimator for the missing mass under the\nassumption that the tail of the type proportions decays to zero as a regularly\nvarying function with parameter $\\alpha\\in(0,1)$. In this paper we rely on\ntools from Bayesian nonparametrics to provide an alternative, and simpler,\nproof of the impossibility of a distribution-free estimation of the missing\nmass. Up to our knowledge, the use of Bayesian ideas to study large sample\nasymptotics for the missing mass is new, and it could be of independent\ninterest. Still relying on Bayesian nonparametric tools, we then show that\nunder regularly varying type proportions the convergence rate of the\nGood-Turing estimator is the best rate that any estimator can achieve, up to a\nslowly varying function, and that minimax rate must be at least\n$n^{-\\alpha/2}$. We conclude with a discussion of our results, and by\nconjecturing that the Good-Turing estimator is an rate optimal minimax\nestimator under regularly varying type proportions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 22:02:35 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ayed", "Fadhel", ""], ["Battiston", "Marco", ""], ["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""]]}, {"id": "1806.09730", "submitter": "Jens Behrmann", "authors": "Jens Behrmann, S\\\"oren Dittmer, Pascal Fernsel, Peter Maa{\\ss}", "title": "Analysis of Invariance and Robustness via Invertibility of ReLU-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the invertibility of deep neural networks (DNNs) provides a\nprincipled approach to better understand the behavior of these powerful models.\nDespite being a promising diagnostic tool, a consistent theory on their\ninvertibility is still lacking. We derive a theoretically motivated approach to\nexplore the preimages of ReLU-layers and mechanisms affecting the stability of\nthe inverse. Using the developed theory, we numerically show how this approach\nuncovers characteristic properties of the network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 23:57:19 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 16:55:32 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Behrmann", "Jens", ""], ["Dittmer", "S\u00f6ren", ""], ["Fernsel", "Pascal", ""], ["Maa\u00df", "Peter", ""]]}, {"id": "1806.09736", "submitter": "Amir Karami", "authors": "Amir Karami and Noelle M. Pendergraft", "title": "Computational Analysis of Insurance Complaints: GEICO Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online environment has provided a great opportunity for insurance\npolicyholders to share their complaints with respect to different services.\nThese complaints can reveal valuable information for insurance companies who\nseek to improve their services; however, analyzing a huge number of online\ncomplaints is a complicated task for human and must involve computational\nmethods to create an efficient process. This research proposes a computational\napproach to characterize the major topics of a large number of online\ncomplaints. Our approach is based on using the topic modeling approach to\ndisclose the latent semantic of complaints. The proposed approach deployed on\nthousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates\nthat there are 30 major complains in four categories: (1) customer service, (2)\ninsurance coverage, paperwork, policy, and reports, (3) legal issues, and (4)\ncosts, estimates, and payments. This research approach can be used in other\napplications to explore a large number of reviews.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:12:14 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Karami", "Amir", ""], ["Pendergraft", "Noelle M.", ""]]}, {"id": "1806.09737", "submitter": "Xi Zhang", "authors": "Xi Sheryl Zhang, Dandi Chen, Yongjun Zhu, Chao Che, Chang Su, Sendong\n  Zhao, Xu Min, Fei Wang", "title": "A Multi-View Ensemble Classification Model for Clinically Actionable\n  Genetic Mutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents details of our winning solutions to the task IV of NIPS\n2017 Competition Track entitled Classifying Clinically Actionable Genetic\nMutations. The machine learning task aims to classify genetic mutations based\non text evidence from clinical literature with promising performance. We\ndevelop a novel multi-view machine learning framework with ensemble\nclassification models to solve the problem. During the Challenge, feature\ncombinations derived from three views including document view, entity text\nview, and entity name view, which complements each other, are comprehensively\nexplored. As the final solution, we submitted an ensemble of nine basic\ngradient boosting models which shows the best performance in the evaluation.\nThe approach scores 0.5506 and 0.6694 in terms of logarithmic loss on a fixed\nsplit in stage-1 testing phase and 5-fold cross validation respectively, which\nalso makes us ranked as a top-1 team out of more than 1,300 solutions in NIPS\n2017 Competition Track IV.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:17:15 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 21:36:49 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhang", "Xi Sheryl", ""], ["Chen", "Dandi", ""], ["Zhu", "Yongjun", ""], ["Che", "Chao", ""], ["Su", "Chang", ""], ["Zhao", "Sendong", ""], ["Min", "Xu", ""], ["Wang", "Fei", ""]]}, {"id": "1806.09748", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong\n  Chul Ye", "title": "Cycle Consistent Adversarial Denoising Network for Multiphase Coronary\n  CT Angiography", "comments": "This work is accepted in Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13284", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In coronary CT angiography, a series of CT images are taken at different\nlevels of radiation dose during the examination. Although this reduces the\ntotal radiation dose, the image quality during the low-dose phases is\nsignificantly degraded. To address this problem, here we propose a novel\nsemi-supervised learning technique that can remove the noises of the CT images\nobtained in the low-dose phases by learning from the CT images in the routine\ndose phases. Although a supervised learning approach is not possible due to the\ndifferences in the underlying heart structure in two phases, the images in the\ntwo phases are closely related so that we propose a cycle-consistent\nadversarial denoising network to learn the non-degenerate mapping between the\nlow and high dose cardiac phases. Experimental results showed that the proposed\nmethod effectively reduces the noise in the low-dose CT image while the\npreserving detailed texture and edge information. Moreover, thanks to the\ncyclic consistency and identity loss, the proposed network does not create any\nartificial features that are not present in the input images. Visual grading\nand quality evaluation also confirm that the proposed method provides\nsignificant improvement in diagnostic quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 01:17:51 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:14:42 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 14:21:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Kang", "Eunhee", ""], ["Koo", "Hyun Jung", ""], ["Yang", "Dong Hyun", ""], ["Seo", "Joon Bum", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.09762", "submitter": "Yichen Zhou", "authors": "Yichen Zhou, Giles Hooker", "title": "Boulevard: Regularized Stochastic Gradient Boosted Trees and Their\n  Limiting Distribution", "comments": "45 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a novel gradient boosting framework for regression. We\nregularize gradient boosted trees by introducing subsampling and employ a\nmodified shrinkage algorithm so that at every boosting stage the estimate is\ngiven by an average of trees. The resulting algorithm, titled Boulevard, is\nshown to converge as the number of trees grows. We also demonstrate a central\nlimit theorem for this limit, allowing a characterization of uncertainty for\npredictions. A simulation study and real world examples provide support for\nboth the predictive accuracy of the model and its limiting behavior.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:22:13 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 15:11:58 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 04:11:02 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zhou", "Yichen", ""], ["Hooker", "Giles", ""]]}, {"id": "1806.09764", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui\n  Qin, Haoye Dong, Eric Xing", "title": "Deep Generative Models with Learnable Knowledge Constraints", "comments": "Neural Information Processing Systems (NeurIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:31:35 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:10:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Salakhutdinov", "Ruslan", ""], ["Liang", "Xiaodan", ""], ["Qin", "Lianhui", ""], ["Dong", "Haoye", ""], ["Xing", "Eric", ""]]}, {"id": "1806.09777", "submitter": "Raman Arora", "authors": "Poorya Mianjy, Raman Arora, Rene Vidal", "title": "On the Implicit Bias of Dropout", "comments": "17 pages, 3 figures, In Proceedings of the Thirty-fifth International\n  Conference on Machine Learning (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic approaches endow deep learning systems with implicit bias that\nhelps them generalize even in over-parametrized settings. In this paper, we\nfocus on understanding such a bias induced in learning through dropout, a\npopular technique to avoid overfitting in deep learning. For single\nhidden-layer linear neural networks, we show that dropout tends to make the\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\naddition, we provide a complete characterization of the optimization landscape\ninduced by dropout.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:08:21 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Mianjy", "Poorya", ""], ["Arora", "Raman", ""], ["Vidal", "Rene", ""]]}, {"id": "1806.09780", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin, Adrian Wills, Brett Ninness", "title": "Correlated pseudo-marginal Metropolis-Hastings using quasi-Newton\n  proposals", "comments": "45 pages and 11 figures. Submitted to journal. Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-marginal Metropolis-Hastings (pmMH) is a versatile algorithm for\nsampling from target distributions which are not easy to evaluate point-wise.\nHowever, pmMH requires good proposal distributions to sample efficiently from\nthe target, which can be problematic to construct in practice. This is\nespecially a problem for high-dimensional targets when the standard random-walk\nproposal is inefficient. We extend pmMH to allow for constructing the proposal\nbased on information from multiple past iterations. As a consequence,\nquasi-Newton (qN) methods can be employed to form proposals which utilize\ngradient information to guide the Markov chain to areas of high probability and\nto construct approximations of the local curvature to scale step sizes. The\nproposed method is demonstrated on several problems which indicate that qN\nproposals can perform better than other common Hessian-based proposals.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:17:45 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 01:21:24 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Dahlin", "Johan", ""], ["Wills", "Adrian", ""], ["Ninness", "Brett", ""]]}, {"id": "1806.09783", "submitter": "Heeyoul Choi", "authors": "Sangchul Hahn and Heeyoul Choi", "title": "Understanding Dropout as an Optimization Trick", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of standard approaches to train deep neural networks, dropout has been\napplied to regularize large models to avoid overfitting, and the improvement in\nperformance by dropout has been explained as avoiding co-adaptation between\nnodes. However, when correlations between nodes are compared after training the\nnetworks with or without dropout, one question arises if co-adaptation\navoidance explains the dropout effect completely. In this paper, we propose an\nadditional explanation of why dropout works and propose a new technique to\ndesign better activation functions. First, we show that dropout can be\nexplained as an optimization technique to push the input towards the saturation\narea of nonlinear activation function by accelerating gradient information\nflowing even in the saturation area in backpropagation. Based on this\nexplanation, we propose a new technique for activation functions, {\\em gradient\nacceleration in activation function (GAAF)}, that accelerates gradients to flow\neven in the saturation area. Then, input to the activation function can climb\nonto the saturation area which makes the network more robust because the model\nconverges on a flat region. Experiment results support our explanation of\ndropout and confirm that the proposed GAAF technique improves image\nclassification performance with expected properties.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:43:17 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 09:11:43 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 06:27:52 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Hahn", "Sangchul", ""], ["Choi", "Heeyoul", ""]]}, {"id": "1806.09795", "submitter": "Xiaomin Lin", "authors": "Xiaomin Lin and Stephen C. Adams and Peter A. Beling", "title": "Multi-agent Inverse Reinforcement Learning for Certain General-sum\n  Stochastic Games", "comments": "30 pages", "journal-ref": "Journal of Artificial Intelligence Research 66 (2019), pp 473-502", "doi": "10.1613/jair.1.11541", "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of multi-agent inverse reinforcement\nlearning (MIRL) in a two-player general-sum stochastic game framework. Five\nvariants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and\nuNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a\ncooperative game in which the agents employ cooperative strategies that aim to\nmaximize the total game value. In problem uCE-MIRL, agents are assumed to\nfollow strategies that constitute a correlated equilibrium while maximizing\ntotal game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value\nmaximization, but it is assumed that the agents are playing a Nash equilibrium.\nProblems advE-MIRL and cooE-MIRL assume agents are playing an adversarial\nequilibrium and a coordination equilibrium, respectively. We propose novel\napproaches to address these five problems under the assumption that the game\nobserver either knows or is able to accurate estimate the policies and solution\nconcepts for players. For uCS-MIRL, we first develop a characteristic set of\nsolutions ensuring that the observed bi-policy is a uCS and then apply a\nBayesian inverse learning method. For uCE-MIRL, we develop a linear programming\nproblem subject to constraints that define necessary and sufficient conditions\nfor the observed policies to be correlated equilibria. The objective is to\nchoose a solution that not only minimizes the total game value difference\nbetween the observed bi-policy and a local uCS, but also maximizes the scale of\nthe solution. We apply a similar treatment to the problem of uNE-MIRL. The\nremaining two problems can be solved efficiently by taking advantage of\nsolution uniqueness and setting up a convex optimization problem. Results are\nvalidated on various benchmark grid-world games.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 05:14:13 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 01:35:32 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 01:32:22 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lin", "Xiaomin", ""], ["Adams", "Stephen C.", ""], ["Beling", "Peter A.", ""]]}, {"id": "1806.09823", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Piotr Indyk, Ilya Razenshteyn", "title": "Approximate Nearest Neighbor Search in High Dimensions", "comments": "27 pages, no figures; to appear in the proceedings of ICM 2018\n  (accompanying the talk by P. Indyk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor problem is defined as follows: Given a set $P$ of $n$\npoints in some metric space $(X,D)$, build a data structure that, given any\npoint $q$, returns a point in $P$ that is closest to $q$ (its \"nearest\nneighbor\" in $P$). The data structure stores additional information about the\nset $P$, which is then used to find the nearest neighbor without computing all\ndistances between $q$ and $P$. The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point $p' \\in P$ such\nthat the distance from $q$ to $p'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$,\nfor some $c \\geq 1$. Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:35:45 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Andoni", "Alexandr", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1806.09827", "submitter": "Sim\\'on Roca-Sotelo", "authors": "Sim\\'on Roca-Sotelo and Jer\\'onimo Arenas-Garc\\'ia", "title": "Unveiling the semantic structure of text documents using paragraph-aware\n  Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic Topic Models are built under the Bag Of Words assumption, in which\nword position is ignored for simplicity. Besides, symmetric priors are\ntypically used in most applications. In order to easily learn topics with\ndifferent properties among the same corpus, we propose a new line of work in\nwhich the paragraph structure is exploited. Our proposal is based on the\nfollowing assumption: in many text document corpora there are formal\nconstraints shared across all the collection, e.g. sections. When this\nassumption is satisfied, some paragraphs may be related to general concepts\nshared by all documents in the corpus, while others would contain the genuine\ndescription of documents. Assuming each paragraph can be semantically more\ngeneral, specific, or hybrid, we look for ways to measure this, transferring\nthis distinction to topics and being able to learn what we call specific and\ngeneral topics. Experiments show that this is a proper methodology to highlight\ncertain paragraphs in structured documents at the same time we learn\ninteresting and more diverse topics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:50:37 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Roca-Sotelo", "Sim\u00f3n", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""]]}, {"id": "1806.09842", "submitter": "Pan Li", "authors": "Pan Li, Niao He, Olgica Milenkovic", "title": "Quadratic Decomposable Submodular Function Minimization", "comments": "A part of this work will be presented in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new convex optimization problem, termed quadratic decomposable\nsubmodular function minimization. The problem is closely related to\ndecomposable submodular function minimization and arises in many learning on\ngraphs and hypergraphs settings, such as graph-based semi-supervised learning\nand PageRank. We approach the problem via a new dual strategy and describe an\nobjective that may be optimized via random coordinate descent (RCD) methods and\nprojections onto cones. We also establish the linear convergence rate of the\nRCD algorithm and develop efficient projection algorithms with provable\nperformance guarantees. Numerical experiments in semi-supervised learning on\nhypergraphs confirm the efficiency of the proposed algorithm and demonstrate\nthe significant improvements in prediction accuracy with respect to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:47:05 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 07:27:42 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 16:24:23 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Li", "Pan", ""], ["He", "Niao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1806.09856", "submitter": "Evgenii Tsymbalov", "authors": "Evgenii Tsymbalov, Maxim Panov, Alexander Shapeev", "title": "Dropout-based Active Learning for Regression", "comments": "Report on AIST 2018; will be published in Springer LNCS series\n  (Analysis of Images, Social Networks and Texts - 7th International\n  Conference, AIST 2018)", "journal-ref": "Analysis of Images, Social Networks and Texts - 7th International\n  Conference, AIST 2018, Lecture Notes in Computer Science book series (LNCS),\n  volume 11179, pp. 247-258", "doi": "10.1007/978-3-030-11027-7_24", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is relevant and challenging for high-dimensional regression\nmodels when the annotation of the samples is expensive. Yet most of the\nexisting sampling methods cannot be applied to large-scale problems, consuming\ntoo much time for data processing. In this paper, we propose a fast active\nlearning algorithm for regression, tailored for neural network models. It is\nbased on uncertainty estimation from stochastic dropout output of the network.\nExperiments on both synthetic and real-world datasets show comparable or better\nperformance (depending on the accuracy metric) as compared to the baselines.\nThis approach can be generalized to other deep learning architectures. It can\nbe used to systematically improve a machine-learning model as it offers a\ncomputationally efficient way of sampling additional data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 09:06:40 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:42:19 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Tsymbalov", "Evgenii", ""], ["Panov", "Maxim", ""], ["Shapeev", "Alexander", ""]]}, {"id": "1806.09888", "submitter": "Michael Murray", "authors": "Michael Murray, Jared Tanner", "title": "Towards an understanding of CNNs: analysing the recovery of activation\n  pathways via Deep Convolutional Sparse Coding", "comments": "Long version (8 pages excluding references) of paper accepted at the\n  IEEE 2018 Data Science Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep\nconvolutional neural networks (DCNNs), but by omitting the learning of the\ndictionaries one can more transparently analyse the role of the activation\nfunction and its ability to recover activation paths through the layers.\nPapyan, Romano, and Elad conducted an analysis of such an architecture,\ndemonstrated the relationship with DCNNs and proved conditions under which the\nD-CSC is guaranteed to recover specific activation paths. A technical\ninnovation of their work highlights that one can view the efficacy of the ReLU\nnonlinear activation function of a DCNN through a new variant of the tensor's\nsparsity, referred to as stripe-sparsity. Using this they proved that\nrepresentations with an activation density proportional to the ambient\ndimension of the data are recoverable. We extend their uniform guarantees to a\nmodified model and prove that with high probability the true activation is\ntypically possible to recover for a greater density of activations per layer.\nOur extension follows from incorporating the prior work on one step\nthresholding by Schnass and Vandergheynst.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 10:21:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 15:13:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Murray", "Michael", ""], ["Tanner", "Jared", ""]]}, {"id": "1806.09905", "submitter": "Vijay Thakkar", "authors": "Rachel Manzelli, Vijay Thakkar, Ali Siahkamari, Brian Kulis", "title": "Conditioning Deep Generative Raw Audio Models for Structured Automatic\n  Music", "comments": "Presented at the ISMIR 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing automatic music generation approaches that feature deep learning can\nbe broadly classified into two types: raw audio models and symbolic models.\nSymbolic models, which train and generate at the note level, are currently the\nmore prevalent approach; these models can capture long-range dependencies of\nmelodic structure, but fail to grasp the nuances and richness of raw audio\ngenerations. Raw audio models, such as DeepMind's WaveNet, train directly on\nsampled audio waveforms, allowing them to produce realistic-sounding, albeit\nunstructured music. In this paper, we propose an automatic music generation\nmethodology combining both of these approaches to create structured,\nrealistic-sounding compositions. We consider a Long Short Term Memory network\nto learn the melodic structure of different styles of music, and then use the\nunique symbolic generations from this model as a conditioning input to a\nWaveNet-based raw audio generator, creating a model for automatic, novel music.\nWe then evaluate this approach by showcasing results of this work.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:10:19 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Manzelli", "Rachel", ""], ["Thakkar", "Vijay", ""], ["Siahkamari", "Ali", ""], ["Kulis", "Brian", ""]]}, {"id": "1806.09908", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Carlo Ciliberto, Gian Maria Marconi, Lorenzo Rosasco", "title": "Manifold Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction provides a general framework to deal with supervised\nproblems where the outputs have semantically rich structure. While classical\napproaches consider finite, albeit potentially huge, output spaces, in this\npaper we discuss how structured prediction can be extended to a continuous\nscenario. Specifically, we study a structured prediction approach to manifold\nvalued regression. We characterize a class of problems for which the considered\napproach is statistically consistent and study how geometric optimization can\nbe used to compute the corresponding estimator. Promising experimental results\non both simulated and real data complete our study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:12:58 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Rudi", "Alessandro", ""], ["Ciliberto", "Carlo", ""], ["Marconi", "Gian Maria", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1806.09918", "submitter": "Jakub Tomczak Ph.D.", "authors": "Philip Botros and Jakub M. Tomczak", "title": "Hierarchical VampPrior Variational Fair Auto-Encoder", "comments": "ICML Workshop on Theoretical Foundations and Applications of Deep\n  Generative Models 2018, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making is a process that is extremely prone to different biases. In\nthis paper we consider learning fair representations that aim at removing\nnuisance (sensitive) information from the decision process. For this purpose,\nwe propose to use deep generative modeling and adapt a hierarchical Variational\nAuto-Encoder to learn these fair representations. Moreover, we utilize the\nmutual information as a useful regularizer for enforcing fairness of a\nrepresentation. In experiments on two benchmark datasets and two scenarios\nwhere the sensitive variables are fully and partially observable, we show that\nthe proposed approach either outperforms or performs on par with the current\nbest model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:31:47 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 14:02:29 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Botros", "Philip", ""], ["Tomczak", "Jakub M.", ""]]}, {"id": "1806.09919", "submitter": "Fredrik Bagge Carlson", "authors": "Fredrik Bagge Carlson, Rolf Johansson, Anders Robertsson", "title": "Tangent-Space Regularization for Neural-Network Models of Dynamical\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the concept of tangent space regularization for\nneural-network models of dynamical systems. The tangent space to the dynamics\nfunction of many physical systems of interest in control applications exhibits\nuseful properties, e.g., smoothness, motivating regularization of the model\nJacobian along system trajectories using assumptions on the tangent space of\nthe dynamics. Without assumptions, large amounts of training data are required\nfor a neural network to learn the full non-linear dynamics without overfitting.\nWe compare different network architectures on one-step prediction and\nsimulation performance and investigate the propensity of different\narchitectures to learn models with correct input-output Jacobian. Furthermore,\nthe influence of $L_2$ weight regularization on the learned Jacobian eigenvalue\nspectrum, and hence system stability, is investigated.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:35:16 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Carlson", "Fredrik Bagge", ""], ["Johansson", "Rolf", ""], ["Robertsson", "Anders", ""]]}, {"id": "1806.09976", "submitter": "Brian Karrer", "authors": "Carlos Alberto Gomez-Uribe, Brian Karrer", "title": "The decoupled extended Kalman filter for dynamic exponential-family\n  factorization models", "comments": "29 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research (JMLR), 22(5):1-25, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the needs of online large-scale recommender systems, we\nspecialize the decoupled extended Kalman filter (DEKF) to factorization models,\nincluding factorization machines, matrix and tensor factorization, and\nillustrate the effectiveness of the approach through numerical experiments on\nsynthetic and on real-world data. Online learning of model parameters through\nthe DEKF makes factorization models more broadly useful by (i) allowing for\nmore flexible observations through the entire exponential family, (ii) modeling\nparameter drift, and (iii) producing parameter uncertainty estimates that can\nenable explore/exploit and other applications. We use a different parameter\ndynamics than the standard DEKF, allowing parameter drift while encouraging\nreasonable values. We also present an alternate derivation of the extended\nKalman filter and DEKF that highlights the role of the Fisher information\nmatrix in the EKF.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:41:10 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:08:16 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Gomez-Uribe", "Carlos Alberto", ""], ["Karrer", "Brian", ""]]}, {"id": "1806.09981", "submitter": "Margarita Osadchy", "authors": "Jinchao Liu, Stuart J. Gibson, James Mills, Margarita Osadchy", "title": "Dynamic Spectrum Matching with One-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been shown to provide a good\nsolution for classification problems that utilize data obtained from\nvibrational spectroscopy. Moreover, CNNs are capable of identification from\nnoisy spectra without the need for additional preprocessing. However, their\napplication in practical spectroscopy is limited due to two shortcomings. The\neffectiveness of the classification using CNNs drops rapidly when only a small\nnumber of spectra per substance are available for training (which is a typical\nsituation in real applications). Additionally, to accommodate new, previously\nunseen substance classes, the network must be retrained which is\ncomputationally intensive. Here we address these issues by reformulating a\nmulti-class classification problem with a large number of classes, but a small\nnumber of samples per class, to a binary classification problem with sufficient\ndata available for representation learning. Namely, we define the learning task\nas identifying pairs of inputs as belonging to the same or different classes.\nWe achieve this using a Siamese convolutional neural network. A novel sampling\nstrategy is proposed to address the imbalance problem in training the Siamese\nNetwork. The trained network can effectively classify samples of unseen\nsubstance classes using just a single reference sample (termed as one-shot\nlearning in the machine learning community). Our results demonstrate better\naccuracy than other practical systems to date, while allowing effortless\nupdates of the system's database with novel substance classes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 16:38:18 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Liu", "Jinchao", ""], ["Gibson", "Stuart J.", ""], ["Mills", "James", ""], ["Osadchy", "Margarita", ""]]}, {"id": "1806.10019", "submitter": "Zhang-Wei Hong", "authors": "Zhang-Wei Hong, Tsu-Jui Fu, Tzu-Yun Shann, Yi-Hsiang Chang, Chun-Yi\n  Lee", "title": "Adversarial Active Exploration for Inverse Dynamics Model Learning", "comments": "Published as a conference paper at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adversarial active exploration for inverse dynamics model\nlearning, a simple yet effective learning scheme that incentivizes exploration\nin an environment without any human intervention. Our framework consists of a\ndeep reinforcement learning (DRL) agent and an inverse dynamics model\ncontesting with each other. The former collects training samples for the\nlatter, with an objective to maximize the error of the latter. The latter is\ntrained with samples collected by the former, and generates rewards for the\nformer when it fails to predict the actual action taken by the former. In such\na competitive setting, the DRL agent learns to generate samples that the\ninverse dynamics model fails to predict correctly, while the inverse dynamics\nmodel learns to adapt to the challenging samples. We further propose a reward\nstructure that ensures the DRL agent to collect only moderately hard samples\nbut not overly hard ones that prevent the inverse model from predicting\neffectively. We evaluate the effectiveness of our method on several robotic arm\nand hand manipulation tasks against multiple baseline models. Experimental\nresults show that our method is comparable to those directly trained with\nexpert demonstrations, and superior to the other baselines even without any\nhuman priors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:33:22 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 03:48:55 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hong", "Zhang-Wei", ""], ["Fu", "Tsu-Jui", ""], ["Shann", "Tzu-Yun", ""], ["Chang", "Yi-Hsiang", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1806.10064", "submitter": "Leon Ren\\'e S\\\"utfeld", "authors": "Leon Ren\\'e S\\\"utfeld, Flemming Brieger, Holger Finger, Sonja\n  F\\\"ullhase, Gordon Pipa", "title": "Adaptive Blending Units: Trainable Activation Functions for Deep Neural\n  Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used activation functions in current deep feed-forward neural\nnetworks are rectified linear units (ReLU), and many alternatives have been\nsuccessfully applied, as well. However, none of the alternatives have managed\nto consistently outperform the rest and there is no unified theory connecting\nproperties of the task and network with properties of activation functions for\nmost efficient training. A possible solution is to have the network learn its\npreferred activation functions. In this work, we introduce Adaptive Blending\nUnits (ABUs), a trainable linear combination of a set of activation functions.\nSince ABUs learn the shape, as well as the overall scaling of the activation\nfunction, we also analyze the effects of adaptive scaling in common activation\nfunctions. We experimentally demonstrate advantages of both adaptive scaling\nand ABUs over common activation functions across a set of systematically varied\nnetwork specifications. We further show that adaptive scaling works by\nmitigating covariate shifts during training, and that the observed advantages\nin performance of ABUs likewise rely largely on the activation function's\nability to adapt over the course of training.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:32:48 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["S\u00fctfeld", "Leon Ren\u00e9", ""], ["Brieger", "Flemming", ""], ["Finger", "Holger", ""], ["F\u00fcllhase", "Sonja", ""], ["Pipa", "Gordon", ""]]}, {"id": "1806.10069", "submitter": "Thibaut Thonet PhD", "authors": "Maziar Moradi Fard, Thibaut Thonet, Eric Gaussier", "title": "Deep $k$-Means: Jointly clustering with $k$-Means and learning\n  representations", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the problem of jointly clustering and learning\nrepresentations. As several previous studies have shown, learning\nrepresentations that are both faithful to the data to be clustered and adapted\nto the clustering algorithm can lead to better clustering performance, all the\nmore so that the two tasks are performed jointly. We propose here such an\napproach for $k$-Means clustering based on a continuous reparametrization of\nthe objective function that leads to a truly joint solution. The behavior of\nour approach is illustrated on various datasets showing its efficacy in\nlearning representations for objects while clustering them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:43:03 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 10:46:23 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Fard", "Maziar Moradi", ""], ["Thonet", "Thibaut", ""], ["Gaussier", "Eric", ""]]}, {"id": "1806.10077", "submitter": "Jeff Z. HaoChen", "authors": "Jeff Z. HaoChen, Suvrit Sra", "title": "Random Shuffling Beats SGD after Finite Epochs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing problem in the theory of stochastic gradient descent (SGD) is\nto prove that its without-replacement version RandomShuffle converges faster\nthan the usual with-replacement version. We present the first (to our\nknowledge) non-asymptotic solution to this problem, which shows that after a\n\"reasonable\" number of epochs RandomShuffle indeed converges faster than SGD.\nSpecifically, we prove that under strong convexity and second-order smoothness,\nthe sequence generated by RandomShuffle converges to the optimal solution at\nthe rate O(1/T^2 + n^3/T^3), where n is the number of components in the\nobjective, and T is the total number of iterations. This result shows that\nafter a reasonable number of epochs RandomShuffle is strictly better than SGD\n(which converges as O(1/T)). The key step toward showing this better dependence\non T is the introduction of n into the bound; and as our analysis will show, in\ngeneral a dependence on n is unavoidable without further changes to the\nalgorithm. We show that for sparse data RandomShuffle has the rate O(1/T^2),\nagain strictly better than SGD. Furthermore, we discuss extensions to nonconvex\ngradient dominated functions, as well as non-strongly convex settings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:54:31 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 03:41:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["HaoChen", "Jeff Z.", ""], ["Sra", "Suvrit", ""]]}, {"id": "1806.10080", "submitter": "Anirban Mukhopadhyay", "authors": "Anirban Mukhopadhyay", "title": "A Theory of Diagnostic Interpretation in Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable deep learning is a fundamental building block towards safer AI,\nespecially when the deployment possibilities of deep learning-based\ncomputer-aided medical diagnostic systems are so eminent. However, without a\ncomputational formulation of black-box interpretation, general interpretability\nresearch rely heavily on subjective bias. Clear decision structure of the\nmedical diagnostics lets us approximate the decision process of a radiologist\nas a model - removed from subjective bias. We define the process of\ninterpretation as a finite communication between a known model and a black-box\nmodel to optimally map the black box's decision process in the known model.\nConsequently, we define interpretability as maximal information gain over the\ninitial uncertainty about the black-box's decision within finite communication.\nWe relax this definition based on the observation that diagnostic\ninterpretation is typically achieved by a process of minimal querying. We\nderive an algorithm to calculate diagnostic interpretability. The usual\nquestion of accuracy-interpretability tradeoff, i.e. whether a black-box\nmodel's prediction accuracy is dependent on its ability to be interpreted by a\nknown source model, does not arise in this theory. With multiple example\nsimulation experiments of various complexity levels, we demonstrate the working\nof such a theoretical model in synthetic supervised classification scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:58:39 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Mukhopadhyay", "Anirban", ""]]}, {"id": "1806.10131", "submitter": "Xiaoyang Wang", "authors": "Shujian Yu, Xiaoyang Wang, Jose C. Principe", "title": "Request-and-Reverify: Hierarchical Hypothesis Testing for Concept Drift\n  Detection with Expensive Labels", "comments": "Published as a conference paper at IJCAI 2018", "journal-ref": "Proceedings of the Twenty-Seventh International Joint Conference\n  on Artificial Intelligence (2018) 3033-3039", "doi": "10.24963/ijcai.2018/421", "report-no": "ITD-18-58133N", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important assumption underlying common classification models is the\nstationarity of the data. However, in real-world streaming applications, the\ndata concept indicated by the joint distribution of feature and label is not\nstationary but drifting over time. Concept drift detection aims to detect such\ndrifts and adapt the model so as to mitigate any deterioration in the model's\npredictive performance. Unfortunately, most existing concept drift detection\nmethods rely on a strong and over-optimistic condition that the true labels are\navailable immediately for all already classified instances. In this paper, a\nnovel Hierarchical Hypothesis Testing framework with Request-and-Reverify\nstrategy is developed to detect concept drifts by requesting labels only when\nnecessary. Two methods, namely Hierarchical Hypothesis Testing with\nClassification Uncertainty (HHT-CU) and Hierarchical Hypothesis Testing with\nAttribute-wise \"Goodness-of-fit\" (HHT-AG), are proposed respectively under the\nnovel framework. In experiments with benchmark datasets, our methods\ndemonstrate overwhelming advantages over state-of-the-art unsupervised drift\ndetectors. More importantly, our methods even outperform DDM (the widely used\nsupervised drift detector) when we use significantly fewer labels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 22:15:19 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 18:07:13 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Yu", "Shujian", ""], ["Wang", "Xiaoyang", ""], ["Principe", "Jose C.", ""]]}, {"id": "1806.10166", "submitter": "Ferran Alet", "authors": "Ferran Alet, Tom\\'as Lozano-P\\'erez, Leslie P. Kaelbling", "title": "Modular meta-learning", "comments": "Presented at CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction problems, such as those that arise in the context of\nrobotics, have a simplifying underlying structure that, if known, could\naccelerate learning. In this paper, we present a strategy for learning a set of\nneural network modules that can be combined in different ways. We train\ndifferent modular structures on a set of related tasks and generalize to new\ntasks by composing the learned modules in new ways. By reusing modules to\ngeneralize we achieve combinatorial generalization, akin to the \"infinite use\nof finite means\" displayed in language. Finally, we show this improves\nperformance in two robotics-related problems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 18:41:35 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 22:05:28 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Alet", "Ferran", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""], ["Kaelbling", "Leslie P.", ""]]}, {"id": "1806.10174", "submitter": "Emilia Apostolova PhD", "authors": "Tony Wang, Tom Velez, Emilia Apostolova, Tim Tschampel, Thuy L. Ngo,\n  Joy Hardison", "title": "Semantically Enhanced Dynamic Bayesian Network for Detecting Sepsis\n  Mortality Risk in ICU Patients with Infection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although timely sepsis diagnosis and prompt interventions in Intensive Care\nUnit (ICU) patients are associated with reduced mortality, early clinical\nrecognition is frequently impeded by non-specific signs of infection and\nfailure to detect signs of sepsis-induced organ dysfunction in a constellation\nof dynamically changing physiological data. The goal of this work is to\nidentify patient at risk of life-threatening sepsis utilizing a data-centered\nand machine learning-driven approach. We derive a mortality risk predictive\ndynamic Bayesian network (DBN) guided by a customized sepsis knowledgebase and\ncompare the predictive accuracy of the derived DBN with the Sepsis-related\nOrgan Failure Assessment (SOFA) score, the Quick SOFA (qSOFA) score, the\nSimplified Acute Physiological Score (SAPS-II) and the Modified Early Warning\nScore (MEWS) tools.\n  A customized sepsis ontology was used to derive the DBN node structure and\nsemantically characterize temporal features derived from both structured\nphysiological data and unstructured clinical notes. We assessed the performance\nin predicting mortality risk of the DBN predictive model and compared\nperformance to other models using Receiver Operating Characteristic (ROC)\ncurves, area under curve (AUROC), calibration curves, and risk distributions.\n  The derived dataset consists of 24,506 ICU stays from 19,623 patients with\nevidence of suspected infection, with 2,829 patients deceased at discharge. The\nDBN AUROC was found to be 0.91, which outperformed the SOFA (0.843), qSOFA\n(0.66), MEWS (0.73), and SAPS-II (0.77) scoring tools. Continuous Net\nReclassification Index and Integrated Discrimination Improvement analysis\nsupported the superiority DBN. Compared with conventional rule-based risk\nscoring tools, the sepsis knowledgebase-driven DBN algorithm offers improved\nperformance for predicting mortality of infected patients in ICUs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:09:19 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wang", "Tony", ""], ["Velez", "Tom", ""], ["Apostolova", "Emilia", ""], ["Tschampel", "Tim", ""], ["Ngo", "Thuy L.", ""], ["Hardison", "Joy", ""]]}, {"id": "1806.10175", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi, Felix X. Yu,\n  Daniel Holtmann-Rice, Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar", "title": "Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling", "comments": "17 pages, 7 tables, 8 figures, published in ICML 2019; part of this\n  work was done while Shanshan was an intern at Google Research, New York", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear encoding of sparse vectors is widely popular, but is commonly\ndata-independent -- missing any possible extra (but a priori unknown) structure\nbeyond sparsity. In this paper we present a new method to learn linear encoders\nthat adapt to data, while still performing well with the widely used $\\ell_1$\ndecoder. The convex $\\ell_1$ decoder prevents gradient propagation as needed in\nstandard gradient-based training. Our method is based on the insight that\nunrolling the convex decoder into $T$ projected subgradient steps can address\nthis issue. Our method can be seen as a data-driven way to learn a compressed\nsensing measurement matrix. We compare the empirical performance of 10\nalgorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments\nshow that there is indeed additional structure beyond sparsity in the real\ndatasets; our method is able to discover it and exploit it to create excellent\nreconstructions with fewer measurements (by a factor of 1.1-3x) compared to the\nprevious state-of-the-art methods. We illustrate an application of our method\nin learning label embeddings for extreme multi-label classification, and\nempirically show that our method is able to match or outperform the precision\nscores of SLEEC, which is one of the state-of-the-art embedding-based\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:11:22 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 15:36:39 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 18:59:53 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 16:07:17 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wu", "Shanshan", ""], ["Dimakis", "Alexandros G.", ""], ["Sanghavi", "Sujay", ""], ["Yu", "Felix X.", ""], ["Holtmann-Rice", "Daniel", ""], ["Storcheus", "Dmitry", ""], ["Rostamizadeh", "Afshin", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1806.10179", "submitter": "Tobias Glasmachers", "authors": "Sahar Qaadan and Tobias Glasmachers", "title": "Multi-Merge Budget Maintenance for Stochastic Gradient Descent SVM\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Budgeted Stochastic Gradient Descent (BSGD) is a state-of-the-art technique\nfor training large-scale kernelized support vector machines. The budget\nconstraint is maintained incrementally by merging two points whenever the\npre-defined budget is exceeded. The process of finding suitable merge partners\nis costly; it can account for up to 45% of the total training time. In this\npaper we investigate computationally more efficient schemes that merge more\nthan two points at once. We obtain significant speed-ups without sacrificing\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:23:45 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Qaadan", "Sahar", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "1806.10180", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers and Sahar Qaadan", "title": "Speeding Up Budgeted Stochastic Gradient Descent SVM Training with\n  Precomputed Golden Section Search", "comments": "arXiv admin note: text overlap with arXiv:1806.10179", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limiting the model size of a kernel support vector machine to a pre-defined\nbudget is a well-established technique that allows to scale SVM learning and\nprediction to large-scale data. Its core addition to simple stochastic gradient\ntraining is budget maintenance through merging of support vectors. This\nrequires solving an inner optimization problem with an iterative method many\ntimes per gradient step. In this paper we replace the iterative procedure with\na fast lookup. We manage to reduce the merging time by up to 65% and the total\ntraining time by 44% without any loss of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:28:43 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Qaadan", "Sahar", ""]]}, {"id": "1806.10181", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John Hopfield", "title": "Unsupervised Learning by Competing Hidden Units", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the USA, 116\n  (16) 7723-7731 (2019)", "doi": "10.1073/pnas.1820458116", "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:32:58 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:36:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John", ""]]}, {"id": "1806.10182", "submitter": "Tobias Glasmachers", "authors": "Sahar Qaadan, Merlin Sch\\\"uler and Tobias Glasmachers", "title": "Dual SVM Training on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dual subspace ascent algorithm for support vector machine\ntraining that respects a budget constraint limiting the number of support\nvectors. Budget methods are effective for reducing the training time of kernel\nSVM while retaining high accuracy. To date, budget training is available only\nfor primal (SGD-based) solvers. Dual subspace ascent methods like sequential\nminimal optimization are attractive for their good adaptation to the problem\nstructure, their fast convergence rate, and their practical speed. By\nincorporating a budget constraint into a dual algorithm, our method enjoys the\nbest of both worlds. We demonstrate considerable speed-ups over primal budget\ntraining methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:38:11 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Qaadan", "Sahar", ""], ["Sch\u00fcler", "Merlin", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "1806.10188", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani, Ohad Shamir, Nathan Srebro", "title": "A Tight Convergence Analysis for Stochastic Gradient Descent with\n  Delayed Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide tight finite-time convergence bounds for gradient descent and\nstochastic gradient descent on quadratic functions, when the gradients are\ndelayed and reflect iterates from $\\tau$ rounds ago. First, we show that\nwithout stochastic noise, delays strongly affect the attainable optimization\nerror: In fact, the error can be as bad as non-delayed gradient descent ran on\nonly $1/\\tau$ of the gradients. In sharp contrast, we quantify how stochastic\nnoise makes the effect of delays negligible, improving on previous work which\nonly showed this phenomenon asymptotically or for much smaller delays. Also, in\nthe context of distributed optimization, the results indicate that the\nperformance of gradient descent with delays is competitive with synchronous\napproaches such as mini-batching. Our results are based on a novel technique\nfor analyzing convergence of optimization algorithms using generating\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:53:59 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shamir", "Ohad", ""], ["Srebro", "Nathan", ""]]}, {"id": "1806.10206", "submitter": "Edo Collins", "authors": "Edo Collins, Radhakrishna Achanta, Sabine S\\\"usstrunk", "title": "Deep Feature Factorization For Concept Discovery", "comments": "The European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Feature Factorization (DFF), a method capable of localizing\nsimilar semantic concepts within an image or a set of images. We use DFF to\ngain insight into a deep convolutional neural network's learned features, where\nwe detect hierarchical cluster structures in feature space. This is visualized\nas heat maps, which highlight semantically matching regions across a set of\nimages, revealing what the network `perceives' as similar. DFF can also be used\nto perform co-segmentation and co-localization, and we report state-of-the-art\nresults on these tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 20:39:13 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 13:05:06 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 11:40:15 GMT"}, {"version": "v4", "created": "Wed, 19 Sep 2018 09:21:36 GMT"}, {"version": "v5", "created": "Mon, 8 Oct 2018 10:49:15 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Collins", "Edo", ""], ["Achanta", "Radhakrishna", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1806.10222", "submitter": "Brendan Juba", "authors": "John Hainline, Brendan Juba, Hai S.Le, David Woodruff", "title": "Conditional Sparse $\\ell_p$-norm Regression With Optimal Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following conditional linear regression problem: the task is\nto identify both (i) a $k$-DNF condition $c$ and (ii) a linear rule $f$ such\nthat the probability of $c$ is (approximately) at least some given bound $\\mu$,\nand $f$ minimizes the $\\ell_p$ loss of predicting the target $z$ in the\ndistribution of examples conditioned on $c$. Thus, the task is to identify a\nportion of the distribution on which a linear rule can provide a good fit.\nAlgorithms for this task are useful in cases where simple, learnable rules only\naccurately model portions of the distribution. The prior state-of-the-art for\nsuch algorithms could only guarantee finding a condition of probability\n$\\Omega(\\mu/n^k)$ when a condition of probability $\\mu$ exists, and achieved an\n$O(n^k)$-approximation to the target loss, where $n$ is the number of Boolean\nattributes. Here, we give efficient algorithms for solving this task with a\ncondition $c$ that nearly matches the probability of the ideal condition, while\nalso improving the approximation to the target loss. We also give an algorithm\nfor finding a $k$-DNF reference class for prediction at a given query point,\nthat obtains a sparse regression fit that has loss within $O(n^k)$ of optimal\namong all sparse regression parameters and sufficiently large $k$-DNF reference\nclasses containing the query point.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 21:49:37 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Hainline", "John", ""], ["Juba", "Brendan", ""], ["Le", "Hai S.", ""], ["Woodruff", "David", ""]]}, {"id": "1806.10230", "submitter": "Niru Maheswaranathan", "authors": "Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, Jascha\n  Sohl-Dickstein", "title": "Guided evolutionary strategies: Augmenting random search with surrogate\n  gradients", "comments": "Published at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in machine learning require optimizing a function whose\ntrue gradient is unknown, but where surrogate gradient information (directions\nthat may be correlated with, but not necessarily identical to, the true\ngradient) is available instead. This arises when an approximate gradient is\neasier to compute than the full gradient (e.g. in meta-learning or unrolled\noptimization), or when a true gradient is intractable and is replaced with a\nsurrogate (e.g. in certain reinforcement learning applications, or when using\nsynthetic gradients). We propose Guided Evolutionary Strategies, a method for\noptimally using surrogate gradient directions along with random search. We\ndefine a search distribution for evolutionary strategies that is elongated\nalong a guiding subspace spanned by the surrogate gradients. This allows us to\nestimate a descent direction which can then be passed to a first-order\noptimizer. We analytically and numerically characterize the tradeoffs that\nresult from tuning how strongly the search distribution is stretched along the\nguiding subspace, and we use this to derive a setting of the hyperparameters\nthat works well across problems. Finally, we apply our method to example\nproblems, demonstrating an improvement over both standard evolutionary\nstrategies and first-order methods (that directly follow the surrogate\ngradient). We provide a demo of Guided ES at\nhttps://github.com/brain-research/guided-evolutionary-strategies\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 22:14:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 06:08:23 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 18:17:14 GMT"}, {"version": "v4", "created": "Mon, 10 Jun 2019 18:19:33 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Maheswaranathan", "Niru", ""], ["Metz", "Luke", ""], ["Tucker", "George", ""], ["Choi", "Dami", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.10234", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Miko{\\l}aj Kasprzak, Tamara\n  Broderick", "title": "Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees", "comments": "20 pages, 7 figures, 1 table, including Appendix. Code available at\n  https://github.com/trevorcampbell/fishergp", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR:\n  Volume 89", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 22:42:15 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 16:49:53 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 21:24:19 GMT"}, {"version": "v4", "created": "Wed, 27 Mar 2019 13:50:14 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1806.10270", "submitter": "Kartik Ahuja", "authors": "Kartik Ahuja, William Zame, Mihaela van der Schaar", "title": "Optimal Piecewise Local-Linear Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on \"black-box\" model interpretation use local-linear\napproximations to explain the predictions made for each data instance in terms\nof the importance assigned to the different features for arriving at the\nprediction. These works provide instancewise explanations and thus give a local\nview of the model. To be able to trust the model it is important to understand\nthe global model behavior and there are relatively fewer works which do the\nsame. Piecewise local-linear models provide a natural way to extend\nlocal-linear models to explain the global behavior of the model. In this work,\nwe provide a dynamic programming based framework to obtain piecewise\napproximations of the black-box model. We also provide provable fidelity, i.e.,\nhow well the explanations reflect the black-box model, guarantees. We carry out\nsimulations on synthetic and real datasets to show the utility of the proposed\napproach. At the end, we show that the ideas developed for our framework can\nalso be used to address the problem of clustering for one-dimensional data. We\ngive a polynomial time algorithm and prove that it achieves optimal clustering.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:10:56 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 18:58:51 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 05:21:26 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 16:30:31 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ahuja", "Kartik", ""], ["Zame", "William", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1806.10282", "submitter": "Haifeng Jin", "authors": "Haifeng Jin, Qingquan Song, Xia Hu", "title": "Auto-Keras: An Efficient Neural Architecture Search System", "comments": "The code of Auto-Keras is available at https://autokeras.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has been proposed to automatically tune deep\nneural networks, but existing search algorithms, e.g., NASNet, PNAS, usually\nsuffer from expensive computational cost. Network morphism, which keeps the\nfunctionality of a neural network while changing its neural architecture, could\nbe helpful for NAS by enabling more efficient training during the search. In\nthis paper, we propose a novel framework enabling Bayesian optimization to\nguide the network morphism for efficient neural architecture search. The\nframework develops a neural network kernel and a tree-structured acquisition\nfunction optimization algorithm to efficiently explores the search space.\nIntensive experiments on real-world benchmark datasets have been done to\ndemonstrate the superior performance of the developed framework over the\nstate-of-the-art methods. Moreover, we build an open-source AutoML system based\non our method, namely Auto-Keras. The system runs in parallel on CPU and GPU,\nwith an adaptive search strategy for different GPU memory limits.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 03:18:35 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 20:00:49 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 04:38:37 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Jin", "Haifeng", ""], ["Song", "Qingquan", ""], ["Hu", "Xia", ""]]}, {"id": "1806.10283", "submitter": "Yusheng Luo", "authors": "Yusheng Luo, Min Xian, Manish Mohanpurkar, Bishnu P. Bhattarai,\n  Anudeep Medam, Rahul Kadavil and Rob Hovsapian", "title": "Optimal Scheduling of Electrolyzer in Power Market with Dynamic Prices", "comments": null, "journal-ref": null, "doi": "10.1109/PMAPS.2018.8440508", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal scheduling of hydrogen production in dynamic pricing power market can\nmaximize the profit of hydrogen producer; however, it highly depends on the\naccurate forecast of hydrogen consumption. In this paper, we propose a deep\nleaning based forecasting approach for predicting hydrogen consumption of fuel\ncell vehicles in future taxi industry. The cost of hydrogen production is\nminimized by utilizing the proposed forecasting tool to reduce the hydrogen\nproduced during high cost on-peak hours and guide hydrogen producer to store\nsufficient hydrogen during low cost off-peak hours.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 03:31:46 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Luo", "Yusheng", ""], ["Xian", "Min", ""], ["Mohanpurkar", "Manish", ""], ["Bhattarai", "Bishnu P.", ""], ["Medam", "Anudeep", ""], ["Kadavil", "Rahul", ""], ["Hovsapian", "Rob", ""]]}, {"id": "1806.10293", "submitter": "Alexander Irpan", "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander\n  Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent\n  Vanhoucke, Sergey Levine", "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\n  Manipulation", "comments": "CoRL 2018 camera ready. 23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 04:34:30 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 19:08:00 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 02:40:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kalashnikov", "Dmitry", ""], ["Irpan", "Alex", ""], ["Pastor", "Peter", ""], ["Ibarz", "Julian", ""], ["Herzog", "Alexander", ""], ["Jang", "Eric", ""], ["Quillen", "Deirdre", ""], ["Holly", "Ethan", ""], ["Kalakrishnan", "Mrinal", ""], ["Vanhoucke", "Vincent", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.10308", "submitter": "Lijun Zhang", "authors": "Yuanyu Wan, Jinfeng Yi, Lijun Zhang", "title": "Matrix Completion from Non-Uniformly Sampled Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider matrix completion from non-uniformly sampled\nentries including fully observed and partially observed columns. Specifically,\nwe assume that a small number of columns are randomly selected and fully\nobserved, and each remaining column is partially observed with uniform\nsampling. To recover the unknown matrix, we first recover its column space from\nthe fully observed columns. Then, for each partially observed column, we\nrecover it by finding a vector which lies in the recovered column space and\nconsists of the observed entries. When the unknown $m\\times n$ matrix is\nlow-rank, we show that our algorithm can exactly recover it from merely\n$\\Omega(rn\\ln n)$ entries, where $r$ is the rank of the matrix. Furthermore,\nfor a noisy low-rank matrix, our algorithm computes a low-rank approximation of\nthe unknown matrix and enjoys an additive error bound measured by Frobenius\nnorm. Experimental results on synthetic datasets verify our theoretical claims\nand demonstrate the effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 05:57:09 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wan", "Yuanyu", ""], ["Yi", "Jinfeng", ""], ["Zhang", "Lijun", ""]]}, {"id": "1806.10317", "submitter": "Kuan-Chieh Wang", "authors": "Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, Richard\n  Zemel", "title": "Adversarial Distillation of Bayesian Neural Network Posteriors", "comments": "accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural networks (BNNs) allow us to reason about uncertainty in a\nprincipled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient\nBNN learning by drawing samples from the BNN posterior using mini-batches.\nHowever, SGLD and its extensions require storage of many copies of the model\nparameters, a potentially prohibitive cost, especially for large neural\nnetworks. We propose a framework, Adversarial Posterior Distillation, to\ndistill the SGLD samples using a Generative Adversarial Network (GAN). At\ntest-time, samples are generated by the GAN. We show that this distillation\nframework incurs no loss in performance on recent BNN applications including\nanomaly detection, active learning, and defense against adversarial attacks. By\nconstruction, our framework not only distills the Bayesian predictive\ndistribution, but the posterior itself. This allows one to compute quantities\nsuch as the approximate model variance, which is useful in downstream tasks. To\nour knowledge, these are the first results applying MCMC-based BNNs to the\naforementioned downstream applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 06:24:53 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wang", "Kuan-Chieh", ""], ["Vicol", "Paul", ""], ["Lucas", "James", ""], ["Gu", "Li", ""], ["Grosse", "Roger", ""], ["Zemel", "Richard", ""]]}, {"id": "1806.10332", "submitter": "Chi-Hung Hsu", "authors": "Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping Chou,\n  Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei and\n  Da-Cheng Juan", "title": "MONAS: Multi-Objective Neural Architecture Search using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on neural architecture search have shown that automatically\ndesigned neural networks perform as good as expert-crafted architectures. While\nmost existing works aim at finding architectures that optimize the prediction\naccuracy, these architectures may have complexity and is therefore not suitable\nbeing deployed on certain computing environment (e.g., with limited power\nbudgets). We propose MONAS, a framework for Multi-Objective Neural\nArchitectural Search that employs reward functions considering both prediction\naccuracy and other important objectives (e.g., power consumption) when\nsearching for neural network architectures. Experimental results showed that,\ncompared to the state-ofthe-arts, models found by MONAS achieve comparable or\nbetter classification accuracy on computer vision applications, while\nsatisfying the additional objectives such as peak power.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:12:01 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 06:54:48 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hsu", "Chi-Hung", ""], ["Chang", "Shu-Huan", ""], ["Liang", "Jhao-Hong", ""], ["Chou", "Hsin-Ping", ""], ["Liu", "Chun-Hao", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1806.10349", "submitter": "Kristof Sch\\\"utt", "authors": "Kristof T. Sch\\\"utt, Michael Gastegger, Alexandre Tkatchenko,\n  Klaus-Robert M\\\"uller", "title": "Quantum-chemical insights from interpretable atomistic neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of deep neural networks for quantum chemistry applications,\nthere is a pressing need for architectures that, beyond delivering accurate\npredictions of chemical properties, are readily interpretable by researchers.\nHere, we describe interpretation techniques for atomistic neural networks on\nthe example of Behler-Parrinello networks as well as the end-to-end model\nSchNet. Both models obtain predictions of chemical properties by aggregating\natom-wise contributions. These latent variables can serve as local explanations\nof a prediction and are obtained during training without additional cost. Due\nto their correspondence to well-known chemical concepts such as atomic energies\nand partial charges, these atom-wise explanations enable insights not only\nabout the model but more importantly about the underlying quantum-chemical\nregularities. We generalize from atomistic explanations to 3d space, thus\nobtaining spatially resolved visualizations which further improve\ninterpretability. Finally, we analyze learned embeddings of chemical elements\nthat exhibit a partial ordering that resembles the order of the periodic table.\nAs the examined neural networks show excellent agreement with chemical\nknowledge, the presented techniques open up new venues for data-driven research\nin chemistry, physics and materials science.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:59:11 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Sch\u00fctt", "Kristof T.", ""], ["Gastegger", "Michael", ""], ["Tkatchenko", "Alexandre", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1806.10410", "submitter": "Yining Wang", "authors": "Xi Chen and Chao Shi and Yining Wang and Yuan Zhou", "title": "Dynamic Assortment Selection under the Nested Logit Models", "comments": "final version", "journal-ref": "Production and Operations Management 30(1):85-102, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a stylized dynamic assortment planning problem during a selling\nseason of finite length $T$. At each time period, the seller offers an arriving\ncustomer an assortment of substitutable products and the customer makes the\npurchase among offered products according to a discrete choice model. The goal\nof the seller is to maximize the expected revenue, or equivalently, to minimize\nthe worst-case expected regret. One key challenge is that utilities of products\nare unknown to the seller and need to be learned. Although the dynamic\nassortment planning problem has received increasing attention in revenue\nmanagement, most existing work is based on the multinomial logit choice models\n(MNL). In this paper, we study the problem of dynamic assortment planning under\na more general choice model -- the nested logit model, which models\nhierarchical choice behavior and is ``the most widely used member of the GEV\n(generalized extreme value) family''. By leveraging the revenue-ordered\nstructure of the optimal assortment within each nest, we develop a novel upper\nconfidence bound (UCB) policy with an aggregated estimation scheme. Our policy\nsimultaneously learns customers' choice behavior and makes dynamic decisions on\nassortments based on the current knowledge. It achieves the accumulated regret\nat the order of $\\tilde{O}(\\sqrt{MNT})$, where $M$ is the number of nests and\n$N$ is the number of products in each nest. We further provide a lower bound\nresult of $\\Omega(\\sqrt{MT})$, which shows the near optimality of the upper\nbound when $T$ is much larger than $M$ and $N$. When the number of items per\nnest $N$ is large, we further provide a discretization heuristic for better\nperformance of our algorithm. Numerical results are presented to demonstrate\nthe empirical performance of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:13:07 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 19:02:28 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Chen", "Xi", ""], ["Shi", "Chao", ""], ["Wang", "Yining", ""], ["Zhou", "Yuan", ""]]}, {"id": "1806.10474", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, A\\\"aron van den Oord, Karen Simonyan", "title": "The challenge of realistic music generation: modelling raw audio at\n  scale", "comments": "13 pages, 2 figures, submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic music generation is a challenging task. When building generative\nmodels of music that are learnt from data, typically high-level representations\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\nparticular performance. But these nuances are very important for our perception\nof musicality and realism, so in this work we embark on modelling music in the\nraw audio domain. It has been shown that autoregressive models excel at\ngenerating raw audio waveforms of speech, but when applied to music, we find\nthem biased towards capturing local signal structure at the expense of\nmodelling long-range correlations. This is problematic because music exhibits\nstructure at many different timescales. In this work, we explore autoregressive\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\ncapture long-range correlations in waveforms. We find that they allow us to\nunconditionally generate piano music directly in the raw audio domain, which\nshows stylistic consistency across tens of seconds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 16:48:59 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Dieleman", "Sander", ""], ["Oord", "A\u00e4ron van den", ""], ["Simonyan", "Karen", ""]]}, {"id": "1806.10480", "submitter": "Rahul Yedida", "authors": "Rahul Yedida, Rahul Reddy, Rakshit Vahi, Rahul Jana, Abhilash GV,\n  Deepti Kulkarni", "title": "Employee Attrition Prediction", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We aim to predict whether an employee of a company will leave or not, using\nthe k-Nearest Neighbors algorithm. We use evaluation of employee performance,\naverage monthly hours at work and number of years spent in the company, among\nothers, as our features. Other approaches to this problem include the use of\nANNs, decision trees and logistic regression. The dataset was split, using 70%\nfor training the algorithm and 30% for testing it, achieving an accuracy of\n94.32%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:42:01 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Yedida", "Rahul", ""], ["Reddy", "Rahul", ""], ["Vahi", "Rakshit", ""], ["Jana", "Rahul", ""], ["GV", "Abhilash", ""], ["Kulkarni", "Deepti", ""]]}, {"id": "1806.10547", "submitter": "Shangshu Zhao", "authors": "Shangshu Zhao, Zhaowei Zhu, Fuqian Yang, and Xiliang Luo", "title": "Online optimal task offloading with one-bit feedback", "comments": "We have submitted this paper to GlobalSIP 2018 on Jun. 29th", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task offloading is an emerging technology in fog-enabled networks. It allows\nusers to transmit tasks to neighbor fog nodes so as to utilize the computing\nresources of the networks. In this paper, we investigate a stochastic task\noffloading model and propose a multi-armed bandit framework to formulate this\nmodel. We consider the fact that different helper nodes prefer different kinds\nof tasks. Further, we assume each helper node just feeds back one-bit\ninformation to the task node to indicate the level of happiness. The key\nchallenge of this problem lies in the exploration-exploitation tradeoff. We\nthus implement a UCB-type algorithm to maximize the long-term happiness metric.\nNumerical simulations are given in the end of the paper to corroborate our\nstrategy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 16:04:00 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 06:06:59 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhao", "Shangshu", ""], ["Zhu", "Zhaowei", ""], ["Yang", "Fuqian", ""], ["Luo", "Xiliang", ""]]}, {"id": "1806.10574", "submitter": "Chaofan Chen", "authors": "Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su,\n  Cynthia Rudin", "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition", "comments": "Chaofan Chen and Oscar Li contributed equally to this work. This work\n  has been accepted for spotlight presentation (top 3% of papers) at NeurIPS\n  2019", "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture -- prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:18:03 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:44:23 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 17:35:02 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 18:10:36 GMT"}, {"version": "v5", "created": "Sat, 28 Dec 2019 20:12:11 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Chaofan", ""], ["Li", "Oscar", ""], ["Tao", "Chaofan", ""], ["Barnett", "Alina Jade", ""], ["Su", "Jonathan", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1806.10586", "submitter": "Yu Bai", "authors": "Yu Bai, Tengyu Ma, Andrej Risteski", "title": "Approximability of Discriminators Implies Diversity in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) have empirically produced\nimpressive results on learning complex real-world distributions, recent works\nhave shown that they suffer from lack of diversity or mode collapse. The\ntheoretical work of Arora et al. suggests a dilemma about GANs' statistical\nproperties: powerful discriminators cause overfitting, whereas weak\ndiscriminators cannot detect mode collapse.\n  By contrast, we show in this paper that GANs can in principle learn\ndistributions in Wasserstein distance (or KL-divergence in many cases) with\npolynomial sample complexity, if the discriminator class has strong\ndistinguishing power against the particular generator class (instead of against\nall possible generators). For various generator classes such as mixture of\nGaussians, exponential families, and invertible and injective neural networks\ngenerators, we design corresponding discriminators (which are often neural nets\nof specific architectures) such that the Integral Probability Metric (IPM)\ninduced by the discriminators can provably approximate the Wasserstein distance\nand/or KL-divergence. This implies that if the training is successful, then the\nlearned distribution is close to the true distribution in Wasserstein distance\nor KL divergence, and thus cannot drop modes. Our preliminary experiments show\nthat on synthetic datasets the test IPM is well correlated with KL divergence\nor the Wasserstein distance, indicating that the lack of diversity in GANs may\nbe caused by the sub-optimality in optimization instead of statistical\ninefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:33:52 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 06:15:38 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 06:19:39 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 05:59:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bai", "Yu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1806.10648", "submitter": "Jonathan Weed", "authors": "Philippe Rigollet and Jonathan Weed", "title": "Uncoupled isotonic regression via minimum Wasserstein deconvolution", "comments": "To appear in Information and Inference: a Journal of the IMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic regression is a standard problem in shape-constrained estimation\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\nfrom independent pairs $(x_i, y_i)$ where $\\mathbb{E}[y_i]=f(x_i), i=1, \\ldots\nn$. While this problem is well understood both statistically and\ncomputationally, much less is known about its uncoupled counterpart where one\nis given only the unordered sets $\\{x_1, \\ldots, x_n\\}$ and $\\{y_1, \\ldots,\ny_n\\}$. In this work, we leverage tools from optimal transport theory to derive\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\nalgorithm achieving optimal rates. Both upper and lower bounds employ\nmoment-matching arguments that are also pertinent to learning mixtures of\ndistributions and deconvolution.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:10:56 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 11:29:07 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Rigollet", "Philippe", ""], ["Weed", "Jonathan", ""]]}, {"id": "1806.10692", "submitter": "Arya Farahi", "authors": "Jacob Abernethy, Alex Chojnacki, Arya Farahi, Eric Schwartz, Jared\n  Webb", "title": "ActiveRemediation: The Search for Lead Pipes in Flint, Michigan", "comments": "10 pages, 10 figures, To appear in KDD 2018, For associated\n  promotional video, see https://www.youtube.com/watch?v=YbIn_axYu9E", "journal-ref": null, "doi": "10.1145/3219819.3219896", "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detail our ongoing work in Flint, Michigan to detect pipes made of lead\nand other hazardous metals. After elevated levels of lead were detected in\nresidents' drinking water, followed by an increase in blood lead levels in area\nchildren, the state and federal governments directed over $125 million to\nreplace water service lines, the pipes connecting each home to the water\nsystem. In the absence of accurate records, and with the high cost of\ndetermining buried pipe materials, we put forth a number of predictive and\nprocedural tools to aid in the search and removal of lead infrastructure.\nAlongside these statistical and machine learning approaches, we describe our\ninteractions with government officials in recommending homes for both\ninspection and replacement, with a focus on the statistical model that adapts\nto incoming information. Finally, in light of discussions about increased\nspending on infrastructure development by the federal government, we explore\nhow our approach generalizes beyond Flint to other municipalities nationwide.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 13:04:53 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 17:10:17 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Abernethy", "Jacob", ""], ["Chojnacki", "Alex", ""], ["Farahi", "Arya", ""], ["Schwartz", "Eric", ""], ["Webb", "Jared", ""]]}, {"id": "1806.10698", "submitter": "Yura Perov N", "authors": "Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie\n  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,\n  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri", "title": "A comparative study of artificial intelligence and human doctors for the\n  purpose of triage and diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online symptom checkers have significant potential to improve patient care,\nhowever their reliability and accuracy remain variable. We hypothesised that an\nartificial intelligence (AI) powered triage and diagnostic system would compare\nfavourably with human doctors with respect to triage and diagnostic accuracy.\nWe performed a prospective validation study of the accuracy and safety of an AI\npowered triage and diagnostic system. Identical cases were evaluated by both an\nAI system and human doctors. Differential diagnoses and triage outcomes were\nevaluated by an independent judge, who was blinded from knowing the source (AI\nsystem or human doctor) of the outcomes. Independently of these cases,\nvignettes from publicly available resources were also assessed to provide a\nbenchmark to previous studies and the diagnostic component of the MRCGP exam.\nOverall we found that the Babylon AI powered Triage and Diagnostic System was\nable to identify the condition modelled by a clinical vignette with accuracy\ncomparable to human doctors (in terms of precision and recall). In addition, we\nfound that the triage advice recommended by the AI System was, on average,\nsafer than that of human doctors, when compared to the ranges of acceptable\ntriage provided by independent expert judges, with only a minimal reduction in\nappropriateness.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 21:18:37 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Razzaki", "Salman", ""], ["Baker", "Adam", ""], ["Perov", "Yura", ""], ["Middleton", "Katherine", ""], ["Baxter", "Janie", ""], ["Mullarkey", "Daniel", ""], ["Sangar", "Davinder", ""], ["Taliercio", "Michael", ""], ["Butt", "Mobasher", ""], ["Majeed", "Azeem", ""], ["DoRosario", "Arnold", ""], ["Mahoney", "Megan", ""], ["Johri", "Saurabh", ""]]}, {"id": "1806.10701", "submitter": "Victor Veitch", "authors": "Victor Veitch, Morgane Austern, Wenda Zhou, David M. Blei, Peter\n  Orbanz", "title": "Empirical Risk Minimization and Stochastic Gradient Descent for\n  Relational Data", "comments": "Accepted as AISTATS 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization is the main tool for prediction problems, but its\nextension to relational data remains unsolved. We solve this problem using\nrecent ideas from graph sampling theory to (i) define an empirical risk for\nrelational data and (ii) obtain stochastic gradients for this empirical risk\nthat are automatically unbiased. This is achieved by considering the method by\nwhich data is sampled from a graph as an explicit component of model design. By\nintegrating fast implementations of graph sampling schemes with standard\nautomatic differentiation tools, we provide an efficient turnkey solver for the\nrisk minimization problem. We establish basic theoretical properties of the\nprocedure. Finally, we demonstrate relational ERM with application to two\nnon-standard problems: one-stage training for semi-supervised node\nclassification, and learning embedding vectors for vertex attributes.\nExperiments confirm that the turnkey inference procedure is effective in\npractice, and that the sampling scheme used for model specification has a\nstrong effect on model performance. Code is available at\nhttps://github.com/wooden-spoon/relational-ERM.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 22:08:54 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 18:23:16 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Veitch", "Victor", ""], ["Austern", "Morgane", ""], ["Zhou", "Wenda", ""], ["Blei", "David M.", ""], ["Orbanz", "Peter", ""]]}, {"id": "1806.10714", "submitter": "Chao Chen", "authors": "Chao Chen, Xiuyan Ni, Qinxun Bai, Yusu Wang", "title": "A Topological Regularizer for Classifiers via Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization plays a crucial role in supervised learning. Most existing\nmethods enforce a global regularization in a structure agnostic manner. In this\npaper, we initiate a new direction and propose to enforce the structural\nsimplicity of the classification boundary by regularizing over its topological\ncomplexity. In particular, our measurement of topological complexity\nincorporates the importance of topological features (e.g., connected\ncomponents, handles, and so on) in a meaningful manner, and provides a direct\ncontrol over spurious topological structures. We incorporate the new\nmeasurement as a topological penalty in training classifiers. We also pro- pose\nan efficient algorithm to compute the gradient of such penalty. Our method pro-\nvides a novel way to topologically simplify the global structure of the model,\nwithout having to sacrifice too much of the flexibility of the model. We\ndemonstrate the effectiveness of our new topological regularizer on a range of\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 23:42:37 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 19:25:33 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 13:49:56 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chen", "Chao", ""], ["Ni", "Xiuyan", ""], ["Bai", "Qinxun", ""], ["Wang", "Yusu", ""]]}, {"id": "1806.10728", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "Deep Echo State Networks with Uncertainty Quantification for\n  Spatio-Temporal Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-lead forecasting for spatio-temporal systems can often entail complex\nnonlinear dynamics that are difficult to specify it a priori. Current\nstatistical methodologies for modeling these processes are often highly\nparameterized and thus, challenging to implement from a computational\nperspective. One potential parsimonious solution to this problem is a method\nfrom the dynamical systems and engineering literature referred to as an echo\nstate network (ESN). ESN models use so-called {\\it reservoir computing} to\nefficiently compute recurrent neural network (RNN) forecasts. Moreover,\nso-called \"deep\" models have recently been shown to be successful at predicting\nhigh-dimensional complex nonlinear processes, particularly those with multiple\nspatial and temporal scales of variability (such as we often find in\nspatio-temporal environmental data). Here we introduce a deep ensemble ESN\n(D-EESN) model. We present two versions of this model for spatio-temporal\nprocesses that both produce forecasts and associated measures of uncertainty.\nThe first approach utilizes a bootstrap ensemble framework and the second is\ndeveloped within a hierarchical Bayesian framework (BD-EESN). This more general\nhierarchical Bayesian framework naturally accommodates non-Gaussian data types\nand multiple levels of uncertainties. The methodology is first applied to a\ndata set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical\nsystem simulation model and then to a long-lead United States (U.S.) soil\nmoisture forecasting application.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 01:12:32 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 21:20:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1806.10729", "submitter": "Niels Justesen", "authors": "Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed\n  Khalifa, Julian Togelius, Sebastian Risi", "title": "Illuminating Generalization in Deep Reinforcement Learning through\n  Procedural Level Generation", "comments": "Accepted to NeurIPS Deep RL Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has shown impressive results in a variety of\ndomains, learning directly from high-dimensional sensory streams. However, when\nneural networks are trained in a fixed environment, such as a single level in a\nvideo game, they will usually overfit and fail to generalize to new levels.\nWhen RL models overfit, even slight modifications to the environment can result\nin poor agent performance. This paper explores how procedurally generated\nlevels during training can increase generality. We show that for some games\nprocedural level generation enables generalization to new levels within the\nsame distribution. Additionally, it is possible to achieve better performance\nwith less data by manipulating the difficulty of the levels in response to the\nperformance of the agent. The generality of the learned behaviors is also\nevaluated on a set of human-designed levels. The results suggest that the\nability to generalize to human-designed levels highly depends on the design of\nthe level generators. We apply dimensionality reduction and clustering\ntechniques to visualize the generators' distributions of levels and analyze to\nwhat degree they can produce levels similar to those designed by a human.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 01:16:11 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:41:25 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 17:54:57 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 11:52:17 GMT"}, {"version": "v5", "created": "Thu, 29 Nov 2018 18:10:13 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Justesen", "Niels", ""], ["Torrado", "Ruben Rodriguez", ""], ["Bontrager", "Philip", ""], ["Khalifa", "Ahmed", ""], ["Togelius", "Julian", ""], ["Risi", "Sebastian", ""]]}, {"id": "1806.10736", "submitter": "Michael Brand", "authors": "Michael Brand", "title": "Risk-averse estimation, an axiomatic approach to inference, and\n  Wallace-Freeman without MML", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new class of Bayesian point estimators, which we refer to as risk\naverse. Using this definition, we formulate axioms that provide natural\nrequirements for inference, e.g. in a scientific setting, and show that for\nwell-behaved estimation problems the axioms uniquely characterise an estimator.\nNamely, for estimation problems in which some parameter values have a positive\nposterior probability (such as, e.g., problems with a discrete hypothesis\nspace), the axioms characterise Maximum A Posteriori (MAP) estimation, whereas\nelsewhere (such as in continuous estimation) they characterise the\nWallace-Freeman estimator.\n  Our results provide a novel justification for the Wallace-Freeman estimator,\nwhich previously was derived only as an approximation to the\ninformation-theoretic Strict Minimum Message Length estimator. By contrast, our\nderivation requires neither approximations nor coding.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 01:57:37 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 11:51:31 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 03:33:42 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 07:35:07 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Brand", "Michael", ""]]}, {"id": "1806.10745", "submitter": "Akshay Krishnamurthy", "authors": "Dylan J. Foster and Akshay Krishnamurthy", "title": "Contextual bandits with surrogate losses: Margin bounds and efficient\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use surrogate losses to obtain several new regret bounds and new\nalgorithms for contextual bandit learning. Using the ramp loss, we derive new\nmargin-based regret bounds in terms of standard sequential complexity measures\nof a benchmark class of real-valued regression functions. Using the hinge loss,\nwe derive an efficient algorithm with a $\\sqrt{dT}$-type mistake bound against\nbenchmark policies induced by $d$-dimensional regressors. Under realizability\nassumptions, our results also yield classical regret bounds.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:50:38 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 12:50:58 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Foster", "Dylan J.", ""], ["Krishnamurthy", "Akshay", ""]]}, {"id": "1806.10758", "submitter": "Sara Hooker", "authors": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim", "title": "A Benchmark for Interpretability Methods in Deep Neural Networks", "comments": "In NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical measure of the approximate accuracy of feature\nimportance estimates in deep neural networks. Our results across several\nlarge-scale image classification datasets show that many popular\ninterpretability methods produce estimates of feature importance that are not\nbetter than a random designation of feature importance. Only certain ensemble\nbased approaches---VarGrad and SmoothGrad-Squared---outperform such a random\nassignment of importance. The manner of ensembling remains critical, we show\nthat some approaches do no better then the underlying method but carry a far\nhigher computational burden.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 03:46:57 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 21:55:38 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 02:25:30 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Hooker", "Sara", ""], ["Erhan", "Dumitru", ""], ["Kindermans", "Pieter-Jan", ""], ["Kim", "Been", ""]]}, {"id": "1806.10773", "submitter": "Yang Yang", "authors": "Yang Yang and Marius Pesavento and Symeon Chatzinotas and Bj\\\"orn\n  Ottersten", "title": "Successive Convex Approximation Algorithms for Sparse Signal Estimation\n  with Nonconvex Regularizations", "comments": "submitted to IEEE Journal of Selected Topics in Signal Processing,\n  special issue in Robust Subspace Learning", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2877584", "report-no": null, "categories": "cs.LG cs.DC cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a successive convex approximation framework for\nsparse optimization where the nonsmooth regularization function in the\nobjective function is nonconvex and it can be written as the difference of two\nconvex functions. The proposed framework is based on a nontrivial combination\nof the majorization-minimization framework and the successive convex\napproximation framework proposed in literature for a convex regularization\nfunction. The proposed framework has several attractive features, namely, i)\nflexibility, as different choices of the approximate function lead to different\ntype of algorithms; ii) fast convergence, as the problem structure can be\nbetter exploited by a proper choice of the approximate function and the\nstepsize is calculated by the line search; iii) low complexity, as the\napproximate function is convex and the line search scheme is carried out over a\ndifferentiable function; iv) guaranteed convergence to a stationary point. We\ndemonstrate these features by two example applications in subspace learning,\nnamely, the network anomaly detection problem and the sparse subspace\nclustering problem. Customizing the proposed framework by adopting the\nbest-response type approximation, we obtain soft-thresholding with exact line\nsearch algorithms for which all elements of the unknown parameter are updated\nin parallel according to closed-form expressions. The attractive features of\nthe proposed algorithms are illustrated numerically.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:21:16 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""], ["Chatzinotas", "Symeon", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1806.10787", "submitter": "Vijay Gabale Dr", "authors": "Vijay Gabale, Anand Prabhu Subramanian", "title": "How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning", "comments": "6 pages, 3 figures, AI for Fashion, KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 06:23:56 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Gabale", "Vijay", ""], ["Subramanian", "Anand Prabhu", ""]]}, {"id": "1806.10792", "submitter": "Kazeto Yamamoto", "authors": "Kazeto Yamamoto and Takashi Onishi and Yoshimasa Tsuruoka", "title": "Hierarchical Reinforcement Learning with Abductive Planning", "comments": "7 pages, 6 figures, ICML/IJCAI/AAMAS 2018 Workshop on Planning and\n  Learning (PAL-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in applying reinforcement learning to real-life\nproblems is that the amount of train-and-error required to learn a good policy\nincreases drastically as the task becomes complex. One potential solution to\nthis problem is to combine reinforcement learning with automated symbol\nplanning and utilize prior knowledge on the domain. However, existing methods\nhave limitations in their applicability and expressiveness. In this paper we\npropose a hierarchical reinforcement learning method based on abductive\nsymbolic planning. The planner can deal with user-defined evaluation functions\nand is not based on the Herbrand theorem. Therefore it can utilize prior\nknowledge of the rewards and can work in a domain where the state space is\nunknown. We demonstrate empirically that our architecture significantly\nimproves learning efficiency with respect to the amount of training examples on\nthe evaluation domain, in which the state space is unknown and there exist\nmultiple goals.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 06:56:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Yamamoto", "Kazeto", ""], ["Onishi", "Takashi", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1806.10840", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Andrei Stoain, Jean-Fran\\c{c}ois Goudou, David\n  Filliat", "title": "Training Discriminative Models to Evaluate Generative Ones", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30508-6_48", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are known to be difficult to assess. Recent works,\nespecially on generative adversarial networks (GANs), produce good visual\nsamples of varied categories of images. However, the validation of their\nquality is still difficult to define and there is no existing agreement on the\nbest evaluation process. This paper aims at making a step toward an objective\nevaluation process for generative models. It presents a new method to assess a\ntrained generative model by evaluating the test accuracy of a classifier\ntrained with generated data. The test set is composed of real images.\nTherefore, The classifier accuracy is used as a proxy to evaluate if the\ngenerative model fit the true data distribution. By comparing results with\ndifferent generated datasets we are able to classify and compare generative\nmodels. The motivation of this approach is also to evaluate if generative\nmodels can help discriminative neural networks to learn, i.e., measure if\ntraining on generated data is able to make a model successful at testing on\nreal settings. Our experiments compare different generators from the\nVariational Auto-Encoders (VAE) and Generative Adversarial Network (GAN)\nframeworks on MNIST and fashion MNIST datasets. Our results show that none of\nthe generative models is able to replace completely true data to train a\ndiscriminative model. But they also show that the initial GAN and WGAN are the\nbest choices to generate on MNIST database (Modified National Institute of\nStandards and Technology database) and fashion MNIST database.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 09:23:07 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 14:03:55 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["Stoain", "Andrei", ""], ["Goudou", "Jean-Fran\u00e7ois", ""], ["Filliat", "David", ""]]}, {"id": "1806.10861", "submitter": "L\\'eo Gautheron", "authors": "L\\'eo Gautheron, Ievgen Redko, Carole Lartizien", "title": "Feature Selection for Unsupervised Domain Adaptation using Optimal\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new feature selection method for unsupervised\ndomain adaptation based on the emerging optimal transportation theory. We build\nupon a recent theoretical analysis of optimal transport in domain adaptation\nand show that it can directly suggest a feature selection procedure leveraging\nthe shift between the domains. Based on this, we propose a novel algorithm that\naims to sort features by their similarity across the source and target domains,\nwhere the order is obtained by analyzing the coupling matrix representing the\nsolution of the proposed optimal transportation problem. We evaluate our method\non a well-known benchmark data set and illustrate its capability of selecting\ncorrelated features leading to better classification performances. Furthermore,\nwe show that the proposed algorithm can be used as a pre-processing step for\nexisting domain adaptation techniques ensuring an important speed-up in terms\nof the computational time while maintaining comparable results. Finally, we\nvalidate our algorithm on clinical imaging databases for computer-aided\ndiagnosis task with promising results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 10:04:54 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Gautheron", "L\u00e9o", ""], ["Redko", "Ievgen", ""], ["Lartizien", "Carole", ""]]}, {"id": "1806.10873", "submitter": "Seth Nabarro", "authors": "Seth Nabarro, Tristan Fletcher and John Shawe-Taylor", "title": "Spatiotemporal Prediction of Ambulance Demand using Gaussian Process\n  Regression", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting when and where ambulance call-outs occur can reduce\nresponse times and ensure the patient receives urgent care sooner. Here we\npresent a novel method for ambulance demand prediction using Gaussian process\nregression (GPR) in time and geographic space. The method exhibits superior\naccuracy to MEDIC, a method which has been used in industry. The use of GPR has\nadditional benefits such as the quantification of uncertainty with each\nprediction, the choice of kernel functions to encode prior knowledge and the\nability to capture spatial correlation. Measures to increase the utility of GPR\nin the current context, with large training sets and a Poisson-distributed\noutput, are outlined.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 10:45:26 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Nabarro", "Seth", ""], ["Fletcher", "Tristan", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1806.10897", "submitter": "Mathias Kraus", "authors": "Mathias Kraus, Stefan Feuerriegel, Asil Oztekin", "title": "Deep learning in business analytics and operations research: Models,\n  applications and managerial implications", "comments": "Accepted for publication in European Journal of Operational Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business analytics refers to methods and practices that create value through\ndata for individuals, firms, and organizations. This field is currently\nexperiencing a radical shift due to the advent of deep learning: deep neural\nnetworks promise improvements in prediction performance as compared to models\nfrom traditional machine learning. However, our research into the existing body\nof literature reveals a scarcity of research works utilizing deep learning in\nour discipline. Accordingly, the objectives of this overview article are as\nfollows: (1) we review research on deep learning for business analytics from an\noperational point of view. (2) We motivate why researchers and practitioners\nfrom business analytics should utilize deep neural networks and review\npotential use cases, necessary requirements, and benefits. (3) We investigate\nthe added value to operations research in different case studies with real data\nfrom entrepreneurial undertakings. All such cases demonstrate improvements in\noperational performance over traditional machine learning and thus direct value\ngains. (4) We provide guidelines and implications for researchers, managers and\npractitioners in operations research who want to advance their capabilities for\nbusiness analytics with regard to deep learning. (5) Our computational\nexperiments find that default, out-of-the-box architectures are often\nsuboptimal and thus highlight the value of customized architectures by\nproposing a novel deep-embedded network.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 11:48:36 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 15:43:36 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 08:09:59 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Kraus", "Mathias", ""], ["Feuerriegel", "Stefan", ""], ["Oztekin", "Asil", ""]]}, {"id": "1806.10909", "submitter": "Hongzhou Lin", "authors": "Hongzhou Lin, Stefanie Jegelka", "title": "ResNet with one-neuron hidden layers is a Universal Approximator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a very deep ResNet with stacked modules with one neuron\nper hidden layer and ReLU activation functions can uniformly approximate any\nLebesgue integrable function in $d$ dimensions, i.e. $\\ell_1(\\mathbb{R}^d)$.\nBecause of the identity mapping inherent to ResNets, our network has\nalternating layers of dimension one and $d$. This stands in sharp contrast to\nfully connected networks, which are not universal approximators if their width\nis the input dimension $d$ [Lu et al, 2017; Hanin and Sellke, 2017]. Hence, our\nresult implies an increase in representational power for narrow deep networks\nby the ResNet architecture.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 12:15:44 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 14:08:21 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Lin", "Hongzhou", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1806.10961", "submitter": "Philipp Probst", "authors": "Daniel K\\\"uhn, Philipp Probst, Janek Thomas and Bernd Bischl", "title": "Automatic Exploration of Machine Learning Experiments on OpenML", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of hyperparameters on the performance of a\nmachine learning algorithm is an important scientific topic in itself and can\nhelp to improve automatic hyperparameter tuning procedures. Unfortunately,\nexperimental meta data for this purpose is still rare. This paper presents a\nlarge, free and open dataset addressing this problem, containing results on 38\nOpenML data sets, six different machine learning algorithms and many different\nhyperparameter configurations. Results where generated by an automated random\nsampling strategy, termed the OpenML Random Bot. Each algorithm was\ncross-validated up to 20.000 times per dataset with different hyperparameters\nsettings, resulting in a meta dataset of around 2.5 million experiments\noverall.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 13:37:10 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:48:38 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:06:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["K\u00fchn", "Daniel", ""], ["Probst", "Philipp", ""], ["Thomas", "Janek", ""], ["Bischl", "Bernd", ""]]}, {"id": "1806.11006", "submitter": "Suman Ravuri", "authors": "Suman Ravuri, Shakir Mohamed, Mihaela Rosca, Oriol Vinyals", "title": "Learning Implicit Generative Models with the Method of Learned Moments", "comments": "ICML 2018, 6 figures, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of moments (MoM) algorithm for training large-scale\nimplicit generative models. Moment estimation in this setting encounters two\nproblems: it is often difficult to define the millions of moments needed to\nlearn the model parameters, and it is hard to determine which properties are\nuseful when specifying moments. To address the first issue, we introduce a\nmoment network, and define the moments as the network's hidden units and the\ngradient of the network's output with the respect to its parameters. To tackle\nthe second problem, we use asymptotic theory to highlight desiderata for\nmoments -- namely they should minimize the asymptotic variance of estimated\nmodel parameters -- and introduce an objective to learn better moments. The\nsequence of objectives created by this Method of Learned Moments (MoLM) can\ntrain high-quality neural image samplers. On CIFAR-10, we demonstrate that\nMoLM-trained generators achieve significantly higher Inception Scores and lower\nFrechet Inception Distances than those trained with gradient\npenalty-regularized and spectrally-normalized adversarial objectives. These\ngenerators also achieve nearly perfect Multi-Scale Structural Similarity Scores\non CelebA, and can create high-quality samples of 128x128 images.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:30:26 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ravuri", "Suman", ""], ["Mohamed", "Shakir", ""], ["Rosca", "Mihaela", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1806.11015", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Eduardo C. Garrido-Merch\\'an, Daniel\n  Hern\\'andez-Lobato, Concha Bielza, Pedro Larra\\~naga", "title": "Bayesian optimization of the PC algorithm for learning Gaussian Bayesian\n  networks", "comments": null, "journal-ref": "Lecture Notes in Artificial Intelligence (CAEPIA 2018),\n  11160:44:54, 2018", "doi": "10.1007/978-3-030-00374-6_5", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PC algorithm is a popular method for learning the structure of Gaussian\nBayesian networks. It carries out statistical tests to determine absent edges\nin the network. It is hence governed by two parameters: (i) The type of test,\nand (ii) its significance level. These parameters are usually set to values\nrecommended by an expert. Nevertheless, such an approach can suffer from human\nbias, leading to suboptimal reconstruction results. In this paper we consider a\nmore principled approach for choosing these parameters in an automatic way. For\nthis we optimize a reconstruction score evaluated on a set of different\nGaussian Bayesian networks. This objective is expensive to evaluate and lacks a\nclosed-form expression, which means that Bayesian optimization (BO) is a\nnatural choice. BO methods use a model to guide the search and are hence able\nto exploit smoothness properties of the objective surface. We show that the\nparameters found by a BO method outperform those found by a random search\nstrategy and the expert recommendation. Importantly, we have found that an\noften overlooked statistical test provides the best over-all reconstruction\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:44:22 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Garrido-Merch\u00e1n", "Eduardo C.", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1806.11027", "submitter": "Kaiwen Zhou", "authors": "Kaiwen Zhou, Fanhua Shang, James Cheng", "title": "A Simple Stochastic Variance Reduced Algorithm with Fast Convergence\n  Rates", "comments": "ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed exciting progress in the study of stochastic\nvariance reduced gradient methods (e.g., SVRG, SAGA), their accelerated\nvariants (e.g, Katyusha) and their extensions in many different settings (e.g.,\nonline, sparse, asynchronous, distributed). Among them, accelerated methods\nenjoy improved convergence rates but have complex coupling structures, which\nmakes them hard to be extended to more settings (e.g., sparse and asynchronous)\ndue to the existence of perturbation. In this paper, we introduce a simple\nstochastic variance reduced algorithm (MiG), which enjoys the best-known\nconvergence rates for both strongly convex and non-strongly convex problems.\nMoreover, we also present its efficient sparse and asynchronous variants, and\ntheoretically analyze its convergence rates in these settings. Finally,\nextensive experiments for various machine learning problems such as logistic\nregression are given to illustrate the practical improvement in both serial and\nasynchronous settings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:08:58 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zhou", "Kaiwen", ""], ["Shang", "Fanhua", ""], ["Cheng", "James", ""]]}, {"id": "1806.11038", "submitter": "Fatemeh Shah-Mohammadi", "authors": "Fatemeh Shah-Mohammadi and Andres Kwasinski", "title": "Neural Network Cognitive Engine for Autonomous and Distributed Underlay\n  Dynamic Spectrum Access", "comments": "Submitted to IEEE Access Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two key challenges in underlay dynamic spectrum access (DSA) are how to\nestablish an interference limit from the primary network (PN) and how cognitive\nradios (CRs) in the secondary network (SN) become aware of the interference\nthey create on the PN, especially when there is no exchange of information\nbetween the two networks. These challenges are addressed in this paper by\npresenting a fully autonomous and distributed underlay DSA scheme where each CR\noperates based on predicting its transmission effect on the PN. The scheme is\nbased on a cognitive engine with an artificial neural network that predicts,\nwithout exchanging information between the networks, the adaptive modulation\nand coding configuration for the primary link nearest to a transmitting CR. By\nmanaging the effect of the SN on the PN, the presented technique maintains the\nrelative average throughput change in the PN within a prescribed maximum value,\nwhile also finding transmit settings for the CRs that result in throughput as\nlarge as allowed by the PN interference limit. Simulation results show that the\nability of the cognitive engine in estimating the effect of a CR transmission\non the full adaptive modulation and coding (AMC) mode leads to a much more fine\nunderlay transmit power control. This ability also provides higher transmission\nopportunities for the CRs, compared to a scheme that can only estimate the\nmodulation scheme used at the PN link.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:35:12 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 17:08:23 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 02:12:40 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 21:02:14 GMT"}, {"version": "v5", "created": "Mon, 5 Oct 2020 00:44:50 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shah-Mohammadi", "Fatemeh", ""], ["Kwasinski", "Andres", ""]]}, {"id": "1806.11048", "submitter": "Kaiwen Zhou", "authors": "Kaiwen Zhou", "title": "Direct Acceleration of SAGA using Sampled Negative Momentum", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction is a simple and effective technique that accelerates\nconvex (or non-convex) stochastic optimization. Among existing variance\nreduction methods, SVRG and SAGA adopt unbiased gradient estimators and are the\nmost popular variance reduction methods in recent years. Although various\naccelerated variants of SVRG (e.g., Katyusha and Acc-Prox-SVRG) have been\nproposed, the direct acceleration of SAGA still remains unknown. In this paper,\nwe propose a directly accelerated variant of SAGA using a novel Sampled\nNegative Momentum (SSNM), which achieves the best known oracle complexity for\nstrongly convex problems (with known strong convexity parameter). Consequently,\nour work fills the void of directly accelerated SAGA.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 16:00:48 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 16:10:12 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 14:31:58 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 14:26:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhou", "Kaiwen", ""]]}, {"id": "1806.11096", "submitter": "Eric Chi", "authors": "Eric C. Chi and Stefan Steinerberger", "title": "Recovering Trees with Convex Clustering", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering refers, for given $\\left\\{x_1, \\dots, x_n\\right\\} \\subset\n\\mathbb{R}^p$, to the minimization of \\begin{eqnarray*} u(\\gamma) & = &\n\\underset{u_1, \\dots, u_n }{\\arg\\min}\\;\\sum_{i=1}^{n}{\\lVert x_i - u_i\n\\rVert^2} + \\gamma \\sum_{i,j=1}^{n}{w_{ij} \\lVert u_i - u_j\\rVert},\\\\\n\\end{eqnarray*} where $w_{ij} \\geq 0$ is an affinity that quantifies the\nsimilarity between $x_i$ and $x_j$. We prove that if the affinities $w_{ij}$\nreflect a tree structure in the $\\left\\{x_1, \\dots, x_n\\right\\}$, then the\nconvex clustering solution path reconstructs the tree exactly. The main\ntechnical ingredient implies the following combinatorial byproduct: for every\nset $\\left\\{x_1, \\dots, x_n \\right\\} \\subset \\mathbb{R}^p$ of $n \\geq 2$\ndistinct points, there exist at least $n/6$ points with the property that for\nany of these points $x$ there is a unit vector $v \\in \\mathbb{R}^p$ such that,\nwhen viewed from $x$, `most' points lie in the direction $v$ \\begin{eqnarray*}\n\\frac{1}{n-1}\\sum_{i=1 \\atop x_i \\neq x}^{n}{ \\left\\langle \\frac{x_i -\nx}{\\lVert x_i - x \\rVert}, v \\right\\rangle} & \\geq & \\frac{1}{4}.\n\\end{eqnarray*}\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 17:36:23 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 00:59:46 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chi", "Eric C.", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1806.11146", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed, Ian Goodfellow, Jascha Sohl-Dickstein", "title": "Adversarial Reprogramming of Neural Networks", "comments": null, "journal-ref": "International Conference on Learning Representations 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are susceptible to \\emph{adversarial} attacks. In\ncomputer vision, well-crafted perturbations to images can cause neural networks\nto make mistakes such as confusing a cat with a computer. Previous adversarial\nattacks have been designed to degrade performance of models or cause machine\nlearning models to produce specific outputs chosen ahead of time by the\nattacker. We introduce attacks that instead {\\em reprogram} the target model to\nperform a task chosen by the attacker---without the attacker needing to specify\nor compute the desired output for each test-time input. This attack finds a\nsingle adversarial perturbation, that can be added to all test-time inputs to a\nmachine learning model in order to cause the model to perform a task chosen by\nthe adversary---even if the model was not trained to do this task. These\nperturbations can thus be considered a program for the new task. We demonstrate\nadversarial reprogramming on six ImageNet classification models, repurposing\nthese models to perform a counting task, as well as classification tasks:\nclassification of MNIST and CIFAR-10 examples presented as inputs to the\nImageNet model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 19:06:26 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 22:50:01 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Goodfellow", "Ian", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.11187", "submitter": "Guofei Pang", "authors": "Guofei Pang, Liu Yang, George Em Karniadakis", "title": "Neural-net-induced Gaussian process regression for function\n  approximation and PDE solution", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.01.045", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-net-induced Gaussian process (NNGP) regression inherits both the high\nexpressivity of deep neural networks (deep NNs) as well as the uncertainty\nquantification property of Gaussian processes (GPs). We generalize the current\nNNGP to first include a larger number of hyperparameters and subsequently train\nthe model by maximum likelihood estimation. Unlike previous works on NNGP that\ntargeted classification, here we apply the generalized NNGP to function\napproximation and to solving partial differential equations (PDEs).\nSpecifically, we develop an analytical iteration formula to compute the\ncovariance function of GP induced by deep NN with an error-function\nnonlinearity. We compare the performance of the generalized NNGP for function\napproximations and PDE solutions with those of GPs and fully-connected NNs. We\nobserve that for smooth functions the generalized NNGP can yield the same order\nof accuracy with GP, while both NNGP and GP outperform deep NN. For non-smooth\nfunctions, the generalized NNGP is superior to GP and comparable or superior to\ndeep NN.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 03:02:36 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Pang", "Guofei", ""], ["Yang", "Liu", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1806.11189", "submitter": "Veera Raghavendra Chikka", "authors": "Veera Raghavendra Chikka, Kamalakar Karlapalem", "title": "A hybrid deep learning approach for medical relation extraction", "comments": "4 pages, 4 tables, 1 figure, 2018 KDD workshop on Machine Learning\n  for Medicine and Healthcare", "journal-ref": "MLMH 2018 2018 KDD workshop on Machine Learning for Medicine and\n  Healthcare. London, United Kingdom. August 20, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Mining relationships between treatment(s) and medical problem(s) is vital in\nthe biomedical domain. This helps in various applications, such as decision\nsupport system, safety surveillance, and new treatment discovery. We propose a\ndeep learning approach that utilizes both word level and sentence-level\nrepresentations to extract the relationships between treatment and problem.\nWhile deep learning techniques demand a large amount of data for training, we\nmake use of a rule-based system particularly for relationship classes with\nfewer samples. Our final relations are derived by jointly combining the results\nfrom deep learning and rule-based models. Our system achieved a promising\nperformance on the relationship classes of I2b2 2010 relation extraction task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:38:01 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chikka", "Veera Raghavendra", ""], ["Karlapalem", "Kamalakar", ""]]}, {"id": "1806.11202", "submitter": "Serena Wang", "authors": "Serena Wang, Maya Gupta, Seungil You", "title": "Quit When You Can: Efficient Evaluation of Ensembles with Ordering\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a classifier ensemble and a set of examples to be classified, many\nexamples may be confidently and accurately classified after only a subset of\nthe base models in the ensemble are evaluated. This can reduce both mean\nlatency and CPU while maintaining the high accuracy of the original ensemble.\nTo achieve such gains, we propose jointly optimizing a fixed evaluation order\nof the base models and early-stopping thresholds. Our proposed objective is a\ncombinatorial optimization problem, but we provide a greedy algorithm that\nachieves a 4-approximation of the optimal solution for certain cases. For those\ncases, this is also the best achievable polynomial time approximation bound\nunless $P = NP$. Experiments on benchmark and real-world problems show that the\nproposed Quit When You Can (QWYC) algorithm can speed-up average evaluation\ntime by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC's joint\noptimization of ordering and thresholds also performed better in experiments\nthan various fixed orderings, including gradient boosted trees' ordering.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 21:29:23 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Wang", "Serena", ""], ["Gupta", "Maya", ""], ["You", "Seungil", ""]]}, {"id": "1806.11212", "submitter": "Serena Wang", "authors": "Maya Gupta, Andrew Cotter, Mahdi Milani Fard, Serena Wang", "title": "Proxy Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of improving fairness when one lacks access to a\ndataset labeled with protected groups, making it difficult to take advantage of\nstrategies that can improve fairness but require protected group labels, either\nat training or runtime. To address this, we investigate improving fairness\nmetrics for proxy groups, and test whether doing so results in improved\nfairness for the true sensitive groups. Results on benchmark and real-world\ndatasets demonstrate that such a proxy fairness strategy can work well in\npractice. However, we caution that the effectiveness likely depends on the\nchoice of fairness metric, as well as how aligned the proxy groups are with the\ntrue protected groups in terms of the constrained model parameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:05:46 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Gupta", "Maya", ""], ["Cotter", "Andrew", ""], ["Fard", "Mahdi Milani", ""], ["Wang", "Serena", ""]]}, {"id": "1806.11222", "submitter": "Jason Ansel", "authors": "Dongqi Su, Ying Yin Ting, Jason Ansel", "title": "Tight Prediction Intervals Using Expanded Interval Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction intervals are a valuable way of quantifying uncertainty in\nregression problems. Good prediction intervals should be both correct,\ncontaining the actual value between the lower and upper bound at least a target\npercentage of the time; and tight, having a small mean width of the bounds.\nMany prior techniques for generating prediction intervals make assumptions on\nthe distribution of error, which causes them to work poorly for problems with\nasymmetric distributions.\n  This paper presents Expanded Interval Minimization (EIM), a novel loss\nfunction for generating prediction intervals using neural networks. This loss\nfunction uses minibatch statistics to estimate the coverage and optimize the\nwidth of the prediction intervals. It does not make the same assumptions on the\ndistributions of data and error as prior work. We compare to three published\ntechniques and show EIM produces on average 1.37x tighter prediction intervals\nand in the worst case 1.06x tighter intervals across two large real-world\ndatasets and varying coverage levels.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:04:15 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Su", "Dongqi", ""], ["Ting", "Ying Yin", ""], ["Ansel", "Jason", ""]]}, {"id": "1806.11244", "submitter": "Wonjoon Goo", "authors": "Wonjoon Goo, Scott Niekum", "title": "One-Shot Learning of Multi-Step Tasks from Observation via Activity\n  Localization in Auxiliary Video", "comments": "ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to burdensome data requirements, learning from demonstration often falls\nshort of its promise to allow users to quickly and naturally program robots.\nDemonstrations are inherently ambiguous and incomplete, making correct\ngeneralization to unseen situations difficult without a large number of\ndemonstrations in varying conditions. By contrast, humans are often able to\nlearn complex tasks from a single demonstration (typically observations without\naction labels) by leveraging context learned over a lifetime. Inspired by this\ncapability, our goal is to enable robots to perform one-shot learning of\nmulti-step tasks from observation by leveraging auxiliary video data as\ncontext. Our primary contribution is a novel system that achieves this goal by:\n(1) using a single user-segmented demonstration to define the primitive actions\nthat comprise a task, (2) localizing additional examples of these actions in\nunsegmented auxiliary videos via a metalearning-based approach, (3) using these\nadditional examples to learn a reward function for each action, and (4)\nperforming reinforcement learning on top of the inferred reward functions to\nlearn action policies that can be combined to accomplish the task. We\nempirically demonstrate that a robot can learn multi-step tasks more\neffectively when provided auxiliary video, and that performance greatly\nimproves when localizing individual actions, compared to learning from\nunsegmented videos.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 01:11:56 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 01:39:26 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 15:01:09 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Goo", "Wonjoon", ""], ["Niekum", "Scott", ""]]}, {"id": "1806.11248", "submitter": "Rory Mitchell", "authors": "Rory Mitchell, Andrey Adinets, Thejaswi Rao, Eibe Frank", "title": "XGBoost: Scalable GPU Accelerated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the multi-GPU gradient boosting algorithm implemented in the\nXGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,\nscalable training on multi-GPU systems with all of the features of the XGBoost\nlibrary. We employ data compression techniques to minimise the usage of scarce\nGPU memory while still allowing highly efficient implementation. Using our\nalgorithm we show that it is possible to process 115 million training instances\nin under three minutes on a publicly available cloud computing instance. The\nalgorithm is implemented using end-to-end GPU parallelism, with prediction,\ngradient calculation, feature quantisation, decision tree construction and\nevaluation phases all computed on device.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 02:05:32 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Mitchell", "Rory", ""], ["Adinets", "Andrey", ""], ["Rao", "Thejaswi", ""], ["Frank", "Eibe", ""]]}, {"id": "1806.11258", "submitter": "Chuanxing Geng", "authors": "Chuanxing Geng and Songcan Chen", "title": "Collective decision for open set recognition", "comments": "Accepted by IEEE TKDE. Previous title was \"Hierarchical Dirichlet\n  Process-based Open Set Recognition\"", "journal-ref": null, "doi": "10.1109/TKDE.2020.2978199", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open set recognition (OSR), almost all existing methods are designed\nspecially for recognizing individual instances, even these instances are\ncollectively coming in batch. Recognizers in decision either reject or\ncategorize them to some known class using empirically-set threshold. Thus the\ndecision threshold plays a key role. However, the selection for it usually\ndepends on the knowledge of known classes, inevitably incurring risks due to\nlacking available information from unknown classes. On the other hand, a more\nrealistic OSR system should NOT just rest on a reject decision but should go\nfurther, especially for discovering the hidden unknown classes among the reject\ninstances, whereas existing OSR methods do not pay special attention. In this\npaper, we introduce a novel collective/batch decision strategy with an aim to\nextend existing OSR for new class discovery while considering correlations\namong the testing instances. Specifically, a collective decision-based OSR\nframework (CD-OSR) is proposed by slightly modifying the Hierarchical Dirichlet\nprocess (HDP). Thanks to HDP, our CD-OSR does not need to define the decision\nthreshold and can implement the open set recognition and new class discovery\nsimultaneously. Finally, extensive experiments on benchmark datasets indicate\nthe validity of CD-OSR.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 03:32:29 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 13:12:09 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 03:18:11 GMT"}, {"version": "v4", "created": "Sat, 29 Jun 2019 07:48:12 GMT"}, {"version": "v5", "created": "Sat, 21 Mar 2020 13:39:29 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Geng", "Chuanxing", ""], ["Chen", "Songcan", ""]]}, {"id": "1806.11302", "submitter": "Zigeng Xia", "authors": "Fuzhou Gong and Zigeng Xia", "title": "Generate the corresponding Image from Text Description using Modified\n  GAN-CLS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing images or texts automatically is a useful research area in the\nartificial intelligence nowadays. Generative adversarial networks (GANs), which\nare proposed by Goodfellow in 2014, make this task to be done more efficiently\nby using deep neural networks. We consider generating corresponding images from\nan input text description using a GAN. In this paper, we analyze the GAN-CLS\nalgorithm, which is a kind of advanced method of GAN proposed by Scott Reed in\n2016. First, we find the problem with this algorithm through inference. Then we\ncorrect the GAN-CLS algorithm according to the inference by modifying the\nobjective function of the model. Finally, we do the experiments on the\nOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can\ngenerate images which are more plausible than the GAN-CLS algorithm in some\ncases. Also, some of the generated images match the input texts better.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:31:07 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Gong", "Fuzhou", ""], ["Xia", "Zigeng", ""]]}, {"id": "1806.11311", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Guaranteed Deterministic Bounds on the Total Variation Distance between\n  Univariate Mixtures", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation distance is a core statistical distance between\nprobability measures that satisfies the metric axioms, with value always\nfalling in $[0,1]$. This distance plays a fundamental role in machine learning\nand signal processing: It is a member of the broader class of $f$-divergences,\nand it is related to the probability of error in Bayesian hypothesis testing.\nSince the total variation distance does not admit closed-form expressions for\nstatistical mixtures (like Gaussian mixture models), one often has to rely in\npractice on costly numerical integrations or on fast Monte Carlo approximations\nthat however do not guarantee deterministic lower and upper bounds. In this\nwork, we consider two methods for bounding the total variation of univariate\nmixture models: The first method is based on the information monotonicity\nproperty of the total variation to design guaranteed nested deterministic lower\nbounds. The second method relies on computing the geometric lower and upper\nenvelopes of weighted mixture components to derive deterministic bounds based\non density ratio. We demonstrate the tightness of our bounds in a series of\nexperiments on Gaussian, Gamma and Rayleigh mixture models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 09:22:41 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1806.11326", "submitter": "Jacob Kauffmann", "authors": "Jacob Kauffmann, Gr\\'egoire Montavon, Luiz Alberto Lima, Shinichi\n  Nakajima, Klaus-Robert M\\\"uller, Nico G\\\"ornitz", "title": "Unsupervised Detection and Explanation of Latent-class Contextual\n  Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and explaining anomalies is a challenging effort. This holds\nespecially true when data exhibits strong dependencies and single measurements\nneed to be assessed and analyzed in their respective context. In this work, we\nconsider scenarios where measurements are non-i.i.d, i.e. where samples are\ndependent on corresponding discrete latent variables which are connected\nthrough some given dependency structure, the contextual information. Our\ncontribution is twofold: (i) Building atop of support vector data description\n(SVDD), we derive a method able to cope with latent-class dependency structure\nthat can still be optimized efficiently. We further show that our approach\nneatly generalizes vanilla SVDD as well as k-means and conditional random\nfields (CRF) and provide a corresponding probabilistic interpretation. (ii) In\nunsupervised scenarios where it is not possible to quantify the accuracy of an\nanomaly detector, having an human-interpretable solution is the key to success.\nBased on deep Taylor decomposition and a reformulation of our trained anomaly\ndetector as a neural network, we are able to backpropagate predictions to\npixel-domain and thus identify features and regions of high relevance. We\ndemonstrate the usefulness of our novel approach on toy data with known\nspatio-temporal structure and successfully validate on synthetic as well as\nreal world off-shore data from the oil industry.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 09:53:35 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Kauffmann", "Jacob", ""], ["Montavon", "Gr\u00e9goire", ""], ["Lima", "Luiz Alberto", ""], ["Nakajima", "Shinichi", ""], ["M\u00fcller", "Klaus-Robert", ""], ["G\u00f6rnitz", "Nico", ""]]}, {"id": "1806.11332", "submitter": "Naoya Takeishi", "authors": "Naoya Takeishi and Kosuke Akimoto", "title": "Knowledge-Based Distant Regularization in Learning Probabilistic Models", "comments": "7 pages, 2 figures, presented in the Eighth International Workshop on\n  Statistical Relational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the appropriate inductive bias based on the knowledge of data is\nessential for achieving good performance in statistical machine learning. In\npractice, however, the domain knowledge of interest often provides information\non the relationship of data attributes only distantly, which hinders direct\nutilization of such domain knowledge in popular regularization methods. In this\npaper, we propose the knowledge-based distant regularization framework, in\nwhich we utilize the distant information encoded in a knowledge graph for\nregularization of probabilistic model estimation. In particular, we propose to\nimpose prior distributions on model parameters specified by knowledge graph\nembeddings. As an instance of the proposed framework, we present the factor\nanalysis model with the knowledge-based distant regularization. We show the\nresults of preliminary experiments on the improvement of the generalization\ncapability of such model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:13:01 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Takeishi", "Naoya", ""], ["Akimoto", "Kosuke", ""]]}, {"id": "1806.11345", "submitter": "Jinsung Yoon", "authors": "James Jordon, Jinsung Yoon, Mihaela van der Schaar", "title": "Measuring the quality of Synthetic data for use in competitions", "comments": "3 pages, 1 figure, 2018 KDD Workshop on Machine Learning for Medicine\n  and Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has the potential to assist many communities in using the\nlarge datasets that are becoming more and more available. Unfortunately, much\nof that potential is not being realized because it would require sharing data\nin a way that compromises privacy. In order to overcome this hurdle, several\nmethods have been proposed that generate synthetic data while preserving the\nprivacy of the real data. In this paper we consider a key characteristic that\nsynthetic data should have in order to be useful for machine learning\nresearchers - the relative performance of two algorithms (trained and tested)\non the synthetic dataset should be the same as their relative performance (when\ntrained and tested) on the original dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:39:59 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Jordon", "James", ""], ["Yoon", "Jinsung", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1806.11377", "submitter": "Rune Kok Nielsen", "authors": "Rune Kok Nielsen, Andreas Nugaard Holm, Aasa Feragen", "title": "Learning from graphs with structural variation", "comments": "Presented at the NIPS 2017 workshop \"Learning on Distributions,\n  Functions, Graphs and Groups\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of structural variation in graph data on the predictive\nperformance of graph kernels. To this end, we introduce a novel, noise-robust\nadaptation of the GraphHopper kernel and validate it on benchmark data,\nobtaining modestly improved predictive performance on a range of datasets.\nNext, we investigate the performance of the state-of-the-art Weisfeiler-Lehman\ngraph kernel under increasing synthetic structural errors and find that the\neffect of introducing errors depends strongly on the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:29:12 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Nielsen", "Rune Kok", ""], ["Holm", "Andreas Nugaard", ""], ["Feragen", "Aasa", ""]]}, {"id": "1806.11379", "submitter": "Andrzej Banburski", "authors": "Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier\n  Boix and Jack Hidary", "title": "Theory IIIb: Generalization in Deep Networks", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main puzzle of deep neural networks (DNNs) revolves around the apparent\nabsence of \"overfitting\", defined in this paper as follows: the expected error\ndoes not get worse when increasing the number of neurons or of iterations of\ngradient descent. This is surprising because of the large capacity demonstrated\nby DNNs to fit randomly labeled data and the absence of explicit\nregularization. Recent results by Srebro et al. provide a satisfying solution\nof the puzzle for linear networks used in binary classification. They prove\nthat minimization of loss functions such as the logistic, the cross-entropy and\nthe exp-loss yields asymptotic, \"slow\" convergence to the maximum margin\nsolution for linearly separable datasets, independently of the initial\nconditions. Here we prove a similar result for nonlinear multilayer DNNs near\nzero minima of the empirical loss. The result holds for exponential-type losses\nbut not for the square loss. In particular, we prove that the weight matrix at\neach layer of a deep network converges to a minimum norm solution up to a scale\nfactor (in the separable case). Our analysis of the dynamical system\ncorresponding to gradient descent of a multilayer network suggests a simple\ncriterion for ranking the generalization performance of different zero\nminimizers of the empirical loss.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:39:08 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Poggio", "Tomaso", ""], ["Liao", "Qianli", ""], ["Miranda", "Brando", ""], ["Banburski", "Andrzej", ""], ["Boix", "Xavier", ""], ["Hidary", "Jack", ""]]}, {"id": "1806.11382", "submitter": "Samuel Barnett", "authors": "Samuel A. Barnett", "title": "Convergence Problems with Generative Adversarial Networks (GANs)", "comments": "47 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) are a novel approach to generative\nmodelling, a task whose goal it is to learn a distribution of real data points.\nThey have often proved difficult to train: GANs are unlike many techniques in\nmachine learning, in that they are best described as a two-player game between\na discriminator and generator. This has yielded both unreliability in the\ntraining process, and a general lack of understanding as to how GANs converge,\nand if so, to what. The purpose of this dissertation is to provide an account\nof the theory of GANs suitable for the mathematician, highlighting both\npositive and negative results. This involves identifying the problems when\ntraining GANs, and how topological and game-theoretic perspectives of GANs have\ncontributed to our understanding and improved our techniques in recent years.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:41:12 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Barnett", "Samuel A.", ""]]}, {"id": "1806.11391", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic, Alberto Garcia-Duran, Mathias Niepert", "title": "A Comparative Study of Distributional and Symbolic Paradigms for\n  Relational Learning", "comments": "corrected version: incorrect evaluation fixed; IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world domains can be expressed as graphs and, more generally, as\nmulti-relational knowledge graphs. Though reasoning and learning with knowledge\ngraphs has traditionally been addressed by symbolic approaches, recent methods\nin (deep) representation learning has shown promising results for specialized\ntasks such as knowledge base completion. These approaches abandon the\ntraditional symbolic paradigm by replacing symbols with vectors in Euclidean\nspace. With few exceptions, symbolic and distributional approaches are explored\nin different communities and little is known about their respective strengths\nand weaknesses. In this work, we compare representation learning and relational\nlearning on various relational classification and clustering tasks and analyse\nthe complexity of the rules used implicitly by these approaches. Preliminary\nresults reveal possible indicators that could help in choosing one approach\nover the other for particular knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:01:24 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 09:03:54 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 10:52:15 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 17:59:22 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Garcia-Duran", "Alberto", ""], ["Niepert", "Mathias", ""]]}, {"id": "1806.11416", "submitter": "Mohammad Mehrabi", "authors": "Mohammad Mehrabi, Aslan Tchamkerten, Mansoor I. Yousefi", "title": "Bounds on the Approximation Power of Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation power of general feedforward neural networks with piecewise\nlinear activation functions is investigated. First, lower bounds on the size of\na network are established in terms of the approximation error and network depth\nand width. These bounds improve upon state-of-the-art bounds for certain\nclasses of functions, such as strongly convex functions. Second, an upper bound\nis established on the difference of two neural networks with identical weights\nbut different activation functions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:50:19 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Mehrabi", "Mohammad", ""], ["Tchamkerten", "Aslan", ""], ["Yousefi", "Mansoor I.", ""]]}, {"id": "1806.11424", "submitter": "Aniket Jain", "authors": "Aniket Jain, Yadunath Gupta, Pawan Kumar Singh, Aruna Rajan", "title": "Understanding Fashionability: What drives sales of a style?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use customer demand data for fashion articles on Myntra, and derive a\nfashionability or style quotient, which represents customer demand for the\nstylistic content of a fashion article, decoupled with its commercials (price,\noffers, etc.). We demonstrate learning for assortment planning in fashion that\nwould aim to keep a healthy mix of breadth and depth across various styles, and\nwe show the relationship between a customer's perception of a style vs a\nmerchandiser's catalogue of styles. We also backtest our method to calculate\nprediction errors in our style quotient and customer demand, and discuss\nvarious implications and findings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 17:09:58 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Jain", "Aniket", ""], ["Gupta", "Yadunath", ""], ["Singh", "Pawan Kumar", ""], ["Rajan", "Aruna", ""]]}, {"id": "1806.11429", "submitter": "Shuyang Ling", "authors": "Shuyang Ling, Thomas Strohmer", "title": "Certifying Global Optimality of Graph Cuts via Semidefinite Relaxation:\n  A Performance Guarantee for Spectral Clustering", "comments": "The assumptions of the main theorem, i.e., (3.4) and (3.5), are\n  significantly improved which now are independent of the number clusters k", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has become one of the most widely used clustering\ntechniques when the structure of the individual clusters is non-convex or\nhighly anisotropic. Yet, despite its immense popularity, there exists fairly\nlittle theory about performance guarantees for spectral clustering. This issue\nis partly due to the fact that spectral clustering typically involves two steps\nwhich complicated its theoretical analysis: first, the eigenvectors of the\nassociated graph Laplacian are used to embed the dataset, and second, k-means\nclustering algorithm is applied to the embedded dataset to get the labels. This\npaper is devoted to the theoretical foundations of spectral clustering and\ngraph cuts. We consider a convex relaxation of graph cuts, namely ratio cuts\nand normalized cuts, that makes the usual two-step approach of spectral\nclustering obsolete and at the same time gives rise to a rigorous theoretical\nanalysis of graph cuts and spectral clustering. We derive deterministic bounds\nfor successful spectral clustering via a spectral proximity condition that\nnaturally depends on the algebraic connectivity of each cluster and the\ninter-cluster connectivity. Moreover, we demonstrate by means of some popular\nexamples that our bounds can achieve near-optimality. Our findings are also\nfundamental for the theoretical understanding of kernel k-means. Numerical\nsimulations confirm and complement our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:16:19 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 02:20:51 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 19:57:56 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ling", "Shuyang", ""], ["Strohmer", "Thomas", ""]]}, {"id": "1806.11463", "submitter": "Alejandro Pozas-Kerstjens", "authors": "Zhikuan Zhao, Alejandro Pozas-Kerstjens, Patrick Rebentrost, Peter\n  Wittek", "title": "Bayesian Deep Learning on a Quantum Computer", "comments": "11 pages, 3 figures. RevTeX 4.1. Code is available at\n  https://gitlab.com/apozas/bayesian-dl-quantum/ V3: Updated to match published\n  version", "journal-ref": "Quantum Machine Intelligence 1, 4 (2019)", "doi": "10.1007/s42484-019-00004-7", "report-no": null, "categories": "quant-ph cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods in machine learning, such as Gaussian processes, have great\nadvantages com-pared to other techniques. In particular, they provide estimates\nof the uncertainty associated with a prediction. Extending the Bayesian\napproach to deep architectures has remained a major challenge. Recent results\nconnected deep feedforward neural networks with Gaussian processes, allowing\ntraining without backpropagation. This connection enables us to leverage a\nquantum algorithm designed for Gaussian processes and develop a new algorithm\nfor Bayesian deep learning on quantum computers. The properties of the kernel\nmatrix in the Gaussian process ensure the efficient execution of the core\ncomponent of the protocol, quantum matrix inversion, providing an at least\npolynomial speedup over classical algorithms. Furthermore, we demonstrate the\nexecution of the algorithm on contemporary quantum computers and analyze its\nrobustness with respect to realistic noise models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:08:45 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 12:13:47 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 07:51:29 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Zhao", "Zhikuan", ""], ["Pozas-Kerstjens", "Alejandro", ""], ["Rebentrost", "Patrick", ""], ["Wittek", "Peter", ""]]}, {"id": "1806.11494", "submitter": "Val\\'erie Poulin", "authors": "Val\\'erie Poulin and Fran\\c{c}ois Th\\'eberge", "title": "Comparing Graph Clusterings: Set partition measures vs. Graph-aware\n  measures", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3009862", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a family of graph partition similarity measures\nthat take the topology of the graph into account. These graph-aware measures\nare alternatives to using set partition similarity measures that are not\nspecifically designed for graph partitions. The two types of measures,\ngraph-aware and set partition measures, are shown to have opposite behaviors\nwith respect to resolution issues and provide complementary information\nnecessary to assess that two graph partitions are similar.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:53:22 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:36:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Poulin", "Val\u00e9rie", ""], ["Th\u00e9berge", "Fran\u00e7ois", ""]]}, {"id": "1806.11500", "submitter": "Ben London", "authors": "Ben London and Ted Sandler", "title": "Bayesian Counterfactual Risk Minimization", "comments": "Extended version of the paper published at the 2019 International\n  Conference on Machine Learning (ICML). Contains some additional citations;\n  fewer deferred proofs; and slightly more detailed analysis. Latest revision\n  fixes the order of authors in a reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian view of counterfactual risk minimization (CRM) for\noffline learning from logged bandit feedback. Using PAC-Bayesian analysis, we\nderive a new generalization bound for the truncated inverse propensity score\nestimator. We apply the bound to a class of Bayesian policies, which motivates\na novel, potentially data-dependent, regularization technique for CRM.\nExperimental results indicate that this technique outperforms standard $L_2$\nregularization, and that it is competitive with variance regularization while\nbeing both simpler to implement and more computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:01:34 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 21:47:31 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 23:29:11 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 18:42:25 GMT"}, {"version": "v5", "created": "Mon, 24 Feb 2020 23:32:23 GMT"}, {"version": "v6", "created": "Thu, 2 Apr 2020 17:52:27 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["London", "Ben", ""], ["Sandler", "Ted", ""]]}, {"id": "1806.11518", "submitter": "Viktor Stojkoski MSc", "authors": "Melanie F. Pradier, Viktor Stojkoski, Zoran Utkovski, Ljupco Kocarev\n  and Fernando Perez-Cruz", "title": "Sparse Three-parameter Restricted Indian Buffet Process for\n  Understanding International Trade", "comments": "To appear in the proceedings of ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian nonparametric latent feature model specially\nsuitable for exploratory analysis of high-dimensional count data. We perform a\nnon-negative doubly sparse matrix factorization that has two main advantages:\nnot only we are able to better approximate the row input distributions, but the\ninferred topics are also easier to interpret. By combining the three-parameter\nand restricted Indian buffet processes into a single prior, we increase the\nmodel flexibility, allowing for a full spectrum of sparse solutions in the\nlatent space. We demonstrate the usefulness of our approach in the analysis of\ncountries' economic structure. Compared to other approaches, empirical results\nshow our model's ability to give easy-to-interpret information and better\ncapture the underlying sparsity structure of data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:20:40 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Pradier", "Melanie F.", ""], ["Stojkoski", "Viktor", ""], ["Utkovski", "Zoran", ""], ["Kocarev", "Ljupco", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1806.11532", "submitter": "Marc-Alexandre C\\^ot\\'e", "authors": "Marc-Alexandre C\\^ot\\'e, \\'Akos K\\'ad\\'ar, Xingdi Yuan, Ben Kybartas,\n  Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla\n  El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler", "title": "TextWorld: A Learning Environment for Text-based Games", "comments": "Presented at the Computer Games Workshop at IJCAI 2018, Stockholm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TextWorld, a sandbox learning environment for the training and\nevaluation of RL agents on text-based games. TextWorld is a Python library that\nhandles interactive play-through of text games, as well as backend functions\nlike state tracking and reward assignment. It comes with a curated list of\ngames whose features and challenges we have analyzed. More significantly, it\nenables users to handcraft or automatically generate new games. Its generative\nmechanisms give precise control over the difficulty, scope, and language of\nconstructed games, and can be used to relax challenges inherent to commercial\ntext games like partial observability and sparse rewards. By generating sets of\nvaried but similar games, TextWorld can also be used to study generalization\nand transfer learning. We cast text-based games in the Reinforcement Learning\nformalism, use our framework to develop a set of benchmark games, and evaluate\nseveral baseline agents on this set and the curated list.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:56:07 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:57:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Yuan", "Xingdi", ""], ["Kybartas", "Ben", ""], ["Barnes", "Tavian", ""], ["Fine", "Emery", ""], ["Moore", "James", ""], ["Tao", "Ruo Yu", ""], ["Hausknecht", "Matthew", ""], ["Asri", "Layla El", ""], ["Adada", "Mahmoud", ""], ["Tay", "Wendy", ""], ["Trischler", "Adam", ""]]}, {"id": "1806.11536", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ramtin\n  Pedarsani", "title": "An Exact Quantized Decentralized Gradient Descent Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2932876", "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decentralized consensus optimization, where the\nsum of $n$ smooth and strongly convex functions are minimized over $n$\ndistributed agents that form a connected network. In particular, we consider\nthe case that the communicated local decision variables among nodes are\nquantized in order to alleviate the communication bottleneck in distributed\noptimization. We propose the Quantized Decentralized Gradient Descent (QDGD)\nalgorithm, in which nodes update their local decision variables by combining\nthe quantized information received from their neighbors with their local\ninformation. We prove that under standard strong convexity and smoothness\nassumptions for the objective function, QDGD achieves a vanishing mean solution\nerror under customary conditions for quantizers. To the best of our knowledge,\nthis is the first algorithm that achieves vanishing consensus error in the\npresence of quantization noise. Moreover, we provide simulation results that\nshow tight agreement between our derived theoretical convergence rate and the\nnumerical results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:02:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 23:27:11 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 01:10:39 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "1806.11544", "submitter": "Simon Lyddon", "authors": "S. P. Lyddon, S. G. Walker, C. C. Holmes", "title": "Nonparametric learning from Bayesian models with randomized objective\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is built on an assumption that the model space contains a\ntrue reflection of the data generating mechanism. This assumption is\nproblematic, particularly in complex data environments. Here we present a\nBayesian nonparametric approach to learning that makes use of statistical\nmodels, but does not assume that the model is true. Our approach has provably\nbetter properties than using a parametric model and admits a Monte Carlo\nsampling scheme that can afford massive scalability on modern computer\narchitectures. The model-based aspect of learning is particularly attractive\nfor regularizing nonparametric inference when the sample size is small, and\nalso for correcting approximate approaches such as variational Bayes (VB). We\ndemonstrate the approach on a number of examples including VB classifiers and\nBayesian random forests.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:18:28 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 22:03:43 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lyddon", "S. P.", ""], ["Walker", "S. G.", ""], ["Holmes", "C. C.", ""]]}]