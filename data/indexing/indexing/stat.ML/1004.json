[{"id": "1004.0085", "submitter": "Akisato Kimura", "authors": "Akisato kimura, Derek Pang, Tatsuto Takeuchi, Kouji Miyazato, Junji\n  Yamato and Kunio Kashino", "title": "A stochastic model of human visual attention with a dynamic Bayesian\n  network", "comments": "24 pages, single-column, 13 figures excluding portlaits, submitted to\n  IEEE Transactions on Pattern Analysis and Machine Intelligence.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent studies in the field of human vision science suggest that the human\nresponses to the stimuli on a visual display are non-deterministic. People may\nattend to different locations on the same visual input at the same time. Based\non this knowledge, we propose a new stochastic model of visual attention by\nintroducing a dynamic Bayesian network to predict the likelihood of where\nhumans typically focus on a video scene. The proposed model is composed of a\ndynamic Bayesian network with 4 layers. Our model provides a framework that\nsimulates and combines the visual saliency response and the cognitive state of\na person to estimate the most probable attended regions. Sample-based inference\nwith Markov chain Monte-Carlo based particle filter and stream processing with\nmulti-core processors enable us to estimate human visual attention in near real\ntime. Experimental results have demonstrated that our model performs\nsignificantly better in predicting human visual attention compared to the\nprevious deterministic models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 08:51:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["kimura", "Akisato", ""], ["Pang", "Derek", ""], ["Takeuchi", "Tatsuto", ""], ["Miyazato", "Kouji", ""], ["Yamato", "Junji", ""], ["Kashino", "Kunio", ""]]}, {"id": "1004.0089", "submitter": "Francois Bavaud", "authors": "Fran\\c{c}ois Bavaud", "title": "On the Schoenberg Transformations in Data Analysis: Theory and\n  Illustrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of Schoenberg transformations, embedding Euclidean distances into\nhigher dimensional Euclidean spaces, is presented, and derived from theorems on\npositive definite and conditionally negative definite matrices. Original\nresults on the arc lengths, angles and curvature of the transformations are\nproposed, and visualized on artificial data sets by classical multidimensional\nscaling. A simple distance-based discriminant algorithm illustrates the theory,\nintimately connected to the Gaussian kernels of Machine Learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 09:16:53 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2010 03:24:12 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bavaud", "Fran\u00e7ois", ""]]}, {"id": "1004.0209", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Robert Tibshirani", "title": "Inference with Transposable Data: Modeling the Effects of Row and Column\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of large-scale inference on the row or column\nvariables of data in the form of a matrix. Often this data is transposable,\nmeaning that both the row variables and column variables are of potential\ninterest. An example of this scenario is detecting significant genes in\nmicroarrays when the samples or arrays may be dependent due to underlying\nrelationships. We study the effect of both row and column correlations on\ncommonly used test-statistics, null distributions, and multiple testing\nprocedures, by explicitly modeling the covariances with the matrix-variate\nnormal distribution. Using this model, we give both theoretical and simulation\nresults revealing the problems associated with using standard statistical\nmethodology on transposable data. We solve these problems by estimating the row\nand column covariances simultaneously, with transposable regularized covariance\nmodels, and de-correlating or sphering the data as a pre-processing step. Under\nreasonable assumptions, our method gives test statistics that follow the scaled\ntheoretical null distribution and are approximately independent. Simulations\nbased on various models with structured and observed covariances from real\nmicroarray data reveal that our method offers substantial improvements in two\nareas: 1) increased statistical power and 2) correct estimation of false\ndiscovery rates.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 19:19:31 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1004.0314", "submitter": "Simone Fiori", "authors": "Simone Fiori", "title": "Visualization of Manifold-Valued Elements by Multidimensional Scaling", "comments": "The paper has been published in 2011, hence I think it is time to\n  withdraw its draft version from the arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present contribution suggests the use of a multidimensional scaling (MDS)\nalgorithm as a visualization tool for manifold-valued elements. A visualization\ntool of this kind is useful in signal processing and machine learning whenever\nlearning/adaptation algorithms insist on high-dimensional parameter manifolds.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2010 10:41:38 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 16:36:59 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Fiori", "Simone", ""]]}, {"id": "1004.0456", "submitter": "Fabrice Rossi", "authors": "Georges H\\'ebrail and Bernard Hugueney and Yves Lechevallier and\n  Fabrice Rossi", "title": "Exploratory Analysis of Functional Data via Clustering and Optimal\n  Segmentation", "comments": null, "journal-ref": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1125-1141", "doi": "10.1016/j.neucom.2009.11.022", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper an exploratory analysis algorithm for functional\ndata. The method partitions a set of functions into $K$ clusters and represents\neach cluster by a simple prototype (e.g., piecewise constant). The total number\nof segments in the prototypes, $P$, is chosen by the user and optimally\ndistributed among the clusters via two dynamic programming algorithms. The\npractical relevance of the method is shown on two real world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2010 16:28:47 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["H\u00e9brail", "Georges", ""], ["Hugueney", "Bernard", ""], ["Lechevallier", "Yves", ""], ["Rossi", "Fabrice", ""]]}, {"id": "1004.0524", "submitter": "Yunxiao He", "authors": "Yunxiao He and Chuanhai Liu", "title": "The Dynamic ECME Algorithm", "comments": "24 pages, 9 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ECME algorithm has proven to be an effective way of accelerating the EM\nalgorithm for many problems. Recognising the limitation of using prefixed\nacceleration subspace in ECME, we propose the new Dynamic ECME (DECME)\nalgorithm which allows the acceleration subspace to be chosen dynamically. Our\ninvestigation of an inefficient special case of DECME, the classical Successive\nOverrelaxation (SOR) method, leads to an efficient, simple, and widely\napplicable DECME implementation, called DECME_v1. The fast convergence of\nDECME_v1 is established by the theoretical result that, in a small\nneighbourhood of the maximum likelihood estimate (MLE), DECME_v1 is equivalent\nto a conjugate direction method. Numerical results show that DECME_v1 and its\ntwo variants are very stable and often converge faster than EM by a factor of\none hundred in terms of number of iterations and a factor of thirty in terms of\nCPU time when EM is very slow.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 18:21:57 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["He", "Yunxiao", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1004.1341", "submitter": "Giuseppe Jurman", "authors": "Giuseppe Jurman, Samantha Riccadonna, Roberto Visintainer, Cesare\n  Furlanello", "title": "Algebraic Comparison of Partial Lists in Bioinformatics", "comments": null, "journal-ref": "PLoS ONE 7(5): e36540 (2012)", "doi": "10.1371/journal.pone.0036540", "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outcome of a functional genomics pipeline is usually a partial list of\ngenomic features, ranked by their relevance in modelling biological phenotype\nin terms of a classification or regression model. Due to resampling protocols\nor just within a meta-analysis comparison, instead of one list it is often the\ncase that sets of alternative feature lists (possibly of different lengths) are\nobtained. Here we introduce a method, based on the algebraic theory of\nsymmetric groups, for studying the variability between lists (\"list stability\")\nin the case of lists of unequal length. We provide algorithms evaluating\nstability for lists embedded in the full feature set or just limited to the\nfeatures occurring in the partial lists. The method is demonstrated first on\nsynthetic data in a gene filtering task and then for finding gene profiles on a\nrecent prostate cancer dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 13:58:13 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Jurman", "Giuseppe", ""], ["Riccadonna", "Samantha", ""], ["Visintainer", "Roberto", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1004.2006", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "Machine learning approach to inverse problem and unfolding procedure", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A procedure for unfolding the true distribution from experimental data is\npresented. Machine learning methods are applied for simultaneous identification\nof an apparatus function and solving of an inverse problem. A priori\ninformation about the true distribution from theory or previous experiments is\nused for Monte-Carlo simulation of the training sample. The training sample can\nbe used to calculate a transformation from the true distribution to the\nmeasured one. This transformation provides a robust solution for an unfolding\nproblem with minimal biases and statistical errors for the set of distributions\nused to create the training sample. The dimensionality of the solved problem\ncan be arbitrary. A numerical example is presented to illustrate and validate\nthe procedure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 16:34:37 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 18:01:58 GMT"}, {"version": "v3", "created": "Wed, 25 May 2011 13:32:49 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1004.2027", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Vicenc Gomez and Hilbert J. Kappen", "title": "Dynamic Policy Programming", "comments": "Submitted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel policy iteration method, called dynamic\npolicy programming (DPP), to estimate the optimal policy in the\ninfinite-horizon Markov decision processes. We prove the finite-iteration and\nasymptotic l\\infty-norm performance-loss bounds for DPP in the presence of\napproximation/estimation error. The bounds are expressed in terms of the\nl\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of\nthe error in the case of the standard approximate value iteration (AVI) and the\napproximate policy iteration (API). This suggests that DPP can achieve a better\nperformance than AVI and API since it averages out the simulation noise caused\nby Monte-Carlo sampling throughout the learning process. We examine this\ntheoretical results numerically by com- paring the performance of the\napproximate variants of DPP with existing reinforcement learning (RL) methods\non different problem domains. Our results show that, in all cases, DPP-based\nalgorithms outperform other RL methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 19:09:43 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2011 20:23:59 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Gomez", "Vicenc", ""], ["Kappen", "Hilbert J.", ""]]}, {"id": "1004.2287", "submitter": "Vivian Viallon", "authors": "Vivian Viallon, Onureena Banerjee, Gregoire Rey, Eric Jougla, Joel\n  Coste", "title": "An empirical comparative study of approximate methods for binary\n  graphical models; application to the search of associations among causes of\n  death in French death certificates", "comments": "29 pages, 4 figures.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking for associations among multiple variables is a topical issue in\nstatistics due to the increasing amount of data encountered in biology,\nmedicine and many other domains involving statistical applications. Graphical\nmodels have recently gained popularity for this purpose in the statistical\nliterature. Following the ideas of the LASSO procedure designed for the linear\nregression framework, recent developments dealing with graphical model\nselection have been based on $\\ell_1$-penalization. In the binary case,\nhowever, exact inference is generally very slow or even intractable because of\nthe form of the so-called log-partition function. Various approximate methods\nhave recently been proposed in the literature and the main objective of this\npaper is to compare them. Through an extensive simulation study, we show that a\nsimple modification of a method relying on a Gaussian approximation achieves\ngood performance and is very fast. We present a real application in which we\nsearch for associations among causes of death recorded on French death\ncertificates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 23:21:54 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Viallon", "Vivian", ""], ["Banerjee", "Onureena", ""], ["Rey", "Gregoire", ""], ["Jougla", "Eric", ""], ["Coste", "Joel", ""]]}, {"id": "1004.2304", "submitter": "Patrick Harrington Jr.", "authors": "Patrick L. Harrington Jr., Alfred O. Hero III", "title": "Spatio-Temporal Graphical Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the topology of spatial interactions in\na discrete state, discrete time spatio-temporal graphical model where the\ninteractions affect the temporal evolution of each agent in a network. Among\nother models, the susceptible, infected, recovered ($SIR$) model for\ninteraction events fall into this framework. We pose the problem as a structure\nlearning problem and solve it using an $\\ell_1$-penalized likelihood convex\nprogram. We evaluate the solution on a simulated spread of infectious over a\ncomplex network. Our topology estimates outperform those of a standard spatial\nMarkov random field graphical model selection using $\\ell_1$-regularized\nlogistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 01:50:42 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Harrington", "Patrick L.", "Jr."], ["Hero", "Alfred O.", "III"]]}, {"id": "1004.2468", "submitter": "Madalin Guta", "authors": "Madalin Guta and Wojciech Kotlowski", "title": "Quantum learning: optimal classification of qubit states", "comments": "24 pages, 4 figures", "journal-ref": "New J. Phys. 12 123032 (2010)", "doi": "10.1088/1367-2630/12/12/123032", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition is a central topic in Learning Theory with numerous\napplications such as voice and text recognition, image analysis, computer\ndiagnosis. The statistical set-up in classification is the following: we are\ngiven an i.i.d. training set $(X_{1},Y_{1}),... (X_{n},Y_{n})$ where $X_{i}$\nrepresents a feature and $Y_{i}\\in \\{0,1\\}$ is a label attached to that\nfeature. The underlying joint distribution of $(X,Y)$ is unknown, but we can\nlearn about it from the training set and we aim at devising low error\nclassifiers $f:X\\to Y$ used to predict the label of new incoming features.\n  Here we solve a quantum analogue of this problem, namely the classification\nof two arbitrary unknown qubit states. Given a number of `training' copies from\neach of the states, we would like to `learn' about them by performing a\nmeasurement on the training set. The outcome is then used to design mesurements\nfor the classification of future systems with unknown labels. We find the\nasymptotically optimal classification strategy and show that typically, it\nperforms strictly better than a plug-in strategy based on state estimation.\n  The figure of merit is the excess risk which is the difference between the\nprobability of error and the probability of error of the optimal measurement\nwhen the states are known, that is the Helstrom measurement. We show that the\nexcess risk has rate $n^{-1}$ and compute the exact constant of the rate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 18:30:19 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Guta", "Madalin", ""], ["Kotlowski", "Wojciech", ""]]}, {"id": "1004.3101", "submitter": "Vladimir Nikulin", "authors": "Vladimir Nikulin and Geoffrey J. McLachlan", "title": "Strong Consistency of Prototype Based Clustering in Probabilistic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate in general terms an approach to prove strong\nconsistency of the Empirical Risk Minimisation inductive principle applied to\nthe prototype or distance based clustering. This approach was motivated by the\nDivisive Information-Theoretic Feature Clustering model in probabilistic space\nwith Kullback-Leibler divergence which may be regarded as a special case within\nthe Clustering Minimisation framework. Also, we propose clustering\nregularization restricting creation of additional clusters which are not\nsignificant or are not essentially different comparing with existing clusters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 06:17:19 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Nikulin", "Vladimir", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1004.4668", "submitter": "John Moriarty", "authors": "Nick S. Jones and John Moriarty", "title": "Evolutionary Inference for Function-valued Traits: Gaussian Process\n  Regression on Phylogenies", "comments": "7 pages, 1 figure", "journal-ref": "Journal of the Royal Society Interface vol. 10 no. 78 20120616\n  (2013)", "doi": "10.1098/rsif.2012.0616", "report-no": null, "categories": "q-bio.QM cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological data objects often have both of the following features: (i) they\nare functions rather than single numbers or vectors, and (ii) they are\ncorrelated due to phylogenetic relationships. In this paper we give a flexible\nstatistical model for such data, by combining assumptions from phylogenetics\nwith Gaussian processes. We describe its use as a nonparametric Bayesian prior\ndistribution, both for prediction (placing posterior distributions on ancestral\nfunctions) and model selection (comparing rates of evolution across a\nphylogeny, or identifying the most likely phylogenies consistent with the\nobserved data). Our work is integrative, extending the popular phylogenetic\nBrownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian\ninference, and extending Gaussian Process regression to phylogenies. We provide\na brief illustration of the application of our method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 22:22:18 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2011 00:04:22 GMT"}, {"version": "v3", "created": "Fri, 3 Aug 2012 07:42:47 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Jones", "Nick S.", ""], ["Moriarty", "John", ""]]}, {"id": "1004.4965", "submitter": "Mikhail Zaslavskiy", "authors": "Mikhail Zaslavskiy (CBIO), Francis Bach (INRIA Rocquencourt, LIENS),\n  Jean-Philippe Vert (CBIO)", "title": "Many-to-Many Graph Matching: a Continuous Relaxation Approach", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs provide an efficient tool for object representation in various\ncomputer vision applications. Once graph-based representations are constructed,\nan important question is how to compare graphs. This problem is often\nformulated as a graph matching problem where one seeks a mapping between\nvertices of two graphs which optimally aligns their structure. In the classical\nformulation of graph matching, only one-to-one correspondences between vertices\nare considered. However, in many applications, graphs cannot be matched\nperfectly and it is more interesting to consider many-to-many correspondences\nwhere clusters of vertices in one graph are matched to clusters of vertices in\nthe other graph. In this paper, we formulate the many-to-many graph matching\nproblem as a discrete optimization problem and propose an approximate algorithm\nbased on a continuous relaxation of the combinatorial problem. We compare our\nmethod with other existing methods on several benchmark computer vision\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 07:46:55 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Zaslavskiy", "Mikhail", "", "CBIO"], ["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1004.5031", "submitter": "Amparo Baillo", "authors": "Amparo Ba\\'illo, Juan Antonio Cuesta-Albertos and Antonio Cuevas", "title": "Supervised classification for a family of Gaussian functional models", "comments": "30 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of supervised classification (discrimination) for functional\ndata, it is shown that the optimal classification rule can be explicitly\nobtained for a class of Gaussian processes with \"triangular\" covariance\nfunctions. This explicit knowledge has two practical consequences. First, the\nconsistency of the well-known nearest neighbors classifier (which is not\nguaranteed in the problems with functional data) is established for the\nindicated class of processes. Second, and more important, parametric and\nnonparametric plug-in classifiers can be obtained by estimating the unknown\nelements in the optimal rule. The performance of these new plug-in classifiers\nis checked, with positive results, through a simulation study and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 13:59:29 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Ba\u00edllo", "Amparo", ""], ["Cuesta-Albertos", "Juan Antonio", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1004.5194", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Lille - Nord Europe)", "title": "Clustering processes", "comments": "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this\n  article please see: arXiv:1005.0826v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 06:38:47 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Lille - Nord Europe"]]}, {"id": "1004.5229", "submitter": "Sarah Filippi", "authors": "Sarah Filippi (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (LTCI)", "title": "Optimism in Reinforcement Learning and Kullback-Leibler Divergence", "comments": "This work has been accepted and presented at ALLERTON 2010;\n  Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton\n  Conference on, Monticello (Illinois) : \\'Etats-Unis (2010)", "journal-ref": null, "doi": "10.1109/ALLERTON.2010.5706896", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model-based reinforcement learning in finite Markov De- cision\nProcesses (MDPs), focussing on so-called optimistic strategies. In MDPs,\noptimism can be implemented by carrying out extended value it- erations under a\nconstraint of consistency with the estimated model tran- sition probabilities.\nThe UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this\nstrategy, has recently been shown to guarantee near-optimal regret bounds. In\nthis paper, we strongly argue in favor of using the Kullback-Leibler (KL)\ndivergence for this purpose. By studying the linear maximization problem under\nKL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for\nsolving KL-optimistic extended value iteration. Using recent deviation bounds\non the KL divergence, we prove that KL-UCRL provides the same guarantees as\nUCRL2 in terms of regret. However, numerical experiments on classical\nbenchmarks show a significantly improved behavior, particularly when the MDP\nhas reduced connectivity. To support this observation, we provide elements of\ncom- parison between the two algorithms based on geometric considerations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 09:31:55 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2010 09:56:58 GMT"}, {"version": "v3", "created": "Wed, 13 Oct 2010 10:11:39 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Filippi", "Sarah", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "LTCI"]]}, {"id": "1004.5265", "submitter": "Ricardo Henao", "authors": "Ricardo Henao and Ole Winther", "title": "Sparse Linear Identifiable Multivariate Modeling", "comments": "45 pages, 17 figures", "journal-ref": "JMLR, 12(Mar):863-905, 2011", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider sparse and identifiable linear latent variable\n(factor) and linear Bayesian network models for parsimonious analysis of\nmultivariate data. We propose a computationally efficient method for joint\nparameter and model inference, and model comparison. It consists of a fully\nBayesian hierarchy for sparse models using slab and spike priors (two-component\ndelta-function and continuous mixtures), non-Gaussian latent factors and a\nstochastic search over the ordering of the variables. The framework, which we\ncall SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and\nbench-marked on artificial and real biological data sets. SLIM is closest in\nspirit to LiNGAM (Shimizu et al., 2006), but differs substantially in\ninference, Bayesian network structure learning and model comparison.\nExperimentally, SLIM performs equally well or better than LiNGAM with\ncomparable computational complexity. We attribute this mainly to the stochastic\nsearch strategy used, and to parsimony (sparsity and identifiability), which is\nan explicit part of the model. We propose two extensions to the basic i.i.d.\nlinear framework: non-linear dependence on observed variables, called SNIM\n(Sparse Non-linear Identifiable Multivariate modeling) and allowing for\ncorrelations between latent variables, called CSLIM (Correlated SLIM), for the\ntemporal and/or spatial data. The source code and scripts are available from\nhttp://cogsys.imm.dtu.dk/slim/.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 12:37:20 GMT"}, {"version": "v2", "created": "Wed, 20 Oct 2010 14:02:15 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2011 09:14:36 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Henao", "Ricardo", ""], ["Winther", "Ole", ""]]}]