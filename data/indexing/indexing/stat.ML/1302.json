[{"id": "1302.0082", "submitter": "Barnabas Poczos", "authors": "Barnabas Poczos, Alessandro Rinaldo, Aarti Singh, Larry Wasserman", "title": "Distribution-Free Distribution Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  `Distribution regression' refers to the situation where a response Y depends\non a covariate P where P is a probability distribution. The model is Y=f(P) +\nmu where f is an unknown regression function and mu is a random error.\nTypically, we do not observe P directly, but rather, we observe a sample from\nP. In this paper we develop theory and methods for distribution-free versions\nof distribution regression. This means that we do not make distributional\nassumptions about the error term mu and covariate P. We prove that when the\neffective dimension is small enough (as measured by the doubling dimension),\nthen the excess prediction risk converges to zero with a polynomial rate.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 05:35:48 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Poczos", "Barnabas", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1302.0256", "submitter": "Woncheol Jang", "authors": "Woncheol Jang, Johan Lim, Nicole A. Lazar, Ji Meng Loh, Donghyeon Yu", "title": "Regression shrinkage and grouping of highly correlated predictors with\n  HORSES", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying homogeneous subgroups of variables can be challenging in high\ndimensional data analysis with highly correlated predictors. We propose a new\nmethod called Hexagonal Operator for Regression with Shrinkage and Equality\nSelection, HORSES for short, that simultaneously selects positively correlated\nvariables and identifies them as predictive clusters. This is achieved via a\nconstrained least-squares problem with regularization that consists of a linear\ncombination of an L_1 penalty for the coefficients and another L_1 penalty for\npairwise differences of the coefficients. This specification of the penalty\nfunction encourages grouping of positively correlated predictors combined with\na sparsity solution. We construct an efficient algorithm to implement the\nHORSES procedure. We show via simulation that the proposed method outperforms\nother variable selection methods in terms of prediction error and parsimony.\nThe technique is demonstrated on two data sets, a small data set from analysis\nof soil in Appalachia, and a high dimensional data set from a near infrared\n(NIR) spectroscopy study, showing the flexibility of the methodology.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 19:18:11 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Jang", "Woncheol", ""], ["Lim", "Johan", ""], ["Lazar", "Nicole A.", ""], ["Loh", "Ji Meng", ""], ["Yu", "Donghyeon", ""]]}, {"id": "1302.0315", "submitter": "Mehrdad Mahdavi", "authors": "Rong Jin, Tianbao Yang, Mehrdad Mahdavi", "title": "Sparse Multiple Kernel Learning with Geometric Convergence Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of sparse multiple kernel learning (MKL),\nwhere the goal is to efficiently learn a combination of a fixed small number of\nkernels from a large pool that could lead to a kernel classifier with a small\nprediction error. We develop an efficient algorithm based on the greedy\ncoordinate descent algorithm, that is able to achieve a geometric convergence\nrate under appropriate conditions. The convergence rate is achieved by\nmeasuring the size of functional gradients by an empirical $\\ell_2$ norm that\ndepends on the empirical data distribution. This is in contrast to previous\nalgorithms that use a functional norm to measure the size of gradients, which\nis independent from the data samples. We also establish a generalization error\nbound of the learned sparse kernel classifier using the technique of local\nRademacher complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 23:28:43 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Jin", "Rong", ""], ["Yang", "Tianbao", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "1302.0336", "submitter": "Geoffrey Schiebinger", "authors": "Adityanand Guntuboyina, Sujayam Saha and Geoffrey Schiebinger", "title": "Sharp Inequalities for $f$-divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $f$-divergences are a general class of divergences between probability\nmeasures which include as special cases many commonly used divergences in\nprobability, mathematical statistics and information theory such as\nKullback-Leibler divergence, chi-squared divergence, squared Hellinger\ndistance, total variation distance etc. In this paper, we study the problem of\nmaximizing or minimizing an $f$-divergence between two probability measures\nsubject to a finite number of constraints on other $f$-divergences. We show\nthat these infinite-dimensional optimization problems can all be reduced to\noptimization problems over small finite dimensional spaces which are tractable.\nOur results lead to a comprehensive and unified treatment of the problem of\nobtaining sharp inequalities between $f$-divergences. We demonstrate that many\nof the existing results on inequalities between $f$-divergences can be obtained\nas special cases of our results and we also improve on some existing non-sharp\ninequalities.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 03:15:48 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 19:02:51 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Saha", "Sujayam", ""], ["Schiebinger", "Geoffrey", ""]]}, {"id": "1302.0406", "submitter": "Purushottam Kar", "authors": "Purushottam Kar", "title": "Generalization Guarantees for a Binary Classification Framework for\n  Two-Stage Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present generalization bounds for the TS-MKL framework for two stage\nmultiple kernel learning. We also present bounds for sparse kernel learning\nformulations within the TS-MKL framework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 17:20:47 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Kar", "Purushottam", ""]]}, {"id": "1302.0581", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "SMML estimators for exponential families with continuous sufficient\n  statistics", "comments": "Revised to include new insights and results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum message length principle is an information theoretic criterion\nthat links data compression with statistical inference. This paper studies the\nstrict minimum message length (SMML) estimator for $d$-dimensional exponential\nfamilies with continuous sufficient statistics, for all $d \\ge 1$. The\npartition of an SMML estimator is shown to consist of convex polytopes (i.e.\nconvex polygons when $d=2$) which can be described explicitly in terms of the\nassertions and coding probabilities. While this result is known, we give a new\nproof based on the calculus of variations, and this approach gives some\ninteresting new inequalities for SMML estimators. We also use this result to\nconstruct an SMML estimator for a $2$-dimensional normal random variable with\nknown variance and a normal prior on its mean.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 04:56:21 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 02:50:18 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1302.0870", "submitter": "Brian Baingana Mr", "authors": "Brian Baingana, Georgios B. Giannakis", "title": "Centrality-constrained graph embedding", "comments": "Submitted to ICASSP May, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates a\ngraph embedding approach with centrality considerations to comply with node\nhierarchy. The problem is formulated as one of constrained multi-dimensional\nscaling (MDS), and it is solved via block coordinate descent iterations with\nsuccessive approximations and guaranteed convergence to a KKT point. In\naddition, a regularization term enforcing graph smoothness is incorporated with\nthe goal of reducing edge crossings. Experimental results demonstrate that the\nalgorithm converges, and can be used to efficiently embed large graphs on the\norder of thousands of nodes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 21:26:47 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1302.0895", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Exact Sparse Recovery with L0 Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications concern sparse signals, for example, detecting anomalies\nfrom the differences between consecutive images taken by surveillance cameras.\nThis paper focuses on the problem of recovering a K-sparse signal x in N\ndimensions. In the mainstream framework of compressed sensing (CS), the vector\nx is recovered from M non-adaptive linear measurements y = xS, where S (of size\nN x M) is typically a Gaussian (or Gaussian-like) design matrix, through some\noptimization procedure such as linear programming (LP).\n  In our proposed method, the design matrix S is generated from an\n$\\alpha$-stable distribution with $\\alpha\\approx 0$. Our decoding algorithm\nmainly requires one linear scan of the coordinates, followed by a few\niterations on a small number of coordinates which are \"undetermined\" in the\nprevious iteration. Comparisons with two strong baselines, linear programming\n(LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm can\nbe significantly faster in decoding speed and more accurate in recovery\nquality, for the task of exact spare recovery. Our procedure is robust against\nmeasurement noise. Even when there are no sufficient measurements, our\nalgorithm can still reliably recover a significant portion of the nonzero\ncoordinates.\n  To provide the intuition for understanding our method, we also analyze the\nprocedure by assuming an idealistic setting. Interestingly, when K=2, the\n\"idealized\" algorithm achieves exact recovery with merely 3 measurements,\nregardless of N. For general K, the required sample size of the \"idealized\"\nalgorithm is about 5K.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 22:51:56 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1302.1519", "submitter": "Eric Bauer", "authors": "Eric Bauer, Daphne Koller, Yoram Singer", "title": "Update Rules for Parameter Estimation in Bayesian Networks", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-3-13", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-examines the problem of parameter estimation in Bayesian\nnetworks with missing values and hidden variables from the perspective of\nrecent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unified\nframework for parameter estimation that encompasses both on-line learning,\nwhere the model is continuously adapted to new data cases as they arrive, and\nthe more traditional batch learning, where a pre-accumulated set of samples is\nused in a one-time model selection process. In the batch case, our framework\nencompasses both the gradient projection algorithm and the EM algorithm for\nBayesian networks. The framework also leads to new on-line and batch parameter\nupdate schemes, including a parameterized version of EM. We provide both\nempirical and theoretical results indicating that parameterized EM allows\nfaster convergence to the maximum likelihood parameters than does standard EM.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:53:33 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Bauer", "Eric", ""], ["Koller", "Daphne", ""], ["Singer", "Yoram", ""]]}, {"id": "1302.1528", "submitter": "Max Chickering", "authors": "David Maxwell Chickering, David Heckerman, Christopher Meek", "title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-80-89", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently several researchers have investigated techniques for using data to\nlearn Bayesian networks containing compact representations for the conditional\nprobability distributions (CPDs) stored at each node. The majority of this work\nhas concentrated on using decision-tree representations for the CPDs. In\naddition, researchers typically apply non-Bayesian (or asymptotically Bayesian)\nscoring functions such as MDL to evaluate the goodness-of-fit of networks to\nthe data. In this paper we investigate a Bayesian approach to learning Bayesian\nnetworks that contain the more general decision-graph representations of the\nCPDs. First, we describe how to evaluate the posterior probability that is, the\nBayesian score of such a network, given a database of observed cases. Second,\nwe describe various search spaces that can be used, in conjunction with a\nscoring function and a search procedure, to identify one or more high-scoring\nnetworks. Finally, we present an experimental evaluation of the search spaces,\nusing a greedy algorithm and a Bayesian scoring function.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:54:25 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:29:15 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.1545", "submitter": "David Heckerman", "authors": "David Heckerman, Christopher Meek", "title": "Models and Selection Criteria for Regression and Classification", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-223-228", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing regression or classification, we are interested in the\nconditional probability distribution for an outcome or class variable Y given a\nset of explanatoryor input variables X. We consider Bayesian models for this\ntask. In particular, we examine a special class of models, which we call\nBayesian regression/classification (BRC) models, that can be factored into\nindependent conditional (y|x) and input (x) models. These models are\nconvenient, because the conditional model (the portion of the full model that\nwe care about) can be analyzed by itself. We examine the practice of\ntransforming arbitrary Bayesian models to BRC models, and argue that this\npractice is often inappropriate because it ignores prior knowledge that may be\nimportant for learning. In addition, we examine Bayesian methods for learning\nmodels from data. We discuss two criteria for Bayesian model selection that are\nappropriate for repression/classification: one described by Spiegelhalter et\nal. (1993), and another by Buntine (1993). We contrast these two criteria using\nthe prequential framework of Dawid (1984), and give sufficient conditions under\nwhich the criteria agree.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:56:07 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.1552", "submitter": "Michael Kearns", "authors": "Michael Kearns, Yishay Mansour, Andrew Y. Ng", "title": "An Information-Theoretic Analysis of Hard and Soft Assignment Methods\n  for Clustering", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-282-293", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assignment methods are at the heart of many algorithms for unsupervised\nlearning and clustering - in particular, the well-known K-means and\nExpectation-Maximization (EM) algorithms. In this work, we study several\ndifferent methods of assignment, including the \"hard\" assignments used by\nK-means and the ?soft' assignments used by EM. While it is known that K-means\nminimizes the distortion on the data and EM maximizes the likelihood, little is\nknown about the systematic differences of behavior between the two algorithms.\nHere we shed light on these differences via an information-theoretic analysis.\nThe cornerstone of our results is a simple decomposition of the expected\ndistortion, showing that K-means (and its extension for inferring general\nparametric densities from unlabeled sample data) must implicitly manage a\ntrade-off between how similar the data assigned to each cluster are, and how\nthe data are balanced among the clusters. How well the data are balanced is\nmeasured by the entropy of the partition defined by the hard assignments. In\naddition to letting us predict and verify systematic differences between\nK-means and EM on specific examples, the decomposition allows us to give a\nrather general argument showing that K ?means will consistently find densities\nwith less \"overlap\" than EM. We also study a third natural assignment method\nthat we call posterior assignment, that is close in spirit to the soft\nassignments of EM, but leads to a surprisingly different algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:57:20 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Kearns", "Michael", ""], ["Mansour", "Yishay", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1302.1611", "submitter": "Philippe Rigollet", "authors": "S\\'ebastien Bubeck, Vianney Perchet and Philippe Rigollet", "title": "Bounded regret in stochastic multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 23:20:20 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 15:48:55 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1302.1733", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "Fernando Gonz\\'alez, Llu\\'is A. Belanche", "title": "Feature Selection for Microarray Gene Expression Data using Simulated\n  Annealing guided by the Multivariate Joint Entropy", "comments": "12 pages, 6 Tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a new way to calculate the multivariate joint entropy is\npresented. This measure is the basis for a fast information-theoretic based\nevaluation of gene relevance in a Microarray Gene Expression data context. Its\nlow complexity is based on the reuse of previous computations to calculate\ncurrent feature relevance. The mu-TAFS algorithm --named as such to\ndifferentiate it from previous TAFS algorithms-- implements a simulated\nannealing technique specially designed for feature subset selection. The\nalgorithm is applied to the maximization of gene subset relevance in several\npublic-domain microarray data sets. The experimental results show a notoriously\nhigh classification performance and low size subsets formed by biologically\nmeaningful genes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 12:49:57 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Gonz\u00e1lez", "Fernando", ""], ["Belanche", "Llu\u00eds A.", ""]]}, {"id": "1302.2068", "submitter": "Cheryl Flynn", "authors": "Cheryl J. Flynn, Clifford M. Hurvich, and Jeffrey S. Simonoff", "title": "Efficiency for Regularization Parameter Selection in Penalized\n  Likelihood Estimation of Misspecified Models", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2013.801775", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that AIC-type criteria are asymptotically efficient\nselectors of the tuning parameter in non-concave penalized regression methods\nunder the assumption that the population variance is known or that a consistent\nestimator is available. We relax this assumption to prove that AIC itself is\nasymptotically efficient and we study its performance in finite samples. In\nclassical regression, it is known that AIC tends to select overly complex\nmodels when the dimension of the maximum candidate model is large relative to\nthe sample size. Simulation studies suggest that AIC suffers from the same\nshortcomings when used in penalized regression. We therefore propose the use of\nthe classical corrected AIC (AICc) as an alternative and prove that it\nmaintains the desired asymptotic properties. To broaden our results, we further\nprove the efficiency of AIC for penalized likelihood methods in the context of\ngeneralized linear models with no dispersion parameter. Similar results exist\nin the literature but only for a restricted set of candidate models. By\nemploying results from the classical literature on maximum-likelihood\nestimation in misspecified models, we are able to establish this result for a\ngeneral set of candidate models. We use simulations to assess the performance\nof AIC and AICc, as well as that of other selectors, in finite samples for both\nSCAD-penalized and Lasso regressions and a real data example is considered.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 16:02:53 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Flynn", "Cheryl J.", ""], ["Hurvich", "Clifford M.", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1302.2325", "submitter": "Anatoli Juditsky B.", "authors": "Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski", "title": "Conditional Gradient Algorithms for Norm-Regularized Smooth Convex\n  Optimization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by some applications in signal processing and machine learning, we\nconsider two convex optimization problems where, given a cone $K$, a norm\n$\\|\\cdot\\|$ and a smooth convex function $f$, we want either 1) to minimize the\nnorm over the intersection of the cone and a level set of $f$, or 2) to\nminimize over the cone the sum of $f$ and a multiple of the norm. We focus on\nthe case where (a) the dimension of the problem is too large to allow for\ninterior point algorithms, (b) $\\|\\cdot\\|$ is \"too complicated\" to allow for\ncomputationally cheap Bregman projections required in the first-order proximal\ngradient algorithms. On the other hand, we assume that {it is relatively easy\nto minimize linear forms over the intersection of $K$ and the unit\n$\\|\\cdot\\|$-ball}. Motivating examples are given by the nuclear norm with $K$\nbeing the entire space of matrices, or the positive semidefinite cone in the\nspace of symmetric matrices, and the Total Variation norm on the space of 2D\nimages. We discuss versions of the Conditional Gradient algorithm capable to\nhandle our problems of interest, provide the related theoretical efficiency\nestimates and outline some applications.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2013 12:24:44 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 15:30:42 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 12:57:54 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2013 16:54:05 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Harchaoui", "Zaid", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1302.2489", "submitter": "Adam D. Bull", "authors": "Adam D. Bull", "title": "Adaptive-treed bandits", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ644 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 4, 2289-2307", "doi": "10.3150/14-BEJ644", "report-no": "IMS-BEJ-BEJ644", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel algorithm for noisy global optimisation and\ncontinuum-armed bandits, with good convergence properties over any continuous\nreward function having finitely many polynomial maxima. Over such functions,\nour algorithm achieves square-root regret in bandits, and inverse-square-root\nerror in optimisation, without prior information. Our algorithm works by\nreducing these problems to tree-armed bandits, and we also provide new results\nin this setting. We show it is possible to adaptively combine multiple trees so\nas to minimise the regret, and also give near-matching lower bounds on the\nregret in terms of the zooming dimension.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 14:50:21 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 15:08:16 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2014 16:48:52 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2015 10:04:17 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Bull", "Adam D.", ""]]}, {"id": "1302.2569", "submitter": "Olivier Catoni", "authors": "Olivier Catoni and Thomas Mainguy", "title": "Toric grammars: a new statistical approach to natural language modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical model for computational linguistics. Rather than\ntrying to estimate directly the probability distribution of a random sentence\nof the language, we define a Markov chain on finite sets of sentences with many\nfinite recurrent communicating classes and define our language model as the\ninvariant probability measures of the chain on each recurrent communicating\nclass. This Markov chain, that we call a communication model, recombines at\neach step randomly the set of sentences forming its current state, using some\ngrammar rules. When the grammar rules are fixed and known in advance instead of\nbeing estimated on the fly, we can prove supplementary mathematical properties.\nIn particular, we can prove in this case that all states are recurrent states,\nso that the chain defines a partition of its state space into finite recurrent\ncommunicating classes. We show that our approach is a decisive departure from\nMarkov models at the sentence level and discuss its relationships with Context\nFree Grammars. Although the toric grammars we use are closely related to\nContext Free Grammars, the way we generate the language from the grammar is\nqualitatively different. Our communication model has two purposes. On the one\nhand, it is used to define indirectly the probability distribution of a random\nsentence of the language. On the other hand it can serve as a (crude) model of\nlanguage transmission from one speaker to another speaker through the\ncommunication of a (large) set of sentences.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 18:51:03 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Catoni", "Olivier", ""], ["Mainguy", "Thomas", ""]]}, {"id": "1302.2576", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo and Cheng Lee and Joydeep Ghosh", "title": "The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking", "comments": "14 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 19:16:25 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Lee", "Cheng", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1302.2645", "submitter": "Alexander Gorban", "authors": "E. M. Mirkes, A. Zinovyev, A. N. Gorban", "title": "Geometrical complexity of data approximators", "comments": "10 pages, 3 figures, minor correction and extension", "journal-ref": "IWANN 2013, Advances in Computation Intelligence, Springer LNCS\n  7902, pp. 500-509, 2013", "doi": "10.1007/978-3-642-38679-4_50", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There are many methods developed to approximate a cloud of vectors embedded\nin high-dimensional space by simpler objects: starting from principal points\nand linear manifolds to self-organizing maps, neural gas, elastic maps, various\ntypes of principal curves and principal trees, and so on. For each type of\napproximators the measure of the approximator complexity was developed too.\nThese measures are necessary to find the balance between accuracy and\ncomplexity and to define the optimal approximations of a given type. We propose\na measure of complexity (geometrical complexity) which is applicable to\napproximators of several types and which allows comparing data approximations\nof different types.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 21:14:43 GMT"}, {"version": "v2", "created": "Sat, 4 May 2013 01:22:48 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Mirkes", "E. M.", ""], ["Zinovyev", "A.", ""], ["Gorban", "A. N.", ""]]}, {"id": "1302.2671", "submitter": "Yoon-Sik Cho", "authors": "Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita", "title": "Latent Self-Exciting Point Process Model for Spatial-Temporal Networks", "comments": "20 pages, 6 figures (v3); 11 pages, 6 figures (v2); previous version\n  appeared in the 9th Bayesian Modeling Applications Workshop, UAI'12", "journal-ref": "DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS SERIES B, Vol. 19, pp.\n  1335-1354, 2014", "doi": "10.3934/dcdsb.2014.19.1335", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a latent self-exciting point process model that describes\ngeographically distributed interactions between pairs of entities. In contrast\nto most existing approaches that assume fully observable interactions, here we\nconsider a scenario where certain interaction events lack information about\nparticipants. Instead, this information needs to be inferred from the available\nobservations. We develop an efficient approximate algorithm based on\nvariational expectation-maximization to infer unknown participants in an event\ngiven the location and the time of the event. We validate the model on\nsynthetic as well as real-world data, and obtain very promising results on the\nidentity-inference task. We also use our model to predict the timing and\nparticipants of future events, and demonstrate that it compares favorably with\nbaseline approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 00:01:02 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 18:02:36 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 23:42:52 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Cho", "Yoon-Sik", ""], ["Galstyan", "Aram", ""], ["Brantingham", "P. Jeffrey", ""], ["Tita", "George", ""]]}, {"id": "1302.2672", "submitter": "Alexander Rakhlin", "authors": "Wei Han, Alexander Rakhlin, Karthik Sridharan", "title": "Competing With Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online learning with a notion of regret defined with\nrespect to a set of strategies. We develop tools for analyzing the minimax\nrates and for deriving regret-minimization algorithms in this scenario. While\nthe standard methods for minimizing the usual notion of regret fail, through\nour analysis we demonstrate existence of regret-minimization methods that\ncompete with such sets of strategies as: autoregressive algorithms, strategies\nbased on statistical models, regularized least squares, and follow the\nregularized leader strategies. In several cases we also derive efficient\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 00:14:44 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Han", "Wei", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1302.2684", "submitter": "Rong Ge", "authors": "Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade", "title": "A Tensor Approach to Learning Mixed Membership Community Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is the task of detecting hidden communities from observed\ninteractions. Guaranteed community detection has so far been mostly limited to\nmodels with non-overlapping communities such as the stochastic block model. In\nthis paper, we remove this restriction, and provide guaranteed community\ndetection for a family of probabilistic network models with overlapping\ncommunities, termed as the mixed membership Dirichlet model, first introduced\nby Airoldi et al. This model allows for nodes to have fractional memberships in\nmultiple communities and assumes that the community memberships are drawn from\na Dirichlet distribution. Moreover, it contains the stochastic block model as a\nspecial case. We propose a unified approach to learning these models via a\ntensor spectral decomposition method. Our estimator is based on low-order\nmoment tensor of the observed network, consisting of 3-star counts. Our\nlearning method is fast and is based on simple linear algebraic operations,\ne.g. singular value decomposition and tensor power iterations. We provide\nguaranteed recovery of community memberships and model parameters and present a\ncareful finite sample analysis of our learning method. As an important special\ncase, our results match the best known scaling requirements for the\n(homogeneous) stochastic block model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 01:48:14 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2013 10:10:41 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2013 03:55:04 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2013 21:30:08 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1302.2686", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis and Alfred O. Hero III", "title": "Covariance Estimation in High Dimensions via Kronecker Product\n  Expansions", "comments": "47 pages, accepted to IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 61, No. 21, pp. 5347\n  - 5360, November 2013", "doi": "10.1109/TSP.2013.2279355", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for estimating high dimensional covariance\nmatrices. The method, permuted rank-penalized least-squares (PRLS), is based on\na Kronecker product series expansion of the true covariance matrix. Assuming an\ni.i.d. Gaussian random sample, we establish high dimensional rates of\nconvergence to the true covariance as both the number of samples and the number\nof variables go to infinity. For covariance matrices of low separation rank,\nour results establish that PRLS has significantly faster convergence than the\nstandard sample covariance matrix (SCM) estimator. The convergence rate\ncaptures a fundamental tradeoff between estimation error and approximation\nerror, thus providing a scalable covariance estimation framework in terms of\nseparation rank, similar to low rank approximation of covariance matrices. The\nMSE convergence rates generalize the high dimensional rates recently obtained\nfor the ML Flip-flop algorithm for Kronecker product covariance estimation. We\nshow that a class of block Toeplitz covariance matrices is approximatable by\nlow separation rank and give bounds on the minimal separation rank $r$ that\nensures a given level of bias. Simulations are presented to validate the\ntheoretical bounds. As a real world application, we illustrate the utility of\nthe proposed Kronecker covariance estimator for spatio-temporal linear least\nsquares prediction of multivariate wind speed measurements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 02:40:43 GMT"}, {"version": "v10", "created": "Mon, 23 Dec 2013 22:16:13 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 19:40:19 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2013 01:42:11 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 21:59:13 GMT"}, {"version": "v5", "created": "Tue, 9 Jul 2013 17:32:16 GMT"}, {"version": "v6", "created": "Fri, 1 Nov 2013 18:47:11 GMT"}, {"version": "v7", "created": "Mon, 4 Nov 2013 15:06:28 GMT"}, {"version": "v8", "created": "Tue, 19 Nov 2013 01:47:27 GMT"}, {"version": "v9", "created": "Thu, 21 Nov 2013 21:35:59 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1302.2752", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer", "title": "Adaptive Metric Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adaptive data-dependent dimensionality reduction in the context of\nsupervised learning in general metric spaces. Our main statistical contribution\nis a generalization bound for Lipschitz functions in metric spaces that are\ndoubling, or nearly doubling. On the algorithmic front, we describe an analogue\nof PCA for metric spaces: namely an efficient procedure that approximates the\ndata's intrinsic dimension, which is often much lower than the ambient\ndimension. Our approach thus leverages the dual benefits of low dimensionality:\n(1) more efficient algorithms, e.g., for proximity search, and (2) more\noptimistic generalization bounds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 10:20:21 GMT"}, {"version": "v2", "created": "Sun, 12 May 2013 14:58:17 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 12:18:55 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1302.2767", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly and Louis Theran", "title": "Coherence and sufficient sampling densities for reconstruction in\n  compressed sensing", "comments": "16 pages, 1 figure. v2 streamlines the exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.AG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new, very general, formulation of the compressed sensing problem in\nterms of coordinate projections of an analytic variety, and derive sufficient\nsampling rates for signal reconstruction. Our bounds are linear in the\ncoherence of the signal space, a geometric parameter independent of the\nspecific signal and measurement, and logarithmic in the ambient dimension where\nthe signal is presented. We exemplify our approach by deriving sufficient\nsampling densities for low-rank matrix completion and distance matrix\ncompletion which are independent of the true matrix.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 12:15:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2013 14:02:46 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""]]}, {"id": "1302.2969", "submitter": "Nabin Malakar", "authors": "N. K. Malakar, D. J. Lary, D. Gencaga, A. Albayrak, J. Wei", "title": "Towards Identification of Relevant Variables in the observed Aerosol\n  Optical Depth Bias between MODIS and AERONET observations", "comments": null, "journal-ref": null, "doi": "10.1063/1.4819985", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements made by satellite remote sensing, Moderate Resolution Imaging\nSpectroradiometer (MODIS), and globally distributed Aerosol Robotic Network\n(AERONET) are compared. Comparison of the two datasets measurements for aerosol\noptical depth values show that there are biases between the two data products.\nIn this paper, we present a general framework towards identifying relevant set\nof variables responsible for the observed bias. We present a general framework\nto identify the possible factors influencing the bias, which might be\nassociated with the measurement conditions such as the solar and sensor zenith\nangles, the solar and sensor azimuth, scattering angles, and surface\nreflectivity at the various measured wavelengths, etc. Specifically, we\nperformed analysis for remote sensing Aqua-Land data set, and used machine\nlearning technique, neural network in this case, to perform multivariate\nregression between the ground-truth and the training data sets. Finally, we\nused mutual information between the observed and the predicted values as the\nmeasure of similarity to identify the most relevant set of variables. The\nsearch is brute force method as we have to consider all possible combinations.\nThe computations involves a huge number crunching exercise, and we implemented\nit by writing a job-parallel program.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 02:11:37 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Malakar", "N. K.", ""], ["Lary", "D. J.", ""], ["Gencaga", "D.", ""], ["Albayrak", "A.", ""], ["Wei", "J.", ""]]}, {"id": "1302.3407", "submitter": "Azadeh Khaleghi", "authors": "Azaden Khaleghi and Daniil Ryabko", "title": "A consistent clustering-based approach to estimating the number of\n  change-points in highly dependent time-series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of change-point estimation is considered under a general\nframework where the data are generated by unknown stationary ergodic process\ndistributions. In this context, the consistent estimation of the number of\nchange-points is provably impossible. However, it is shown that a consistent\nclustering method may be used to estimate the number of change points, under\nthe additional constraint that the correct number of process distributions that\ngenerate the data is provided. This additional parameter has a natural\ninterpretation in many real-world applications. An algorithm is proposed that\nestimates the number of change-points and locates the changes. The proposed\nalgorithm is shown to be asymptotically consistent; its empirical evaluations\nare provided.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 14:15:14 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Khaleghi", "Azaden", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1302.3463", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Locally epistatic genomic relationship matrices for genomic association,\n  prediction and selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount and complexity of genetic information increases it is necessary\nthat we explore some efficient ways of handling these data. This study takes\nthe \"divide and conquer\" approach for analyzing high dimensional genomic data.\nOur aims include reducing the dimensionality of the problem that has to be\ndealt one at a time, improving the performance and interpretability of the\nmodels. We propose using the inherent structures in the genome; to divide the\nbigger problem into manageable parts. In plant and animal breeding studies a\ndistinction is made between the commercial value (additive + epistatic genetic\neffects) and the breeding value (additive genetic effects) of an individual\nsince it is expected that some of the epistatic genetic effects will be lost\ndue to recombination. In this paper, we argue that the breeder can take\nadvantage of some of the epistatic marker effects in regions of low\nrecombination. The models introduced here aim to estimate local epistatic line\nheritability by using the genetic map information and combine the local\nadditive and epistatic effects. To this end, we have used semi-parametric mixed\nmodels with multiple local genomic relationship matrices with hierarchical\ntesting designs and lasso post-processing for sparsity in the final model and\nspeed. Our models produce good predictive performance along with genetic\nassociation information.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 16:46:59 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 15:27:42 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2013 16:00:35 GMT"}, {"version": "v4", "created": "Thu, 18 Apr 2013 12:43:25 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2013 15:25:07 GMT"}, {"version": "v6", "created": "Wed, 14 Aug 2013 15:40:18 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1302.3566", "submitter": "David Maxwell Chickering", "authors": "David Maxwell Chickering", "title": "Learning Equivalence Classes of Bayesian Networks Structures", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-150-157", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to learning Bayesian networks from data typically combine a\nscoring function with a heuristic search procedure. Given a Bayesian network\nstructure, many of the scoring functions derived in the literature return a\nscore for the entire equivalence class to which the structure belongs. When\nusing such a scoring function, it is appropriate for the heuristic search\nalgorithm to search over equivalence classes of Bayesian networks as opposed to\nindividual structures. We present the general formulation of a search space for\nwhich the states of the search correspond to equivalence classes of structures.\nUsing this space, any one of a number of heuristic search algorithms can easily\nbe applied. We compare greedy search performance in the proposed search space\nto greedy search performance in a search space for which the states correspond\nto individual Bayesian network structures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:12:58 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Chickering", "David Maxwell", ""]]}, {"id": "1302.3567", "submitter": "Max Chickering", "authors": "David Maxwell Chickering, David Heckerman", "title": "Efficient Approximations for the Marginal Likelihood of Incomplete Data\n  Given a Bayesian Network", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-158-168", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian methods for learning Bayesian networks when data sets are\nincomplete. In particular, we examine asymptotic approximations for the\nmarginal likelihood of incomplete data given a Bayesian network. We consider\nthe Laplace approximation and the less accurate but more efficient BIC/MDL\napproximation. We also consider approximations proposed by Draper (1993) and\nCheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,\nbut their accuracy has not been studied in any depth. We compare the accuracy\nof these approximations under the assumption that the Laplace approximation is\nthe most accurate. In experiments using synthetic data generated from discrete\nnaive-Bayes models having a hidden root node, we find that the CS measure is\nthe most accurate.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:13:03 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 00:07:34 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Heckerman", "David", ""]]}, {"id": "1302.3577", "submitter": "Nir Friedman", "authors": "Nir Friedman, Moises Goldszmidt", "title": "Learning Bayesian Networks with Local Structure", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-252-262", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a novel addition to the known methods for learning\nBayesian networks from data that improves the quality of the learned networks.\nOur approach explicitly represents and learns the local structure in the\nconditional probability tables (CPTs), that quantify these networks. This\nincreases the space of possible models, enabling the representation of CPTs\nwith a variable number of parameters that depends on the learned local\nstructures. The resulting learning procedure is capable of inducing models that\nbetter emulate the real complexity of the interactions present in the data. We\ndescribe the theoretical foundations and practical aspects of learning local\nstructures, as well as an empirical evaluation of the proposed method. This\nevaluation indicates that learning curves characterizing the procedure that\nexploits the local structure converge faster than these of the standard\nprocedure. Our results also show that networks learned with local structure\ntend to be more complex (in terms of arcs), yet require less parameters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:02 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Friedman", "Nir", ""], ["Goldszmidt", "Moises", ""]]}, {"id": "1302.3579", "submitter": "Nir Friedman", "authors": "Nir Friedman, Zohar Yakhini", "title": "On the Sample Complexity of Learning Bayesian Networks", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-274-282", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increasing interest in learning Bayesian\nnetworks from data. One of the most effective methods for learning such\nnetworks is based on the minimum description length (MDL) principle. Previous\nwork has shown that this learning procedure is asymptotically successful: with\nprobability one, it will converge to the target distribution, given a\nsufficient number of samples. However, the rate of this convergence has been\nhitherto unknown. In this work we examine the sample complexity of MDL based\nlearning procedures for Bayesian networks. We show that the number of samples\nneeded to learn an epsilon-close approximation (in terms of entropy distance)\nwith confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog\n(1/delta)). This means that the sample complexity is a low-order polynomial in\nthe error threshold and sub-linear in the confidence bound. We also discuss how\nthe constants in this term depend on the complexity of the target distribution.\nFinally, we address questions of asymptotic minimality and propose a method for\nusing the sample complexity results to speed up the learning process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:13 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Friedman", "Nir", ""], ["Yakhini", "Zohar", ""]]}, {"id": "1302.3580", "submitter": "Dan Geiger", "authors": "Dan Geiger, David Heckerman, Christopher Meek", "title": "Asymptotic Model Selection for Directed Networks with Hidden Variables", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-283-290", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Bayesian Information Criterion (BIC), an asymptotic\napproximation for the marginal likelihood, to Bayesian networks with hidden\nvariables. This approximation can be used to select models given large samples\nof data. The standard BIC as well as our extension punishes the complexity of a\nmodel according to the dimension of its parameters. We argue that the dimension\nof a Bayesian network with hidden variables is the rank of the Jacobian matrix\nof the transformation between the parameters of the network and the parameters\nof the observable variables. We compute the dimensions of several networks\nincluding the naive Bayes model with a hidden root node.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:19 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:35:58 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.3590", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey, Laura Martignon", "title": "Bayesian Learning of Loglinear Models for Neural Connectivity", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-373-380", "categories": "cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian approach to learning the connectivity\nstructure of a group of neurons from data on configuration frequencies. A major\nobjective of the research is to provide statistical tools for detecting changes\nin firing patterns with changing stimuli. Our framework is not restricted to\nthe well-understood case of pair interactions, but generalizes the Boltzmann\nmachine model to allow for higher order interactions. The paper applies a\nMarkov Chain Monte Carlo Model Composition (MC3) algorithm to search over\nconnectivity structures and uses Laplace's method to approximate posterior\nprobabilities of structures. Performance of the methods was tested on synthetic\ndata. The models were also applied to data obtained by Vaadia on multi-unit\nrecordings of several neurons in the visual cortex of a rhesus monkey in two\ndifferent attentional states. Results confirmed the experimenters' conjecture\nthat different attentional states were associated with different interaction\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:15:20 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""], ["Martignon", "Laura", ""]]}, {"id": "1302.3639", "submitter": "George Chen", "authors": "George H. Chen, Stanislav Nikolov, Devavrat Shah", "title": "A Latent Source Model for Nonparametric Time Series Classification", "comments": "Advances in Neural Information Processing Systems (NIPS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classifying time series, a nearest-neighbor approach is widely used in\npractice with performance often competitive with or better than more elaborate\nmethods such as neural networks, decision trees, and support vector machines.\nWe develop theoretical justification for the effectiveness of\nnearest-neighbor-like classification of time series. Our guiding hypothesis is\nthat in many applications, such as forecasting which topics will become trends\non Twitter, there aren't actually that many prototypical time series to begin\nwith, relative to the number of time series we have access to, e.g., topics\nbecome trends on Twitter only in a few distinct manners whereas we can collect\nmassive amounts of Twitter data. To operationalize this hypothesis, we propose\na latent source model for time series, which naturally leads to a \"weighted\nmajority voting\" classification rule that can be approximated by a\nnearest-neighbor classifier. We establish nonasymptotic performance guarantees\nof both weighted majority voting and nearest-neighbor classification under our\nmodel accounting for how much of the time series we observe and the model\ncomplexity. Experimental results on synthetic data show weighted majority\nvoting achieving the same misclassification rate as nearest-neighbor\nclassification while observing less of the time series. We then use weighted\nmajority to forecast which news topics on Twitter become trends, where we are\nable to detect such \"trending topics\" in advance of Twitter 79% of the time,\nwith a mean early advantage of 1 hour and 26 minutes, a true positive rate of\n95%, and a false positive rate of 4%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 22:12:40 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 01:37:27 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 15:28:08 GMT"}, {"version": "v4", "created": "Sat, 9 Nov 2013 00:21:07 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2013 04:20:34 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Chen", "George H.", ""], ["Nikolov", "Stanislav", ""], ["Shah", "Devavrat", ""]]}, {"id": "1302.3668", "submitter": "Ajit Narayanan", "authors": "Ajit Narayanan and Yi Chen", "title": "Bio-inspired data mining: Treating malware signatures as biosequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of machine learning to bioinformatics problems is well\nestablished. Less well understood is the application of bioinformatics\ntechniques to machine learning and, in particular, the representation of\nnon-biological data as biosequences. The aim of this paper is to explore the\neffects of giving amino acid representation to problematic machine learning\ndata and to evaluate the benefits of supplementing traditional machine learning\nwith bioinformatics tools and techniques. The signatures of 60 computer viruses\nand 60 computer worms were converted into amino acid representations and first\nmultiply aligned separately to identify conserved regions across different\nfamilies within each class (virus and worm). This was followed by a second\nalignment of all 120 aligned signatures together so that non-conserved regions\nwere identified prior to input to a number of machine learning techniques.\nDifferences in length between virus and worm signatures after the first\nalignment were resolved by the second alignment. Our first set of experiments\nindicates that representing computer malware signatures as amino acid sequences\nfollowed by alignment leads to greater classification and prediction accuracy.\nOur second set of experiments indicates that checking the results of data\nmining from artificial virus and worm data against known proteins can lead to\ngeneralizations being made from the domain of naturally occurring proteins to\nmalware signatures. However, further work is needed to determine the advantages\nand disadvantages of different representations and sequence alignment methods\nfor handling problematic machine learning data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 03:54:53 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Narayanan", "Ajit", ""], ["Chen", "Yi", ""]]}, {"id": "1302.3700", "submitter": "John Quinn", "authors": "John A. Quinn, Masashi Sugiyama", "title": "Density Ratio Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models and their variants are the predominant sequential\nclassification method in such domains as speech recognition, bioinformatics and\nnatural language processing. Being generative rather than discriminative\nmodels, however, their classification performance is a drawback. In this paper\nwe apply ideas from the field of density ratio estimation to bypass the\ndifficult step of learning likelihood functions in HMMs. By reformulating\ninference and model fitting in terms of density ratios and applying a fast\nkernel-based estimation method, we show that it is possible to obtain a\nstriking increase in discriminative performance while retaining the\nprobabilistic qualities of the HMM. We demonstrate experimentally that this\nformulation makes more efficient use of training data than alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 08:16:14 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Quinn", "John A.", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1302.3913", "submitter": "Ekaterina Merkurjev", "authors": "Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi,\n  Arjuna Flenner, Allon Percus", "title": "Multiclass Data Segmentation using Diffuse Interface Methods on Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two graph-based algorithms for multiclass segmentation of\nhigh-dimensional data. The algorithms use a diffuse interface model based on\nthe Ginzburg-Landau functional, related to total variation compressed sensing\nand image processing. A multiclass extension is introduced using the Gibbs\nsimplex, with the functional's double-well potential modified to handle the\nmulticlass case. The first algorithm minimizes the functional using a convex\nsplitting numerical scheme. The second algorithm is a uses a graph adaptation\nof the classical numerical Merriman-Bence-Osher (MBO) scheme, which alternates\nbetween diffusion and thresholding. We demonstrate the performance of both\nalgorithms experimentally on synthetic data, grayscale and color images, and\nseveral benchmark data sets such as MNIST, COIL and WebKB. We also make use of\nfast numerical solvers for finding the eigenvectors and eigenvalues of the\ngraph Laplacian, and take advantage of the sparsity of the matrix. Experiments\nindicate that the results are competitive with or better than the current\nstate-of-the-art multiclass segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 23:49:21 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2014 05:06:04 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Garcia-Cardona", "Cristina", ""], ["Merkurjev", "Ekaterina", ""], ["Bertozzi", "Andrea L.", ""], ["Flenner", "Arjuna", ""], ["Percus", "Allon", ""]]}, {"id": "1302.3931", "submitter": "Xiaozhao Zhao", "authors": "Xiaozhao Zhao and Yuexian Hou and Qian Yu and Dawei Song and Wenjie Li", "title": "Understanding Boltzmann Machine and Deep Learning via A Confident\n  Information First Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical dimensionality reduction methods focus on directly reducing the\nnumber of random variables while retaining maximal variations in the data. In\nthis paper, we consider the dimensionality reduction in parameter spaces of\nbinary multivariate distributions. We propose a general\nConfident-Information-First (CIF) principle to maximally preserve parameters\nwith confident estimates and rule out unreliable or noisy parameters. Formally,\nthe confidence of a parameter can be assessed by its Fisher information, which\nestablishes a connection with the inverse variance of any unbiased estimate for\nthe parameter via the Cram\\'{e}r-Rao bound. We then revisit Boltzmann machines\n(BM) and theoretically show that both single-layer BM without hidden units\n(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.\nThis can not only help us uncover and formalize the essential parts of the\ntarget density that SBM and RBM capture, but also suggest that the deep neural\nnetwork consisting of several layers of RBM can be seen as the layer-wise\napplication of CIF. Guided by the theoretical analysis, we develop a\nsample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and\na CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are\nstudied in a series of density estimation experiments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 05:49:15 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 06:53:55 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2013 06:20:51 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2013 13:28:59 GMT"}, {"version": "v5", "created": "Wed, 17 Apr 2013 11:05:06 GMT"}, {"version": "v6", "created": "Mon, 3 Jun 2013 11:18:46 GMT"}, {"version": "v7", "created": "Wed, 9 Oct 2013 16:55:26 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhao", "Xiaozhao", ""], ["Hou", "Yuexian", ""], ["Yu", "Qian", ""], ["Song", "Dawei", ""], ["Li", "Wenjie", ""]]}, {"id": "1302.3956", "submitter": "Farzad Didehvar", "authors": "Raheleh Namayandeh, Farzad Didehvar, Zahra Shojaei", "title": "Clustering validity based on the most similarity", "comments": "4 pages,2 figueres, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One basic requirement of many studies is the necessity of classifying data.\nClustering is a proposed method for summarizing networks. Clustering methods\ncan be divided into two categories named model-based approaches and algorithmic\napproaches. Since the most of clustering methods depend on their input\nparameters, it is important to evaluate the result of a clustering algorithm\nwith its different input parameters, to choose the most appropriate one. There\nare several clustering validity techniques based on inner density and outer\ndensity of clusters that represent different metrics to choose the most\nappropriate clustering independent of the input parameters. According to\ndependency of previous methods on the input parameters, one challenge in facing\nwith large systems, is to complete data incrementally that effects on the final\nchoice of the most appropriate clustering. Those methods define the existence\nof high intensity in a cluster, and low intensity among different clusters as\nthe measure of choosing the optimal clustering. This measure has a tremendous\nproblem, not availing all data at the first stage. In this paper, we introduce\nan efficient measure in which maximum number of repetitions for various initial\nvalues occurs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 11:10:17 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Namayandeh", "Raheleh", ""], ["Didehvar", "Farzad", ""], ["Shojaei", "Zahra", ""]]}, {"id": "1302.3979", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Jos\\'e Miguel Hern\\'andez-Lobato, Zoubin Ghahramani", "title": "Gaussian Process Vine Copulas for Multivariate Dependence", "comments": "Accepted to International Conference in Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas allow to learn marginal distributions separately from the\nmultivariate dependence structure (copula) that links them together into a\ndensity function. Vine factorizations ease the learning of high-dimensional\ncopulas by constructing a hierarchy of conditional bivariate copulas. However,\nto simplify inference, it is common to assume that each of these conditional\nbivariate copulas is independent from its conditioning variables. In this\npaper, we relax this assumption by discovering the latent functions that\nspecify the shape of a conditional copula given its conditioning variables We\nlearn these functions by following a Bayesian approach based on sparse Gaussian\nprocesses with expectation propagation for scalable, approximate inference.\nExperiments on real-world datasets show that, when modeling all conditional\ndependencies, we obtain better estimates of the underlying copula of the data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 17:29:33 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Lopez-Paz", "David", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1302.4141", "submitter": "David  Gao", "authors": "Vittorio Latorre and David Yang Gao", "title": "Canonical dual solutions to nonconvex radial basis neural network\n  optimization problem", "comments": "10 pages, 7 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in\nregression problems. One of their principal drawbacks is that the formulation\ncorresponding to the training with the supervision of both the centers and the\nweights is a highly non-convex optimization problem, which leads to some\nfundamentally difficulties for traditional optimization theory and methods.\n  This paper presents a generalized canonical duality theory for solving this\nchallenging problem. We demonstrate that by sequential canonical dual\ntransformations, the nonconvex optimization problem of the RBFNN can be\nreformulated as a canonical dual problem (without duality gap). Both global\noptimal solution and local extrema can be classified. Several applications to\none of the most used Radial Basis Functions, the Gaussian function, are\nillustrated. Our results show that even for one-dimensional case, the global\nminimizer of the nonconvex problem may not be the best solution to the RBFNNs,\nand the canonical dual theory is a promising tool for solving general neural\nnetworks training problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 00:28:31 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Latorre", "Vittorio", ""], ["Gao", "David Yang", ""]]}, {"id": "1302.4242", "submitter": "Sylvain Chevallier", "authors": "Sylvain Chevallier and Quentin Barth\\'elemy and Jamal Atif", "title": "Metrics for Multivariate Dictionaries", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854993", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Overcomplete representations and dictionary learning algorithms kept\nattracting a growing interest in the machine learning community. This paper\naddresses the emerging problem of comparing multivariate overcomplete\nrepresentations. Despite a recurrent need to rely on a distance for learning or\nassessing multivariate overcomplete representations, no metrics in their\nunderlying spaces have yet been proposed. Henceforth we propose to study\novercomplete representations from the perspective of frame theory and matrix\nmanifolds. We consider distances between multivariate dictionaries as distances\nbetween their spans which reveal to be elements of a Grassmannian manifold. We\nintroduce Wasserstein-like set-metrics defined on Grassmannian spaces and study\ntheir properties both theoretically and numerically. Indeed a deep experimental\nstudy based on tailored synthetic datasetsand real EEG signals for\nBrain-Computer Interfaces (BCI) have been conducted. In particular, the\nintroduced metrics have been embedded in clustering algorithm and applied to\nBCI Competition IV-2a for dataset quality assessment. Besides, a principled\nconnection is made between three close but still disjoint research fields,\nnamely, Grassmannian packing, dictionary learning and compressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 12:25:07 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 09:19:18 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chevallier", "Sylvain", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Atif", "Jamal", ""]]}, {"id": "1302.4245", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson and Ryan Prescott Adams", "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation", "comments": "10 pages, 5 figures, 1 table. Minor edits and titled changed from\n  \"Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation\"\n  to \"Gaussian Process Kernels for Pattern Discovery and Extrapolation\".\n  Appears at the International Conference on Machine Learning (ICML), JMLR W&CP\n  28(3):1067-1075, 2013", "journal-ref": "International Conference on Machine Learning (ICML), JMLR W&CP\n  28(3):1067-1075, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are rich distributions over functions, which provide a\nBayesian nonparametric approach to smoothing and interpolation. We introduce\nsimple closed form kernels that can be used with Gaussian processes to discover\npatterns and enable extrapolation. These kernels are derived by modelling a\nspectral density -- the Fourier transform of a kernel -- with a Gaussian\nmixture. The proposed kernels support a broad class of stationary covariances,\nbut Gaussian process inference remains simple and analytic. We demonstrate the\nproposed kernels by discovering patterns and performing long range\nextrapolation on synthetic examples, as well as atmospheric CO2 trends and\nairline passenger data. We also show that we can reconstruct standard\ncovariances within our framework.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 12:41:50 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 12:52:04 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 16:41:30 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1302.4297", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Adam Kalai", "title": "Feature Multi-Selection among Subjective Features", "comments": null, "journal-ref": "S. Sabato and A. Kalai, \"Feature Multi-Selection among Subjective\n  Features\", Proceedings of the 30th International Conference on Machine\n  Learning (ICML), 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with subjective, noisy, or otherwise nebulous features, the\n\"wisdom of crowds\" suggests that one may benefit from multiple judgments of the\nsame feature on the same object. We give theoretically-motivated `feature\nmulti-selection' algorithms that choose, among a large set of candidate\nfeatures, not only which features to judge but how many times to judge each\none. We demonstrate the effectiveness of this approach for linear regression on\na crowdsourced learning task of predicting people's height and weight from\nphotos, using features such as 'gender' and 'estimated weight' as well as\nculturally fraught ones such as 'attractive'.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 15:00:47 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2013 17:03:56 GMT"}, {"version": "v3", "created": "Tue, 14 May 2013 21:35:25 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Sabato", "Sivan", ""], ["Kalai", "Adam", ""]]}, {"id": "1302.4343", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harish Karnick", "title": "On Translation Invariant Kernels and Screw Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the connection between Hilbertian metrics and positive definite\nkernels on the real line. In particular, we look at a well-known\ncharacterization of translation invariant Hilbertian metrics on the real line\nby von Neumann and Schoenberg (1941). Using this result we are able to give an\nalternate proof of Bochner's theorem for translation invariant positive\ndefinite kernels on the real line (Rudin, 1962).\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 16:42:27 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Kar", "Purushottam", ""], ["Karnick", "Harish", ""]]}, {"id": "1302.4385", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Robert Luce", "title": "Robust Near-Separable Nonnegative Matrix Factorization Using Linear\n  Optimization", "comments": "27 page; 4 figures. New Example, new experiment on the Swimmer data\n  set", "journal-ref": "Journal of Machine Learning Research 15 (Apr), pp. 1249-1280, 2014", "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has been shown recently to be\ntractable under the separability assumption, under which all the columns of the\ninput data matrix belong to the convex cone generated by only a few of these\ncolumns. Bittorf, Recht, R\\'e and Tropp (`Factoring nonnegative matrices with\nlinear programs', NIPS 2012) proposed a linear programming (LP) model, referred\nto as Hottopixx, which is robust under any small perturbation of the input\nmatrix. However, Hottopixx has two important drawbacks: (i) the input matrix\nhas to be normalized, and (ii) the factorization rank has to be known in\nadvance. In this paper, we generalize Hottopixx in order to resolve these two\ndrawbacks, that is, we propose a new LP model which does not require\nnormalization and detects the factorization rank automatically. Moreover, the\nnew LP model is more flexible, significantly more tolerant to noise, and can\neasily be adapted to handle outliers and other noise models. Finally, we show\non several synthetic datasets that it outperforms Hottopixx while competing\nfavorably with two state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:45:41 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 16:58:40 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Gillis", "Nicolas", ""], ["Luce", "Robert", ""]]}, {"id": "1302.4387", "submitter": "Ohad Shamir", "authors": "Nicolo Cesa-Bianchi, Ofer Dekel and Ohad Shamir", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of different types of adaptive (nonoblivious) adversaries\nin the setting of prediction with expert advice, under both full-information\nand bandit feedback. We measure the player's performance using a new notion of\nregret, also known as policy regret, which better captures the adversary's\nadaptiveness to the player's behavior. In a setting where losses are allowed to\ndrift, we characterize ---in a nearly complete manner--- the power of adaptive\nadversaries with bounded memories and switching costs. In particular, we show\nthat with switching costs, the attainable rate with bandit feedback is\n$\\widetilde{\\Theta}(T^{2/3})$. Interestingly, this rate is significantly worse\nthan the $\\Theta(\\sqrt{T})$ rate attainable with switching costs in the\nfull-information case. Via a novel reduction from experts to bandits, we also\nshow that a bounded memory adversary can force $\\widetilde{\\Theta}(T^{2/3})$\nregret even in the full information case, proving that switching costs are\neasier to control than bounded memory adversaries. Our lower bounds rely on a\nnew stochastic adversary strategy that generates loss processes with strong\ndependencies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:46:37 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 09:35:15 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Cesa-Bianchi", "Nicolo", ""], ["Dekel", "Ofer", ""], ["Shamir", "Ohad", ""]]}, {"id": "1302.4389", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron\n  Courville and Yoshua Bengio", "title": "Maxout Networks", "comments": "This is the version of the paper that appears in ICML 2013", "journal-ref": "JMLR WCP 28 (3): 1319-1327, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing models to leverage a recently introduced\napproximate model averaging technique called dropout. We define a simple new\nmodel called maxout (so named because its output is the max of a set of inputs,\nand because it is a natural companion to dropout) designed to both facilitate\noptimization by dropout and improve the accuracy of dropout's fast approximate\nmodel averaging technique. We empirically verify that the model successfully\naccomplishes both of these tasks. We use maxout and dropout to demonstrate\nstate of the art classification performance on four benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100, and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:59:07 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 04:39:48 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2013 22:33:13 GMT"}, {"version": "v4", "created": "Fri, 20 Sep 2013 08:54:35 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Warde-Farley", "David", ""], ["Mirza", "Mehdi", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1302.4549", "submitter": "Nir Ailon", "authors": "Nir Ailon and Yudong Chen and Xu Huan", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates graph clustering in the planted cluster model in the\npresence of {\\em small clusters}. Traditional results dictate that for an\nalgorithm to provably correctly recover the clusters, {\\em all} clusters must\nbe sufficiently large (in particular, $\\tilde{\\Omega}(\\sqrt{n})$ where $n$ is\nthe number of nodes of the graph). We show that this is not really a\nrestriction: by a more refined analysis of the trace-norm based recovery\napproach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that\nsmall clusters, under certain mild assumptions, do not hinder recovery of large\nones.\n  Based on this result, we further devise an iterative algorithm to recover\n{\\em almost all clusters} via a \"peeling strategy\", i.e., recover large\nclusters first, leading to a reduced problem, and repeat this procedure. These\nresults are extended to the {\\em partial observation} setting, in which only a\n(chosen) part of the graph is observed.The peeling strategy gives rise to an\nactive learning algorithm, in which edges adjacent to smaller clusters are\nqueried more often as large clusters are learned (and removed).\n  From a high level, this paper sheds novel insights on high-dimensional\nstatistics and learning structured data, by presenting a structured matrix\nlearning problem for which a one shot convex relaxation approach necessarily\nfails, but a carefully constructed sequence of convex relaxationsdoes the job.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 09:21:09 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 08:35:39 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Ailon", "Nir", ""], ["Chen", "Yudong", ""], ["Huan", "Xu", ""]]}, {"id": "1302.4773", "submitter": "Paulo Urriza", "authors": "Paulo Urriza, Eric Rebeiz, Danijela Cabric", "title": "Optimal Discriminant Functions Based On Sampled Distribution Distance\n  for Modulation Classification", "comments": "4 pages, 3 figures, submitted to IEEE Communications Letters", "journal-ref": null, "doi": "10.1109/LCOMM.2013.082113.131131", "report-no": null, "categories": "stat.ML cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we derive the optimal discriminant functions for modulation\nclassification based on the sampled distribution distance. The proposed method\nclassifies various candidate constellations using a low complexity approach\nbased on the distribution distance at specific testpoints along the cumulative\ndistribution function. This method, based on the Bayesian decision criteria,\nasymptotically provides the minimum classification error possible given a set\nof testpoints. Testpoint locations are also optimized to improve classification\nperformance. The method provides significant gains over existing approaches\nthat also use the distribution of the signal features.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 22:59:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Urriza", "Paulo", ""], ["Rebeiz", "Eric", ""], ["Cabric", "Danijela", ""]]}, {"id": "1302.4853", "submitter": "Misha Denil", "authors": "Misha Denil and David Matheson and Nando de Freitas", "title": "Consistency of Online Random Forests", "comments": "To appear in Proceedings of the 30th International Conference on\n  Machine Learning, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a testament to their success, the theory of random forests has long been\noutpaced by their application in practice. In this paper, we take a step\ntowards narrowing this gap by providing a consistency result for online random\nforests.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 09:48:49 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 18:59:28 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Denil", "Misha", ""], ["Matheson", "David", ""], ["de Freitas", "Nando", ""]]}, {"id": "1302.4886", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht\n  and Felix J. Herrmann", "title": "Fast methods for denoising matrix completion formulations, with\n  applications to robust seismic data interpolation", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent SVD-free matrix factorization formulations have enabled rank\nminimization for systems with millions of rows and columns, paving the way for\nmatrix completion in extremely large-scale applications, such as seismic data\ninterpolation.\n  In this paper, we consider matrix completion formulations designed to hit a\ntarget data-fitting error level provided by the user, and propose an algorithm\ncalled LR-BPDN that is able to exploit factorized formulations to solve the\ncorresponding optimization problem. Since practitioners typically have strong\nprior knowledge about target error level, this innovation makes it easy to\napply the algorithm in practice, leaving only the factor rank to be determined.\n  Within the established framework, we propose two extensions that are highly\nrelevant to solving practical challenges of data interpolation. First, we\npropose a weighted extension that allows known subspace information to improve\nthe results of matrix completion formulations. We show how this weighting can\nbe used in the context of frequency continuation, an essential aspect to\nseismic data interpolation. Second, we propose matrix completion formulations\nthat are robust to large measurement errors in the available data.\n  We illustrate the advantages of LR-BPDN on the collaborative filtering\nproblem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use\nthe new method, along with its robust and subspace re-weighted extensions, to\nobtain high-quality reconstructions for large scale seismic interpolation\nproblems with real data, even in the presence of data contamination.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 12:31:30 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 10:03:30 GMT"}, {"version": "v3", "created": "Wed, 5 Mar 2014 10:29:18 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Kumar", "Rajiv", ""], ["Mansour", "Hassan", ""], ["Recht", "Ben", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1302.4922", "submitter": "David Duvenaud", "authors": "David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum,\n  Zoubin Ghahramani", "title": "Structure Discovery in Nonparametric Regression through Compositional\n  Kernel Search", "comments": "9 pages, 7 figures, To appear in proceedings of the 2013\n  International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its importance, choosing the structural form of the kernel in\nnonparametric regression remains a black art. We define a space of kernel\nstructures which are built compositionally by adding and multiplying a small\nnumber of base kernels. We present a method for searching over this space of\nstructures which mirrors the scientific discovery process. The learned\nstructures can often decompose functions into interpretable components and\nenable long-range extrapolation on time-series datasets. Our structure search\nmethod outperforms many widely used kernels and kernel combination methods on a\nvariety of prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 14:53:13 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 11:48:12 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2013 16:53:30 GMT"}, {"version": "v4", "created": "Mon, 13 May 2013 13:10:31 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Duvenaud", "David", ""], ["Lloyd", "James Robert", ""], ["Grosse", "Roger", ""], ["Tenenbaum", "Joshua B.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1302.4964", "submitter": "George H. John", "authors": "George H. John, Pat Langley", "title": "Estimating Continuous Distributions in Bayesian Classifiers", "comments": "Appears in Proceedings of the Eleventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1995)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1995-PG-338-345", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling a probability distribution with a Bayesian network, we are\nfaced with the problem of how to handle continuous variables. Most previous\nwork has either solved the problem by discretizing, or assumed that the data\nare generated by a single Gaussian. In this paper we abandon the normality\nassumption and instead use statistical methods for nonparametric density\nestimation. For a naive Bayesian classifier, we present experimental results on\na variety of natural and artificial domains, comparing two methods of density\nestimation: assuming normality and modeling each conditional distribution with\na single Gaussian; and using nonparametric kernel density estimation. We\nobserve large reductions in error on several natural and artificial data sets,\nwhich suggests that kernel estimation is a useful tool for learning Bayesian\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 15:22:01 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["John", "George H.", ""], ["Langley", "Pat", ""]]}, {"id": "1302.5010", "submitter": "Mingkui Tan", "authors": "Mingkui Tan and Ivor W. Tsang and Li Wang", "title": "Matching Pursuit LASSO Part II: Applications and Sparse Recovery over\n  Batch Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching Pursuit LASSIn Part I \\cite{TanPMLPart1}, a Matching Pursuit LASSO\n({MPL}) algorithm has been presented for solving large-scale sparse recovery\n(SR) problems. In this paper, we present a subspace search to further improve\nthe performance of MPL, and then continue to address another major challenge of\nSR -- batch SR with many signals, a consideration which is absent from most of\nprevious $\\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed to\nvastly speed up sparse recovery of many signals simultaneously. Comprehensive\nnumerical experiments on compressive sensing and face recognition tasks\ndemonstrate the superior performance of MPL and BMPL over other methods\nconsidered in this paper, in terms of sparse recovery ability and efficiency.\nIn particular, BMPL is up to 400 times faster than existing $\\ell_1$-norm\nmethods considered to be state-of-the-art.O Part II: Applications and Sparse\nRecovery over Batch Signals\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 16:09:38 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 00:14:31 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Tan", "Mingkui", ""], ["Tsang", "Ivor W.", ""], ["Wang", "Li", ""]]}, {"id": "1302.5125", "submitter": "Oren Rippel", "authors": "Oren Rippel, Ryan Prescott Adams", "title": "High-Dimensional Probability Estimation with Deep Density Models", "comments": "12 pages, 4 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in machine learning is the estimation of a\nprobability distribution from data. Many techniques have been proposed to study\nthe structure of data, most often building around the assumption that\nobservations lie on a lower-dimensional manifold of high probability. It has\nbeen more difficult, however, to exploit this insight to build explicit,\ntractable density models for high-dimensional data. In this paper, we introduce\nthe deep density model (DDM), a new approach to density estimation. We exploit\ninsights from deep learning to construct a bijective map to a representation\nspace, under which the transformation of the distribution of the data is\napproximately factorized and has identical and known marginal densities. The\nsimplicity of the latent distribution under the model allows us to feasibly\nexplore it, and the invertibility of the map to characterize contraction of\nmeasure across it. This enables us to compute normalized densities for\nout-of-sample data. This combination of tractability and flexibility allows us\nto tackle a variety of probabilistic tasks on high-dimensional datasets,\nincluding: rapid computation of normalized densities at test-time without\nevaluating a partition function; generation of samples without MCMC; and\ncharacterization of the joint entropy of the data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 21:20:30 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Rippel", "Oren", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1302.5134", "submitter": "Jing Qian", "authors": "Jing Qian and Venkatesh Saligrama", "title": "Spectral Clustering with Unbalanced Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1205.1496", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering (SC) and graph-based semi-supervised learning (SSL)\nalgorithms are sensitive to how graphs are constructed from data. In particular\nif the data has proximal and unbalanced clusters these algorithms can lead to\npoor performance on well-known graphs such as $k$-NN, full-RBF,\n$\\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) or\nnormalized cut (NCut) attempt to tradeoff cut values with cluster sizes, which\nare not tailored to unbalanced data. We propose a novel graph partitioning\nframework, which parameterizes a family of graphs by adaptively modulating node\ndegrees in a $k$-NN graph. We then propose a model selection scheme to choose\nsizable clusters which are separated by smallest cut values. Our framework is\nable to adapt to varying levels of unbalancedness of data and can be naturally\nused for small cluster detection. We theoretically justify our ideas through\nlimit cut analysis. Unsupervised and semi-supervised experiments on synthetic\nand real data sets demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 21:54:04 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1302.5337", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly and Louis Theran", "title": "Obtaining error-minimizing estimates and universal entry-wise error\n  bounds for low-rank matrix completion", "comments": "14 pages with appendix, 2 figures, v2 adds larger experiments and\n  smoother exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.AG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for reconstructing and denoising single\nentries of incomplete and noisy entries. We describe: effective algorithms for\ndeciding if and entry can be reconstructed and, if so, for reconstructing and\ndenoising it; and a priori bounds on the error of each entry, individually. In\nthe noiseless case our algorithm is exact. For rank-one matrices, the new\nalgorithm is fast, admits a highly-parallel implementation, and produces an\nerror minimizing estimate that is qualitatively close to our theoretical and\nthe state-of-the-are Nuclear Norm and OptSpace methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 17:05:24 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 21:43:05 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""]]}, {"id": "1302.5449", "submitter": "Juan Andres Bazerque", "authors": "Juan Andres Bazerque and Georgios B. Giannakis", "title": "Nonparametric Basis Pursuit via Sparse Kernel-based Learning", "comments": "IEEE SIGNAL PROCESSING MAGAZINE, 2013 (TO APPEAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal processing tasks as fundamental as sampling, reconstruction, minimum\nmean-square error interpolation and prediction can be viewed under the prism of\nreproducing kernel Hilbert spaces. Endowing this vantage point with\ncontemporary advances in sparsity-aware modeling and processing, promotes the\nnonparametric basis pursuit advocated in this paper as the overarching\nframework for the confluence of kernel-based learning (KBL) approaches\nleveraging sparse linear regression, nuclear-norm regularization, and\ndictionary learning. The novel sparse KBL toolbox goes beyond translating\nsparse parametric approaches to their nonparametric counterparts, to\nincorporate new possibilities such as multi-kernel selection and matrix\nsmoothing. The impact of sparse KBL to signal processing applications is\nillustrated through test cases from cognitive radio sensing, microarray data\nimputation, and network traffic prediction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 22:59:12 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Bazerque", "Juan Andres", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1302.5608", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers and \\\"Ur\\\"un Dogan", "title": "Accelerated Linear SVM Training with Adaptive Variable Selection\n  Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) training is an active research area since the\ndawn of the method. In recent years there has been increasing interest in\nspecialized solvers for the important case of linear models. The algorithm\npresented by Hsieh et al., probably best known under the name of the\n\"liblinear\" implementation, marks a major breakthrough. The method is analog to\nestablished dual decomposition algorithms for training of non-linear SVMs, but\nwith greatly reduced computational complexity per update step. This comes at\nthe cost of not keeping track of the gradient of the objective any more, which\nexcludes the application of highly developed working set selection algorithms.\nWe present an algorithmic improvement to this method. We replace uniform\nworking set selection with an online adaptation of selection frequencies. The\nadaptation criterion is inspired by modern second order working set selection\nmethods. The same mechanism replaces the shrinking heuristic. This novel\ntechnique speeds up training in some cases by more than an order of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 14:36:59 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Dogan", "\u00dcr\u00fcn", ""]]}, {"id": "1302.5729", "submitter": "Ivan Selesnick", "authors": "Ivan W. Selesnick and Ilker Bayram", "title": "Sparse Signal Estimation by Maximally Sparse Convex Optimization", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2298839", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sparsity penalized least squares for\napplications in sparse signal processing, e.g. sparse deconvolution. This paper\naims to induce sparsity more strongly than L1 norm regularization, while\navoiding non-convex optimization. For this purpose, this paper describes the\ndesign and use of non-convex penalty functions (regularizers) constrained so as\nto ensure the convexity of the total cost function, F, to be minimized. The\nmethod is based on parametric penalty functions, the parameters of which are\nconstrained to ensure convexity of F. It is shown that optimal parameters can\nbe obtained by semidefinite programming (SDP). This maximally sparse convex\n(MSC) approach yields maximally non-convex sparsity-inducing penalty functions\nconstrained such that the total cost function, F, is convex. It is demonstrated\nthat iterative MSC (IMSC) can yield solutions substantially more sparse than\nthe standard convex sparsity-inducing approach, i.e., L1 norm minimization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 22:36:08 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 13:47:53 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 15:48:14 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Selesnick", "Ivan W.", ""], ["Bayram", "Ilker", ""]]}, {"id": "1302.6009", "submitter": "Aryeh Kontorovich", "authors": "Aryeh Kontorovich, Boaz Nadler, Roi Weiss", "title": "On learning parametric-output HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for learning an HMM whose outputs are distributed\naccording to a parametric family. This is done by {\\em decoupling} the learning\ntask into two steps: first estimating the output parameters, and then\nestimating the hidden states transition probabilities. The first step is\naccomplished by fitting a mixture model to the output stationary distribution.\nGiven the parameters of this mixture model, the second step is formulated as\nthe solution of an easily solvable convex quadratic program. We provide an\nerror analysis for the estimated transition probabilities and show they are\nrobust to small perturbations in the estimates of the mixture parameters.\nFinally, we support our analysis with some encouraging empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 07:20:19 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Nadler", "Boaz", ""], ["Weiss", "Roi", ""]]}, {"id": "1302.6194", "submitter": "Ondrej \\v{S}uch", "authors": "Ondrej Such and Lenka Mackovicova", "title": "Phoneme discrimination using $KS$-algebra II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $KS$-algebra consists of expressions constructed with four kinds operations,\nthe minimum, maximum, difference and additively homogeneous generalized means.\nFive families of $Z$-classifiers are investigated on binary classification\ntasks between English phonemes. It is shown that the classifiers are able to\nreflect well known formant characteristics of vowels, while having very small\nKolmogoroff's complexity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 18:56:49 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Such", "Ondrej", ""], ["Mackovicova", "Lenka", ""]]}, {"id": "1302.6434", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and James V. Burke and Alessandro Chiuso and\n  Gianluigi Pillonetto", "title": "Convex vs nonconvex approaches for sparse estimation: GLasso, Multiple\n  Kernel Learning and Hyperparameter GLasso", "comments": "50 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular Lasso approach for sparse estimation can be derived via\nmarginalization of a joint density associated with a particular stochastic\nmodel. A different marginalization of the same probabilistic model leads to a\ndifferent non-convex estimator where hyperparameters are optimized. Extending\nthese arguments to problems where groups of variables have to be estimated, we\nstudy a computational scheme for sparse estimation that differs from the Group\nLasso. Although the underlying optimization problem defining this estimator is\nnon-convex, an initialization strategy based on a univariate Bayesian forward\nselection scheme is presented. This also allows us to define an effective\nnon-convex estimator where only one scalar variable is involved in the\noptimization process. Theoretical arguments, independent of the correctness of\nthe priors entering the sparse model, are included to clarify the advantages of\nthis non-convex technique in comparison with other convex estimators. Numerical\nexperiments are also used to compare the performance of these approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 13:53:36 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 01:21:37 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Chiuso", "Alessandro", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1302.6452", "submitter": "Jing Lei", "authors": "Jing Lei, Alessandro Rinaldo, Larry Wasserman", "title": "A Conformal Prediction Approach to Explore Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies conformal prediction techniques to compute simultaneous\nprediction bands and clustering trees for functional data. These tools can be\nused to detect outliers and clusters. Both our prediction bands and clustering\ntrees provide prediction sets for the underlying stochastic process with a\nguaranteed finite sample behavior, under no distributional assumptions. The\nprediction sets are also informative in that they correspond to the high\ndensity region of the underlying process. While ordinary conformal prediction\nhas high computational cost for functional data, we use the inductive conformal\npredictor, together with several novel choices of conformity scores, to\nsimplify the computation. Our methods are illustrated on some real data\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 15:16:32 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Lei", "Jing", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1302.6584", "submitter": "Qiang Liu", "authors": "Qiang Liu and Alexander Ihler", "title": "Variational Algorithms for Marginal MAP", "comments": "This is a journal version of our conference paper \"variational\n  algorithms for marginal MAP\" in UAI 201 [arXiv:1202.3742]; this version is\n  considerably expanded, with more detail in its development, examples,\n  algorithms, and proofs; additional experiments; and a junction graph version\n  of the central message-passing algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal maximum a posteriori probability (MAP) estimation problem, which\ncalculates the mode of the marginal posterior distribution of a subset of\nvariables with the remaining variables marginalized, is an important inference\nproblem in many models, such as those with hidden variables or uncertain\nparameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has\nattracted less attention in the literature compared to the joint MAP\n(maximization) and marginalization problems. We derive a general dual\nrepresentation for marginal MAP that naturally integrates the marginalization\nand maximization operations into a joint variational optimization problem,\nmaking it possible to easily extend most or all variational-based algorithms to\nmarginal MAP. In particular, we derive a set of \"mixed-product\" message passing\nalgorithms for marginal MAP, whose form is a hybrid of max-product, sum-product\nand a novel \"argmax-product\" message updates. We also derive a class of\nconvergent algorithms based on proximal point methods, including one that\ntransforms the marginal MAP problem into a sequence of standard marginalization\nproblems. Theoretically, we provide guarantees under which our algorithms give\nglobally or locally optimal solutions, and provide novel upper bounds on the\noptimal objectives. Empirically, we demonstrate that our algorithms\nsignificantly outperform the existing approaches, including a state-of-the-art\nalgorithm based on local search methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 20:58:59 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 18:33:03 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2013 00:29:57 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1302.6613", "submitter": "Ratnadip Adhikari", "authors": "Ratnadip Adhikari, R. K. Agrawal", "title": "An Introductory Study on Time Series Modeling and Forecasting", "comments": "67 pages, 29 figures, 33 references, book", "journal-ref": "LAP Lambert Academic Publishing, Germany, 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series modeling and forecasting has fundamental importance to various\npractical domains. Thus a lot of active research works is going on in this\nsubject during several years. Many important models have been proposed in\nliterature for improving the accuracy and effectiveness of time series\nforecasting. The aim of this dissertation work is to present a concise\ndescription of some popular time series forecasting models used in practice,\nwith their salient features. In this thesis, we have described three important\nclasses of time series models, viz. the stochastic, neural networks and SVM\nbased models, together with their inherent forecasting strengths and\nweaknesses. We have also discussed about the basic issues related to time\nseries modeling, such as stationarity, parsimony, overfitting, etc. Our\ndiscussion about different time series models is supported by giving the\nexperimental forecast results, performed on six real time series datasets.\nWhile fitting a model to a dataset, special care is taken to select the most\nparsimonious one. To evaluate forecast accuracy as well as to compare among\ndifferent models fitted to a time series, we have used the five performance\nmeasures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of the\nsix datasets, we have shown the obtained forecast diagram which graphically\ndepicts the closeness between the original and forecasted observations. To have\nauthenticity as well as clarity in our discussion about time series modeling\nand forecasting, we have taken the help of various published research works\nfrom reputed journals and some standard books.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 22:18:55 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Adhikari", "Ratnadip", ""], ["Agrawal", "R. K.", ""]]}, {"id": "1302.6677", "submitter": "Ashish Sabharwal", "authors": "Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration is affected by the curse of dimensionality and quickly becomes\nintractable as the dimensionality of the problem grows. We propose a randomized\nalgorithm that, with high probability, gives a constant-factor approximation of\na general discrete integral defined over an exponentially large set. This\nalgorithm relies on solving only a small number of instances of a discrete\ncombinatorial optimization problem subject to randomly generated parity\nconstraints used as a hash function. As an application, we demonstrate that\nwith a small number of MAP queries we can efficiently approximate the partition\nfunction of discrete graphical models, which can in turn be used, for instance,\nfor marginal computation or model selection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 06:45:28 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Ermon", "Stefano", ""], ["Gomes", "Carla P.", ""], ["Sabharwal", "Ashish", ""], ["Selman", "Bart", ""]]}, {"id": "1302.6766", "submitter": "Kevin Fran\\c{c}oisse", "authors": "Kevin Fran\\c{c}oisse, Ilkka Kivim\\\"aki, Amin Mantrach, Fabrice Rossi,\n  Marco Saerens", "title": "A bag-of-paths framework for network data analysis", "comments": "Manuscript submitted for publication", "journal-ref": "Neural Networks, 90, pp. 90-111 (2017)", "doi": "10.1016/j.neunet.2017.03.010", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a generic framework, called the bag-of-paths (BoP), for\nlink and network data analysis. The central idea is to assign a probability\ndistribution on the set of all paths in a network. More precisely, a\nGibbs-Boltzmann distribution is defined over a bag of paths in a network, that\nis, on a representation that considers all paths independently. We show that,\nunder this distribution, the probability of drawing a path connecting two nodes\ncan easily be computed in closed form by simple matrix inversion. This\nprobability captures a notion of relatedness between nodes of the graph: two\nnodes are considered as highly related when they are connected by many,\npreferably low-cost, paths. As an application, two families of distances\nbetween nodes are derived from the BoP probabilities. Interestingly, the second\ndistance family interpolates between the shortest path distance and the\nresistance distance. In addition, it extends the Bellman-Ford formula for\ncomputing the shortest path distance in order to integrate sub-optimal paths by\nsimply replacing the minimum operator by the soft minimum operator.\nExperimental results on semi-supervised classification show that both of the\nnew distance families are competitive with other state-of-the-art approaches.\nIn addition to the distance measures studied in this paper, the bag-of-paths\nframework enables straightforward computation of many other relevant network\nmeasures.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 13:41:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 10:21:49 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 08:47:37 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Fran\u00e7oisse", "Kevin", ""], ["Kivim\u00e4ki", "Ilkka", ""], ["Mantrach", "Amin", ""], ["Rossi", "Fabrice", ""], ["Saerens", "Marco", ""]]}, {"id": "1302.6768", "submitter": "Gil Shabat", "authors": "Gil Shabat, Yaniv Shmueli and Amir Averbuch", "title": "Missing Entries Matrix Approximation and Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe several algorithms for matrix completion and matrix approximation\nwhen only some of its entries are known. The approximation constraint can be\nany whose approximated solution is known for the full matrix. For low rank\napproximations, similar algorithms appears recently in the literature under\ndifferent names. In this work, we introduce new theorems for matrix\napproximation and show that these algorithms can be extended to handle\ndifferent constraints such as nuclear norm, spectral norm, orthogonality\nconstraints and more that are different than low rank approximations. As the\nalgorithms can be viewed from an optimization point of view, we discuss their\nconvergence to global solution for the convex case. We also discuss the optimal\nstep size and show that it is fixed in each iteration. In addition, the derived\nmatrix completion flow is robust and does not require any parameters. This\nmatrix completion flow is applicable to different spectral minimizations and\ncan be applied to physics, mathematics and electrical engineering problems such\nas data reconstruction of images and data coming from PDEs such as Helmholtz\nequation used for electromagnetic waves.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 13:47:45 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 09:25:20 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Shabat", "Gil", ""], ["Shmueli", "Yaniv", ""], ["Averbuch", "Amir", ""]]}, {"id": "1302.6808", "submitter": "David Heckerman", "authors": "Dan Geiger and David Heckerman", "title": "Learning Gaussian Networks", "comments": "This version has improved pointers to the literature", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-235-243", "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe algorithms for learning Bayesian networks from a combination of\nuser knowledge and statistical data. The algorithms have two components: a\nscoring metric and a search procedure. The scoring metric takes a network\nstructure, statistical data, and a user's prior knowledge, and returns a score\nproportional to the posterior probability of the network structure given the\ndata. The search procedure generates networks for evaluation by the scoring\nmetric. Previous work has concentrated on metrics for domains containing only\ndiscrete variables, under the assumption that data represents a multinomial\nsample. In this paper, we extend this work, developing scoring metrics for\ndomains containing all continuous variables or a mixture of discrete and\ncontinuous variables, under the assumption that continuous data is sampled from\na multivariate normal distribution. Our work extends traditional statistical\napproaches for identifying vanishing regression coefficients in that we\nidentify two important assumptions, called event equivalence and parameter\nmodularity, that when combined allow the construction of prior distributions\nfor multivariate normal parameters from a single prior Bayesian network\nspecified by a user.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:16:08 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:10:19 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 16:16:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1302.6828", "submitter": "Pat Langley", "authors": "Pat Langley, Stephanie Sage", "title": "Induction of Selective Bayesian Classifiers", "comments": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-399-406", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine previous work on the naive Bayesian classifier and\nreview its limitations, which include a sensitivity to correlated features. We\nrespond to this problem by embedding the naive Bayesian induction scheme within\nan algorithm that c arries out a greedy search through the space of features.\nWe hypothesize that this approach will improve asymptotic accuracy in domains\nthat involve correlated features without reducing the rate of learning in ones\nthat do not. We report experimental results on six natural domains, including\ncomparisons with decision-tree induction, that support these hypotheses. In\nclosing, we discuss other approaches to extending naive Bayesian classifiers\nand outline some directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:18:05 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Langley", "Pat", ""], ["Sage", "Stephanie", ""]]}, {"id": "1302.7043", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis, Tom M. Mitchell, Nicholas D. Sidiropoulos,\n  Christos Faloutsos, Partha Pratim Talukdar, Brian Murphy", "title": "Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we correlate neural activity in the human brain as it responds to\nwords, with behavioral data expressed as answers to questions about these same\nwords? In short, we want to find latent variables, that explain both the brain\nactivity, as well as the behavioral responses. We show that this is an instance\nof the Coupled Matrix-Tensor Factorization (CMTF) problem. We propose\nScoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problem\nand produces a sparse latent low-rank subspace of the data. In our experiments,\nwe find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm\nfor CMTF, along with a 5 fold increase in sparsity. Moreover, we extend\nScoup-SMT to handle missing data without degradation of performance. We apply\nScoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, human\nsubjects) tensor and a (nouns, properties) matrix, with coupling along the\nnouns dimension. Scoup-SMT is able to find meaningful latent variables, as well\nas to predict brain activity with competitive accuracy. Finally, we demonstrate\nthe generality of Scoup-SMT, by applying it on a Facebook dataset (users,\nfriends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 00:37:29 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Papalexakis", "Evangelos E.", ""], ["Mitchell", "Tom M.", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Faloutsos", "Christos", ""], ["Talukdar", "Partha Pratim", ""], ["Murphy", "Brian", ""]]}, {"id": "1302.7056", "submitter": "Wesam Elshamy", "authors": "Wesam Elshamy, Doina Caragea, William Hsu", "title": "KSU KDD: Word Sense Induction by Clustering in Topic Space", "comments": null, "journal-ref": "Proceedings of the 5th International Workshop on Semantic\n  Evaluation, pages 367-370, Uppsala, Sweden, July 2010. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our language-independent unsupervised word sense induction\nsystem. This system only uses topic features to cluster different word senses\nin their global context topic space. Using unlabeled data, this system trains a\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\ndistribution of the test instances. By clustering these topics distributions in\ntheir topic space we cluster them into different senses. Our hypothesis is that\ncloseness in topic space reflects similarity between different word senses.\nThis system participated in SemEval-2 word sense induction and disambiguation\ntask and achieved the second highest V-measure score among all other systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 02:10:38 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Elshamy", "Wesam", ""], ["Caragea", "Doina", ""], ["Hsu", "William", ""]]}, {"id": "1302.7088", "submitter": "Wesam Elshamy", "authors": "Wesam Elshamy", "title": "Continuous-time Infinite Dynamic Topic Models", "comments": "Ph.D. dissertation, Kansas State University, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are probabilistic models for discovering topical themes in\ncollections of documents. In real world applications, these models provide us\nwith the means of organizing what would otherwise be unstructured collections.\nThey can help us cluster a huge collection into different topics or find a\nsubset of the collection that resembles the topical theme found in an article\nat hand.\n  The first wave of topic models developed were able to discover the prevailing\ntopics in a big collection of documents spanning a period of time. It was later\nrealized that these time-invariant models were not capable of modeling 1) the\ntime varying number of topics they discover and 2) the time changing structure\nof these topics. Few models were developed to address this two deficiencies.\nThe online-hierarchical Dirichlet process models the documents with a time\nvarying number of topics. It varies the structure of the topics over time as\nwell. However, it relies on document order, not timestamps to evolve the model\nover time. The continuous-time dynamic topic model evolves topic structure in\ncontinuous-time. However, it uses a fixed number of topics over time.\n  In this dissertation, I present a model, the continuous-time infinite dynamic\ntopic model, that combines the advantages of these two models 1) the\nonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic\nmodel. More specifically, the model I present is a probabilistic topic model\nthat does the following: 1) it changes the number of topics over continuous\ntime, and 2) it changes the topic structure over continuous-time.\n  I compared the model I developed with the two other models with different\nsetting values. The results obtained were favorable to my model and showed the\nneed for having a model that has a continuous-time varying number of topics and\ntopic structure.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 05:30:41 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Elshamy", "Wesam", ""]]}, {"id": "1302.7099", "submitter": "Nicolas Verzelen", "authors": "Ery Arias-Castro (Math Dept, UCSD), Nicolas Verzelen (MISTEA)", "title": "Community Detection in Random Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the problem of detecting a community in a network into testing\nwhether in a given (random) graph there is a subgraph that is unusually dense.\nWe observe an undirected and unweighted graph on N nodes. Under the null\nhypothesis, the graph is a realization of an Erd\\\"os-R\\'enyi graph with\nprobability p0. Under the (composite) alternative, there is a subgraph of n\nnodes where the probability of connection is p1 > p0. We derive a detection\nlower bound for detecting such a subgraph in terms of N, n, p0, p1 and exhibit\na test that achieves that lower bound. We do this both when p0 is known and\nunknown. We also consider the problem of testing in polynomial-time. As an\naside, we consider the problem of detecting a clique, which is intimately\nrelated to the planted clique problem. Our focus in this paper is in the\nquasi-normal regime where n p0 is either bounded away from zero, or tends to\nzero slowly.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 07:04:35 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Arias-Castro", "Ery", "", "Math Dept, UCSD"], ["Verzelen", "Nicolas", "", "MISTEA"]]}, {"id": "1302.7175", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt", "title": "Estimating the Maximum Expected Value: An Analysis of (Nested) Cross\n  Validation and the Maximum Sample Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the accuracy of the two most common estimators for the maximum\nexpected value of a general set of random variables: a generalization of the\nmaximum sample average, and cross validation. No unbiased estimator exists and\nwe show that it is non-trivial to select a good estimator without knowledge\nabout the distributions of the random variables. We investigate and bound the\nbias and variance of the aforementioned estimators and prove consistency. The\nvariance of cross validation can be significantly reduced, but not without\nrisking a large bias. The bias and variance of different variants of cross\nvalidation are shown to be very problem-dependent, and a wrong choice can lead\nto very inaccurate estimates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 12:48:32 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2013 15:04:48 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["van Hasselt", "Hado", ""]]}, {"id": "1302.7220", "submitter": "Amir Atiya", "authors": "Amir F. Atiya, Hatem A. Fayed, Ahmed H. Abdel-Gawad", "title": "A New Monte Carlo Based Algorithm for the Gaussian Process\n  Classification Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process is a very promising novel technology that has been applied\nto both the regression problem and the classification problem. While for the\nregression problem it yields simple exact solutions, this is not the case for\nthe classification problem, because we encounter intractable integrals. In this\npaper we develop a new derivation that transforms the problem into that of\nevaluating the ratio of multivariate Gaussian orthant integrals. Moreover, we\ndevelop a new Monte Carlo procedure that evaluates these integrals. It is based\non some aspects of bootstrap sampling and acceptancerejection. The proposed\napproach has beneficial properties compared to the existing Markov Chain Monte\nCarlo approach, such as simplicity, reliability, and speed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 15:02:34 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 01:31:29 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Atiya", "Amir F.", ""], ["Fayed", "Hatem A.", ""], ["Abdel-Gawad", "Ahmed H.", ""]]}, {"id": "1302.7280", "submitter": "Eric Lock", "authors": "Eric F. Lock and David B. Dunson", "title": "Bayesian Consensus Clustering", "comments": "32 pages, 13 figures", "journal-ref": "Bioinformatics 29 (2013) 2610-2616", "doi": "10.1093/bioinformatics/btt425", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of clustering a set of objects based on multiple sources of data\narises in several modern applications. We propose an integrative statistical\nmodel that permits a separate clustering of the objects for each data source.\nThese separate clusterings adhere loosely to an overall consensus clustering,\nand hence they are not independent. We describe a computationally scalable\nBayesian framework for simultaneous estimation of both the consensus clustering\nand the source-specific clusterings. We demonstrate that this flexible approach\nis more robust than joint clustering of all data sources, and is more powerful\nthan clustering each data source separately. This work is motivated by the\nintegrated analysis of heterogeneous biomedical data, and we present an\napplication to subtype identification of breast cancer tumor samples using\npublicly available data from The Cancer Genome Atlas. Software is available at\nhttp://people.duke.edu/~el113/software.html.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 18:40:14 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lock", "Eric F.", ""], ["Dunson", "David B.", ""]]}]