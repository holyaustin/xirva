[{"id": "1504.00052", "submitter": "Eric Bax", "authors": "Eric Bax", "title": "Improved Error Bounds Based on Worst Likely Assignments", "comments": "IJCNN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error bounds based on worst likely assignments use permutation tests to\nvalidate classifiers. Worst likely assignments can produce effective bounds\neven for data sets with 100 or fewer training examples. This paper introduces a\nstatistic for use in the permutation tests of worst likely assignments that\nimproves error bounds, especially for accurate classifiers, which are typically\nthe classifiers of interest.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 21:48:56 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Bax", "Eric", ""]]}, {"id": "1504.00064", "submitter": "James Zou", "authors": "James Y. Zou, Kamalika Chaudhuri, Adam Tauman Kalai", "title": "Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised approach to efficiently discover the underlying\nfeatures in a data set via crowdsourcing. Our queries ask crowd members to\narticulate a feature common to two out of three displayed examples. In addition\nwe also ask the crowd to provide binary labels to the remaining examples based\non the discovered features. The triples are chosen adaptively based on the\nlabels of the previously discovered features on the data set. In two natural\nmodels of features, hierarchical and independent, we show that a simple\nadaptive algorithm, using \"two-out-of-three\" similarity queries, recovers all\nfeatures with less labor than any nonadaptive algorithm. Experimental results\nvalidate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 23:27:03 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Zou", "James Y.", ""], ["Chaudhuri", "Kamalika", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1504.00083", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Robert C. Williamson", "title": "A Theory of Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Learning aims to extract relevant information contained in data sets\nin an automated fashion. It is driving force behind the current deep learning\ntrend, a set of methods that have had widespread empirical success. What is\nlacking is a theoretical understanding of different feature learning schemes.\nThis work provides a theoretical framework for feature learning and then\ncharacterizes when features can be learnt in an unsupervised fashion. We also\nprovide means to judge the quality of features via rate-distortion theory and\nits generalizations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 02:31:55 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1504.00091", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Robert C. Williamson", "title": "Learning in the Presence of Corruption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning one wishes to identify a pattern present in a joint\ndistribution $P$, of instances, label pairs, by providing a function $f$ from\ninstances to labels that has low risk $\\mathbb{E}_{P}\\ell(y,f(x))$. To do so,\nthe learner is given access to $n$ iid samples drawn from $P$. In many real\nworld problems clean samples are not available. Rather, the learner is given\naccess to samples from a corrupted distribution $\\tilde{P}$ from which to\nlearn, while the goal of predicting the clean pattern remains. There are many\ndifferent types of corruption one can consider, and as of yet there is no\ngeneral means to compare the relative ease of learning under these different\ncorruption processes. In this paper we develop a general framework for tackling\nsuch problems as well as introducing upper and lower bounds on the risk for\nlearning in the presence of corruption. Our ultimate goal is to be able to make\ninformed economic decisions in regards to the acquisition of data sets. For a\ncertain subclass of corruption processes (those that are\n\\emph{reconstructible}) we achieve this goal in a particular sense. Our lower\nbounds are in terms of the coefficient of ergodicity, a simple to calculate\nproperty of stochastic matrices. Our upper bounds proceed via a generalization\nof the method of unbiased estimators appearing in recent work of Natarajan et\nal and implicit in the earlier work of Kearns.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 02:54:38 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 14:09:14 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1504.00284", "submitter": "Adrian  Calma", "authors": "Adrian Calma, Tobias Reitmaier, Bernhard Sick, Paul Lukowicz, Mark\n  Embrechts", "title": "A New Vision of Collaborative Active Learning", "comments": "16 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) is a learning paradigm where an active learner has to\ntrain a model (e.g., a classifier) which is in principal trained in a\nsupervised way, but in AL it has to be done by means of a data set with\ninitially unlabeled samples. To get labels for these samples, the active\nlearner has to ask an oracle (e.g., a human expert) for labels. The goal is to\nmaximize the performance of the model and to minimize the number of queries at\nthe same time. In this article, we first briefly discuss the state of the art\nand own, preliminary work in the field of AL. Then, we propose the concept of\ncollaborative active learning (CAL). With CAL, we will overcome some of the\nharsh limitations of current AL. In particular, we envision scenarios where an\nexpert may be wrong for various reasons, there might be several or even many\nexperts with different expertise, the experts may label not only samples but\nalso knowledge at a higher level such as rules, and we consider that the\nlabeling costs depend on many conditions. Moreover, in a CAL process human\nexperts will profit by improving their own knowledge, too.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 16:39:26 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 15:17:33 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 16:11:15 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Calma", "Adrian", ""], ["Reitmaier", "Tobias", ""], ["Sick", "Bernhard", ""], ["Lukowicz", "Paul", ""], ["Embrechts", "Mark", ""]]}, {"id": "1504.00298", "submitter": "Richard Everitt", "authors": "Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina\n  Evdemon-Hogan", "title": "Bayesian model comparison with un-normalised likelihoods", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9629-2", "report-no": null, "categories": "stat.CO physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for which the likelihood function can be evaluated only up to a\nparameter-dependent unknown normalising constant, such as Markov random field\nmodels, are used widely in computer science, statistical physics, spatial\nstatistics, and network analysis. However, Bayesian analysis of these models\nusing standard Monte Carlo methods is not possible due to the intractability of\ntheir likelihood functions. Several methods that permit exact, or close to\nexact, simulation from the posterior distribution have recently been developed.\nHowever, estimating the evidence and Bayes' factors (BFs) for these models\nremains challenging in general. This paper describes new random weight\nimportance sampling and sequential Monte Carlo methods for estimating BFs that\nuse simulation to circumvent the evaluation of the intractable likelihood, and\ncompares them to existing methods. In some cases we observe an advantage in the\nuse of biased weight estimates. An initial investigation into the theoretical\nand empirical properties of this class of methods is presented. Some support\nfor the use of biased estimates is presented, but we advocate caution in the\nuse of such estimates.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:10:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 15:42:08 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2016 23:33:41 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Everitt", "Richard G.", ""], ["Johansen", "Adam M.", ""], ["Rowing", "Ellen", ""], ["Evdemon-Hogan", "Melina", ""]]}, {"id": "1504.00377", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang, Debdeep Pati, Anuj Srivastava", "title": "Bayesian Clustering of Shapes of Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised clustering of curves according to their shapes is an important\nproblem with broad scientific applications. The existing model-based clustering\ntechniques either rely on simple probability models (e.g., Gaussian) that are\nnot generally valid for shape analysis or assume the number of clusters. We\ndevelop an efficient Bayesian method to cluster curve data using an elastic\nshape metric that is based on joint registration and comparison of shapes of\ncurves. The elastic-inner product matrix obtained from the data is modeled\nusing a Wishart distribution whose parameters are assigned carefully chosen\nprior distributions to allow for automatic inference on the number of clusters.\nPosterior is sampled through an efficient Markov chain Monte Carlo procedure\nbased on the Chinese restaurant process to infer (1) the posterior distribution\non the number of clusters, and (2) clustering configuration of shapes. This\nmethod is demonstrated on a variety of synthetic data and real data examples on\nprotein structure analysis, cell shape analysis in microscopy images, and\nclustering of shaped from MPEG7 database.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 20:35:33 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Pati", "Debdeep", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1504.00386", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield and Sarah Marzen", "title": "Signatures of Infinity: Nonergodicity and Resource Scaling in\n  Prediction, Complexity, and Learning", "comments": "8 pages, 1 figure; http://csc.ucdavis.edu/~cmg/compmech/pubs/soi.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple analysis of the structural complexity of\ninfinite-memory processes built from random samples of stationary, ergodic\nfinite-memory component processes. Such processes are familiar from the well\nknown multi-arm Bandit problem. We contrast our analysis with\ncomputation-theoretic and statistical inference approaches to understanding\ntheir complexity. The result is an alternative view of the relationship between\npredictability, complexity, and learning that highlights the distinct ways in\nwhich informational and correlational divergences arise in complex ergodic and\nnonergodic processes. We draw out consequences for the resource divergences\nthat delineate the structural hierarchy of ergodic processes and for processes\nthat are themselves hierarchical.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 20:55:10 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Crutchfield", "James P.", ""], ["Marzen", "Sarah", ""]]}, {"id": "1504.00593", "submitter": "Emanuele Olivetti", "authors": "Emanuele Olivetti, Thien Bao Nguyen, Paolo Avesani", "title": "The Approximation of the Dissimilarity Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3D\npathways of axons within the white matter of the brain as a tractography. The\nanalysis of tractographies has drawn attention from the machine learning and\npattern recognition communities providing novel challenges such as finding an\nappropriate representation space for the data. Many of the current learning\nalgorithms require the input to be from a vectorial space. This requirement\ncontrasts with the intrinsic nature of the tractography because its basic\nelements, called streamlines or tracks, have different lengths and different\nnumber of points and for this reason they cannot be directly represented in a\ncommon vectorial space. In this work we propose the adoption of the\ndissimilarity representation which is an Euclidean embedding technique defined\nby selecting a set of streamlines called prototypes and then mapping any new\nstreamline to the vector of distances from prototypes. We investigate the\ndegree of approximation of this projection under different prototype selection\npolicies and prototype set sizes in order to characterise its use on\ntractography data. Additionally we propose the use of a scalable approximation\nof the most effective prototype selection policy that provides fast and\naccurate dissimilarity approximations of complete tractographies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 15:47:46 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Nguyen", "Thien Bao", ""], ["Avesani", "Paolo", ""]]}, {"id": "1504.00624", "submitter": "Song Liu Dr.", "authors": "Song Liu, Taiji Suzuki, Masashi Sugiyama, Kenji Fukumizu", "title": "Structure Learning of Partitioned Markov Networks", "comments": "Camera Ready for ICML 2016. Fixed some minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn the structure of a Markov Network between two groups of random\nvariables from joint observations. Since modelling and learning the full MN\nstructure may be hard, learning the links between two groups directly may be a\npreferable option. We introduce a novel concept called the \\emph{partitioned\nratio} whose factorization directly associates with the Markovian properties of\nrandom variables across two groups. A simple one-shot convex optimization\nprocedure is proposed for learning the \\emph{sparse} factorizations of the\npartitioned ratio and it is theoretically guaranteed to recover the correct\ninter-group structure under mild conditions. The performance of the proposed\nmethod is experimentally compared with the state of the art MN structure\nlearning methods using ROC curves. Real applications on analyzing\nbipartisanship in US congress and pairwise DNA/time-series alignments are also\nreported.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 17:24:59 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 23:18:49 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 10:42:36 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2016 12:03:06 GMT"}, {"version": "v5", "created": "Fri, 27 May 2016 00:02:33 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Liu", "Song", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1504.00641", "submitter": "Ankit Patel", "authors": "Ankit B. Patel, Tan Nguyen and Richard G. Baraniuk", "title": "A Probabilistic Theory of Deep Learning", "comments": "56 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "Rice University Electrical and Computer Engineering Dept. Technical\n  Report No 2015-1", "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 18:38:38 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1504.00680", "submitter": "Justin Cheng", "authors": "Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec", "title": "Antisocial Behavior in Online Discussion Communities", "comments": "ICWSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User contributions in the form of posts, comments, and votes are essential to\nthe success of online communities. However, allowing user participation also\ninvites undesirable behavior such as trolling. In this paper, we characterize\nantisocial behavior in three large online discussion communities by analyzing\nusers who were banned from these communities. We find that such users tend to\nconcentrate their efforts in a small number of threads, are more likely to post\nirrelevantly, and are more successful at garnering responses from other users.\nStudying the evolution of these users from the moment they join a community up\nto when they get banned, we find that not only do they write worse than other\nusers over time, but they also become increasingly less tolerated by the\ncommunity. Further, we discover that antisocial behavior is exacerbated when\ncommunity feedback is overly harsh. Our analysis also reveals distinct groups\nof users with different levels of antisocial behavior that can change over\ntime. We use these insights to identify antisocial users early on, a task of\nhigh practical importance to community maintainers.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 20:04:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 17:31:50 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Cheng", "Justin", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1504.00722", "submitter": "Joseph Woodworth", "authors": "Mihai Cucuringu and Joseph Woodworth", "title": "Point Localization and Density Estimation from Ordinal kNN graphs using\n  Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of embedding unweighted, directed k-nearest neighbor\ngraphs in low-dimensional Euclidean space. The k-nearest neighbors of each\nvertex provides ordinal information on the distances between points, but not\nthe distances themselves. We use this ordinal information along with the\nlow-dimensionality to recover the coordinates of the points up to arbitrary\nsimilarity transformations (rigid transformations and scaling). Furthermore, we\nalso illustrate the possibility of robustly recovering the underlying density\nvia the Total Variation Maximum Penalized Likelihood Estimation (TV-MPLE)\nmethod. We make existing approaches scalable by using an instance of a\nlocal-to-global algorithm based on group synchronization, recently proposed in\nthe literature in the context of sensor network localization and structural\nbiology, which we augment with a scaling synchronization step. We demonstrate\nthe scalability of our approach on large graphs, and show how it compares to\nthe Local Ordinal Embedding (LOE) algorithm, which was recently proposed for\nrecovering the configuration of a cloud of points from pairwise ordinal\ncomparisons between a sparse set of distances.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 01:16:12 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 07:46:35 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Woodworth", "Joseph", ""]]}, {"id": "1504.00757", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Learning Mixed Membership Mallows Models from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel parameterized family of Mixed Membership Mallows Models\n(M4) to account for variability in pairwise comparisons generated by a\nheterogeneous population of noisy and inconsistent users. M4 models individual\npreferences as a user-specific probabilistic mixture of shared latent Mallows\ncomponents. Our key algorithmic insight for estimation is to establish a\nstatistical connection between M4 and topic models by viewing pairwise\ncomparisons as words, and users as documents. This key insight leads us to\nexplore Mallows components with a separable structure and leverage recent\nadvances in separable topic discovery. While separability appears to be overly\nrestrictive, we nevertheless show that it is an inevitable outcome of a\nrelatively small number of latent Mallows components in a world of large number\nof items. We then develop an algorithm based on robust extreme-point\nidentification of convex polygons to learn the reference rankings, and is\nprovably consistent with polynomial sample complexity guarantees. We\ndemonstrate that our new model is empirically competitive with the current\nstate-of-the-art approaches in predicting real-world preferences.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 07:02:49 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1504.00781", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth\n  Selection in Univariate and Multivariate Kernel Density Estimations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives a novel Gram-Charlier A (GCA) Series based Extended\nRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation\n(KDE). There are existing various bandwidth selection rules achieving\nminimization of the Asymptotic Mean Integrated Square Error (AMISE) between the\nestimated probability density function (PDF) and the actual PDF. The rules\ndiffer in a way to estimate the integration of the squared second order\nderivative of an unknown PDF $(f(\\cdot))$, identified as the roughness\n$R(f''(\\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\\cdot))$\nwith an assumption that the density being estimated is Gaussian. Intuitively,\nbetter estimation of $R(f''(\\cdot))$ and consequently better bandwidth\nselection rules can be derived, if the unknown PDF is approximated through an\ninfinite series expansion based on a more generalized density assumption. As a\ndemonstration and verification to this concept, the ExROT derived in the\narticle uses an extended assumption that the density being estimated is near\nGaussian. This helps use of the GCA expansion as an approximation to the\nunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for\nmultivariate KDE. The required multivariate AMISE criteria is re-derived using\nelementary calculus of several variables, instead of Tensor calculus. The\nderivation uses the Kronecker product and the vector differential operator to\nachieve the AMISE expression in vector notations. There is also derived ExROT\nfor kernel based density derivative estimator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:42:44 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.00854", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Evaluation Evaluation a Monte Carlo study", "comments": "5 pages, 14 Equations, 2 Figures, 1 Table, as submitted to European\n  Conference on Artificial Intelligence (shorter version published with 2\n  pages, 4 Equations, 0 Figures, 1 Table)", "journal-ref": "ECAI 2008, pp.843-844", "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade there has been increasing concern about the biases\nembodied in traditional evaluation methods for Natural Language\nProcessing/Learning, particularly methods borrowed from Information Retrieval.\nWithout knowledge of the Bias and Prevalence of the contingency being tested,\nor equivalently the expectation due to chance, the simple conditional\nprobabilities Recall, Precision and Accuracy are not meaningful as evaluation\nmeasures, either individually or in combinations such as F-factor. The\nexistence of bias in NLP measures leads to the 'improvement' of systems by\nincreasing their bias, such as the practice of improving tagging and parsing\nscores by using most common value (e.g. water is always a Noun) rather than the\nattempting to discover the correct one. The measures Cohen Kappa and Powers\nInformedness are discussed as unbiased alternative to Recall and related to the\npsychologically significant measure DeltaP. In this paper we will analyze both\nbiased and unbiased measures theoretically, characterizing the precise\nrelationship between all these measures as well as evaluating the evaluation\nmeasures themselves empirically using a Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 14:46:29 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1504.00923", "submitter": "Fred Richardson", "authors": "Fred Richardson, Douglas Reynolds, Najim Dehak", "title": "A Unified Deep Neural Network for Speaker and Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned feature representations and sub-phoneme posteriors from Deep Neural\nNetworks (DNNs) have been used separately to produce significant performance\ngains for speaker and language recognition tasks. In this work we show how\nthese gains are possible using a single DNN for both speaker and language\nrecognition. The unified DNN approach is shown to yield substantial performance\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\ncondition).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:57:06 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Richardson", "Fred", ""], ["Reynolds", "Douglas", ""], ["Dehak", "Najim", ""]]}, {"id": "1504.01044", "submitter": "Heng Wang", "authors": "Heng Wang and Zubin Abraham", "title": "Concept Drift Detection for Streaming Data", "comments": "9 pages, accepted in the International Joint Conference of Neural\n  Networks 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical prediction models often require and assume stationarity in\nthe data. However, in many practical applications, changes in the relationship\nof the response and predictor variables are regularly observed over time,\nresulting in the deterioration of the predictive performance of these models.\nThis paper presents Linear Four Rates (LFR), a framework for detecting these\nconcept drifts and subsequently identifying the data points that belong to the\nnew concept (for relearning the model). Unlike conventional concept drift\ndetection approaches, LFR can be applied to both batch and stream data; is not\nlimited by the distribution properties of the response variable (e.g., datasets\nwith imbalanced labels); is independent of the underlying statistical-model;\nand uses user-specified parameters that are intuitively comprehensible. The\nperformance of LFR is compared to benchmark approaches using both simulated and\ncommonly used public datasets that span the gamut of concept drift types. The\nresults show LFR significantly outperforms benchmark approaches in terms of\nrecall, accuracy and delay in detection of concept drifts across datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 19:55:35 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 22:11:21 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wang", "Heng", ""], ["Abraham", "Zubin", ""]]}, {"id": "1504.01046", "submitter": "Yining Wang", "authors": "Yining Wang, Yu-Xiang Wang and Aarti Singh", "title": "Graph Connectivity in Noisy Sparse Subspace Clustering", "comments": "14 pages. To appear in The 19th International Conference on\n  Artificial Intelligence and Statistics, held at Cadiz, Spain in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the problem of clustering data points into a union of\nlow-dimensional linear/affine subspaces. It is the mathematical abstraction of\nmany important problems in computer vision, image processing and machine\nlearning. A line of recent work (4, 19, 24, 20) provided strong theoretical\nguarantee for sparse subspace clustering (4), the state-of-the-art algorithm\nfor subspace clustering, on both noiseless and noisy data sets. It was shown\nthat under mild conditions, with high probability no two points from different\nsubspaces are clustered together. Such guarantee, however, is not sufficient\nfor the clustering to be correct, due to the notorious \"graph connectivity\nproblem\" (15). In this paper, we investigate the graph connectivity problem for\nnoisy sparse subspace clustering and show that a simple post-processing\nprocedure is capable of delivering consistent clustering under certain \"general\nposition\" or \"restricted eigenvalue\" assumptions. We also show that our\ncondition is almost tight with adversarial noise perturbation by constructing a\ncounter-example. These results provide the first exact clustering guarantee of\nnoisy SSC for subspaces of dimension greater then 3.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:05:17 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 15:30:48 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Yu-Xiang", ""], ["Singh", "Aarti", ""]]}, {"id": "1504.01070", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu", "title": "Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via\n  Eigenvector and Semidefinite Programming Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of establishing a statistical ranking of a\nset of n items given a set of inconsistent and incomplete pairwise comparisons\nbetween such items. Instantiations of this problem occur in numerous\napplications in data analysis (e.g., ranking teams in sports data), computer\nvision, and machine learning. We formulate the above problem of ranking with\nincomplete noisy information as an instance of the group synchronization\nproblem over the group SO(2) of planar rotations, whose usefulness has been\ndemonstrated in numerous applications in recent years. Its least squares\nsolution can be approximated by either a spectral or a semidefinite programming\n(SDP) relaxation, followed by a rounding procedure. We perform extensive\nnumerical simulations on both synthetic and real-world data sets, showing that\nour proposed method compares favorably to other algorithms from the recent\nliterature. Existing theoretical guarantees on the group synchronization\nproblem imply lower bounds on the largest amount of noise permissible in the\nranking data while still achieving exact recovery. We propose a similar\nsynchronization-based algorithm for the rank-aggregation problem, which\nintegrates in a globally consistent ranking pairwise comparisons given by\ndifferent rating systems on the same set of items. We also discuss the problem\nof semi-supervised ranking when there is available information on the ground\ntruth rank of a subset of players, and propose an algorithm based on SDP which\nrecovers the ranks of the remaining players. Finally, synchronization-based\nranking, combined with a spectral technique for the densest subgraph problem,\nallows one to extract locally-consistent partial rankings, in other words, to\nidentify the rank of a small subset of players whose pairwise comparisons are\nless noisy than the rest of the data, which other methods are not able to\nidentify.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 01:40:35 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Cucuringu", "Mihai", ""]]}, {"id": "1504.01132", "submitter": "Susan Athey", "authors": "Susan Athey and Guido Imbens", "title": "Recursive Partitioning for Heterogeneous Causal Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problems of estimating heterogeneity in causal\neffects in experimental or observational studies and conducting inference about\nthe magnitude of the differences in treatment effects across subsets of the\npopulation. In applications, our method provides a data-driven approach to\ndetermine which subpopulations have large or small treatment effects and to\ntest hypotheses about the differences in these effects. For experiments, our\nmethod allows researchers to identify heterogeneity in treatment effects that\nwas not specified in a pre-analysis plan, without concern about invalidating\ninference due to multiple testing. In most of the literature on supervised\nmachine learning (e.g. regression trees, random forests, LASSO, etc.), the goal\nis to build a model of the relationship between a unit's attributes and an\nobserved outcome. A prominent role in these methods is played by\ncross-validation which compares predictions to actual outcomes in test samples,\nin order to select the level of complexity of the model that provides the best\npredictive power. Our method is closely related, but it differs in that it is\ntailored for predicting causal effects of a treatment rather than a unit's\noutcome. The challenge is that the \"ground truth\" for a causal effect is not\nobserved for any individual unit: we observe the unit with the treatment, or\nwithout the treatment, but not both at the same time. Thus, it is not obvious\nhow to use cross-validation to determine whether a causal effect has been\naccurately predicted. We propose several novel cross-validation criteria for\nthis problem and demonstrate through simulations the conditions under which\nthey perform better than standard methods for the problem of causal effects. We\nthen apply the method to a large-scale field experiment re-ranking results on a\nsearch engine.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 16:01:44 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 18:24:56 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2015 18:01:20 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""]]}, {"id": "1504.01169", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker, Shannon M. Hughes", "title": "Efficient Dictionary Learning via Very Sparse Random Projections", "comments": "5 pages, 2 figures, accepted in Sampling Theory and Applications\n  (SampTA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing signal processing tasks on compressive measurements of data has\nreceived great attention in recent years. In this paper, we extend previous\nwork on compressive dictionary learning by showing that more general random\nprojections may be used, including sparse ones. More precisely, we examine\ncompressive K-means clustering as a special case of compressive dictionary\nlearning and give theoretical guarantees for its performance for a very general\nclass of random projections. We then propose a memory and computation efficient\ndictionary learning algorithm, specifically designed for analyzing large\nvolumes of high-dimensional data, which learns the dictionary from very sparse\nrandom projections. Experimental results demonstrate that our approach allows\nfor reduction of computational complexity and memory/data access, with\ncontrollable loss in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 23:20:47 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""], ["Hughes", "Shannon M.", ""]]}, {"id": "1504.01255", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization\n  via Region Embedding", "comments": "v1 has a different title, and the results there are obsolete. The\n  current version is to appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new semi-supervised framework with convolutional neural\nnetworks (CNNs) for text categorization. Unlike the previous approaches that\nrely on word embeddings, our method learns embeddings of small text regions\nfrom unlabeled data for integration into a supervised CNN. The proposed scheme\nfor embedding learning is based on the idea of two-view semi-supervised\nlearning, which is intended to be useful for the task of interest even though\nthe training is done on unlabeled data. Our models achieve better results than\nprevious approaches on sentiment classification and topic classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 10:42:07 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 11:32:44 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2015 15:26:16 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1504.01294", "submitter": "Tsvetan Asamov", "authors": "Tsvetan Asamov and Adi Ben-Israel", "title": "A Probabilistic $\\ell_1$ Method for Clustering High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, the clustering problem is NP-hard, and global optimality cannot\nbe established for non-trivial instances. For high-dimensional data,\ndistance-based methods for clustering or classification face an additional\ndifficulty, the unreliability of distances in very high-dimensional spaces. We\npropose a distance-based iterative method for clustering data in very\nhigh-dimensional space, using the $\\ell_1$-metric that is less sensitive to\nhigh dimensionality than the Euclidean distance. For $K$ clusters in\n$\\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by\nprobabilities, and an iteration reduces to finding $Kn$ weighted medians of\npoints on a line. The complexity of the algorithm is linear in the dimension of\nthe data space, and its performance was observed to improve significantly as\nthe dimension increases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 14:49:13 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 21:58:42 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Asamov", "Tsvetan", ""], ["Ben-Israel", "Adi", ""]]}, {"id": "1504.01344", "submitter": "David Duvenaud", "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams", "title": "Early Stopping is Nonparametric Variational Inference", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that unconverged stochastic gradient descent can be interpreted as a\nprocedure that samples from a nonparametric variational approximate posterior\ndistribution. This distribution is implicitly defined as the transformation of\nan initial distribution by a sequence of optimization updates. By tracking the\nchange in entropy over this sequence of transformations during optimization, we\nform a scalable, unbiased estimate of the variational lower bound on the log\nmarginal likelihood. We can use this bound to optimize hyperparameters instead\nof using cross-validation. This Bayesian interpretation of SGD suggests\nimproved, overfitting-resistant optimization procedures, and gives a\ntheoretical foundation for popular tricks such as early stopping and\nensembling. We investigate the properties of this marginal likelihood estimator\non neural network models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 18:19:45 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Duvenaud", "David", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1504.01362", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano", "title": "A New Approach to Building the Interindustry Input--Output Table", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to estimating the interdependence of industries in\nan economy by applying data science solutions. By exploiting interfirm\nbuyer--seller network data, we show that the problem of estimating the\ninterdependence of industries is similar to the problem of uncovering the\nlatent block structure in network science literature. To estimate the\nunderlying structure with greater accuracy, we propose an extension of the\nsparse block model that incorporates node textual information and an unbounded\nnumber of industries and interactions among them. The latter task is\naccomplished by extending the well-known Chinese restaurant process to two\ndimensions. Inference is based on collapsed Gibbs sampling, and the model is\nevaluated on both synthetic and real-world datasets. We show that the proposed\nmodel improves in predictive accuracy and successfully provides a satisfactory\nsolution to the motivated problem. We also discuss issues that affect the\nfuture performance of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 19:18:49 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 13:18:26 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 02:47:01 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2015 15:14:57 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2016 14:28:59 GMT"}, {"version": "v6", "created": "Thu, 11 Feb 2016 08:21:35 GMT"}, {"version": "v7", "created": "Sun, 29 May 2016 14:07:02 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Hisano", "Ryohei", ""]]}, {"id": "1504.01369", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Changho Suh, Andrea J. Goldsmith", "title": "Information Recovery from Pairwise Measurements", "comments": "This work has been presented in part in ISIT 2014\n  (http://arxiv.org/abs/1404.7105) and ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with jointly recovering $n$ node-variables $\\left\\{\nx_{i}\\right\\}_{1\\leq i\\leq n}$ from a collection of pairwise difference\nmeasurements. Imagine we acquire a few observations taking the form of\n$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph\n$\\mathcal{G}$ with an edge set $\\mathcal{E}$ such that $x_{i}-x_{j}$ is\nobserved if and only if $(i,j)\\in\\mathcal{E}$. To account for noisy\nmeasurements in a general manner, we model the data acquisition process by a\nset of channels with given input/output transition measures. Employing\ninformation-theoretic tools applied to channel decoding problems, we develop a\n\\emph{unified} framework to characterize the fundamental recovery criterion,\nwhich accommodates general graph structures, alphabet sizes, and channel\ntransition measures. In particular, our results isolate a family of\n\\emph{minimum} \\emph{channel divergence measures} to characterize the degree of\nmeasurement corruption, which together with the size of the minimum cut of\n$\\mathcal{G}$ dictates the feasibility of exact information recovery. For\nvarious homogeneous graphs, the recovery condition depends almost only on the\nedge sparsity of the measurement graph irrespective of other graphical metrics;\nalternatively, the minimum sample complexity required for these graphs scales\nlike \\[ \\text{minimum sample complexity }\\asymp\\frac{n\\log\nn}{\\mathsf{Hel}_{1/2}^{\\min}} \\] for certain information metric\n$\\mathsf{Hel}_{1/2}^{\\min}$ defined in the main text, as long as the alphabet\nsize is not super-polynomial in $n$. We apply our general theory to three\nconcrete applications, including the stochastic block model, the outlier model,\nand the haplotype assembly problem. Our theory leads to order-wise tight\nrecovery conditions for all these scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 19:47:01 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 14:07:04 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2015 05:37:56 GMT"}, {"version": "v4", "created": "Fri, 6 May 2016 03:18:52 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Chen", "Yuxin", ""], ["Suh", "Changho", ""], ["Goldsmith", "Andrea J.", ""]]}, {"id": "1504.01482", "submitter": "William Chan", "authors": "William Chan, Ian Lane", "title": "Deep Recurrent Neural Networks for Acoustic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:12:14 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01483", "submitter": "William Chan", "authors": "William Chan and Nan Rosemary Ke and Ian Lane", "title": "Transferring Knowledge from a RNN to a DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:15:44 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Ke", "Nan Rosemary", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01492", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "Efficient SDP Inference for Fully-connected CRFs Based on Low-rank\n  Decomposition", "comments": "15 pages. A conference version of this work appears in Proc. IEEE\n  Conference on Computer Vision and Pattern Recognition, 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298942", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRF) have been widely used in a variety of\ncomputer vision tasks. Conventional CRFs typically define edges on neighboring\nimage pixels, resulting in a sparse graph such that efficient inference can be\nperformed. However, these CRFs fail to model long-range contextual\nrelationships. Fully-connected CRFs have thus been proposed. While there are\nefficient approximate inference methods for such CRFs, usually they are\nsensitive to initialization and make strong assumptions. In this work, we\ndevelop an efficient, yet general algorithm for inference on fully-connected\nCRFs. The algorithm is based on a scalable SDP algorithm and the low- rank\napproximation of the similarity/kernel matrix. The core of the proposed\nalgorithm is a tailored quasi-Newton method that takes advantage of the\nlow-rank matrix approximation when solving the specialized SDP dual problem.\nExperiments demonstrate that our method can be applied on fully-connected CRFs\nthat cannot be solved previously, such as pixel-level image co-segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:43:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1504.01515", "submitter": "Paris Giampouras", "authors": "Paris Giampouras, Konstantinos Themelis, Athanasios Rontogiannis and\n  Konstantinos Koutroumbas", "title": "Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TGRS.2016.2551327", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 08:23:45 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 16:53:41 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Giampouras", "Paris", ""], ["Themelis", "Konstantinos", ""], ["Rontogiannis", "Athanasios", ""], ["Koutroumbas", "Konstantinos", ""]]}, {"id": "1504.01577", "submitter": "Nicolas Flammarion", "authors": "Nicolas Flammarion (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "From Averaging to Acceleration, There is Only a Step-size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that accelerated gradient descent, averaged gradient descent and the\nheavy-ball method for non-strongly-convex problems may be reformulated as\nconstant parameter second-order difference equation algorithms, where stability\nof the system is equivalent to convergence at rate O(1/n 2), where n is the\nnumber of iterations. We provide a detailed analysis of the eigenvalues of the\ncorresponding linear dynamical system , showing various oscillatory and\nnon-oscillatory behaviors, together with a sharp stability result with explicit\nconstants. We also consider the situation where noisy gradients are available,\nwhere we extend our general convergence result, which suggests an alternative\nalgorithm (i.e., with different step sizes) that exhibits the good aspects of\nboth averaging and acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 12:29:59 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Flammarion", "Nicolas", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1504.01697", "submitter": "Alex Gittens", "authors": "Jiyan Yang and Alex Gittens", "title": "Tensor machines for learning target-specific polynomial features", "comments": "19 pages, 4 color figures, 2 tables. Submitted to ECML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have demonstrated that using random feature maps can\nsignificantly decrease the training and testing times of kernel-based\nalgorithms without significantly lowering their accuracy. Regrettably, because\nrandom features are target-agnostic, typically thousands of such features are\nnecessary to achieve acceptable accuracies. In this work, we consider the\nproblem of learning a small number of explicit polynomial features. Our\napproach, named Tensor Machines, finds a parsimonious set of features by\noptimizing over the hypothesis class introduced by Kar and Karnick for random\nfeature maps in a target-specific manner. Exploiting a natural connection\nbetween polynomials and tensors, we provide bounds on the generalization error\nof Tensor Machines. Empirically, Tensor Machines behave favorably on several\nreal-world datasets compared to other state-of-the-art techniques for learning\npolynomial features, and deliver significantly more parsimonious models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 18:21:37 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Yang", "Jiyan", ""], ["Gittens", "Alex", ""]]}, {"id": "1504.01823", "submitter": "Anru Zhang", "authors": "Tianxi Cai, T. Tony Cai, Anru Zhang", "title": "Structured Matrix Completion with Applications to Genomic Data\n  Integration", "comments": "Accepted for publication in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Matrix completion has attracted significant recent attention in many fields\nincluding statistics, applied mathematics and electrical engineering. Current\nliterature on matrix completion focuses primarily on independent sampling\nmodels under which the individual observed entries are sampled independently.\nMotivated by applications in genomic data integration, we propose a new\nframework of structured matrix completion (SMC) to treat structured missingness\nby design. Specifically, our proposed method aims at efficient matrix recovery\nwhen a subset of the rows and columns of an approximately low-rank matrix are\nobserved. We provide theoretical justification for the proposed SMC method and\nderive lower bound for the estimation errors, which together establish the\noptimal rate of recovery over certain classes of approximately low-rank\nmatrices. Simulation studies show that the method performs well in finite\nsample under a variety of configurations. The method is applied to integrate\nseveral ovarian cancer genomic studies with different extent of genomic\nmeasurements, which enables us to construct more accurate prediction rules for\novarian cancer survival.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 04:14:07 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Cai", "Tianxi", ""], ["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1504.02191", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "`local' vs. `global' parameters -- breaking the gaussian complexity\n  barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that if $F$ is a convex class of functions that is $L$-subgaussian,\nthe error rate of learning problems generated by independent noise is\nequivalent to a fixed point determined by `local' covering estimates of the\nclass, rather than by the gaussian averages. To that end, we establish new\nsharp upper and lower estimates on the error rate for such problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 04:54:29 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1504.02247", "submitter": "Alexey Melnikov", "authors": "Alexey A. Melnikov, Adi Makmal, Vedran Dunjko and Hans J. Briegel", "title": "Projective simulation with generalization", "comments": "14 pages, 9 figures", "journal-ref": "Sci. Rep. 7, 14430 (2017)", "doi": "10.1038/s41598-017-14740-y", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generalize is an important feature of any intelligent agent.\nNot only because it may allow the agent to cope with large amounts of data, but\nalso because in some environments, an agent with no generalization capabilities\ncannot learn. In this work we outline several criteria for generalization, and\npresent a dynamic and autonomous machinery that enables projective simulation\nagents to meaningfully generalize. Projective simulation, a novel, physical\napproach to artificial intelligence, was recently shown to perform well in\nstandard reinforcement learning problems, with applications in advanced\nrobotics as well as quantum experiments. Both the basic projective simulation\nmodel and the presented generalization machinery are based on very simple\nprinciples. This allows us to provide a full analytical analysis of the agent's\nperformance and to illustrate the benefit the agent gains by generalizing.\nSpecifically, we show that already in basic (but extreme) environments,\nlearning without generalization may be impossible, and demonstrate how the\npresented generalization machinery enables the projective simulation agent to\nlearn.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 10:37:11 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:18:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Melnikov", "Alexey A.", ""], ["Makmal", "Adi", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1504.02338", "submitter": "Devis Tuia", "authors": "Devis Tuia and Gustau Camps-Valls", "title": "Kernel Manifold Alignment", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0148655", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a kernel method for manifold alignment (KEMA) and domain\nadaptation that can match an arbitrary number of data sources without needing\ncorresponding pairs, just few labeled examples in all domains. KEMA has\ninteresting properties: 1) it generalizes other manifold alignment methods, 2)\nit can align manifolds of very different complexities, performing a sort of\nmanifold unfolding plus alignment, 3) it can define a domain-specific metric to\ncope with multimodal specificities, 4) it can align data spaces of different\ndimensionality, 5) it is robust to strong nonlinear feature deformations, and\n6) it is closed-form invertible which allows transfer across-domains and data\nsynthesis. We also present a reduced-rank version for computational efficiency\nand discuss the generalization performance of KEMA under Rademacher principles\nof stability. KEMA exhibits very good performance over competing methods in\nsynthetic examples, visual object recognition and recognition of facial\nexpressions tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 14:51:49 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 08:04:51 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 20:58:53 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Tuia", "Devis", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1504.02382", "submitter": "Shahab Basiri", "authors": "Shahab Basiri, Esa Ollila and Visa Koivunen", "title": "Robust, scalable and fast bootstrap method for analyzing large scale\n  data", "comments": "This paper is submitted for publication in IEEE Transactions On\n  Signal Processing, 8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2498121", "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of performing statistical inference for\nlarge scale data sets i.e., Big Data. The volume and dimensionality of the data\nmay be so high that it cannot be processed or stored in a single computing\nnode. We propose a scalable, statistically robust and computationally efficient\nbootstrap method, compatible with distributed processing and storage systems.\nBootstrap resamples are constructed with smaller number of distinct data points\non multiple disjoint subsets of data, similarly to the bag of little bootstrap\nmethod (BLB) [1]. Then significant savings in computation is achieved by\navoiding the re-computation of the estimator for each bootstrap sample.\nInstead, a computationally efficient fixed-point estimation equation is\nanalytically solved via a smart approximation following the Fast and Robust\nBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use\nof highly robust statistical methods in analyzing large scale data sets. The\nfavorable statistical properties of the method are established analytically.\nNumerical examples demonstrate scalability, low complexity and robust\nstatistical performance of the method in analyzing large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:48:28 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 20:01:28 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Basiri", "Shahab", ""], ["Ollila", "Esa", ""], ["Koivunen", "Visa", ""]]}, {"id": "1504.02406", "submitter": "Maja", "authors": "Maja Temerinac-Ott and Armaghan W. Naik and Robert F. Murphy", "title": "Deciding when to stop: Efficient stopping of active learning guided\n  drug-target prediction", "comments": "This paper was selected for oral presentation at RECOMB 2015 and an\n  abstract is published in the conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has shown to reduce the number of experiments needed to\nobtain high-confidence drug-target predictions. However, in order to actually\nsave experiments using active learning, it is crucial to have a method to\nevaluate the quality of the current prediction and decide when to stop the\nexperimentation process. Only by applying reliable stoping criteria to active\nlearning, time and costs in the experimental process can be actually saved. We\ncompute active learning traces on simulated drug-target matrices in order to\nlearn a regression model for the accuracy of the active learner. By analyzing\nthe performance of the regression model on simulated data, we design stopping\ncriteria for previously unseen experimental matrices. We demonstrate on four\npreviously characterized drug effect data sets that applying the stopping\ncriteria can result in upto 40% savings of the total experiments for highly\naccurate predictions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 18:10:38 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Temerinac-Ott", "Maja", ""], ["Naik", "Armaghan W.", ""], ["Murphy", "Robert F.", ""]]}, {"id": "1504.02412", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Alfred O. Hero III", "title": "Phase Transitions in Spectral Community Detection of Large Noisy\n  Networks", "comments": "conference paper at IEEE ICASSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the sensitivity of the spectral clustering based\ncommunity detection algorithm subject to a Erdos-Renyi type random noise model.\nWe prove phase transitions in community detectability as a function of the\nexternal edge connection probability and the noisy edge presence probability\nunder a general network model where two arbitrarily connected communities are\ninterconnected by random external edges. Specifically, the community detection\nperformance transitions from almost perfect detectability to low detectability\nas the inter-community edge connection probability exceeds some critical value.\nWe derive upper and lower bounds on the critical value and show that the bounds\nare identical when the two communities have the same size. The phase transition\nresults are validated using network simulations. Using the derived expressions\nfor the phase transition threshold we propose a method for estimating this\nthreshold from observed data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 18:23:23 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 20:37:42 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1504.02462", "submitter": "Suresh Venkatasubramanian", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "A Group Theoretic Perspective on Unsupervised Deep Learning", "comments": "2-page version of arXiv:1412.6621 prepared for presentation at ICLR\n  2015 workshop as required by ICLR PC). This version has some minor formatting\n  changes as required by the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called {\\em pretraining}: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:39:05 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 22:03:36 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 06:05:52 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1504.02712", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "Gradient of Probability Density Functions based Contrasts for Blind\n  Source Separation (BSS)", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives some novel independence measures and contrast functions\nfor Blind Source Separation (BSS) application. For the $k^{th}$ order\ndifferentiable multivariate functions with equal hyper-volumes (region bounded\nby hyper-surfaces) and with a constraint of bounded support for $k>1$, it\nproves that equality of any $k^{th}$ order derivatives implies equality of the\nfunctions. The difference between product of marginal Probability Density\nFunctions (PDFs) and joint PDF of a random vector is defined as Function\nDifference (FD) of a random vector. Assuming the PDFs are $k^{th}$ order\ndifferentiable, the results on generalized functions are applied to the\nindependence condition. This brings new sets of independence measures and BSS\ncontrasts based on the $L^p$-Norm, $ p \\geq 1$ of - FD, gradient of FD (GFD)\nand Hessian of FD (HFD). Instead of a conventional two stage indirect\nestimation method for joint PDF based BSS contrast estimation, a single stage\ndirect estimation of the contrasts is desired. The article targets both the\nefficient estimation of the proposed contrasts and extension of the potential\ntheory for an information field. The potential theory has a concept of\nreference potential and it is used to derive closed form expression for the\nrelative analysis of potential field. Analogous to it, there are introduced\nconcepts of Reference Information Potential (RIP) and Cross Reference\nInformation Potential (CRIP) based on the potential due to kernel functions\nplaced at selected sample points as basis in kernel methods. The quantities are\nused to derive closed form expressions for information field analysis using\nleast squares. The expressions are used to estimate $L^2$-Norm of FD and\n$L^2$-Norm of GFD based contrasts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 15:28:37 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.02719", "submitter": "Jian Peng", "authors": "Hyunghoon Cho, Bonnie Berger and Jian Peng", "title": "Diffusion Component Analysis: Unraveling Functional Topology in\n  Biological Networks", "comments": "RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex biological systems have been successfully modeled by biochemical and\ngenetic interaction networks, typically gathered from high-throughput (HTP)\ndata. These networks can be used to infer functional relationships between\ngenes or proteins. Using the intuition that the topological role of a gene in a\nnetwork relates to its biological function, local or diffusion based\n\"guilt-by-association\" and graph-theoretic methods have had success in\ninferring gene functions. Here we seek to improve function prediction by\nintegrating diffusion-based methods with a novel dimensionality reduction\ntechnique to overcome the incomplete and noisy nature of network data. In this\npaper, we introduce diffusion component analysis (DCA), a framework that plugs\nin a diffusion model and learns a low-dimensional vector representation of each\nnode to encode the topological properties of a network. As a proof of concept,\nwe demonstrate DCA's substantial improvement over state-of-the-art\ndiffusion-based approaches in predicting protein function from molecular\ninteraction networks. Moreover, our DCA framework can integrate multiple\nnetworks from heterogeneous sources, consisting of genomic information,\nbiochemical experiments and other resources, to even further improve function\nprediction. Yet another layer of performance gain is achieved by integrating\nthe DCA framework with support vector machines that take our node vector\nrepresentations as features. Overall, our DCA framework provides a novel\nrepresentation of nodes in a network that can be used as a plug-in architecture\nto other machine learning algorithms to decipher topological properties of and\nobtain novel insights into interactomes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 15:42:11 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Cho", "Hyunghoon", ""], ["Berger", "Bonnie", ""], ["Peng", "Jian", ""]]}, {"id": "1504.02723", "submitter": "Lo\\\"ic Schwaller", "authors": "Lo\\\"ic Schwaller, St\\'ephane Robin, Michael Stumpf", "title": "A closed-form approach to Bayesian inference in tree-structured\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inference of the structure of an undirected graphical model\nin an exact Bayesian framework. More specifically we aim at achieving the\ninference with close-form posteriors, avoiding any sampling step. This task\nwould be intractable without any restriction on the considered graphs, so we\nlimit our exploration to mixtures of spanning trees. We consider the inference\nof the structure of an undirected graphical model in a Bayesian framework. To\navoid convergence issues and highly demanding Monte Carlo sampling, we focus on\nexact inference. More specifically we aim at achieving the inference with\nclose-form posteriors, avoiding any sampling step. To this aim, we restrict the\nset of considered graphs to mixtures of spanning trees. We investigate under\nwhich conditions on the priors - on both tree structures and parameters - exact\nBayesian inference can be achieved. Under these conditions, we derive a fast an\nexact algorithm to compute the posterior probability for an edge to belong to\n{the tree model} using an algebraic result called the Matrix-Tree theorem. We\nshow that the assumption we have made does not prevent our approach to perform\nwell on synthetic and flow cytometry data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 16:01:15 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 09:07:38 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2015 09:54:30 GMT"}, {"version": "v4", "created": "Mon, 1 May 2017 09:45:32 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Schwaller", "Lo\u00efc", ""], ["Robin", "St\u00e9phane", ""], ["Stumpf", "Michael", ""]]}, {"id": "1504.02800", "submitter": "Nicole Croteau", "authors": "Nicole Croteau, Farouk S. Nathoo, Jiguo Cao, Ryan Budney", "title": "High-Dimensional Classification for Brain Decoding", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding involves the determination of a subject's cognitive state or\nan associated stimulus from functional neuroimaging data measuring brain\nactivity. In this setting the cognitive state is typically characterized by an\nelement of a finite set, and the neuroimaging data comprise voluminous amounts\nof spatiotemporal data measuring some aspect of the neural signal. The\nassociated statistical problem is one of classification from high-dimensional\ndata. We explore the use of functional principal component analysis, mutual\ninformation networks, and persistent homology for examining the data through\nexploratory analysis and for constructing features characterizing the neural\nsignal for brain decoding. We review each approach from this perspective, and\nwe incorporate the features into a classifier based on symmetric multinomial\nlogistic regression with elastic net regularization. The approaches are\nillustrated in an application where the task is to infer, from brain activity\nmeasured with magnetoencephalography (MEG), the type of video stimulus shown to\na subject.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 21:54:48 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Croteau", "Nicole", ""], ["Nathoo", "Farouk S.", ""], ["Cao", "Jiguo", ""], ["Budney", "Ryan", ""]]}, {"id": "1504.02813", "submitter": "Camila Pedroso Estevam de Souza Dr.", "authors": "Camila P. E. de Souza, Nancy E. Heckman and Helena Xu", "title": "Switching nonparametric regression models for multi-curve data", "comments": "24 pages, 4 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and apply an approach for analyzing multi-curve data where each\ncurve is driven by a latent state process. The state at any particular point\ndetermines a smooth function, forcing the individual curve to switch from one\nfunction to another. Thus each curve follows what we call a switching\nnonparametric regression model. We develop an EM algorithm to estimate the\nmodel parameters. We also obtain standard errors for the parameter estimates of\nthe state process. We consider several types of state processes: independent\nand identically distributed, independent but depending on a covariate and\nMarkov. Simulation studies show the frequentist properties of our estimates. We\napply our methods to a data set of a building's power usage.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 23:04:27 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 21:36:30 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 16:42:09 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["de Souza", "Camila P. E.", ""], ["Heckman", "Nancy E.", ""], ["Xu", "Helena", ""]]}, {"id": "1504.02870", "submitter": "Ichiro Takeuchi Prof.", "authors": "Shota Okumura and Yoshiki Suzuki and Ichiro Takeuchi", "title": "Quick sensitivity analysis for incremental data modification and its\n  application to leave-one-out CV in linear classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel sensitivity analysis framework for large scale\nclassification problems that can be used when a small number of instances are\nincrementally added or removed. For quickly updating the classifier in such a\nsituation, incremental learning algorithms have been intensively studied in the\nliterature. Although they are much more efficient than solving the optimization\nproblem from scratch, their computational complexity yet depends on the entire\ntraining set size. It means that, if the original training set is large,\ncompletely solving an incremental learning problem might be still rather\nexpensive. To circumvent this computational issue, we propose a novel framework\nthat allows us to make an inference about the updated classifier without\nactually re-optimizing it. Specifically, the proposed framework can quickly\nprovide a lower and an upper bounds of a quantity on the unknown updated\nclassifier. The main advantage of the proposed framework is that the\ncomputational cost of computing these bounds depends only on the number of\nupdated instances. This property is quite advantageous in a typical sensitivity\nanalysis task where only a small number of instances are updated. In this paper\nwe demonstrate that the proposed framework is applicable to various practical\nsensitivity analysis tasks, and the bounds provided by the framework are often\nsufficiently tight for making desired inferences.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 13:25:37 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Okumura", "Shota", ""], ["Suzuki", "Yoshiki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1504.02931", "submitter": "Badong Chen", "authors": "Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, Jos\\'e C.\n  Pr\\'incipe", "title": "Generalized Correntropy for Robust Adaptive Filtering", "comments": "34 pages, 9 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": "IEEE Trans. on Signal Processing, vol. 64, no. 13, pp. 3376-3387,\n  2016", "doi": "10.1109/TSP.2016.2539127", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a robust nonlinear similarity measure in kernel space, correntropy has\nreceived increasing attention in domains of machine learning and signal\nprocessing. In particular, the maximum correntropy criterion (MCC) has recently\nbeen successfully applied in robust regression and filtering. The default\nkernel function in correntropy is the Gaussian kernel, which is, of course, not\nalways the best choice. In this work, we propose a generalized correntropy that\nadopts the generalized Gaussian density (GGD) function as the kernel (not\nnecessarily a Mercer kernel), and present some important properties. We further\npropose the generalized maximum correntropy criterion (GMCC), and apply it to\nadaptive filtering. An adaptive algorithm, called the GMCC algorithm, is\nderived, and the mean square convergence performance is studied. We show that\nthe proposed algorithm is very stable and can achieve zero probability of\ndivergence (POD). Simulation results confirm the theoretical expectations and\ndemonstrate the desirable performance of the new algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 03:47:46 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Chen", "Badong", ""], ["Xing", "Lei", ""], ["Zhao", "Haiquan", ""], ["Zheng", "Nanning", ""], ["Pr\u00edncipe", "Jos\u00e9 C.", ""]]}, {"id": "1504.03156", "submitter": "Seyoung Yun", "authors": "Se-Young Yun and Marc Lelarge and Alexandre Proutiere", "title": "Streaming, Memory Limited Matrix Completion with Noise", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the streaming memory-limited matrix completion\nproblem when the observed entries are noisy versions of a small random fraction\nof the original entries. We are interested in scenarios where the matrix size\nis very large so the matrix is very hard to store and manipulate. Here, columns\nof the observed matrix are presented sequentially and the goal is to complete\nthe missing entries after one pass on the data with limited memory space and\nlimited computational complexity. We propose a streaming algorithm which\nproduces an estimate of the original matrix with a vanishing mean square error,\nuses memory space scaling linearly with the ambient dimension of the matrix,\ni.e. the memory required to store the output alone, and spends computations as\nmuch as the number of non-zero entries of the input matrix.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 12:52:31 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Yun", "Se-Young", ""], ["Lelarge", "Marc", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1504.03183", "submitter": "Barbara Engelhardt", "authors": "Gregory Darnell and Stoyan Georgiev and Sayan Mukherjee and Barbara E\n  Engelhardt", "title": "Adaptive Randomized Dimension Reduction on Massive Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1211.1642", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of statistical estimators is of increasing importance in\nmodern applications. One approach to implementing scalable algorithms is to\ncompress data into a low dimensional latent space using dimension reduction\nmethods. In this paper we develop an approach for dimension reduction that\nexploits the assumption of low rank structure in high dimensional data to gain\nboth computational and statistical advantages. We adapt recent randomized\nlow-rank approximation algorithms to provide an efficient solution to principal\ncomponent analysis (PCA), and we use this efficient solver to improve parameter\nestimation in large-scale linear mixed models (LMM) for association mapping in\nstatistical and quantitative genomics. A key observation in this paper is that\nrandomization serves a dual role, improving both computational and statistical\nperformance by implicitly regularizing the covariance matrix estimate of the\nrandom effect in a LMM. These statistical and computational advantages are\nhighlighted in our experiments on simulated data and large-scale genomic\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 13:52:17 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Darnell", "Gregory", ""], ["Georgiev", "Stoyan", ""], ["Mukherjee", "Sayan", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1504.03413", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Swastik Brahma, Pramod K. Varshney", "title": "Consensus based Detection in the Presence of Data Falsification Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/TSIPN.2016.2607119", "report-no": null, "categories": "cs.SY cs.DC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detection in distributed networks in the\npresence of data falsification (Byzantine) attacks. Detection approaches\nconsidered in the paper are based on fully distributed consensus algorithms,\nwhere all of the nodes exchange information only with their neighbors in the\nabsence of a fusion center. In such networks, we characterize the negative\neffect of Byzantines on the steady-state and transient detection performance of\nthe conventional consensus based detection algorithms. To address this issue,\nwe study the problem from the network designer's perspective. More\nspecifically, we first propose a distributed weighted average consensus\nalgorithm that is robust to Byzantine attacks. We show that, under reasonable\nassumptions, the global test statistic for detection can be computed locally at\neach node using our proposed consensus algorithm. We exploit the statistical\ndistribution of the nodes' data to devise techniques for mitigating the\ninfluence of data falsifying Byzantines on the distributed detection system.\nSince some parameters of the statistical distribution of the nodes' data might\nnot be known a priori, we propose learning based techniques to enable an\nadaptive design of the local fusion or update rules.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 03:43:05 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Brahma", "Swastik", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1504.03415", "submitter": "Darshana Wickramarachchi Mr", "authors": "D. C. Wickramarachchi, B. L. Robertson, M. Reale, C. J. Price and J.\n  Brown", "title": "HHCART: An Oblique Decision Tree", "comments": "13 Pages, 1 Figure, 4 Tables, 1 Algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are a popular technique in statistical data classification.\nThey recursively partition the feature space into disjoint sub-regions until\neach sub-region becomes homogeneous with respect to a particular class. The\nbasic Classification and Regression Tree (CART) algorithm partitions the\nfeature space using axis parallel splits. When the true decision boundaries are\nnot aligned with the feature axes, this approach can produce a complicated\nboundary structure. Oblique decision trees use oblique decision boundaries to\npotentially simplify the boundary structure. The major limitation of this\napproach is that the tree induction algorithm is computationally expensive. In\nthis article we present a new decision tree algorithm, called HHCART. The\nmethod utilizes a series of Householder matrices to reflect the training data\nat each node during the tree construction. Each reflection is based on the\ndirections of the eigenvectors from each classes' covariance matrix.\nConsidering axis parallel splits in the reflected training data provides an\nefficient way of finding oblique splits in the unreflected training data.\nExperimental results show that the accuracy and size of the HHCART trees are\ncomparable with some benchmark methods in the literature. The appealing feature\nof HHCART is that it can handle both qualitative and quantitative features in\nthe same oblique split.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 04:04:00 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Wickramarachchi", "D. C.", ""], ["Robertson", "B. L.", ""], ["Reale", "M.", ""], ["Price", "C. J.", ""], ["Brown", "J.", ""]]}, {"id": "1504.03509", "submitter": "Shuang Liu", "authors": "Shuang Liu, Cheng Chen and Zhihua Zhang", "title": "Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the distributed stochastic multi-armed bandit\nproblem, where a global arm set can be accessed by multiple players\nindependently. The players are allowed to exchange their history of\nobservations with each other at specific points in time. We study the\nrelationship between regret and communication. When the time horizon is known,\nwe propose the Over-Exploration strategy, which only requires one-round\ncommunication and whose regret does not scale with the number of players. When\nthe time horizon is unknown, we measure the frequency of communication through\na new notion called the density of the communication set, and give an exact\ncharacterization of the interplay between regret and communication.\nSpecifically, a lower bound is established and stable strategies that match the\nlower bound are developed. The results and analyses in this paper are specific\nbut can be translated into more general settings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 12:14:46 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 20:26:00 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Liu", "Shuang", ""], ["Chen", "Cheng", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1504.03701", "submitter": "Julia Vogt", "authors": "Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya\n  Prabhakaran, Volker Roth and Gunnar R\\\"atsch", "title": "Probabilistic Clustering of Time-Evolving Distance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel probabilistic clustering model for objects that are\nrepresented via pairwise distances and observed at different time points. The\nproposed method utilizes the information given by adjacent time points to find\nthe underlying cluster structure and obtain a smooth cluster evolution. This\napproach allows the number of objects and clusters to differ at every time\npoint, and no identification on the identities of the objects is needed.\nFurther, the model does not require the number of clusters being specified in\nadvance -- they are instead determined automatically using a Dirichlet process\nprior. We validate our model on synthetic data showing that the proposed method\nis more accurate than state-of-the-art clustering methods. Finally, we use our\ndynamic clustering model to analyze and illustrate the evolution of brain\ncancer patients over time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:05:45 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Vogt", "Julia E.", ""], ["Kloft", "Marius", ""], ["Stark", "Stefan", ""], ["Raman", "Sudhir S.", ""], ["Prabhakaran", "Sandhya", ""], ["Roth", "Volker", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1504.03991", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu", "title": "Theory of Dual-sparse Regularized Randomized Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study randomized reduction methods, which reduce\nhigh-dimensional features into low-dimensional space by randomized methods\n(e.g., random projection, random hashing), for large-scale high-dimensional\nclassification. Previous theoretical results on randomized reduction methods\nhinge on strong assumptions about the data, e.g., low rank of the data matrix\nor a large separable margin of classification, which hinder their applications\nin broad domains. To address these limitations, we propose dual-sparse\nregularized randomized reduction methods that introduce a sparse regularizer\ninto the reduced dual problem. Under a mild condition that the original dual\nsolution is a (nearly) sparse vector, we show that the resulting dual solution\nis close to the original dual solution and concentrates on its support set. In\nnumerical experiments, we present an empirical study to support the analysis\nand we also present a novel application of the dual-sparse regularized\nrandomized reduction methods to reducing the communication cost of distributed\nlearning from large-scale high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 19:16:54 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 13:03:39 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 21:44:02 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2015 21:16:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1504.04054", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Lawrence Carin", "title": "A Generative Model for Deep Convolutional Learning", "comments": "3 pages, 1 figure, ICLR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model is developed for deep (multi-layered) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\n(refinement) probabilistic learning. Experimental results demonstrate powerful\ncapabilities of the model to learn multi-layer features from images, and\nexcellent classification results are obtained on the MNIST and Caltech 101\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 21:31:58 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1504.04114", "submitter": "Nir Levin", "authors": "Nir Levine, Timothy A. Mann, Shie Mannor", "title": "Actively Learning to Attract Followers on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter, a popular social network, presents great opportunities for on-line\nmachine learning research. However, previous research has focused almost\nentirely on learning from passively collected data. We study the problem of\nlearning to acquire followers through normative user behavior, as opposed to\nthe mass following policies applied by many bots. We formalize the problem as a\ncontextual bandit problem, in which we consider retweeting content to be the\naction chosen and each tweet (content) is accompanied by context. We design\nreward signals based on the change in followers. The result of our month long\nexperiment with 60 agents suggests that (1) aggregating experience across\nagents can adversely impact prediction accuracy and (2) the Twitter community's\nresponse to different actions is non-stationary. Our findings suggest that\nactively learning on-line can provide deeper insights about how to attract\nfollowers than machine learning over passively collected data alone.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 07:26:11 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Levine", "Nir", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.04184", "submitter": "Esa Ollila", "authors": "Esa Ollila", "title": "Multichannel sparse recovery of complex-valued signals using Huber's\n  criterion", "comments": "To appear in CoSeRa'15 (Pisa, Italy, June 16-19, 2015). arXiv admin\n  note: text overlap with arXiv:1502.02441", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize Huber's criterion to multichannel sparse\nrecovery problem of complex-valued measurements where the objective is to find\ngood recovery of jointly sparse unknown signal vectors from the given multiple\nmeasurement vectors which are different linear combinations of the same known\nelementary vectors. This requires careful characterization of robust\ncomplex-valued loss functions as well as Huber's criterion function for the\nmultivariate sparse regression problem. We devise a greedy algorithm based on\nsimultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlike\nthe conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, is\nrobust under heavy-tailed non-Gaussian noise conditions, yet has a negligible\nperformance loss compared to SNIHT under Gaussian noise. Usefulness of the\nmethod is illustrated in source localization application with sensor arrays.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 11:25:40 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Ollila", "Esa", ""]]}, {"id": "1504.04343", "submitter": "Ce Zhang", "authors": "Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher R\\'e", "title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Caffe con Troll (CcT), a fully compatible end-to-end version of\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\nperformance characteristics of training and deploying general-purpose\nconvolutional neural networks across different hardware architectures. We find\nthat, by employing standard batching optimizations for CPU training, we achieve\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\nMoreover, with these improvements, the end-to-end training time for CNNs is\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\nefficiently train hybrid CPU-GPU systems for CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 19:11:08 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 20:12:33 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Hadjis", "Stefan", ""], ["Abuzaid", "Firas", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1504.04406", "submitter": "Mark Schmidt", "authors": "Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann\n  Clifton, Anoop Sarkar", "title": "Non-Uniform Stochastic Average Gradient Method for Training Conditional\n  Random Fields", "comments": "AI/Stats 2015, 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply stochastic average gradient (SAG) algorithms for training\nconditional random fields (CRFs). We describe a practical implementation that\nuses structure in the CRF gradient to reduce the memory requirement of this\nlinearly-convergent stochastic gradient method, propose a non-uniform sampling\nscheme that substantially improves practical performance, and analyze the rate\nof convergence of the SAGA variant under non-uniform sampling. Our experimental\nresults reveal that our method often significantly outperforms existing methods\nin terms of the training objective, and performs as well or better than\noptimally-tuned stochastic gradient methods in terms of test error.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 23:26:35 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Schmidt", "Mark", ""], ["Babanezhad", "Reza", ""], ["Ahmed", "Mohamed Osama", ""], ["Defazio", "Aaron", ""], ["Clifton", "Ann", ""], ["Sarkar", "Anoop", ""]]}, {"id": "1504.04407", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y, Jie Liu, Peter Richt\\'arik, Martin Tak\\'a\\v{c}", "title": "Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2015.2505682", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose mS2GD: a method incorporating a mini-batching scheme for improving\nthe theoretical complexity and practical performance of semi-stochastic\ngradient descent (S2GD). We consider the problem of minimizing a strongly\nconvex function represented as the sum of an average of a large number of\nsmooth convex functions, and a simple nonsmooth convex regularizer. Our method\nfirst performs a deterministic step (computation of the gradient of the\nobjective function at the starting point), followed by a large number of\nstochastic steps. The process is repeated a few times with the last iterate\nbecoming the new starting point. The novelty of our method is in introduction\nof mini-batching into the computation of stochastic steps. In each step,\ninstead of choosing a single function, we sample $b$ functions, compute their\ngradients, and compute the direction based on this. We analyze the complexity\nof the method and show that it benefits from two speedup effects. First, we\nprove that as long as $b$ is below a certain threshold, we can reach any\npredefined accuracy with less overall work than without mini-batching. Second,\nour mini-batching scheme admits a simple parallel implementation, and hence is\nsuitable for further acceleration by parallelization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 23:31:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 13:26:17 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Liu", "Jie", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1504.04599", "submitter": "Bhaswar Bhattacharya", "authors": "Bhaswar B. Bhattacharya and Gregory Valiant", "title": "Testing Closeness With Unequal Sized Samples", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of closeness testing for two discrete distributions\nin the practically relevant setting of \\emph{unequal} sized samples drawn from\neach of them. Specifically, given a target error parameter $\\varepsilon > 0$,\n$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from\nan unknown distribution $q$, we describe a test for distinguishing the case\nthat $p=q$ from the case that $||p-q||_1 \\geq \\varepsilon$. If $p$ and $q$ are\nsupported on at most $n$ elements, then our test is successful with high\nprobability provided $m_1\\geq n^{2/3}/\\varepsilon^{4/3}$ and $m_2 =\n\\Omega(\\max\\{\\frac{n}{\\sqrt m_1\\varepsilon^2}, \\frac{\\sqrt\nn}{\\varepsilon^2}\\});$ we show that this tradeoff is optimal throughout this\nrange, to constant factors. These results extend the recent work of Chan et al.\nwho established the sample complexity when the two samples have equal sizes,\nand tightens the results of Acharya et al. by polynomials factors in both $n$\nand $\\varepsilon$. As a consequence, we obtain an algorithm for estimating the\nmixing time of a Markov chain on $n$ states up to a $\\log n$ factor that uses\n$\\tilde{O}(n^{3/2} \\tau_{mix})$ queries to a \"next node\" oracle, improving upon\nthe $\\tilde{O}(n^{5/3}\\tau_{mix})$ query algorithm of Batu et al. Finally, we\nnote that the core of our testing algorithm is a relatively simple statistic\nthat seems to perform well in practice, both on synthetic data and on natural\nlanguage data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 18:35:35 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""], ["Valiant", "Gregory", ""]]}, {"id": "1504.04739", "submitter": "Wojciech Czarnecki", "authors": "Rafal Jozefowicz, Wojciech Marian Czarnecki", "title": "Fast optimization of Multithreshold Entropy Linear Classifier", "comments": "Presented at Theoretical Foundations of Machine Learning 2015\n  (http://tfml.gmum.net), final version published in Schedae Informaticae\n  Journal", "journal-ref": null, "doi": "10.4467/20838476SI.14.005.3022", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreshold Entropy Linear Classifier (MELC) is a density based model\nwhich searches for a linear projection maximizing the Cauchy-Schwarz Divergence\nof dataset kernel density estimation. Despite its good empirical results, one\nof its drawbacks is the optimization speed. In this paper we analyze how one\ncan speed it up through solving an approximate problem. We analyze two methods,\nboth similar to the approximate solutions of the Kernel Density Estimation\nquerying and provide adaptive schemes for selecting a crucial parameters based\non user-specified acceptable error. Furthermore we show how one can exploit\nwell known conjugate gradients and L-BFGS optimizers despite the fact that the\noriginal optimization problem should be solved on the sphere. All above methods\nand modifications are tested on 10 real life datasets from UCI repository to\nconfirm their practical usability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 16:19:22 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Jozefowicz", "Rafal", ""], ["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1504.04740", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki", "title": "On the consistency of Multithreshold Entropy Linear Classifier", "comments": "Presented at Theoretical Foundations of Machine Learning 2015\n  (http://tfml.gmum.net), final version published in Schedae Informaticae\n  Journal", "journal-ref": null, "doi": "10.4467/20838476SI.15.012.3034", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreshold Entropy Linear Classifier (MELC) is a recent classifier idea\nwhich employs information theoretic concept in order to create a multithreshold\nmaximum margin model. In this paper we analyze its consistency over\nmultithreshold linear models and show that its objective function upper bounds\nthe amount of misclassified points in a similar manner like hinge loss does in\nsupport vector machines. For further confirmation we also conduct some\nnumerical experiments on five datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 16:29:26 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1504.05006", "submitter": "Jack Kuipers", "authors": "Jack Kuipers and Giusi Moffa", "title": "Partition MCMC for inference on acyclic digraphs", "comments": "Revised version. 34 pages, 16 figures. R code available at\n  https://github.com/annlia/partitionMCMC", "journal-ref": "J. Am. Stat. Assoc. 112 (2017) 282-299", "doi": "10.1080/01621459.2015.1133426", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic digraphs are the underlying representation of Bayesian networks, a\nwidely used class of probabilistic graphical models. Learning the underlying\ngraph from data is a way of gaining insights about the structural properties of\na domain. Structure learning forms one of the inference challenges of\nstatistical graphical models.\n  MCMC methods, notably structure MCMC, to sample graphs from the posterior\ndistribution given the data are probably the only viable option for Bayesian\nmodel averaging. Score modularity and restrictions on the number of parents of\neach node allow the graphs to be grouped into larger collections, which can be\nscored as a whole to improve the chain's convergence. Current examples of\nalgorithms taking advantage of grouping are the biased order MCMC, which acts\non the alternative space of permuted triangular matrices, and non ergodic edge\nreversal moves.\n  Here we propose a novel algorithm, which employs the underlying combinatorial\nstructure of DAGs to define a new grouping. As a result convergence is improved\ncompared to structure MCMC, while still retaining the property of producing an\nunbiased sample. Finally the method can be combined with edge reversal moves to\nimprove the sampler further.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 10:47:27 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 12:13:24 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""]]}, {"id": "1504.05059", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Helmut B\\\"olcskei", "title": "Nonparametric Nearest Neighbor Random Process Clustering", "comments": "IEEE International Symposium on Information Theory (ISIT), June 2015,\n  to appear", "journal-ref": null, "doi": "10.1109/ISIT.2015.7282647", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their nonparametric generative\nmodels without prior knowledge of the model statistics and the number of\ngenerative models. Two algorithms, both using the L1-distance between estimated\npower spectral densities (PSDs) as a measure of dissimilarity, are analyzed.\nThe first algorithm, termed nearest neighbor process clustering (NNPC), to the\nbest of our knowledge, is new and relies on partitioning the nearest neighbor\ngraph of the observations via spectral clustering. The second algorithm, simply\nreferred to as k-means (KM), consists of a single k-means iteration with\nfarthest point initialization and was considered before in the literature,\nalbeit with a different measure of dissimilarity and with asymptotic\nperformance results only. We show that both NNPC and KM succeed with high\nprobability under noise and even when the generative process PSDs overlap\nsignificantly, all provided that the observation length is sufficiently large.\nOur results quantify the tradeoff between the overlap of the generative process\nPSDs, the noise variance, and the observation length. Finally, we present\nnumerical performance results for synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 13:48:45 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1504.05229", "submitter": "Yao Xie", "authors": "Yang Cao and Yao Xie", "title": "Poisson Matrix Recovery and Completion", "comments": "Submitted to IEEE Journal. Parts of the paper have appeared in\n  GlobalSIP 2013, GlobalSIP 2014, and ISIT 2015. arXiv admin note: substantial\n  text overlap with arXiv:1501.06243", "journal-ref": null, "doi": "10.1109/TSP.2015.2500192", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of low-rank matrix recovery and completion to the case\nwhen Poisson observations for a linear combination or a subset of the entries\nof a matrix are available, which arises in various applications with count\ndata. We consider the usual matrix recovery formulation through maximum\nlikelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,\nand establish theoretical upper and lower bounds on the recovery error. Our\nbounds for matrix completion are nearly optimal up to a factor on the order of\n$\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by combing techniques\nfor compressed sensing for sparse vectors with Poisson noise and for analyzing\nlow-rank matrices, as well as adapting the arguments used for one-bit matrix\ncompletion \\cite{davenport20121} (although these two problems are different in\nnature) and the adaptation requires new techniques exploiting properties of the\nPoisson likelihood function and tackling the difficulties posed by the locally\nsub-Gaussian characteristic of the Poisson distribution. Our results highlight\na few important distinctions of the Poisson case compared to the prior work\nincluding having to impose a minimum signal-to-noise requirement on each\nobserved entry and a gap in the upper and lower bounds. We also develop a set\nof efficient iterative algorithms and demonstrate their good performance on\nsynthetic examples and real data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 21:09:47 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 02:13:26 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1504.05287", "submitter": "Tengyu Ma", "authors": "Rong Ge, Tengyu Ma", "title": "Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor rank and low-rank tensor decompositions have many applications in\nlearning and complexity theory. Most known algorithms use unfoldings of tensors\nand can only handle rank up to $n^{\\lfloor p/2 \\rfloor}$ for a $p$-th order\ntensor in $\\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose\n3rd order tensors when the rank is super-linear in the dimension. Using ideas\nfrom sum-of-squares hierarchy, we give the first quasi-polynomial time\nalgorithm that can decompose a random 3rd order tensor decomposition when the\nrank is as large as $n^{3/2}/\\textrm{polylog} n$.\n  We also give a polynomial time algorithm for certifying the injective norm of\nrandom low rank tensors. Our tensor decomposition algorithm exploits the\nrelationship between injective norm and the tensor components. The proof relies\non interesting tools for decoupling random variables to prove better matrix\nconcentration bounds, which can be useful in other settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 03:21:53 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1504.05392", "submitter": "Stephen Bamattre", "authors": "Stephen Bamattre, Rex Hu and Joseph S. Verducci", "title": "Nonparametric Testing for Heterogeneous Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of weak overall correlation, it may be useful to investigate\nif the correlation is significantly and substantially more pronounced over a\nsubpopulation. Two different testing procedures are compared. Both are based on\nthe rankings of the values of two variables from a data set with a large number\nn of observations. The first maintains its level against Gaussian copulas; the\nsecond adapts to general alternatives in the sense that that the number of\nparameters used in the test grows with n. An analysis of wine quality\nillustrates how the methods detect heterogeneity of association between\nchemical properties of the wine, which are attributable to a mix of different\ncultivars.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 11:48:14 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Bamattre", "Stephen", ""], ["Hu", "Rex", ""], ["Verducci", "Joseph S.", ""]]}, {"id": "1504.05427", "submitter": "Siheng Chen", "authors": "Siheng Chen and Rohan Varma and Aarti Singh and Jelena Kova\\v{c}evi\\'c", "title": "Signal Recovery on Graphs: Random versus Experimentally Designed\n  Sampling", "comments": "Correct some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study signal recovery on graphs based on two sampling strategies: random\nsampling and experimentally designed sampling. We propose a new class of smooth\ngraph signals, called approximately bandlimited, which generalizes the\nbandlimited class and is similar to the globally smooth class. We then propose\ntwo recovery strategies based on random sampling and experimentally designed\nsampling. The proposed recovery strategy based on experimentally designed\nsampling is similar to the leverage scores used in the matrix approximation. We\nshow that while both strategies are unbiased estimators for the low-frequency\ncomponents, the convergence rate of experimentally designed sampling is much\nfaster than that of random sampling when a graph is irregular. We validate the\nproposed recovery strategies on three specific graphs: a ring graph, an\nErd\\H{o}s-R\\'enyi graph, and a star graph. The simulation results support the\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 13:28:17 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 14:39:29 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Chen", "Siheng", ""], ["Varma", "Rohan", ""], ["Singh", "Aarti", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1504.05434", "submitter": "Helene Massam", "authors": "Helene Massam and Nanwei Wang", "title": "A local approach to estimation in discrete loglinear models", "comments": "36 pages, 1 figure and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two connected aspects of maximum likelihood estimation of the\nparameter for high-dimensional discrete graphical models: the existence of the\nmaximum likelihood estimate (mle) and its computation.\n  When the data is sparse, there are many zeros in the contingency table and\nthe maximum likelihood estimate of the parameter may not exist. Fienberg and\nRinaldo (2012) have shown that the mle does not exists iff the data vector\nbelongs to a face of the so-called marginal cone spanned by the rows of the\ndesign matrix of the model. Identifying these faces in high-dimension is\nchallenging. In this paper, we take a local approach : we show that one such\nface, albeit possibly not the smallest one, can be identified by looking at a\ncollection of marginal graphical models generated by induced subgraphs\n$G_i,i=1,\\ldots,k$ of $G$. This is our first contribution.\n  Our second contribution concerns the composite maximum likelihood estimate.\nWhen the dimension of the problem is large, estimating the parameters of a\ngiven graphical model through maximum likelihood is challenging, if not\nimpossible. The traditional approach to this problem has been local with the\nuse of composite likelihood based on local conditional likelihoods.\n  A more recent development is to have the components of the composite\nlikelihood be marginal likelihoods centred around each $v$. We first show that\nthe estimates obtained by consensus through local conditional and marginal\nlikelihoods are identical. We then study the asymptotic properties of the\ncomposite maximum likelihood estimate when both the dimension of the model and\nthe sample size $N$ go to infinity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 13:51:47 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Massam", "Helene", ""], ["Wang", "Nanwei", ""]]}, {"id": "1504.05473", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky, Dmitry I. Ignatov", "title": "Can FCA-based Recommender System Suggest a Proper Classifier?", "comments": "10 pages, 1 figure, 4 tables, ECAI 2014, workshop \"What FCA can do\n  for \"Artifficial Intelligence\"", "journal-ref": "CEUR Workshop Proceedings, 1257, pp. 17-26 (2014)", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper briefly introduces multiple classifier systems and describes a new\nalgorithm, which improves classification accuracy by means of recommendation of\na proper algorithm to an object classification. This recommendation is done\nassuming that a classifier is likely to predict the label of the object\ncorrectly if it has correctly classified its neighbors. The process of\nassigning a classifier to each object is based on Formal Concept Analysis. We\nexplain the idea of the algorithm with a toy example and describe our first\nexperiments with real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:38:23 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kashnitsky", "Yury", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1504.05487", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Helmut B\\\"olcskei", "title": "Deep Convolutional Neural Networks Based on Semi-Discrete Frames", "comments": "Proc. of IEEE International Symposium on Information Theory (ISIT),\n  Hong Kong, China, June 2015, to appear", "journal-ref": "Proc. of IEEE International Symposium on Information Theory\n  (ISIT), Hong Kong, China, pp. 1212-1216, June 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have led to breakthrough results in\npractical feature extraction applications. The mathematical analysis of these\nnetworks was pioneered by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on identical semi-discrete wavelet frames\nin each network layer, and proved translation-invariance as well as deformation\nstability of the resulting feature extractor. The purpose of this paper is to\ndevelop Mallat's theory further by allowing for different and, most\nimportantly, general semi-discrete frames (such as, e.g., Gabor frames,\nwavelets, curvelets, shearlets, ridgelets) in distinct network layers. This\nallows to extract wider classes of features than point singularities resolved\nby the wavelet transform. Our generalized feature extractor is proven to be\ntranslation-invariant, and we develop deformation stability results for a\nlarger class of deformations than those considered by Mallat. For Mallat's\nwavelet-based feature extractor, we get rid of a number of technical\nconditions. The mathematical engine behind our results is continuous frame\ntheory, which allows us to completely detach the invariance and deformation\nstability proofs from the particular algebraic structure of the underlying\nframes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 16:01:00 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1504.05665", "submitter": "Kohei Hayashi", "authors": "Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki", "title": "Rebuilding Factorized Information Criterion: Asymptotically Accurate\n  Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorized information criterion (FIC) is a recently developed approximation\ntechnique for the marginal log-likelihood, which provides an automatic model\nselection framework for a few latent variable models (LVMs) with tractable\ninference algorithms. This paper reconsiders FIC and fills theoretical gaps of\nprevious FIC studies. First, we reveal the core idea of FIC that allows\ngeneralization for a broader class of LVMs, including continuous LVMs, in\ncontrast to previous FICs, which are applicable only to binary LVMs. Second, we\ninvestigate the model selection mechanism of the generalized FIC. Our analysis\nprovides a formal justification of FIC as a model selection criterion for LVMs\nand also a systematic procedure for pruning redundant latent variables that\nhave been removed heuristically in previous studies. Third, we provide an\ninterpretation of FIC as a variational free energy and uncover a few\npreviously-unknown their relationships. A demonstrative study on Bayesian\nprincipal component analysis is provided and numerical experiments support our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 06:27:19 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Hayashi", "Kohei", ""], ["Maeda", "Shin-ichi", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1504.05823", "submitter": "Michael Katehakis", "authors": "Wesley Cowan and Junya Honda and Michael N. Katehakis", "title": "Normal Bandits of Unknown Means and Variances: Asymptotic Optimality,\n  Finite Horizon Regret Bounds, and a Solution to an Open Problem", "comments": "15 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of sampling sequentially from a finite number of $N \\geq\n2$ populations, specified by random variables $X^i_k$, $ i = 1,\\ldots , N,$ and\n$k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from population $i$ the\n$k^{th}$ time it is sampled. It is assumed that for each fixed $i$,\n  $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. normal random variables,\nwith unknown mean $\\mu_i$ and unknown variance $\\sigma_i^2$.\n  The objective is to have a policy $\\pi$ for deciding from which of the $N$\npopulations to sample form at any time $n=1,2,\\ldots$ so as to maximize the\nexpected sum of outcomes of $n$ samples or equivalently to minimize the regret\ndue to lack on information of the parameters $\\mu_i$ and $\\sigma_i^2$. In this\npaper, we present a simple inflated sample mean (ISM) index policy that is\nasymptotically optimal in the sense of Theorem 4 below. This resolves a\nstanding open problem from Burnetas and Katehakis (1996). Additionally, finite\nhorizon regret bounds are given.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:30:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 03:15:23 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Cowan", "Wesley", ""], ["Honda", "Junya", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1504.05880", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan and Mark Rudelson", "title": "Spectral Norm of Random Kernel Matrices with Applications to Privacy", "comments": "16 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are an extremely popular set of techniques used for many\nimportant machine learning and data analysis applications. In addition to\nhaving good practical performances, these methods are supported by a\nwell-developed theory. Kernel methods use an implicit mapping of the input data\ninto a high dimensional feature space defined by a kernel function, i.e., a\nfunction returning the inner product between the images of two data points in\nthe feature space. Central to any kernel method is the kernel matrix, which is\nbuilt by evaluating the kernel function on a given sample dataset.\n  In this paper, we initiate the study of non-asymptotic spectral theory of\nrandom kernel matrices. These are n x n random matrices whose (i,j)th entry is\nobtained by evaluating the kernel function on $x_i$ and $x_j$, where\n$x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Our\nmain contribution is to obtain tight upper bounds on the spectral norm (largest\neigenvalue) of random kernel matrices constructed by commonly used kernel\nfunctions based on polynomials and Gaussian radial basis.\n  As an application of these results, we provide lower bounds on the distortion\nneeded for releasing the coefficients of kernel ridge regression under\nattribute privacy, a general privacy notion which captures a large class of\nprivacy definitions. Kernel ridge regression is standard method for performing\nnon-parametric regression that regularly outperforms traditional regression\napproaches in various domains. Our privacy distortion lower bounds are the\nfirst for any kernel technique, and our analysis assumes realistic scenarios\nfor the input, unlike all previous lower bounds for other release problems\nwhich only hold under very restrictive input settings.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 16:54:48 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""]]}, {"id": "1504.05929", "submitter": "Bishan Yang", "authors": "Bishan Yang and Claire Cardie and Peter Frazier", "title": "A Hierarchical Distance-dependent Bayesian Model for Event Coreference\n  Resolution", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical distance-dependent Bayesian model for event\ncoreference resolution. While existing generative models for event coreference\nresolution are completely unsupervised, our model allows for the incorporation\nof pairwise distances between event mentions -- information that is widely used\nin supervised coreference models to guide the generative clustering processing\nfor better event clustering both within and across documents. We model the\ndistances between event mentions using a feature-rich learnable distance\nfunction and encode them as Bayesian priors for nonparametric clustering.\nExperiments on the ECB+ corpus show that our model outperforms state-of-the-art\nmethods for both within- and cross-document event coreference resolution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 19:13:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 05:41:59 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Yang", "Bishan", ""], ["Cardie", "Claire", ""], ["Frazier", "Peter", ""]]}, {"id": "1504.05994", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a, Jouni Hartikainen, Lennart Svensson and Fredrik\n  Sandblom", "title": "On the relation between Gaussian process quadratures and sigma-point\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with Gaussian process quadratures, which are\nnumerical integration methods based on Gaussian process regression methods, and\nsigma-point methods, which are used in advanced non-linear Kalman filtering and\nsmoothing algorithms. We show that many sigma-point methods can be interpreted\nas Gaussian quadrature based methods with suitably selected covariance\nfunctions. We show that this interpretation also extends to more general\nmultivariate Gauss--Hermite integration methods and related spherical cubature\nrules. Additionally, we discuss different criteria for selecting the\nsigma-point locations: exactness for multivariate polynomials up to a given\norder, minimum average error, and quasi-random point sets. The performance of\nthe different methods is tested in numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 21:49:42 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["Hartikainen", "Jouni", ""], ["Svensson", "Lennart", ""], ["Sandblom", "Fredrik", ""]]}, {"id": "1504.06026", "submitter": "Han Liu", "authors": "Junwei Lu, Han Liu", "title": "Graphical Fermat's Principle and Triangle-Free Graph Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating undirected triangle-free graphs of high\ndimensional distributions. Triangle-free graphs form a rich graph family which\nallows arbitrary loopy structures but 3-cliques. For inferential tractability,\nwe propose a graphical Fermat's principle to regularize the distribution\nfamily. Such principle enforces the existence of a distribution-dependent\npseudo-metric such that any two nodes have a smaller distance than that of two\nother nodes who have a geodesic path include these two nodes. Guided by this\nprinciple, we show that a greedy strategy is able to recover the true graph.\nThe resulting algorithm only requires a pairwise distance matrix as input and\nis computationally even more efficient than calculating the minimum spanning\ntree. We consider graph estimation problems under different settings, including\ndiscrete and nonparametric distribution families. Thorough numerical results\nare provided to illustrate the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 02:28:16 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Lu", "Junwei", ""], ["Liu", "Han", ""]]}, {"id": "1504.06043", "submitter": "Arunselvan Ramaswamy", "authors": "Arunselvan Ramaswamy and Shalabh Bhatnagar", "title": "Stability of Stochastic Approximations with `Controlled Markov' Noise\n  and Temporal Difference Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in understanding stability (almost sure boundedness) of\nstochastic approximation algorithms (SAs) driven by a `controlled Markov'\nprocess. Analyzing this class of algorithms is important, since many\nreinforcement learning (RL) algorithms can be cast as SAs driven by a\n`controlled Markov' process. In this paper, we present easily verifiable\nsufficient conditions for stability and convergence of SAs driven by a\n`controlled Markov' process. Many RL applications involve continuous state\nspaces. While our analysis readily ensures stability for such continuous state\napplications, traditional analyses do not. As compared to literature, our\nanalysis presents a two-fold generalization (a) the Markov process may evolve\nin a continuous state space and (b) the process need not be ergodic under any\ngiven stationary policy. Temporal difference learning (TD) is an important\npolicy evaluation method in reinforcement learning. The theory developed\nherein, is used to analyze generalized $TD(0)$, an important variant of TD. Our\ntheory is also used to analyze a TD formulation of supervised learning for\nforecasting problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 04:50:27 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 06:54:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1504.06274", "submitter": "Qiang Wu", "authors": "Dong Mao, Yang Wang and Qiang Wu", "title": "A new approach for physiological time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new approach for the analysis of physiological time series. An\niterative convolution filter is used to decompose the time series into various\ncomponents. Statistics of these components are extracted as features to\ncharacterize the mechanisms underlying the time series. Motivated by the\nstudies that show many normal physiological systems involve irregularity while\nthe decrease of irregularity usually implies the abnormality, the statistics\nfor \"outliers\" in the components are used as features measuring irregularity.\nSupport vector machines are used to select the most relevant features that are\nable to differentiate the time series from normal and abnormal systems. This\nnew approach is successfully used in the study of congestive heart failure by\nheart beat interval time series.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 17:56:33 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Mao", "Dong", ""], ["Wang", "Yang", ""], ["Wu", "Qiang", ""]]}, {"id": "1504.06305", "submitter": "Ping Li", "authors": "Martin Slawski, Ping Li, Matthias Hein", "title": "Regularization-free estimation in trace regression with symmetric\n  positive semidefinite matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, trace regression models have received considerable\nattention in the context of matrix completion, quantum state tomography, and\ncompressed sensing. Estimation of the underlying matrix from\nregularization-based approaches promoting low-rankedness, notably nuclear norm\nregularization, have enjoyed great popularity. In the present paper, we argue\nthat such regularization may no longer be necessary if the underlying matrix is\nsymmetric positive semidefinite (\\textsf{spd}) and the design satisfies certain\nconditions. In this situation, simple least squares estimation subject to an\n\\textsf{spd} constraint may perform as well as regularization-based approaches\nwith a proper choice of the regularization parameter, which entails knowledge\nof the noise level and/or tuning. By contrast, constrained least squares\nestimation comes without any tuning parameter and may hence be preferred due to\nits simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 19:30:38 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Slawski", "Martin", ""], ["Li", "Ping", ""], ["Hein", "Matthias", ""]]}, {"id": "1504.06329", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and John Grothendieck", "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "comments": "10 pages, 8 tables; appeared in Proceedings of the Seventeenth\n  Conference on Computational Natural Language Learning, August 2013", "journal-ref": "In Proceedings of the Seventeenth Conference on Computational\n  Natural Language Learning, pages 10-19, Sofia, Bulgaria, August 2013.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the natural language processing (NLP) community, active learning has\nbeen widely investigated and applied in order to alleviate the annotation\nbottleneck faced by developers of new NLP systems and technologies. This paper\npresents the first theoretical analysis of stopping active learning based on\nstabilizing predictions (SP). The analysis has revealed three elements that are\ncentral to the success of the SP method: (1) bounds on Cohen's Kappa agreement\nbetween successively trained models impose bounds on differences in F-measure\nperformance of the models; (2) since the stop set does not have to be labeled,\nit can be made large in practice, helping to guarantee that the results\ntransfer to previously unseen streams of examples at test/application time; and\n(3) good (low variance) sample estimates of Kappa between successive models can\nbe obtained. Proofs of relationships between the level of Kappa agreement and\nthe difference in performance between consecutive models are presented.\nSpecifically, if the Kappa agreement between two models exceeds a threshold T\n(where $T>0$), then the difference in F-measure performance between those\nmodels is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the\npositive conjunction of the models is assumed to be $p$, then the bound can be\ntightened to $\\frac{4(1-T)}{(p+1)T}$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:07:01 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Bloodgood", "Michael", ""], ["Grothendieck", "John", ""]]}, {"id": "1504.06394", "submitter": "Jing Wang", "authors": "Jing Wang and Jie Shen and Huan Xu", "title": "Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social trust prediction addresses the significant problem of exploring\ninteractions among users in social networks. Naturally, this problem can be\nformulated in the matrix completion framework, with each entry indicating the\ntrustness or distrustness. However, there are two challenges for the social\ntrust problem: 1) the observed data are with sign (1-bit) measurements; 2) they\nare typically sampled non-uniformly. Most of the previous matrix completion\nmethods do not well handle the two issues. Motivated by the recent progress of\nmax-norm, we propose to solve the problem with a 1-bit max-norm constrained\nformulation. Since max-norm is not easy to optimize, we utilize a reformulation\nof max-norm which facilitates an efficient projected gradient decent algorithm.\nWe demonstrate the superiority of our formulation on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 05:01:12 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Wang", "Jing", ""], ["Shen", "Jie", ""], ["Xu", "Huan", ""]]}, {"id": "1504.06553", "submitter": "Daniel Trejo Banos", "authors": "D Trejo, AJ Millar and G Sanguinetti", "title": "A Bayesian approach for structure learning in oscillating regulatory\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oscillations lie at the core of many biological processes, from the cell\ncycle, to circadian oscillations and developmental processes. Time-keeping\nmechanisms are essential to enable organisms to adapt to varying conditions in\nenvironmental cycles, from day/night to seasonal. Transcriptional regulatory\nnetworks are one of the mechanisms behind these biological oscillations.\nHowever, while identifying cyclically expressed genes from time series\nmeasurements is relatively easy, determining the structure of the interaction\nnetwork underpinning the oscillation is a far more challenging problem. Here,\nwe explicitly leverage the oscillatory nature of the transcriptional signals\nand present a method for reconstructing network interactions tailored to this\nspecial but important class of genetic circuits. Our method is based on\nprojecting the signal onto a set of oscillatory basis functions using a\nDiscrete Fourier Transform. We build a Bayesian Hierarchical model within a\nfrequency domain linear model in order to enforce sparsity and incorporate\nprior knowledge about the network structure. Experiments on real and simulated\ndata show that the method can lead to substantial improvements over competing\napproaches if the oscillatory assumption is met, and remains competitive also\nin cases it is not.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 16:08:30 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Trejo", "D", ""], ["Millar", "AJ", ""], ["Sanguinetti", "G", ""]]}, {"id": "1504.06650", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan and Michael Collins", "title": "Learning Dictionaries for Named Entity Recognition using Minimal\n  Supervision", "comments": "In 14th Conference of the European Chapter of the Association for\n  Computational Linguistic, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach for automatic construction of dictionaries\nfor Named Entity Recognition (NER) using large amounts of unlabeled data and a\nfew seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower\ndimensional embeddings (representations) for candidate phrases and classify\nthese phrases using a small number of labeled examples. Our method achieves\n16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER\nrespectively. We also show that by adding candidate phrase embeddings as\nfeatures in a sequence tagger gives better performance compared to using word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 21:43:55 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Collins", "Michael", ""]]}, {"id": "1504.06654", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Jeevan Shankar, Alexandre Passos and Andrew\n  McCallum", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in\n  Vector Space", "comments": "In Conference on Empirical Methods in Natural Language Processing,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is rising interest in vector-space word embeddings and their use in\nNLP, especially given recent methods for their fast estimation at very large\nscale. Nearly all this work, however, assumes a single vector per word type\nignoring polysemy and thus jeopardizing their usefulness for downstream tasks.\nWe present an extension to the Skip-gram model that efficiently learns multiple\nembeddings per word type. It differs from recent related work by jointly\nperforming word sense discrimination and embedding learning, by\nnon-parametrically estimating the number of senses per word type, and by its\nefficiency and scalability. We present new state-of-the-art results in the word\nsimilarity in context task and demonstrate its scalability by training with one\nmachine on a corpus of nearly 1 billion tokens in less than 6 hours.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:12:14 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Shankar", "Jeevan", ""], ["Passos", "Alexandre", ""], ["McCallum", "Andrew", ""]]}, {"id": "1504.06658", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan and Ming-Wei Chang", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion:\n  New Dataset and Methods", "comments": "North American Chapter of the Association for Computational\n  Linguistics- Human Language Technologies, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous work in knowledge base (KB) completion has focused on the\nproblem of relation extraction. In this work, we focus on the task of inferring\nmissing entity type instances in a KB, a fundamental task for KB competition\nyet receives little attention. Due to the novelty of this task, we construct a\nlarge-scale dataset and design an automatic evaluation methodology. Our\nknowledge base completion method uses information within the existing KB and\nexternal information from Wikipedia. We show that individual methods trained\nwith a global objective that considers unobserved cells from both the entity\nand the type side gives consistently higher quality predictions compared to\nbaseline methods. We also perform manual evaluation on a small subset of the\ndata to verify the effectiveness of our knowledge base completion methods and\nthe correctness of our proposed automatic evaluation method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:32:40 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Chang", "Ming-Wei", ""]]}, {"id": "1504.06662", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Benjamin Roth and Andrew McCallum", "title": "Compositional Vector Space Models for Knowledge Base Completion", "comments": "The 53rd Annual Meeting of the Association for Computational\n  Linguistics and The 7th International Joint Conference of the Asian\n  Federation of Natural Language Processing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base (KB) completion adds new facts to a KB by making inferences\nfrom existing facts, for example by inferring with high likelihood\nnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop\nrelational synonyms like this, or use as evidence a multi-hop relational path\ntreated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper\npresents an approach that reasons about conjunctions of multi-hop relations\nnon-atomically, composing the implications of a path using a recursive neural\nnetwork (RNN) that takes as inputs vector embeddings of the binary relation in\nthe path. Not only does this allow us to generalize to paths unseen at training\ntime, but also, with a single high-capacity RNN, to predict new relation types\nnot seen when the compositional model was trained (zero-shot learning). We\nassemble a new dataset of over 52M relational triples, and show that our method\nimproves over a traditional classifier by 11%, and a method leveraging\npre-trained embeddings by 7%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:06:10 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 21:23:45 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Roth", "Benjamin", ""], ["McCallum", "Andrew", ""]]}, {"id": "1504.06701", "submitter": "Felix Rios", "authors": "Felix L. Rios, John M. Noble, Timo J.T. Koski", "title": "A Prior Distribution over Directed Acyclic Graphs for Sparse Bayesian\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this article is a new prior distribution over\ndirected acyclic graphs, which gives larger weight to sparse graphs. This\ndistribution is intended for structured Bayesian networks, where the structure\nis given by an ordered block model. That is, the nodes of the graph are objects\nwhich fall into categories (or blocks); the blocks have a natural ordering. The\npresence of a relationship between two objects is denoted by an arrow, from the\nobject of lower category to the object of higher category. The models\nconsidered here were introduced in Kemp et al. (2004) for relational data and\nextended to multivariate data in Mansinghka et al. (2006). The prior over graph\nstructures presented here has an explicit formula. The number of nodes in each\nlayer of the graph follow a Hoppe Ewens urn model.\n  We consider the situation where the nodes of the graph represent random\nvariables, whose joint probability distribution factorises along the DAG. We\ndescribe Monte Carlo schemes for finding the optimal aposteriori structure\ngiven a data matrix and compare the performance with Mansinghka et al. (2006)\nand also with the uniform prior.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 08:35:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Rios", "Felix L.", ""], ["Noble", "John M.", ""], ["Koski", "Timo J. T.", ""]]}, {"id": "1504.06779", "submitter": "Emerson Machado", "authors": "Emerson Lopes Machado, Cristiano Jacques Miosso, Ricardo von Borries,\n  Murilo Coutinho, Pedro de Azevedo Berger, Thiago Marques, Ricardo Pezzuol\n  Jacobi", "title": "Computational Cost Reduction in Learned Transform Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis and empirical evaluations of a novel set of\ntechniques for computational cost reduction of classifiers that are based on\nlearned transform and soft-threshold. By modifying optimization procedures for\ndictionary and classifier training, as well as the resulting dictionary\nentries, our techniques allow to reduce the bit precision and to replace each\nfloating-point multiplication by a single integer bit shift. We also show how\nthe optimization algorithms in some dictionary training methods can be modified\nto penalize higher-energy dictionaries. We applied our techniques with the\nclassifier Learning Algorithm for Soft-Thresholding, testing on the datasets\nused in its original paper. Our results indicate it is feasible to use solely\nsums and bit shifts of integers to classify at test time with a limited\nreduction of the classification accuracy. These low power operations are a\nvaluable trade off in FPGA implementations as they increase the classification\nthroughput while decrease both energy consumption and manufacturing cost.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 01:16:44 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 15:03:29 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Machado", "Emerson Lopes", ""], ["Miosso", "Cristiano Jacques", ""], ["von Borries", "Ricardo", ""], ["Coutinho", "Murilo", ""], ["Berger", "Pedro de Azevedo", ""], ["Marques", "Thiago", ""], ["Jacobi", "Ricardo Pezzuol", ""]]}, {"id": "1504.06785", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere", "comments": "104 pages, 5 figures. Due to length constraint of publication, this\n  long paper are subsequently divided into two papers (arXiv:1511.03607 and\n  arXiv:1511.04777). Further updates will be made only to the two papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 04:57:19 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 06:06:04 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 21:56:30 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1504.06796", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba and Shie Mannor", "title": "Overlapping Communities Detection via Measure Space Embedding", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 28, 2890--2898,\n  2015", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for community detection. The algorithm uses random\nwalks to embed the graph in a space of measures, after which a modification of\n$k$-means in that space is applied. The algorithm is therefore fast and easily\nparallelizable. We evaluate the algorithm on standard random graph benchmarks,\nincluding some overlapping community benchmarks, and find its performance to be\nbetter or at least as good as previously known algorithms. We also prove a\nlinear time (in number of edges) guarantee for the algorithm on a\n$p,q$-stochastic block model with $p \\geq c\\cdot N^{-\\frac{1}{2} + \\epsilon}$\nand $p-q \\geq c' \\sqrt{p N^{-\\frac{1}{2} + \\epsilon} \\log N}$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 10:00:29 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 08:00:34 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "1504.06817", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou", "title": "Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a theoretical analysis of the nuclear-norm\nregularized least squares for full-rank matrix completion. Although similar\nformulations have been examined by previous studies, their results are\nunsatisfactory because only additive upper bounds are provided. Under the\nassumption that the top eigenspaces of the target matrix are incoherent, we\nderive a relative upper bound for recovering the best low-rank approximation of\nthe unknown matrix. Our relative upper bound is tighter than previous additive\nbounds of other methods if the mass of the target matrix is concentrated on its\ntop eigenspaces, and also implies perfect recovery if it is low-rank. The\nanalysis is built upon the optimality condition of the regularized formulation\nand existing guarantees for low-rank matrix completion. To the best of our\nknowledge, this is first time such a relative bound is proved for the\nregularized formulation of matrix completion.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 13:12:16 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1504.06837", "submitter": "Marc Claesen", "authors": "Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor", "title": "Assessing binary classifiers using only positive and unlabeled data", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the performance of a learned model is a crucial part of machine\nlearning. However, in some domains only positive and unlabeled examples are\navailable, which prohibits the use of most standard evaluation metrics. We\npropose an approach to estimate any metric based on contingency tables,\nincluding ROC and PR curves, using only positive and unlabeled data. Estimating\nthese performance metrics is essentially reduced to estimating the fraction of\n(latent) positives in the unlabeled set, assuming known positives are a random\nsample of all positives. We provide theoretical bounds on the quality of our\nestimates, illustrate the importance of estimating the fraction of positives in\nthe unlabeled set and demonstrate empirically that we are able to reliably\nestimate ROC and PR curves on real data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:59:12 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 13:18:43 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Claesen", "Marc", ""], ["Davis", "Jesse", ""], ["De Smet", "Frank", ""], ["De Moor", "Bart", ""]]}, {"id": "1504.06848", "submitter": "David Tolpin", "authors": "David Tolpin, Frank Wood", "title": "Maximum a Posteriori Estimation by Search in Probabilistic Programs", "comments": "To appear in proceedings of SOCS15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approximate search algorithm for fast maximum a posteriori\nprobability estimation in probabilistic programs, which we call Bayesian ascent\nMonte Carlo (BaMC). Probabilistic programs represent probabilistic models with\nvarying number of mutually dependent finite, countable, and continuous random\nvariables. BaMC is an anytime MAP search algorithm applicable to any\ncombination of random variables and dependencies. We compare BaMC to other MAP\nestimation algorithms and show that BaMC is faster and more robust on a range\nof probabilistic models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 17:23:06 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Tolpin", "David", ""], ["Wood", "Frank", ""]]}, {"id": "1504.06877", "submitter": "Giulio Bottegal", "authors": "Giulio Bottegal, Gianluigi Pillonetto and H{\\aa}kan Hjalmarsson", "title": "Bayesian kernel-based system identification with quantized output data", "comments": "Submitted to IFAC SysId 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method for linear system identification\nwith quantized output data. We model the impulse response as a zero-mean\nGaussian process whose covariance (kernel) is given by the recently proposed\nstable spline kernel, which encodes information on regularity and exponential\nstability. This serves as a starting point to cast our system identification\nproblem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC)\nmethods to provide an estimate of the system. In particular, we show how to\ndesign a Gibbs sampler which quickly converges to the target distribution.\nNumerical simulations show a substantial improvement in the accuracy of the\nestimates over state-of-the-art kernel-based methods when employed in\nidentification of systems with quantized data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 20:08:51 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Bottegal", "Giulio", ""], ["Pillonetto", "Gianluigi", ""], ["Hjalmarsson", "H\u00e5kan", ""]]}, {"id": "1504.06937", "submitter": "Huasen Wu", "authors": "Huasen Wu, R. Srikant, Xin Liu, and Chong Jiang", "title": "Algorithms with Logarithmic or Sublinear Regret for Constrained\n  Contextual Bandits", "comments": "36 pages, 4 figures; accepted by the 29th Annual Conference on Neural\n  Information Processing Systems (NIPS), Montr\\'eal, Canada, Dec. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study contextual bandits with budget and time constraints, referred to as\nconstrained contextual bandits.The time and budget constraints significantly\ncomplicate the exploration and exploitation tradeoff because they introduce\ncomplex coupling among contexts over time.Such coupling effects make it\ndifficult to obtain oracle solutions that assume known statistics of bandits.\nTo gain insight, we first study unit-cost systems with known context\ndistribution. When the expected rewards are known, we develop an approximation\nof the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves\nnear-optimality and only requires the ordering of expected rewards. With these\nhighly desirable features, we then combine ALP with the upper-confidence-bound\n(UCB) method in the general case where the expected rewards are unknown {\\it a\npriori}. We show that the proposed UCB-ALP algorithm achieves logarithmic\nregret except for certain boundary cases. Further, we design algorithms and\nobtain similar regret analysis results for more general systems with unknown\ncontext distribution and heterogeneous costs. To the best of our knowledge,\nthis is the first work that shows how to achieve logarithmic regret in\nconstrained contextual bandits. Moreover, this work also sheds light on the\nstudy of computationally efficient algorithms for general constrained\ncontextual bandits.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 06:03:50 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 17:55:35 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 16:47:20 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Wu", "Huasen", ""], ["Srikant", "R.", ""], ["Liu", "Xin", ""], ["Jiang", "Chong", ""]]}, {"id": "1504.06964", "submitter": "Fulton Wang", "authors": "Fulton Wang and Tyler H. McCormick and Cynthia Rudin and John Gore", "title": "Modeling Recovery Curves With Application to Prostatectomy", "comments": "Accepted to Biostatistics, 2018. Includes supplementary material and\n  high resolution images of predictions for patients", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian model that predicts recovery curves based on\ninformation available before the disruptive event. A recovery curve of interest\nis the quantified sexual function of prostate cancer patients after\nprostatectomy surgery. We illustrate the utility of our model as a\npre-treatment medical decision aid, producing personalized predictions that are\nboth interpretable and accurate. We uncover covariate relationships that agree\nwith and supplement that in existing medical literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:14:33 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 16:52:41 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 04:45:09 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 15:54:40 GMT"}, {"version": "v5", "created": "Tue, 27 Feb 2018 14:18:27 GMT"}, {"version": "v6", "created": "Mon, 5 Mar 2018 03:42:28 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wang", "Fulton", ""], ["McCormick", "Tyler H.", ""], ["Rudin", "Cynthia", ""], ["Gore", "John", ""]]}, {"id": "1504.07027", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "Alexander G. de G. Matthews, James Hensman, Richard E. Turner, Zoubin\n  Ghahramani", "title": "On Sparse variational methods and the Kullback-Leibler divergence\n  between stochastic processes", "comments": "9 pages. No figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational framework for learning inducing variables (Titsias, 2009a)\nhas had a large impact on the Gaussian process literature. The framework may be\ninterpreted as minimizing a rigorously defined Kullback-Leibler divergence\nbetween the approximating and posterior processes. To our knowledge this\nconnection has thus far gone unremarked in the literature. In this paper we\ngive a substantial generalization of the literature on this topic. We give a\nnew proof of the result for infinite index sets which allows inducing points\nthat are not data points and likelihoods that depend on all function values. We\nthen discuss augmented index sets and show that, contrary to previous works,\nmarginal consistency of augmentation is not enough to guarantee consistency of\nvariational inference with the original model. We then characterize an extra\ncondition where such a guarantee is obtainable. Finally we show how our\nframework sheds light on interdomain sparse approximations and sparse\napproximations for Cox processes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:01:50 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 11:27:01 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Matthews", "Alexander G. de G.", ""], ["Hensman", "James", ""], ["Turner", "Richard E.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1504.07107", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Jun Zhu, Bo Zhang", "title": "Fast Sampling for Bayesian Max-Margin Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian max-margin models have shown superiority in various practical\napplications, such as text categorization, collaborative prediction, social\nnetwork link prediction and crowdsourcing, and they conjoin the flexibility of\nBayesian modeling and predictive strengths of max-margin learning. However,\nMonte Carlo sampling for these models still remains challenging, especially for\napplications that involve large-scale datasets. In this paper, we present the\nstochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to\nimplement and computationally efficient. We show the approximate detailed\nbalance property of subgradient HMC which reveals a natural and validated\ngeneralization of the ordinary HMC. Furthermore, we investigate the variants\nthat use stochastic subsampling and thermostats for better scalability and\nmixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we\nefficiently solve the posterior inference task of various Bayesian max-margin\nmodels and extensive experimental results demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:29:40 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 12:28:41 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 07:26:02 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2015 12:53:35 GMT"}, {"version": "v5", "created": "Tue, 18 Oct 2016 13:44:30 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Hu", "Wenbo", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1504.07218", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Changho Suh", "title": "Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons", "comments": "accepted to International Conference on Machine Learning (ICML), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the preference-based top-$K$ rank aggregation problem.\nSuppose that a collection of items is repeatedly compared in pairs, and one\nwishes to recover a consistent ordering that emphasizes the top-$K$ ranked\nitems, based on partially revealed preferences. We focus on the\nBradley-Terry-Luce (BTL) model that postulates a set of latent preference\nscores underlying all items, where the odds of paired comparisons depend only\non the relative scores of the items involved.\n  We characterize the minimax limits on identifiability of top-$K$ ranked\nitems, in the presence of random and non-adaptive sampling. Our results\nhighlight a separation measure that quantifies the gap of preference scores\nbetween the $K^{\\text{th}}$ and $(K+1)^{\\text{th}}$ ranked items. The minimum\nsample complexity required for reliable top-$K$ ranking scales inversely with\nthe separation measure irrespective of other preference distribution metrics.\nTo approach this minimax limit, we propose a nearly linear-time ranking scheme,\ncalled \\emph{Spectral MLE}, that returns the indices of the top-$K$ items in\naccordance to a careful score estimate. In a nutshell, Spectral MLE starts with\nan initial score estimate with minimal squared loss (obtained via a spectral\nmethod), and then successively refines each component with the assistance of\ncoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item\nidentification under minimal sample complexity. The practical applicability of\nSpectral MLE is further corroborated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:30:01 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 06:04:15 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Chen", "Yuxin", ""], ["Suh", "Changho", ""]]}, {"id": "1504.07225", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran", "title": "Correlational Neural Networks", "comments": "27 pages. To Appear in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common Representation Learning (CRL), wherein different descriptions (or\nviews) of the data are embedded in a common subspace, is receiving a lot of\nattention recently. Two popular paradigms here are Canonical Correlation\nAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA\nbased approaches learn a joint representation by maximizing correlation of the\nviews when projected to the common subspace. AE based methods learn a common\nrepresentation by minimizing the error of reconstructing the two views. Each of\nthese approaches has its own advantages and disadvantages. For example, while\nCCA based approaches outperform AE based approaches for the task of transfer\nlearning, they are not as scalable as the latter. In this work we propose an AE\nbased approach called Correlational Neural Network (CorrNet), that explicitly\nmaximizes correlation among the views when projected to the common subspace.\nThrough a series of experiments, we demonstrate that the proposed CorrNet is\nbetter than the above mentioned approaches with respect to its ability to learn\ncorrelated common representations. Further, we employ CorrNet for several cross\nlanguage tasks and show that the representations learned using CorrNet perform\nbetter than the ones learned using other state of the art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:51:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 20:34:28 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 19:14:05 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Chandar", "Sarath", ""], ["Khapra", "Mitesh M.", ""], ["Larochelle", "Hugo", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1504.07235", "submitter": "Ping Li", "authors": "Ping Li", "title": "Sign Stable Random Projections for Large-Scale Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:50:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1504.07389", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Pieter Gillard, Chantal Mathieu, Bart De\n  Moor", "title": "Building Classifiers to Predict the Start of Glucose-Lowering\n  Pharmacotherapy Using Belgian Health Expenditure Data", "comments": "23 pages, 5 figures, submitted to JMLR special issue on Learning from\n  Electronic Health Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis is important for type 2 diabetes (T2D) to improve patient\nprognosis, prevent complications and reduce long-term treatment costs. We\npresent a novel risk profiling approach based exclusively on health expenditure\ndata that is available to Belgian mutual health insurers. We used expenditure\ndata related to drug purchases and medical provisions to construct models that\npredict whether a patient will start glucose-lowering pharmacotherapy in the\ncoming years, based on that patient's recent medical expenditure history. The\ndesign and implementation of the modeling strategy are discussed in detail and\nseveral learning methods are benchmarked for our application. Our best\nperforming model obtains between 74.9% and 76.8% area under the ROC curve,\nwhich is comparable to state-of-the-art risk prediction approaches for T2D\nbased on questionnaires. In contrast to other methods, our approach can be\nimplemented on a population-wide scale at virtually no extra operational cost.\nPossibly, our approach can be further improved by additional information about\nsome risk factors of T2D that is unavailable in health expenditure data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 09:27:03 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Gillard", "Pieter", ""], ["Mathieu", "Chantal", ""], ["De Moor", "Bart", ""]]}, {"id": "1504.07468", "submitter": "Xin Yuan", "authors": "Xin Yuan, Ricardo Henao, Ephraim L. Tsalik, Raymond J. Langley,\n  Lawrence Carin", "title": "Non-Gaussian Discriminative Factor Models via the Max-Margin\n  Rank-Likelihood", "comments": "14 pages, 7 figures, ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discriminative factor analysis for data that are\nin general non-Gaussian. A Bayesian model based on the ranks of the data is\nproposed. We first introduce a new {\\em max-margin} version of the\nrank-likelihood. A discriminative factor model is then developed, integrating\nthe max-margin rank-likelihood and (linear) Bayesian support vector machines,\nwhich are also built on the max-margin principle. The discriminative factor\nmodel is further extended to the {\\em nonlinear} case through mixtures of local\nlinear classifiers, via Dirichlet processes. Fully local conjugacy of the model\nyields efficient inference with both Markov Chain Monte Carlo and variational\nBayes approaches. Extensive experiments on benchmark and real data demonstrate\nsuperior performance of the proposed model and its potential for applications\nin computational biology.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 13:40:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 20:10:52 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 19:24:46 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Yuan", "Xin", ""], ["Henao", "Ricardo", ""], ["Tsalik", "Ephraim L.", ""], ["Langley", "Raymond J.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1504.07550", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi and Romain H\\'erault and Cl\\'ement Chatelain and\n  S\\'ebastien Adam", "title": "Deep Neural Networks Regularization for Structured Output Prediction", "comments": "Submitted to Neurocomputing, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network model is a powerful framework for learning\nrepresentations. Usually, it is used to learn the relation $x \\to y$ by\nexploiting the regularities in the input $x$. In structured output prediction\nproblems, $y$ is multi-dimensional and structural relations often exist between\nthe dimensions. The motivation of this work is to learn the output dependencies\nthat may lie in the output data in order to improve the prediction accuracy.\nUnfortunately, feedforward networks are unable to exploit the relations between\nthe outputs. In order to overcome this issue, we propose in this paper a\nregularization scheme for training neural networks for these particular tasks\nusing a multi-task framework. Our scheme aims at incorporating the learning of\nthe output representation $y$ in the training process in an unsupervised\nfashion while learning the supervised mapping function $x \\to y$.\n  We evaluate our framework on a facial landmark detection problem which is a\ntypical structured output task. We show over two public challenging datasets\n(LFPW and HELEN) that our regularization scheme improves the generalization of\ndeep neural networks and accelerates their training. The use of unlabeled data\nand label-only data is also explored, showing an additional improvement of the\nresults. We provide an opensource implementation\n(https://github.com/sbelharbi/structured-output-ae) of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 16:11:15 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 13:10:53 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 12:30:27 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 15:30:04 GMT"}, {"version": "v5", "created": "Mon, 3 Apr 2017 11:05:23 GMT"}, {"version": "v6", "created": "Mon, 30 Oct 2017 17:00:29 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Belharbi", "Soufiane", ""], ["H\u00e9rault", "Romain", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Adam", "S\u00e9bastien", ""]]}, {"id": "1504.07575", "submitter": "Oisin Mac Aodha", "authors": "Edward Johns and Oisin Mac Aodha and Gabriel J. Brostow", "title": "Becoming the Expert - Interactive Multi-Class Machine Teaching", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 17:22:29 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Johns", "Edward", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.07676", "submitter": "Matt Olson", "authors": "Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease", "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating\n  Classifiers", "comments": "40 pages, 11 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large literature explaining why AdaBoost is a successful\nclassifier. The literature on AdaBoost focuses on classifier margins and\nboosting's interpretation as the optimization of an exponential likelihood\nfunction. These existing explanations, however, have been pointed out to be\nincomplete. A random forest is another popular ensemble method for which there\nis substantially less explanation in the literature. We introduce a novel\nperspective on AdaBoost and random forests that proposes that the two\nalgorithms work for similar reasons. While both classifiers achieve similar\npredictive accuracy, random forests cannot be conceived as a direct\noptimization procedure. Rather, random forests is a self-averaging,\ninterpolating algorithm which creates what we denote as a \"spikey-smooth\"\nclassifier, and we view AdaBoost in the same light. We conjecture that both\nAdaBoost and random forests succeed because of this mechanism. We provide a\nnumber of examples and some theoretical justification to support this\nexplanation. In the process, we question the conventional wisdom that suggests\nthat boosting algorithms for classification require regularization or early\nstopping and should be limited to low complexity classes of learners, such as\ndecision stumps. We conclude that boosting should be used like random forests:\nwith large decision trees and without direct regularization or early stopping.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 22:34:25 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 23:25:20 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Wyner", "Abraham J.", ""], ["Olson", "Matthew", ""], ["Bleich", "Justin", ""], ["Mease", "David", ""]]}, {"id": "1504.07829", "submitter": "Samuele Soraggi", "authors": "Sara Rebagliati and Emanuela Sasso and Samuele Soraggi", "title": "Market forecasting using Hidden Markov Models", "comments": "This paper has been withdrawn by the author due to many errors and\n  not very precise results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working on the daily closing prices and logreturns, in this paper we deal\nwith the use of Hidden Markov Models (HMMs) to forecast the price of the\nEUR/USD Futures. The aim of our work is to understand how the HMMs describe\ndifferent financial time series depending on their structure. Subsequently, we\nanalyse the forecasting methods exposed in the previous literature, putting on\nevidence their pros and cons.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:21:49 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 11:59:23 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Rebagliati", "Sara", ""], ["Sasso", "Emanuela", ""], ["Soraggi", "Samuele", ""]]}, {"id": "1504.08142", "submitter": "Haiping Lu", "authors": "Qiquan Shi and Haiping Lu", "title": "Semi-Orthogonal Multilinear PCA with Relaxed Start", "comments": "8 pages, 2 figures, to appear in Proceedings of the 24th\n  International Joint Conference on Artificial Intelligence (IJCAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is an unsupervised method for learning\nlow-dimensional features with orthogonal projections. Multilinear PCA methods\nextend PCA to deal with multidimensional data (tensors) directly via\ntensor-to-tensor projection or tensor-to-vector projection (TVP). However,\nunder the TVP setting, it is difficult to develop an effective multilinear PCA\nmethod with the orthogonality constraint. This paper tackles this problem by\nproposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA\nlearns low-dimensional features directly from tensors via TVP by imposing the\northogonality constraint in only one mode. This formulation results in more\ncaptured variance and more learned features than full orthogonality. For better\ngeneralization, we further introduce a relaxed start (RS) strategy to get\nSO-MPCA-RS by fixing the starting projection vectors, which increases the bias\nand reduces the variance of the learning model. Experiments on both face (2D)\nand gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing\nalgorithms on the whole, and the relaxed start strategy is also effective for\nother TVP-based PCA methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:40:09 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 01:40:27 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Shi", "Qiquan", ""], ["Lu", "Haiping", ""]]}, {"id": "1504.08190", "submitter": "Riccardo Sven Risuleo", "authors": "Riccardo Sven Risuleo, Giulio Bottegal and H{\\aa}kan Hjalmarsson", "title": "A new kernel-based approach for overparameterized Hammerstein system\n  identification", "comments": "17 pages, submitted to IEEE Conference on Decision and Control 2015", "journal-ref": null, "doi": "10.1109/cdc.2015.7402095", "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new identification scheme for Hammerstein systems,\nwhich are dynamic systems consisting of a static nonlinearity and a linear\ntime-invariant dynamic system in cascade. We assume that the nonlinear function\ncan be described as a linear combination of $p$ basis functions. We reconstruct\nthe $p$ coefficients of the nonlinearity together with the first $n$ samples of\nthe impulse response of the linear system by estimating an $np$-dimensional\noverparameterized vector, which contains all the combinations of the unknown\nvariables. To avoid high variance in these estimates, we adopt a regularized\nkernel-based approach and, in particular, we introduce a new kernel tailored\nfor Hammerstein system identification. We show that the resulting scheme\nprovides an estimate of the overparameterized vector that can be uniquely\ndecomposed as the combination of an impulse response and $p$ coefficients of\nthe static nonlinearity. We also show, through several numerical experiments,\nthat the proposed method compares very favorably with two standard methods for\nHammerstein system identification.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 12:24:38 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 10:07:34 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Risuleo", "Riccardo Sven", ""], ["Bottegal", "Giulio", ""], ["Hjalmarsson", "H\u00e5kan", ""]]}, {"id": "1504.08196", "submitter": "Riccardo Sven Risuleo", "authors": "Riccardo Sven Risuleo, Giulio Bottegal and H{\\aa}kan Hjalmarsson", "title": "On the estimation of initial conditions in kernel-based system\n  identification", "comments": "16 pages, accepted for publication at IEEE Conference on Decision and\n  Control 2015", "journal-ref": null, "doi": "10.1109/cdc.2015.7402361", "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in system identification have brought attention to\nregularized kernel-based methods, where, adopting the recently introduced\nstable spline kernel, prior information on the unknown process is enforced.\nThis reduces the variance of the estimates and thus makes kernel-based methods\nparticularly attractive when few input-output data samples are available. In\nsuch cases however, the influence of the system initial conditions may have a\nsignificant impact on the output dynamics. In this paper, we specifically\naddress this point. We propose three methods that deal with the estimation of\ninitial conditions using different types of information. The methods consist in\nvarious mixed maximum likelihood--a posteriori estimators which estimate the\ninitial conditions and tune the hyperparameters characterizing the stable\nspline kernel. To solve the related optimization problems, we resort to the\nexpectation-maximization method, showing that the solutions can be attained by\niterating among simple update steps. Numerical experiments show the advantages,\nin terms of accuracy in reconstructing the system impulse response, of the\nproposed strategies, compared to other kernel-based schemes not accounting for\nthe effect initial conditions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 12:41:49 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 14:54:44 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 11:17:26 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Risuleo", "Riccardo Sven", ""], ["Bottegal", "Giulio", ""], ["Hjalmarsson", "H\u00e5kan", ""]]}, {"id": "1504.08215", "submitter": "Tapani Raiko", "authors": "Antti Rasmus, Harri Valpola, Tapani Raiko", "title": "Lateral Connections in Denoising Autoencoders Support Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for\nlayer-wise pretraining. It improves the state of the art significantly in the\npermutation-invariant MNIST classification task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:26:46 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Raiko", "Tapani", ""]]}, {"id": "1504.08219", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha and Neill D.F. Campbell and Jan Kautz and Gabriel J.\n  Brostow", "title": "Hierarchical Subquery Evaluation for Active Learning on a Graph", "comments": "CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train good supervised and semi-supervised object classifiers, it is\ncritical that we not waste the time of the human experts who are providing the\ntraining labels. Existing active learning strategies can have uneven\nperformance, being efficient on some datasets but wasteful on others, or\ninconsistent just between runs on the same dataset. We propose perplexity based\ngraph construction and a new hierarchical subquery evaluation algorithm to\ncombat this variability, and to release the potential of Expected Error\nReduction.\n  Under some specific circumstances, Expected Error Reduction has been one of\nthe strongest-performing informativeness criteria for active learning. Until\nnow, it has also been prohibitively costly to compute for sizeable datasets. We\ndemonstrate our highly practical algorithm, comparing it to other active\nlearning measures on classification datasets that vary in sparsity,\ndimensionality, and size. Our algorithm is consistent over multiple runs and\nachieves high accuracy, while querying the human expert for labels at a\nfrequency that matches their desired time budget.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:35:59 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Campbell", "Neill D. F.", ""], ["Kautz", "Jan", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.08291", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy?", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2546221", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 16:14:52 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 11:30:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 13:53:11 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 19:25:05 GMT"}, {"version": "v5", "created": "Mon, 14 Mar 2016 19:17:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}]